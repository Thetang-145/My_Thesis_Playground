,paper_id,input,output_bestRouge1,output_bestRougeAvg
0,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,Federated learning ( FL ) USED-FOR machine learning models. decentralized data sources USED-FOR machine learning models. local differential privacy constraints USED-FOR FL. communication efficiency CONJUNCTION highdimensional compatibility. highdimensional compatibility CONJUNCTION communication efficiency. sqSGD ( selective quantized stochastic gradient descent ) HYPONYM-OF gradient - based learning algorithm. privacy - preserving quantization scheme USED-FOR algorithm. training performance CONJUNCTION communication costs. communication costs CONJUNCTION training performance. fixed privacy budget USED-FOR gradient subsampling strategy. communication costs EVALUATE-FOR gradient subsampling strategy. training performance EVALUATE-FOR gradient subsampling strategy. randomized rotation USED-FOR quantization error. quantization CONJUNCTION perturbation. perturbation CONJUNCTION quantization. perturbation USED-FOR FL algorithm. quantization USED-FOR FL algorithm. privacy and communication constraints FEATURE-OF FL algorithm. benchmark datasets EVALUATE-FOR framework. LeNet CONJUNCTION ResNet. ResNet CONJUNCTION LeNet. sqSGD USED-FOR large models. local privacy constraints FEATURE-OF large models. ResNet HYPONYM-OF large models. LeNet HYPONYM-OF large models. sqSGD COMPARE baseline algorithms. baseline algorithms COMPARE sqSGD. fixed privacy and communication level FEATURE-OF sqSGD. OtherScientificTerm is sensitive data disclosures. Method is privacy - preserving FL algorithms. Generic is base algorithm. ,"This paper proposes a new gradient-based learning algorithm called sqSGD (selective quantized stochastic gradient descent) for federated learning (FL) to train machine learning models with decentralized data sources under local differential privacy constraints. The proposed algorithm is based on a privacy-preserving quantization scheme, and the authors propose a gradient subsampling strategy with a fixed privacy budget to improve the training performance and communication costs. The key idea is to use randomized rotation to reduce the quantization error, and then use the quantized data from the base algorithm as a perturbation for the FL algorithm. The authors show that the gradient subsample of the FL algorithms is more efficient than the base FL algorithm under the same privacy and communication constraints. They also show that their framework is robust to sensitive data disclosures and can be applied to large models with local privacy constraints (LeNet and ResNet). Finally, the authors demonstrate the effectiveness of their framework on a number of benchmark datasets. They show the convergence rate and the convergence speed of their algorithm under a fixed private and public privacy budget. In addition, they also show the practicality of their sqSgd with fixed privacy and communicating level.","This paper proposes a new gradient-based learning algorithm called sqSGD (selective quantized stochastic gradient descent) for federated learning (FL) to train machine learning models with decentralized data sources under local differential privacy constraints. The proposed algorithm is based on a privacy-preserving quantization scheme, and the authors propose a gradient subsampling strategy with a fixed privacy budget to improve the training performance and communication costs. The key idea is to use randomized rotation to reduce the quantization error, and then use the quantized data from the base algorithm as a perturbation for the FL algorithm. The authors show that the gradient subsample of the FL algorithms is more efficient than the base FL algorithm under the same privacy and communication constraints. They also show that their framework is robust to sensitive data disclosures and can be applied to large models with local privacy constraints (LeNet and ResNet). Finally, the authors demonstrate the effectiveness of their framework on a number of benchmark datasets. They show the convergence rate and the convergence speed of their algorithm under a fixed private and public privacy budget. In addition, they also show the practicality of their sqSgd with fixed privacy and communicating level."
9,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,Self - attention networks ( SANs ) USED-FOR natural language processing tasks. it USED-FOR language representation. language knowledge USED-FOR it. prior knowledge USED-FOR language representation. prior knowledge USED-FOR general representation method. method USED-FOR SANs. prior knowledge USED-FOR SANs. it USED-FOR language representation. it USED-FOR prior word frequency knowledge. prior word frequency knowledge CONJUNCTION prior translation lexicon knowledge. prior translation lexicon knowledge CONJUNCTION prior word frequency knowledge. it USED-FOR prior translation lexicon knowledge. prior translation lexicon knowledge USED-FOR bilingual data. prior word frequency knowledge CONJUNCTION monolingual data. monolingual data CONJUNCTION prior word frequency knowledge. method COMPARE Transformer - based baseline. Transformer - based baseline COMPARE method. Method is neural networks. ,"Self-attention networks (SANs) have been widely used for natural language processing tasks. This paper proposes a general representation method that leverages prior knowledge to learn a language representation and uses it to improve the performance of language representation. The authors propose a method that uses prior knowledge for SANs. Specifically, it uses prior word frequency knowledge for bilingual data, prior translation lexicon knowledge for monolingual data, and it uses it for language representation for both bilingual data. Experiments show that the proposed method outperforms the Transformer-based baseline.   ","Self-attention networks (SANs) have been widely used for natural language processing tasks. This paper proposes a general representation method that leverages prior knowledge to learn a language representation and uses it to improve the performance of language representation. The authors propose a method that uses prior knowledge for SANs. Specifically, it uses prior word frequency knowledge for bilingual data, prior translation lexicon knowledge for monolingual data, and it uses it for language representation for both bilingual data. Experiments show that the proposed method outperforms the Transformer-based baseline.   "
18,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,"cat - and - mouse game USED-FOR cybersecurity. Moving Target Defense ( MTD ) HYPONYM-OF proactive defense methods. leader - follower games USED-FOR MTD. models USED-FOR sequential settings. incomplete information FEATURE-OF rational adversary. learning defense policies USED-FOR cyber - security. learning defense policies USED-FOR sequential settings. sequential settings USED-FOR cyber - security. optimal movement policy USED-FOR BSMGs. interaction USED-FOR optimal movement policy. Bayesian Stackelberg Markov Games ( BSMGs ) HYPONYM-OF game - theoretic model. BSMGs PART-OF landscape of incomplete - information Markov games. Strong Stackelberg Equilibrium ( SSE ) FEATURE-OF them. learning approach USED-FOR SSE. learning approach USED-FOR BSMG. MTD USED-FOR web - application security. optimal policy USED-FOR MTD domains. movement policy USED-FOR optimal policy. SSE FEATURE-OF BSMG. incomplete information FEATURE-OF MTD domains. MTD EVALUATE-FOR movement policy. OtherScientificTerm are reconnaissance, sub - optimal movement strategies, incomplete - information Markov games, and prior information. Method are movement strategies, defense policies, single - agent reinforcement learning techniques, and MTD system. Generic is they. ","This paper studies the problem of cybersecurity in a cat-and-mouse game, where the adversary is able to observe the agent’s movements during the course of reconnaissance. The authors propose two novel proactive defense methods, namely, Moving Target Defense (MTD) and a Bayesian Stackelberg Markov game (BSMG). MTD is based on leader-follower games in which the agent is encouraged to move in a way that minimizes the risk of being detected by the adversary. However, in this setting, the adversary has access to information about the agent's current state and the current movement strategies. The paper proposes learning defense policies for learning these sequential settings for the purposes of cyber-security. The key idea is to learn an optimal movement policy for BSMGs, which is a game-theoretic model based on the notion of interaction between the agent and the adversary, and to use these models for sequential settings where the attacker has access only to the current state of the game.    The authors show that under certain conditions (i.e., that the attacker does not have access to prior information, that the agent has sub-optimal movement strategies, and that there is a rational adversary with incomplete information), the optimal defense can be learned by learning a learning defense policy that maximizes the mutual information between the attacker and the agent.  The paper also shows that the optimal policy for MTD in MTD domains can be learnt using a single movement policy, and can be trained using single-agent reinforcement learning techniques.  In particular, the authors propose Bayesian Markov Markov Games (BSMGs) which are incomplete-information Markov games, which are a subset of the landscape of landscape of incomplete-informal games. BSMGCs are a family of games where the agents are allowed to explore the entire game space, and the goal is to find a solution that maximally minimises the expected return of the attacker. In order to do so, the paper proposes a learning approach to learn a BSMG that maximises the SSE between the optimal and suboptimal solutions of the two players in the game, which allows them to avoid the sub-optimality issue of the Bayesian game. They show that this learning approach leads to an SSE that matches the optimal solution of the MTD system. They also show that MTD can be used for web-applications security, and they show that their optimal policy achieves SSE on MTD","This paper studies the problem of cybersecurity in a cat-and-mouse game, where the adversary is able to observe the agent’s movements during the course of reconnaissance. The authors propose two novel proactive defense methods, namely, Moving Target Defense (MTD) and a Bayesian Stackelberg Markov game (BSMG). MTD is based on leader-follower games in which the agent is encouraged to move in a way that minimizes the risk of being detected by the adversary. However, in this setting, the adversary has access to information about the agent's current state and the current movement strategies. The paper proposes learning defense policies for learning these sequential settings for the purposes of cyber-security. The key idea is to learn an optimal movement policy for BSMGs, which is a game-theoretic model based on the notion of interaction between the agent and the adversary, and to use these models for sequential settings where the attacker has access only to the current state of the game.    The authors show that under certain conditions (i.e., that the attacker does not have access to prior information, that the agent has sub-optimal movement strategies, and that there is a rational adversary with incomplete information), the optimal defense can be learned by learning a learning defense policy that maximizes the mutual information between the attacker and the agent.  The paper also shows that the optimal policy for MTD in MTD domains can be learnt using a single movement policy, and can be trained using single-agent reinforcement learning techniques.  In particular, the authors propose Bayesian Markov Markov Games (BSMGs) which are incomplete-information Markov games, which are a subset of the landscape of landscape of incomplete-informal games. BSMGCs are a family of games where the agents are allowed to explore the entire game space, and the goal is to find a solution that maximally minimises the expected return of the attacker. In order to do so, the paper proposes a learning approach to learn a BSMG that maximises the SSE between the optimal and suboptimal solutions of the two players in the game, which allows them to avoid the sub-optimality issue of the Bayesian game. They show that this learning approach leads to an SSE that matches the optimal solution of the MTD system. They also show that MTD can be used for web-applications security, and they show that their optimal policy achieves SSE on MTD"
27,SP:97911e02bf06b34d022e7548beb5169a1d825903,"unsupervised disentangled representation learning EVALUATE-FOR Variational Autoencoder ( VAE ) based frameworks. VAE im3 plementation choices USED-FOR PCA - like behavior. PCA - like behavior FEATURE-OF data sam4 ples. local orthogonality CONJUNCTION data re6 construction. data re6 construction CONJUNCTION local orthogonality. models USED-FOR entangled representations. architecture CONJUNCTION hyperparameter 7 setting. hyperparameter 7 setting CONJUNCTION architecture. architecture USED-FOR models. hyperparameter 7 setting FEATURE-OF models. multi9 ple VAEs PART-OF VAE ensemble framework. VAE ensemble 15 objective USED-FOR linear transformations. approach COMPARE unsupervised disen18 tangled representation learning approaches. unsupervised disen18 tangled representation learning approaches COMPARE approach. OtherScientificTerm are model identifiability, disentangled representations, signed permutation transformation, pair - wise linear transformations, VAEs, triv16 ial transformations, and latent representations. Method are VAE based disentanglement 5 frameworks, and VAE ensemble. Generic is It. ","This paper studies the problem of unsupervised disentangled representation learning in Variational Autoencoder (VAE) based frameworks. The authors show that VAE im3 plementation choices exhibit a PCA-like behavior in data sam4 ples, which is consistent with previous VAE based disentanglement 5 frameworks. They also show that models trained with the same architecture and hyperparameter 7 setting are able to learn entangled representations.    The main contribution of this paper is to show that the model identifiability of VAE ensembles is not a result of local orthogonality or data re6 construction, but rather of a signed permutation transformation. This is because VAEs are trained with pair-wise linear transformations, and the VAE ensemble 15 objective only considers linear transformations that are orthogonal to each other. It is also shown that VAEs can be trained with triv16 ial transformations (i.e. pairwise permutations of the latent representations).  The paper also shows that the multi9 ple VAEs in the VAe ensemble framework can be seen as a special case of multi9ple VAEs.  The authors further show that their approach is able to outperform state-of-the-art state-and-art disen18 tangled representation learning approaches in terms of performance. ","This paper studies the problem of unsupervised disentangled representation learning in Variational Autoencoder (VAE) based frameworks. The authors show that VAE im3 plementation choices exhibit a PCA-like behavior in data sam4 ples, which is consistent with previous VAE based disentanglement 5 frameworks. They also show that models trained with the same architecture and hyperparameter 7 setting are able to learn entangled representations.    The main contribution of this paper is to show that the model identifiability of VAE ensembles is not a result of local orthogonality or data re6 construction, but rather of a signed permutation transformation. This is because VAEs are trained with pair-wise linear transformations, and the VAE ensemble 15 objective only considers linear transformations that are orthogonal to each other. It is also shown that VAEs can be trained with triv16 ial transformations (i.e. pairwise permutations of the latent representations).  The paper also shows that the multi9 ple VAEs in the VAe ensemble framework can be seen as a special case of multi9ple VAEs.  The authors further show that their approach is able to outperform state-of-the-art state-and-art disen18 tangled representation learning approaches in terms of performance. "
36,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,zero - shot approach USED-FOR automated machine learning ( AutoML ). model USED-FOR supervised learning task. zero - shot approach USED-FOR model. approach USED-FOR AutoML. meta - feature extractor USED-FOR data. free - text descriptions CONJUNCTION meta - feature extractor. meta - feature extractor CONJUNCTION free - text descriptions. transformer - based language embedding USED-FOR algorithms. meta - feature extractor USED-FOR method. free - text descriptions USED-FOR algorithms. transformer - based language embedding USED-FOR method. meta - feature extractor USED-FOR algorithms. graph neural network USED-FOR machine learning pipeline. approach USED-FOR AutoML. unsupervised representation learning USED-FOR AutoML. unsupervised representation learning USED-FOR natural language processing. unsupervised representation learning USED-FOR approach. Method is AutoML systems. Metric is running time. OtherScientificTerm is prediction time. ,"This paper proposes a zero-shot approach to automated machine learning (AutoML) where a model is trained on a supervised learning task and then used to train a model on a new task. The proposed approach to AutoML is based on a meta-feature extractor that learns to extract features from the data. The method uses a transformer-based language embedding to train the algorithms based on free-text descriptions and meta-features extracted from existing AutoML systems. A graph neural network is used to guide the machine learning pipeline. The paper shows that the proposed approach is able to improve the performance of AutoML by unsupervised representation learning for natural language processing, and that the running time is significantly faster than the previous state-of-the-art.  ","This paper proposes a zero-shot approach to automated machine learning (AutoML) where a model is trained on a supervised learning task and then used to train a model on a new task. The proposed approach to AutoML is based on a meta-feature extractor that learns to extract features from the data. The method uses a transformer-based language embedding to train the algorithms based on free-text descriptions and meta-features extracted from existing AutoML systems. A graph neural network is used to guide the machine learning pipeline. The paper shows that the proposed approach is able to improve the performance of AutoML by unsupervised representation learning for natural language processing, and that the running time is significantly faster than the previous state-of-the-art.  "
45,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,optimization process USED-FOR non - compositional solutions. compositionality learning approaches USED-FOR compositionality. model architecture design USED-FOR compositionality learning approaches. compositional learning CONJUNCTION gradient descent. gradient descent CONJUNCTION compositional learning. machine learning models USED-FOR human - level intelligence. Method is neural network optimization. Task is compositional generalization. ,"This paper studies the problem of compositional generalization in the context of neural network optimization. The authors show that the optimization process can lead to non-compositional solutions when the model architecture design is not well-suited for compositionality learning approaches that aim to learn compositionality. They also show that compositional learning and gradient descent can be seen as a special case of this problem. Finally, they show that machine learning models can be used to capture human-level intelligence and generalize well.","This paper studies the problem of compositional generalization in the context of neural network optimization. The authors show that the optimization process can lead to non-compositional solutions when the model architecture design is not well-suited for compositionality learning approaches that aim to learn compositionality. They also show that compositional learning and gradient descent can be seen as a special case of this problem. Finally, they show that machine learning models can be used to capture human-level intelligence and generalize well."
54,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"Knowledge graph ( KG ) representation learning USED-FOR entity alignment. machine translation CONJUNCTION feature extraction. feature extraction CONJUNCTION machine translation. methods COMPARE embeddingbased ones. embeddingbased ones COMPARE methods. embedding spaces PART-OF KGs. pre - aligned entities USED-FOR embedding spaces. scoring function USED-FOR embedding learning. margin FEATURE-OF scoring function. margin USED-FOR representation discrepancy. approach USED-FOR KG - invariant and principled entity representations. feature distribution CONJUNCTION ontology knowledge. ontology knowledge CONJUNCTION feature distribution. neural ontologies PART-OF KGs. state - of - the - art ones HYPONYM-OF embedding - based entity alignment methods. Generic are they, paradigm, and model. Method is alignment learning. OtherScientificTerm is geometric distance. ","Knowledge graph (KG) representation learning for entity alignment is an important problem in machine translation and feature extraction. However, existing methods do not compare favourably to embeddingbased ones because they do not consider the feature distribution or the ontology knowledge in KGs. In this paper, the authors propose a new paradigm where the embedding spaces of KGs are learned from pre-aligned entities. The key idea is to use a geometric distance between two entities in the KG as a scoring function for embedding learning. The margin of this scoring function is then used to measure the representation discrepancy between the two entities.  The authors show that the proposed approach is able to learn KG-invariant and principled entity representations that are invariant to alignment learning. They also show that this approach can be applied to any embedding-based entity alignment methods, including state-of-the-art ones. The paper also shows that neural ontologies can be embedded into KGs and that the learned embedding space is invariant.   ","Knowledge graph (KG) representation learning for entity alignment is an important problem in machine translation and feature extraction. However, existing methods do not compare favourably to embeddingbased ones because they do not consider the feature distribution or the ontology knowledge in KGs. In this paper, the authors propose a new paradigm where the embedding spaces of KGs are learned from pre-aligned entities. The key idea is to use a geometric distance between two entities in the KG as a scoring function for embedding learning. The margin of this scoring function is then used to measure the representation discrepancy between the two entities.  The authors show that the proposed approach is able to learn KG-invariant and principled entity representations that are invariant to alignment learning. They also show that this approach can be applied to any embedding-based entity alignment methods, including state-of-the-art ones. The paper also shows that neural ontologies can be embedded into KGs and that the learned embedding space is invariant.   "
63,SP:0e42de72d10040289283516ec1bd324788f7d371,"Convolutional Neural Networks ( CNNs ) powered functionalities USED-FOR ubiquitous intelligent “ IoT cameras ”. medicineand wearable - related ones HYPONYM-OF applications. CNNs COMPARE IoT devices. IoT devices COMPARE CNNs. limited resources FEATURE-OF IoT devices. storage and energy cost USED-FOR CNNs. form factor FEATURE-OF PhlatCam. compression techniques USED-FOR storage and energy reduction. Sensor Algorithm Co - Design framework USED-FOR CNN - powered PhlatCam. SACoD USED-FOR CNN - powered PhlatCam. SACoD HYPONYM-OF Sensor Algorithm Co - Design framework. mask CONJUNCTION backend CNN model. backend CNN model CONJUNCTION mask. PhlatCam sensor CONJUNCTION backend CNN model. backend CNN model CONJUNCTION PhlatCam sensor. mask PART-OF PhlatCam sensor. differential neural architecture search USED-FOR mask. model compression CONJUNCTION energy savings. energy savings CONJUNCTION model compression. energy savings EVALUATE-FOR SACoD framework. model compression EVALUATE-FOR SACoD framework. task accuracy EVALUATE-FOR SACoD framework. PhlatCam imaging system EVALUATE-FOR SACoD. Method are IoT systems, CNN algorithm, and SOTA ) designs. Metric is camera form factor. OtherScientificTerm is model parameters. Generic is tasks. ","This paper proposes a method to improve the efficiency of convolutional neural networks (CNNs) in the context of intelligent cameras. The authors propose a method called SACoD, which is based on the SAC algorithm. The method is applied to the design of a new type of camera called PhlatCam, which can be seen as an extension of the recently proposed SOTA SOTA camera (SOTA-IoT).    The main contribution of the paper is that the proposed method is able to achieve state-of-the-art performance in terms of model compression and energy efficiency. ","This paper proposes a method to improve the efficiency of convolutional neural networks (CNNs) in the context of intelligent cameras. The authors propose a method called SACoD, which is based on the SAC algorithm. The method is applied to the design of a new type of camera called PhlatCam, which can be seen as an extension of the recently proposed SOTA SOTA camera (SOTA-IoT).    The main contribution of the paper is that the proposed method is able to achieve state-of-the-art performance in terms of model compression and energy efficiency. "
72,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"Honey bees USED-FOR complex social systems. dataset USED-FOR temporal matrix factorization model. temporal matrix factorization model USED-FOR average developmental path. lifetime trajectories PART-OF dataset. social sciences CONJUNCTION neuroscience. neuroscience CONJUNCTION social sciences. behavioral biology CONJUNCTION social sciences. social sciences CONJUNCTION behavioral biology. neuroscience CONJUNCTION information science. information science CONJUNCTION neuroscience. method USED-FOR behavioral heterogeneity. behavioral heterogeneity FEATURE-OF complex social systems. information science HYPONYM-OF fields. behavioral biology HYPONYM-OF fields. social sciences HYPONYM-OF fields. neuroscience HYPONYM-OF fields. OtherScientificTerm are global behavior, and social network. Material is honey bee colonies. ","This paper presents a new dataset of life history trajectories of honey bees to study complex social systems. The dataset consists of lifetime trajectories and a temporal matrix factorization model for the average developmental path of each individual in the dataset. The authors show that the global behavior of a population of bees is highly heterogeneous, and that the behavior of individual bees in a social network is highly homogenous. They also show that their method is able to capture the behavioral heterogeneity in complex social system. The paper is well-written, well-motivated, and well-structured. It is a well-designed paper with interesting contributions in a number of fields, including behavioral biology, social sciences, neuroscience, and information science.    The paper also provides a detailed analysis of the behavior in the honey bee colonies. ","This paper presents a new dataset of life history trajectories of honey bees to study complex social systems. The dataset consists of lifetime trajectories and a temporal matrix factorization model for the average developmental path of each individual in the dataset. The authors show that the global behavior of a population of bees is highly heterogeneous, and that the behavior of individual bees in a social network is highly homogenous. They also show that their method is able to capture the behavioral heterogeneity in complex social system. The paper is well-written, well-motivated, and well-structured. It is a well-designed paper with interesting contributions in a number of fields, including behavioral biology, social sciences, neuroscience, and information science.    The paper also provides a detailed analysis of the behavior in the honey bee colonies. "
81,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"Deep neural networks USED-FOR image restoration and reconstruction tasks. noisy or corrupted measurement USED-FOR networks. pipeline USED-FOR data augmentation. Data Augmentation ( DA ) USED-FOR classification problems. data augmentation USED-FOR image reconstruction tasks. medical imaging FEATURE-OF image reconstruction tasks. invariances FEATURE-OF medical imaging measurements. naive DA strategies USED-FOR DA pipeline. invariances USED-FOR DA pipeline. problem regimes EVALUATE-FOR DA. fastMRI dataset EVALUATE-FOR DA. training data USED-FOR single - coil reconstruction. training data USED-FOR multi - coil reconstruction. multi - coil reconstruction CONJUNCTION single - coil reconstruction. single - coil reconstruction CONJUNCTION multi - coil reconstruction. training data CONJUNCTION training data. training data CONJUNCTION training data. Task is accelerated magnetic resonance imaging. OtherScientificTerm are under - sampled linear measurements, and high - data regime. Method is data augmentation pipeline. ","This paper studies the problem of data augmentation in accelerated magnetic resonance imaging.   Deep neural networks are trained for image restoration and reconstruction tasks, but under-sampled linear measurements can be noisy or corrupted, which can cause the networks to learn a noisy or corruptive bias.  Data Augmentation (DA) has been shown to be effective for classification problems, but not for image reconstruction tasks in medical imaging. In this paper, the authors propose a new pipeline to perform data augmentations in the high-data regime.  The authors show that the proposed DA pipeline is robust to invariances of medical imaging measurements, and that naive DA strategies do not work well in the DA pipeline.  Experiments are conducted on two problem regimes, where DA is applied to the fastMRI dataset, and show that DA works well on multi-coil reconstruction with training data, multi- coil reconstruction with the same training data as in the original paper, and single-coillrop reconstruction with different training data. The authors also show that a data augmentation pipeline can be used to improve the performance of existing DA strategies. ","This paper studies the problem of data augmentation in accelerated magnetic resonance imaging.   Deep neural networks are trained for image restoration and reconstruction tasks, but under-sampled linear measurements can be noisy or corrupted, which can cause the networks to learn a noisy or corruptive bias.  Data Augmentation (DA) has been shown to be effective for classification problems, but not for image reconstruction tasks in medical imaging. In this paper, the authors propose a new pipeline to perform data augmentations in the high-data regime.  The authors show that the proposed DA pipeline is robust to invariances of medical imaging measurements, and that naive DA strategies do not work well in the DA pipeline.  Experiments are conducted on two problem regimes, where DA is applied to the fastMRI dataset, and show that DA works well on multi-coil reconstruction with training data, multi- coil reconstruction with the same training data as in the original paper, and single-coillrop reconstruction with different training data. The authors also show that a data augmentation pipeline can be used to improve the performance of existing DA strategies. "
90,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"deep repulsive clustering ( DRC ) algorithm USED-FOR order learning. ordered data USED-FOR deep repulsive clustering ( DRC ) algorithm. order - related feature CONJUNCTION identity feature. identity feature CONJUNCTION order - related feature. facial age estimation CONJUNCTION aesthetic score regression. aesthetic score regression CONJUNCTION facial age estimation. aesthetic score regression CONJUNCTION historical color image classification. historical color image classification CONJUNCTION aesthetic score regression. algorithm USED-FOR ordered data. historical color image classification EVALUATE-FOR algorithm. facial age estimation EVALUATE-FOR algorithm. rank estimation EVALUATE-FOR algorithm. Method is order - identity decomposition ( ORID ) network. OtherScientificTerm are identity features, repulsive term, and rank. ","This paper proposes a deep repulsive clustering (DRC) algorithm for order learning on ordered data. The authors propose an order-identity decomposition (ORID) network, where the order-related feature is combined with the identity feature, and the identity features are repulsively repulsive. The repulsive term is defined as the difference between the rank of the ordered data and that of the identity data. Experiments on facial age estimation, aesthetic score regression, and historical color image classification show that the proposed algorithm is able to learn ordered data with high rank estimation. ","This paper proposes a deep repulsive clustering (DRC) algorithm for order learning on ordered data. The authors propose an order-identity decomposition (ORID) network, where the order-related feature is combined with the identity feature, and the identity features are repulsively repulsive. The repulsive term is defined as the difference between the rank of the ordered data and that of the identity data. Experiments on facial age estimation, aesthetic score regression, and historical color image classification show that the proposed algorithm is able to learn ordered data with high rank estimation. "
99,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"Exploration PART-OF model - free reinforcement learning. sparse reward USED-FOR Exploration. intrinsic rewards USED-FOR state - of - the - art methods. methods USED-FOR procedurally - generated environments. intrinsic rewards USED-FOR methods. episode - level exploration method USED-FOR procedurally - generated environments. RAPID HYPONYM-OF episode - level exploration method. per - episode and long - term views FEATURE-OF episodic exploration score. episodic exploration score EVALUATE-FOR RAPID. sparse MuJoCo tasks EVALUATE-FOR method. procedurally - generated MiniGrid environments EVALUATE-FOR method. RAPID COMPARE intrinsic reward strategies. intrinsic reward strategies COMPARE RAPID. sample efficiency EVALUATE-FOR intrinsic reward strategies. sample efficiency EVALUATE-FOR RAPID. OtherScientificTerm are uncertain environment dynamics, and ranking buffer. Material is MiniWorld. ","Exploration in model-free reinforcement learning is an important problem in the presence of uncertain environment dynamics. Exploration with sparse reward is one of the most commonly used techniques in the literature. However, state-of-the-art methods rely on intrinsic rewards, which can be prohibitively expensive for procedurally-generated environments. This paper proposes a novel episode-level exploration method called RAPID, which is an extension of previous methods to procedurally generated environments. The authors show that the episodic exploration score is a weighted sum of the per-episode and long-term views of the environment, and that RAPid can be used to compute the per episode and per-long-term exploration score. The method is evaluated on sparse MuJoCo tasks, and is shown to outperform previous methods on procedurally -generated MiniGrid environments. RapID is also shown to be more sample efficient than previous intrinsic reward strategies, and the authors also show that Rapid is able to learn a ranking buffer that is more robust to changes in the environment dynamics, and can be trained on MiniWorld. ","Exploration in model-free reinforcement learning is an important problem in the presence of uncertain environment dynamics. Exploration with sparse reward is one of the most commonly used techniques in the literature. However, state-of-the-art methods rely on intrinsic rewards, which can be prohibitively expensive for procedurally-generated environments. This paper proposes a novel episode-level exploration method called RAPID, which is an extension of previous methods to procedurally generated environments. The authors show that the episodic exploration score is a weighted sum of the per-episode and long-term views of the environment, and that RAPid can be used to compute the per episode and per-long-term exploration score. The method is evaluated on sparse MuJoCo tasks, and is shown to outperform previous methods on procedurally -generated MiniGrid environments. RapID is also shown to be more sample efficient than previous intrinsic reward strategies, and the authors also show that Rapid is able to learn a ranking buffer that is more robust to changes in the environment dynamics, and can be trained on MiniWorld. "
108,SP:30024ac5aef153ae24c893a53bad93ead2526476,"semantic space of class attributes CONJUNCTION visual space of images. visual space of images CONJUNCTION semantic space of class attributes. Isometric Propagation Network ( IPN ) USED-FOR class dependency. IPN USED-FOR class representations. auto - generated graph USED-FOR class representations. ZSL benchmarks EVALUATE-FOR IPN. them USED-FOR IPN. Method are Zero - shot learning ( ZSL ), static representation, and dynamic propagation procedures. OtherScientificTerm are class attributes, imbalanced supervision, and semantic and the visual space. Generic are representations, and mapping. Task is ZSL settings. Material is seen - class data. Metric is consistency loss. ","Zero-shot learning (ZSL) is an important problem in which the goal is to learn representations that generalize well to unseen classes. In ZSL settings, there is no imbalanced supervision, and the class attributes are assumed to be independent of each other. In order to learn a static representation that generalizes well, the authors propose Isometric Propagation Network (IPN), which learns a mapping between the semantic space of class attributes and the visual space of images. The authors show that the class dependency of the representations learned by IPN is similar to that of seen-class data, and that the consistency loss between semantic and visual representations is the same. They also show that IPN can learn class representations from an auto-generated graph.   The authors also propose dynamic propagation procedures that can be applied to any ZSL benchmarks, and use them to train the IPN. ","Zero-shot learning (ZSL) is an important problem in which the goal is to learn representations that generalize well to unseen classes. In ZSL settings, there is no imbalanced supervision, and the class attributes are assumed to be independent of each other. In order to learn a static representation that generalizes well, the authors propose Isometric Propagation Network (IPN), which learns a mapping between the semantic space of class attributes and the visual space of images. The authors show that the class dependency of the representations learned by IPN is similar to that of seen-class data, and that the consistency loss between semantic and visual representations is the same. They also show that IPN can learn class representations from an auto-generated graph.   The authors also propose dynamic propagation procedures that can be applied to any ZSL benchmarks, and use them to train the IPN. "
117,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"multi - task model USED-FOR tasks. HyperGrid Transformers HYPONYM-OF Transformer architecture. task - conditioned hyper networks USED-FOR feed - forward layers. task - conditioned hyper networks USED-FOR Transformer architecture. decomposable hypernetwork USED-FOR grid - wise projections. global ( task - agnostic ) state CONJUNCTION local task - specific state. local task - specific state CONJUNCTION global ( task - agnostic ) state. method USED-FOR hypernetwork. SuperGLUE test set EVALUATE-FOR state - of - the - art. fine - tuning CONJUNCTION multi - task learning approaches. multi - task learning approaches CONJUNCTION fine - tuning. method USED-FOR fine - tuning. method USED-FOR multi - task learning approaches. Task is natural language understanding tasks. Generic are model, and approach. OtherScientificTerm is weight matrices. Material is GLUE / SuperGLUE. ","This paper proposes HyperGrid Transformers, a multi-task learning model for natural language understanding tasks. The authors propose a Transformer architecture based on task-conditioned hyper networks to replace the feed-forward layers in the standard Transformer model for different tasks. Specifically, a decomposable hypernetwork is used to map grid-wise projections from a global (task-agnostic) state to a local task-specific state. The proposed model is trained on GLUE/SuperGLUE and is evaluated on the SuperGLUE test set against the state-of-the-art. The method is applied to fine-tune the hypernetwork and is shown to be effective for fine-tuning as well as for improving the performance of the model on a variety of tasks. ","This paper proposes HyperGrid Transformers, a multi-task learning model for natural language understanding tasks. The authors propose a Transformer architecture based on task-conditioned hyper networks to replace the feed-forward layers in the standard Transformer model for different tasks. Specifically, a decomposable hypernetwork is used to map grid-wise projections from a global (task-agnostic) state to a local task-specific state. The proposed model is trained on GLUE/SuperGLUE and is evaluated on the SuperGLUE test set against the state-of-the-art. The method is applied to fine-tune the hypernetwork and is shown to be effective for fine-tuning as well as for improving the performance of the model on a variety of tasks. "
126,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"lighting CONJUNCTION weather. weather CONJUNCTION lighting. weather CONJUNCTION visibility conditions. visibility conditions CONJUNCTION weather. image input USED-FOR autonomous driving. image input USED-FOR learning algorithm. algorithm USED-FOR task. sensitivity analysis USED-FOR algorithm. sensitivity analysis USED-FOR task. approach USED-FOR learning outcomes. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. neural network training USED-FOR self - driving cars. approach COMPARE techniques. techniques COMPARE approach. algorithm USED-FOR neural network training. robustness EVALUATE-FOR algorithm. adversarial training HYPONYM-OF techniques. data augmentation HYPONYM-OF techniques. OtherScientificTerm are external and environmental factors, and sensors. Task is perceptual data processing. ","This paper proposes a new learning algorithm for learning from image input for autonomous driving, where external and environmental factors such as lighting, weather, and visibility conditions can affect the performance of the learning algorithm. The authors propose an algorithm based on sensitivity analysis for this task. The proposed approach is able to learn learning outcomes that are robust to changes in external factors (e.g. weather, lighting, etc). The authors show that the proposed approach outperforms existing techniques such as data augmentation and adversarial training in neural network training for self-driving cars. The algorithm is also able to improve the robustness of the proposed algorithm in the presence of changes to the sensors and perceptual data processing. ","This paper proposes a new learning algorithm for learning from image input for autonomous driving, where external and environmental factors such as lighting, weather, and visibility conditions can affect the performance of the learning algorithm. The authors propose an algorithm based on sensitivity analysis for this task. The proposed approach is able to learn learning outcomes that are robust to changes in external factors (e.g. weather, lighting, etc). The authors show that the proposed approach outperforms existing techniques such as data augmentation and adversarial training in neural network training for self-driving cars. The algorithm is also able to improve the robustness of the proposed algorithm in the presence of changes to the sensors and perceptual data processing. "
135,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"deep networks USED-FOR approximate solvers. hard constraints FEATURE-OF Large optimization problems. hard constraints FEATURE-OF problems. method USED-FOR feasibility. gradient - based corrections USED-FOR inequality constraints. differentiable procedure USED-FOR method. differentiable procedure USED-FOR feasibility. DC3 USED-FOR AC optimal power flow. DC3 USED-FOR synthetic optimization tasks. synthetic optimization tasks CONJUNCTION AC optimal power flow. AC optimal power flow CONJUNCTION synthetic optimization tasks. hard constraints FEATURE-OF physics of the electrical grid. DC3 USED-FOR near - optimal objective values. feasibility FEATURE-OF DC3. Method are classical solvers, deep learning approaches, and Deep Constraint Completion and Correction ( DC3 ). OtherScientificTerm are infeasible solutions, and equality constraints. Generic is algorithm. ","Large optimization problems with hard constraints can be solved by deep networks, but classical solvers are infeasible due to the infeasibility of existing deep learning approaches. This paper proposes Deep Constraint Completion and Correction (DC3), a method to improve the feasibility of these approximate solvers using gradient-based corrections to inequality constraints. The proposed method is based on a differentiable procedure that can be applied to any algorithm. DC3 is tested on synthetic optimization tasks and the AC optimal power flow, and is shown to achieve near-optimal objective values. The authors also show that DC3 can be used to improve feasibility of existing algorithms with equality constraints. In addition, the authors show that the hard constraints of the physics of the electrical grid can be incorporated into DC3. ","Large optimization problems with hard constraints can be solved by deep networks, but classical solvers are infeasible due to the infeasibility of existing deep learning approaches. This paper proposes Deep Constraint Completion and Correction (DC3), a method to improve the feasibility of these approximate solvers using gradient-based corrections to inequality constraints. The proposed method is based on a differentiable procedure that can be applied to any algorithm. DC3 is tested on synthetic optimization tasks and the AC optimal power flow, and is shown to achieve near-optimal objective values. The authors also show that DC3 can be used to improve feasibility of existing algorithms with equality constraints. In addition, the authors show that the hard constraints of the physics of the electrical grid can be incorporated into DC3. "
144,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"Regularization USED-FOR sparsity. Regularization USED-FOR deep neural network pruning. pruning schedule CONJUNCTION weight importance scoring. weight importance scoring CONJUNCTION pruning schedule. problems PART-OF pruning. weight importance scoring HYPONYM-OF pruning. pruning schedule HYPONYM-OF pruning. weight importance scoring HYPONYM-OF problems. pruning schedule HYPONYM-OF problems. it COMPARE one - shot counterpart. one - shot counterpart COMPARE it. L2 regularization variant COMPARE one - shot counterpart. one - shot counterpart COMPARE L2 regularization variant. rising penalty factors FEATURE-OF L2 regularization variant. Hessian information USED-FOR pruning. growing penalty scheme USED-FOR approach. approach USED-FOR Hessian information. networks USED-FOR structured and unstructured pruning. algorithms USED-FOR large datasets. large datasets CONJUNCTION networks. networks CONJUNCTION large datasets. networks EVALUATE-FOR algorithms. CIFAR and ImageNet datasets EVALUATE-FOR deep neural networks. OtherScientificTerm are small penalty strength regime, and regularization. Task is Hessian approximation problems. Generic is state - of - the - art algorithms. ","Regularization in deep neural network pruning is a well-studied topic in the literature. Regularization is used to encourage sparsity in the small penalty strength regime.  This paper considers two problems in pruning: pruning schedule and weight importance scoring.  The authors consider Hessian approximation problems and show that it is a special case of the one-shot counterpart. They also show that the L2 regularization variant with rising penalty factors is more effective than its one-step counterpart.  Finally, the authors propose an approach to use Hessian information for pruning based on a growing penalty scheme.  Experiments are conducted on both structured and unstructured pruning on networks for both large datasets and networks for deep neural networks on CIFAR and ImageNet datasets. The results show that their algorithms perform well on large datasets, networks, and networks with a large number of layers.  They also compare against state-of-the-art algorithms.","Regularization in deep neural network pruning is a well-studied topic in the literature. Regularization is used to encourage sparsity in the small penalty strength regime.  This paper considers two problems in pruning: pruning schedule and weight importance scoring.  The authors consider Hessian approximation problems and show that it is a special case of the one-shot counterpart. They also show that the L2 regularization variant with rising penalty factors is more effective than its one-step counterpart.  Finally, the authors propose an approach to use Hessian information for pruning based on a growing penalty scheme.  Experiments are conducted on both structured and unstructured pruning on networks for both large datasets and networks for deep neural networks on CIFAR and ImageNet datasets. The results show that their algorithms perform well on large datasets, networks, and networks with a large number of layers.  They also compare against state-of-the-art algorithms."
153,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"Model - based planning USED-FOR deep, careful reasoning. deep, careful reasoning CONJUNCTION generalization. generalization CONJUNCTION deep, careful reasoning. Model - based planning USED-FOR generalization. generalization USED-FOR artificial agents. deep, careful reasoning USED-FOR artificial agents. deep function approximation USED-FOR model - based reinforcement learning ( MBRL ). planning USED-FOR MBRL agents. planning USED-FOR generalization. MuZero HYPONYM-OF MBRL algorithm. MuZero COMPARE MBRL algorithms. MBRL algorithms COMPARE MuZero. overlapping components CONJUNCTION MBRL algorithms. MBRL algorithms CONJUNCTION overlapping components. MBRL algorithm COMPARE MBRL algorithms. MBRL algorithms COMPARE MBRL algorithm. overlapping components PART-OF MuZero. overlapping components PART-OF MBRL algorithm. control tasks CONJUNCTION Atari. Atari CONJUNCTION control tasks. Atari CONJUNCTION 9x9 Go. 9x9 Go CONJUNCTION Atari. Planning USED-FOR learning process. Planning USED-FOR policy updates. Planning USED-FOR data distribution. Monte - Carlo rollouts USED-FOR shallow trees. Planning USED-FOR generalization. planning USED-FOR reinforcement learning settings. zeroand few - shot learning CONJUNCTION strategic thinking. strategic thinking CONJUNCTION zeroand few - shot learning. Model - based reinforcement learning ( MBRL ) COMPARE model - free methods. model - free methods COMPARE Model - based reinforcement learning ( MBRL ). data efficiency CONJUNCTION zeroand few - shot learning. zeroand few - shot learning CONJUNCTION data efficiency. zeroand few - shot learning EVALUATE-FOR model - free methods. data efficiency EVALUATE-FOR model - free methods. planning CONJUNCTION learning. learning CONJUNCTION planning. learning PART-OF methods. planning PART-OF methods. models USED-FOR intelligent artificial agents. models USED-FOR discrete search. planning PART-OF MBRL algorithm. MuZero HYPONYM-OF MBRL algorithm. value estimation CONJUNCTION policy optimization. policy optimization CONJUNCTION value estimation. learned model CONJUNCTION value estimation. value estimation CONJUNCTION learned model. search - based planning CONJUNCTION","Model-based planning enables deep, careful reasoning and good generalization for artificial agents trained with deep function approximation in model-based reinforcement learning (MBRL). The authors propose a new MBRL algorithm called MuZero, which has overlapping components that are more efficient than existing MBRL algorithms. Planning is used to guide the learning process for policy updates, and the authors show that planning improves the generalization performance of MBRL agents when using planning in reinforcement learning settings. The authors also show that Monte-Carlo rollouts can be used to train shallow trees, and that planning can be applied to any data distribution. Experiments are conducted on control tasks, Atari, and 9x9 Go, and show that MuZero outperforms other MBRL methods in terms of data efficiency, zeroand few-shot learning, and strategic thinking. Model-based RL is shown to outperform other model-free methods that combine planning and learning. The paper also shows that models trained to learn intelligent artificial agents can also be used as models for discrete search.    The paper is well-written and well-motivated. It is clear that the authors have made an important contribution to the field of learning with models. However, there is a lack of comparison to other methods that incorporate planning or learning in a more general way. The main contribution of the paper is that the paper does not compare the performance of MuZero with existing methods that do not include planning in the algorithm, which is a major limitation of the work. This paper also does not provide a thorough a thorough comparison between search-based and planning-based methods, which could be useful for the community. ","Model-based planning enables deep, careful reasoning and good generalization for artificial agents trained with deep function approximation in model-based reinforcement learning (MBRL). The authors propose a new MBRL algorithm called MuZero, which has overlapping components that are more efficient than existing MBRL algorithms. Planning is used to guide the learning process for policy updates, and the authors show that planning improves the generalization performance of MBRL agents when using planning in reinforcement learning settings. The authors also show that Monte-Carlo rollouts can be used to train shallow trees, and that planning can be applied to any data distribution. Experiments are conducted on control tasks, Atari, and 9x9 Go, and show that MuZero outperforms other MBRL methods in terms of data efficiency, zeroand few-shot learning, and strategic thinking. Model-based RL is shown to outperform other model-free methods that combine planning and learning. The paper also shows that models trained to learn intelligent artificial agents can also be used as models for discrete search.    The paper is well-written and well-motivated. It is clear that the authors have made an important contribution to the field of learning with models. However, there is a lack of comparison to other methods that incorporate planning or learning in a more general way. The main contribution of the paper is that the paper does not compare the performance of MuZero with existing methods that do not include planning in the algorithm, which is a major limitation of the work. This paper also does not provide a thorough a thorough comparison between search-based and planning-based methods, which could be useful for the community. "
162,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"long - range reasoning CONJUNCTION understanding of environment dynamics. understanding of environment dynamics CONJUNCTION long - range reasoning. Value Iteration Networks ( VINs ) USED-FOR implicit planning. long - range reasoning USED-FOR tasks. deep reinforcement learning USED-FOR implicit planning. understanding of environment dynamics USED-FOR tasks. graph representation learning CONJUNCTION neural algorithmic reasoning. neural algorithmic reasoning CONJUNCTION graph representation learning. contrastive self - supervised learning CONJUNCTION graph representation learning. graph representation learning CONJUNCTION contrastive self - supervised learning. generic environments USED-FOR VIN - style models. XLVINs COMPARE VIN - like models. VIN - like models COMPARE XLVINs. XLVINs COMPARE model - free baselines. model - free baselines COMPARE XLVINs. MDP USED-FOR VIN - like models. Generic is model. Task is planning computations. OtherScientificTerm are state space, and Markov decision process ( MDP ). Method is Latent Value Iteration Networks ( XLVINs ). ","This paper proposes Latent Value Iteration Networks (XLVINs) for implicit planning in the context of deep reinforcement learning, which aims to combine long-range reasoning and understanding of environment dynamics for tasks that require planning computations. The proposed model is based on the idea of contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning. The authors show that VIN-style models can be trained on generic environments where the state space is a Markov decision process (MDP) and the goal is to learn a good representation of the environment dynamics. They also show that XLVIN-like models trained on the same MDP are able to learn better representations of the MDP than VIN - like models that are trained on a different MDP.   The authors also propose a new model called Latent Validation Networks (XLVINs), which is a generalization of Latent value iteration Networks (LVIN) and show that it is able to generalize well to new environments.  The paper is well-written and well-motivated, and the paper is easy to follow.","This paper proposes Latent Value Iteration Networks (XLVINs) for implicit planning in the context of deep reinforcement learning, which aims to combine long-range reasoning and understanding of environment dynamics for tasks that require planning computations. The proposed model is based on the idea of contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning. The authors show that VIN-style models can be trained on generic environments where the state space is a Markov decision process (MDP) and the goal is to learn a good representation of the environment dynamics. They also show that XLVIN-like models trained on the same MDP are able to learn better representations of the MDP than VIN - like models that are trained on a different MDP.   The authors also propose a new model called Latent Validation Networks (XLVINs), which is a generalization of Latent value iteration Networks (LVIN) and show that it is able to generalize well to new environments.  The paper is well-written and well-motivated, and the paper is easy to follow."
171,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"Learning functions PART-OF machine learning. Boolean variables FEATURE-OF Learning functions. neural networks USED-FOR functions. distribution free setting FEATURE-OF functions. networks USED-FOR they. read - once DNFs HYPONYM-OF functions. convex neural network CONJUNCTION gradient descent. gradient descent CONJUNCTION convex neural network. convex neural network USED-FOR functions. gradient descent USED-FOR functions. inductive bias FEATURE-OF learning process. ones HYPONYM-OF networks. networks USED-FOR risk. gradient descent USED-FOR compact representation. process USED-FOR DNF. it USED-FOR process. computer assisted proof USED-FOR inductive bias. inductive bias FEATURE-OF DNFs. computer assisted proof USED-FOR DNFs. network USED-FOR process. network USED-FOR DNF. optimization USED-FOR inductive bias. learning process CONJUNCTION optimization. optimization CONJUNCTION learning process. network USED-FOR l2 norm. network USED-FOR DNF terms. margin constraints FEATURE-OF l2 norm. OtherScientificTerm are uniform distribution, neurons, logical formulas, and high dimensional DNFs. Method is zero - error networks. Metric is population risk. Material is tabular datasets. ","This paper studies the inductive bias of learning functions in machine learning. Learning functions with Boolean variables are a special case of neural networks. In a distribution free setting, the authors show that neural networks can learn functions that are inductive biased, i.e., functions that depend on a uniform distribution of the input variables.    They show that they can learn such functions using networks that are read-once DNFs, which is a class of functions that can be written as a function that depends on a set of neurons.  They also show that a convex neural network and gradient descent can be used to learn these functions.  In particular, they show that for zero-error networks, the population risk of learning a DNF is a function of the number of neurons and the size of the data set.  The authors then show that these networks (especially the ones that are high-dimensional) can be seen as risk minimizers of the risk.  Finally, they provide a computer assisted proof that shows that a compact representation can be learned by gradient descent, and that it is a process that is inductive-bias-free. They also provide a proof that this process converges to the DNF when the network is trained on a large enough data set, and show that it converges in a similar way to the one that was used to train the network to learn the original DNF. ","This paper studies the inductive bias of learning functions in machine learning. Learning functions with Boolean variables are a special case of neural networks. In a distribution free setting, the authors show that neural networks can learn functions that are inductive biased, i.e., functions that depend on a uniform distribution of the input variables.    They show that they can learn such functions using networks that are read-once DNFs, which is a class of functions that can be written as a function that depends on a set of neurons.  They also show that a convex neural network and gradient descent can be used to learn these functions.  In particular, they show that for zero-error networks, the population risk of learning a DNF is a function of the number of neurons and the size of the data set.  The authors then show that these networks (especially the ones that are high-dimensional) can be seen as risk minimizers of the risk.  Finally, they provide a computer assisted proof that shows that a compact representation can be learned by gradient descent, and that it is a process that is inductive-bias-free. They also provide a proof that this process converges to the DNF when the network is trained on a large enough data set, and show that it converges in a similar way to the one that was used to train the network to learn the original DNF. "
180,SP:6e600bedbf995375fd41cc0b517ddefb918318af,exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. sparse environment FEATURE-OF map. graph structure USED-FOR exploration directions. Graph Structured Reinforcement Learning ( GSRL ) USED-FOR value function estimation. graph structure USED-FOR value function estimation. graph structure PART-OF historical trajectories. graph structure USED-FOR Graph Structured Reinforcement Learning ( GSRL ). state transitions PART-OF replay buffer. GSRL USED-FOR dynamic graph. attention strategy USED-FOR map. state transitions USED-FOR dynamic graph. historical trajectories USED-FOR dynamic graph. graph structure USED-FOR value learning. GSRL COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE GSRL. sample efficiency EVALUATE-FOR state - of - the - art algorithms. sample efficiency EVALUATE-FOR GSRL. Task is reinforcement learning. OtherScientificTerm is sparse reward functions. ,"This paper proposes Graph Structured Reinforcement Learning (GSRL), a novel approach for reinforcement learning with sparse reward functions. The key idea is to use graph structure in the historical trajectories to guide the exploration directions, and to use the graph structure to guide value function estimation. In particular, GSRL learns a dynamic graph based on the state transitions in the replay buffer, and uses an attention strategy to learn a map in a sparse environment. The paper shows that GSRL outperforms previous state-of-the-art algorithms in terms of sample efficiency. ","This paper proposes Graph Structured Reinforcement Learning (GSRL), a novel approach for reinforcement learning with sparse reward functions. The key idea is to use graph structure in the historical trajectories to guide the exploration directions, and to use the graph structure to guide value function estimation. In particular, GSRL learns a dynamic graph based on the state transitions in the replay buffer, and uses an attention strategy to learn a map in a sparse environment. The paper shows that GSRL outperforms previous state-of-the-art algorithms in terms of sample efficiency. "
189,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"Simulated environments EVALUATE-FOR systematic generalization of reinforcement learning agents. procedurally generated content FEATURE-OF Simulated environments. positions of entities CONJUNCTION asset appearances. asset appearances CONJUNCTION positions of entities. layout CONJUNCTION positions of entities. positions of entities CONJUNCTION layout. asset appearances CONJUNCTION rules. rules CONJUNCTION asset appearances. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. generalization EVALUATE-FOR test levels. robustness EVALUATE-FOR test levels. levels USED-FOR learning progress. framework USED-FOR future learning potential. Prioritized Level Replay HYPONYM-OF framework. Prioritized Level Replay USED-FOR future learning potential. sample - efficiency CONJUNCTION generalization. generalization CONJUNCTION sample - efficiency. Procgen Benchmark environments CONJUNCTION MiniGrid environments. MiniGrid environments CONJUNCTION Procgen Benchmark environments. Prioritized Level Replay USED-FOR implicit curriculum. Generic is environment. OtherScientificTerm are environment transitions, training levels, agent, and temporal - difference ( TD ) errors. ","This paper studies systematic generalization of reinforcement learning agents in simulated environments with procedurally generated content. Simulated environments are defined as environments in which the environment transitions between different training levels are differentiable and the agent is able to learn to generalize to new environments. The authors propose a framework called Prioritized Level Replay, a framework that aims to improve future learning potential by prioritizing training levels that are more likely to be relevant to the agent’s generalization and robustness. This is achieved by considering different levels of learning progress as a function of layout, positions of entities, asset appearances, and rules, and then using these levels to guide learning progress. The paper also proposes an implicit curriculum based on prior experience replay, which is shown to improve sample-efficiency and generalization. Experiments are conducted on Procgen Benchmark environments and MiniGrid environments, and show that the agent generalizes to unseen environments with fewer temporal-difference (TD) errors. ","This paper studies systematic generalization of reinforcement learning agents in simulated environments with procedurally generated content. Simulated environments are defined as environments in which the environment transitions between different training levels are differentiable and the agent is able to learn to generalize to new environments. The authors propose a framework called Prioritized Level Replay, a framework that aims to improve future learning potential by prioritizing training levels that are more likely to be relevant to the agent’s generalization and robustness. This is achieved by considering different levels of learning progress as a function of layout, positions of entities, asset appearances, and rules, and then using these levels to guide learning progress. The paper also proposes an implicit curriculum based on prior experience replay, which is shown to improve sample-efficiency and generalization. Experiments are conducted on Procgen Benchmark environments and MiniGrid environments, and show that the agent generalizes to unseen environments with fewer temporal-difference (TD) errors. "
198,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"pre - training CONJUNCTION multitask learning. multitask learning CONJUNCTION pre - training. deep learning USED-FOR data - rich settings. pre - training USED-FOR tasks. multitask learning USED-FOR tasks. automatic differentiation procedures CONJUNCTION randomized singular value decomposition. randomized singular value decomposition CONJUNCTION automatic differentiation procedures. randomized singular value decomposition USED-FOR scalability. scalability EVALUATE-FOR method. automatic differentiation procedures USED-FOR method. randomized singular value decomposition USED-FOR method. approach COMPARE baselines. baselines COMPARE approach. out - of - distribution data USED-FOR Text and Image classification tasks. Text and Image classification tasks EVALUATE-FOR approach. out - of - distribution data EVALUATE-FOR baselines. out - of - distribution data EVALUATE-FOR approach. OtherScientificTerm are model parameterizations, auxiliary tasks, auxiliary task gradients, auxiliary updates, and primary task loss. Method is modelagnostic framework. Generic are algorithm, and framework. ","This paper proposes a modelagnostic framework for pre-training and multitask learning in data-rich settings, where the model parameterizations are not available for all tasks but are only available for a subset of them. The authors propose an algorithm that can be applied to any pre-trained model, pre-finetune it on a set of auxiliary tasks, and then fine-tune on all the auxiliary tasks at the end of training. The method is based on automatic differentiation procedures and randomized singular value decomposition to improve the scalability of the method. The auxiliary task gradients are computed as a function of the number of auxiliary updates performed during training, and the primary task loss. The proposed approach is evaluated on Text and Image classification tasks on out-of-distribution data and outperforms the baselines.  ","This paper proposes a modelagnostic framework for pre-training and multitask learning in data-rich settings, where the model parameterizations are not available for all tasks but are only available for a subset of them. The authors propose an algorithm that can be applied to any pre-trained model, pre-finetune it on a set of auxiliary tasks, and then fine-tune on all the auxiliary tasks at the end of training. The method is based on automatic differentiation procedures and randomized singular value decomposition to improve the scalability of the method. The auxiliary task gradients are computed as a function of the number of auxiliary updates performed during training, and the primary task loss. The proposed approach is evaluated on Text and Image classification tasks on out-of-distribution data and outperforms the baselines.  "
207,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"large text - based neural language models USED-FOR one - shot learning. RL algorithms USED-FOR one - shot word learning. short - term, within - episode knowledge CONJUNCTION long - term lexical and motor knowledge. long - term lexical and motor knowledge CONJUNCTION short - term, within - episode knowledge. memory writing mechanism USED-FOR one - shot word - object binding. dual - coding memory USED-FOR intrinsic motivation. deep neural networks USED-FOR fast - mapping. episodic memory CONJUNCTION multi - modal environment. multi - modal environment CONJUNCTION episodic memory. meta - learning CONJUNCTION episodic memory. episodic memory CONJUNCTION meta - learning. multi - modal environment USED-FOR fast - mapping. transformative capacity FEATURE-OF artificial agents. human cognitive development CONJUNCTION transformative capacity. transformative capacity CONJUNCTION human cognitive development. fast - mapping HYPONYM-OF human cognitive development. meta - learning USED-FOR deep neural networks. multi - modal environment USED-FOR deep neural networks. episodic memory USED-FOR deep neural networks. Material is simulated 3D world. OtherScientificTerm are dual - coding external memory, visual perception and language, and ShapeNet category. ","This paper studies the problem of one-shot learning in large text-based neural language models. The authors propose a novel RL algorithms for the task, where the goal is to learn a dual-coding external memory that encodes both short-term, within-episode knowledge and long-term lexical and motor knowledge. The paper proposes a memory writing mechanism to learn the key word-object binding, which can be used as an intrinsic motivation for training RL algorithms.  The paper also proposes to use the dual-coded memory to learn intrinsic motivation as a way to motivate intrinsic motivation.  Experiments are conducted on a simulated 3D world where the agent is given a set of objects in the scene, and a single object in the environment, and is asked to learn to bind the object to the object using visual perception and language. The agent is trained on the ShapeNet category, where it is shown to be able to bind objects to objects in both visual and language domains.   The authors show that deep neural networks trained with meta-learning, episodic memory and a multi-modal environment are able to learn fast-mapping, which is an important aspect of human cognitive development and transformative capacity of artificial agents. ","This paper studies the problem of one-shot learning in large text-based neural language models. The authors propose a novel RL algorithms for the task, where the goal is to learn a dual-coding external memory that encodes both short-term, within-episode knowledge and long-term lexical and motor knowledge. The paper proposes a memory writing mechanism to learn the key word-object binding, which can be used as an intrinsic motivation for training RL algorithms.  The paper also proposes to use the dual-coded memory to learn intrinsic motivation as a way to motivate intrinsic motivation.  Experiments are conducted on a simulated 3D world where the agent is given a set of objects in the scene, and a single object in the environment, and is asked to learn to bind the object to the object using visual perception and language. The agent is trained on the ShapeNet category, where it is shown to be able to bind objects to objects in both visual and language domains.   The authors show that deep neural networks trained with meta-learning, episodic memory and a multi-modal environment are able to learn fast-mapping, which is an important aspect of human cognitive development and transformative capacity of artificial agents. "
216,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"Few - shot learning USED-FOR models. support set USED-FOR setup. class - imbalance HYPONYM-OF dynamic nature of the real world. backbones USED-FOR few - shot learning methods. strategies USED-FOR imbalance. feature - transfer CONJUNCTION metric - based methods. metric - based methods CONJUNCTION feature - transfer. strategies USED-FOR few - shot case. balanced task COMPARE class - imbalance counterparts. class - imbalance counterparts COMPARE balanced task. imbalance FEATURE-OF supervised learning. strategies USED-FOR supervised learning. imbalance COMPARE support set level. support set level COMPARE imbalance. imbalance FEATURE-OF dataset level. dataset level COMPARE support set level. support set level COMPARE dataset level. class - imbalance counterparts COMPARE optimization - based methods. optimization - based methods COMPARE class - imbalance counterparts. OtherScientificTerm are few - shot class - imbalance, dataset vs. support set imbalance, and imbalance distributions. Method is rebalancing techniques. ","This paper studies the problem of few-shot learning in which models are trained on a support set that is skewed towards a specific class of data. The authors consider the dynamic nature of the real world, i.e., class-imbalance, which is a phenomenon where there is a large imbalance between the training set and the support set for a given setup.   The authors propose two strategies to mitigate the imbalance in the few shot learning setting: feature-transfer and metric-based methods.  They show that both strategies are effective in balancing the imbalance for supervised learning. They also show that the balanced task performs better than the class-intimidating counterparts.  Finally, the authors show that there is no clear correlation between the imbalance of the dataset level and the imbalance at the support level, and that the imbalance is more severe for the dataset vs. support set imbalance.  The paper concludes with a discussion of the advantages of the proposed strategies to balance the imbalance on the supervised learning side. ","This paper studies the problem of few-shot learning in which models are trained on a support set that is skewed towards a specific class of data. The authors consider the dynamic nature of the real world, i.e., class-imbalance, which is a phenomenon where there is a large imbalance between the training set and the support set for a given setup.   The authors propose two strategies to mitigate the imbalance in the few shot learning setting: feature-transfer and metric-based methods.  They show that both strategies are effective in balancing the imbalance for supervised learning. They also show that the balanced task performs better than the class-intimidating counterparts.  Finally, the authors show that there is no clear correlation between the imbalance of the dataset level and the imbalance at the support level, and that the imbalance is more severe for the dataset vs. support set imbalance.  The paper concludes with a discussion of the advantages of the proposed strategies to balance the imbalance on the supervised learning side. "
225,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"convolution operators USED-FOR representations of graphs. Graph Convolutional Neural Networks ( GCNs ) USED-FOR representations of graphs. convolution operators USED-FOR Graph Convolutional Neural Networks ( GCNs ). neighborhood aggregating scheme USED-FOR convolution operators. local topological information USED-FOR convolution operators. decoupled representations USED-FOR them. graph convolution layer USED-FOR neighbouring nodes. topological distances FEATURE-OF neighbouring nodes. readout layers USED-FOR representations. convolution operators CONJUNCTION linear stacking. linear stacking CONJUNCTION convolution operators. Polynomial Graph Convolution ( PGC ) layer COMPARE convolution operators. convolution operators COMPARE Polynomial Graph Convolution ( PGC ) layer. Polynomial Graph Convolution ( PGC ) layer USED-FOR strategy. receptive field FEATURE-OF convolution operator. single PGC layer USED-FOR Graph Neural Network architecture. graph classification benchmarks EVALUATE-FOR Graph Neural Network architecture. OtherScientificTerm are wider topological receptive fields, and GC parameters. Method is non - linear graph convolutions. Metric is neural network expressiveness. ","Graph Convolutional Neural Networks (GCNs) are a class of convolutional neural networks (CNNs) that use convolution operators to learn representations of graphs. In this paper, the authors propose a novel neighborhood aggregating scheme to improve the expressiveness of existing GCNs by incorporating local topological information. The authors propose to use a Polynomial Graph Convolution (PGC) layer to aggregate neighbouring nodes in a graph convolution layer, and decouple them into decoupled representations. The topological distances between neighbouring nodes are used as readout layers for these representations, and wider topological receptive fields are used to make the representations more expressive. The proposed strategy is based on the observation that non-linear graph convolutions are more expressive than conventional convolution operations and linear stacking, and the authors show that the proposed strategy can be used to improve expressiveness by using the Polynomials of the receptive field of a convolution operator. The paper also shows that a single PGC layer can be applied to any Graph Neural Network architecture, and shows that the performance of the proposed method is comparable to the state-of-the-art on several graph classification benchmarks. ","Graph Convolutional Neural Networks (GCNs) are a class of convolutional neural networks (CNNs) that use convolution operators to learn representations of graphs. In this paper, the authors propose a novel neighborhood aggregating scheme to improve the expressiveness of existing GCNs by incorporating local topological information. The authors propose to use a Polynomial Graph Convolution (PGC) layer to aggregate neighbouring nodes in a graph convolution layer, and decouple them into decoupled representations. The topological distances between neighbouring nodes are used as readout layers for these representations, and wider topological receptive fields are used to make the representations more expressive. The proposed strategy is based on the observation that non-linear graph convolutions are more expressive than conventional convolution operations and linear stacking, and the authors show that the proposed strategy can be used to improve expressiveness by using the Polynomials of the receptive field of a convolution operator. The paper also shows that a single PGC layer can be applied to any Graph Neural Network architecture, and shows that the performance of the proposed method is comparable to the state-of-the-art on several graph classification benchmarks. "
234,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,labeled datasets USED-FOR SG generation techniques. neural network models COMPARE real data. real data COMPARE neural network models. synthetic data USED-FOR neural network models. scalable technique USED-FOR sim - to - real transfer. sim - to - real transfer USED-FOR scene graph generation. Sim2SG HYPONYM-OF scalable technique. scalable technique USED-FOR scene graph generation. Sim2SG USED-FOR domain gap. supervision PART-OF real - world dataset. supervision USED-FOR Sim2SG. baselines USED-FOR domain gap. toy simulators CONJUNCTION realistic simulators. realistic simulators CONJUNCTION toy simulators. real - world data USED-FOR realistic simulators. realistic simulators EVALUATE-FOR approach. toy simulators EVALUATE-FOR approach. Task is Scene graph ( SG ) generation. Material is Synthetic data. OtherScientificTerm is appearance. Generic is discrepancies. ,"Scene graph (SG) generation is an important problem in many real-world applications. However, existing SG generation techniques rely on labeled datasets. Synthetic data is not always available, and neural network models trained on synthetic data may not generalize as well as real data. This paper proposes a scalable technique called Sim2SG, which aims at sim-to-real transfer for scene graph generation.    The authors show that the proposed approach, Sim2 SG, is able to reduce the domain gap between synthetic data and real data by reducing the discrepancy between the appearance of the generated scene graph and the original scene graph. Sim2Sim2SG is based on the observation that there is a trade-off between the discrepancies between synthetic and real datasets, and the authors propose to use supervision from a real-life dataset to further reduce the gap. The authors evaluate their approach on toy simulators and realistic simulators, and show that their approach outperforms the baselines in terms of domain gap reduction. ","Scene graph (SG) generation is an important problem in many real-world applications. However, existing SG generation techniques rely on labeled datasets. Synthetic data is not always available, and neural network models trained on synthetic data may not generalize as well as real data. This paper proposes a scalable technique called Sim2SG, which aims at sim-to-real transfer for scene graph generation.    The authors show that the proposed approach, Sim2 SG, is able to reduce the domain gap between synthetic data and real data by reducing the discrepancy between the appearance of the generated scene graph and the original scene graph. Sim2Sim2SG is based on the observation that there is a trade-off between the discrepancies between synthetic and real datasets, and the authors propose to use supervision from a real-life dataset to further reduce the gap. The authors evaluate their approach on toy simulators and realistic simulators, and show that their approach outperforms the baselines in terms of domain gap reduction. "
243,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,model - based methods COMPARE model - free methods. model - free methods COMPARE model - based methods. sample efficiency EVALUATE-FOR model - free methods. model - free methods USED-FOR continuous - action DRL benchmarks. continuous - action DRL benchmarks EVALUATE-FOR model - based methods. sample efficiency EVALUATE-FOR model - based methods. modelbased algorithm USED-FOR MuJoCo benchmark. REDQ COMPARE model - based method. model - based method COMPARE REDQ. parameters USED-FOR model - based method. wall - clock run time EVALUATE-FOR REDQ. parameters USED-FOR REDQ. random subset of Q functions PART-OF ensemble. REDQ USED-FOR it. ensemble of Q functions CONJUNCTION in - target minimization. in - target minimization CONJUNCTION ensemble of Q functions. random subset of Q functions USED-FOR in - target minimization. REDQ CONJUNCTION model - free algorithms. model - free algorithms CONJUNCTION REDQ. model - free DRL algorithm USED-FOR continuous - action spaces. REDQ HYPONYM-OF model - free DRL algorithm. REDQ USED-FOR continuous - action spaces. UTD ratio 1 USED-FOR model - free DRL algorithm. UTD ratio 1 USED-FOR REDQ. Method is modelfree algorithm. OtherScientificTerm is UTD ratio. ,"This paper proposes a model-free DRL algorithm, REDQ, which is a generalization of REDQ to continuous action RL. The authors show that REDQ outperforms the state-of-the-art model-based DRL algorithms on the MuJoCo benchmark. REDQ is based on an ensemble of Q-function approximators, and the authors also show that it is more sample efficient than REDQ in terms of wall-clock run time. ","This paper proposes a model-free DRL algorithm, REDQ, which is a generalization of REDQ to continuous action RL. The authors show that REDQ outperforms the state-of-the-art model-based DRL algorithms on the MuJoCo benchmark. REDQ is based on an ensemble of Q-function approximators, and the authors also show that it is more sample efficient than REDQ in terms of wall-clock run time. "
252,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"deep learning USED-FOR classification or regression. probability distributions USED-FOR deep learning. distribution samples USED-FOR classification or regression. Lipschitz - bounded transformations of the input distribution FEATURE-OF robustness. tasks EVALUATE-FOR approach. DIDA USED-FOR meta - features. DIDA USED-FOR labelled ) dataset. meta - features USED-FOR labelled ) dataset. DIDA USED-FOR tasks. SVM CONJUNCTION logistic regression. logistic regression CONJUNCTION SVM. logistic regression CONJUNCTION linear SGD. linear SGD CONJUNCTION logistic regression. k - NN CONJUNCTION SVM. SVM CONJUNCTION k - NN. hyper - parameter configuration COMPARE configuration. configuration COMPARE hyper - parameter configuration. fixed algorithm USED-FOR hyper - parameter configuration. hyper - parameter configuration USED-FOR learning. dataset EVALUATE-FOR configuration. OpenML benchmarking suite USED-FOR dataset. SVM HYPONYM-OF fixed algorithm. DSS CONJUNCTION DATASET2VEC architectures. DATASET2VEC architectures CONJUNCTION DSS. tasks EVALUATE-FOR DIDA. tasks EVALUATE-FOR models. DIDA COMPARE models. models COMPARE DIDA. DIDA COMPARE DATASET2VEC architectures. DATASET2VEC architectures COMPARE DIDA. tasks EVALUATE-FOR DATASET2VEC architectures. DIDA COMPARE DSS. DSS COMPARE DIDA. DATASET2VEC architectures CONJUNCTION models. models CONJUNCTION DATASET2VEC architectures. hand - crafted meta - features USED-FOR models. OtherScientificTerm are permutation of the samples, and permutation of the features. Method are neural architectures, and universal approximation. Generic are architecture, and task. ","This paper proposes a meta-learning approach for deep learning from probability distributions. The approach is based on the idea that deep learning for classification or regression from distribution samples is robust to the permutation of the samples. The authors argue that the robustness is due to Lipschitz-bounded transformations of the input distribution, and that neural architectures are more robust to such permutation than to universal approximation. The proposed approach, called DIDA, learns meta-features for a (labeled) dataset from the meta-distribution of the distribution of samples.  The authors evaluate the proposed approach on three different tasks, and show that DIDA outperforms DSS and DATASET2VEC architectures on all three tasks. They also show that the hyper-parameter configuration of a fixed algorithm (e.g., k-NN, SVM, logistic regression, linear SGD) is more robust than the configuration learned from a single dataset. The paper also shows that a single configuration of the dataset can be trained on a single OpenML benchmarking suite, and the performance of the architecture is comparable to that of DSS.  Finally, the authors compare DIDA with other models trained with hand-crafted meta-feature, and demonstrate that the proposed DIDA performs better on most of the tasks compared to DSS, and outperforms the models trained from scratch on a subset of tasks. ","This paper proposes a meta-learning approach for deep learning from probability distributions. The approach is based on the idea that deep learning for classification or regression from distribution samples is robust to the permutation of the samples. The authors argue that the robustness is due to Lipschitz-bounded transformations of the input distribution, and that neural architectures are more robust to such permutation than to universal approximation. The proposed approach, called DIDA, learns meta-features for a (labeled) dataset from the meta-distribution of the distribution of samples.  The authors evaluate the proposed approach on three different tasks, and show that DIDA outperforms DSS and DATASET2VEC architectures on all three tasks. They also show that the hyper-parameter configuration of a fixed algorithm (e.g., k-NN, SVM, logistic regression, linear SGD) is more robust than the configuration learned from a single dataset. The paper also shows that a single configuration of the dataset can be trained on a single OpenML benchmarking suite, and the performance of the architecture is comparable to that of DSS.  Finally, the authors compare DIDA with other models trained with hand-crafted meta-feature, and demonstrate that the proposed DIDA performs better on most of the tasks compared to DSS, and outperforms the models trained from scratch on a subset of tasks. "
261,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,data clustering CONJUNCTION visualization. visualization CONJUNCTION data clustering. dimensionality reduction CONJUNCTION data clustering. data clustering CONJUNCTION dimensionality reduction. data representation and analysis CONJUNCTION dimensionality reduction. dimensionality reduction CONJUNCTION data representation and analysis. manifold learning CONJUNCTION data representation and analysis. data representation and analysis CONJUNCTION manifold learning. Graph learning USED-FOR data mining and machine learning tasks. visualization HYPONYM-OF data mining and machine learning tasks. manifold learning HYPONYM-OF data mining and machine learning tasks. data clustering HYPONYM-OF data mining and machine learning tasks. data representation and analysis HYPONYM-OF data mining and machine learning tasks. dimensionality reduction HYPONYM-OF data mining and machine learning tasks. approach USED-FOR ultra - sparse undirected graphs. graphLaplacian - like matrix PART-OF graphical Lasso. graphLaplacian - like matrix FEATURE-OF precision matrix. high - dimensional input data USED-FOR ultra - sparse undirected graphs. GRASPEL USED-FOR graphs. spectrally - critical edges PART-OF graph. nearly - linear time spectral methods USED-FOR ultrasparse yet spectrally - robust graphs. spectral clustering ( SC ) CONJUNCTION dimensionality reduction. dimensionality reduction CONJUNCTION spectral clustering ( SC ). graph learning approaches COMPARE GRASPEL. GRASPEL COMPARE graph learning approaches. manifold learning CONJUNCTION spectral clustering ( SC ). spectral clustering ( SC ) CONJUNCTION manifold learning. computing efficiency CONJUNCTION solution quality. solution quality CONJUNCTION computing efficiency. GRASPEL USED-FOR data mining and machine learning applications. solution quality EVALUATE-FOR data mining and machine learning applications. computing efficiency EVALUATE-FOR data mining and machine learning applications. solution quality EVALUATE-FOR GRASPEL. computing efficiency EVALUATE-FOR GRASPEL. dimensionality reduction HYPONYM-OF data mining and machine learning applications. manifold learning HYPONYM-OF data mining and machine learning applications. spectral clustering ( SC ) HYPONYM-OF data mining and machine learning applications. Method is spectral graph densification approach,"Graph learning is an important problem in data mining and machine learning tasks such as manifold learning, data representation and analysis, dimensionality reduction, data clustering, and visualization. This paper proposes a spectral graph densification approach called GRASPEL. The proposed approach is applied to ultra-sparse undirected graphs with high-dimensional input data. The authors propose a graphLaplacian-like matrix in the form of a graphical Lasso, where the precision matrix is a weighted sum of the number of spectrally-critical edges in the graph. Theoretical analysis is provided to show that the proposed method is nearly-linear time spectral methods for ultrasparse yet spectratically-robust graphs. Experiments show that compared to other graph learning approaches, the proposed GRASNEL improves computing efficiency, computing efficiency and solution quality for data mining, spectral clustering (SC), dimensionality Reduction (DR) and data mining (DQN). ","Graph learning is an important problem in data mining and machine learning tasks such as manifold learning, data representation and analysis, dimensionality reduction, data clustering, and visualization. This paper proposes a spectral graph densification approach called GRASPEL. The proposed approach is applied to ultra-sparse undirected graphs with high-dimensional input data. The authors propose a graphLaplacian-like matrix in the form of a graphical Lasso, where the precision matrix is a weighted sum of the number of spectrally-critical edges in the graph. Theoretical analysis is provided to show that the proposed method is nearly-linear time spectral methods for ultrasparse yet spectratically-robust graphs. Experiments show that compared to other graph learning approaches, the proposed GRASNEL improves computing efficiency, computing efficiency and solution quality for data mining, spectral clustering (SC), dimensionality Reduction (DR) and data mining (DQN). "
270,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"images CONJUNCTION text descriptions. text descriptions CONJUNCTION images. deep reinforcement learning USED-FOR goal - conditioned policy. explicit embedding space USED-FOR nonparametric distance. abstract - level policy CONJUNCTION goal - conditioned policy. goal - conditioned policy CONJUNCTION abstract - level policy. unsupervised learning approach USED-FOR goal - conditioned policy. unsupervised learning approach USED-FOR abstract - level policy. intrinsic motivation ( GPIM ) FEATURE-OF goal - conditioned policy. goal - conditioned policy HYPONYM-OF unsupervised learning approach. discriminator USED-FOR abstract - level policy. latent variable USED-FOR abstract - level policy. discriminator USED-FOR intrinsic reward function. intrinsic reward function USED-FOR goal - conditioned policy. intrinsic reward function USED-FOR trajectory. discriminator USED-FOR goal - conditioned policy. discriminator USED-FOR trajectory. abstract - level policy USED-FOR trajectory. robotic tasks EVALUATE-FOR GPIM method. GPIM method COMPARE prior techniques. prior techniques COMPARE GPIM method. robotic tasks EVALUATE-FOR prior techniques. OtherScientificTerm are perceptually - specific goals, and hand - crafted rewards. Generic is policy. ","This paper proposes an unsupervised learning approach to learn an abstract-level policy and a goal-conditioned policy based on deep reinforcement learning. The goal is to learn to achieve perceptually-specific goals. The paper proposes to learn a nonparametric distance between images and text descriptions in an explicit embedding space, and then use this distance as an intrinsic reward function to train a policy that can be used to guide the agent to achieve the goal. The authors propose to use intrinsic motivation (GPIM) to learn the goal-conditional policy, which is a unsupervisory learning approach that learns an abstract -level policy conditioned on a latent variable, and an intrinsic policy conditioned upon the latent variable. The discriminator is used to train the abstract-levels of the two policies, and the discriminator predicts the intrinsic reward for the learned trajectory. Experiments on robotic tasks show that the proposed GPIM method outperforms prior techniques on a number of robotic tasks with hand-crafted rewards. ","This paper proposes an unsupervised learning approach to learn an abstract-level policy and a goal-conditioned policy based on deep reinforcement learning. The goal is to learn to achieve perceptually-specific goals. The paper proposes to learn a nonparametric distance between images and text descriptions in an explicit embedding space, and then use this distance as an intrinsic reward function to train a policy that can be used to guide the agent to achieve the goal. The authors propose to use intrinsic motivation (GPIM) to learn the goal-conditional policy, which is a unsupervisory learning approach that learns an abstract -level policy conditioned on a latent variable, and an intrinsic policy conditioned upon the latent variable. The discriminator is used to train the abstract-levels of the two policies, and the discriminator predicts the intrinsic reward for the learned trajectory. Experiments on robotic tasks show that the proposed GPIM method outperforms prior techniques on a number of robotic tasks with hand-crafted rewards. "
279,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"policy switches HYPONYM-OF low switching cost. low switching cost USED-FOR deep reinforcement learning problems. robotics CONJUNCTION dialogue agents. dialogue agents CONJUNCTION robotics. recommendation systems CONJUNCTION education. education CONJUNCTION recommendation systems. education CONJUNCTION robotics. robotics CONJUNCTION education. medical domains CONJUNCTION recommendation systems. recommendation systems CONJUNCTION medical domains. recommendation systems CONJUNCTION robotics. robotics CONJUNCTION recommendation systems. dialogue agents HYPONYM-OF applications. medical domains HYPONYM-OF applications. robotics HYPONYM-OF applications. education HYPONYM-OF applications. recommendation systems HYPONYM-OF applications. Q - network CONJUNCTION learning Q - network. learning Q - network CONJUNCTION Q - network. deep Q - networks USED-FOR policy switching criteria. feature distance USED-FOR adaptive approach. medical treatment environment CONJUNCTION Atari games. Atari games CONJUNCTION medical treatment environment. switching cost EVALUATE-FOR feature - switching criterion. sample efficiency EVALUATE-FOR feature - switching criterion. OtherScientificTerm are low - switching - cost constraint, and representation learning perspective. ","This paper studies the problem of low switching cost, i.e., policy switches, in deep reinforcement learning problems. The authors propose an adaptive approach based on the feature distance between the Q-network and learning Q-networks, which is a low-switching-cost constraint. They show that this adaptive approach can be applied to a wide range of applications, including medical domains, recommendation systems, robotics, and dialogue agents. They also show that the policy switching criteria can be learned using deep Q-nets. They evaluate their adaptive approach on a medical treatment environment and a set of Atari games, and show that their feature-switch criterion can reduce the switching cost and improve the sample efficiency. The paper is well-written from a representation learning perspective. ","This paper studies the problem of low switching cost, i.e., policy switches, in deep reinforcement learning problems. The authors propose an adaptive approach based on the feature distance between the Q-network and learning Q-networks, which is a low-switching-cost constraint. They show that this adaptive approach can be applied to a wide range of applications, including medical domains, recommendation systems, robotics, and dialogue agents. They also show that the policy switching criteria can be learned using deep Q-nets. They evaluate their adaptive approach on a medical treatment environment and a set of Atari games, and show that their feature-switch criterion can reduce the switching cost and improve the sample efficiency. The paper is well-written from a representation learning perspective. "
288,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,First - order stochastic methods USED-FOR large - scale non - convex optimization problems. First - order stochastic methods USED-FOR big - data applications. deep neural networks HYPONYM-OF big - data applications. homotopy methods CONJUNCTION SGD. SGD CONJUNCTION homotopy methods. diffusion CONJUNCTION mollifying networks. mollifying networks CONJUNCTION diffusion. Gaussian continuation USED-FOR optimization. diffusion HYPONYM-OF heuristics. mollifying networks HYPONYM-OF heuristics. optimization HYPONYM-OF heuristics. homotopy methods USED-FOR first - order stochastic algorithm. SGD USED-FOR first - order stochastic algorithm. scheme USED-FOR H - SGD. scheme USED-FOR homotopy parameter. fast and inexpensive iterations FEATURE-OF H - SGD. global linear rate of convergence EVALUATE-FOR H - SGD. H - SGD COMPARE SGD. SGD COMPARE H - SGD. Metric is slow global convergence rate. OtherScientificTerm is neighborhood of a minimizer. Generic is algorithm. ,"This paper studies the problem of large-scale non-convex optimization problems using First-order stochastic methods in big-data applications such as deep neural networks. The authors propose a new algorithm H-SGD that uses homotopy methods, SGD, and other heuristics such as diffusion, mollifying networks, and optimization with a Gaussian continuation to achieve a slow global convergence rate. In particular, they propose a scheme to adjust the parameters of the first-order Stochastic algorithm with SGD that allows for fast and inexpensive iterations in the neighborhood of a minimizer. They also provide a global linear rate of convergence for the proposed algorithm. Finally, the authors provide experimental results to show that H -SGD outperforms SGD.","This paper studies the problem of large-scale non-convex optimization problems using First-order stochastic methods in big-data applications such as deep neural networks. The authors propose a new algorithm H-SGD that uses homotopy methods, SGD, and other heuristics such as diffusion, mollifying networks, and optimization with a Gaussian continuation to achieve a slow global convergence rate. In particular, they propose a scheme to adjust the parameters of the first-order Stochastic algorithm with SGD that allows for fast and inexpensive iterations in the neighborhood of a minimizer. They also provide a global linear rate of convergence for the proposed algorithm. Finally, the authors provide experimental results to show that H -SGD outperforms SGD."
297,SP:195d090d9df0bda33103edcbbaf300e43f4562be,Bayesian meta - learning problem USED-FOR shape completion. encoder USED-FOR posterior distribution. encoder USED-FOR latent representation. posterior distribution FEATURE-OF latent representation. sparse cloud USED-FOR latent representation. encoder USED-FOR learning of object shapes. sparse point clouds USED-FOR learning of object shapes. meta - learning algorithm USED-FOR shape completion of newly - encountered objects. object - specific properties CONJUNCTION object - agnostic properties. object - agnostic properties CONJUNCTION object - specific properties. sparse observations USED-FOR shape completion of newly - encountered objects. ICL - NUIM benchmarks EVALUATE-FOR method. Method is shape representations. ,"This paper considers the problem of shape completion in the meta-learning setting, where the goal is to learn a set of object-specific and object-agnostic features for a given set of objects. The paper proposes to solve the problem by solving a Bayesian meta learning problem, where each object is represented as a point cloud, and the task is to complete the shape completion of the point cloud. The problem is formulated as an optimization problem, and a meta learning algorithm is proposed to solve it. The proposed algorithm is evaluated on the ICL-NUIM benchmark, where it is shown to outperform existing methods.","This paper considers the problem of shape completion in the meta-learning setting, where the goal is to learn a set of object-specific and object-agnostic features for a given set of objects. The paper proposes to solve the problem by solving a Bayesian meta learning problem, where each object is represented as a point cloud, and the task is to complete the shape completion of the point cloud. The problem is formulated as an optimization problem, and a meta learning algorithm is proposed to solve it. The proposed algorithm is evaluated on the ICL-NUIM benchmark, where it is shown to outperform existing methods."
306,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. activation magnitudes FEATURE-OF adversarial examples. channels COMPARE natural examples. natural examples COMPARE channels. channel - wise activation perspective FEATURE-OF adversarial examples. defense adversarial training USED-FOR activation magnitudes. CAS USED-FOR model. model USED-FOR adversarial activation. robustness EVALUATE-FOR defense methods. OtherScientificTerm are uniform activation, redundant activation, and adversarial perturbations. Method is intermediate layer activation of DNNs. ","This paper studies the activation magnitudes of adversarial examples from a channel-wise activation perspective. The authors show that adversarial instances with uniform activation are more powerful than natural examples with redundant activation. In addition, they show that under certain conditions, adversarial perturbations on the channels of the intermediate layer activation of DNNs can be much larger than those of natural examples. Based on this observation, the authors propose a defense adversarial training to reduce the magnitude of the activation of the channels. They also show that the model trained with CAS is more robust to adversarial activation, and the robustness of existing defense methods can be improved.","This paper studies the activation magnitudes of adversarial examples from a channel-wise activation perspective. The authors show that adversarial instances with uniform activation are more powerful than natural examples with redundant activation. In addition, they show that under certain conditions, adversarial perturbations on the channels of the intermediate layer activation of DNNs can be much larger than those of natural examples. Based on this observation, the authors propose a defense adversarial training to reduce the magnitude of the activation of the channels. They also show that the model trained with CAS is more robust to adversarial activation, and the robustness of existing defense methods can be improved."
315,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"random initialization USED-FOR gradient descent. generalization EVALUATE-FOR Neural networks. gradient descent USED-FOR Neural networks. random initialization USED-FOR Neural networks. Neural Tangent Kernel ( NTK ) USED-FOR implicit regularization effect. gradient flow / descent USED-FOR infinitely wide neural networks. implicit regularization effect FEATURE-OF gradient flow / descent. random initialization USED-FOR gradient flow / descent. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. generalization performance CONJUNCTION initialization. initialization CONJUNCTION generalization performance. optimization USED-FOR finite width networks. initialization USED-FOR finite width networks. optimization CONJUNCTION overparametrization. overparametrization CONJUNCTION optimization. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. generalization performance EVALUATE-FOR overparametrization. hidden layer width CONJUNCTION scaled ) random initialization. scaled ) random initialization CONJUNCTION hidden layer width. low - dimensional manifold FEATURE-OF network parameters. min - norm solution USED-FOR linear case. O(h−1/2 ) upper - bound FEATURE-OF operator norm distance. network CONJUNCTION min - norm solution. min - norm solution CONJUNCTION network. operator norm distance FEATURE-OF network. operator norm distance EVALUATE-FOR min - norm solution. OtherScientificTerm are regularization, and imbalance of the network weights. Method are non - asymptotic analysis, overparametrized single - hidden layer linear networks, and gradient flow. Metric are squared loss, and convergence rate. Generic is manifold. ","This paper studies the generalization performance of Neural networks trained with gradient descent with random initialization. The authors show that the implicit regularization effect of gradient flow/descent in infinitely wide neural networks based on the Neural Tangent Kernel (NTK) can be understood as a result of the regularization induced by the imbalance of the network weights. They also provide a non-asymptotic analysis that shows that the convergence rate is O(1/\sqrt{n}^d) for overparametrized single-hidden layer linear networks, and O(n) for finite width networks with initialization, optimization, and initialization. Finally, the authors provide an analysis of the min-norm solution of the linear case, which shows that for any network parameters on a low-dimensional manifold, the operator norm distance between the network and the min -norm solution has an O(h−1/2) upper-bound. This result is consistent with the observation that the squared loss of the gradient flow converges to a solution on the manifold when the number of hidden layer width and the (scaled) random initialization is large enough.   The authors also show that for infinite-width neural networks, optimization and initialization converge to a min-normal solution when the network parameters are large enough, and that optimization converges when the weights are small enough. The paper also shows that optimization and overparameterization of the overparameters are not the only factors that affect the convergence performance, but also that the optimization and optimization converge to min-min solutions when the parameters are small. ","This paper studies the generalization performance of Neural networks trained with gradient descent with random initialization. The authors show that the implicit regularization effect of gradient flow/descent in infinitely wide neural networks based on the Neural Tangent Kernel (NTK) can be understood as a result of the regularization induced by the imbalance of the network weights. They also provide a non-asymptotic analysis that shows that the convergence rate is O(1/\sqrt{n}^d) for overparametrized single-hidden layer linear networks, and O(n) for finite width networks with initialization, optimization, and initialization. Finally, the authors provide an analysis of the min-norm solution of the linear case, which shows that for any network parameters on a low-dimensional manifold, the operator norm distance between the network and the min -norm solution has an O(h−1/2) upper-bound. This result is consistent with the observation that the squared loss of the gradient flow converges to a solution on the manifold when the number of hidden layer width and the (scaled) random initialization is large enough.   The authors also show that for infinite-width neural networks, optimization and initialization converge to a min-normal solution when the network parameters are large enough, and that optimization converges when the weights are small enough. The paper also shows that optimization and overparameterization of the overparameters are not the only factors that affect the convergence performance, but also that the optimization and optimization converge to min-min solutions when the parameters are small. "
324,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,Deep networks COMPARE shallow ones. shallow ones COMPARE Deep networks. approximation EVALUATE-FOR shallow ones. tractable algorithms USED-FOR deep models. deep networks COMPARE shallow ones. shallow ones COMPARE deep networks. approximation USED-FOR kernels. tractable ) kernel methods USED-FOR over - parameterized regime. gradient descent USED-FOR deep networks. architecture USED-FOR kernel. eigenvalue decay FEATURE-OF integral operator. kernels COMPARE shallow ” two - layer counterpart. shallow ” two - layer counterpart COMPARE kernels. kernels USED-FOR ReLU activations. deep fully - connected networks USED-FOR kernels. approximation properties EVALUATE-FOR kernels. kernel framework USED-FOR deep architectures. differentiability properties FEATURE-OF kernel function. sphere FEATURE-OF kernels. kernel function USED-FOR eigenvalue decays. differentiability properties USED-FOR eigenvalue decays. ,"This paper studies the approximation properties of deep networks compared to shallow ones. Deep networks have been shown to have better approximation properties than shallow ones in the over-parameterized regime, and tractable algorithms for training deep models have been proposed. This paper shows that (tractable) kernel methods can be applied to deep networks trained with gradient descent, and that the approximation of kernels in deep networks is similar to that of shallow ones when the architecture of the kernel is a two-layer neural network. In particular, the authors show that kernels with ReLU activations can be approximated by deep fully-connected networks, and they show that the eigenvalue decay of the integral operator of a kernel on the sphere of the sphere is differentiable. The authors also show that a kernel function with differentiability properties on the kernel function can be used to explain the differentiability of different kernel function in deep architectures. ","This paper studies the approximation properties of deep networks compared to shallow ones. Deep networks have been shown to have better approximation properties than shallow ones in the over-parameterized regime, and tractable algorithms for training deep models have been proposed. This paper shows that (tractable) kernel methods can be applied to deep networks trained with gradient descent, and that the approximation of kernels in deep networks is similar to that of shallow ones when the architecture of the kernel is a two-layer neural network. In particular, the authors show that kernels with ReLU activations can be approximated by deep fully-connected networks, and they show that the eigenvalue decay of the integral operator of a kernel on the sphere of the sphere is differentiable. The authors also show that a kernel function with differentiability properties on the kernel function can be used to explain the differentiability of different kernel function in deep architectures. "
333,SP:3dd495394b880cf2fa055ee3fe218477625d2605,amplified value estimates CONJUNCTION suboptimal policies. suboptimal policies CONJUNCTION amplified value estimates. overestimation problem PART-OF deep value learning. underestimation bias CONJUNCTION instability. instability CONJUNCTION underestimation bias. algorithm USED-FOR overestimation. overestimation issues FEATURE-OF continuous control. algorithm USED-FOR policy improvement. deep reinforcement learning USED-FOR continuous control. deep reinforcement learning USED-FOR overestimation issues. combined value of weighted critics USED-FOR policy. weight factor USED-FOR independent critics. method USED-FOR policy improvement. algorithms USED-FOR continuous control. algorithms COMPARE algorithms. algorithms COMPARE algorithms. classical control tasks EVALUATE-FOR method. OtherScientificTerm is function approximation errors. ,"This paper studies the overestimation problem in deep value learning, which is the problem of underestimation bias and instability due to amplified value estimates and suboptimal policies. The authors propose an algorithm to mitigate overestimation issues in continuous control using deep reinforcement learning for continuous control. The algorithm is based on the idea that the combined value of weighted critics can be used as the policy, and independent critics are used as a weight factor. The proposed method is shown to be effective for policy improvement in the presence of function approximation errors. Experiments on classical control tasks show that the proposed algorithms outperform existing algorithms and are competitive with existing algorithms. ","This paper studies the overestimation problem in deep value learning, which is the problem of underestimation bias and instability due to amplified value estimates and suboptimal policies. The authors propose an algorithm to mitigate overestimation issues in continuous control using deep reinforcement learning for continuous control. The algorithm is based on the idea that the combined value of weighted critics can be used as the policy, and independent critics are used as a weight factor. The proposed method is shown to be effective for policy improvement in the presence of function approximation errors. Experiments on classical control tasks show that the proposed algorithms outperform existing algorithms and are competitive with existing algorithms. "
342,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"inverse reinforcement learning ( IRL ) problem USED-FOR reward functions. expert demonstrations USED-FOR reward functions. policy USED-FOR reward functions. expert demonstrations USED-FOR policies. ill - posed inverse problem HYPONYM-OF IRL problem. IRL problem USED-FOR well - posed expectation optimization problem. solution USED-FOR SIRL problem. solution USED-FOR learning task. solutions USED-FOR IRL problem. solution USED-FOR solutions. formulation USED-FOR IRL problem. objectworld EVALUATE-FOR approach. OtherScientificTerm are probability distribution over reward functions, and probability distribution. ","This paper considers the inverse reinforcement learning (IRL) problem to learn reward functions from expert demonstrations, where the goal is to learn a probability distribution over reward functions that maximizes the expected return of a policy trained on expert demonstrations. The IRL problem is called the ill-posed inverse problem, which is a well-posed expectation optimization problem. The authors propose a solution to the SIRL problem, and show that this solution can be used to solve any learning task. They also show that the proposed solution can also be used as a solution for any IRL problems, and that solutions to the IRL task can be obtained from this formulation. The proposed approach is tested on the objectworld, and is shown to be effective. ","This paper considers the inverse reinforcement learning (IRL) problem to learn reward functions from expert demonstrations, where the goal is to learn a probability distribution over reward functions that maximizes the expected return of a policy trained on expert demonstrations. The IRL problem is called the ill-posed inverse problem, which is a well-posed expectation optimization problem. The authors propose a solution to the SIRL problem, and show that this solution can be used to solve any learning task. They also show that the proposed solution can also be used as a solution for any IRL problems, and that solutions to the IRL task can be obtained from this formulation. The proposed approach is tested on the objectworld, and is shown to be effective. "
351,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"Self - training algorithms USED-FOR model. model USED-FOR pseudolabels. model USED-FOR pseudolabels. neural networks USED-FOR unlabeled data. self - training USED-FOR linear models. unsupervised domain adaptation CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION unsupervised domain adaptation. self - training USED-FOR semi - supervised learning. deep networks USED-FOR semi - supervised learning. deep networks USED-FOR unsupervised domain adaptation. semi - supervised learning CONJUNCTION unsupervised domain adaptation. unsupervised domain adaptation CONJUNCTION semi - supervised learning. self - training USED-FOR unsupervised domain adaptation. deep networks USED-FOR self - training. self - training CONJUNCTION input - consistency regularization. input - consistency regularization CONJUNCTION self - training. accuracy EVALUATE-FOR minimizers of population objectives. self - training USED-FOR minimizers of population objectives. input - consistency regularization USED-FOR minimizers of population objectives. margin CONJUNCTION Lipschitzness. Lipschitzness CONJUNCTION margin. sample complexity guarantees FEATURE-OF neural nets. margin FEATURE-OF neural nets. Lipschitzness FEATURE-OF neural nets. input consistency regularization USED-FOR self - training algorithms. OtherScientificTerm are neighborhoods, ground - truth labels, and generalization bounds. Generic is assumptions. ","Self-training algorithms are used to train a model to generate pseudolabels from a set of unlabeled data. In this paper, the authors show that self-training for linear models is a special case of this problem, where neural networks are trained with unlabeling data from neighborhoods. They show that under certain assumptions on the ground-truth labels, the model is able to learn pseudolabeled pseudo-labels from the ground truth labels. They also show that for semi-supervised learning and unsupervised domain adaptation using deep networks with sample complexity guarantees, self-learning and input-consistency regularization can improve the accuracy of minimizers of population objectives. The authors also provide generalization bounds for the case where the margin and Lipschitzness of the neural nets are known.    The paper is well-written, well-motivated, and well-structured. The results are well-supported by the theoretical analysis. The main contribution of the paper is a theoretical analysis of the effect of the margin of neural nets and the input consistency regularization on the performance of self-trained algorithms.","Self-training algorithms are used to train a model to generate pseudolabels from a set of unlabeled data. In this paper, the authors show that self-training for linear models is a special case of this problem, where neural networks are trained with unlabeling data from neighborhoods. They show that under certain assumptions on the ground-truth labels, the model is able to learn pseudolabeled pseudo-labels from the ground truth labels. They also show that for semi-supervised learning and unsupervised domain adaptation using deep networks with sample complexity guarantees, self-learning and input-consistency regularization can improve the accuracy of minimizers of population objectives. The authors also provide generalization bounds for the case where the margin and Lipschitzness of the neural nets are known.    The paper is well-written, well-motivated, and well-structured. The results are well-supported by the theoretical analysis. The main contribution of the paper is a theoretical analysis of the effect of the margin of neural nets and the input consistency regularization on the performance of self-trained algorithms."
360,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"hierarchical models USED-FOR video prediction. stochastic recurrent estimator USED-FOR long - term prediction. car driving CONJUNCTION human dancing. human dancing CONJUNCTION car driving. it USED-FOR complicated scene structures. video prediction COMPARE approaches. approaches COMPARE video prediction. Generic is method. OtherScientificTerm are semantic structures, structures, and discrete semantic structure space. Method is videoto - video translation. ","This paper proposes a new hierarchical models for video prediction based on hierarchical models. The proposed method is based on the observation that hierarchical models learn to predict semantic structures, and that these structures can be represented as a discrete semantic structure space. The authors propose a stochastic recurrent estimator for long-term prediction, and show that it can capture complicated scene structures (e.g. car driving and human dancing). The authors also show that video prediction can be more interpretable than previous approaches, and demonstrate that videoto-video translation can be performed in a more interpretability way.","This paper proposes a new hierarchical models for video prediction based on hierarchical models. The proposed method is based on the observation that hierarchical models learn to predict semantic structures, and that these structures can be represented as a discrete semantic structure space. The authors propose a stochastic recurrent estimator for long-term prediction, and show that it can capture complicated scene structures (e.g. car driving and human dancing). The authors also show that video prediction can be more interpretable than previous approaches, and demonstrate that videoto-video translation can be performed in a more interpretability way."
369,SP:e50b1931800daa7de577efd3edca523771227b3f,"undirected graph CONJUNCTION directed graph. directed graph CONJUNCTION undirected graph. Iterated Graph Neural Network System ( IGNNS ) HYPONYM-OF Graph Neural Networks ( GNNs ). Iterated Function System ( IFS ) HYPONYM-OF fractal geometry. Iterated Function System ( IFS ) PART-OF IGNNS. adjoint probability vector USED-FOR IFS layer. affine transformations USED-FOR IGNNS. geometric properties FEATURE-OF IGNNS. dynamical system USED-FOR IGNNS. dynamical system USED-FOR geometric properties. Frobenius norm FEATURE-OF constant matrix. IGNNS USED-FOR IFS. Hausdorff distance FEATURE-OF fractal set of IFS. Cora CONJUNCTION PubMed. PubMed CONJUNCTION Cora. citeser CONJUNCTION Cora. Cora CONJUNCTION citeser. citation network datasets USED-FOR semi - supervised node classification. PubMed HYPONYM-OF citation network datasets. citeser HYPONYM-OF citation network datasets. Cora HYPONYM-OF citation network datasets. OtherScientificTerm are graph nodes, latent space, and node features. Method are high - level representation of graph nodes, and fractal representation of graph nodes. Generic is method. ","This paper introduces the Iterated Graph Neural Network System (IGNNS), an extension of Graph Neural Networks (GNNs) that is inspired by the fractal geometry, i.e., the iterated Function System (IFS). IGNNS is a high-level representation of graph nodes that can be seen as a combination of undirected graph and directed graph. The IFS layer takes the adjoint probability vector of each node in the latent space as input and maps it to a fractal representation of the node features. The authors propose to use affine transformations on the IFS representation of a node in order to improve the geometric properties of IGNNs. They show that the Frobenius norm of the constant matrix of IFS can be reduced to that of a dynamical system. They also show that IFS is a special case of the IGNNN, and that the Hausdorff distance between IFS and IFS in the original and fractal set of INS is a function of the Frobonius norm. They validate their method on three citation network datasets for semi-supervised node classification: citeser, Cora, and PubMed. ","This paper introduces the Iterated Graph Neural Network System (IGNNS), an extension of Graph Neural Networks (GNNs) that is inspired by the fractal geometry, i.e., the iterated Function System (IFS). IGNNS is a high-level representation of graph nodes that can be seen as a combination of undirected graph and directed graph. The IFS layer takes the adjoint probability vector of each node in the latent space as input and maps it to a fractal representation of the node features. The authors propose to use affine transformations on the IFS representation of a node in order to improve the geometric properties of IGNNs. They show that the Frobenius norm of the constant matrix of IFS can be reduced to that of a dynamical system. They also show that IFS is a special case of the IGNNN, and that the Hausdorff distance between IFS and IFS in the original and fractal set of INS is a function of the Frobonius norm. They validate their method on three citation network datasets for semi-supervised node classification: citeser, Cora, and PubMed. "
378,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"modeling complex relations CONJUNCTION modeling isomorphic graphs. modeling isomorphic graphs CONJUNCTION modeling complex relations. GG - GAN USED-FOR graphs. GG - GAN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE GG - GAN. GG - GAN USED-FOR distribution statistics. Task is graph generation. OtherScientificTerm are similarity function, complex relations, isomorphic graphs, latent distribution, and problem - specific knowledge. Method are geometric interpretation, and Wasserstein GAN. ","This paper considers the problem of graph generation, where the goal is to learn a similarity function between two graphs where the similarity function is defined as the sum of complex relations between two isomorphic graphs. The authors propose a geometric interpretation of the problem and propose a Wasserstein GAN (GG-GAN) to solve the problem. GG-GAN can be used to learn graphs that are more general than existing state-of-the-art methods, and is able to learn complex relations, modeling complex relations and modeling isomorph graphs without requiring problem-specific knowledge of the underlying structure of the graph. They also propose to learn the distribution statistics of the generated graphs using GG-GAN.   ","This paper considers the problem of graph generation, where the goal is to learn a similarity function between two graphs where the similarity function is defined as the sum of complex relations between two isomorphic graphs. The authors propose a geometric interpretation of the problem and propose a Wasserstein GAN (GG-GAN) to solve the problem. GG-GAN can be used to learn graphs that are more general than existing state-of-the-art methods, and is able to learn complex relations, modeling complex relations and modeling isomorph graphs without requiring problem-specific knowledge of the underlying structure of the graph. They also propose to learn the distribution statistics of the generated graphs using GG-GAN.   "
387,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"activation values FEATURE-OF network. Minimum Description Length principle USED-FOR problem. rules CONJUNCTION super - charge prototyping. super - charge prototyping CONJUNCTION rules. Generic are method, and they. Method are neural network, and unsupervised EXPLAINN algorithm. OtherScientificTerm are noise - robust rules, class - specific traits, and convolutional layers. Material is activation data. ","This paper proposes a method for learning noise-robust rules for a neural network. The problem is formulated as the Minimum Description Length principle, which states that the activation values of a network should be at least as long as the training data. The authors propose an unsupervised EXPLAINN algorithm, which is based on the assumption that the noise in the activation data is limited to a small number of classes, and that the class-specific traits (e.g. activation values) of each class are well-separated from each other. They show that this is the case for rules and super-charge prototyping, and show that they can be learned in a supervised way. They also show that their method can be used to learn rules that are invariant to the number of convolutional layers.   ","This paper proposes a method for learning noise-robust rules for a neural network. The problem is formulated as the Minimum Description Length principle, which states that the activation values of a network should be at least as long as the training data. The authors propose an unsupervised EXPLAINN algorithm, which is based on the assumption that the noise in the activation data is limited to a small number of classes, and that the class-specific traits (e.g. activation values) of each class are well-separated from each other. They show that this is the case for rules and super-charge prototyping, and show that they can be learned in a supervised way. They also show that their method can be used to learn rules that are invariant to the number of convolutional layers.   "
396,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"worst - case guarantees FEATURE-OF fixed - dataset policy optimization algorithms. algorithms USED-FOR regime. principle USED-FOR algorithms. tabular gridworld CONJUNCTION deep learning. deep learning CONJUNCTION tabular gridworld. MinAtar environments EVALUATE-FOR deep learning. Method are naı̈ve approaches, pessimism principle, and pessimistic algorithms. OtherScientificTerm are erroneous value overestimation, and policy. "," and naı̈ve approaches are known to suffer from the problem of erroneous value overestimation. This paper studies the worst-case guarantees of fixed-dataset policy optimization algorithms. The authors propose a pessimism principle, which states that pessimistic algorithms are not guaranteed to converge to the optimal policy. They show that this principle can be applied to existing algorithms in this regime. They also show that deep learning can benefit from this principle in tabular gridworld and deep learning in MinAtar environments. "," and naı̈ve approaches are known to suffer from the problem of erroneous value overestimation. This paper studies the worst-case guarantees of fixed-dataset policy optimization algorithms. The authors propose a pessimism principle, which states that pessimistic algorithms are not guaranteed to converge to the optimal policy. They show that this principle can be applied to existing algorithms in this regime. They also show that deep learning can benefit from this principle in tabular gridworld and deep learning in MinAtar environments. "
405,SP:363661edd15a06a800b51abc1541a3191311ee0e,"Neural ordinary differential equations ( Neural ODEs ) HYPONYM-OF deeplearning models. continuous depth FEATURE-OF deeplearning models. naive method CONJUNCTION adaptive checkpoint adjoint method ( ACA ). adaptive checkpoint adjoint method ( ACA ) CONJUNCTION naive method. continuous case FEATURE-OF numerical estimation of the gradient. accuracy EVALUATE-FOR gradient estimation. accuracy EVALUATE-FOR reverse - time trajectory. constant memory cost FEATURE-OF ALF Integrator ( MALI ). heavy memory burden CONJUNCTION inaccuracy. inaccuracy CONJUNCTION heavy memory burden. MALI COMPARE ResNet. ResNet COMPARE MALI. MALI COMPARE adjoint method. adjoint method COMPARE MALI. MALI USED-FOR Neural ODE. MALI COMPARE methods. methods COMPARE MALI. image recognition tasks EVALUATE-FOR MALI. tasks EVALUATE-FOR MALI. ImageNet USED-FOR Neural ODE. image recognition tasks HYPONYM-OF tasks. tasks EVALUATE-FOR MALI. MALI USED-FOR continuous generative models. image recognition tasks EVALUATE-FOR MALI. adjoint method USED-FOR time series modeling. MALI USED-FOR time series modeling. Metric is memory cost. OtherScientificTerm are integration time, and solver steps. Method are asynchronous leapfrog ( ALF ) solver, and pypi package. ","This paper studies the problem of learning neural ODEs (Neural ordinary differential equations) with continuous depth, which is an important problem for deeplearning models such as neural ordinary differential equation (NODEs) and Neural ODE (NeurIPS). The authors propose an adaptive checkpoint adjoint method (ALF Integrator) to reduce the memory cost of learning the Neural OED. The main contribution of this paper is to extend the naive method (Chen et al., 2017) and the adaptive checkpoint adaption method (Burgess et al. 2017) to the continuous case. The authors show that the numerical estimation of the gradient in continuous case has a constant memory cost, and the authors propose a new asynchronous leapfrog (ALKF) solver, called MALI, which can be used to reduce this memory cost.    The main idea is to use the ALF Integrators to compute the reverse-time trajectory of the forward and backward-time trajectories, and then use the accuracy of gradient estimation to improve the accuracy in the reverse time.  The authors demonstrate that MALi achieves the same accuracy as ResNet, but at the cost of heavy memory burden and inaccuracy due to the large integration time. They also show that with the same number of solver steps, the authors can achieve better accuracy than ResNet.  In addition, they also demonstrate that the authors have the ability to train a Neural OODE on ImageNet with ImageNet, and that the proposed MALIs outperform the previous methods on a number of tasks such as image recognition tasks.  Finally, they show that they can train continuous generative models with the proposed ALF and show that their MALF can be applied to the pypi package.  They show that in time series modeling, they can improve the performance of MALNets by using the proposed mALI to learn a continuous time series model with constant memory. In addition to this, they demonstrate that their mALIs can also be used as a generalization of the adjoint algorithm of Chen et al (2017) to time series models. ","This paper studies the problem of learning neural ODEs (Neural ordinary differential equations) with continuous depth, which is an important problem for deeplearning models such as neural ordinary differential equation (NODEs) and Neural ODE (NeurIPS). The authors propose an adaptive checkpoint adjoint method (ALF Integrator) to reduce the memory cost of learning the Neural OED. The main contribution of this paper is to extend the naive method (Chen et al., 2017) and the adaptive checkpoint adaption method (Burgess et al. 2017) to the continuous case. The authors show that the numerical estimation of the gradient in continuous case has a constant memory cost, and the authors propose a new asynchronous leapfrog (ALKF) solver, called MALI, which can be used to reduce this memory cost.    The main idea is to use the ALF Integrators to compute the reverse-time trajectory of the forward and backward-time trajectories, and then use the accuracy of gradient estimation to improve the accuracy in the reverse time.  The authors demonstrate that MALi achieves the same accuracy as ResNet, but at the cost of heavy memory burden and inaccuracy due to the large integration time. They also show that with the same number of solver steps, the authors can achieve better accuracy than ResNet.  In addition, they also demonstrate that the authors have the ability to train a Neural OODE on ImageNet with ImageNet, and that the proposed MALIs outperform the previous methods on a number of tasks such as image recognition tasks.  Finally, they show that they can train continuous generative models with the proposed ALF and show that their MALF can be applied to the pypi package.  They show that in time series modeling, they can improve the performance of MALNets by using the proposed mALI to learn a continuous time series model with constant memory. In addition to this, they demonstrate that their mALIs can also be used as a generalization of the adjoint algorithm of Chen et al (2017) to time series models. "
414,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,methodology EVALUATE-FOR complex scene conditional generation models. model USED-FOR seen conditionings. seen object combinations FEATURE-OF unseen conditionings. unseen object combinations USED-FOR unseen conditionings. methods USED-FOR recognizable scenes. compositionality USED-FOR unseen conditionings. compositionality USED-FOR methods. seen conditionings USED-FOR methods. seen object combinations USED-FOR unseen conditionings. unseen object combinations FEATURE-OF conditionings. image quality degradation EVALUATE-FOR methods. semantically aware losses USED-FOR generation process. robustness EVALUATE-FOR unseen conditionings. robustness FEATURE-OF unseen conditionings. instance - wise spatial conditioning normalizations USED-FOR compositionality. scene - graph perceptual similarity HYPONYM-OF semantically aware losses. Generic is models. Method is pipeline components. ,"This paper presents a new methodology for evaluating complex scene conditional generation models. The authors propose a new model for evaluating unseen conditionings based on seen object combinations of unseen object combinations. They show that existing methods for generating recognizable scenes based on compositionality are not robust to image quality degradation, and propose two methods that are. The first is to use semantically aware losses (e.g., scene-graph perceptual similarity) to guide the generation process, and the second is to apply instance-wise spatial conditioning normalizations to improve the compositionality. The models are evaluated on a number of datasets, and show that the proposed pipeline components are able to generate images that are more robust to unseen conditioning.","This paper presents a new methodology for evaluating complex scene conditional generation models. The authors propose a new model for evaluating unseen conditionings based on seen object combinations of unseen object combinations. They show that existing methods for generating recognizable scenes based on compositionality are not robust to image quality degradation, and propose two methods that are. The first is to use semantically aware losses (e.g., scene-graph perceptual similarity) to guide the generation process, and the second is to apply instance-wise spatial conditioning normalizations to improve the compositionality. The models are evaluated on a number of datasets, and show that the proposed pipeline components are able to generate images that are more robust to unseen conditioning."
423,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"expressive power CONJUNCTION learning. learning CONJUNCTION expressive power. multi - hop operators FEATURE-OF graph. multi - hop operators USED-FOR node features. GA - MLPs USED-FOR non - isomorphic graphs. WeisfeilerLehman ( WL ) test CONJUNCTION GNNs. GNNs CONJUNCTION WeisfeilerLehman ( WL ) test. operators USED-FOR GA - MLPs. GA - MLPs CONJUNCTION GNNs. GNNs CONJUNCTION GA - MLPs. expressive power EVALUATE-FOR GNNs. expressive power EVALUATE-FOR GA - MLPs. node - level functions USED-FOR them. GNNs COMPARE GA - MLPs. GA - MLPs COMPARE GNNs. GA - MLPs USED-FOR attributed walks. community detection EVALUATE-FOR GA - MLPs. GNNs USED-FOR learning. operator family USED-FOR GA - MLPs. Generic is alternative. Method is learnable node - wise functions. OtherScientificTerm are node - wise functions, and rooted graphs. Task is graph isomorphism testing. ","This paper proposes a new variant of graph isomorphism-based MLPs (GA-MLPs) for non-isomorphic graphs. The main idea is to extend the WeisfeilerLehman (WL) test and GNNs to the case of multi-hop operators on the graph, which allows for node features to be learned via learnable node-wise functions. The paper shows that the expressive power and learning performance of the proposed GA-MLP is comparable to those of the original WL test, and that the proposed alternative can be seen as an alternative to GNN.   The paper also shows that GA- MLPs can be used to learn non-Isomorphic graphs with the same expressive power as the original GNN, and also that they can learn them with node-level functions.  Finally, the paper shows how the proposed operators can be combined with existing operators in order to improve the performance of GA-mlPs. The authors also show that the learned operator family can be extended to the context of graph-isomorphism testing, which is a useful tool for the community detection task.  Experiments are conducted on several datasets to demonstrate that the performance gains of GA - MLPs over GNN are due to the use of the operator family and not to the fact that they are able to learn node-specific functions. In addition, the authors show that GA -MLPs can learn attributed walks in a similar way as GNN and that are more expressive than the original one, and can be applied to community detection tasks.","This paper proposes a new variant of graph isomorphism-based MLPs (GA-MLPs) for non-isomorphic graphs. The main idea is to extend the WeisfeilerLehman (WL) test and GNNs to the case of multi-hop operators on the graph, which allows for node features to be learned via learnable node-wise functions. The paper shows that the expressive power and learning performance of the proposed GA-MLP is comparable to those of the original WL test, and that the proposed alternative can be seen as an alternative to GNN.   The paper also shows that GA- MLPs can be used to learn non-Isomorphic graphs with the same expressive power as the original GNN, and also that they can learn them with node-level functions.  Finally, the paper shows how the proposed operators can be combined with existing operators in order to improve the performance of GA-mlPs. The authors also show that the learned operator family can be extended to the context of graph-isomorphism testing, which is a useful tool for the community detection task.  Experiments are conducted on several datasets to demonstrate that the performance gains of GA - MLPs over GNN are due to the use of the operator family and not to the fact that they are able to learn node-specific functions. In addition, the authors show that GA -MLPs can learn attributed walks in a similar way as GNN and that are more expressive than the original one, and can be applied to community detection tasks."
432,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"model complexity EVALUATE-FOR Reinforcement Learning ( RL ) agents. real - world applications EVALUATE-FOR Reinforcement Learning ( RL ) agents. robotics HYPONYM-OF real - world applications. acting USED-FOR distributed RL settings. unaccelerated hardware USED-FOR acting. CPUs HYPONYM-OF unaccelerated hardware. model complexity EVALUATE-FOR supervised learning. distillation USED-FOR learning progress. large capacity learner model COMPARE small capacity actor model. small capacity actor model COMPARE large capacity learner model. system USED-FOR acting. transformer models COMPARE LSTMs. LSTMs COMPARE transformer models. procedure USED-FOR partially - observable environments. computational complexity EVALUATE-FOR transformer models. transformer models CONJUNCTION LSTMs. LSTMs CONJUNCTION transformer models. fast inference CONJUNCTION total training time. total training time CONJUNCTION fast inference. Actor - Learner Distillation USED-FOR transformer learner model. total training time EVALUATE-FOR LSTM actor model. fast inference EVALUATE-FOR LSTM actor model. Actor - Learner Distillation USED-FOR memory environments. OtherScientificTerm are compute, model size, intractable experiment run times, actor - latency ” constrained settings, and model capacity. ","This paper studies the model complexity of Reinforcement Learning (RL) agents in real-world applications. The authors consider the problem of distributed RL settings where there is a large amount of compute and the number of agents is limited. In particular, the authors consider a setting where the agents are trained on unaccelerated hardware (e.g. CPUs) and the goal is to learn a system that performs well in a distributed RL setting.    The authors show that a large capacity learner model outperforms a small capacity actor model in this setting. This is due to the large model size and the fact that the learning progress is accelerated through distillation.  The main contribution of the paper is that the system is able to perform well in the setting where there are many agents and each agent has a limited amount of computing resources. The system is also able to do acting in the presence of a large number of machines (i.e. agents that share the same hardware) and can be trained in a way that allows for the system to perform acting on a limited number of devices (which can be expensive). The authors also show that this system can be used to reduce the computational complexity of supervised learning by reducing model complexity.  In addition, they show that their procedure can be applied to partially-observable environments, where the agent is trained in memory and the system needs to be able to run the system for a long time. They show that transformer models and LSTMs have similar computational complexity as transformer models but with much faster inference and total training time.  Finally, they propose an Actor-Learner Distillation to improve the performance of the transformer learner by reducing the model size, which is shown to be effective in terms of fast inference and intractable experiment run times. They also demonstrate that in the case of “actor-latency ” constrained settings, their system performs better than a LSTM actor model, which has much more computational complexity.","This paper studies the model complexity of Reinforcement Learning (RL) agents in real-world applications. The authors consider the problem of distributed RL settings where there is a large amount of compute and the number of agents is limited. In particular, the authors consider a setting where the agents are trained on unaccelerated hardware (e.g. CPUs) and the goal is to learn a system that performs well in a distributed RL setting.    The authors show that a large capacity learner model outperforms a small capacity actor model in this setting. This is due to the large model size and the fact that the learning progress is accelerated through distillation.  The main contribution of the paper is that the system is able to perform well in the setting where there are many agents and each agent has a limited amount of computing resources. The system is also able to do acting in the presence of a large number of machines (i.e. agents that share the same hardware) and can be trained in a way that allows for the system to perform acting on a limited number of devices (which can be expensive). The authors also show that this system can be used to reduce the computational complexity of supervised learning by reducing model complexity.  In addition, they show that their procedure can be applied to partially-observable environments, where the agent is trained in memory and the system needs to be able to run the system for a long time. They show that transformer models and LSTMs have similar computational complexity as transformer models but with much faster inference and total training time.  Finally, they propose an Actor-Learner Distillation to improve the performance of the transformer learner by reducing the model size, which is shown to be effective in terms of fast inference and intractable experiment run times. They also demonstrate that in the case of “actor-latency ” constrained settings, their system performs better than a LSTM actor model, which has much more computational complexity."
441,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,"benchmarks USED-FOR problem. Meta - Dataset HYPONYM-OF benchmarks. universal features USED-FOR few - shot classification. Meta - Dataset EVALUATE-FOR URT. model USED-FOR cross - domain generalization. Task are Few - shot classification, multi - domain few - shot image classification, and multi - domain setting. Material is diverse data sources. Method are feature representations, Universal Representation Transformer ( URT ) layer, and domain - specific representations. Generic is it. OtherScientificTerm is attention score heatmaps. ","This paper tackles the problem of few-shot classification with diverse data sources, where the goal is to learn feature representations that are transferable across different domains. The authors propose a new benchmark, Meta-Dataset, for this problem, which consists of two benchmarks: 1) Meta-DATASet, which is a benchmark for multi-domain few-distribution image classification, and 2) meta-dataset for cross-domain generalization. The paper proposes a new Universal Representation Transformer (URT) layer, which learns universal features for few-domain classification. The URT is evaluated on Meta- Dataset and Meta-CIFAR-10, and it is shown to outperform the previous state-of-the-art methods. The model is also shown to be able to improve cross-domains generalization by learning domain-specific representations.   The paper is well-written and well-motivated, and the experiments show that the proposed model is able to generalize to new domains with high attention score heatmaps. ","This paper tackles the problem of few-shot classification with diverse data sources, where the goal is to learn feature representations that are transferable across different domains. The authors propose a new benchmark, Meta-Dataset, for this problem, which consists of two benchmarks: 1) Meta-DATASet, which is a benchmark for multi-domain few-distribution image classification, and 2) meta-dataset for cross-domain generalization. The paper proposes a new Universal Representation Transformer (URT) layer, which learns universal features for few-domain classification. The URT is evaluated on Meta- Dataset and Meta-CIFAR-10, and it is shown to outperform the previous state-of-the-art methods. The model is also shown to be able to improve cross-domains generalization by learning domain-specific representations.   The paper is well-written and well-motivated, and the experiments show that the proposed model is able to generalize to new domains with high attention score heatmaps. "
450,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"non - stationary stream of unlabeled data USED-FOR salient representations. representations USED-FOR classification tasks. Self - Taught Associative Memory ( STAM ) HYPONYM-OF online clustering module. architecture USED-FOR UPL problem. online clustering module PART-OF architecture. online clustering CONJUNCTION novelty detection. novelty detection CONJUNCTION online clustering. novelty detection CONJUNCTION forgetting outliers. forgetting outliers CONJUNCTION novelty detection. novelty detection USED-FOR Layered hierarchies of STAM modules. online clustering USED-FOR Layered hierarchies of STAM modules. UPL context EVALUATE-FOR latter. Task is Unsupervised Progressive Learning ( UPL ) problem. Material is limited labeled data. Method are prototypical representations, and STAM architecture. ","This paper tackles the Unsupervised Progressive Learning (UPL) problem where there is a non-stationary stream of unlabeled data, and the goal is to learn salient representations from a limited amount of labeled data. The paper proposes a novel architecture called Self-Taught Associative Memory (STAM) which is an online clustering module that learns to learn representations for classification tasks with limited labeled data, which are then used to learn prototypical representations. The proposed architecture is designed to solve the UPL problem by incorporating the benefits of the STAM architecture. Layered hierarchies of STAM modules are learned based on the use of the online clusterering, novelty detection, and forgetting outliers. The latter is shown to be effective in a UPL context. ","This paper tackles the Unsupervised Progressive Learning (UPL) problem where there is a non-stationary stream of unlabeled data, and the goal is to learn salient representations from a limited amount of labeled data. The paper proposes a novel architecture called Self-Taught Associative Memory (STAM) which is an online clustering module that learns to learn representations for classification tasks with limited labeled data, which are then used to learn prototypical representations. The proposed architecture is designed to solve the UPL problem by incorporating the benefits of the STAM architecture. Layered hierarchies of STAM modules are learned based on the use of the online clusterering, novelty detection, and forgetting outliers. The latter is shown to be effective in a UPL context. "
459,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"communication topology CONJUNCTION data partitioning. data partitioning CONJUNCTION communication topology. network size CONJUNCTION communication topology. communication topology CONJUNCTION network size. models COMPARE models. models COMPARE models. data partitioning HYPONYM-OF parameters. communication topology HYPONYM-OF parameters. network size HYPONYM-OF parameters. network topology CONJUNCTION learning rate. learning rate CONJUNCTION network topology. generalization gap FEATURE-OF decentralized deep learning. Method are deep learning models, on - device learning over networks, decentralized training, centralized training, communication efficient training schemes, and training schemes. OtherScientificTerm are large compute clusters, and consensus distance. "," to train deep learning models. This paper studies the generalization gap in decentralized deep learning. The authors consider on-device learning over networks, where the network size, network topology, communication topology and data partitioning are all important parameters. They show that models trained with these parameters are more general than models trained using centralized training. They also show that decentralized training is more robust to large compute clusters, and that centralized training is less robust to communication efficient training schemes. Finally, they show that there is a trade-off between the generalizability of the trained model and the communication efficiency of the training scheme.    The authors also provide a theoretical analysis that shows that there exists a tradeoff between network size and learning rate, and show that the tradeoff depends on the number of clients and the consensus distance."," to train deep learning models. This paper studies the generalization gap in decentralized deep learning. The authors consider on-device learning over networks, where the network size, network topology, communication topology and data partitioning are all important parameters. They show that models trained with these parameters are more general than models trained using centralized training. They also show that decentralized training is more robust to large compute clusters, and that centralized training is less robust to communication efficient training schemes. Finally, they show that there is a trade-off between the generalizability of the trained model and the communication efficiency of the training scheme.    The authors also provide a theoretical analysis that shows that there exists a tradeoff between network size and learning rate, and show that the tradeoff depends on the number of clients and the consensus distance."
468,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,activity recognition CONJUNCTION natural language processing. natural language processing CONJUNCTION activity recognition. sequence alignment approaches CONJUNCTION representation learning. representation learning CONJUNCTION sequence alignment approaches. Sequence metric learning USED-FOR applications. sequential multi - variate data USED-FOR applications. natural language processing HYPONYM-OF applications. activity recognition HYPONYM-OF sequential multi - variate data. natural language processing HYPONYM-OF sequential multi - variate data. sequence alignment approaches USED-FOR applications. activity recognition HYPONYM-OF applications. representation learning USED-FOR applications. synchronized trajectories CONJUNCTION distance between similar sequences. distance between similar sequences CONJUNCTION synchronized trajectories. dynamical systems USED-FOR synchronized trajectories. siamese recurrent neural network USED-FOR distance between similar sequences. sub - networks CONJUNCTION dynamical systems. dynamical systems CONJUNCTION sub - networks. dynamical systems PART-OF siamese recurrent network. sub - networks PART-OF siamese recurrent network. gate PART-OF classical Gated Recurrent Unit architecture. neural network model USED-FOR coupling. gate USED-FOR neural network model. gate USED-FOR coupling. model USED-FOR synchronization of unaligned multi - variate sequences. model USED-FOR similarity metric. similarity metric CONJUNCTION synchronization of unaligned multi - variate sequences. synchronization of unaligned multi - variate sequences CONJUNCTION similarity metric. coupling USED-FOR siamese Gated Recurrent Unit architecture. activity recognition dataset EVALUATE-FOR siamese Gated Recurrent Unit architecture. Method is dynamical system theory. ,"This paper studies the problem of learning representations for sequential multi-variate data (e.g. activity recognition and natural language processing). Sequence metric learning is an important problem in these applications, and there are many recent advances in sequence alignment approaches and representation learning. This paper proposes a siamese recurrent neural network to learn the distance between similar sequences using a sequence of synchronized trajectories and a distance between different sequences using dynamical systems.   The core idea is to use a series of sub-networks, sub-dynamical systems, and a classical Gated Recurrent Unit architecture with a gate to learn a coupling between the sub-network and the dynamical system theory.  The proposed siamesese recurrent network consists of two sub-nets: one for each sub-sequence, and one for the whole sequence.  This paper also proposes a neural network model to learn this coupling.  Experiments on the activity recognition dataset show that the siamesed recurrent unit architecture achieves state-of-the-art performance on the proposed coupling, and the proposed model can also be used for learning a similarity metric and for the synchronization of unaligned multi-viate sequences. ","This paper studies the problem of learning representations for sequential multi-variate data (e.g. activity recognition and natural language processing). Sequence metric learning is an important problem in these applications, and there are many recent advances in sequence alignment approaches and representation learning. This paper proposes a siamese recurrent neural network to learn the distance between similar sequences using a sequence of synchronized trajectories and a distance between different sequences using dynamical systems.   The core idea is to use a series of sub-networks, sub-dynamical systems, and a classical Gated Recurrent Unit architecture with a gate to learn a coupling between the sub-network and the dynamical system theory.  The proposed siamesese recurrent network consists of two sub-nets: one for each sub-sequence, and one for the whole sequence.  This paper also proposes a neural network model to learn this coupling.  Experiments on the activity recognition dataset show that the siamesed recurrent unit architecture achieves state-of-the-art performance on the proposed coupling, and the proposed model can also be used for learning a similarity metric and for the synchronization of unaligned multi-viate sequences. "
477,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"codistillation PART-OF distributed training setup. models COMPARE models. models COMPARE models. codistillation USED-FOR models. synchronization mechanism USED-FOR models. synchronous data - parallel methods USED-FOR models. synchronization mechanism USED-FOR models. synchronous data - parallel methods USED-FOR models. codistillation USED-FOR models. batch sizes CONJUNCTION learning rate schedules. learning rate schedules CONJUNCTION batch sizes. it USED-FOR distributed computing environment. Method is Codistillation. OtherScientificTerm are auxiliary loss, model replicas, large batch sizes, and moderate batch sizes. Metric is accuracy. ","Codistillation is a technique that is used in distributed training to improve the performance of models in a distributed training setup. The idea is that models trained with codistillation are more accurate than models trained using synchronous data-parallel methods. Codistillation consists of two steps: (1) an auxiliary loss is added to the loss of the original model, (2) a synchronization mechanism is used to ensure that the models are trained with the same synchronization mechanism as the original models.    The authors show that the effectiveness of the proposed approach is demonstrated by comparing the accuracy of model replicas trained with large batch sizes to models trained in a setting with moderate batch sizes. The authors also show that models that use the proposed method (codistillation) outperform models that do not use the synchronization mechanism.  The paper also shows that it can be applied to any distributed computing environment where the batch sizes and learning rate schedules are different. ","Codistillation is a technique that is used in distributed training to improve the performance of models in a distributed training setup. The idea is that models trained with codistillation are more accurate than models trained using synchronous data-parallel methods. Codistillation consists of two steps: (1) an auxiliary loss is added to the loss of the original model, (2) a synchronization mechanism is used to ensure that the models are trained with the same synchronization mechanism as the original models.    The authors show that the effectiveness of the proposed approach is demonstrated by comparing the accuracy of model replicas trained with large batch sizes to models trained in a setting with moderate batch sizes. The authors also show that models that use the proposed method (codistillation) outperform models that do not use the synchronization mechanism.  The paper also shows that it can be applied to any distributed computing environment where the batch sizes and learning rate schedules are different. "
486,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"capacity CONJUNCTION complexity. complexity CONJUNCTION capacity. stochastic gradient descent ( SGD ) USED-FOR deep learning. SGD USED-FOR local minimum. SGD iterates USED-FOR heavy - tailed stationary distribution. algorithm parameters CONJUNCTION b. b CONJUNCTION algorithm parameters. independent and identically distributed Gaussian data USED-FOR linear regression problem. dimension CONJUNCTION curvature. curvature CONJUNCTION dimension. algorithm parameters CONJUNCTION dimension. dimension CONJUNCTION algorithm parameters. algorithm parameters USED-FOR tails. SGD USED-FOR deep learning. synthetic data CONJUNCTION fully connected neural networks. fully connected neural networks CONJUNCTION synthetic data. OtherScientificTerm are eigenvalues of the Hessian, batch size b, stochastic gradient noise, tail - index ’, network weights, and Hessian. Metric is tail - index. Task are generalization, and quadratic optimization. ","This paper studies the generalization performance of SGD in deep learning with stochastic gradient descent (SGD) in the setting where the eigenvalues of the Hessian are heavy tail-tailed. The authors show that SGD converges to a local minimum when the tail-index of the gradient is large enough. They show that the heavy-tailed stationary distribution can be approximated by SGD iterates. They also show that if the batch size b is small enough, SGD will converge to a heavy-tail stationary distribution.    The authors consider a linear regression problem on independent and identically distributed Gaussian data, where the weights of the network are Gaussians. They consider the case where there is a ‘tail-index’, i.e. the number of times that the tail is larger than a certain threshold, and they show that under certain assumptions on the algorithm parameters and b, the capacity and complexity of the tail are equal to that of the capacity of the algorithm and the complexity of b. In particular, the authors consider quadratic optimization, which is a special case of the case when the weights are Gaussian.  They also consider a case where the network weights are non-Gaussian, and show that in this case, the ‘heavy-tailed’ eigenvectors of the tails can be represented as a function of the dimension and curvature of the data, and the authors show empirically that for any algorithm parameters (or dimension, or curvature) and the dimension of the dataset, the tails will be close to the local minimum. They then show that for synthetic data and fully connected fully connected neural networks, and for some synthetic data with fully connected weights, that the tails tend to be larger than the local minimizers of the SGD for deep learning. Finally, they also provide some theoretical results on generalization.","This paper studies the generalization performance of SGD in deep learning with stochastic gradient descent (SGD) in the setting where the eigenvalues of the Hessian are heavy tail-tailed. The authors show that SGD converges to a local minimum when the tail-index of the gradient is large enough. They show that the heavy-tailed stationary distribution can be approximated by SGD iterates. They also show that if the batch size b is small enough, SGD will converge to a heavy-tail stationary distribution.    The authors consider a linear regression problem on independent and identically distributed Gaussian data, where the weights of the network are Gaussians. They consider the case where there is a ‘tail-index’, i.e. the number of times that the tail is larger than a certain threshold, and they show that under certain assumptions on the algorithm parameters and b, the capacity and complexity of the tail are equal to that of the capacity of the algorithm and the complexity of b. In particular, the authors consider quadratic optimization, which is a special case of the case when the weights are Gaussian.  They also consider a case where the network weights are non-Gaussian, and show that in this case, the ‘heavy-tailed’ eigenvectors of the tails can be represented as a function of the dimension and curvature of the data, and the authors show empirically that for any algorithm parameters (or dimension, or curvature) and the dimension of the dataset, the tails will be close to the local minimum. They then show that for synthetic data and fully connected fully connected neural networks, and for some synthetic data with fully connected weights, that the tails tend to be larger than the local minimizers of the SGD for deep learning. Finally, they also provide some theoretical results on generalization."
495,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,models USED-FOR community detection. spectral manipulations USED-FOR models. bandpass filtering USED-FOR GCN. high - frequencies USED-FOR community detection. images HYPONYM-OF Euclidean graph. spectral components USED-FOR supervised community detection task. graph structure USED-FOR cascade of filtering. low - frequency domain FEATURE-OF spectral components. low frequencies USED-FOR classifiers. Task is nodes classification. Method is GCNs. ,"This paper proposes to use spectral manipulations to improve the performance of models for community detection with high-frequencies. Specifically, the authors propose to use bandpass filtering in a GCN, which can be seen as an extension of previous work on nodes classification. The authors show that the spectral components of the supervised community detection task can be manipulated in the low-frequency domain, and that the cascade of filtering can be controlled by the graph structure of the Euclidean graph (e.g. images). The authors also show that classifiers trained with low frequencies are more robust to such spectral components than GCNs.","This paper proposes to use spectral manipulations to improve the performance of models for community detection with high-frequencies. Specifically, the authors propose to use bandpass filtering in a GCN, which can be seen as an extension of previous work on nodes classification. The authors show that the spectral components of the supervised community detection task can be manipulated in the low-frequency domain, and that the cascade of filtering can be controlled by the graph structure of the Euclidean graph (e.g. images). The authors also show that classifiers trained with low frequencies are more robust to such spectral components than GCNs."
504,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"graph structure USED-FOR Graph neural networks ( GNNs ). structure COMPARE real - world applications. real - world applications COMPARE structure. structure CONJUNCTION GNN parameters. GNN parameters CONJUNCTION structure. taskspecific supervision USED-FOR structure. taskspecific supervision USED-FOR GNN parameters. method USED-FOR supervision. method USED-FOR graph structure. self - supervision USED-FOR method. models USED-FOR task - specific graph structure. SLAPS COMPARE models. models COMPARE SLAPS. OtherScientificTerm are task - specific latent structure, graph structures, and Self - supervision. Method is GNN. ","Graph neural networks (GNNs) have been shown to be able to learn a graph structure that can be used for task-specific latent structure. However, this structure has not been widely explored in real-world applications. This paper shows that this structure and GNN parameters can be learned from taskspecific supervision. The authors propose a method for learning this graph structure from supervision using self-supervision. Self-supervised supervision is used to train a GNN to learn the graph structures. The proposed method, SLAPS, is shown to outperform existing models in learning a task- specific graph structure.","Graph neural networks (GNNs) have been shown to be able to learn a graph structure that can be used for task-specific latent structure. However, this structure has not been widely explored in real-world applications. This paper shows that this structure and GNN parameters can be learned from taskspecific supervision. The authors propose a method for learning this graph structure from supervision using self-supervision. Self-supervised supervision is used to train a GNN to learn the graph structures. The proposed method, SLAPS, is shown to outperform existing models in learning a task- specific graph structure."
513,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,Continual Learning USED-FOR catastrophic forgetting. supervised training USED-FOR they. model USED-FOR label - agnostic incremental setting. network confusion USED-FOR novelty detection method. class - imbalance USED-FOR detection method. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. CIFAR-100 CONJUNCTION CRIB. CRIB CONJUNCTION CIFAR-100. image classification benchmarks EVALUATE-FOR approach. CRIB HYPONYM-OF image classification benchmarks. MNIST HYPONYM-OF image classification benchmarks. CIFAR-100 HYPONYM-OF image classification benchmarks. CIFAR-10 HYPONYM-OF image classification benchmarks. SVHN HYPONYM-OF image classification benchmarks. ,"This paper studies the problem of catastrophic forgetting in Continual Learning. The authors propose a novel novelty detection method based on network confusion and class-imbalance. They also propose a new model for the label-agnostic incremental setting where they do not require any supervised training. The proposed approach is evaluated on several image classification benchmarks (MNIST, SVHN, CIFAR-10, CifAR-100, and CRIB). ","This paper studies the problem of catastrophic forgetting in Continual Learning. The authors propose a novel novelty detection method based on network confusion and class-imbalance. They also propose a new model for the label-agnostic incremental setting where they do not require any supervised training. The proposed approach is evaluated on several image classification benchmarks (MNIST, SVHN, CIFAR-10, CifAR-100, and CRIB). "
522,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,robotics CONJUNCTION autonomous cars. autonomous cars CONJUNCTION robotics. domain expertise USED-FOR specifications. natural language constraints USED-FOR safe RL. HAZARDWORLD HYPONYM-OF multi - task benchmark. agent USED-FOR multi - task benchmark. free - form text USED-FOR constraints. agent USED-FOR tasks. modular architecture FEATURE-OF agent. policy network USED-FOR policy. constraint interpreter USED-FOR spatial and temporal representations of forbidden states. constraint interpreter USED-FOR textual constraints. representations USED-FOR policy. minimal constraint violations FEATURE-OF policy. representations USED-FOR policy network. policy network PART-OF model. constraint interpreter PART-OF model. method COMPARE approaches. approaches COMPARE method. rewards CONJUNCTION constraint violations. constraint violations CONJUNCTION rewards. HAZARDWORLD EVALUATE-FOR method. constraint violations EVALUATE-FOR method. rewards EVALUATE-FOR method. Task is safe reinforcement learning ( RL ). OtherScientificTerm is mathematical form. ,"This paper addresses the problem of safe reinforcement learning (RL) in the presence of natural language constraints. This is an important problem in robotics, autonomous cars, and robotics with domain expertise. In this paper, the authors focus on safe RL in the case of natural languages, where the constraints are expressed in a mathematical form and the agent is trained on a multi-task benchmark (HAZARDWORLD). The agent has a modular architecture, where constraints are represented as free-form text, and the agents are trained on different tasks. The proposed model consists of a constraint interpreter that generates spatial and temporal representations of forbidden states, and a policy network that uses these representations to learn a policy with minimal constraint violations. The method is evaluated on the task of safe RL on the multi-tasks on HAZARDWLD, and shows that the proposed method outperforms existing approaches in terms of rewards and constraint violations on the tasks. ","This paper addresses the problem of safe reinforcement learning (RL) in the presence of natural language constraints. This is an important problem in robotics, autonomous cars, and robotics with domain expertise. In this paper, the authors focus on safe RL in the case of natural languages, where the constraints are expressed in a mathematical form and the agent is trained on a multi-task benchmark (HAZARDWORLD). The agent has a modular architecture, where constraints are represented as free-form text, and the agents are trained on different tasks. The proposed model consists of a constraint interpreter that generates spatial and temporal representations of forbidden states, and a policy network that uses these representations to learn a policy with minimal constraint violations. The method is evaluated on the task of safe RL on the multi-tasks on HAZARDWLD, and shows that the proposed method outperforms existing approaches in terms of rewards and constraint violations on the tasks. "
531,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"few - shot semantic edge detection USED-FOR boundaries of novel categories. few - shot semantic edge detection HYPONYM-OF few - shot learning challenge. image generation CONJUNCTION medical imaging. medical imaging CONJUNCTION image generation. semantic segmentation CONJUNCTION localization. localization CONJUNCTION semantic segmentation. object reconstruction CONJUNCTION image generation. image generation CONJUNCTION object reconstruction. boundary information USED-FOR semantic segmentation. boundary information USED-FOR localization. boundary information USED-FOR object reconstruction. Few - shot semantic edge detection USED-FOR recovery of accurate boundaries. small - scale FEATURE-OF semantic segmentation module. semantic segmentation module USED-FOR CAFENet. predicted segmentation mask USED-FOR attention map. multi - split matching USED-FOR regularization method. meta - training USED-FOR metric - learning problem. highdimensional vectors USED-FOR metric - learning problem. FSE-1000 CONJUNCTION SBD-5. SBD-5 CONJUNCTION FSE-1000. them EVALUATE-FOR CAFENet. FSE-1000 HYPONYM-OF datasets. SBD-5 HYPONYM-OF datasets. CAFENet COMPARE baseline methods. baseline methods COMPARE CAFENet. fine - tuning or few - shot segmentation USED-FOR CAFENet. fine - tuning or few - shot segmentation USED-FOR baseline methods. Method are meta - learning strategy, and decoder module. OtherScientificTerm are lack of semantic information, and low - dimensional sub - vectors. ","This paper tackles the few-shot semantic edge detection, which is a well-studied problem in the context of image generation and medical imaging, where the goal is to recover the boundaries of novel categories. The paper proposes a meta-learning strategy to address the lack of semantic information in the training data and the recovery of accurate boundaries.    The key idea of the paper is to use meta-training to tackle the metric-learning problem with highdimensional vectors, where low-dimensional sub-vows are used to learn a decoder module, and then use the predicted segmentation mask as an attention map to learn the attention map of the semantic segmentation module on a small-scale.  Few-shot semantically edge detection is an important problem in this setting, as the boundary information is important for object reconstruction, image generation, and localization.  CAFENet uses a simple yet effective meta-learner that learns the semantic segments of the training images, and uses the predicted segments as a regularization method based on multi-split matching.  Experiments are conducted on two datasets, FSE-1000 and SBD-5, and CAFenet is shown to outperform other baseline methods that use fine-tuning or few -shot segmentation. ","This paper tackles the few-shot semantic edge detection, which is a well-studied problem in the context of image generation and medical imaging, where the goal is to recover the boundaries of novel categories. The paper proposes a meta-learning strategy to address the lack of semantic information in the training data and the recovery of accurate boundaries.    The key idea of the paper is to use meta-training to tackle the metric-learning problem with highdimensional vectors, where low-dimensional sub-vows are used to learn a decoder module, and then use the predicted segmentation mask as an attention map to learn the attention map of the semantic segmentation module on a small-scale.  Few-shot semantically edge detection is an important problem in this setting, as the boundary information is important for object reconstruction, image generation, and localization.  CAFENet uses a simple yet effective meta-learner that learns the semantic segments of the training images, and uses the predicted segments as a regularization method based on multi-split matching.  Experiments are conducted on two datasets, FSE-1000 and SBD-5, and CAFenet is shown to outperform other baseline methods that use fine-tuning or few -shot segmentation. "
540,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"explainability EVALUATE-FOR GNN. feature attribution USED-FOR explanation generation. causal interpretability FEATURE-OF GNNs. It USED-FOR graph feature. causal attribution FEATURE-OF graph feature. edge HYPONYM-OF graph feature. Causal Screening USED-FOR GNN model. Causal Screening USED-FOR model - agnostic tool. Causal Screening COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE Causal Screening. graph classification datasets EVALUATE-FOR Causal Screening. predictive accuracy CONJUNCTION contrastivity. contrastivity CONJUNCTION predictive accuracy. predictive accuracy HYPONYM-OF quantitative metrics. contrastivity HYPONYM-OF quantitative metrics. Method is graph neural networks ( GNNs ). OtherScientificTerm are graph features, features, cause - effect, and sanity checks. Metric is statistical interpretability. Generic is method. ","This paper studies the problem of causal interpretability of graph neural networks (GNNs). The authors consider the question of how well does the explainability of a GNN depend on feature attribution for explanation generation. The authors propose Causal Screening, a model-agnostic tool that can be applied to any GNN model. It measures the causal attribution of a graph feature (e.g. edge, node, edge, edge) to a given graph feature, which is a measure of statistical interpretability.  The authors show that the proposed method can be used to train a model agnostic GNN that is able to interpret graph features in a causal way. The paper also shows that the features are interpretable if the cause-effect between two features is known.  Experiments are conducted on several graph classification datasets, and the authors compare the performance of the proposed Cause Screening with state-of-the-art approaches. They show that their method performs better in terms of predictive accuracy, contrastivity, and other quantitative metrics. They also show that they can perform sanity checks to make sure that the method is interpretable. ","This paper studies the problem of causal interpretability of graph neural networks (GNNs). The authors consider the question of how well does the explainability of a GNN depend on feature attribution for explanation generation. The authors propose Causal Screening, a model-agnostic tool that can be applied to any GNN model. It measures the causal attribution of a graph feature (e.g. edge, node, edge, edge) to a given graph feature, which is a measure of statistical interpretability.  The authors show that the proposed method can be used to train a model agnostic GNN that is able to interpret graph features in a causal way. The paper also shows that the features are interpretable if the cause-effect between two features is known.  Experiments are conducted on several graph classification datasets, and the authors compare the performance of the proposed Cause Screening with state-of-the-art approaches. They show that their method performs better in terms of predictive accuracy, contrastivity, and other quantitative metrics. They also show that they can perform sanity checks to make sure that the method is interpretable. "
549,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"parameter norms HYPONYM-OF simplicity. measure EVALUATE-FOR network ’s simplicity. measure USED-FOR model. convolutional networks USED-FOR model. CIFAR-10 USED-FOR convolutional networks. flatness of minima CONJUNCTION optimization speed. optimization speed CONJUNCTION flatness of minima. models ’ margin CONJUNCTION flatness of minima. flatness of minima CONJUNCTION models ’ margin. mutual information EVALUATE-FOR measures. mutual information EVALUATE-FOR measure. measure COMPARE measures. measures COMPARE measure. flatness of minima USED-FOR measures. optimization speed USED-FOR measures. models ’ margin USED-FOR measures. measure COMPARE flatness - based measures. flatness - based measures COMPARE measure. Method is over - parameterized neural networks. Generic is they. Task are machine learning, and pruning. OtherScientificTerm are Occam ’s razor, and network ’s parameters. Metric is training loss. ","This paper proposes a new measure for measuring the network’s simplicity (parameter norms) in the context of over-parameterized neural networks. The proposed measure is based on the observation that a model trained on convolutional networks trained on CIFAR-10 with the same number of parameters as the training loss can be asymptotically similar to that trained on the same amount of training data as the original model. The authors argue that this is due to the fact that in machine learning, there is a trade-off between the number of training samples and the size of the model.    The authors show that the proposed measure, which they call Occam's razor, is a measure of the simplicity of a model. They show that this measure is more robust to changes in training loss and training speed than existing measures based on models’ margin, flatness of minima, and optimization speed. They also show that their measure has better mutual information with respect to other measures that rely on the mutual information between the model's parameters and the network's parameters.  The paper also shows that the measure can be more robust than other flatness-based measures, and that pruning is more effective at reducing the size and complexity of the network. ","This paper proposes a new measure for measuring the network’s simplicity (parameter norms) in the context of over-parameterized neural networks. The proposed measure is based on the observation that a model trained on convolutional networks trained on CIFAR-10 with the same number of parameters as the training loss can be asymptotically similar to that trained on the same amount of training data as the original model. The authors argue that this is due to the fact that in machine learning, there is a trade-off between the number of training samples and the size of the model.    The authors show that the proposed measure, which they call Occam's razor, is a measure of the simplicity of a model. They show that this measure is more robust to changes in training loss and training speed than existing measures based on models’ margin, flatness of minima, and optimization speed. They also show that their measure has better mutual information with respect to other measures that rely on the mutual information between the model's parameters and the network's parameters.  The paper also shows that the measure can be more robust than other flatness-based measures, and that pruning is more effective at reducing the size and complexity of the network. "
558,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,Discrete representations USED-FOR temporally - extended tasks. expert specifications USED-FOR they. deep reinforcement learning USED-FOR long - horizon tasks. exploratory video data USED-FOR temporally - abstracted discrete representations. mutual information maximization objective USED-FOR temporally - abstracted discrete representations. abstract states USED-FOR low - level model - predictive controller. DORP USED-FOR low - level model - predictive controller. DORP USED-FOR abstract states. DORP USED-FOR long - horizon tasks. it USED-FOR binary properties. key - and - door HYPONYM-OF binary properties. ,"Discrete representations for temporally-extended tasks can be learned from expert specifications, and they can be used in deep reinforcement learning for long-horizon tasks. In this paper, the authors propose a novel mutual information maximization objective to learn temporally - abstracted discrete representations from exploratory video data. The authors show that DORP can learn abstract states that are useful for a low-level model-predictive controller, and that it can capture binary properties such as key-and-door. ","Discrete representations for temporally-extended tasks can be learned from expert specifications, and they can be used in deep reinforcement learning for long-horizon tasks. In this paper, the authors propose a novel mutual information maximization objective to learn temporally - abstracted discrete representations from exploratory video data. The authors show that DORP can learn abstract states that are useful for a low-level model-predictive controller, and that it can capture binary properties such as key-and-door. "
567,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"performance CONJUNCTION compression rate. compression rate CONJUNCTION performance. Mixed - precision quantization USED-FOR deep neural networks. compression rate EVALUATE-FOR deep neural networks. performance EVALUATE-FOR deep neural networks. compression rate EVALUATE-FOR Mixed - precision quantization. neural architecture search USED-FOR vast search space. manuallydesigned search space USED-FOR methods. neural architecture search USED-FOR methods. bit - level sparsity quantization ( BSQ ) USED-FOR mixed - precision quantization. bit - level sparsity quantization ( BSQ ) USED-FOR inducing bit - level sparsity. BSQ USED-FOR dynamic precision reduction. BSQ USED-FOR mixed - precision quantization scheme. BSQ USED-FOR all - zero bits. mixed - precision quantization scheme USED-FOR model. hyperparameter USED-FOR gradient - based optimization process. gradient - based optimization process USED-FOR method. BSQ COMPARE methods. methods COMPARE BSQ. accuracy CONJUNCTION bit reduction. bit reduction CONJUNCTION accuracy. model architectures EVALUATE-FOR BSQ. CIFAR-10 and ImageNet datasets EVALUATE-FOR model architectures. bit reduction EVALUATE-FOR BSQ. accuracy EVALUATE-FOR BSQ. CIFAR-10 and ImageNet datasets EVALUATE-FOR BSQ. Generic are it, and approaches. OtherScientificTerm are quantization scheme, optimal quantization scheme, independent trainable variable, and weight elements. Method is differentiable bit - sparsity regularizer. Metric is compression. ","Mixed-precision quantization improves the performance and compression rate of deep neural networks by inducing bit-level sparsity in the quantization scheme, but it is computationally expensive because of the need to search a vast search space, which can be prohibitively expensive due to neural architecture search in the vast space. This paper proposes to use bit-wise sparsity quantization (BSQ) to improve the compression rate and accuracy of the optimal quantization.    The authors propose to use BSQ to perform dynamic precision reduction, which is a differentiable bit-sparsity regularizer. The authors show that BSQ can be used to improve a mixup of the standard mixed-proprioception quantization and the standard mixup for all-zero bits, and that it can be applied to a wide range of different approaches.  The main contribution of the paper is the introduction of BSQ, which extends the work of [1] and [2] to the case of mixed-primal quantization, and the authors show empirically that the proposed method is more computationally efficient by using a gradient-based optimization process with a single hyperparameter, and it is more efficient than existing methods that use neural architectures search in a manuallydesigned search space.  Experiments on CIFAR-10 and ImageNet datasets with different model architectures and model architectures show that the BSQ outperforms existing methods in terms of accuracy, bit reduction, and compression.  In addition, the authors also show that their method can be combined with any existing mixed-predictive quantization method to obtain a more efficient method.","Mixed-precision quantization improves the performance and compression rate of deep neural networks by inducing bit-level sparsity in the quantization scheme, but it is computationally expensive because of the need to search a vast search space, which can be prohibitively expensive due to neural architecture search in the vast space. This paper proposes to use bit-wise sparsity quantization (BSQ) to improve the compression rate and accuracy of the optimal quantization.    The authors propose to use BSQ to perform dynamic precision reduction, which is a differentiable bit-sparsity regularizer. The authors show that BSQ can be used to improve a mixup of the standard mixed-proprioception quantization and the standard mixup for all-zero bits, and that it can be applied to a wide range of different approaches.  The main contribution of the paper is the introduction of BSQ, which extends the work of [1] and [2] to the case of mixed-primal quantization, and the authors show empirically that the proposed method is more computationally efficient by using a gradient-based optimization process with a single hyperparameter, and it is more efficient than existing methods that use neural architectures search in a manuallydesigned search space.  Experiments on CIFAR-10 and ImageNet datasets with different model architectures and model architectures show that the BSQ outperforms existing methods in terms of accuracy, bit reduction, and compression.  In addition, the authors also show that their method can be combined with any existing mixed-predictive quantization method to obtain a more efficient method."
576,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,memory consumption CONJUNCTION faster computation. faster computation CONJUNCTION memory consumption. bitwise operations FEATURE-OF quantized networks. bitwise operations USED-FOR faster computation. generalization capabilities EVALUATE-FOR they. quantized networks USED-FOR gradient based adversarial attacks. robustness FEATURE-OF quantized networks. robustness FEATURE-OF quantized models. gradient vanishing issues FEATURE-OF quantized models. temperature scaling approach USED-FOR decision boundary. forward - backward signal propagation PART-OF network. forward - backward signal propagation USED-FOR gradient vanishing. adversarially trained models CONJUNCTION floating - point networks. floating - point networks CONJUNCTION adversarially trained models. temperature scaled attacks COMPARE attacks. attacks COMPARE temperature scaled attacks. CIFAR-10/100 datasets CONJUNCTION multiple network architectures. multiple network architectures CONJUNCTION CIFAR-10/100 datasets. near - perfect success rate EVALUATE-FOR quantized networks. quantized networks EVALUATE-FOR temperature scaled attacks. adversarially trained models EVALUATE-FOR attacks. floating - point networks EVALUATE-FOR attacks. near - perfect success rate EVALUATE-FOR temperature scaled attacks. Method is Neural network quantization. ,"This paper studies the gradient based adversarial attacks on quantized neural networks. The authors propose a temperature scaling approach to scale the decision boundary of quantized networks to reduce memory consumption and faster computation through bitwise operations. They show that gradient vanishing happens due to the forward-backward signal propagation in the network, and that quantized models suffer from gradient vanishing issues. Neural network quantization has been shown to be effective in reducing the gradient vanishing issue, and they also show that they improve the generalization capabilities of the network. They also show empirically that temperature scaled attacks have a near-perfect success rate compared to other attacks that do not scale. They evaluate attacks on adversarially trained models and floating-point networks on CIFAR-10/100 datasets and multiple network architectures. ","This paper studies the gradient based adversarial attacks on quantized neural networks. The authors propose a temperature scaling approach to scale the decision boundary of quantized networks to reduce memory consumption and faster computation through bitwise operations. They show that gradient vanishing happens due to the forward-backward signal propagation in the network, and that quantized models suffer from gradient vanishing issues. Neural network quantization has been shown to be effective in reducing the gradient vanishing issue, and they also show that they improve the generalization capabilities of the network. They also show empirically that temperature scaled attacks have a near-perfect success rate compared to other attacks that do not scale. They evaluate attacks on adversarially trained models and floating-point networks on CIFAR-10/100 datasets and multiple network architectures. "
585,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,prototype trajectories PART-OF interpretable recurrent neural network ( RNN ) model. ProtoryNet HYPONYM-OF interpretable recurrent neural network ( RNN ) model. prototype theory USED-FOR ProtoryNet. prototype USED-FOR ProtoryNet. RNN backbone USED-FOR temporal pattern. RNN backbone USED-FOR prototypes. temporal pattern FEATURE-OF prototypes. method COMPARE prototype - based method. prototype - based method COMPARE method. ProtoryNet COMPARE prototype - based methods. prototype - based methods COMPARE ProtoryNet. Material is text sequence. Generic is model. ,"This paper introduces ProtoryNet, an interpretable recurrent neural network (RNN) model that incorporates prototype trajectories into the training process. The authors draw inspiration from prototype theory and propose a novel RNN architecture that can be used to learn a sequence of prototypes from a text sequence. The prototypes are learned using an RNN backbone that predicts the temporal pattern of the prototypes. The proposed method is evaluated on a number of tasks and compared to a previous prototype-based method.  ","This paper introduces ProtoryNet, an interpretable recurrent neural network (RNN) model that incorporates prototype trajectories into the training process. The authors draw inspiration from prototype theory and propose a novel RNN architecture that can be used to learn a sequence of prototypes from a text sequence. The prototypes are learned using an RNN backbone that predicts the temporal pattern of the prototypes. The proposed method is evaluated on a number of tasks and compared to a previous prototype-based method.  "
594,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,Hidden Markov models ( HMMs ) USED-FOR disease progression modeling. patient covariates USED-FOR estimation. local optima FEATURE-OF HMMs. HMRNN COMPARE discrete - observation HMM. discrete - observation HMM COMPARE HMRNN. likelihood function FEATURE-OF HMRNN. hidden Markov recurrent neural networks ( HMRNNs ) HYPONYM-OF recurrent neural networks ( RNNs ). HMRNN CONJUNCTION predictive neural networks. predictive neural networks CONJUNCTION HMRNN. patient covariate information USED-FOR predictive neural networks. Baum - Welch algorithm USED-FOR HMRNN parameter estimates. HMRNN USED-FOR parameter estimation. HMRNN CONJUNCTION neural networks. neural networks CONJUNCTION HMRNN. neural networks USED-FOR parameter estimation. Alzheimer ’s disease dataset EVALUATE-FOR HMRNN. HMRNN ’s solution COMPARE HMM. HMM COMPARE HMRNN ’s solution. HMRNN ’s solution USED-FOR clinical interpretation. HMRNN ’s solution USED-FOR disease forecasting. HMM USED-FOR clinical interpretation. OtherScientificTerm is patient health state. ,"This paper studies the problem of disease progression modeling using Hidden Markov models (HMMs). The authors propose a variant of recurrent neural networks (RNNs) called hidden Markov recurrent neural nets (HMRNNs). They show that the local optima of HMMs can be approximated by a HMRNN with the same likelihood function as a discrete-observation HMM. They also show that HMMs with patient covariates are more suitable for estimation with respect to the patient health state. The authors also propose to use the Baum-Welch algorithm to improve the performance of the H MRNN parameter estimates. Finally, the authors show that a combination of the theoretical results and empirical results on the Alzheimer’s disease dataset show that HMRnn and predictive neural networks trained with the patient covariate information are more robust to the use of patient health information.   The main contribution of this paper is that the authors propose to combine the theoretical findings of the authors of the previous work on HMM with the HMM to improve parameter estimation performance of HMRN and neural networks for parameter estimation. The paper also shows that the proposed solution is more robust than HMM for disease forecasting and can be used for clinical interpretation. ","This paper studies the problem of disease progression modeling using Hidden Markov models (HMMs). The authors propose a variant of recurrent neural networks (RNNs) called hidden Markov recurrent neural nets (HMRNNs). They show that the local optima of HMMs can be approximated by a HMRNN with the same likelihood function as a discrete-observation HMM. They also show that HMMs with patient covariates are more suitable for estimation with respect to the patient health state. The authors also propose to use the Baum-Welch algorithm to improve the performance of the H MRNN parameter estimates. Finally, the authors show that a combination of the theoretical results and empirical results on the Alzheimer’s disease dataset show that HMRnn and predictive neural networks trained with the patient covariate information are more robust to the use of patient health information.   The main contribution of this paper is that the authors propose to combine the theoretical findings of the authors of the previous work on HMM with the HMM to improve parameter estimation performance of HMRN and neural networks for parameter estimation. The paper also shows that the proposed solution is more robust than HMM for disease forecasting and can be used for clinical interpretation. "
603,SP:6355337707f1dd373813290e26e9c0a264b993f9,"phenotypes FEATURE-OF structural and functional properties of neuronal types. supervised learning approach USED-FOR gene expression data. components USED-FOR phenotypic characteristics. phenotypic feature CONJUNCTION feature combination. feature combination CONJUNCTION phenotypic feature. sparsity - based regularization algorithm USED-FOR feature combination. sparsity - based regularization algorithm USED-FOR phenotypic feature. sparsity - based regularization algorithm USED-FOR approach. dendritic and axonal phenotypes FEATURE-OF single - cell RNA - Seq dataset. Drosophila T4 / T5 neurons FEATURE-OF single - cell RNA - Seq dataset. single - cell RNA - Seq dataset EVALUATE-FOR approach. analysis EVALUATE-FOR methods. Task are neurobiology, and linear transformation of gene expressions. Material is Single - cell RNA sequencing. OtherScientificTerm are neuronal phenotypes, phenotypic factor, and genetic organization. Generic is method. Method is factorized linear discriminant analysis ( FLDA ). ","This paper proposes a supervised learning approach for gene expression data in neurobiology. Single-cell RNA sequencing is an important tool for studying neuronal phenotypes. The authors propose a method to learn the structural and functional properties of neuronal types in terms of their phenotypes, which can be interpreted as a linear transformation of gene expressions. The proposed method is based on factorized linear discriminant analysis (FLDA). The authors use two components to learn phenotypic characteristics: (1) the phenotyping factor, and (2) the genetic organization. The approach uses a sparsity-based regularization algorithm to ensure that the learned feature combination of phenotyped feature and the feature combination from the two components is close to each other.  The proposed approach is evaluated on a single-cell RN-Seq dataset of dendritic and axonal phenotypes of Drosophila T4/T5 neurons. The results show that the proposed methods outperform the existing methods in the analysis.   ","This paper proposes a supervised learning approach for gene expression data in neurobiology. Single-cell RNA sequencing is an important tool for studying neuronal phenotypes. The authors propose a method to learn the structural and functional properties of neuronal types in terms of their phenotypes, which can be interpreted as a linear transformation of gene expressions. The proposed method is based on factorized linear discriminant analysis (FLDA). The authors use two components to learn phenotypic characteristics: (1) the phenotyping factor, and (2) the genetic organization. The approach uses a sparsity-based regularization algorithm to ensure that the learned feature combination of phenotyped feature and the feature combination from the two components is close to each other.  The proposed approach is evaluated on a single-cell RN-Seq dataset of dendritic and axonal phenotypes of Drosophila T4/T5 neurons. The results show that the proposed methods outperform the existing methods in the analysis.   "
612,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"Saliency maps USED-FOR image classifier. interpretability method USED-FOR posterior distribution. posterior distribution FEATURE-OF saliency map. random variable FEATURE-OF saliency map. image USED-FOR classifier ’s predictive probability. approximate posterior USED-FOR classifier. OtherScientificTerm are likelihood function, prior distribution, positive correlation, and auxiliary information. Method is variational approximation. Generic is It. ","Saliency maps are used to train an image classifier. In this paper, the authors propose an interpretability method to learn the posterior distribution of the saliency map over a random variable, which is a likelihood function. The prior distribution is defined as the prior distribution of a prior distribution over a set of samples. The authors propose a variational approximation to this prior distribution. It is based on the observation that positive correlation between the prior and the posterior of the prior is not always the same, and that the classifier’s predictive probability on an image can be approximated by a classifier trained on an approximate posterior. The paper also proposes to use auxiliary information, which can be used to learn a saliency maps.   ","Saliency maps are used to train an image classifier. In this paper, the authors propose an interpretability method to learn the posterior distribution of the saliency map over a random variable, which is a likelihood function. The prior distribution is defined as the prior distribution of a prior distribution over a set of samples. The authors propose a variational approximation to this prior distribution. It is based on the observation that positive correlation between the prior and the posterior of the prior is not always the same, and that the classifier’s predictive probability on an image can be approximated by a classifier trained on an approximate posterior. The paper also proposes to use auxiliary information, which can be used to learn a saliency maps.   "
621,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"Pretrained text encoders USED-FOR natural language processing ( NLP ) tasks. BERT HYPONYM-OF Pretrained text encoders. social bias FEATURE-OF pretrained NLP models. sentence - level fairness EVALUATE-FOR pretrained encoders. neural debiasing method USED-FOR pretrained sentence encoder. fair filter ( FairFil ) network USED-FOR debiased representations. filtered embeddings CONJUNCTION bias words. bias words CONJUNCTION filtered embeddings. contrastive learning framework USED-FOR FairFil. real - world datasets EVALUATE-FOR FairFil. FairFil USED-FOR bias degree. FairFil USED-FOR pretrained text encoders. bias degree FEATURE-OF pretrained text encoders. FairFil USED-FOR downstream tasks. Task is word - level debiasing. OtherScientificTerm are pretrained encoder outputs, and rich semantic information. Method are post hoc method, and text encoders. ","This paper studies word-level debiasing in the context of natural language processing (NLP) tasks. Pretrained text encoders (e.g., BERT) have been shown to suffer from social bias in pretrained NLP models. This paper proposes a neural debiased method to debiase a pretrained sentence encoder to achieve sentence-level fairness. The authors propose a fair filter (FairFil) network to learn debiaded representations. FairFil is based on the contrastive learning framework, where the pretrained encoder outputs are denoised to remove rich semantic information. The paper shows that FairFil outperforms the state-of-the-art post hoc method on two real-world datasets. In addition, FairFil can be applied to further reduce the bias degree of pretrained text embeddings and bias words, and is shown to be effective on downstream tasks. ","This paper studies word-level debiasing in the context of natural language processing (NLP) tasks. Pretrained text encoders (e.g., BERT) have been shown to suffer from social bias in pretrained NLP models. This paper proposes a neural debiased method to debiase a pretrained sentence encoder to achieve sentence-level fairness. The authors propose a fair filter (FairFil) network to learn debiaded representations. FairFil is based on the contrastive learning framework, where the pretrained encoder outputs are denoised to remove rich semantic information. The paper shows that FairFil outperforms the state-of-the-art post hoc method on two real-world datasets. In addition, FairFil can be applied to further reduce the bias degree of pretrained text embeddings and bias words, and is shown to be effective on downstream tasks. "
630,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"certified models USED-FOR machine learning security. certified models USED-FOR adversarial perturbations. randomized smoothing USED-FOR models. l2 perturbations FEATURE-OF models. test accuracy CONJUNCTION average certified robust radius. average certified robust radius CONJUNCTION test accuracy. sample - wise randomized smoothing USED-FOR noise levels. sample - wise randomized smoothing USED-FOR defense. robust regions USED-FOR certification. accuracy - robustness trade - off FEATURE-OF transductive setting. transductive setting EVALUATE-FOR method. accuracy - robustness trade - off EVALUATE-FOR method. Task is certifying l2 perturbations. OtherScientificTerm are Gaussian noise, and noise level. Method is pretrain - to - finetune framework. Generic is model. Material is CIFAR-10 and MNIST datasets. ","This paper studies the problem of certifying models for machine learning security against adversarial perturbations using certified models. In particular, the authors focus on certifying l2 robustness against Gaussian noise. They propose a pretrain-to-finetune framework, where a model is pretrained and finetuneed on a pre-defined set of robust regions, and then the certified models are used to defend against l2 perturbation. The key idea is to use sample-wise randomized smoothing to balance the noise levels for different noise levels in the certified regions. The authors show that under certain assumptions on the noise level, certified models can be certified to be robust to l2 adversarial attacks. They also show that the certified robustness of a model can be improved by the use of the pretrained version of the model.    The authors also propose a defense based on sample-wide randomized smoothed, and show that this method improves the accuracy-robustness trade-off in the transductive setting, where the test accuracy and the average certified robust radius are the same, but the certification is performed in different robust regions. Experiments are conducted on CIFAR-10 and MNIST datasets. ","This paper studies the problem of certifying models for machine learning security against adversarial perturbations using certified models. In particular, the authors focus on certifying l2 robustness against Gaussian noise. They propose a pretrain-to-finetune framework, where a model is pretrained and finetuneed on a pre-defined set of robust regions, and then the certified models are used to defend against l2 perturbation. The key idea is to use sample-wise randomized smoothing to balance the noise levels for different noise levels in the certified regions. The authors show that under certain assumptions on the noise level, certified models can be certified to be robust to l2 adversarial attacks. They also show that the certified robustness of a model can be improved by the use of the pretrained version of the model.    The authors also propose a defense based on sample-wide randomized smoothed, and show that this method improves the accuracy-robustness trade-off in the transductive setting, where the test accuracy and the average certified robust radius are the same, but the certification is performed in different robust regions. Experiments are conducted on CIFAR-10 and MNIST datasets. "
639,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,probabilistic method USED-FOR unsupervised recovery of corrupted data. method USED-FOR posteriors of clean values. degraded samples USED-FOR method. variational methods USED-FOR collapsed densities. reduced entropy condition approximate inference method USED-FOR rich posteriors. imputation CONJUNCTION de - noising. de - noising CONJUNCTION imputation. missing values CONJUNCTION noise. noise CONJUNCTION missing values. variational methods USED-FOR de - noising. variational methods USED-FOR imputation. model COMPARE variational methods. variational methods COMPARE model. real data sets USED-FOR variational methods. real data sets USED-FOR de - noising. data recovery task EVALUATE-FOR model. propagating uncertainty USED-FOR downstream tasks. classification accuracy EVALUATE-FOR imputation. model USED-FOR downstream tasks. OtherScientificTerm is solution space. ,"This paper proposes a probabilistic method for unsupervised recovery of corrupted data. The proposed method learns the posteriors of clean values from degraded samples. The authors propose a reduced entropy condition approximate inference method to learn rich posteriors, which is a generalization of variational methods for collapsed densities. Experiments on real data sets show that the proposed model outperforms the state-of-the-art for imputation and de-noising on both missing values and noise. The model is also able to perform well on the data recovery task, and the authors show that propagating uncertainty for downstream tasks improves the classification accuracy.","This paper proposes a probabilistic method for unsupervised recovery of corrupted data. The proposed method learns the posteriors of clean values from degraded samples. The authors propose a reduced entropy condition approximate inference method to learn rich posteriors, which is a generalization of variational methods for collapsed densities. Experiments on real data sets show that the proposed model outperforms the state-of-the-art for imputation and de-noising on both missing values and noise. The model is also able to perform well on the data recovery task, and the authors show that propagating uncertainty for downstream tasks improves the classification accuracy."
648,SP:4b7d050f57507166992034e5e264cccab3cb874f,"Self - attention mechanism USED-FOR graph neural networks ( GNNs ). Self - attention mechanism USED-FOR graph representation learning task. multi - hop context information USED-FOR attention computation. long - range interactions PART-OF GNN. MAGNA USED-FOR attention. diffusion prior USED-FOR MAGNA. MAGNA USED-FOR large - scale structural information. MAGNA USED-FOR informative attention. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. Citeseer CONJUNCTION Pubmed. Pubmed CONJUNCTION Citeseer. node classification CONJUNCTION knowledge graph completion benchmarks. knowledge graph completion benchmarks CONJUNCTION node classification. knowledge graph completion benchmarks EVALUATE-FOR MAGNA. MAGNA COMPARE state - of - the - art. state - of - the - art COMPARE MAGNA. relative error reduction EVALUATE-FOR state - of - the - art. node classification EVALUATE-FOR MAGNA. MAGNA COMPARE MAGNA. MAGNA COMPARE MAGNA. knowledge graph completion benchmarks EVALUATE-FOR MAGNA. MAGNA COMPARE state - of - the - art. state - of - the - art COMPARE MAGNA. node classification EVALUATE-FOR MAGNA. Citeseer EVALUATE-FOR state - of - the - art. Pubmed EVALUATE-FOR state - of - the - art. Cora EVALUATE-FOR state - of - the - art. Cora EVALUATE-FOR MAGNA. relative error reduction EVALUATE-FOR MAGNA. relative error reduction EVALUATE-FOR MAGNA. large - scale Open Graph Benchmark dataset EVALUATE-FOR MAGNA. WN18RR CONJUNCTION FB15k237. FB15k237 CONJUNCTION WN18RR. Method is attention mechanism. OtherScientificTerm are nodes, network context, attention scores, and receptive field. Generic is network. Task is knowledge graph completion. Metric is performance metrics. ","This paper proposes a novel Self-attention mechanism for graph neural networks (GNNs) for the graph representation learning task. The proposed attention mechanism is based on the observation that multi-hop context information in GNNs can be used to improve the efficiency of the attention computation, and that long-range interactions between nodes in a GNN can be exploited. The authors propose to use a diffusion prior to guide the attention of the network, and propose MAGNA, a variant of the diffusion prior for the attention. They show that MAGNA is able to capture large-scale structural information in the network context, which can be useful for the task of knowledge graph completion. They also show that the proposed MAGNA can learn informative attention that can be applied to any node in the receptive field. They evaluate the performance of MAGNA on node classification, node classification with WN18RR and FB15k237 as well as on the standard Knowledge Graph completion benchmarks, showing relative error reduction compared to the state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also shows competitive performance on the large scale Open Graph Benchmark dataset. ","This paper proposes a novel Self-attention mechanism for graph neural networks (GNNs) for the graph representation learning task. The proposed attention mechanism is based on the observation that multi-hop context information in GNNs can be used to improve the efficiency of the attention computation, and that long-range interactions between nodes in a GNN can be exploited. The authors propose to use a diffusion prior to guide the attention of the network, and propose MAGNA, a variant of the diffusion prior for the attention. They show that MAGNA is able to capture large-scale structural information in the network context, which can be useful for the task of knowledge graph completion. They also show that the proposed MAGNA can learn informative attention that can be applied to any node in the receptive field. They evaluate the performance of MAGNA on node classification, node classification with WN18RR and FB15k237 as well as on the standard Knowledge Graph completion benchmarks, showing relative error reduction compared to the state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also shows competitive performance on the large scale Open Graph Benchmark dataset. "
657,SP:36310d761deb19e71c8a57de19b48f857707d48b,"test EVALUATE-FOR text model. multitask accuracy EVALUATE-FOR test. multitask accuracy EVALUATE-FOR text model. computer science CONJUNCTION law. law CONJUNCTION computer science. US history CONJUNCTION computer science. computer science CONJUNCTION US history. elementary mathematics CONJUNCTION US history. US history CONJUNCTION elementary mathematics. elementary mathematics CONJUNCTION computer science. computer science CONJUNCTION elementary mathematics. tasks PART-OF test. elementary mathematics HYPONYM-OF tasks. law HYPONYM-OF tasks. US history HYPONYM-OF tasks. computer science HYPONYM-OF tasks. world knowledge CONJUNCTION problem solving ability. problem solving ability CONJUNCTION world knowledge. world knowledge FEATURE-OF models. problem solving ability FEATURE-OF models. GPT-3 model COMPARE random chance. random chance COMPARE GPT-3 model. models COMPARE GPT-3 model. GPT-3 model COMPARE models. near random - chance accuracy EVALUATE-FOR models. tasks EVALUATE-FOR models. lopsided performance FEATURE-OF Models. morality CONJUNCTION law. law CONJUNCTION morality. nearrandom accuracy EVALUATE-FOR they. academic and professional understanding FEATURE-OF model. Metric are accuracy, and expert - level accuracy. ","This paper presents a new test for multitask accuracy of a text model. The test consists of three tasks: US history, computer science, and law. The paper shows that models with different levels of world knowledge and problem solving ability have lopsided performance on all three tasks. The GPT-3 model achieves near random-like accuracy on all tasks compared to random chance. Models with higher levels of expert-level accuracy are also shown to have better performance on these tasks, but they achieve near random accuracy only on law and morality. The authors also show that a model with high academic and professional understanding can achieve near-optimal performance on a number of tasks.","This paper presents a new test for multitask accuracy of a text model. The test consists of three tasks: US history, computer science, and law. The paper shows that models with different levels of world knowledge and problem solving ability have lopsided performance on all three tasks. The GPT-3 model achieves near random-like accuracy on all tasks compared to random chance. Models with higher levels of expert-level accuracy are also shown to have better performance on these tasks, but they achieve near random accuracy only on law and morality. The authors also show that a model with high academic and professional understanding can achieve near-optimal performance on a number of tasks."
666,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"pre - training approach USED-FOR table semantic parsing. GRAPPA HYPONYM-OF pre - training approach. synchronous context - free grammar ( SCFG ) USED-FOR synthetic question - SQL pairs. GRAPPA USED-FOR structural properties. structural properties PART-OF pre - training language model. structural properties PART-OF table semantic parsing. synthetic data USED-FOR GRAPPA. masked language modeling ( MLM ) USED-FOR pre - training process. model USED-FOR real - world data. table - and - language datasets USED-FOR masked language modeling ( MLM ). OtherScientificTerm are compositional inductive bias, and pre - trained embeddings. Method is pre - training strategy. ","This paper proposes a new pre-training approach for table semantic parsing, GRAPPA, which is based on synchronous context-free grammar (SCFG) to generate synthetic question-SQL pairs. The authors show that GRAPP can capture structural properties of the pre-trained language model, which are important for compositional inductive bias. They also show that the pretraining on synthetic data is sufficient to learn the structural properties in table semantic Parsing.   The authors also propose a new way of using masked language modeling (MLM) on table-and-language datasets to improve the performance of the pretrained model on real-world data. The paper also shows that the proposed pre-trainable embeddings are more robust to changes in the number of syntactic words in the question, and that the model is able to generalize better to unseen data.  The paper is well-written and well-motivated. However, the paper suffers from a lack of novelty, and the authors do not provide a clear explanation of the design choices of the model, or the motivation behind the proposed pretraining strategy. ","This paper proposes a new pre-training approach for table semantic parsing, GRAPPA, which is based on synchronous context-free grammar (SCFG) to generate synthetic question-SQL pairs. The authors show that GRAPP can capture structural properties of the pre-trained language model, which are important for compositional inductive bias. They also show that the pretraining on synthetic data is sufficient to learn the structural properties in table semantic Parsing.   The authors also propose a new way of using masked language modeling (MLM) on table-and-language datasets to improve the performance of the pretrained model on real-world data. The paper also shows that the proposed pre-trainable embeddings are more robust to changes in the number of syntactic words in the question, and that the model is able to generalize better to unseen data.  The paper is well-written and well-motivated. However, the paper suffers from a lack of novelty, and the authors do not provide a clear explanation of the design choices of the model, or the motivation behind the proposed pretraining strategy. "
675,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"random matrix analysis USED-FOR Gaussian mixture data model. random matrix analysis USED-FOR MTL LS - SVM. small - dimensional ) statistics FEATURE-OF deterministic limit. single - task LS - SVMs COMPARE MTL approach. MTL approach COMPARE single - task LS - SVMs. sufficient statistics USED-FOR method. MTL LS - SVM method COMPARE multi - task and transfer learning techniques. multi - task and transfer learning techniques COMPARE MTL LS - SVM method. Method are multi - task and transfer learning methods, MTL LS - SVM algorithm, and cross - validation procedure. OtherScientificTerm is hyperparameters. ","This paper proposes a new multi-task and transfer learning method called MTL LS-SVM based on random matrix analysis for a Gaussian mixture data model. The authors show that the deterministic limit of (small-dimensional) statistics of the MTL-LSVM algorithm can be derived from the (random matrix analysis) results of the cross-validation procedure. They show that under sufficient statistics, the proposed method can achieve better performance than single-task LS-VMs, and outperform the standard MTL approach when the number of tasks is small and the hyperparameters are well-calibrated. They also show that their proposed method is more robust to hyperparameter shifts than the standard LS-VIMs. Finally, they compare the performance of the proposed MTL LSTM with the state-of-the-art MTL SVM method, and show that it outperforms the state of the art in terms of performance.   ","This paper proposes a new multi-task and transfer learning method called MTL LS-SVM based on random matrix analysis for a Gaussian mixture data model. The authors show that the deterministic limit of (small-dimensional) statistics of the MTL-LSVM algorithm can be derived from the (random matrix analysis) results of the cross-validation procedure. They show that under sufficient statistics, the proposed method can achieve better performance than single-task LS-VMs, and outperform the standard MTL approach when the number of tasks is small and the hyperparameters are well-calibrated. They also show that their proposed method is more robust to hyperparameter shifts than the standard LS-VIMs. Finally, they compare the performance of the proposed MTL LSTM with the state-of-the-art MTL SVM method, and show that it outperforms the state of the art in terms of performance.   "
684,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,group equivariant conditional neural process ( EquivCNP ) HYPONYM-OF metalearning method. permutation invariance FEATURE-OF data set. data space FEATURE-OF transformation equivariance. permutation invariance FEATURE-OF metalearning method. transformation equivariance FEATURE-OF it. rotation and scaling equivariance HYPONYM-OF group equivariance. EquivCNPs USED-FOR group symmetries. decomposition theorem USED-FOR permutation - invariant and group - equivariant maps. infinite - dimensional latent space USED-FOR group symmetries. decomposition theorem USED-FOR EquivCNPs. infinite - dimensional latent space USED-FOR EquivCNPs. Lie group convolutional layers USED-FOR architecture. EquivCNP COMPARE CNPs. CNPs COMPARE EquivCNP. translation equivariance FEATURE-OF EquivCNP. 1D regression task EVALUATE-FOR EquivCNP. 1D regression task EVALUATE-FOR CNPs. EquivCNP USED-FOR zero - shot generalization. zero - shot generalization USED-FOR image - completion task. EquivCNP USED-FOR image - completion task. Lie group equivariance PART-OF EquivCNP. Lie group equivariance USED-FOR EquivCNP. Method is conditional neural processes ( CNPs ). OtherScientificTerm is symmetry of real - world data. ,"This paper proposes a new metalearning method, called group equivariant conditional neural process (EquivCNP), which is an extension of existing conditional neural processes (CNPs). The authors show that it can be seen as an extension to the permutation invariance of the data set, which is a well-studied property of metalearnings method. The authors also show that EquivCNPs are group-equivariant under certain group symmetries (e.g., rotation and scaling equivariance), and that it is also transformation equivarient under certain transformations in the data space.  The authors prove a decomposition theorem for permutation-invariant and group-Equivariant maps in an infinite-dimensional latent space, and show that the decomposition is satisfied for EquivCPs. They then propose a new architecture based on Lie group convolutional layers, which they call Lie group CNP, and demonstrate that it preserves the symmetry of real-world data.  Experiments are conducted on a 1D regression task, and on an image-completion task, where they show that they outperform CNPs in terms of zero-shot generalization. They also show how EquivCCNP can be used to improve the performance of existing CNPs.  Finally, the authors show how to use the Lie group Equivariance of EquivNP to improve performance of the original CNPs, and how to apply EquivNCPs in a more general way.   ","This paper proposes a new metalearning method, called group equivariant conditional neural process (EquivCNP), which is an extension of existing conditional neural processes (CNPs). The authors show that it can be seen as an extension to the permutation invariance of the data set, which is a well-studied property of metalearnings method. The authors also show that EquivCNPs are group-equivariant under certain group symmetries (e.g., rotation and scaling equivariance), and that it is also transformation equivarient under certain transformations in the data space.  The authors prove a decomposition theorem for permutation-invariant and group-Equivariant maps in an infinite-dimensional latent space, and show that the decomposition is satisfied for EquivCPs. They then propose a new architecture based on Lie group convolutional layers, which they call Lie group CNP, and demonstrate that it preserves the symmetry of real-world data.  Experiments are conducted on a 1D regression task, and on an image-completion task, where they show that they outperform CNPs in terms of zero-shot generalization. They also show how EquivCCNP can be used to improve the performance of existing CNPs.  Finally, the authors show how to use the Lie group Equivariance of EquivNP to improve performance of the original CNPs, and how to apply EquivNCPs in a more general way.   "
693,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"autoregressive models CONJUNCTION maximum likelihood estimation. maximum likelihood estimation CONJUNCTION autoregressive models. approaches USED-FOR text generation. autoregressive models USED-FOR approaches. maximum likelihood estimation USED-FOR approaches. mismatched history distributions FEATURE-OF exposure bias. expert demonstrations USED-FOR offline reinforcement learning ( RL ) problem. offline reinforcement learning ( RL ) problem USED-FOR text generation. demonstrations USED-FOR easy - to - optimize algorithm. importance weighting USED-FOR easy - to - optimize algorithm. optimization issues FEATURE-OF prior RL approaches. online data collection USED-FOR prior RL approaches. MLE CONJUNCTION policy gradient. policy gradient CONJUNCTION MLE. summarization CONJUNCTION question generation. question generation CONJUNCTION summarization. question generation CONJUNCTION machine translation. machine translation CONJUNCTION question generation. models COMPARE those. those COMPARE models. automatic and human evaluation EVALUATE-FOR models. policy gradient USED-FOR question generation. MLE USED-FOR question generation. GOLD USED-FOR those. policy gradient USED-FOR summarization. policy gradient USED-FOR machine translation. summarization EVALUATE-FOR those. summarization EVALUATE-FOR models. MLE USED-FOR models. MLE USED-FOR those. policy gradient USED-FOR those. GOLD USED-FOR models. models USED-FOR exposure bias. decoding algorithms USED-FOR models. Generic is paradigm. OtherScientificTerm are mismatched learning objective, and model - generated histories. ","This paper studies the problem of offline reinforcement learning (RL) with expert demonstrations, where the goal is to learn from expert demonstrations in an offline RL setting. The paper proposes a new paradigm, called “exposure bias”, which aims to address the issue of mismatched history distributions in the offline RL problem. The authors propose two approaches for text generation based on autoregressive models and maximum likelihood estimation. They propose an easy-to-optimize algorithm based on expert demonstrations and importance weighting. They show that prior RL approaches with online data collection suffer from optimization issues due to the mismatched learning objective. They also show that models trained with MLE and policy gradient outperform those trained with GOLD in summarization, question generation, and machine translation. Finally, they show that these models are able to mitigate the exposure bias by decoding algorithms that do not rely on model-generated histories. The models are evaluated on both automatic and human evaluation, and those trained using MLE or policy gradient are shown to outperform models trained using standard decoding algorithms.","This paper studies the problem of offline reinforcement learning (RL) with expert demonstrations, where the goal is to learn from expert demonstrations in an offline RL setting. The paper proposes a new paradigm, called “exposure bias”, which aims to address the issue of mismatched history distributions in the offline RL problem. The authors propose two approaches for text generation based on autoregressive models and maximum likelihood estimation. They propose an easy-to-optimize algorithm based on expert demonstrations and importance weighting. They show that prior RL approaches with online data collection suffer from optimization issues due to the mismatched learning objective. They also show that models trained with MLE and policy gradient outperform those trained with GOLD in summarization, question generation, and machine translation. Finally, they show that these models are able to mitigate the exposure bias by decoding algorithms that do not rely on model-generated histories. The models are evaluated on both automatic and human evaluation, and those trained using MLE or policy gradient are shown to outperform models trained using standard decoding algorithms."
702,SP:e77eca51db362909681965092186af2e502aaedc,"intermediate activations USED-FOR back - propagation. gradient - isolated modules PART-OF network. local supervision USED-FOR network. early layers FEATURE-OF task - relevant information. E2E loss FEATURE-OF local modules. information propagation ( InfoPro ) loss USED-FOR local modules. reconstruction loss CONJUNCTION normal cross - entropy / contrastive term. normal cross - entropy / contrastive term CONJUNCTION reconstruction loss. ImageNet CONJUNCTION Cityscapes. Cityscapes CONJUNCTION ImageNet. STL-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL-10. SVHN CONJUNCTION STL-10. STL-10 CONJUNCTION SVHN. InfoPro COMPARE E2E training. E2E training COMPARE InfoPro. CIFAR CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR. CIFAR CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR. datasets EVALUATE-FOR InfoPro. memory footprint EVALUATE-FOR E2E training. Cityscapes HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR HYPONYM-OF datasets. STL-10 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. method USED-FOR training acceleration. local modules USED-FOR training acceleration. method USED-FOR local modules. Method are deep networks, and locally supervised learning. Metric is GPUs memory footprint. Generic are model, and algorithm. OtherScientificTerm are useful information, task - irrelevant information, InfoPro loss, surrogate optimization objective, and GPU memory constraint. ","This paper studies the problem of training deep networks with GPUs memory footprint. The authors propose a new model called InfoPro, which learns gradient-separated modules in a network with local supervision. The local modules are trained with the information propagation (InfoPro) loss, where intermediate activations are used for back-propagation. The idea is that the early layers of the network are the ones that contain task-relevant information, while the later layers contain useful information that is irrelevant to the task-irrelevant information. The InfoPro loss is used as a surrogate optimization objective, where the reconstruction loss is replaced by a normal cross-entropy/contrastive term. Experiments on three datasets (CIFAR, SVHN, STL-10, ImageNet, and Cityscapes) show that InfoPro outperforms E2E training with the same memory footprint, and the authors also show that the proposed method can be used for training acceleration by learning local modules that are more robust to the GPU memory constraint. ","This paper studies the problem of training deep networks with GPUs memory footprint. The authors propose a new model called InfoPro, which learns gradient-separated modules in a network with local supervision. The local modules are trained with the information propagation (InfoPro) loss, where intermediate activations are used for back-propagation. The idea is that the early layers of the network are the ones that contain task-relevant information, while the later layers contain useful information that is irrelevant to the task-irrelevant information. The InfoPro loss is used as a surrogate optimization objective, where the reconstruction loss is replaced by a normal cross-entropy/contrastive term. Experiments on three datasets (CIFAR, SVHN, STL-10, ImageNet, and Cityscapes) show that InfoPro outperforms E2E training with the same memory footprint, and the authors also show that the proposed method can be used for training acceleration by learning local modules that are more robust to the GPU memory constraint. "
711,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,Graph neural networks ( GNNs ) USED-FOR framework. expressive power FEATURE-OF learning representation of nodes and graphs. expressive power FEATURE-OF GNNs. multiple aggregation functions HYPONYM-OF complex neighborhood aggregation functions. injective aggregation function HYPONYM-OF complex neighborhood aggregation functions. aggregation function USED-FOR expressive power. framework USED-FOR GNNs. framework USED-FOR expressive power. expressive power EVALUATE-FOR GNNs. diverse sampling USED-FOR diverse neighborhoods. diverse sampling USED-FOR representation of target node. GNN model USED-FOR representation of diverse neighborhoods. representation of diverse neighborhoods USED-FOR representation of target node. rooted sub - graphs HYPONYM-OF diverse neighborhoods. diversity of different neighborhoods USED-FOR expressive power. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. GNNs EVALUATE-FOR framework. GAT HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. multi - class node classification task CONJUNCTION multi - label node classification task. multi - label node classification task CONJUNCTION multi - class node classification task. benchmark datasets CONJUNCTION multi - label node classification task. multi - label node classification task CONJUNCTION benchmark datasets. benchmark datasets EVALUATE-FOR multi - class node classification task. dataset USED-FOR multi - label node classification task. method USED-FOR GNN models. framework USED-FOR GNN models. framework USED-FOR GNNs. Method is layer - wise neighborhood aggregation. ,"Graph neural networks (GNNs) have recently been proposed as a framework to improve the expressive power of learning representation of nodes and graphs. This paper proposes a new aggregation function to improve expressive power for GNNs, which is based on multiple aggregation functions (e.g. injective aggregation function). The authors show that the expressive powers of GNN models can be improved by learning a representation of diverse neighborhoods (i.e. rooted sub-graphs) that are more expressive than layer-wise neighborhood aggregation. They also show that a GNN model can be trained to learn the representation of such diverse neighborhoods using a diverse sampling from these diverse neighborhoods.   The paper also shows that the diversity of different neighborhoods can be used to increase expressive power. The proposed framework is tested on several benchmark datasets, including a multi-class node classification task on a new dataset, multi-label graph classification task, and multi-labels classification task. The framework is shown to be able to achieve better expressive power than existing GNN's (GCN, GAT, etc.) on all of these tasks. The authors also demonstrate that the proposed method can be applied to several existing and recent GNN approaches, and show that this framework can improve the expressiveness of existing and novel GNN methods.","Graph neural networks (GNNs) have recently been proposed as a framework to improve the expressive power of learning representation of nodes and graphs. This paper proposes a new aggregation function to improve expressive power for GNNs, which is based on multiple aggregation functions (e.g. injective aggregation function). The authors show that the expressive powers of GNN models can be improved by learning a representation of diverse neighborhoods (i.e. rooted sub-graphs) that are more expressive than layer-wise neighborhood aggregation. They also show that a GNN model can be trained to learn the representation of such diverse neighborhoods using a diverse sampling from these diverse neighborhoods.   The paper also shows that the diversity of different neighborhoods can be used to increase expressive power. The proposed framework is tested on several benchmark datasets, including a multi-class node classification task on a new dataset, multi-label graph classification task, and multi-labels classification task. The framework is shown to be able to achieve better expressive power than existing GNN's (GCN, GAT, etc.) on all of these tasks. The authors also demonstrate that the proposed method can be applied to several existing and recent GNN approaches, and show that this framework can improve the expressiveness of existing and novel GNN methods."
720,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,network transmission failures CONJUNCTION hardware errors. hardware errors CONJUNCTION network transmission failures. defenses USED-FOR corruptions. video machine learning models USED-FOR bit - level network and file corruptions. robustness EVALUATE-FOR video machine learning models. common action recognition CONJUNCTION multi - object tracking tasks. multi - object tracking tasks CONJUNCTION common action recognition. corruption levels FEATURE-OF network and file corruptions. defenses USED-FOR bit - level corruptions. corruption - agnostic and corruption - aware defenses HYPONYM-OF defenses. corruption - agnostic defenses COMPARE no - defense baseline. no - defense baseline COMPARE corruption - agnostic defenses. adversarial training HYPONYM-OF corruption - agnostic defenses. Bit - corruption Augmented Training ( BAT ) HYPONYM-OF corruptionaware baseline. model invariance USED-FOR corruptions. knowledge of bit - level corruptions FEATURE-OF corruptionaware baseline. BAT COMPARE corruption - agnostic defenses. corruption - agnostic defenses COMPARE BAT. BAT COMPARE no - defense baseline. no - defense baseline COMPARE BAT. highly - corrupted videos EVALUATE-FOR no - defense baseline. highly - corrupted videos EVALUATE-FOR BAT. Material is clean / near - clean data. ,"This paper studies the robustness of video machine learning models to bit-level network and file corruptions. The authors propose two defenses against corruptions: corruption-agnostic and corruption-aware defenses. They show that common action recognition and multi-object tracking tasks can be robust to corruptions of different corruption levels. They also show that adversarial training is one of the most robust of the corruption-gnostic defenses. Finally, they propose a corruptionaware baseline, called Bit-corruption Augmented Training (BAT), which is a corruption aware baseline that uses the knowledge of the corruptions and model invariance to model corruptions to improve robustness. BAT outperforms the previous state-of-the-art corruption agnostic defenses and the no-defense baseline on highly-corrupted videos. ","This paper studies the robustness of video machine learning models to bit-level network and file corruptions. The authors propose two defenses against corruptions: corruption-agnostic and corruption-aware defenses. They show that common action recognition and multi-object tracking tasks can be robust to corruptions of different corruption levels. They also show that adversarial training is one of the most robust of the corruption-gnostic defenses. Finally, they propose a corruptionaware baseline, called Bit-corruption Augmented Training (BAT), which is a corruption aware baseline that uses the knowledge of the corruptions and model invariance to model corruptions to improve robustness. BAT outperforms the previous state-of-the-art corruption agnostic defenses and the no-defense baseline on highly-corrupted videos. "
729,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"Representation learning models USED-FOR graphs. machine learning algorithms USED-FOR feature spaces. feature spaces FEATURE-OF nodes. skip - gram embedding approach USED-FOR implicit tensor factorization. implicit tensor factorization USED-FOR tensor representations of time - varying graphs. skip - gram embedding approach USED-FOR tensor representations of time - varying graphs. learned representations COMPARE state - of - the - art methods. state - of - the - art methods COMPARE learned representations. learned representations USED-FOR downstream tasks. state - of - the - art methods USED-FOR downstream tasks. approach USED-FOR downstream tasks. network reconstruction HYPONYM-OF downstream tasks. method USED-FOR contagion risk. method USED-FOR early risk awareness. disease spreading HYPONYM-OF dynamical processes. contact tracing data USED-FOR early risk awareness. Material is real - world networks. Generic are techniques, and approaches. ","Representation learning models for graphs are an important problem in real-world networks, and many machine learning algorithms have been developed to learn feature spaces for nodes in the feature spaces. This paper proposes a skip-gram embedding approach to learn tensor representations of time-varying graphs using implicit tensor factorization. The authors show that learned representations can outperform state-of-the-art methods on downstream tasks such as network reconstruction, and that the proposed approach can be applied to other downstream tasks as well. The proposed method is applied to contagion risk prediction and early risk awareness based on contact tracing data, and is shown to be effective for both dynamical processes such as disease spreading. The paper is well-written and well-motivated. However, there are a few issues with the presentation of the paper, and the experimental results are not convincing enough to support the effectiveness of the proposed techniques.   ","Representation learning models for graphs are an important problem in real-world networks, and many machine learning algorithms have been developed to learn feature spaces for nodes in the feature spaces. This paper proposes a skip-gram embedding approach to learn tensor representations of time-varying graphs using implicit tensor factorization. The authors show that learned representations can outperform state-of-the-art methods on downstream tasks such as network reconstruction, and that the proposed approach can be applied to other downstream tasks as well. The proposed method is applied to contagion risk prediction and early risk awareness based on contact tracing data, and is shown to be effective for both dynamical processes such as disease spreading. The paper is well-written and well-motivated. However, there are a few issues with the presentation of the paper, and the experimental results are not convincing enough to support the effectiveness of the proposed techniques.   "
738,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,self - supervised language modeling USED-FOR logical reasoning. self - supervised language modeling USED-FOR mathematical formulas. logical reasoning abilities EVALUATE-FOR language models. evaluation ( downstream ) tasks EVALUATE-FOR logical reasoning abilities. evaluation ( downstream ) tasks EVALUATE-FOR language models. language models USED-FOR formal mathematics. skip - tree task USED-FOR language models. models COMPARE models. models COMPARE models. skip - tree task USED-FOR models. mathematical reasoning abilities FEATURE-OF models. skipsequence tasks USED-FOR models. ,"This paper studies the use of self-supervised language modeling for logical reasoning in mathematical formulas. The authors show that the logical reasoning abilities of language models trained on evaluation (downstream) tasks can be compared to those of other language models for formal mathematics. They also show that models trained with a skip-tree task outperform other models on the same level of mathematical reasoning abilities. Finally, they show that these models can be trained on skipsequence tasks as well.","This paper studies the use of self-supervised language modeling for logical reasoning in mathematical formulas. The authors show that the logical reasoning abilities of language models trained on evaluation (downstream) tasks can be compared to those of other language models for formal mathematics. They also show that models trained with a skip-tree task outperform other models on the same level of mathematical reasoning abilities. Finally, they show that these models can be trained on skipsequence tasks as well."
747,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"robustness EVALUATE-FOR defense model. robustness PART-OF adversarial robustness research. defense model PART-OF adversarial robustness research. Obfuscated gradients HYPONYM-OF gradient masking. Obfuscated gradients USED-FOR defense methods. Margin Decomposition ( MD ) attack USED-FOR margin loss. attackability FEATURE-OF terms. Margin Decomposition ( MD ) attack USED-FOR imbalanced gradients. two - stage process USED-FOR terms. two - stage process USED-FOR attackability. models USED-FOR imbalanced gradients. label smoothing USED-FOR models. PGD attack EVALUATE-FOR PGD robustness. PGD robustness EVALUATE-FOR MD attacks. attack USED-FOR defenses. PGD robustness EVALUATE-FOR defenses. PGD robustness EVALUATE-FOR attack. adversarial robustness EVALUATE-FOR imbalanced gradients. OtherScientificTerm are Imbalanced Gradients, and gradient. Metric is overestimated adversarial robustness. Method is defense models. ","This paper studies the problem of improving the robustness of a defense model in adversarial robustness research. Imbalanced Gradients have been shown to be a major problem in the literature. Obfuscated gradients, which is a variant of gradient masking, have been proposed as a way to improve the performance of existing defense methods. This paper proposes to use Margin Decomposition (MD) attack to attack the margin loss of the defense models. The authors show that the attackability of the two terms is enhanced by a two-stage process, where the first term is used to mask the gradient of the model and the second term is applied to the model's output. They also show that models trained with label smoothing are more robust to imbalanced gradients. Finally, the authors demonstrate that the PGD attack improves PGD robustness against the MD attacks. ","This paper studies the problem of improving the robustness of a defense model in adversarial robustness research. Imbalanced Gradients have been shown to be a major problem in the literature. Obfuscated gradients, which is a variant of gradient masking, have been proposed as a way to improve the performance of existing defense methods. This paper proposes to use Margin Decomposition (MD) attack to attack the margin loss of the defense models. The authors show that the attackability of the two terms is enhanced by a two-stage process, where the first term is used to mask the gradient of the model and the second term is applied to the model's output. They also show that models trained with label smoothing are more robust to imbalanced gradients. Finally, the authors demonstrate that the PGD attack improves PGD robustness against the MD attacks. "
756,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,rich multi - type symbolic language USED-FOR linear algebra. proving semantic equivalence FEATURE-OF complex expressions. expressions USED-FOR system. typed trees HYPONYM-OF complex expressions. rich multi - type symbolic language USED-FOR expressions. rich multi - type symbolic language USED-FOR system. graph - to - sequence deep learning system USED-FOR axiomatic proofs of equivalence. operators PART-OF expressions. scalars PART-OF expressions. incremental graph - to - sequence networks USED-FOR complex and verifiable symbolic reasoning. robustness EVALUATE-FOR system. zero false positives EVALUATE-FOR It. average true positive coverage EVALUATE-FOR It. OtherScientificTerm is axioms of equivalence. ,"This paper proposes a system for proving semantic equivalence for complex expressions (e.g., typed trees) using a rich multi-type symbolic language for linear algebra. The system is built on top of a rich, multi-class symbolic language, which allows the system to learn a set of expressions that can be used for proving axioms of equivalence. These expressions are composed of operators that are equivariant to scalars, and a graph-to-sequence deep learning system is used to learn axiomatic proofs of equivalences. It achieves zero false positives and achieves an average true positive coverage that matches the state-of-the-art. The paper also shows that the system achieves robustness to attacks that are more likely to occur in complex and verifiable symbolic reasoning using incremental graph-tuple networks. ","This paper proposes a system for proving semantic equivalence for complex expressions (e.g., typed trees) using a rich multi-type symbolic language for linear algebra. The system is built on top of a rich, multi-class symbolic language, which allows the system to learn a set of expressions that can be used for proving axioms of equivalence. These expressions are composed of operators that are equivariant to scalars, and a graph-to-sequence deep learning system is used to learn axiomatic proofs of equivalences. It achieves zero false positives and achieves an average true positive coverage that matches the state-of-the-art. The paper also shows that the system achieves robustness to attacks that are more likely to occur in complex and verifiable symbolic reasoning using incremental graph-tuple networks. "
765,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"it USED-FOR multimodal setting. Transformers USED-FOR language domain. Transformers USED-FOR multimodal setting. language model USED-FOR visual model. multimodal Transformers USED-FOR audio - visual video representation learning. modality - specific and modality - shared parts PART-OF Transformer. low - rank approximation USED-FOR parameter sharing scheme. approach USED-FOR Transformers. approach USED-FOR model. model CONJUNCTION Transformers. Transformers CONJUNCTION model. CNN embedding space FEATURE-OF instance similarity. instance similarity USED-FOR negative sampling approach. it USED-FOR audio - visual classification tasks. Method is vision module. OtherScientificTerm are cross - modal information, and memory requirement. Material is Kinetics-700. ","This paper proposes a new approach to learn multimodal Transformers in the multimodality setting. The authors propose a new way to share information across modalities, which they call ""cross-modal information sharing"". The idea is to share the information between modality-specific and shared parts of a Transformer, and then use a low-rank approximation to learn a parameter sharing scheme. The proposed approach is evaluated on Kinetics-700, Kinetics and Kinetics+ and it is shown to perform well on audio-visual classification tasks. ","This paper proposes a new approach to learn multimodal Transformers in the multimodality setting. The authors propose a new way to share information across modalities, which they call ""cross-modal information sharing"". The idea is to share the information between modality-specific and shared parts of a Transformer, and then use a low-rank approximation to learn a parameter sharing scheme. The proposed approach is evaluated on Kinetics-700, Kinetics and Kinetics+ and it is shown to perform well on audio-visual classification tasks. "
774,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"few - shot learning USED-FOR online, continual setting. large scale indoor imagery USED-FOR visual experience. large scale indoor imagery USED-FOR few - shot learning dataset. spatiotemporal contextual information USED-FOR contextual prototypical memory model. Task are human and machine - learning environments, and online few - shot learning setting. Generic are setting, and models. OtherScientificTerm are spatiotemporal context, and Object classes. Method is few - shot learning approaches. ","This paper proposes a new setting for few-shot learning, where the goal is to learn a representation of a scene in an online, continual setting. The setting is very similar to human and machine-learning environments, but the authors extend the setting to the online setting, where there is no spatiotemporal context. The authors propose an online few-set learning setting where a few classes of objects are available in the scene, and the task is to select the most relevant object from a set of classes. Object classes are selected based on their similarity to other classes in the set. The paper proposes to use a few examples from a large scale indoor imagery as a visual experience to train a contextual prototypical memory model, which is then used to train an online model that can be applied to any online few set of objects.   The authors also propose to use the large-scale indoor imagery collected from a few sets of objects as a few-shots learning dataset, and to use this dataset as a benchmark to evaluate the performance of existing few-shots learning approaches.  The main contribution of the paper is that the authors propose to leverage the spatio-temporal contextual information from the indoor imagery to train the contextual prototype memory model. They show that their models are able to generalize to unseen objects in the indoor scene.","This paper proposes a new setting for few-shot learning, where the goal is to learn a representation of a scene in an online, continual setting. The setting is very similar to human and machine-learning environments, but the authors extend the setting to the online setting, where there is no spatiotemporal context. The authors propose an online few-set learning setting where a few classes of objects are available in the scene, and the task is to select the most relevant object from a set of classes. Object classes are selected based on their similarity to other classes in the set. The paper proposes to use a few examples from a large scale indoor imagery as a visual experience to train a contextual prototypical memory model, which is then used to train an online model that can be applied to any online few set of objects.   The authors also propose to use the large-scale indoor imagery collected from a few sets of objects as a few-shots learning dataset, and to use this dataset as a benchmark to evaluate the performance of existing few-shots learning approaches.  The main contribution of the paper is that the authors propose to leverage the spatio-temporal contextual information from the indoor imagery to train the contextual prototype memory model. They show that their models are able to generalize to unseen objects in the indoor scene."
783,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"distribution shift CONJUNCTION gbv rowing. gbv rowing CONJUNCTION distribution shift. temporal graphs USED-FOR GNNs. GNN architectures CONJUNCTION scalable GNN techniques. scalable GNN techniques CONJUNCTION GNN architectures. vertices CONJUNCTION edges. edges CONJUNCTION vertices. accuracy EVALUATE-FOR GNN ’s receptive field. Method is graph neural networks ( GNNs ). OtherScientificTerm are full graph, and temporal window. ",This paper studies the problem of distribution shift and gbv rowing in graph neural networks (GNNs). The authors show that GNNs trained on temporal graphs are sensitive to distribution shift. They also show that a GNN’s receptive field is highly sensitive to the number of vertices and edges in the full graph. The paper also shows that the accuracy of GNN architectures and scalable GNN techniques can be improved when the temporal window is large enough.,This paper studies the problem of distribution shift and gbv rowing in graph neural networks (GNNs). The authors show that GNNs trained on temporal graphs are sensitive to distribution shift. They also show that a GNN’s receptive field is highly sensitive to the number of vertices and edges in the full graph. The paper also shows that the accuracy of GNN architectures and scalable GNN techniques can be improved when the temporal window is large enough.
792,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"Representation learning USED-FOR deep reinforcement learning ( RL ). visualized input USED-FOR Representation learning. visualized input USED-FOR feature space. technique USED-FOR representation feature space. cross - state self - constraint(CSSC ) HYPONYM-OF technique. constraint USED-FOR general feature recognition. implicit feedback USED-FOR constraint. learning process USED-FOR general feature recognition. generalization EVALUATE-FOR constraint. generalization EVALUATE-FOR method. OpenAI ProcGen benchmark EVALUATE-FOR method. Method is RL agent. OtherScientificTerm are representation similarity, and Procgen games. ","Representation learning for deep reinforcement learning (RL) with visualized input is an important problem. In this paper, the authors propose a technique called cross-state self-constraint(CSSC) that constrains the representation feature space of an RL agent to be similar to the representation similarity of the current and previous states of the agent. The authors show that this constraint can be used to improve general feature recognition in the learning process through implicit feedback. The proposed method is tested on the OpenAI ProcGen benchmark and shows that the proposed constraint improves the generalization performance of the proposed method. The paper also shows that CSSC improves performance in Procgen games.","Representation learning for deep reinforcement learning (RL) with visualized input is an important problem. In this paper, the authors propose a technique called cross-state self-constraint(CSSC) that constrains the representation feature space of an RL agent to be similar to the representation similarity of the current and previous states of the agent. The authors show that this constraint can be used to improve general feature recognition in the learning process through implicit feedback. The proposed method is tested on the OpenAI ProcGen benchmark and shows that the proposed constraint improves the generalization performance of the proposed method. The paper also shows that CSSC improves performance in Procgen games."
801,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"GNNs USED-FOR real - world applications. robustness FEATURE-OF GNNs. adversarial attacks FEATURE-OF GNNs. model parameters CONJUNCTION model predictions. model predictions CONJUNCTION model parameters. restricted near - black - box setup FEATURE-OF GNNs. attacks CONJUNCTION influence maximization problem. influence maximization problem CONJUNCTION attacks. influence maximization problem FEATURE-OF graph. adversarial attack FEATURE-OF GNNs. strategies COMPARE baseline adversarial attack strategies. baseline adversarial attack strategies COMPARE strategies. GNN models COMPARE baseline adversarial attack strategies. baseline adversarial attack strategies COMPARE GNN models. GNN models EVALUATE-FOR strategies. Method are Graph neural networks ( GNNs ), and near - black - box attack strategies. Material is realistic setups. OtherScientificTerm is features. ","Graph neural networks (GNNs) have been shown to be robust to adversarial attacks in real-world applications. However, the robustness of GNNs is not well studied in the restricted near-black-box setup where the model parameters and model predictions are not available.  This paper investigates the problem of robustness against adversarial attack in GNN's under realistic setups. The authors show that under certain assumptions on the model parameter and the model predictions, GNN’s can be attacked in a restricted near black-box setting. They also show that the attacks and the influence maximization problem over the graph of a graph can be solved efficiently.  The authors also propose two new strategies to attack GNN models that outperform baseline adversarial threat models. The proposed strategies are based on the observation that the features of a GNN can be modified in a way that makes it more robust to the attacks. The paper also shows that the proposed strategies can be combined with other baseline adversaries to improve the performance of the GNN.    The paper is well-written and well-motivated. It provides a detailed analysis of the problem and provides a number of near-hidden-box attack strategies. ","Graph neural networks (GNNs) have been shown to be robust to adversarial attacks in real-world applications. However, the robustness of GNNs is not well studied in the restricted near-black-box setup where the model parameters and model predictions are not available.  This paper investigates the problem of robustness against adversarial attack in GNN's under realistic setups. The authors show that under certain assumptions on the model parameter and the model predictions, GNN’s can be attacked in a restricted near black-box setting. They also show that the attacks and the influence maximization problem over the graph of a graph can be solved efficiently.  The authors also propose two new strategies to attack GNN models that outperform baseline adversarial threat models. The proposed strategies are based on the observation that the features of a GNN can be modified in a way that makes it more robust to the attacks. The paper also shows that the proposed strategies can be combined with other baseline adversaries to improve the performance of the GNN.    The paper is well-written and well-motivated. It provides a detailed analysis of the problem and provides a number of near-hidden-box attack strategies. "
810,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"directed acyclic graphs ( DAGs ) USED-FOR learning causal structures. ( weighted ) adjacency matrix FEATURE-OF DAG causal model. low rank assumption FEATURE-OF ( weighted ) adjacency matrix. low rank assumption USED-FOR DAG causal model. methods USED-FOR causal structure learning. interpretable graphical conditions CONJUNCTION low rank assumption. low rank assumption CONJUNCTION interpretable graphical conditions. assumption USED-FOR methods. maximum rank FEATURE-OF hubs. low rank FEATURE-OF scale - free networks. rank FEATURE-OF DAG. they COMPARE algorithms. algorithms COMPARE they. OtherScientificTerm are causal structures, graphs, and low rank condition. Task is high dimensional settings. Method is low rank adaptations. ","This paper studies the problem of learning causal structures in directed acyclic graphs (DAGs). The authors propose two methods for causal structure learning based on interpretable graphical conditions and a low rank assumption on the (weighted) adjacency matrix of a DAG causal model under the low-rank assumption. The authors show that the maximum rank of the hubs of DAGs is low in high dimensional settings. They also show that scale-free networks with low rank have a similar maximum rank to hubs with a similar low rank. Finally, they show that low rank adaptations can be made to existing algorithms, and that they outperform existing algorithms under the same low rank condition. ","This paper studies the problem of learning causal structures in directed acyclic graphs (DAGs). The authors propose two methods for causal structure learning based on interpretable graphical conditions and a low rank assumption on the (weighted) adjacency matrix of a DAG causal model under the low-rank assumption. The authors show that the maximum rank of the hubs of DAGs is low in high dimensional settings. They also show that scale-free networks with low rank have a similar maximum rank to hubs with a similar low rank. Finally, they show that low rank adaptations can be made to existing algorithms, and that they outperform existing algorithms under the same low rank condition. "
819,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"neural architecture search ( NAS ) CONJUNCTION hyper - parameter optimization ( HPO ). hyper - parameter optimization ( HPO ) CONJUNCTION neural architecture search ( NAS ). automated data augmentation ( DA ) CONJUNCTION neural architecture search ( NAS ). neural architecture search ( NAS ) CONJUNCTION automated data augmentation ( DA ). components PART-OF automated machine learning ( AutoML ) pipeline. automated data augmentation ( DA ) PART-OF automated machine learning ( AutoML ) pipeline. automated data augmentation ( DA ) HYPONYM-OF components. hyper - parameter optimization ( HPO ) HYPONYM-OF components. neural architecture search ( NAS ) HYPONYM-OF components. components USED-FOR joint optimization. it USED-FOR NAS. end - to - end solution USED-FOR ready - to - use model. hyper - parameters CONJUNCTION data augmentation policies. data augmentation policies CONJUNCTION hyper - parameters. co - optimization USED-FOR neural architectures. co - optimization USED-FOR method. data augmentation policies USED-FOR method. DiffAutoML COMPARE end - to - end AutoML algorithms. end - to - end AutoML algorithms COMPARE DiffAutoML. DiffAutoML COMPARE multi - stage AutoML algorithms. multi - stage AutoML algorithms COMPARE DiffAutoML. ImageNet EVALUATE-FOR end - to - end AutoML algorithms. computational efficiency EVALUATE-FOR multi - stage AutoML algorithms. ImageNet EVALUATE-FOR DiffAutoML. NAS CONJUNCTION HPO. HPO CONJUNCTION NAS. automated DA CONJUNCTION NAS. NAS CONJUNCTION automated DA. en - to - end manner FEATURE-OF HPO. Generic is component. OtherScientificTerm is search dimension. Task is search and retraining stages. Method are differentiable joint optimization solution, model retraining, and retraining. ","This paper proposes a new automated machine learning (AutoML) pipeline that combines three components: automated data augmentation (DA), neural architecture search (NAS), hyper-parameter optimization (HPO), and joint optimization. Each component has its own advantages and disadvantages. The authors propose an end-to-end solution that can be used to train a ready -to-use model and then apply it to NAS, HPO, or joint optimization tasks. The proposed method is based on co-optimization for neural architectures, where the search and retraining stages are differentiable, and a differentiable joint optimization solution is proposed. The search dimension of the joint optimization is also differentiable.  The authors also propose a method that uses co-optation for joint optimization, which is a method based on the idea of learning a joint solution that optimally optimizes all the components of a joint optimization (hyper-parameters as well as the model retraining).  Experiments on ImageNet show that the proposed DiffAutoML outperforms the state-of-the-art multi-stage AutoML algorithms in terms of computational efficiency. In addition, the authors show that HPO can be optimized in an en- to-end manner in a similar way as in the case of automated DA, NAS, and HPO. ","This paper proposes a new automated machine learning (AutoML) pipeline that combines three components: automated data augmentation (DA), neural architecture search (NAS), hyper-parameter optimization (HPO), and joint optimization. Each component has its own advantages and disadvantages. The authors propose an end-to-end solution that can be used to train a ready -to-use model and then apply it to NAS, HPO, or joint optimization tasks. The proposed method is based on co-optimization for neural architectures, where the search and retraining stages are differentiable, and a differentiable joint optimization solution is proposed. The search dimension of the joint optimization is also differentiable.  The authors also propose a method that uses co-optation for joint optimization, which is a method based on the idea of learning a joint solution that optimally optimizes all the components of a joint optimization (hyper-parameters as well as the model retraining).  Experiments on ImageNet show that the proposed DiffAutoML outperforms the state-of-the-art multi-stage AutoML algorithms in terms of computational efficiency. In addition, the authors show that HPO can be optimized in an en- to-end manner in a similar way as in the case of automated DA, NAS, and HPO. "
828,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,Prior Networks HYPONYM-OF models. interpretable measures of uncertainty EVALUATE-FOR models. tasks EVALUATE-FOR ensemble approaches. calibration CONJUNCTION uncertainty estimates. uncertainty estimates CONJUNCTION calibration. They USED-FOR ensemble of models. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. uncertainty estimates PART-OF model. Ensemble Distribution Distillation ( EnD ) USED-FOR ensemble of models. Prior Networks USED-FOR classification tasks. Prior Networks CONJUNCTION EnD. EnD CONJUNCTION Prior Networks. EnD USED-FOR regression tasks. Prior Networks USED-FOR regression tasks. synthetic data CONJUNCTION UCI datasets. UCI datasets CONJUNCTION synthetic data. UCI datasets CONJUNCTION monocular depth estimation tasks. monocular depth estimation tasks CONJUNCTION UCI datasets. monocular depth estimation tasks EVALUATE-FOR Regression Prior Networks. UCI datasets EVALUATE-FOR Regression Prior Networks. synthetic data EVALUATE-FOR Regression Prior Networks. They COMPARE ensemble approaches. ensemble approaches COMPARE They. OtherScientificTerm is Normal - Wishart distribution. ,"This paper proposes two models called Prior Networks and Ensemble Distribution Distillation (EnD) to provide interpretable measures of uncertainty for models. They are used to train an ensemble of models to improve accuracy, calibration, and uncertainty estimates in a model. Prior Networks are used for classification tasks and EnD is used for regression tasks. Regression Prior Networks outperform EnD on synthetic data, UCI datasets, and monocular depth estimation tasks. They also outperform other ensemble approaches. ","This paper proposes two models called Prior Networks and Ensemble Distribution Distillation (EnD) to provide interpretable measures of uncertainty for models. They are used to train an ensemble of models to improve accuracy, calibration, and uncertainty estimates in a model. Prior Networks are used for classification tasks and EnD is used for regression tasks. Regression Prior Networks outperform EnD on synthetic data, UCI datasets, and monocular depth estimation tasks. They also outperform other ensemble approaches. "
837,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,catastrophic forgetting FEATURE-OF Neural networks. problem USED-FOR large - scale supervised classification. catastrophic forgetting USED-FOR few - shot classification problems. few - shot tasks USED-FOR Few - shot metalearning algorithms. Bayesian online meta - learning framework USED-FOR sequential few - shot tasks problems. Bayesian online meta - learning framework USED-FOR catastrophic forgetting. catastrophic forgetting CONJUNCTION sequential few - shot tasks problems. sequential few - shot tasks problems CONJUNCTION catastrophic forgetting. Laplace approximation CONJUNCTION variational inference. variational inference CONJUNCTION Laplace approximation. MAML PART-OF Bayesian online learning algorithm. Laplace approximation USED-FOR Bayesian online learning algorithm. variational inference USED-FOR Bayesian online learning algorithm. MAML PART-OF framework. framework USED-FOR few - shot classification. sequentially arriving datasets USED-FOR few - shot classification. framework USED-FOR catastrophic forgetting. framework USED-FOR online meta - learning. online meta - learning USED-FOR few - shot classification settings. Material is sequential datasets. Generic is algorithm. Method is meta - learned model. Task is sequentially arriving few - shot tasks. ,"This paper studies the problem of catastrophic forgetting in Neural networks for few-shot classification problems. Few-shot metalearning algorithms have been shown to be effective for a number of few-set classification problems on sequential datasets, but this problem is not well studied for large-scale supervised classification. This paper proposes a Bayesian online meta-learning framework to address the catastrophic forgetting for both catastrophic forgetting and sequential few-task tasks problems. The proposed framework combines MAML with the Laplace approximation and variational inference to improve the performance of the algorithm. The authors also propose a meta-learned model that can be applied to a variety of tasks. The paper shows that the proposed framework can be used to improve catastrophic forgetting on few- shot classification on sequentially arriving datasets. In addition, the authors also show that the framework can also be used for the task of online meta -learning in the few-shooting classification settings. ","This paper studies the problem of catastrophic forgetting in Neural networks for few-shot classification problems. Few-shot metalearning algorithms have been shown to be effective for a number of few-set classification problems on sequential datasets, but this problem is not well studied for large-scale supervised classification. This paper proposes a Bayesian online meta-learning framework to address the catastrophic forgetting for both catastrophic forgetting and sequential few-task tasks problems. The proposed framework combines MAML with the Laplace approximation and variational inference to improve the performance of the algorithm. The authors also propose a meta-learned model that can be applied to a variety of tasks. The paper shows that the proposed framework can be used to improve catastrophic forgetting on few- shot classification on sequentially arriving datasets. In addition, the authors also show that the framework can also be used for the task of online meta -learning in the few-shooting classification settings. "
846,SP:89d2765946e70455105a608d998c3b900969cb8d,"expressive power EVALUATE-FOR higher - order GNNs. computational cost CONJUNCTION expressive power. expressive power CONJUNCTION computational cost. model USED-FOR subgraphs. RNP - GNNs COMPARE higher - order k - GNN. higher - order k - GNN COMPARE RNP - GNNs. higher - order k - GNN CONJUNCTION Local Relational Pooling ( LRP ) networks. Local Relational Pooling ( LRP ) networks CONJUNCTION higher - order k - GNN. RNP - GNNs COMPARE Local Relational Pooling ( LRP ) networks. Local Relational Pooling ( LRP ) networks COMPARE RNP - GNNs. computational complexity EVALUATE-FOR higher - order k - GNN. computational complexity EVALUATE-FOR Local Relational Pooling ( LRP ) networks. computational complexity EVALUATE-FOR RNP - GNNs. Task is learning with graphs. Method are recursive pooling technique of local neighborhoods, and low - order GNNs. OtherScientificTerm is local neighborhoods. ","This paper studies the problem of learning with graphs. The authors propose a new recursive pooling technique of local neighborhoods, which they call RNP-GNNs. They show that the model is able to learn subgraphs that are more expressive than low-order GNNs in terms of both computational cost and expressive power. They also show that under certain conditions, the RNP - GNN is more efficient than a higher-order k-GAN and a Local Relational Pooling (LRP) networks. ","This paper studies the problem of learning with graphs. The authors propose a new recursive pooling technique of local neighborhoods, which they call RNP-GNNs. They show that the model is able to learn subgraphs that are more expressive than low-order GNNs in terms of both computational cost and expressive power. They also show that under certain conditions, the RNP - GNN is more efficient than a higher-order k-GAN and a Local Relational Pooling (LRP) networks. "
855,SP:c43f5deb340555d78599a3496318514a826b1aae,"competitive environments CONJUNCTION games. games CONJUNCTION competitive environments. GANs HYPONYM-OF games. irregular behaviors FEATURE-OF systems. Multiplicative Weights Update ( MWU ) HYPONYM-OF learning algorithms. canonical game decomposition USED-FOR zero - sum and coordination components. volume - expansion argument USED-FOR characterizations. canonical game decomposition USED-FOR volume - expansion argument. components USED-FOR volume - changing behaviors. matrix domination CONJUNCTION linear program. linear program CONJUNCTION matrix domination. general games CONJUNCTION graphical games. graphical games CONJUNCTION general games. MWU CONJUNCTION OMWU. OMWU CONJUNCTION MWU. MWU USED-FOR potential games. OMWU USED-FOR potential games. local equivalence of volume change USED-FOR multi - player games. Method is Machine Learning. Material is two - person zero - sum games. OtherScientificTerm are Lyapunov chaos, cumulative payoff space, persistent chaos, and zero - sum games. Task are normal - form game settings, and bimatrix games. ","This paper studies the volume-expansion argument for characterizations based on the canonical game decomposition of the zero-sum and coordination components. The authors consider competitive environments and games (e.g., GANs) where there is a Lyapunov chaos in the cumulative payoff space. They show that these systems exhibit irregular behaviors and that learning algorithms such as Multiplicative Weights Update (MWU) can be seen as learning algorithms that are invariant to these irregular behaviors. They also show that in two-person zero-sketch games, there is an equivalence between the two components of volume-changing behaviors. In normal-form game settings, this equivalence holds for matrix domination and a linear program, but not in bimatrix games.  The authors show that for general games and graphical games, MWU and OMWU can learn potential games that are equivalent to potential games in terms of the local equivalence of volume change in multi-player games. In the case of bimatial games, they show that persistent chaos can be learned in a similar way as it is in zero-Sum games. However, for bimetric games, it is not possible. ","This paper studies the volume-expansion argument for characterizations based on the canonical game decomposition of the zero-sum and coordination components. The authors consider competitive environments and games (e.g., GANs) where there is a Lyapunov chaos in the cumulative payoff space. They show that these systems exhibit irregular behaviors and that learning algorithms such as Multiplicative Weights Update (MWU) can be seen as learning algorithms that are invariant to these irregular behaviors. They also show that in two-person zero-sketch games, there is an equivalence between the two components of volume-changing behaviors. In normal-form game settings, this equivalence holds for matrix domination and a linear program, but not in bimatrix games.  The authors show that for general games and graphical games, MWU and OMWU can learn potential games that are equivalent to potential games in terms of the local equivalence of volume change in multi-player games. In the case of bimatial games, they show that persistent chaos can be learned in a similar way as it is in zero-Sum games. However, for bimetric games, it is not possible. "
864,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"adaptive algorithms USED-FOR deep learning. AMSGrad CONJUNCTION Radam. Radam CONJUNCTION AMSGrad. Radam HYPONYM-OF adaptive algorithms. AMSGrad HYPONYM-OF adaptive algorithms. convergence rate EVALUATE-FOR adaptive algorithms. marginal regret bound minimization HYPONYM-OF proximal function of adaptive algorithms. adaptive algorithms COMPARE adaptive algorithms. adaptive algorithms COMPARE adaptive algorithms. marginal optimality FEATURE-OF adaptive algorithms. deep learning EVALUATE-FOR adaptive algorithms. Generic are modifications, and algorithm. ","This paper considers the problem of adaptive algorithms for deep learning, and studies the convergence rate of two adaptive algorithms, AMSGrad and Radam. In particular, the authors show that adaptive algorithms with certain modifications (e.g. marginal regret bound minimization, which is a proximal function of adaptive algorithm) have a better convergence rate than adaptive algorithms without these modifications. The authors also show that the marginal optimality of the adaptive algorithms in deep learning is better than that of the original adaptive algorithms. ","This paper considers the problem of adaptive algorithms for deep learning, and studies the convergence rate of two adaptive algorithms, AMSGrad and Radam. In particular, the authors show that adaptive algorithms with certain modifications (e.g. marginal regret bound minimization, which is a proximal function of adaptive algorithm) have a better convergence rate than adaptive algorithms without these modifications. The authors also show that the marginal optimality of the adaptive algorithms in deep learning is better than that of the original adaptive algorithms. "
873,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"risk management USED-FOR real - world decision - making problems. mean - variance criterion HYPONYM-OF risk management approaches. quadratic utility function USED-FOR risk management. reward - constrained variance minimization CONJUNCTION regularization. regularization CONJUNCTION reward - constrained variance minimization. EQUM COMPARE mean - variance RL methods. mean - variance RL methods COMPARE EQUM. double sampling USED-FOR mean - variance RL methods. RL and financial data EVALUATE-FOR EQUM. Method are expected quadratic utility maximization ( EQUM ), and mean - variance control. Task is agent utility maximization. ",This paper proposes expected quadratic utility maximization (EQUM) as a new risk-minimization method for risk-averse reinforcement learning (RL) problems. The authors show that the proposed method can be used to improve the performance of existing risk minimization methods in RL and financial decision-making problems. They also show that it can be combined with existing risk-control methods to achieve better performance.  ,This paper proposes expected quadratic utility maximization (EQUM) as a new risk-minimization method for risk-averse reinforcement learning (RL) problems. The authors show that the proposed method can be used to improve the performance of existing risk minimization methods in RL and financial decision-making problems. They also show that it can be combined with existing risk-control methods to achieve better performance.  
882,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"neural networks USED-FOR auxiliary tasks. auxiliary tasks PART-OF coherent loss. network USED-FOR coherent objective function. network USED-FOR nonlinear interactions. network USED-FOR auxiliary task. it COMPARE methods. methods COMPARE it. tasks EVALUATE-FOR AuxiLearn. image segmentation HYPONYM-OF tasks. Task are multi - task learning setting, and designing useful auxiliary tasks. Generic is framework. OtherScientificTerm are implicit differentiation, useful auxiliaries, and low data regime. "," the multi-task learning setting, the authors propose AuxiLearn, a framework for designing auxiliary tasks for neural networks. The auxiliary tasks are part of a coherent loss, and the network is trained to learn a coherent objective function. The authors show that auxiliary tasks can be learned through implicit differentiation, and that useful auxiliaries can be found in the low data regime. They also show that the network can learn nonlinear interactions between the auxiliary task and the original task, and show that it outperforms existing methods on a number of tasks, including image segmentation. "," the multi-task learning setting, the authors propose AuxiLearn, a framework for designing auxiliary tasks for neural networks. The auxiliary tasks are part of a coherent loss, and the network is trained to learn a coherent objective function. The authors show that auxiliary tasks can be learned through implicit differentiation, and that useful auxiliaries can be found in the low data regime. They also show that the network can learn nonlinear interactions between the auxiliary task and the original task, and show that it outperforms existing methods on a number of tasks, including image segmentation. "
891,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,out - of - training - distribution sentences PART-OF Neural Machine Translation. Bayesian Deep Learning equivalent of Transformer models USED-FOR out - of - training - distribution sentences. measure USED-FOR long sequences of discrete random variables. approaches USED-FOR long sentences. measure USED-FOR Transformer model. dropout approximate inference USED-FOR Transformer model. WMT13 CONJUNCTION Europarl. Europarl CONJUNCTION WMT13. dropout uncertainty USED-FOR measure. model COMPARE German. German COMPARE model. measure USED-FOR Dutch source sentences. measure USED-FOR German - English translation. WMT13 USED-FOR German - English translation. Europarl USED-FOR German - English translation. ,"This paper proposes a new measure for out-of-distribution sentences in Neural Machine Translation, a Bayesian Deep Learning equivalent of Transformer models. The proposed measure is designed to capture long sequences of discrete random variables, which are common in existing approaches for long sentences. This measure is used to train a Transformer model with dropout approximate inference. The authors show that the proposed measure can be applied to German-English translation using WMT13 and Europarl, and can be used to measure the dropout uncertainty for Dutch source sentences as well. They also show that their model performs better than German in this setting. ","This paper proposes a new measure for out-of-distribution sentences in Neural Machine Translation, a Bayesian Deep Learning equivalent of Transformer models. The proposed measure is designed to capture long sequences of discrete random variables, which are common in existing approaches for long sentences. This measure is used to train a Transformer model with dropout approximate inference. The authors show that the proposed measure can be applied to German-English translation using WMT13 and Europarl, and can be used to measure the dropout uncertainty for Dutch source sentences as well. They also show that their model performs better than German in this setting. "
900,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"SNIP CONJUNCTION GraSP. GraSP CONJUNCTION SNIP. GraSP CONJUNCTION SynFlow. SynFlow CONJUNCTION GraSP. SynFlow CONJUNCTION magnitude pruning. magnitude pruning CONJUNCTION SynFlow. GraSP CONJUNCTION magnitude pruning. magnitude pruning CONJUNCTION GraSP. methods COMPARE random pruning. random pruning COMPARE methods. accuracy EVALUATE-FOR magnitude pruning. they COMPARE magnitude pruning. magnitude pruning COMPARE they. methods USED-FOR per - weight pruning decisions. Task is pruning neural networks. Method are neural networks, and pruning heuristics. ","This paper studies the problem of pruning neural networks. The authors consider three different pruning heuristics: SNIP, GraSP, and magnitude pruning. They show that these methods are more effective than random pruning, and that they can achieve better accuracy than the previous state-of-the-art methods. They also show that per-weight pruning decisions can be made using these methods. ","This paper studies the problem of pruning neural networks. The authors consider three different pruning heuristics: SNIP, GraSP, and magnitude pruning. They show that these methods are more effective than random pruning, and that they can achieve better accuracy than the previous state-of-the-art methods. They also show that per-weight pruning decisions can be made using these methods. "
909,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,semi - honest server CONJUNCTION Byzantine malicious clients. Byzantine malicious clients CONJUNCTION semi - honest server. FED - LEARNING HYPONYM-OF federated learning protocol. robust mean estimator USED-FOR FED - LEARNING. FED - LEARNING HYPONYM-OF FL protocol. FED - LEARNING USED-FOR dimension - free estimation error. robust mean estimator USED-FOR FL protocol. FL protocol USED-FOR dimension - free estimation error. FilterL2 HYPONYM-OF robust mean estimator. secure aggregation USED-FOR FED - LEARNING. FilterL2 CONJUNCTION secure aggregation. secure aggregation CONJUNCTION FilterL2. optimal or close - to - optimal performance EVALUATE-FOR FED - LEARNING. OtherScientificTerm is shards. Method is robust FL protocols. ,"This paper proposes a new federated learning protocol called FED-LEARNING, which uses a robust mean estimator called FilterL2 to estimate the dimension-free estimation error of a FL protocol (i.e. semi-honest server and Byzantine malicious clients) in the presence of shards. The authors show that the robust mean of the FL protocol is a function of the number of shards and the number (and the size) of the clients. They also show that FED - LEARNING can be used to compute the dimension of the dimension free estimation error for any FL protocol. The paper also shows that the optimal or close-to-optimal performance can be obtained for FEDs using both the robust FL protocols.    The authors also provide a theoretical analysis of the robustness of the proposed method and show that it is more robust than the existing methods. Finally, the authors provide experimental results that show the effectiveness of the new method, which is based on the combination of filterL2 and secure aggregation. ","This paper proposes a new federated learning protocol called FED-LEARNING, which uses a robust mean estimator called FilterL2 to estimate the dimension-free estimation error of a FL protocol (i.e. semi-honest server and Byzantine malicious clients) in the presence of shards. The authors show that the robust mean of the FL protocol is a function of the number of shards and the number (and the size) of the clients. They also show that FED - LEARNING can be used to compute the dimension of the dimension free estimation error for any FL protocol. The paper also shows that the optimal or close-to-optimal performance can be obtained for FEDs using both the robust FL protocols.    The authors also provide a theoretical analysis of the robustness of the proposed method and show that it is more robust than the existing methods. Finally, the authors provide experimental results that show the effectiveness of the new method, which is based on the combination of filterL2 and secure aggregation. "
918,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"aircraft CONJUNCTION robot morphology. robot morphology CONJUNCTION aircraft. design input USED-FOR unknown objective function. robot morphology HYPONYM-OF domains. aircraft HYPONYM-OF domains. method USED-FOR function. offline MBO methods USED-FOR highdimensional problems. high - capacity deep neural network function approximators USED-FOR highdimensional problems. high - capacity deep neural network function approximators USED-FOR offline MBO methods. evaluation tasks EVALUATE-FOR field. Design - Bench HYPONYM-OF offline MBO tasks. Design - Bench HYPONYM-OF benchmark suite. benchmark suite EVALUATE-FOR offline MBO tasks. unified evaluation protocol USED-FOR benchmark suite. unified evaluation protocol USED-FOR offline MBO tasks. biology CONJUNCTION material science. material science CONJUNCTION biology. material science CONJUNCTION robotics. robotics CONJUNCTION material science. benchmark EVALUATE-FOR offline MBO methods. Generic are problems, and benchmarks. OtherScientificTerm are feedback, and objective function. Task is data - driven offline MBO setting. ","This paper proposes a data-driven offline MBO setting, where the goal is to solve a set of problems where the objective function is unknown, and the design input is an unknown objective function. Two domains are considered: (1) aircraft and (2) robot morphology. The authors propose a method to learn a function that can be used to solve the problem in both cases. They show that existing offline MOB methods using high-capacity deep neural network function approximators can solve highdimensional problems, and demonstrate that their method can be applied to a wide range of problems.   The paper also proposes a new benchmark suite called the Design-Bench, which consists of a suite of offline-MBO tasks, and a unified evaluation protocol for all of these tasks. The paper shows that the evaluation tasks in this field are well-motivated and well-suited for evaluation tasks where there is no feedback from the target function.  The authors also show that the proposed benchmark suite is a unified benchmark for all the different types of off-line MBO methods, and that their benchmark suite can be easily extended to a wider range of offline tasks.  Finally, the authors show that their benchmarks are well suited for a variety of domains, including biology, material science, robotics, and robotics.","This paper proposes a data-driven offline MBO setting, where the goal is to solve a set of problems where the objective function is unknown, and the design input is an unknown objective function. Two domains are considered: (1) aircraft and (2) robot morphology. The authors propose a method to learn a function that can be used to solve the problem in both cases. They show that existing offline MOB methods using high-capacity deep neural network function approximators can solve highdimensional problems, and demonstrate that their method can be applied to a wide range of problems.   The paper also proposes a new benchmark suite called the Design-Bench, which consists of a suite of offline-MBO tasks, and a unified evaluation protocol for all of these tasks. The paper shows that the evaluation tasks in this field are well-motivated and well-suited for evaluation tasks where there is no feedback from the target function.  The authors also show that the proposed benchmark suite is a unified benchmark for all the different types of off-line MBO methods, and that their benchmark suite can be easily extended to a wider range of offline tasks.  Finally, the authors show that their benchmarks are well suited for a variety of domains, including biology, material science, robotics, and robotics."
927,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"self - supervised generative models USED-FOR ELBO. self - supervised generative models USED-FOR multimodal models. generalized ELBO formulation USED-FOR multimodal data. methods PART-OF objective. method COMPARE state - of - the - art models. state - of - the - art models COMPARE method. selfsupervised, generative learning tasks EVALUATE-FOR state - of - the - art models. selfsupervised, generative learning tasks EVALUATE-FOR method. OtherScientificTerm are real - world phenomena, posterior approximation functions, semantic coherence, and joint data distribution. Generic is them. Task is machine learning research. ","This paper proposes to use self-supervised generative models to approximate the ELBO of multimodal models. The authors propose a generalized ELBO formulation that can be applied to multi-modal data, which is an important problem in real-world phenomena. The paper proposes two different posterior approximation functions, one based on semantic coherence and the other based on joint data distribution. Both of these methods are incorporated into the objective, and the authors show that both of them perform well. The proposed method is compared to state-of-the-art models on a number of selfsupervised, generative learning tasks, and shows that the proposed method outperforms them.    The paper is well-written, well-motivated, and easy to follow. It is a good contribution to the field of machine learning research. ","This paper proposes to use self-supervised generative models to approximate the ELBO of multimodal models. The authors propose a generalized ELBO formulation that can be applied to multi-modal data, which is an important problem in real-world phenomena. The paper proposes two different posterior approximation functions, one based on semantic coherence and the other based on joint data distribution. Both of these methods are incorporated into the objective, and the authors show that both of them perform well. The proposed method is compared to state-of-the-art models on a number of selfsupervised, generative learning tasks, and shows that the proposed method outperforms them.    The paper is well-written, well-motivated, and easy to follow. It is a good contribution to the field of machine learning research. "
936,SP:98004554447b82b3d2eb9724ec551250eec7a595,"Bayesian Optimization ( BO ) USED-FOR optimizing expensive black - box functions. priors USED-FOR PrBO. probabilistic model USED-FOR pseudo - posterior. BO USED-FOR pseudo - posterior. priors CONJUNCTION BO. BO CONJUNCTION priors. priors CONJUNCTION probabilistic model. probabilistic model CONJUNCTION priors. BO CONJUNCTION probabilistic model. probabilistic model CONJUNCTION BO. PrBO USED-FOR pseudo - posterior. probabilistic model USED-FOR PrBO. BO USED-FOR PrBO. priors USED-FOR PrBO. PrBO COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PrBO. PrBO COMPARE random search. random search COMPARE PrBO. state - of - the - art methods COMPARE random search. random search COMPARE state - of - the - art methods. real - world hardware design application EVALUATE-FOR PrBO. it USED-FOR misleading priors. OtherScientificTerm are function evaluations, machine learning hyperparameters, and user priors. Method are Prior - guided Bayesian Optimization ( PrBO ), and optimization process. ","This paper proposes Prior-guided Bayesian Optimization (PrBO) for optimizing expensive black-box functions, which is a generalization of the recently popular Bayes-guided optimization (BO) framework. PrBO uses priors from a probabilistic model to guide the pseudo-posterior of a function evaluations, and BO is used to optimize a pseudo-probabilistic posterior of the function. The authors show that PrBO can be combined with BO, priors, and a good probabilistically model to obtain a good pseudo-prior, and that it can avoid misleading priors. The paper also shows that the performance of PrBO is comparable to state-of-the-art methods, outperforms random search, and is more robust to machine learning hyperparameters.   The paper is well-written, well-motivated, and easy to follow. The experiments are conducted on a real-world hardware design application, where PrBO performs well in terms of performance, and it is shown that it is robust to the presence of user priors in the optimization process, and can be used to avoid misleading ones. ","This paper proposes Prior-guided Bayesian Optimization (PrBO) for optimizing expensive black-box functions, which is a generalization of the recently popular Bayes-guided optimization (BO) framework. PrBO uses priors from a probabilistic model to guide the pseudo-posterior of a function evaluations, and BO is used to optimize a pseudo-probabilistic posterior of the function. The authors show that PrBO can be combined with BO, priors, and a good probabilistically model to obtain a good pseudo-prior, and that it can avoid misleading priors. The paper also shows that the performance of PrBO is comparable to state-of-the-art methods, outperforms random search, and is more robust to machine learning hyperparameters.   The paper is well-written, well-motivated, and easy to follow. The experiments are conducted on a real-world hardware design application, where PrBO performs well in terms of performance, and it is shown that it is robust to the presence of user priors in the optimization process, and can be used to avoid misleading ones. "
945,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"Deep generative models USED-FOR real - world data. execution time FEATURE-OF computational cost. binary weights USED-FOR neural networks. binary neural networks USED-FOR generative models. binary neural networks USED-FOR generative models. computational cost EVALUATE-FOR models. techniques USED-FOR deep generative models. ResNet VAE and Flow++ models HYPONYM-OF deep generative models. Generic is they. Metric is complexity. Method are binary weight normalization, binarized generative models, binary models, and regular models. ","Deep generative models for real-world data can be seen as binary neural networks, where binary weights are used to normalize the weights of the neural networks. The authors show that the computational cost of such models can be reduced significantly in terms of execution time, and that they can be binarized to reduce the complexity of the binary weight normalization. They also show that binarization can be applied to existing deep generative model such as the ResNet VAE and Flow++ models, and show that these models can achieve similar computational cost with a much smaller computational cost.    The authors also propose two techniques to improve the performance of existing binary models. The first technique, binarize, is based on the observation that binary models tend to be more computationally efficient than regular models.  The second technique, binary weight regularization, is a simple modification to the existing binary weights used to regularize the binary models, which can be used to make the binary weights more efficient. ","Deep generative models for real-world data can be seen as binary neural networks, where binary weights are used to normalize the weights of the neural networks. The authors show that the computational cost of such models can be reduced significantly in terms of execution time, and that they can be binarized to reduce the complexity of the binary weight normalization. They also show that binarization can be applied to existing deep generative model such as the ResNet VAE and Flow++ models, and show that these models can achieve similar computational cost with a much smaller computational cost.    The authors also propose two techniques to improve the performance of existing binary models. The first technique, binarize, is based on the observation that binary models tend to be more computationally efficient than regular models.  The second technique, binary weight regularization, is a simple modification to the existing binary weights used to regularize the binary models, which can be used to make the binary weights more efficient. "
954,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"frameworks USED-FOR DL. adversarial training USED-FOR DL. approach USED-FOR DL. adversarial training USED-FOR approach. robustness EVALUATE-FOR DL. adversarial training USED-FOR robustness. norm - bounded perturbations FEATURE-OF DL. robustness EVALUATE-FOR approach. unbounded shifts in the data distribution FEATURE-OF DL. natural, out - of - distribution shifts FEATURE-OF robustness. perturbation - based adversarial robustness CONJUNCTION model - based robust deep learning. model - based robust deep learning CONJUNCTION perturbation - based adversarial robustness. paradigm USED-FOR models of natural variation. model - based robust training algorithms USED-FOR DL. robustness EVALUATE-FOR DL. robustness EVALUATE-FOR model - based robust training algorithms. adversarial training CONJUNCTION domain adaptation techniques. domain adaptation techniques CONJUNCTION adversarial training. ERM CONJUNCTION adversarial training. adversarial training CONJUNCTION ERM. classifiers COMPARE classifiers. classifiers COMPARE classifiers. algorithms COMPARE classifiers. classifiers COMPARE algorithms. domain adaptation techniques USED-FOR classifiers. ERM USED-FOR classifiers. algorithms USED-FOR classifiers. ERM USED-FOR classifiers. domain adaptation techniques USED-FOR classifiers. adversarial training USED-FOR classifiers. adversarial training USED-FOR classifiers. algorithms COMPARE baseline methods. baseline methods COMPARE algorithms. top-1 accuracy EVALUATE-FOR baseline methods. top-1 accuracy EVALUATE-FOR algorithms. Method is deep learning ( DL ). OtherScientificTerm are natural variation, data distribution, and natural conditions. Material are images, ImageNet, ImageNet - c, and natural, out - ofdistribution data. Generic are models, and methods. ","This paper studies the robustness of deep learning (DL) under natural variation, i.e. unbounded shifts in the data distribution. In this paper, the authors propose two frameworks for DL that are based on adversarial training. The first approach, perturbation-based adversarial robustness, is a general approach to improve the performance of DL under norm-bounded perturbations. The second approach, model-based robust deep learning, is an approach that uses adversarial learning to improve robustness under natural, out-of-distribution shifts.    The authors show that under this paradigm, models of natural variation are more robust to adversarial attacks than models that are trained under the same data distribution (e.g., ImageNet-c). They also show that robustness to robustness in DL is more robust under unbounded shift in data distribution than under natural conditions. The authors also provide a theoretical analysis of the relationship between robustness and robustness with respect to the number of data points in the training set.  The paper also shows that under the new paradigm, the robust performance of models trained under this new paradigm is better than under the existing paradigm.  Finally, the paper shows that the proposed paradigm can be used to train models of robustness for DL under a variety of model- based robust training algorithms, and shows that these robustness improves robustness even when the training data distribution is unbounded (i.e., under the assumption that there are no unbounded data points).   Experiments are conducted on ImageNet and CIFAR-10 and ImageNet. The results show that the classifiers trained with ERM, adversarial testing, and domain adaptation techniques are able to achieve better robustness against classifiers that are not trained using ERM or adversarial test-time robustness. The algorithms also show improved top-1 accuracy over baseline methods that do not use ERM/adversarial training but only train classifiers based on ERM. The paper concludes with a discussion of the importance of the naturalness of the data and the role of adversarial and model based methods.","This paper studies the robustness of deep learning (DL) under natural variation, i.e. unbounded shifts in the data distribution. In this paper, the authors propose two frameworks for DL that are based on adversarial training. The first approach, perturbation-based adversarial robustness, is a general approach to improve the performance of DL under norm-bounded perturbations. The second approach, model-based robust deep learning, is an approach that uses adversarial learning to improve robustness under natural, out-of-distribution shifts.    The authors show that under this paradigm, models of natural variation are more robust to adversarial attacks than models that are trained under the same data distribution (e.g., ImageNet-c). They also show that robustness to robustness in DL is more robust under unbounded shift in data distribution than under natural conditions. The authors also provide a theoretical analysis of the relationship between robustness and robustness with respect to the number of data points in the training set.  The paper also shows that under the new paradigm, the robust performance of models trained under this new paradigm is better than under the existing paradigm.  Finally, the paper shows that the proposed paradigm can be used to train models of robustness for DL under a variety of model- based robust training algorithms, and shows that these robustness improves robustness even when the training data distribution is unbounded (i.e., under the assumption that there are no unbounded data points).   Experiments are conducted on ImageNet and CIFAR-10 and ImageNet. The results show that the classifiers trained with ERM, adversarial testing, and domain adaptation techniques are able to achieve better robustness against classifiers that are not trained using ERM or adversarial test-time robustness. The algorithms also show improved top-1 accuracy over baseline methods that do not use ERM/adversarial training but only train classifiers based on ERM. The paper concludes with a discussion of the importance of the naturalness of the data and the role of adversarial and model based methods."
963,SP:011dab90d225550e77235cbec1615e583ae3297e,polynomial complexity FEATURE-OF exact convex optimization formulations. ReLU activations USED-FOR Convolutional Neural Networks ( CNNs ). convex analytic framework USED-FOR convex optimization problems. convex optimization problems USED-FOR twoand three - layer CNN architectures. semi - infinite duality USED-FOR convex analytic framework. ` 2 norm regularized convex program USED-FOR two - layer CNNs. ` 1 regularized convex program USED-FOR sparsity. spectral domain FEATURE-OF sparsity. ` 1 regularized convex program USED-FOR multi - layer circular CNN training problems. ReLU layer USED-FOR multi - layer circular CNN training problems. ReLU layers USED-FOR three - layer CNNs. approach USED-FOR pooling methods. convex regularizers USED-FOR implicit architectural bias. OtherScientificTerm is data dimension. ,"This paper studies the convex optimization of ReLU activations in Convolutional Neural Networks (CNNs) and its polynomial complexity. The authors propose a new convex analytic framework for convex optimizing problems for twoand three-layer CNN architectures with semi-infinite duality. They show that the exact concave optimization formulations have a polynomial complexity with respect to the data dimension. They also show that a `2 norm regularized convex program is sufficient to obtain sparsity in the spectral domain for two-layer convolutional neural networks, and a `1 regularization of the sparsity of a single ReLU layer for multi-layer circular CNN training problems. Finally, the authors show that for ReLU layers in three-layered CNNs with ReLU activation, their approach can be used as a regularizer for pooling methods. They further show that convex regularizers mitigate the implicit architectural bias in the case where the number of layers is large enough. ","This paper studies the convex optimization of ReLU activations in Convolutional Neural Networks (CNNs) and its polynomial complexity. The authors propose a new convex analytic framework for convex optimizing problems for twoand three-layer CNN architectures with semi-infinite duality. They show that the exact concave optimization formulations have a polynomial complexity with respect to the data dimension. They also show that a `2 norm regularized convex program is sufficient to obtain sparsity in the spectral domain for two-layer convolutional neural networks, and a `1 regularization of the sparsity of a single ReLU layer for multi-layer circular CNN training problems. Finally, the authors show that for ReLU layers in three-layered CNNs with ReLU activation, their approach can be used as a regularizer for pooling methods. They further show that convex regularizers mitigate the implicit architectural bias in the case where the number of layers is large enough. "
972,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"rich sensory modalities USED-FOR Robotic manipulation tasks. Human - robot interaction USED-FOR teaching robots. probabilistic generative model USED-FOR optimisation problem. high - capacity neural network USED-FOR model. latent variables USED-FOR model. latent variables CONJUNCTION high - level notions. high - level notions CONJUNCTION latent variables. table - top robot manipulation tasks EVALUATE-FOR approach. PR2 robot USED-FOR table - top robot manipulation tasks. visual information CONJUNCTION arm joint positions. arm joint positions CONJUNCTION visual information. arm joint positions CONJUNCTION arm joint efforts. arm joint efforts CONJUNCTION arm joint positions. robot USED-FOR visual information. robot USED-FOR arm joint positions. arm joint efforts FEATURE-OF robot. OtherScientificTerm are soft sponge, restricted vocabulary, and sponge. Material is rich data streams. Generic are alignment, and tasks. ","Robotic manipulation tasks with rich sensory modalities require the use of a soft sponge. Human-robot interaction is an important problem in teaching robots to learn from rich data streams. The authors propose to use a probabilistic generative model to solve the optimisation problem. The model is trained using a high-capacity neural network.  The model uses latent variables, high-level notions, and a combination of latent variables.   The approach is evaluated on table-top robot manipulation tasks on a PR2 robot. The robot is trained to align visual information from the robot, arm joint positions, and arm joint efforts, and is trained with a restricted vocabulary. The paper shows that the alignment of the robot’s visual information with the visual information of a robot trained with the same robot is beneficial to achieve better performance on the tasks. The main contribution of the paper is that the model is able to learn to align the robot with the soft sponge without the need for a specific vocabulary. ","Robotic manipulation tasks with rich sensory modalities require the use of a soft sponge. Human-robot interaction is an important problem in teaching robots to learn from rich data streams. The authors propose to use a probabilistic generative model to solve the optimisation problem. The model is trained using a high-capacity neural network.  The model uses latent variables, high-level notions, and a combination of latent variables.   The approach is evaluated on table-top robot manipulation tasks on a PR2 robot. The robot is trained to align visual information from the robot, arm joint positions, and arm joint efforts, and is trained with a restricted vocabulary. The paper shows that the alignment of the robot’s visual information with the visual information of a robot trained with the same robot is beneficial to achieve better performance on the tasks. The main contribution of the paper is that the model is able to learn to align the robot with the soft sponge without the need for a specific vocabulary. "
981,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,low - resource scenarios FEATURE-OF overfitting. tasks EVALUATE-FOR large - scale pretrained language models. general - purpose feature extractors USED-FOR models. Variational Information Bottleneck ( VIB ) USED-FOR irrelevant features. Variational Information Bottleneck ( VIB ) USED-FOR fine - tuning. method USED-FOR overfitting. fine - tuning USED-FOR low - resource target tasks. Variational Information Bottleneck ( VIB ) USED-FOR low - resource target tasks. VIB model USED-FOR sentence representations. natural language inference datasets USED-FOR sentence representations. generalization EVALUATE-FOR VIB model. low - resource datasets EVALUATE-FOR method. method USED-FOR transfer learning. low - resource scenarios FEATURE-OF transfer learning. low - resource scenarios EVALUATE-FOR method. generalization EVALUATE-FOR it. Generic is they. OtherScientificTerm is features. Material is out - of - domain datasets. ,"This paper studies the problem of overfitting in low-resource scenarios for large-scale pretrained language models on tasks where the models are trained with general-purpose feature extractors. The authors propose a method called Variational Information Bottleneck (VIB) for fine-tuning to reduce the impact of irrelevant features in order to prevent overfitting. The VIB model is trained to learn sentence representations on natural language inference datasets and transfer them to out-of-domain datasets. The method is shown to be effective for transfer learning in both high-resource (i.e., high-quality) and low-resourced scenarios. In addition, it is shown that it improves the generalization performance and transfer learning performance when the number of features is limited. ","This paper studies the problem of overfitting in low-resource scenarios for large-scale pretrained language models on tasks where the models are trained with general-purpose feature extractors. The authors propose a method called Variational Information Bottleneck (VIB) for fine-tuning to reduce the impact of irrelevant features in order to prevent overfitting. The VIB model is trained to learn sentence representations on natural language inference datasets and transfer them to out-of-domain datasets. The method is shown to be effective for transfer learning in both high-resource (i.e., high-quality) and low-resourced scenarios. In addition, it is shown that it improves the generalization performance and transfer learning performance when the number of features is limited. "
990,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,Natural images HYPONYM-OF projections of 3D objects. 2D image plane FEATURE-OF projections of 3D objects. they USED-FOR 3D object structures. 2D generative models USED-FOR natural image manifold. GANs HYPONYM-OF 2D generative models. knowledge USED-FOR 3D shapes of objects. 2D GAN USED-FOR 3D geometric cues. RGB images USED-FOR 2D GAN. pre - trained GAN USED-FOR 3D shape. rich 3D knowledge PART-OF pre - trained GAN. unsupervised manner USED-FOR 3D shape. iterative strategy USED-FOR diverse viewpoint and lighting variations. diverse viewpoint and lighting variations FEATURE-OF GAN image manifold. iterative strategy USED-FOR framework. 2D keypoint CONJUNCTION 3D annotations. 3D annotations CONJUNCTION 2D keypoint. cars CONJUNCTION buildings. buildings CONJUNCTION cars. it USED-FOR 3D shapes. human faces CONJUNCTION cars. cars CONJUNCTION human faces. precision FEATURE-OF 3D shapes. precision EVALUATE-FOR it. relighting CONJUNCTION object rotation. object rotation CONJUNCTION relighting. 3D shapes USED-FOR image editing. object rotation HYPONYM-OF image editing. relighting HYPONYM-OF image editing. 3D shape reconstruction CONJUNCTION face rotation. face rotation CONJUNCTION 3D shape reconstruction. approach COMPARE methods. methods COMPARE approach. methods USED-FOR face rotation. approach USED-FOR face rotation. methods USED-FOR 3D shape reconstruction. approach USED-FOR 3D shape reconstruction. OtherScientificTerm is object shapes. ,"Natural images are projections of 3D objects on the 2D image plane, and they can be used to learn 3D object structures. 2D generative models (e.g. GANs) have been shown to capture the natural image manifold, and this paper proposes to use this knowledge to learn the 3D shapes of objects. A 2D GAN is trained to capture 3D geometric cues from RGB images. A pre-trained GAN with rich 3D knowledge is used to generate a 3D shape from a 2D keypoint and 3D annotations. In an unsupervised manner, the 3d shape is learned in a similar way to human faces, cars, and buildings. The framework is built on top of an iterative strategy to learn diverse viewpoint and lighting variations on the GAN image manifold. The paper shows that it can learn to learn to reconstruct 3D forms of objects with high precision, and it can also be used for image editing (re-lighting, relighting, and object rotation). Experiments show that the proposed approach outperforms existing methods for 3D Shape reconstruction and face rotation.  ","Natural images are projections of 3D objects on the 2D image plane, and they can be used to learn 3D object structures. 2D generative models (e.g. GANs) have been shown to capture the natural image manifold, and this paper proposes to use this knowledge to learn the 3D shapes of objects. A 2D GAN is trained to capture 3D geometric cues from RGB images. A pre-trained GAN with rich 3D knowledge is used to generate a 3D shape from a 2D keypoint and 3D annotations. In an unsupervised manner, the 3d shape is learned in a similar way to human faces, cars, and buildings. The framework is built on top of an iterative strategy to learn diverse viewpoint and lighting variations on the GAN image manifold. The paper shows that it can learn to learn to reconstruct 3D forms of objects with high precision, and it can also be used for image editing (re-lighting, relighting, and object rotation). Experiments show that the proposed approach outperforms existing methods for 3D Shape reconstruction and face rotation.  "
999,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"recognition methods USED-FOR imbalanced classification. tail accuracies CONJUNCTION head accuracies. head accuracies CONJUNCTION tail accuracies. class re - balancing / re - weighting USED-FOR recognition methods. model bias CONJUNCTION variance analysis. variance analysis CONJUNCTION model bias. RoutIng Diverse Experts ( RIDE ) HYPONYM-OF long - tailed classifier. It USED-FOR model variance. It USED-FOR model bias. computational cost EVALUATE-FOR dynamic expert routing module. distribution - aware diversity loss USED-FOR model bias. CIFAR100 - LT CONJUNCTION ImageNet - LT. ImageNet - LT CONJUNCTION CIFAR100 - LT. ImageNet - LT CONJUNCTION iNaturalist 2018 benchmarks. iNaturalist 2018 benchmarks CONJUNCTION ImageNet - LT. RIDE COMPARE state - of - the - art. state - of - the - art COMPARE RIDE. ImageNet - LT EVALUATE-FOR RIDE. iNaturalist 2018 benchmarks EVALUATE-FOR RIDE. CIFAR100 - LT EVALUATE-FOR RIDE. CIFAR100 - LT EVALUATE-FOR state - of - the - art. backbone networks CONJUNCTION long - tailed algorithms. long - tailed algorithms CONJUNCTION backbone networks. universal framework USED-FOR backbone networks. universal framework USED-FOR long - tailed algorithms. long - tailed algorithms CONJUNCTION training mechanisms. training mechanisms CONJUNCTION long - tailed algorithms. It HYPONYM-OF universal framework. It USED-FOR backbone networks. Material are Natural data, and tail data. OtherScientificTerm are dynamic view of the training data, hard negatives, and tail. Method is long - tail classifiers. Metric is head - tail model bias gap. ","This paper studies the problem of imbalanced classification in recognition methods that rely on class re-balancing/re-weighting. Natural data (tail data) and long-tailed data (head data) are often imbalanced in the sense that tail accuracies are higher than head accuracies, while long-tail classifiers tend to be more robust to hard negatives. The authors propose a new long-tailed classifier called RoutIng Diverse Experts (RIDE), which is a dynamic view of the training data. It aims to reduce the model bias and variance analysis between tail data and head data by learning a distribution-aware diversity loss. It also reduces the model variance by reducing the computational cost of the dynamic expert routing module. RIDE is evaluated on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks, and is shown to outperform the state-of-the-art. It is also shown to be a universal framework that can be applied to backbone networks as well as long-tails algorithms and training mechanisms.    The main contribution of this paper is to study the head-tail model bias gap, which is the difference between the head and tail data. ","This paper studies the problem of imbalanced classification in recognition methods that rely on class re-balancing/re-weighting. Natural data (tail data) and long-tailed data (head data) are often imbalanced in the sense that tail accuracies are higher than head accuracies, while long-tail classifiers tend to be more robust to hard negatives. The authors propose a new long-tailed classifier called RoutIng Diverse Experts (RIDE), which is a dynamic view of the training data. It aims to reduce the model bias and variance analysis between tail data and head data by learning a distribution-aware diversity loss. It also reduces the model variance by reducing the computational cost of the dynamic expert routing module. RIDE is evaluated on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks, and is shown to outperform the state-of-the-art. It is also shown to be a universal framework that can be applied to backbone networks as well as long-tails algorithms and training mechanisms.    The main contribution of this paper is to study the head-tail model bias gap, which is the difference between the head and tail data. "
1008,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"pruning criteria USED-FOR redundant filters. Channel pruning USED-FOR compressing convolutional neural networks ( CNNs ). pruning criteria USED-FOR filters ’ Importance Score. layer - wise pruning CONJUNCTION global pruning. global pruning CONJUNCTION layer - wise pruning. layer - wise pruning USED-FOR pruning criteria. global pruning USED-FOR pruning criteria. Gaussian - alike distribution FEATURE-OF convolutional filters. Material is convolutional neural networks ( CNNs ). Generic is criteria. OtherScientificTerm are pruned structures, and Convolutional Weight Distribution Assumption. Method is ` 1 and ` 2 pruning. ",Channel pruning is a popular technique for compressing convolutional neural networks (CNNs). This paper proposes a new pruning criteria for reducing redundant filters’ Importance Score. The proposed criteria is based on layer-wise pruning and global pruning. The authors provide a theoretical analysis of the pruned structures and show that the proposed pruning criterion is consistent with the Convolutional Weight Distribution Assumption. The paper also provides empirical evidence that the pruning based on the proposed `1 and `2 pruning results in a Gaussian-like distribution for the convolutionsal filters. ,Channel pruning is a popular technique for compressing convolutional neural networks (CNNs). This paper proposes a new pruning criteria for reducing redundant filters’ Importance Score. The proposed criteria is based on layer-wise pruning and global pruning. The authors provide a theoretical analysis of the pruned structures and show that the proposed pruning criterion is consistent with the Convolutional Weight Distribution Assumption. The paper also provides empirical evidence that the pruning based on the proposed `1 and `2 pruning results in a Gaussian-like distribution for the convolutionsal filters. 
1017,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"code completion CONJUNCTION code summarization. code summarization CONJUNCTION code completion. code search CONJUNCTION code completion. code completion CONJUNCTION code search. Pre - trained models USED-FOR programming language. Pre - trained models USED-FOR code - related tasks. code search HYPONYM-OF code - related tasks. code summarization HYPONYM-OF code - related tasks. code completion HYPONYM-OF code - related tasks. code snippet USED-FOR pre - trained models. pre - trained model USED-FOR programming language. GraphCodeBERT HYPONYM-OF pre - trained model. inherent structure of code USED-FOR pre - trained model. data flow USED-FOR pre - training stage. data flow USED-FOR semantic - level structure of code. abstract syntax tree ( AST ) HYPONYM-OF syntactic - level structure of code. Transformer USED-FOR GraphCodeBERT. graph - guided masked attention function USED-FOR code structure. graph - guided masked attention function USED-FOR model. code translation CONJUNCTION code refinement. code refinement CONJUNCTION code translation. clone detection CONJUNCTION code translation. code translation CONJUNCTION clone detection. code search CONJUNCTION clone detection. clone detection CONJUNCTION code search. tasks EVALUATE-FOR model. code refinement HYPONYM-OF tasks. code search HYPONYM-OF tasks. clone detection HYPONYM-OF tasks. code translation HYPONYM-OF tasks. pre - training tasks USED-FOR GraphCodeBERT. code structure CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION code structure. code structure USED-FOR GraphCodeBERT. structure - level attentions COMPARE token - level attentions. token - level attentions COMPARE structure - level attentions. model COMPARE token - level attentions. token - level attentions COMPARE model. structure - level attentions USED-FOR model. OtherScientificTerm are code semantics, semantic - level structure, hierarchy of AST, and code structure edges. Task are code understanding process, masked language modeling, and structure - aware pre - training tasks. Generic is downstream tasks. ","This paper proposes a pre-trained model, GraphCodeBERT, for code-related tasks (code search, code completion, code summarization). Pre-trained models for any programming language can be trained on any code snippet, but the inherent structure of code can be a significant obstacle to the code understanding process. To address this issue, the authors propose to use the inherent structural structure of the code to guide the pre-training stage.    The authors propose a data flow to learn the semantic-level structure of a code, which is the abstract syntax tree (AST) of the program. The authors show that the semantic level of code semantics can be learned through the data flow, and that the code structure of any program can be encoded as a graph, which can be used to learn a model that can be applied to any code language.  The proposed model, called GraphCodeberT, is based on a Transformer, and the model uses a graph-guided masked attention function to learn code structure. The model is trained on a set of tasks (cloned detection, code translation, code refinement, and code translation), and is evaluated on these tasks. The paper shows that the proposed model is able to generalize well to new tasks, and can generalize to new code structures.  In addition, the paper also shows that graph-level attentions of the model are more powerful than token-level attention, which the authors claim is due to the fact that the model learns the structure-level information from the structure of each code structure (e.g., code structure edges).   - The authors also show that their model generalizes well to different types of code structures, and to different downstream tasks.  - They show that a model trained on the same code structure is more robust to masked language modeling, and more robust than a model pretrained on a different code structure, but not on a new code structure - They also show how their model can generalise to a different type of code structure and to a new type of tasks. - They demonstrate that the learned code structure helps the model generalize better to a variety of tasks, including code search, clone detection, etc. - The model also shows how the learned by the model is more generalizable to unseen code structures than to unseen structures.","This paper proposes a pre-trained model, GraphCodeBERT, for code-related tasks (code search, code completion, code summarization). Pre-trained models for any programming language can be trained on any code snippet, but the inherent structure of code can be a significant obstacle to the code understanding process. To address this issue, the authors propose to use the inherent structural structure of the code to guide the pre-training stage.    The authors propose a data flow to learn the semantic-level structure of a code, which is the abstract syntax tree (AST) of the program. The authors show that the semantic level of code semantics can be learned through the data flow, and that the code structure of any program can be encoded as a graph, which can be used to learn a model that can be applied to any code language.  The proposed model, called GraphCodeberT, is based on a Transformer, and the model uses a graph-guided masked attention function to learn code structure. The model is trained on a set of tasks (cloned detection, code translation, code refinement, and code translation), and is evaluated on these tasks. The paper shows that the proposed model is able to generalize well to new tasks, and can generalize to new code structures.  In addition, the paper also shows that graph-level attentions of the model are more powerful than token-level attention, which the authors claim is due to the fact that the model learns the structure-level information from the structure of each code structure (e.g., code structure edges).   - The authors also show that their model generalizes well to different types of code structures, and to different downstream tasks.  - They show that a model trained on the same code structure is more robust to masked language modeling, and more robust than a model pretrained on a different code structure, but not on a new code structure - They also show how their model can generalise to a different type of code structure and to a new type of tasks. - They demonstrate that the learned code structure helps the model generalize better to a variety of tasks, including code search, clone detection, etc. - The model also shows how the learned by the model is more generalizable to unseen code structures than to unseen structures."
1026,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"drug discovery CONJUNCTION material design. material design CONJUNCTION drug discovery. drug discovery HYPONYM-OF scientific fields. material design HYPONYM-OF scientific fields. distribution FEATURE-OF labeled result data. it USED-FOR regression model. skewed data USED-FOR it. approach USED-FOR regression models. accuracy EVALUATE-FOR regression models. accuracy EVALUATE-FOR approach. skewed dataset USED-FOR regression models. forcing algorithm USED-FOR regression. domain knowledge USED-FOR true distribution. neural networks USED-FOR regression model. pLogP CONJUNCTION Diamond. Diamond CONJUNCTION pLogP. pLogP HYPONYM-OF real - world datasets. real - world datasets EVALUATE-FOR approach. Diamond HYPONYM-OF real - world datasets. datasets EVALUATE-FOR approach. approach COMPARE regression models. regression models COMPARE approach. root mean squared error EVALUATE-FOR regression. root mean squared error EVALUATE-FOR regression models. datasets EVALUATE-FOR regression models. adjustment of the distribution USED-FOR regression models. regression EVALUATE-FOR approach. root mean squared error EVALUATE-FOR approach. Generic is method. OtherScientificTerm are regression outputs, and estimated ‘ true ’ distribution. Material is unlabeled data. Method is adversarial network. ","This paper proposes a method to train regression models on a skewed dataset, where the distribution of the labeled result data has a different distribution from that of the unlabeled data. Two scientific fields, drug discovery and material design, are considered, and it is shown that it is possible to train a regression model on skewed data and improve the accuracy of the regression models trained on the skewed dataset. The proposed approach trains regression models with the goal of improving the accuracy on regression models that are trained with the aim of improving accuracy on the true distribution. The authors propose a forcing algorithm to train the regression model using neural networks. The idea is to train an adversarial network that tries to force the regression outputs to be close to the estimated ‘true’ distribution, and then use domain knowledge to learn a true distribution that can be used to adjust the regression. Experiments on two real-world datasets, pLogP and Diamond, show that the proposed approach improves the root mean squared error of regression on both datasets, and the regression on regression with adjustment of the distribution is more accurate than regression without adjustment.","This paper proposes a method to train regression models on a skewed dataset, where the distribution of the labeled result data has a different distribution from that of the unlabeled data. Two scientific fields, drug discovery and material design, are considered, and it is shown that it is possible to train a regression model on skewed data and improve the accuracy of the regression models trained on the skewed dataset. The proposed approach trains regression models with the goal of improving the accuracy on regression models that are trained with the aim of improving accuracy on the true distribution. The authors propose a forcing algorithm to train the regression model using neural networks. The idea is to train an adversarial network that tries to force the regression outputs to be close to the estimated ‘true’ distribution, and then use domain knowledge to learn a true distribution that can be used to adjust the regression. Experiments on two real-world datasets, pLogP and Diamond, show that the proposed approach improves the root mean squared error of regression on both datasets, and the regression on regression with adjustment of the distribution is more accurate than regression without adjustment."
1035,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"Compositional generalization HYPONYM-OF algebraic capacity. known components USED-FOR algebraic capacity. human intelligence USED-FOR out - of - distribution generalization. It PART-OF human intelligence. It USED-FOR out - of - distribution generalization. model USED-FOR representations. representations USED-FOR inference. regularized hidden representations USED-FOR auxiliary reconstruction network. approach COMPARE baselines. baselines COMPARE approach. accuracy EVALUATE-FOR approach. compositional representations USED-FOR it. compositional generalization CONJUNCTION artificial intelligence research. artificial intelligence research CONJUNCTION compositional generalization. Method are neural networks, and extraction network. OtherScientificTerm are extraction ability, divergence of distributions, and compositionality. Task is transferability of compositionality. ","This paper studies the problem of out-of-distribution generalization (OODG) in neural networks. In particular, the authors focus on the algebraic capacity of neural networks with known components, i.e., compositional generalization. It is an important problem in human intelligence, and has been a focus of recent advances in artificial intelligence research.  It is well known that human intelligence is able to learn representations that generalize well to unseen data, but the extraction ability is limited by the divergence of distributions.  This paper proposes an approach to improve the transferability of compositionality in the extraction network. The authors propose an auxiliary reconstruction network that is based on regularized hidden representations. The idea is that the model learns representations that can be used for inference.  The authors show that the proposed approach improves the accuracy of their approach over several baselines. They also show that it can be combined with compositional representations.   The paper is well-written and well-motivated, and the results are interesting.  However, there are a few issues that need to be addressed:  (1) The authors do not provide sufficient discussion of the connection between compositional generality and compositional learning.  (2) The paper does not provide a clear discussion of how the authors define compositionality. ","This paper studies the problem of out-of-distribution generalization (OODG) in neural networks. In particular, the authors focus on the algebraic capacity of neural networks with known components, i.e., compositional generalization. It is an important problem in human intelligence, and has been a focus of recent advances in artificial intelligence research.  It is well known that human intelligence is able to learn representations that generalize well to unseen data, but the extraction ability is limited by the divergence of distributions.  This paper proposes an approach to improve the transferability of compositionality in the extraction network. The authors propose an auxiliary reconstruction network that is based on regularized hidden representations. The idea is that the model learns representations that can be used for inference.  The authors show that the proposed approach improves the accuracy of their approach over several baselines. They also show that it can be combined with compositional representations.   The paper is well-written and well-motivated, and the results are interesting.  However, there are a few issues that need to be addressed:  (1) The authors do not provide sufficient discussion of the connection between compositional generality and compositional learning.  (2) The paper does not provide a clear discussion of how the authors define compositionality. "
1044,SP:ffab573a977c819e86601de74690c29a39c264cd,"Poisoning attacks USED-FOR Reinforcement Learning ( RL ) systems. RL algorithm USED-FOR Poisoning attacks. poisoning methods USED-FOR supervised learning. supervised learning USED-FOR RL. poisoning methods USED-FOR RL. generic poisoning framework USED-FOR online RL. heterogeneous poisoning models USED-FOR RL. heterogeneous poisoning models USED-FOR generic poisoning framework. poisoning method USED-FOR policy - based RL agents. strategic poisoning algorithm USED-FOR on - policy deep RL agents. stability radius FEATURE-OF RL. stability radius HYPONYM-OF metric. stability radius USED-FOR VA2C - P. metric USED-FOR VA2C - P. Task are learning, and poisoning RL. OtherScientificTerm are Markov Decision Process ( MDP ), MDP, and limited attacking budget. Method are RL algorithms, and poisoning algorithm. Material is deep RL agents. ","Poisoning attacks on Reinforcement Learning (RL) systems is an important topic of interest. Poisoning attacks can be applied to any RL algorithm, and poisoning methods have been shown to be effective for supervised learning for RL. However, poisoning methods for RL are not well-studied, and there is a lack of theoretical analysis of poisoning methods in RL. This paper proposes a generic poisoning framework for online RL based on heterogeneous poisoning models, which can be used to attack any Markov Decision Process (MDP). The authors propose a poisoning method to attack policy-based RL agents, and show that poisoning RL is effective when the MDP has a limited attacking budget. The authors also propose a strategic poisoning algorithm to attack on-policy deep RL agents. They show that VA2C-P is robust to poisoning based on a new metric called stability radius, which is a measure of the stability radius of RL algorithms. They also show that the proposed poisoning algorithm is more effective than existing poisoning algorithms.   The paper is well-written and well-motivated, and the authors have done a good job of providing theoretical analysis and empirical evidence to support their claim that poisoning methods are effective for RL, and that the poisoning method is more robust than existing ones.  However, the paper is lacking in theoretical analysis, and it is not clear to me that the authors provide any new insights into poisoning RL. The paper also does not provide any empirical evidence that poisoning algorithms are more effective at learning than existing methods, and does not present any theoretical analysis to show the effectiveness of poisoning RL agents in general.","Poisoning attacks on Reinforcement Learning (RL) systems is an important topic of interest. Poisoning attacks can be applied to any RL algorithm, and poisoning methods have been shown to be effective for supervised learning for RL. However, poisoning methods for RL are not well-studied, and there is a lack of theoretical analysis of poisoning methods in RL. This paper proposes a generic poisoning framework for online RL based on heterogeneous poisoning models, which can be used to attack any Markov Decision Process (MDP). The authors propose a poisoning method to attack policy-based RL agents, and show that poisoning RL is effective when the MDP has a limited attacking budget. The authors also propose a strategic poisoning algorithm to attack on-policy deep RL agents. They show that VA2C-P is robust to poisoning based on a new metric called stability radius, which is a measure of the stability radius of RL algorithms. They also show that the proposed poisoning algorithm is more effective than existing poisoning algorithms.   The paper is well-written and well-motivated, and the authors have done a good job of providing theoretical analysis and empirical evidence to support their claim that poisoning methods are effective for RL, and that the poisoning method is more robust than existing ones.  However, the paper is lacking in theoretical analysis, and it is not clear to me that the authors provide any new insights into poisoning RL. The paper also does not provide any empirical evidence that poisoning algorithms are more effective at learning than existing methods, and does not present any theoretical analysis to show the effectiveness of poisoning RL agents in general."
1053,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,Checkpointing USED-FOR deep learning models. restricted memory budgets FEATURE-OF deep learning models. static computation graphs USED-FOR checkpointing techniques. greedy online algorithm USED-FOR checkpointing. Dynamic Tensor Rematerialization ( DTR ) HYPONYM-OF greedy online algorithm. Dynamic Tensor Rematerialization ( DTR ) USED-FOR online algorithm. DTR USED-FOR anN -layer linear feedforward network. O(N ) tensor operations USED-FOR DTR. Ω ( √ N ) memory budget FEATURE-OF anN -layer linear feedforward network. DTR COMPARE optimal static checkpointing. optimal static checkpointing COMPARE DTR. tensor allocations CONJUNCTION operator calls. operator calls CONJUNCTION tensor allocations. DTR prototype PART-OF PyTorch. lightweight metadata PART-OF tensors. OtherScientificTerm is eviction policy. Method is dynamic models. ,"This paper studies the problem of checkpointing for deep learning models with restricted memory budgets. Previous checkpointing techniques are based on static computation graphs, and the authors propose a greedy online algorithm called Dynamic Tensor Rematerialization (DTR) for checkpointing, which is based on a dynamic online algorithm. DTR uses O(N) tensor operations to compute an anN-layer linear feedforward network with a Ω(√N) memory budget. The authors show that DTR is more efficient than optimal static checkpointing in terms of the number of tensor allocations and operator calls. They also show that the DTR prototype in PyTorch can be integrated into existing dynamic models and that it can be used as an alternative to the standard eviction policy. The paper also shows that the tensors can be decomposed into lightweight metadata, which can be added to existing checkpoints. ","This paper studies the problem of checkpointing for deep learning models with restricted memory budgets. Previous checkpointing techniques are based on static computation graphs, and the authors propose a greedy online algorithm called Dynamic Tensor Rematerialization (DTR) for checkpointing, which is based on a dynamic online algorithm. DTR uses O(N) tensor operations to compute an anN-layer linear feedforward network with a Ω(√N) memory budget. The authors show that DTR is more efficient than optimal static checkpointing in terms of the number of tensor allocations and operator calls. They also show that the DTR prototype in PyTorch can be integrated into existing dynamic models and that it can be used as an alternative to the standard eviction policy. The paper also shows that the tensors can be decomposed into lightweight metadata, which can be added to existing checkpoints. "
1062,SP:20efc610911443724b56f57f857060d0e0302243,"manually annotated evaluation sets USED-FOR task. method USED-FOR hallucination detection. synthetic data USED-FOR pretrained language models. pretrained language models USED-FOR method. machine translation CONJUNCTION abstract text summarization. abstract text summarization CONJUNCTION machine translation. machine translation EVALUATE-FOR approach. abstract text summarization EVALUATE-FOR approach. average F1 EVALUATE-FOR benchmark datasets. average F1 EVALUATE-FOR approach. token - level hallucination labels USED-FOR fine - grained loss. fine - grained loss PART-OF low - resource machine translation. Method is Neural sequence models. Generic are they, model, and baseline methods. OtherScientificTerm is automatically inserted hallucinations. Material is annotated data. ","This paper proposes a method for hallucination detection based on neural sequence models. Neural sequence models are trained on manually annotated evaluation sets for a given task, where the task is to detect a hallucination from a set of annotated data. The proposed method is based on pretrained language models trained on synthetic data, where they are trained to detect automatically inserted hallucinations.   The proposed approach is evaluated on machine translation and abstract text summarization, and shows that the proposed approach achieves an average F1 of $O(1/\sqrt{F1})$ on both benchmark datasets. The authors also propose a fine-grained loss that incorporates token-level hallucination labels into low-resource machine translation, and show that their model is able to detect hallucination in the same way as other baseline methods. ","This paper proposes a method for hallucination detection based on neural sequence models. Neural sequence models are trained on manually annotated evaluation sets for a given task, where the task is to detect a hallucination from a set of annotated data. The proposed method is based on pretrained language models trained on synthetic data, where they are trained to detect automatically inserted hallucinations.   The proposed approach is evaluated on machine translation and abstract text summarization, and shows that the proposed approach achieves an average F1 of $O(1/\sqrt{F1})$ on both benchmark datasets. The authors also propose a fine-grained loss that incorporates token-level hallucination labels into low-resource machine translation, and show that their model is able to detect hallucination in the same way as other baseline methods. "
1071,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"Conditional Generative Adversarial Networks ( cGAN ) USED-FOR images. idea USED-FOR architecture. NAS USED-FOR architecture. NAS USED-FOR idea. reduction of training data USED-FOR class generator. latter USED-FOR class - specific information. regular and class - modulated convolutions PART-OF search space. weight - sharing pipeline CONJUNCTION mixed - architecture optimization. mixed - architecture optimization CONJUNCTION weight - sharing pipeline. weight - sharing pipeline USED-FOR search algorithm. Markov decision process PART-OF search algorithm. Markov decision process USED-FOR sampling policy. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 EVALUATE-FOR approach. CIFAR10 EVALUATE-FOR approach. FID scores FEATURE-OF image generation quality. OtherScientificTerm are class - level distributions, and generating architecture. Metric is search cost. Method are moving average, and cGAN models. ","This paper proposes Conditional Generative Adversarial Networks (cGAN) to generate images from images with different class-level distributions. The idea is based on NAS, and the authors propose a new architecture based on the idea of weight-sharing in NAS. Specifically, the reduction of training data for each class generator is used to reduce the search cost. The search space is divided into regular and class-modulated convolutions, with the latter being used to capture class-specific information. The proposed search algorithm uses a weight-sharing pipeline and mixed-architecture optimization. The Markov decision process of the search algorithm is used as a sampling policy, and a moving average is used for the final selection of the generating architecture. The authors evaluate the proposed approach on CIFAR10 and CIFar100, and show that their approach improves the FID scores of image generation quality.   ","This paper proposes Conditional Generative Adversarial Networks (cGAN) to generate images from images with different class-level distributions. The idea is based on NAS, and the authors propose a new architecture based on the idea of weight-sharing in NAS. Specifically, the reduction of training data for each class generator is used to reduce the search cost. The search space is divided into regular and class-modulated convolutions, with the latter being used to capture class-specific information. The proposed search algorithm uses a weight-sharing pipeline and mixed-architecture optimization. The Markov decision process of the search algorithm is used as a sampling policy, and a moving average is used for the final selection of the generating architecture. The authors evaluate the proposed approach on CIFAR10 and CIFar100, and show that their approach improves the FID scores of image generation quality.   "
1080,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,estimation of causal effects USED-FOR Decision - making. observational data USED-FOR estimation of causal effects. regularization framework USED-FOR unconfoundedness. orthogonality constraint USED-FOR unconfoundedness. asymptotically normal estimator USED-FOR average causal effect. estimators COMPARE asymptotic variance. asymptotic variance COMPARE estimators. regularization framework USED-FOR deep orthogonal networks. deep orthogonal networks USED-FOR unconfounded treatments ( DONUT ). DONUT COMPARE state - of - the - art. state - of - the - art COMPARE DONUT. benchmark datasets EVALUATE-FOR DONUT. benchmark datasets USED-FOR causal inference. benchmark datasets EVALUATE-FOR state - of - the - art. OtherScientificTerm is treatment assignment. ,"This paper studies the estimation of causal effects in observational data for Decision-making. The authors propose a new regularization framework for deep orthogonal networks for unconfounded treatments ( DONUT) based on the orthogonality constraint. They show that the asymptotically normal estimator for the average causal effect can be used to estimate the treatment assignment. They also show that their estimators are more robust to treatment assignment bias than the existing estimators, which are based on asymptonotic variance. They demonstrate that DONUT outperforms the state-of-the-art on several benchmark datasets for causal inference.","This paper studies the estimation of causal effects in observational data for Decision-making. The authors propose a new regularization framework for deep orthogonal networks for unconfounded treatments ( DONUT) based on the orthogonality constraint. They show that the asymptotically normal estimator for the average causal effect can be used to estimate the treatment assignment. They also show that their estimators are more robust to treatment assignment bias than the existing estimators, which are based on asymptonotic variance. They demonstrate that DONUT outperforms the state-of-the-art on several benchmark datasets for causal inference."
1089,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"style transfer CONJUNCTION multitask learning. multitask learning CONJUNCTION style transfer. multitask learning HYPONYM-OF deep learning techniques. style transfer HYPONYM-OF deep learning techniques. affine transformations of features USED-FOR deep learning techniques. affine parameters USED-FOR features. parameters PART-OF BatchNorm. randomly chosen parameters PART-OF network. affine parameters USED-FOR deep learning. shifting and rescaling random features USED-FOR neural networks. Method is affine transform. OtherScientificTerm are random initializations, and random features. Metric is accuracy. ","This paper studies the use of affine transformations of features in deep learning techniques such as style transfer and multitask learning. The authors show that the affine transform can be applied to a wide range of deep learning methods, and that the features learned using affine parameters can be used to improve the performance of a variety of features across different tasks. They also show that random initializations of the features are not necessary to improve accuracy.    The authors propose a simple modification to BatchNorm, where the parameters of the network are shifted and rescaled based on a set of randomly chosen parameters. They show that by shifting and rescaling random features in neural networks, they can improve the accuracy of the networks.","This paper studies the use of affine transformations of features in deep learning techniques such as style transfer and multitask learning. The authors show that the affine transform can be applied to a wide range of deep learning methods, and that the features learned using affine parameters can be used to improve the performance of a variety of features across different tasks. They also show that random initializations of the features are not necessary to improve accuracy.    The authors propose a simple modification to BatchNorm, where the parameters of the network are shifted and rescaled based on a set of randomly chosen parameters. They show that by shifting and rescaling random features in neural networks, they can improve the accuracy of the networks."
1098,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"model USED-FOR test - time adaptation. model USED-FOR confidence. method USED-FOR normalization statistics. method USED-FOR channel - wise affine transformations. Tent USED-FOR source - free domain adaptation. corrupted ImageNet CONJUNCTION CIFAR-10/100. CIFAR-10/100 CONJUNCTION corrupted ImageNet. Tent USED-FOR semantic segmentation. GTA CONJUNCTION Cityscapes. Cityscapes CONJUNCTION GTA. Tent USED-FOR image classification. Tent USED-FOR digit recognition. source - free domain adaptation USED-FOR digit recognition. generalization error EVALUATE-FOR image classification. Tent USED-FOR Tent. CIFAR-10/100 USED-FOR image classification. corrupted ImageNet USED-FOR image classification. VisDA - C benchmark EVALUATE-FOR Tent. generalization error EVALUATE-FOR Tent. Metric is entropy. Material are SVHN, and MNIST / MNIST - M / USPS. Method is test - time optimization. ","This paper proposes a model for test-time adaptation to improve the confidence of the model. The authors propose a method for normalization statistics of the input image to the model, which is based on the idea of entropy. The method is applied to channel-wise affine transformations. Tent is tested on image classification on corrupted ImageNet and CIFAR-10/100, digit recognition on SVHN, and source-free domain adaptation on MNIST/MNIST-M/USPS, and semantic segmentation on GTA and Cityscapes. Tent outperforms Tent on the VisDA-C benchmark on the generalization error of image classification using corrupted ImageNets and MNIST / MNIST - M/S.   ","This paper proposes a model for test-time adaptation to improve the confidence of the model. The authors propose a method for normalization statistics of the input image to the model, which is based on the idea of entropy. The method is applied to channel-wise affine transformations. Tent is tested on image classification on corrupted ImageNet and CIFAR-10/100, digit recognition on SVHN, and source-free domain adaptation on MNIST/MNIST-M/USPS, and semantic segmentation on GTA and Cityscapes. Tent outperforms Tent on the VisDA-C benchmark on the generalization error of image classification using corrupted ImageNets and MNIST / MNIST - M/S.   "
1107,SP:ed544ee661580592063aa17aee8924cc99919130,Uncertainty quantification USED-FOR machine learning systems. recurrent timesteps FEATURE-OF stochastic discrete state transitions. stochastic discrete state transitions USED-FOR recurrent neural networks ( RNNs ). uncertainty quantification COMPARE method. method COMPARE uncertainty quantification. method USED-FOR deterministic and probabilistic automata. well - calibrated models USED-FOR real - world classification tasks. method USED-FOR well - calibrated models. explorationexploitation trade - off FEATURE-OF reinforcement learning. method USED-FOR out - of - distribution detection. method USED-FOR explorationexploitation trade - off. Generic is model. OtherScientificTerm is recurrent state transition distribution. ,"This paper proposes a new method for uncertainty quantification for machine learning systems. The authors propose to use stochastic discrete state transitions in recurrent neural networks (RNNs) with recurrent timesteps, which is an extension of previous work on uncertainty quantization. Unlike previous work, the proposed method can be applied to both deterministic and probabilistic automata. The proposed method is able to train well-calibrated models for real-world classification tasks, and can be used for out-of-distribution detection and explorationexploitation trade-off in reinforcement learning.    The authors provide a theoretical analysis of the proposed model, which shows that the uncertainty of the model depends on the recurrent state transition distribution. The method is also able to be used to improve the performance of the method for explorationexploration trade-offs.  The paper is well-written and easy to follow.","This paper proposes a new method for uncertainty quantification for machine learning systems. The authors propose to use stochastic discrete state transitions in recurrent neural networks (RNNs) with recurrent timesteps, which is an extension of previous work on uncertainty quantization. Unlike previous work, the proposed method can be applied to both deterministic and probabilistic automata. The proposed method is able to train well-calibrated models for real-world classification tasks, and can be used for out-of-distribution detection and explorationexploitation trade-off in reinforcement learning.    The authors provide a theoretical analysis of the proposed model, which shows that the uncertainty of the model depends on the recurrent state transition distribution. The method is also able to be used to improve the performance of the method for explorationexploration trade-offs.  The paper is well-written and easy to follow."
1116,SP:a38c523196f68a90b5db45671f9dbd87981a024c,"Protecting data privacy PART-OF deep learning ( DL ). stochastic differential equation principled residual perturbation USED-FOR privacy - preserving DL. Gaussian noise USED-FOR residual mapping of ResNets. residual perturbation USED-FOR differential privacy ( DP ). generalization gap FEATURE-OF DL. residual perturbation USED-FOR generalization gap. residual perturbation COMPARE DP stochastic gradient descent ( DPSGD ). DP stochastic gradient descent ( DPSGD ) COMPARE residual perturbation. DP stochastic gradient descent ( DPSGD ) USED-FOR membership privacy protection. residual perturbation USED-FOR DL models ’ utility. residual perturbation USED-FOR membership privacy protection. ResNet8 USED-FOR IDC dataset classification. residual perturbation USED-FOR perfect membership privacy. residual perturbation COMPARE DPSGD. DPSGD COMPARE residual perturbation. accuracy EVALUATE-FOR DPSGD. accuracy EVALUATE-FOR residual perturbation. Task is data privacy. OtherScientificTerm are utility degradation, and ResNets. ","Protecting data privacy in deep learning (DL) is an important problem. This paper proposes a stochastic differential equation principled residual perturbation for privacy-preserving DL. Specifically, the authors propose to perturb the residual mapping of ResNets with Gaussian noise. The authors show that the proposed residual perturbing can be used to achieve differential privacy (DP) in DL. They also show that residual perturbed models are able to close the generalization gap in DL, which is a measure of the utility degradation.   The authors also demonstrate that the use of residual perturgation can reduce the DL models’ utility more efficiently than DP stochastastic gradient descent (DPSGD) for membership privacy protection. They show that for IDC dataset classification on ResNet8, the residual of a ResNet trained on IDC datasets is the only one that can achieve perfect membership privacy. They further show that using residual pertubation is more efficient than DPSGD in terms of accuracy.","Protecting data privacy in deep learning (DL) is an important problem. This paper proposes a stochastic differential equation principled residual perturbation for privacy-preserving DL. Specifically, the authors propose to perturb the residual mapping of ResNets with Gaussian noise. The authors show that the proposed residual perturbing can be used to achieve differential privacy (DP) in DL. They also show that residual perturbed models are able to close the generalization gap in DL, which is a measure of the utility degradation.   The authors also demonstrate that the use of residual perturgation can reduce the DL models’ utility more efficiently than DP stochastastic gradient descent (DPSGD) for membership privacy protection. They show that for IDC dataset classification on ResNet8, the residual of a ResNet trained on IDC datasets is the only one that can achieve perfect membership privacy. They further show that using residual pertubation is more efficient than DPSGD in terms of accuracy."
1125,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"limited computational resources USED-FOR inference. natural language processing EVALUATE-FOR transformers. computational efficiency USED-FOR inference. model USED-FOR inference scenario. inefficiency CONJUNCTION redundancy. redundancy CONJUNCTION inefficiency. PoWER - BERT USED-FOR inefficiency. PoWER - BERT USED-FOR redundancy. it USED-FOR inference scenarios. extension USED-FOR large - scale transformer. Length - Adaptive Transformer HYPONYM-OF large - scale transformer. LengthDrop HYPONYM-OF structural variant of dropout. LengthDrop USED-FOR transformer. multi - objective evolutionary search USED-FOR length configuration. accuracy EVALUATE-FOR length configuration. PoWER - BERT USED-FOR token - level classification. sequence - level classification USED-FOR token - level classification. PoWER - BERT USED-FOR sequence - level classification. span - based question - answering HYPONYM-OF token - level classification. SQuAD 1.1 CONJUNCTION MNLI - m. MNLI - m CONJUNCTION SQuAD 1.1. MNLI - m CONJUNCTION SST-2. SST-2 CONJUNCTION MNLI - m. accuracyefficiency trade - off EVALUATE-FOR setups. accuracyefficiency trade - off EVALUATE-FOR approach. SST-2 HYPONYM-OF setups. SQuAD 1.1 HYPONYM-OF setups. MNLI - m HYPONYM-OF setups. Generic are they, and approaches. Metric is computational complexity. OtherScientificTerm are computational budget, Drop - and - Restore, and word - vectors. ","This paper studies the problem of inference with limited computational resources for transformers in the context of natural language processing. The authors propose a novel approach to improve the computational efficiency of inference by reducing the computational complexity of the model for each inference scenario. They propose a structural variant of dropout, called LengthDrop, which is designed to reduce the computational budget and reduce the inefficiency and redundancy of PoWER-BERT, and extend it to other inference scenarios. They also propose an extension to the large-scale transformer, called the Length-Adaptive Transformer, which extends Drop-and-Restore to the case of length dropout. The length configuration is learned via a multi-objective evolutionary search, where the length of the length vector is updated based on the current accuracy of the input word-vector. They evaluate their approach on three different setups: SQuAD 1.1, MNLI-m, and SST-2, and show that the proposed approach achieves a better accuracyefficiency trade-off than the previous approaches.    The authors also show that their extension to PoERWERBER achieves state-of-the-art performance on token-level classification (e.g. token-based question-based answer) and sequence-level learning (eq. 1). They also demonstrate that PoWERBERT is able to achieve better performance than PoERBERT for token-levels classification, and that PoBERT can match the performance of PoERWBERT for sequence-levels of classification. ","This paper studies the problem of inference with limited computational resources for transformers in the context of natural language processing. The authors propose a novel approach to improve the computational efficiency of inference by reducing the computational complexity of the model for each inference scenario. They propose a structural variant of dropout, called LengthDrop, which is designed to reduce the computational budget and reduce the inefficiency and redundancy of PoWER-BERT, and extend it to other inference scenarios. They also propose an extension to the large-scale transformer, called the Length-Adaptive Transformer, which extends Drop-and-Restore to the case of length dropout. The length configuration is learned via a multi-objective evolutionary search, where the length of the length vector is updated based on the current accuracy of the input word-vector. They evaluate their approach on three different setups: SQuAD 1.1, MNLI-m, and SST-2, and show that the proposed approach achieves a better accuracyefficiency trade-off than the previous approaches.    The authors also show that their extension to PoERWERBER achieves state-of-the-art performance on token-level classification (e.g. token-based question-based answer) and sequence-level learning (eq. 1). They also demonstrate that PoWERBERT is able to achieve better performance than PoERBERT for token-levels classification, and that PoBERT can match the performance of PoERWBERT for sequence-levels of classification. "
1134,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"1 - WL test USED-FOR distinguishing graph structures. expressiveness EVALUATE-FOR graph neural networks ( GNNs ). neighborhood aggregation GNNs COMPARE 1 - WL test. 1 - WL test COMPARE neighborhood aggregation GNNs. neighborhood aggregation GNNs USED-FOR distinguishing graph structures. aggregators PART-OF GNNs. aggregators USED-FOR expressiveness. aggregation coefficient matrix USED-FOR aggregators. aggregation coefficient matrix USED-FOR injective aggregators. aggregators CONJUNCTION injective aggregators. injective aggregators CONJUNCTION aggregators. aggregation coefficient matrix USED-FOR aggregation. It USED-FOR rank of hidden features. nonlinear units USED-FOR aggregation - based GNNs. ExpandingConv CONJUNCTION CombConv. CombConv CONJUNCTION ExpandingConv. CombConv HYPONYM-OF GNN layers. ExpandingConv HYPONYM-OF GNN layers. models USED-FOR large and densely connected graphs. OtherScientificTerm are graph structures, weak distinguishing strength, and low - rank transformations. Method is WL test. Generic is it. ","This paper studies the expressiveness of graph neural networks (GNNs) in terms of the 1-WL test for distinguishing graph structures. The paper shows that neighborhood aggregation GNNs are more expressive than the standard 1- WL test, which is based on the weak distinguishing strength. The authors also show that the expressive power of GNN layers (e.g., ExpandingConv, CombConv) is not a result of the WL.    The authors show that aggregation-based GNN with nonlinear units is more expressive. The expressiveness is related to the aggregation coefficient matrix of the aggregators and injective aggregators. It is a measure of the rank of hidden features.  The paper also shows that models for large and densely connected graphs have a higher expressive power than models for small graphs. The main contribution of the paper is that the aggregation coefficients of the aggregation of the two aggregators of different layers of a GNN can be used as a measure for expressiveness, and that it can be interpreted as an indicator of the strength of a particular aggregation. ","This paper studies the expressiveness of graph neural networks (GNNs) in terms of the 1-WL test for distinguishing graph structures. The paper shows that neighborhood aggregation GNNs are more expressive than the standard 1- WL test, which is based on the weak distinguishing strength. The authors also show that the expressive power of GNN layers (e.g., ExpandingConv, CombConv) is not a result of the WL.    The authors show that aggregation-based GNN with nonlinear units is more expressive. The expressiveness is related to the aggregation coefficient matrix of the aggregators and injective aggregators. It is a measure of the rank of hidden features.  The paper also shows that models for large and densely connected graphs have a higher expressive power than models for small graphs. The main contribution of the paper is that the aggregation coefficients of the aggregation of the two aggregators of different layers of a GNN can be used as a measure for expressiveness, and that it can be interpreted as an indicator of the strength of a particular aggregation. "
1143,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"robustness EVALUATE-FOR generative models. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. generalization EVALUATE-FOR generative models. method USED-FOR quantifying disentanglement. conditional submanifolds PART-OF representation. topological similarity FEATURE-OF conditional submanifolds. generative model USED-FOR method. unsupervised and supervised variants PART-OF method. method COMPARE models. models COMPARE method. Task are Learning disentangled representations, and measuring disentanglement. ","This paper proposes a new method for quantifying disentanglement in generative models to improve generalization and robustness. Learning disentangled representations is an important problem in many applications, and this paper aims to address this problem. The proposed method uses a generative model to learn the conditional submanifolds of the representation, and then measures the topological similarity between the two sets of conditional subfolders. The method has two unsupervised and supervised variants of the proposed method. Experiments show that the method outperforms existing models in terms of generalization, robustness, and the quality of the learned representations.   ","This paper proposes a new method for quantifying disentanglement in generative models to improve generalization and robustness. Learning disentangled representations is an important problem in many applications, and this paper aims to address this problem. The proposed method uses a generative model to learn the conditional submanifolds of the representation, and then measures the topological similarity between the two sets of conditional subfolders. The method has two unsupervised and supervised variants of the proposed method. Experiments show that the method outperforms existing models in terms of generalization, robustness, and the quality of the learned representations.   "
1152,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"unauthorized exploitation of personal data USED-FOR commercial models. sample - wise and class - wise forms FEATURE-OF error - minimizing noise. personal data USED-FOR deep learning models. Method is deep learning. Task are unauthorized data exploitation, and face recognition. OtherScientificTerm are Error - minimizing noise, and noise. Generic is model. Metric is normal data utility. ","This paper studies the problem of unauthorized data exploitation in deep learning. The authors show that commercial models are vulnerable to the unauthorized exploitation of personal data in commercial models. Error-minimizing noise in sample-wise and class-wise forms can be exploited in order to exploit personal data for training deep learning models. The paper also shows that the noise can be removed from the model if the model is trained in a way that is consistent with the normal data utility. Experiments are conducted on face recognition, where the noise is applied to the face recognition task. ","This paper studies the problem of unauthorized data exploitation in deep learning. The authors show that commercial models are vulnerable to the unauthorized exploitation of personal data in commercial models. Error-minimizing noise in sample-wise and class-wise forms can be exploited in order to exploit personal data for training deep learning models. The paper also shows that the noise can be removed from the model if the model is trained in a way that is consistent with the normal data utility. Experiments are conducted on face recognition, where the noise is applied to the face recognition task. "
1161,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,chess CONJUNCTION Go. Go CONJUNCTION chess. Go CONJUNCTION shogi. shogi CONJUNCTION Go. MuZero USED-FOR game - playing agents. MuZero COMPARE AlphaZero. AlphaZero COMPARE MuZero. game - playing agents COMPARE AlphaZero. AlphaZero COMPARE game - playing agents. model of environmental dynamics USED-FOR MuZero. deterministic environments USED-FOR MuZero. MuZero USED-FOR Nondeterministic MuZero ( NDMZ ). Nondeterministic Monte Carlo Tree Search CONJUNCTION extensive - form games. extensive - form games CONJUNCTION Nondeterministic Monte Carlo Tree Search. MuZero network architecture CONJUNCTION tree search. tree search CONJUNCTION MuZero network architecture. chance player PART-OF MuZero network architecture. chance player PART-OF tree search. Nondeterministic Monte Carlo Tree Search USED-FOR NDMZ. extensive - form games USED-FOR NDMZ. NDMZ USED-FOR chance. chance player PART-OF NDMZ. NDMZ USED-FOR model. model USED-FOR game. Method is MuZero algorithm. Material is Atari suite. OtherScientificTerm is environmental dynamics. ,"This paper proposes a new MuZero algorithm, Nondeterministic MuZero (NDMZ), which is an extension of MuZero to deterministic environments. The authors show that MuZero outperforms previous game-playing agents on chess, Go, and shogi, and outperforms AlphaZero on the Atari suite. NDMZ is based on a model of environmental dynamics, and the authors propose a MuZero network architecture, tree search, and extensive-form games to learn the environment dynamics. The paper also shows that the MuZero can be used to learn a Nondeteterministic Monte Carlo Tree Search (DMTS) algorithm, where the chance player in tree search is replaced by a chance player, and that the model is able to learn to play the game on the NDMTS.  ","This paper proposes a new MuZero algorithm, Nondeterministic MuZero (NDMZ), which is an extension of MuZero to deterministic environments. The authors show that MuZero outperforms previous game-playing agents on chess, Go, and shogi, and outperforms AlphaZero on the Atari suite. NDMZ is based on a model of environmental dynamics, and the authors propose a MuZero network architecture, tree search, and extensive-form games to learn the environment dynamics. The paper also shows that the MuZero can be used to learn a Nondeteterministic Monte Carlo Tree Search (DMTS) algorithm, where the chance player in tree search is replaced by a chance player, and that the model is able to learn to play the game on the NDMTS.  "
1170,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"Hierarchical approaches USED-FOR reinforcement learning. Hierarchical approaches USED-FOR learning. Hierarchical approaches USED-FOR data efficiency. Hindsight Off - policy Options ( HO2 ) HYPONYM-OF off - policy option learning algorithm. temporal and action abstraction USED-FOR option framework. flat policies COMPARE mixture policies. mixture policies COMPARE flat policies. mixture policies COMPARE option policies. option policies COMPARE mixture policies. flat policies COMPARE on - policy option methods. on - policy option methods COMPARE flat policies. off - policy training CONJUNCTION backpropagation. backpropagation CONJUNCTION off - policy training. policy components USED-FOR backpropagation. dynamic programming inference procedure USED-FOR off - policy training. dynamic programming inference procedure USED-FOR backpropagation. HO2 COMPARE option learning methods. option learning methods COMPARE HO2. raw pixel inputs USED-FOR simulated robot manipulation tasks. intuitive extension USED-FOR temporal abstraction. OtherScientificTerm are abstractions, data - generating behavior policy, trust - region constraints, and pre - trained options. Method are policy optimization, off - policy optimization, and action and temporal abstraction. Task is off - policy option learning. ","This paper proposes a new off-policy option learning algorithm called Hindsight Off-policy Options (HO2) which is based on Hierarchical approaches for reinforcement learning to improve data efficiency. The option framework is built on the idea of temporal and action abstraction, where abstractions are learned for a data-generating behavior policy and then used for policy optimization. The paper shows that flat policies are more efficient than mixture policies, and that option policies trained with flat policies outperform on-policy options.   The paper also shows that the combination of policy optimization and backpropagation of the policy components improves performance.  The main idea of HO2 is to use a dynamic programming inference procedure to guide the training of the off-policies during training, and to use the off policy optimization as a regularizer for the back-propagating policy.  Experiments on simulated robot manipulation tasks with raw pixel inputs show that HO2 outperforms other option learning methods in terms of performance, especially when the trust-region constraints are relaxed and the number of pre-trained options is small.  In addition, an intuitive extension to temporal abstraction is also proposed, where the action and temporal abstraction are learned together. ","This paper proposes a new off-policy option learning algorithm called Hindsight Off-policy Options (HO2) which is based on Hierarchical approaches for reinforcement learning to improve data efficiency. The option framework is built on the idea of temporal and action abstraction, where abstractions are learned for a data-generating behavior policy and then used for policy optimization. The paper shows that flat policies are more efficient than mixture policies, and that option policies trained with flat policies outperform on-policy options.   The paper also shows that the combination of policy optimization and backpropagation of the policy components improves performance.  The main idea of HO2 is to use a dynamic programming inference procedure to guide the training of the off-policies during training, and to use the off policy optimization as a regularizer for the back-propagating policy.  Experiments on simulated robot manipulation tasks with raw pixel inputs show that HO2 outperforms other option learning methods in terms of performance, especially when the trust-region constraints are relaxed and the number of pre-trained options is small.  In addition, an intuitive extension to temporal abstraction is also proposed, where the action and temporal abstraction are learned together. "
1179,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,Reinforcement learning ( RL ) algorithms USED-FOR maximizing the expected cumulative return. drug discovery HYPONYM-OF applications. objective function USED-FOR expected maximum reward. functional form USED-FOR Bellman equation. Bellman operators USED-FOR functional form. formulation USED-FOR synthesizable molecule generation. real - world drug discovery pipeline FEATURE-OF synthesizable molecule generation. Generic is framework. Method is RL agent. OtherScientificTerm is expected cumulative return. ,"This paper studies the problem of maximizing the expected cumulative return in reinforcement learning (RL) algorithms. The authors propose a framework for learning an objective function that maximizes the expected maximum reward, which can be used for a variety of applications such as drug discovery, where the goal is to learn an RL agent that maximises the expected return. The paper proposes a functional form of the Bellman equation, which is based on Bellman operators. The formulation is then applied to synthesizable molecule generation in a real-world drug discovery pipeline. ","This paper studies the problem of maximizing the expected cumulative return in reinforcement learning (RL) algorithms. The authors propose a framework for learning an objective function that maximizes the expected maximum reward, which can be used for a variety of applications such as drug discovery, where the goal is to learn an RL agent that maximises the expected return. The paper proposes a functional form of the Bellman equation, which is based on Bellman operators. The formulation is then applied to synthesizable molecule generation in a real-world drug discovery pipeline. "
1188,SP:bd4b1781448def4327214c78f07538d285119ef9,"neural networks USED-FOR fixed output dimension. neural network architectures USED-FOR output features. neural networks USED-FOR features. Contextual HyperNetwork ( CHN ) HYPONYM-OF auxiliary model. base model USED-FOR feature. CHN COMPARE re - training and fine - tuning approaches. re - training and fine - tuning approaches COMPARE CHN. neural network USED-FOR CHN. CHN USED-FOR partial variational autoencoder ( P - VAE ). partial variational autoencoder ( P - VAE ) HYPONYM-OF deep generative model. deep generative model USED-FOR missing features. missing features PART-OF sparsely - observed data. CHN USED-FOR CHNs. e - learning CONJUNCTION healthcare tasks. healthcare tasks CONJUNCTION e - learning. system COMPARE imputation and meta - learning baselines. imputation and meta - learning baselines COMPARE system. recommender systems CONJUNCTION e - learning. e - learning CONJUNCTION recommender systems. imputation and meta - learning baselines USED-FOR recommender systems. few - shot learning USED-FOR features. system USED-FOR features. few - shot learning EVALUATE-FOR imputation and meta - learning baselines. healthcare tasks EVALUATE-FOR system. recommender systems EVALUATE-FOR system. few - shot learning EVALUATE-FOR system. Method is deep learning. Task are online learning settings, and recommender system. ","This paper proposes Contextual HyperNetwork (CHN), an auxiliary model that uses neural networks with a fixed output dimension to learn features that can be used to improve the performance of deep learning in online learning settings. The authors propose to use different neural network architectures to learn the output features, and then use the features learned by the different neural networks to train a base model for each feature.  The authors show that CHN outperforms existing re-training and fine-tuning approaches, and that the base model can be re-trained to learn a feature that is more robust to missing features in sparsely-observed data. In addition, the authors propose a deep generative model called a partial variational autoencoder (P-VAE) that uses a neural network to generate features for a given feature and then uses the features generated by the CHN to train CHNs.  Experiments are conducted on recommender systems, e-learning, and healthcare tasks, and show that the proposed system outperforms imputation and meta-learning baselines in few-shot learning for features learned in a recommender system.   ","This paper proposes Contextual HyperNetwork (CHN), an auxiliary model that uses neural networks with a fixed output dimension to learn features that can be used to improve the performance of deep learning in online learning settings. The authors propose to use different neural network architectures to learn the output features, and then use the features learned by the different neural networks to train a base model for each feature.  The authors show that CHN outperforms existing re-training and fine-tuning approaches, and that the base model can be re-trained to learn a feature that is more robust to missing features in sparsely-observed data. In addition, the authors propose a deep generative model called a partial variational autoencoder (P-VAE) that uses a neural network to generate features for a given feature and then uses the features generated by the CHN to train CHNs.  Experiments are conducted on recommender systems, e-learning, and healthcare tasks, and show that the proposed system outperforms imputation and meta-learning baselines in few-shot learning for features learned in a recommender system.   "
1197,SP:8e4677cc6071a33397347679308165c10dca2aae,data inefficiency CONJUNCTION catastrophic forgetting. catastrophic forgetting CONJUNCTION data inefficiency. Bayesian paradigm USED-FOR deep learning. poor calibration CONJUNCTION data inefficiency. data inefficiency CONJUNCTION poor calibration. Bayesian inference USED-FOR high - dimensional parameter spaces. high - dimensional parameter spaces FEATURE-OF deep neural networks. deep neural networks USED-FOR Bayesian inference. restrictive approximations USED-FOR Bayesian inference. model parameters USED-FOR inference. expressive posterior approximations USED-FOR full model. Bayesian deep learning method USED-FOR full covariance Gaussian posterior approximation. Bayesian deep learning method USED-FOR point estimate. subnetwork USED-FOR full covariance Gaussian posterior approximation. subnetwork selection procedure USED-FOR posterior uncertainty. approach COMPARE point - estimated networks. point - estimated networks COMPARE approach. approach COMPARE methods. methods COMPARE approach. full network USED-FOR expressive posterior approximations. expressive posterior approximations USED-FOR methods. OtherScientificTerm is point estimates. ,"This paper proposes a Bayesian paradigm for deep learning, which aims to address the issues of poor calibration, data inefficiency, and catastrophic forgetting. The authors argue that Bayesian inference in deep neural networks with high-dimensional parameter spaces is challenging due to restrictive approximations. To address this issue, the authors propose a new Bayesian deep learning method for learning a full covariance Gaussian posterior approximation from a subnetwork to the full model, which is based on the Bayesian assumption that the model parameters for inference can be expressed as a function of the subnetwork parameters. The subnetwork selection procedure is also used to reduce the posterior uncertainty. The proposed approach is shown to outperform existing point-estimated networks and other methods based on expressive posterior approximation of the full network. ","This paper proposes a Bayesian paradigm for deep learning, which aims to address the issues of poor calibration, data inefficiency, and catastrophic forgetting. The authors argue that Bayesian inference in deep neural networks with high-dimensional parameter spaces is challenging due to restrictive approximations. To address this issue, the authors propose a new Bayesian deep learning method for learning a full covariance Gaussian posterior approximation from a subnetwork to the full model, which is based on the Bayesian assumption that the model parameters for inference can be expressed as a function of the subnetwork parameters. The subnetwork selection procedure is also used to reduce the posterior uncertainty. The proposed approach is shown to outperform existing point-estimated networks and other methods based on expressive posterior approximation of the full network. "
1206,SP:be361952fe9de545f68b8a060f790d54c6755998,generalization CONJUNCTION applicability. applicability CONJUNCTION generalization. generalization EVALUATE-FOR embedding techniques. state representations USED-FOR Model - free reinforcement learning approaches. approach USED-FOR jointly learning embeddings. model USED-FOR embeddings. generic architecture USED-FOR policy. these USED-FOR policy. these USED-FOR generic architecture. embedded representations USED-FOR generalization. approach USED-FOR embedded representations. it COMPARE models. models COMPARE it. approach COMPARE it. it COMPARE approach. gaming EVALUATE-FOR it. recommender systems EVALUATE-FOR it. approach COMPARE models. models COMPARE approach. state / action spaces FEATURE-OF discrete / continuous domains. discrete / continuous domains EVALUATE-FOR models. discrete / continuous domains EVALUATE-FOR it. recommender systems EVALUATE-FOR approach. gaming EVALUATE-FOR approach. Method is reinforcement learning. Generic is approaches. Material is discrete and continuous domains. OtherScientificTerm is embedding spaces. ,"Model-free reinforcement learning approaches have been shown to improve generalization and applicability of existing embedding techniques. Model-free RL is a popular approach to jointly learning embeddings for both state representations and actions in reinforcement learning. This paper proposes a new approach for jointly learning the embedding spaces. The key idea is to learn a generic architecture for embedding the policy and then use these to train a policy that can be applied to any embedding space. The paper shows that this approach is able to learn embedded representations that can improve the generalization of the learned policy. The approach is tested in both discrete and continuous domains, and it is shown to outperform existing models in discrete/continuous domains with different state/action spaces. In recommender systems, it is also shown to perform well in gaming. ","Model-free reinforcement learning approaches have been shown to improve generalization and applicability of existing embedding techniques. Model-free RL is a popular approach to jointly learning embeddings for both state representations and actions in reinforcement learning. This paper proposes a new approach for jointly learning the embedding spaces. The key idea is to learn a generic architecture for embedding the policy and then use these to train a policy that can be applied to any embedding space. The paper shows that this approach is able to learn embedded representations that can improve the generalization of the learned policy. The approach is tested in both discrete and continuous domains, and it is shown to outperform existing models in discrete/continuous domains with different state/action spaces. In recommender systems, it is also shown to perform well in gaming. "
1215,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"viewmaker networks HYPONYM-OF generative models. Viewmakers HYPONYM-OF stochastic bounded adversaries. they USED-FOR views. cropping CONJUNCTION color jitter. color jitter CONJUNCTION cropping. transfer accuracy EVALUATE-FOR welltuned SimCLR augmentations. color jitter HYPONYM-OF transformations. cropping HYPONYM-OF transformations. speech recordings CONJUNCTION wearable sensor data. wearable sensor data CONJUNCTION speech recordings. wearable sensor data EVALUATE-FOR baseline augmentations. speech recordings EVALUATE-FOR baseline augmentations. Viewmaker views CONJUNCTION handcrafted views. handcrafted views CONJUNCTION Viewmaker views. transfer performance EVALUATE-FOR they. robustness EVALUATE-FOR they. viewmakers USED-FOR representation learning algorithms. Viewmaker networks USED-FOR unsupervised learning. Viewmaker networks USED-FOR complex and diverse input - dependent views. complex and diverse input - dependent views USED-FOR unsupervised learning. Task is unsupervised representation learning. Generic is models. Method is unsupervised representation learning methods. OtherScientificTerm are ` p - bounded perturbation, common image corruptions, and domain expertise. Material is CIFAR-10. ","This paper studies the problem of unsupervised representation learning in the presence of stochastic bounded adversaries, i.e., viewmaker networks. Viewmaker networks are generative models that can be seen as an extension of SimCLR. Viewmakers are models that are robust to `p-bounded perturbation’, which is defined as a set of common image corruptions (e.g., cropping, color jitter, etc.). The authors show that they can be used to learn views that are transferable between different views, and improve the transfer accuracy of existing welltuned simCLR augmentations. They show that baseline augmentations on speech recordings and wearable sensor data, as well as on CIFAR-10, are transfer-robust, and that they improve transfer performance and robustness. They also show that viewmakers can improve the robustness of existing representation learning algorithms, and demonstrate that they are able to transfer between Viewmaker views and handcrafted views. Finally, they show that Viewmaker neural networks can learn complex and diverse input-dependent views, which can be useful in the case of common corruptions and domain expertise.","This paper studies the problem of unsupervised representation learning in the presence of stochastic bounded adversaries, i.e., viewmaker networks. Viewmaker networks are generative models that can be seen as an extension of SimCLR. Viewmakers are models that are robust to `p-bounded perturbation’, which is defined as a set of common image corruptions (e.g., cropping, color jitter, etc.). The authors show that they can be used to learn views that are transferable between different views, and improve the transfer accuracy of existing welltuned simCLR augmentations. They show that baseline augmentations on speech recordings and wearable sensor data, as well as on CIFAR-10, are transfer-robust, and that they improve transfer performance and robustness. They also show that viewmakers can improve the robustness of existing representation learning algorithms, and demonstrate that they are able to transfer between Viewmaker views and handcrafted views. Finally, they show that Viewmaker neural networks can learn complex and diverse input-dependent views, which can be useful in the case of common corruptions and domain expertise."
1224,SP:ef7735be9423ad53059505c170e75201ca134573,"autonomous driving CONJUNCTION air traffic management. air traffic management CONJUNCTION autonomous driving. deep learning models USED-FOR high - assurance systems. air traffic management CONJUNCTION medical diagnosis. medical diagnosis CONJUNCTION air traffic management. medical diagnosis HYPONYM-OF high - assurance systems. autonomous driving HYPONYM-OF high - assurance systems. air traffic management HYPONYM-OF high - assurance systems. statistical, geometric, or topological signatures USED-FOR techniques. detection approaches USED-FOR outliers. KMNIST CONJUNCTION F - MNIST. F - MNIST CONJUNCTION KMNIST. CIFAR10 ( for SVHN ) CONJUNCTION KMNIST. KMNIST CONJUNCTION CIFAR10 ( for SVHN ). SVHN CONJUNCTION MNIST. MNIST CONJUNCTION SVHN. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. Imagenet CONJUNCTION LSUN. LSUN CONJUNCTION Imagenet. WideResNet CONJUNCTION DenseNet. DenseNet CONJUNCTION WideResNet. DenseNet CONJUNCTION LeNet5. LeNet5 CONJUNCTION DenseNet. in - distribution data CONJUNCTION Imagenet. Imagenet CONJUNCTION in - distribution data. ResNet34 CONJUNCTION WideResNet. WideResNet CONJUNCTION ResNet34. F - MNIST USED-FOR OOD data. F - MNIST HYPONYM-OF DNN architectures. SVHN USED-FOR in - distribution data. MNIST CONJUNCTION Imagenet. Imagenet CONJUNCTION MNIST. MNIST USED-FOR in - distribution data. SVHN CONJUNCTION Imagenet. Imagenet CONJUNCTION SVHN. ResNet34 HYPONYM-OF DNN architectures. LeNet5 HYPONYM-OF DNN architectures. DenseNet HYPONYM-OF DNN architectures. WideResNet HYPONYM-OF DNN architectures. Method are Deep neural networks ( DNNs ), and integrated","Deep neural networks (DNNs) have recently become the state of the art in many high-assurity systems such as autonomous driving, air traffic management, and medical diagnosis. This paper investigates how deep learning models can be used to detect out-of-distribution (OOD) data in high-confident systems (e.g., autonomous driving or medical diagnosis). The authors propose two techniques based on statistical, geometric, or topological signatures to detect OOD data. They also propose two detection approaches to detect outliers. Experiments are conducted on CIFAR10 (for SVHN), KMNIST, and F-MNIST. They show that DNN architectures such as ResNet34, WideResNet, DenseNet, and LeNet5 can detect out of distribution data, and that F-mnIST can detect OLD data. In addition, they also show that MNIST, Imagenet, and LSUN can detect in-distributions data. ","Deep neural networks (DNNs) have recently become the state of the art in many high-assurity systems such as autonomous driving, air traffic management, and medical diagnosis. This paper investigates how deep learning models can be used to detect out-of-distribution (OOD) data in high-confident systems (e.g., autonomous driving or medical diagnosis). The authors propose two techniques based on statistical, geometric, or topological signatures to detect OOD data. They also propose two detection approaches to detect outliers. Experiments are conducted on CIFAR10 (for SVHN), KMNIST, and F-MNIST. They show that DNN architectures such as ResNet34, WideResNet, DenseNet, and LeNet5 can detect out of distribution data, and that F-mnIST can detect OLD data. In addition, they also show that MNIST, Imagenet, and LSUN can detect in-distributions data. "
1233,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"hierarchical VAE COMPARE PixelCNN. PixelCNN COMPARE hierarchical VAE. log - likelihood EVALUATE-FOR PixelCNN. natural image benchmarks EVALUATE-FOR PixelCNN. VAEs USED-FOR autoregressive models. VAEs USED-FOR models. autoregressive models COMPARE VAEs. VAEs COMPARE autoregressive models. loglikelihood EVALUATE-FOR autoregressive models. loglikelihood EVALUATE-FOR VAEs. ImageNet CONJUNCTION FFHQ. FFHQ CONJUNCTION ImageNet. stochastic depth FEATURE-OF VAE. PixelCNN COMPARE VAEs. VAEs COMPARE PixelCNN. likelihoods EVALUATE-FOR VAEs. VAE USED-FOR hierarchical visual representations. FFHQ-256 USED-FOR VAE. VAEs USED-FOR global features. VAEs USED-FOR local details. multiscale generative procedure COMPARE PixelCNN. PixelCNN COMPARE multiscale generative procedure. log - likelihood EVALUATE-FOR PixelCNN. log - likelihood EVALUATE-FOR multiscale generative procedure. Generic is they. OtherScientificTerm are insufficient depth, and Low resolution High resolution. Material is high - resolution images. Method is generative process. ","This paper studies the log-likelihood of autoregressive models trained with VAEs on natural image benchmarks. The authors show that a hierarchical VAE is more likely to achieve better loglikelihood than PixelCNN on ImageNet and FFHQ, and that VAEs can be used to improve the performance of existing models.  The authors also show that the likelihoods of VAEs trained with high-resolution images are better than those trained with insufficient depth (i.e. low-resolution high resolution).  The main contribution of the paper is that the authors propose a multiscale generative procedure that achieves a better log-likely than the previous state-of-the-art, PixelCNN.   Low resolution High resolution: The authors propose to use a VAE trained on FFHQ-256 to learn hierarchical visual representations. The VAE has a stochastic depth of $O(\sqrt{n\log n})$ and the authors claim that they are able to capture local details better than VAEs.  Multiscale Generative Process: The paper proposes to use VAEs to learn global features for each layer of the generative process.  Experiments:   ","This paper studies the log-likelihood of autoregressive models trained with VAEs on natural image benchmarks. The authors show that a hierarchical VAE is more likely to achieve better loglikelihood than PixelCNN on ImageNet and FFHQ, and that VAEs can be used to improve the performance of existing models.  The authors also show that the likelihoods of VAEs trained with high-resolution images are better than those trained with insufficient depth (i.e. low-resolution high resolution).  The main contribution of the paper is that the authors propose a multiscale generative procedure that achieves a better log-likely than the previous state-of-the-art, PixelCNN.   Low resolution High resolution: The authors propose to use a VAE trained on FFHQ-256 to learn hierarchical visual representations. The VAE has a stochastic depth of $O(\sqrt{n\log n})$ and the authors claim that they are able to capture local details better than VAEs.  Multiscale Generative Process: The paper proposes to use VAEs to learn global features for each layer of the generative process.  Experiments:   "
1242,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"methods USED-FOR unsupervised visual representations. uninformative examples PART-OF this. randomly sampled negative examples USED-FOR NCE. semi - hard negatives USED-FOR contrastive representations. bias CONJUNCTION variance. variance CONJUNCTION bias. estimators COMPARE NCE. NCE COMPARE estimators. variance EVALUATE-FOR NCE. CMC CONJUNCTION MoCo. MoCo CONJUNCTION CMC. IR CONJUNCTION CMC. CMC CONJUNCTION IR. image benchmarks EVALUATE-FOR linear evaluation. models USED-FOR approach. IR HYPONYM-OF models. linear evaluation EVALUATE-FOR approach. MoCo HYPONYM-OF models. accuracy EVALUATE-FOR approach. image benchmarks EVALUATE-FOR approach. CMC HYPONYM-OF models. instance segmentation CONJUNCTION key - point detection. key - point detection CONJUNCTION instance segmentation. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. features USED-FOR image distributions. object detection CONJUNCTION key - point detection. key - point detection CONJUNCTION object detection. features USED-FOR downstream tasks. Meta - Dataset collection FEATURE-OF image distributions. key - point detection HYPONYM-OF downstream tasks. object detection HYPONYM-OF downstream tasks. instance segmentation HYPONYM-OF downstream tasks. Method are contrastive learning, metric learning, and mutual information estimators. OtherScientificTerm are noise - contrastive estimation ( NCE ) bound, mutual information, and lower - bounds of mutual information. ","This paper proposes two methods for learning unsupervised visual representations. The first method, contrastive learning, is based on the noise-contrastive estimation (NCE) bound. In contrast to existing methods, this does not rely on uninformative examples, but on randomly sampled negative examples. The authors propose to use semi-hard negatives to learn contrastive representations, where the bias and variance of the NCE is minimized by minimizing the mutual information between the learned representation and the true representation. They show that the proposed estimators are more accurate than the original NCE under the assumption that there are only a finite number of pairs of negative examples, and that this is the case even for metric learning. They also provide lower-bounds of mutual information estimators. The proposed approach is evaluated on standard image benchmarks for linear evaluation using standard models such as IR, CMC, and MoCo, and is shown to improve accuracy. The paper also shows that the learned features are transferable to image distributions from the Meta-Dataset collection, and can be used for downstream tasks such as object detection, instance segmentation, key-point detection, etc.","This paper proposes two methods for learning unsupervised visual representations. The first method, contrastive learning, is based on the noise-contrastive estimation (NCE) bound. In contrast to existing methods, this does not rely on uninformative examples, but on randomly sampled negative examples. The authors propose to use semi-hard negatives to learn contrastive representations, where the bias and variance of the NCE is minimized by minimizing the mutual information between the learned representation and the true representation. They show that the proposed estimators are more accurate than the original NCE under the assumption that there are only a finite number of pairs of negative examples, and that this is the case even for metric learning. They also provide lower-bounds of mutual information estimators. The proposed approach is evaluated on standard image benchmarks for linear evaluation using standard models such as IR, CMC, and MoCo, and is shown to improve accuracy. The paper also shows that the learned features are transferable to image distributions from the Meta-Dataset collection, and can be used for downstream tasks such as object detection, instance segmentation, key-point detection, etc."
1251,SP:613a0e2d8cbe703f37c182553801be7537333f64,"gradient sharing mechanism USED-FOR machine learning systems. gradient sharing mechanism USED-FOR Private training data. federated learning ( FL ) HYPONYM-OF machine learning systems. data leakage attack USED-FOR batch data. shared aggregated gradients USED-FOR batch data. catastrophic data leakage PART-OF federated learning ( CAFE ). data leakage attacks COMPARE CAFE. CAFE COMPARE data leakage attacks. CAFE USED-FOR large - batch data leakage attack. data recovery quality EVALUATE-FOR large - batch data leakage attack. data recovery quality EVALUATE-FOR CAFE. CAFE USED-FOR private data. vertical and horizontal FL settings EVALUATE-FOR CAFE. shared aggregated gradients USED-FOR private data. vertical case HYPONYM-OF FL. data leakage risks FEATURE-OF learning settings. OtherScientificTerm are batch size, data leakage, and training gradients. Generic is method. ","Private training data can be shared in a gradient sharing mechanism in machine learning systems, such as federated learning (FL). However, there exists a data leakage attack that can leak batch data due to the shared aggregated gradients. The authors propose a new method called catastrophic data leakage in federated training (CAFE) to mitigate this issue. CAFE is based on the observation that data leakage attacks can be more powerful than CAFE, and the authors show that CAFE can improve the data recovery quality of batch data. The main contribution of the paper is the theoretical analysis of CAFE in the vertical and horizontal FL settings. The theoretical analysis shows that the data leakage can be reduced to zero when the batch size is large enough. The paper also shows that in vertical FL (the vertical case), CAFE recovers private data with shared aggregations of the training gradients, and CAFE achieves better data leakage results than the state-of-the-art.   The authors also show that in learning settings with high data leakage risks, CAFE does not recover private data at all. ","Private training data can be shared in a gradient sharing mechanism in machine learning systems, such as federated learning (FL). However, there exists a data leakage attack that can leak batch data due to the shared aggregated gradients. The authors propose a new method called catastrophic data leakage in federated training (CAFE) to mitigate this issue. CAFE is based on the observation that data leakage attacks can be more powerful than CAFE, and the authors show that CAFE can improve the data recovery quality of batch data. The main contribution of the paper is the theoretical analysis of CAFE in the vertical and horizontal FL settings. The theoretical analysis shows that the data leakage can be reduced to zero when the batch size is large enough. The paper also shows that in vertical FL (the vertical case), CAFE recovers private data with shared aggregations of the training gradients, and CAFE achieves better data leakage results than the state-of-the-art.   The authors also show that in learning settings with high data leakage risks, CAFE does not recover private data at all. "
1260,SP:ce229295081ff04b26f33829f2c3396b90897b5d,physics CONJUNCTION vision. vision CONJUNCTION physics. vision CONJUNCTION robotics. robotics CONJUNCTION vision. Unsupervised learning of interactions USED-FOR physics. physics CONJUNCTION robotics. robotics CONJUNCTION physics. Unsupervised learning of interactions USED-FOR vision. Unsupervised learning of interactions USED-FOR robotics. multi - agent trajectories USED-FOR Unsupervised learning of interactions. neural relational inference USED-FOR static relations. deep generative model USED-FOR dynamic relations. simulated physics system USED-FOR dynamic relation scenarios. periodic and additive dynamics HYPONYM-OF dynamic relation scenarios. training scheme CONJUNCTION model architecture. model architecture CONJUNCTION training scheme. dynamic relational inference accuracy EVALUATE-FOR model architecture. model USED-FOR coordination and competition patterns. real - world multi - agent basketball trajectories USED-FOR model. real - world multi - agent basketball trajectories USED-FOR coordination and competition patterns. Task is dynamic relational inference. OtherScientificTerm is interactions. ,"This paper presents a novel approach to learning dynamic relational inference for multi-agent interactions. The approach is based on neural relational inference, where the goal is to learn dynamic relations between two agents. The authors propose to use a deep generative model to model dynamic relations in a simulated physics system, and then use it to learn interactions between agents in a real-world robotic system. The paper shows that the proposed approach is able to achieve state-of-the-art performance in both physics and vision.","This paper presents a novel approach to learning dynamic relational inference for multi-agent interactions. The approach is based on neural relational inference, where the goal is to learn dynamic relations between two agents. The authors propose to use a deep generative model to model dynamic relations in a simulated physics system, and then use it to learn interactions between agents in a real-world robotic system. The paper shows that the proposed approach is able to achieve state-of-the-art performance in both physics and vision."
1269,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"Collaborative filtering USED-FOR predicting potential user - item ratings. latent factors PART-OF user - item rating matrix. transductive setting USED-FOR user - specific latent factors. inductive collaborative filtering framework USED-FOR hidden relational graph. rating matrix USED-FOR hidden relational graph. model USED-FOR inductively computing user - specific representations. expressiveness EVALUATE-FOR feature - driven inductive models. model COMPARE feature - driven inductive models. feature - driven inductive models COMPARE model. feature USED-FOR inductively computing user - specific representations. model COMPARE transductive models. transductive models COMPARE model. model USED-FOR inductive learning. cold - start users EVALUATE-FOR them. matrix completion benchmarks EVALUATE-FOR inductive learning. matrix completion benchmarks EVALUATE-FOR model. OtherScientificTerm are user - item ratings, dense weighted graphs, historical rating patterns, relational graphs, and latent space. Method are base matrix factorization model, and relation inference model. ","Collaborative filtering is a popular technique for predicting potential user-item ratings from dense weighted graphs. In this paper, the authors propose a novel inductive collaborative filtering framework for predicting the user-specific latent factors in a user - item rating matrix in a transductive setting, where the base matrix factorization model is trained on a set of historical rating patterns. A hidden relational graph is constructed from the rating matrix, and the latent relational graph can be represented as a rating matrix. A relation inference model is used to infer the relationship between the relational graphs in the latent space. The proposed model is compared to feature-driven inductive models in terms of expressiveness, and is shown to outperform them for cold-start users. The model is also shown to perform better for inductive learning on matrix completion benchmarks. ","Collaborative filtering is a popular technique for predicting potential user-item ratings from dense weighted graphs. In this paper, the authors propose a novel inductive collaborative filtering framework for predicting the user-specific latent factors in a user - item rating matrix in a transductive setting, where the base matrix factorization model is trained on a set of historical rating patterns. A hidden relational graph is constructed from the rating matrix, and the latent relational graph can be represented as a rating matrix. A relation inference model is used to infer the relationship between the relational graphs in the latent space. The proposed model is compared to feature-driven inductive models in terms of expressiveness, and is shown to outperform them for cold-start users. The model is also shown to perform better for inductive learning on matrix completion benchmarks. "
1278,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"autoencoder - based disentangled representation learning methods USED-FOR disentanglement. disentangled representation learning CONJUNCTION reconstruction quality. reconstruction quality CONJUNCTION disentangled representation learning. detail information FEATURE-OF image data. correlated latent variables USED-FOR detail information. deep generative model USED-FOR missing correlated latent variables. deep generative model USED-FOR low - quality reconstruction. β - TCVAE HYPONYM-OF disentangled representation learning method. disentangled representation learning method USED-FOR disentangled factors. normalizing flows CONJUNCTION mixtures of Gaussians. mixtures of Gaussians CONJUNCTION normalizing flows. likelihood - based models CONJUNCTION implicit models. implicit models CONJUNCTION likelihood - based models. implicit models CONJUNCTION tractable models. tractable models CONJUNCTION implicit models. variational autoencoders CONJUNCTION implicit models. implicit models CONJUNCTION variational autoencoders. generative adversarial networks HYPONYM-OF implicit models. variational autoencoders HYPONYM-OF likelihood - based models. tractable models HYPONYM-OF model classes. likelihood - based models HYPONYM-OF model classes. mixtures of Gaussians HYPONYM-OF tractable models. implicit models HYPONYM-OF model classes. normalizing flows HYPONYM-OF tractable models. multi - stage model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE multi - stage model. reconstruction quality EVALUATE-FOR state - of - the - art methods. disentanglement EVALUATE-FOR state - of - the - art methods. disentanglement EVALUATE-FOR multi - stage model. reconstruction quality EVALUATE-FOR multi - stage model. OtherScientificTerm are statistical independence of the latent factors, and D - separation. Generic are approach, and model. Method is multi - stage modelling approach. ","This paper studies the problem of disentanglement in autoencoder-based disentangled representation learning methods. The authors propose a new approach to disentangle the statistical independence of the latent factors, which they call D-separation. The key idea is to use a deep generative model to model missing correlated latent variables in order to preserve the detail information in image data, and then use a disentangling representation learning method such as β-TCVAE to learn disentanged factors. The proposed approach is based on a multi-stage modelling approach, where each stage of the model is trained on a different set of latent factors.  The authors evaluate the performance of likelihood-based models (e.g., variational autoencoders, implicit models, tractable models such as normalizing flows and mixtures of Gaussians such as generative adversarial networks) and model classes (implicit models). They show that the multi-staging model achieves state-of-the-art performance in terms of D-distillation, disentangler, and reconstruction quality. ","This paper studies the problem of disentanglement in autoencoder-based disentangled representation learning methods. The authors propose a new approach to disentangle the statistical independence of the latent factors, which they call D-separation. The key idea is to use a deep generative model to model missing correlated latent variables in order to preserve the detail information in image data, and then use a disentangling representation learning method such as β-TCVAE to learn disentanged factors. The proposed approach is based on a multi-stage modelling approach, where each stage of the model is trained on a different set of latent factors.  The authors evaluate the performance of likelihood-based models (e.g., variational autoencoders, implicit models, tractable models such as normalizing flows and mixtures of Gaussians such as generative adversarial networks) and model classes (implicit models). They show that the multi-staging model achieves state-of-the-art performance in terms of D-distillation, disentangler, and reconstruction quality. "
1287,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information ( MI ) maximization USED-FOR learning representations of data. representations USED-FOR learning. representations USED-FOR reinforcement learning ( RL ). representations USED-FOR RL. MI objectives USED-FOR representations. samples of high - dimensional observations USED-FOR MI. state representation USED-FOR optimal policy. objectives USED-FOR insufficient representations. visual observations FEATURE-OF simulated game environment. OtherScientificTerm are irrelevant and redundant information, MI based objectives, and structure of the MDP. Task is control. Generic is methods. ","This paper studies the problem of learning representations of data using Mutual information (MI) maximization. The authors show that existing representations for reinforcement learning (RL) can be learned using MI objectives that are insensitive to irrelevant and redundant information. They also show that MI objectives for learning representations that are sensitive to irrelevant information can be used to learn representations for RL that are more robust to the structure of the MDP. Finally, they show that using samples of high-dimensional observations from MI is sufficient to learn an optimal policy that maximizes the mutual information between the state representation of the agent and the environment.    The paper is well-written, well-motivated, and well-structured. The paper provides a detailed analysis of existing MI based objectives. They show that insufficient representations are learned using these objectives, and that the optimal policy is learned by learning a state representation that is invariant to the irrelevant information.  They also provide experiments on a simulated game environment with visual observations, where the agent is given a set of visual observations and the goal is to learn a control policy that minimizes the optimal state representation. The results show that the proposed methods outperform existing methods. ","This paper studies the problem of learning representations of data using Mutual information (MI) maximization. The authors show that existing representations for reinforcement learning (RL) can be learned using MI objectives that are insensitive to irrelevant and redundant information. They also show that MI objectives for learning representations that are sensitive to irrelevant information can be used to learn representations for RL that are more robust to the structure of the MDP. Finally, they show that using samples of high-dimensional observations from MI is sufficient to learn an optimal policy that maximizes the mutual information between the state representation of the agent and the environment.    The paper is well-written, well-motivated, and well-structured. The paper provides a detailed analysis of existing MI based objectives. They show that insufficient representations are learned using these objectives, and that the optimal policy is learned by learning a state representation that is invariant to the irrelevant information.  They also provide experiments on a simulated game environment with visual observations, where the agent is given a set of visual observations and the goal is to learn a control policy that minimizes the optimal state representation. The results show that the proposed methods outperform existing methods. "
1296,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"finite dimensional representation FEATURE-OF semi - infinite dual. finite - dimensional convex copositive program USED-FOR non - convex neural network training problem. global optima CONJUNCTION copositive programs. copositive programs CONJUNCTION global optima. neural networks CONJUNCTION copositive programs. copositive programs CONJUNCTION neural networks. neural networks USED-FOR global optima. neural networks USED-FOR copositive programs. semi - nonnegative matrix factorization USED-FOR neural networks. semi - nonnegative matrix factorization USED-FOR copositive programs. algorithms USED-FOR global minimum. algorithms USED-FOR vector output neural network training problem. global minimum FEATURE-OF vector output neural network training problem. computational complexity EVALUATE-FOR filter size. computational complexity EVALUATE-FOR convolutional architectures. global optimum FEATURE-OF neural network training problem. soft - thresholded SVD USED-FOR neural network training problem. OtherScientificTerm is convex semi - infinite dual. Method are copositive relaxation, and Stochastic Gradient Descent. ","This paper studies the non-convex neural network training problem with a finite-dimensional convex copositive program, where the convex semi-infinite dual has a finite dimensional representation. The authors show that neural networks with a semi-nonnegative matrix factorization converge to the global optima and coppositive programs. They also show that the global optimum of neural networks trained with neural networks and copositives can be found by applying the semi-Nonnegative Matrix factorization.  The authors also provide algorithms for computing the global minimum of the vector output neural network learning problem, which is a special case of the copoitive relaxation of Stochastic Gradient Descent (SGD). They also provide a theoretical analysis of the computational complexity of convolutional architectures with respect to computational complexity depending on the filter size. Finally, the authors propose a soft-thresholded SVD to solve the neural model training problem and show that it converges to a global optimum. ","This paper studies the non-convex neural network training problem with a finite-dimensional convex copositive program, where the convex semi-infinite dual has a finite dimensional representation. The authors show that neural networks with a semi-nonnegative matrix factorization converge to the global optima and coppositive programs. They also show that the global optimum of neural networks trained with neural networks and copositives can be found by applying the semi-Nonnegative Matrix factorization.  The authors also provide algorithms for computing the global minimum of the vector output neural network learning problem, which is a special case of the copoitive relaxation of Stochastic Gradient Descent (SGD). They also provide a theoretical analysis of the computational complexity of convolutional architectures with respect to computational complexity depending on the filter size. Finally, the authors propose a soft-thresholded SVD to solve the neural model training problem and show that it converges to a global optimum. "
1305,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"vision USED-FOR learning disentangled, object - centric scene representations. unsupervised object segmentation USED-FOR LORL. MONet and Slot Attention HYPONYM-OF unsupervised object segmentation. algorithms USED-FOR object - centric representation. properties CONJUNCTION spatial relationships. spatial relationships CONJUNCTION properties. object categories CONJUNCTION properties. properties CONJUNCTION object categories. representations USED-FOR concepts. object categories HYPONYM-OF concepts. spatial relationships HYPONYM-OF concepts. properties HYPONYM-OF concepts. object - centric concepts USED-FOR object - centric representations. language USED-FOR object - centric concepts. LORL CONJUNCTION unsupervised segmentation algorithms. unsupervised segmentation algorithms CONJUNCTION LORL. LORL USED-FOR object segmentation. LORL USED-FOR MONet and Slot Attention. MONet and Slot Attention USED-FOR object segmentation. language USED-FOR LORL. concepts USED-FOR tasks. LORL CONJUNCTION segmentation algorithms. segmentation algorithms CONJUNCTION LORL. concepts CONJUNCTION segmentation algorithms. segmentation algorithms CONJUNCTION concepts. segmentation algorithms USED-FOR tasks. LORL USED-FOR concepts. MONet HYPONYM-OF segmentation algorithms. referring expression comprehension HYPONYM-OF tasks. OtherScientificTerm is language input. ","This paper presents a novel approach to learning disentangled, object-centric scene representations from vision. The authors propose LORL, an extension of unsupervised object segmentation based on MONet and Slot Attention. The algorithms are designed to learn an object-centric representation from a set of concepts (e.g., object categories, properties, spatial relationships, etc.). The authors show that the representations learned from these representations can capture a variety of concepts, including object categories and spatial relationships. They also show that object- centric concepts can be learned from language and can be used to guide the learning of object-perturbed representations. The paper also shows that the learned concepts and segmentation algorithms (LORL and MONet) can be combined with existing segmentation methods, and that LorL can be applied to the task of learning MONet or Slot Attention, and is able to achieve state-of-the-art performance on both tasks.    The authors also demonstrate that the language input is sufficient to capture object-related concepts, and show that learning these concepts from language can lead to better performance on other tasks such as referring expression comprehension. ","This paper presents a novel approach to learning disentangled, object-centric scene representations from vision. The authors propose LORL, an extension of unsupervised object segmentation based on MONet and Slot Attention. The algorithms are designed to learn an object-centric representation from a set of concepts (e.g., object categories, properties, spatial relationships, etc.). The authors show that the representations learned from these representations can capture a variety of concepts, including object categories and spatial relationships. They also show that object- centric concepts can be learned from language and can be used to guide the learning of object-perturbed representations. The paper also shows that the learned concepts and segmentation algorithms (LORL and MONet) can be combined with existing segmentation methods, and that LorL can be applied to the task of learning MONet or Slot Attention, and is able to achieve state-of-the-art performance on both tasks.    The authors also demonstrate that the language input is sufficient to capture object-related concepts, and show that learning these concepts from language can lead to better performance on other tasks such as referring expression comprehension. "
1314,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"fact triplets PART-OF KG. fact triplets USED-FOR embedding methods. logic rules USED-FOR rich background information. rules USED-FOR reasoning. EM - RBR USED-FOR multi - relation reasoning link prediction. relational background knowledge USED-FOR multi - relation reasoning link prediction. rules FEATURE-OF relational background knowledge. relational background knowledge USED-FOR EM - RBR. FB15k CONJUNCTION WN18. WN18 CONJUNCTION FB15k. EM - RBR COMPARE models. models COMPARE EM - RBR. dataset EVALUATE-FOR model. WN18 EVALUATE-FOR models. FB15k EVALUATE-FOR EM - RBR. FB15k EVALUATE-FOR models. Task are Knowledge graph completion, and representation of the knowledge graph. OtherScientificTerm are knowledge graph, algebraic space, and relational patterns. Method are embedding models, and embedding. Generic is framework. Metric is prediction accuracy. Material is FB15k - R. ","Knowledge graph completion is an important problem in machine learning, where the goal is to learn a representation of the knowledge graph, and embedding methods typically rely on fact triplets in the KG. However, embedding models are expensive to train due to the need to map the algebraic space to the embedding space. This paper proposes EM-RBR, which uses logic rules to capture rich background information in the representation of a knowledge graph to improve the prediction accuracy. The key idea is to use rules that capture the relational background knowledge of the rules for the reasoning. The proposed model is evaluated on a new dataset, FB15k-R, where it is shown to outperform existing models on FB15K and WN18. ","Knowledge graph completion is an important problem in machine learning, where the goal is to learn a representation of the knowledge graph, and embedding methods typically rely on fact triplets in the KG. However, embedding models are expensive to train due to the need to map the algebraic space to the embedding space. This paper proposes EM-RBR, which uses logic rules to capture rich background information in the representation of a knowledge graph to improve the prediction accuracy. The key idea is to use rules that capture the relational background knowledge of the rules for the reasoning. The proposed model is evaluated on a new dataset, FB15k-R, where it is shown to outperform existing models on FB15K and WN18. "
1323,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"video game HYPONYM-OF structured, dynamic environment. procedural knowledge USED-FOR Black - box models. monolithic hidden state USED-FOR Black - box models. modularity FEATURE-OF knowledge. architecture USED-FOR declarative and procedural knowledge. schemata USED-FOR state updates. active modules CONJUNCTION passive external knowledge sources. passive external knowledge sources CONJUNCTION active modules. passive external knowledge sources USED-FOR state updates. object files HYPONYM-OF active modules. schemata HYPONYM-OF passive external knowledge sources. active modules PART-OF architecture. attention USED-FOR object files. LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. input - output interface FEATURE-OF drop - in replacement. drop - in replacement PART-OF architecture. LSTM HYPONYM-OF normal recurrent networks. GRU HYPONYM-OF normal recurrent networks. OtherScientificTerm are declarative knowledge, systematicity, and object tokens. Generic is they. Metric is generalization. Material is intuitive physics benchmark. ","This paper proposes a novel architecture for learning both declarative and procedural knowledge in a structured, dynamic environment (e.g. a video game). Black-box models with monolithic hidden state typically rely on procedural knowledge, but they lack the modularity of the knowledge. The proposed architecture consists of two active modules (object files) and two passive external knowledge sources (schemata). The object files are learned via attention, and the schemata is used for state updates. The architecture also includes a drop-in replacement with an input-output interface that can be used to replace objects in the state space. The authors show that the proposed architecture outperforms normal recurrent networks such as LSTM and GRU in terms of generalization on the intuitive physics benchmark. The paper also shows that the architecture is able to generalize to a more general class of objects, and that it can generalize well to objects that are not present in the object tokens.  ","This paper proposes a novel architecture for learning both declarative and procedural knowledge in a structured, dynamic environment (e.g. a video game). Black-box models with monolithic hidden state typically rely on procedural knowledge, but they lack the modularity of the knowledge. The proposed architecture consists of two active modules (object files) and two passive external knowledge sources (schemata). The object files are learned via attention, and the schemata is used for state updates. The architecture also includes a drop-in replacement with an input-output interface that can be used to replace objects in the state space. The authors show that the proposed architecture outperforms normal recurrent networks such as LSTM and GRU in terms of generalization on the intuitive physics benchmark. The paper also shows that the architecture is able to generalize to a more general class of objects, and that it can generalize well to objects that are not present in the object tokens.  "
1332,SP:42a3c0453ab136537b5944a577d63412f3c22560,"Neural module networks ( NMN ) USED-FOR image - grounded tasks. synthetic images USED-FOR Visual Question Answering ( VQA ). Visual Question Answering ( VQA ) HYPONYM-OF image - grounded tasks. NMN USED-FOR video - grounded language tasks. complexity EVALUATE-FOR visual tasks. tasks COMPARE visual tasks. visual tasks COMPARE tasks. visual temporal variance FEATURE-OF visual tasks. complexity EVALUATE-FOR tasks. NMN approaches USED-FOR image - grounded tasks. information retrieval process USED-FOR video - grounded language tasks. VilNMN USED-FOR action - based inputs. VilNMN USED-FOR language components. video QA CONJUNCTION video - grounded dialogues. video - grounded dialogues CONJUNCTION video QA. video - grounded language tasks EVALUATE-FOR VilNMN. video - grounded dialogues HYPONYM-OF video - grounded language tasks. video QA HYPONYM-OF video - grounded language tasks. Method are neural modules, and neural module networks. OtherScientificTerm is visual cues. ","This paper proposes Neural module networks (NMN) for image-grounded tasks such as Visual Question Answering (VQA) from synthetic images. NMN has been shown to achieve better complexity than other visual tasks in terms of visual temporal variance, and the authors show that NMN can be applied to video-grounding language tasks as well. The authors also show that existing NMN approaches can be used to perform well on image-grained tasks.   The authors propose VilNMN, which is an extension of existing neural modules that is able to take visual cues as input to the neural module networks. The key idea is to use the information retrieval process to learn the language components of the neural modules, and then use Vil NMN to extract action-based inputs from the visual cues.  The experiments on video QA (video QA) and video -grounded dialogues show that VilNMNs are able to learn language components that are more robust to visual cues, and that the complexity of these visual tasks is comparable to that of the original visual tasks, while being able to generalize to new visual tasks.","This paper proposes Neural module networks (NMN) for image-grounded tasks such as Visual Question Answering (VQA) from synthetic images. NMN has been shown to achieve better complexity than other visual tasks in terms of visual temporal variance, and the authors show that NMN can be applied to video-grounding language tasks as well. The authors also show that existing NMN approaches can be used to perform well on image-grained tasks.   The authors propose VilNMN, which is an extension of existing neural modules that is able to take visual cues as input to the neural module networks. The key idea is to use the information retrieval process to learn the language components of the neural modules, and then use Vil NMN to extract action-based inputs from the visual cues.  The experiments on video QA (video QA) and video -grounded dialogues show that VilNMNs are able to learn language components that are more robust to visual cues, and that the complexity of these visual tasks is comparable to that of the original visual tasks, while being able to generalize to new visual tasks."
1341,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"empirical game analysis CONJUNCTION deep reinforcement learning ( Deep RL ). deep reinforcement learning ( Deep RL ) CONJUNCTION empirical game analysis. Deep RL USED-FOR best response. mixture of opponent policies USED-FOR best response. PSRO USED-FOR Deep RL training. algorithms USED-FOR PSRO. PSRO USED-FOR policies. policies PART-OF empirical game. Mixed - Opponents USED-FOR pure - strategy opponent. strategy ’s action - value estimates COMPARE policies. policies COMPARE strategy ’s action - value estimates. strategy ’s action - value estimates USED-FOR pure - strategy opponent. algorithms USED-FOR PSRO. algorithms USED-FOR game. Method are Policy - Space Response Oracles ( PSRO ), and Mixed - Oracles. Task is learning policies in multiagent systems. Generic are algorithm, first, second, and policy. OtherScientificTerm is unobserved distribution of opponents. ","This paper studies the problem of learning policies in multiagent systems. The authors propose Policy-Space Response Oracles (PSRO), an extension of empirical game analysis and deep reinforcement learning (Deep RL) to the setting of best response in Deep RL with a mixture of opponent policies.  The authors show that PSRO can be used to improve Deep RL training when the best response is based on the mixture of opponents.  PSRO uses two algorithms to train the policies in the empirical game. The first algorithm, called Mixed-Oracles, trains a pure-strategy opponent using the strategy’s action-value estimates instead of the actual policies, while the second, called Pure-strategies with Mixed-Opponents, uses the unobserved distribution of opponents to train a pure strategy opponent.  Experiments show that both algorithms are able to improve the performance of PSRO in the game.  ","This paper studies the problem of learning policies in multiagent systems. The authors propose Policy-Space Response Oracles (PSRO), an extension of empirical game analysis and deep reinforcement learning (Deep RL) to the setting of best response in Deep RL with a mixture of opponent policies.  The authors show that PSRO can be used to improve Deep RL training when the best response is based on the mixture of opponents.  PSRO uses two algorithms to train the policies in the empirical game. The first algorithm, called Mixed-Oracles, trains a pure-strategy opponent using the strategy’s action-value estimates instead of the actual policies, while the second, called Pure-strategies with Mixed-Opponents, uses the unobserved distribution of opponents to train a pure strategy opponent.  Experiments show that both algorithms are able to improve the performance of PSRO in the game.  "
1350,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,attention mechanism CONJUNCTION duration predictor. duration predictor CONJUNCTION attention mechanism. Tacotron 2 text - to - speech model USED-FOR Non - Attentive Tacotron. unaligned duration ratio CONJUNCTION word deletion rate. word deletion rate CONJUNCTION unaligned duration ratio. metrics USED-FOR large - scale robustness evaluation. pre - trained speech recognition model USED-FOR metrics. pre - trained speech recognition model USED-FOR large - scale robustness evaluation. Gaussian upsampling USED-FOR Non - Attentive Tacotron. Non - Attentive Tacotron COMPARE Tacotron 2. Tacotron 2 COMPARE Non - Attentive Tacotron. 5 - scale mean opinion score USED-FOR naturalness. 5 - scale mean opinion score EVALUATE-FOR Non - Attentive Tacotron. fine - grained variational auto - encoder USED-FOR duration predictor. semi - supervised or unsupervised manner USED-FOR duration predictor. semi - supervised or unsupervised manner USED-FOR method. fine - grained variational auto - encoder USED-FOR method. Metric is robustness. Method is supervised training. ,"This paper proposes Non-Attentive Tacotron, an extension of the TacOTron 2 text-to-speech model that combines the attention mechanism and a duration predictor. The authors propose two metrics for large-scale robustness evaluation based on a pre-trained speech recognition model. The key idea is to use unaligned duration ratio and word deletion rate as metrics for robustness.  The authors also propose to use Gaussian upsampling to improve the robustness of the non-attentive version of Non-Tacotron.  Experiments show that the proposed Non-attentionive TCTR achieves better results than the original TCTT with supervised training. The proposed method uses a fine-grained variational auto-encoder to learn the duration predictor in a semi-supervised or unsupervised manner, and uses a 5-scale mean opinion score as a measure of naturalness to measure the naturalness of a sentence.","This paper proposes Non-Attentive Tacotron, an extension of the TacOTron 2 text-to-speech model that combines the attention mechanism and a duration predictor. The authors propose two metrics for large-scale robustness evaluation based on a pre-trained speech recognition model. The key idea is to use unaligned duration ratio and word deletion rate as metrics for robustness.  The authors also propose to use Gaussian upsampling to improve the robustness of the non-attentive version of Non-Tacotron.  Experiments show that the proposed Non-attentionive TCTR achieves better results than the original TCTT with supervised training. The proposed method uses a fine-grained variational auto-encoder to learn the duration predictor in a semi-supervised or unsupervised manner, and uses a 5-scale mean opinion score as a measure of naturalness to measure the naturalness of a sentence."
1359,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"layout estimation USED-FOR planning and navigation. planning and navigation USED-FOR robotics applications. layout estimation USED-FOR robotics applications. self - driving HYPONYM-OF robotics applications. supervised end - to - end framework USED-FOR estimation of bird ’s eye view layout. deep learning networks USED-FOR disparity estimation. deep learning networks USED-FOR network. internal bird ’s eye view feature representation USED-FOR layout estimation. stereo images USED-FOR features. features USED-FOR disparity feature volume. stereo images USED-FOR disparity feature volume. scene structure FEATURE-OF coarse - grained information. rich bird ’s eye view representation USED-FOR spatial reasoning. IPM features USED-FOR rich bird ’s eye view representation. IPM features CONJUNCTION projected feature volume. projected feature volume CONJUNCTION IPM features. projected feature volume USED-FOR rich bird ’s eye view representation. representation USED-FOR BEV semantic map. IPM features USED-FOR supervisory signal. supervisory signal USED-FOR stereo features. IPM features USED-FOR stereo features. datasets EVALUATE-FOR approach. synthetically generated dataset EVALUATE-FOR approach. synthetically generated dataset HYPONYM-OF datasets. datasets EVALUATE-FOR baseline techniques. Method are explicit depth estimation, and inverse perspective mapping ( IPM ). Generic is it. OtherScientificTerm are bird ’s eye view coordinates, bird ’s eye view, and fine - grained texture information. ","This paper proposes a novel approach for learning a bird’s-eye view (BEV) semantic map of a scene, which can be used for layout estimation for planning and navigation in robotics applications (e.g. self-driving). The authors propose a supervised end-to-end framework for learning the representation of the BEV semantic map, which is based on inverse perspective mapping (IPM). The network is trained using deep learning networks to perform disparity estimation, where the features are generated from stereo images. The authors use these features to estimate the disparity feature volume using stereo images, and then use those features to train a supervisory signal using the IPM features and the projected feature volume of the stereo images to learn a rich bird's eye view representation for spatial reasoning. The proposed approach is evaluated on two datasets: a synthetically generated dataset, and a real-world dataset where the authors use explicit depth estimation.   ","This paper proposes a novel approach for learning a bird’s-eye view (BEV) semantic map of a scene, which can be used for layout estimation for planning and navigation in robotics applications (e.g. self-driving). The authors propose a supervised end-to-end framework for learning the representation of the BEV semantic map, which is based on inverse perspective mapping (IPM). The network is trained using deep learning networks to perform disparity estimation, where the features are generated from stereo images. The authors use these features to estimate the disparity feature volume using stereo images, and then use those features to train a supervisory signal using the IPM features and the projected feature volume of the stereo images to learn a rich bird's eye view representation for spatial reasoning. The proposed approach is evaluated on two datasets: a synthetically generated dataset, and a real-world dataset where the authors use explicit depth estimation.   "
1368,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,over - parameterization CONJUNCTION over - smoothing. over - smoothing CONJUNCTION over - parameterization. vanishing gradients CONJUNCTION over - parameterization. over - parameterization CONJUNCTION vanishing gradients. Relational Graph Neural Networks ( GNN ) COMPARE GNNs. GNNs COMPARE Relational Graph Neural Networks ( GNN ). methods USED-FOR GNNs. normalization techniques CONJUNCTION skip connection. skip connection CONJUNCTION normalization techniques. normalization techniques PART-OF methods. learning long - range patterns FEATURE-OF multi - relational graphs. GNNs USED-FOR learning long - range patterns. relation - aware GNN architecture USED-FOR long - range modeling between nodes. vector - based approach USED-FOR relation - aware GNN architecture. gated skip connections USED-FOR relation - aware GNN architecture. Graph Attention Network USED-FOR relation - aware GNN architecture. method COMPARE architectures. architectures COMPARE method. method COMPARE GNN variants. GNN variants COMPARE method. GNN variants COMPARE architectures. architectures COMPARE GNN variants. GNN variants USED-FOR deeper configurations. Method is deeper networks. Material is synthetic and real data. ,"This paper proposes Relational Graph Neural Networks (GNN), which is a generalization of GNNs with vanishing gradients, over-parameterization, and over-smoothing. Previous methods for learning long-range patterns in multi-relational graphs incorporate normalization techniques and a skip connection. The authors propose a relation-aware GNN architecture based on a vector-based approach that uses gated skip connections to learn the relation between two nodes in a graph, and a Graph Attention Network to encode the relation information between nodes in the graph. The proposed method is shown to outperform existing architectures and other GNN variants on both synthetic and real data. It is also shown that the proposed method can be used to learn deeper configurations than existing GNN methods. ","This paper proposes Relational Graph Neural Networks (GNN), which is a generalization of GNNs with vanishing gradients, over-parameterization, and over-smoothing. Previous methods for learning long-range patterns in multi-relational graphs incorporate normalization techniques and a skip connection. The authors propose a relation-aware GNN architecture based on a vector-based approach that uses gated skip connections to learn the relation between two nodes in a graph, and a Graph Attention Network to encode the relation information between nodes in the graph. The proposed method is shown to outperform existing architectures and other GNN variants on both synthetic and real data. It is also shown that the proposed method can be used to learn deeper configurations than existing GNN methods. "
1377,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"Symbolic techniques USED-FOR neural network properties. Satisfiability Modulo Theory ( SMT ) solvers USED-FOR Symbolic techniques. gradient - based methods CONJUNCTION symbolic techniques. symbolic techniques CONJUNCTION gradient - based methods. technique USED-FOR gradient - based methods. technique USED-FOR minimal regions. technique USED-FOR large networks. Integrated Gradients USED-FOR gradient information. gradient information USED-FOR approach. SMT constraints USED-FOR minimal input mask discovery problem. approach USED-FOR mask regions. approach USED-FOR minimal masks. ImageNet CONJUNCTION Beer Reviews. Beer Reviews CONJUNCTION ImageNet. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. saliency scores EVALUATE-FOR gradient - based methods. approach COMPARE gradient - based methods. gradient - based methods COMPARE approach. saliency scores EVALUATE-FOR approach. MNIST EVALUATE-FOR technique. Task are model explanation, and neural network ’s prediction. OtherScientificTerm are threshold, mask, and saliency map. ","Symbolic techniques for learning neural network properties based on Satisfiability Modulo Theory (SMT) solvers have recently gained a lot of attention in the field of model explanation. This paper proposes a new technique to combine gradient-based methods and symbolic techniques to learn minimal regions of the network. The proposed approach is based on Integrated Gradients, which uses gradient information from the input layer to learn the mask of the hidden layer. The key idea is to use SMT constraints to solve the minimal input mask discovery problem. The authors show that their approach is able to learn mask regions that are minimal for a given threshold. They also show that the technique can be applied to large networks. The technique is evaluated on MNIST, ImageNet, and Beer Reviews, and shows that the proposed approach achieves better saliency scores compared to gradient -based methods, and can learn minimal masks that are sufficient to explain the neural network’s prediction.   ","Symbolic techniques for learning neural network properties based on Satisfiability Modulo Theory (SMT) solvers have recently gained a lot of attention in the field of model explanation. This paper proposes a new technique to combine gradient-based methods and symbolic techniques to learn minimal regions of the network. The proposed approach is based on Integrated Gradients, which uses gradient information from the input layer to learn the mask of the hidden layer. The key idea is to use SMT constraints to solve the minimal input mask discovery problem. The authors show that their approach is able to learn mask regions that are minimal for a given threshold. They also show that the technique can be applied to large networks. The technique is evaluated on MNIST, ImageNet, and Beer Reviews, and shows that the proposed approach achieves better saliency scores compared to gradient -based methods, and can learn minimal masks that are sufficient to explain the neural network’s prediction.   "
1386,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,3D pose estimation HYPONYM-OF computer vision. deep learning approaches USED-FOR 3D pose estimation. deep neural networks CONJUNCTION 3D generative representations of objects. 3D generative representations of objects CONJUNCTION deep neural networks. deep neural networks PART-OF unified neural architecture. 3D generative representations of objects PART-OF unified neural architecture. generative vision models USED-FOR partial occlusion. NeMo HYPONYM-OF unified neural architecture. generative model of neural feature activations PART-OF dense 3D mesh. generative model of neural feature activations USED-FOR NeMo. differentiable rendering USED-FOR 3D object pose. NeMo CONJUNCTION feature representation of the target image. feature representation of the target image CONJUNCTION NeMo. reconstruction error EVALUATE-FOR NeMo. feature representations PART-OF mesh. local optima FEATURE-OF reconstruction loss. feature extractor USED-FOR feature representations. contrastive learning USED-FOR feature representations. contrastive learning USED-FOR feature extractor. occluded - PASCAL3D+ CONJUNCTION ObjectNet3D. ObjectNet3D CONJUNCTION occluded - PASCAL3D+. PASCAL3D+ CONJUNCTION occluded - PASCAL3D+. occluded - PASCAL3D+ CONJUNCTION PASCAL3D+. NeMo COMPARE deep networks. deep networks COMPARE NeMo. NeMo USED-FOR partial occlusion. regular data EVALUATE-FOR NeMo. mesh representation USED-FOR object geometry. 3D geometry USED-FOR 3D pose estimation. cuboid USED-FOR mesh representation. mesh representation USED-FOR NeMo. cuboid USED-FOR object geometry. Generic is code. ,"This paper proposes a unified neural architecture called NeMo, which combines deep neural networks and 3D generative representations of objects in a unified framework. NeMo is built on top of existing deep learning approaches for 3D pose estimation, which is an important problem in computer vision. The core idea is to learn a generative model of neural feature activations in a dense 3D mesh using generative vision models for partial occlusion, and then use a differentiable rendering to obtain a 3D object pose. The reconstruction error of NeMo depends on the reconstruction error between NeMo and the feature representation of the target image, and the reconstruction loss depends on local optima. The feature extractor uses contrastive learning to learn the feature representations of the mesh. The code is well-written and easy to follow. Experiments are conducted on PASCAL3D+ and occluded-PASCAL-3D+, and ObjectNet3D, and show that NeMo achieves better performance than other deep networks on the regular data. The main contribution of the paper is the mesh representation learned by NeMo as a cuboid, which can be used as a mesh representation to represent object geometry in 3D geometry.","This paper proposes a unified neural architecture called NeMo, which combines deep neural networks and 3D generative representations of objects in a unified framework. NeMo is built on top of existing deep learning approaches for 3D pose estimation, which is an important problem in computer vision. The core idea is to learn a generative model of neural feature activations in a dense 3D mesh using generative vision models for partial occlusion, and then use a differentiable rendering to obtain a 3D object pose. The reconstruction error of NeMo depends on the reconstruction error between NeMo and the feature representation of the target image, and the reconstruction loss depends on local optima. The feature extractor uses contrastive learning to learn the feature representations of the mesh. The code is well-written and easy to follow. Experiments are conducted on PASCAL3D+ and occluded-PASCAL-3D+, and ObjectNet3D, and show that NeMo achieves better performance than other deep networks on the regular data. The main contribution of the paper is the mesh representation learned by NeMo as a cuboid, which can be used as a mesh representation to represent object geometry in 3D geometry."
1395,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"large scale retrieval - based applications USED-FOR Feature Compatible Learning ( FCL ). approaches USED-FOR feature compatible learning. old training data CONJUNCTION classifiers. classifiers CONJUNCTION old training data. old training data USED-FOR approaches. classifiers USED-FOR approaches. approach USED-FOR feature compatible learning. features USED-FOR approach. unified framework USED-FOR FCL. model USED-FOR pseudo classifier. random walk algorithm USED-FOR it. model USED-FOR embedding features. ImageNet ILSVRC 2012 and Places365 data EVALUATE-FOR approach. Method are embedding model, and Non - Inherent Feature Compatible Learning. Generic is old model. ","This paper proposes Feature Compatible Learning (FCL) for large scale retrieval-based applications. Previous approaches to feature compatible learning rely on old training data and classifiers, but the embedding model is not feature compatible. The authors propose a unified framework to solve the problem of non-inherent Feature Compatibility (NIC) and propose a novel approach to solve Feature Compcompatible Learning (FCL) based on features from the old model. The key idea is to train a pseudo classifier based on the learned embedding of the original model, and then train a model based on these embedding features. The proposed approach is called Non-Inherent feature Compatibility Learning and it is based on a random walk algorithm. Experiments on ImageNet ILSVRC 2012 and Places365 data demonstrate the effectiveness of the proposed approach. ","This paper proposes Feature Compatible Learning (FCL) for large scale retrieval-based applications. Previous approaches to feature compatible learning rely on old training data and classifiers, but the embedding model is not feature compatible. The authors propose a unified framework to solve the problem of non-inherent Feature Compatibility (NIC) and propose a novel approach to solve Feature Compcompatible Learning (FCL) based on features from the old model. The key idea is to train a pseudo classifier based on the learned embedding of the original model, and then train a model based on these embedding features. The proposed approach is called Non-Inherent feature Compatibility Learning and it is based on a random walk algorithm. Experiments on ImageNet ILSVRC 2012 and Places365 data demonstrate the effectiveness of the proposed approach. "
1404,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"generalization error EVALUATE-FOR deep neural networks ( DNNs ). validation accuracy EVALUATE-FOR model selection. gradient norm USED-FOR model selection criterion. generalization error CONJUNCTION gradient norm measures. gradient norm measures CONJUNCTION generalization error. efficiency CONJUNCTION effectiveness. effectiveness CONJUNCTION efficiency. approximated gradient norm USED-FOR models. approximated gradient norm USED-FOR hyper - parameter search objectives. generalization error EVALUATE-FOR models. gradient norm CONJUNCTION generalization error. generalization error CONJUNCTION gradient norm. BOHB HYPONYM-OF bandit - based or population - based algorithms. gradient norm USED-FOR models. gradient norm USED-FOR generalization. generalization EVALUATE-FOR models. gradient norm COMPARE algorithms. algorithms COMPARE gradient norm. models COMPARE algorithms. algorithms COMPARE models. architectures USED-FOR models. Method are neural network architectures, hyper - parameter optimization, and DNNs. Metric are gradient complexity, computation cost, and computation overhead. OtherScientificTerm are loss gradient, and gradient norm objectives. ","This paper studies the generalization error of deep neural networks (DNNs) and their hyperparameter optimization. The authors show that hyperparameters of DNNs can be approximated by the gradient norm of the loss function, which can be used as a generalization metric. The paper also shows that the hyper-parameter search objective can be decomposed into two objectives: efficiency and effectiveness. The efficiency objective is based on the fact that the model selection criterion is a function of the model's generalization performance, while the effectiveness objective is a measure of the efficiency of the network.","This paper studies the generalization error of deep neural networks (DNNs) and their hyperparameter optimization. The authors show that hyperparameters of DNNs can be approximated by the gradient norm of the loss function, which can be used as a generalization metric. The paper also shows that the hyper-parameter search objective can be decomposed into two objectives: efficiency and effectiveness. The efficiency objective is based on the fact that the model selection criterion is a function of the model's generalization performance, while the effectiveness objective is a measure of the efficiency of the network."
1413,SP:13359456defb953dd2d19e1f879100ce392d6be6,"Wikipedia HYPONYM-OF Encyclopedias. entity linking CONJUNCTION open - domain question answering. open - domain question answering CONJUNCTION entity linking. open - domain question answering HYPONYM-OF knowledge - intensive tasks. entity linking HYPONYM-OF knowledge - intensive tasks. weight vectors USED-FOR entity representations. entity meta information USED-FOR entity representations. memory footprint USED-FOR dense representations. vector dot product USED-FOR entity affinity. GENRE HYPONYM-OF system. context CONJUNCTION entity name. entity name CONJUNCTION context. vocabulary size COMPARE entity count. entity count COMPARE vocabulary size. datasets USED-FOR entity disambiguation. datasets EVALUATE-FOR approach. Generic is approaches. Method are classifiers, autoregressive formulation, and encoder - decoder architecture. OtherScientificTerm are missing fine - grained interactions, softmax loss, and entities. Material is negative data. ","This paper proposes a novel approach for entity disambigmenting in knowledge-intensive tasks (e.g., entity linking and open-domain question answering). The authors propose to use a novel autoregressive formulation to learn the entity representations from weight vectors, which are then used to train classifiers. The key idea is to leverage the entity meta information to learn entity representations with a smaller memory footprint, and to learn dense representations that are robust to missing fine-grained interactions. The authors also propose a novel encoder-decoder architecture, where a softmax loss is used to disentangle the entities from each other, and a vector dot product is used for entity affinity. The proposed system, GENRE, is evaluated on two datasets (Encyclopedia of Encyclopedias and Wikipedia). The experiments show that the proposed approach outperforms existing approaches on both datasets in terms of the amount of negative data and the number of entities. The paper also shows that the vocabulary size is much smaller than the total entity count. ","This paper proposes a novel approach for entity disambigmenting in knowledge-intensive tasks (e.g., entity linking and open-domain question answering). The authors propose to use a novel autoregressive formulation to learn the entity representations from weight vectors, which are then used to train classifiers. The key idea is to leverage the entity meta information to learn entity representations with a smaller memory footprint, and to learn dense representations that are robust to missing fine-grained interactions. The authors also propose a novel encoder-decoder architecture, where a softmax loss is used to disentangle the entities from each other, and a vector dot product is used for entity affinity. The proposed system, GENRE, is evaluated on two datasets (Encyclopedia of Encyclopedias and Wikipedia). The experiments show that the proposed approach outperforms existing approaches on both datasets in terms of the amount of negative data and the number of entities. The paper also shows that the vocabulary size is much smaller than the total entity count. "
1422,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"infinite time horizon FEATURE-OF unknown congestion functions. fe CONJUNCTION congestion function. congestion function CONJUNCTION fe. observation USED-FOR routing decisions. algorithm USED-FOR ce. algorithm USED-FOR routing decisions. observation USED-FOR algorithm. total cost CONJUNCTION minimum cost. minimum cost CONJUNCTION total cost. cumulative regret FEATURE-OF algorithm. space complexity CONJUNCTION time complexity. time complexity CONJUNCTION space complexity. time complexity EVALUATE-FOR algorithm. space complexity EVALUATE-FOR algorithm. Task is routing users. OtherScientificTerm are unknown distribution, routing requests, and regret. Material is New York City road networks. ","This paper considers the problem of routing users to routes that are in an unknown distribution, with unknown congestion functions over an infinite time horizon. The authors consider the setting where the routing decisions are made based on observation, and the unknown distribution of the routing requests is unknown. They propose an algorithm that can be used to estimate the total cost and the minimum cost of a route in this setting. The algorithm is based on the observation that the distance between a routing user and the congestion function is a function of the number of times the user has used the route in the past, and that the total distance between the user and a congestion function in the future is the distance from the current routing user to the current congestion function. They show that the algorithm is able to estimate this distance in a finite time horizon (i.e., when the routing users have not yet used it). They also show that their algorithm achieves a cumulative regret of $O(\epsilon^{-1.5})$, which is a lower bound on the regret of the algorithm when the distance is known. Finally, they show that this algorithm has a space complexity that matches the space complexity and the time complexity of the original algorithm.    The paper is well-written and well-motivated, and is well motivated. The paper provides an interesting perspective on routing users in New York City road networks, and provides a theoretical analysis of their algorithm.","This paper considers the problem of routing users to routes that are in an unknown distribution, with unknown congestion functions over an infinite time horizon. The authors consider the setting where the routing decisions are made based on observation, and the unknown distribution of the routing requests is unknown. They propose an algorithm that can be used to estimate the total cost and the minimum cost of a route in this setting. The algorithm is based on the observation that the distance between a routing user and the congestion function is a function of the number of times the user has used the route in the past, and that the total distance between the user and a congestion function in the future is the distance from the current routing user to the current congestion function. They show that the algorithm is able to estimate this distance in a finite time horizon (i.e., when the routing users have not yet used it). They also show that their algorithm achieves a cumulative regret of $O(\epsilon^{-1.5})$, which is a lower bound on the regret of the algorithm when the distance is known. Finally, they show that this algorithm has a space complexity that matches the space complexity and the time complexity of the original algorithm.    The paper is well-written and well-motivated, and is well motivated. The paper provides an interesting perspective on routing users in New York City road networks, and provides a theoretical analysis of their algorithm."
1431,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"BERT HYPONYM-OF Masked Language Models ( MLMs ). uniform masking USED-FOR MLM. PMI - Masking HYPONYM-OF masking strategy. Pointwise Mutual Information ( PMI ) USED-FOR masking strategy. entity / phrase masking CONJUNCTION random - span masking. random - span masking CONJUNCTION entity / phrase masking. whole - word masking CONJUNCTION entity / phrase masking. entity / phrase masking CONJUNCTION whole - word masking. PMIMasking COMPARE prior more heuristic approaches. prior more heuristic approaches COMPARE PMIMasking. entity / phrase masking HYPONYM-OF prior more heuristic approaches. whole - word masking HYPONYM-OF prior more heuristic approaches. random - span masking HYPONYM-OF random uniform token masking. entity / phrase masking HYPONYM-OF random uniform token masking. whole - word masking HYPONYM-OF random uniform token masking. PMI - Masking COMPARE prior masking approaches. prior masking approaches COMPARE PMI - Masking. OtherScientificTerm are shallow local signals, token n - gram, and collocation. Task is pretraining inefficiency. ","This paper proposes a new masking strategy based on Pointwise mutual information (PMI) for Masked Language Models (MLMs) such as BERT. The authors argue that uniform masking in an MLM can be problematic due to shallow local signals. To address this issue, the authors propose PMI-Masking, which is a simple masking technique that is based on the Pointwise Mutual Information (PMI) between a token n-gram and a token r-gram. PMIMasking is shown to outperform prior more heuristic approaches such as whole-word masking, entity/phrase masking and random-span masking such as random uniform token masking (e.g. entity/phrase masking) and random uniform tokens masking like entity/quotes masking. The main contribution of the paper is that the authors show that PMI - Masking outperforms prior masking approaches in terms of pretraining inefficiency, especially when the collocation is low.","This paper proposes a new masking strategy based on Pointwise mutual information (PMI) for Masked Language Models (MLMs) such as BERT. The authors argue that uniform masking in an MLM can be problematic due to shallow local signals. To address this issue, the authors propose PMI-Masking, which is a simple masking technique that is based on the Pointwise Mutual Information (PMI) between a token n-gram and a token r-gram. PMIMasking is shown to outperform prior more heuristic approaches such as whole-word masking, entity/phrase masking and random-span masking such as random uniform token masking (e.g. entity/phrase masking) and random uniform tokens masking like entity/quotes masking. The main contribution of the paper is that the authors show that PMI - Masking outperforms prior masking approaches in terms of pretraining inefficiency, especially when the collocation is low."
1440,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"Amortised inference USED-FOR sequential latent - variable models ( LVMs ). Bayesian filter HYPONYM-OF mixture of smoothing posteriors. ELBO objective USED-FOR partially - conditioned amortised posteriors. partially - conditioned amortised posteriors USED-FOR products of smoothing posteriors. traffic flow CONJUNCTION handwritten digits. handwritten digits CONJUNCTION traffic flow. handwritten digits CONJUNCTION aerial vehicle dynamics. aerial vehicle dynamics CONJUNCTION handwritten digits. aerial vehicle dynamics HYPONYM-OF scenarios. traffic flow HYPONYM-OF scenarios. handwritten digits HYPONYM-OF scenarios. generative modelling CONJUNCTION multi - step prediction. multi - step prediction CONJUNCTION generative modelling. fully - conditioned approximate posteriors USED-FOR generative modelling. OtherScientificTerm are evidence lower bound ( ELBO ), variational posteriors, posteriors, and approximate posteriors. Generic is setting. Method is generative model. ","This paper studies the problem of amortised inference for sequential latent-variable models (LVMs). In this setting, the goal is to maximize the evidence lower bound (ELBO) of the posterior of the variational posteriors. The authors propose a novel mixture of smoothing posteriors (e.g., a Bayesian filter) to achieve this goal. To achieve this, the authors introduce a partially-conditioned version of the ELBO objective, which is based on the product of products of the products of smoothings of the posteriors from the previous step and the current step.   The authors show that this setting can be used to train a generative model that is amortized in the sense that the posterior over the current and previous step is not necessarily the same, but can be a mixture of different products of different smoothings.  The paper also shows that this is the case for a number of scenarios including traffic flow, handwritten digits, and aerial vehicle dynamics. The main contribution of the paper is that the authors propose to use a fully-conditional approximate posteriors for generative modelling and multi-step prediction, and show that the resulting generative models can be trained with fully-conditioning approximate posterior. ","This paper studies the problem of amortised inference for sequential latent-variable models (LVMs). In this setting, the goal is to maximize the evidence lower bound (ELBO) of the posterior of the variational posteriors. The authors propose a novel mixture of smoothing posteriors (e.g., a Bayesian filter) to achieve this goal. To achieve this, the authors introduce a partially-conditioned version of the ELBO objective, which is based on the product of products of the products of smoothings of the posteriors from the previous step and the current step.   The authors show that this setting can be used to train a generative model that is amortized in the sense that the posterior over the current and previous step is not necessarily the same, but can be a mixture of different products of different smoothings.  The paper also shows that this is the case for a number of scenarios including traffic flow, handwritten digits, and aerial vehicle dynamics. The main contribution of the paper is that the authors propose to use a fully-conditional approximate posteriors for generative modelling and multi-step prediction, and show that the resulting generative models can be trained with fully-conditioning approximate posterior. "
1449,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"statistical properties FEATURE-OF distributed kernel ridge regression. distributed kernel ridge regression CONJUNCTION random features ( DKRR - RF ). random features ( DKRR - RF ) CONJUNCTION distributed kernel ridge regression. statistical properties FEATURE-OF random features ( DKRR - RF ). divide - and - conquer technique CONJUNCTION random features. random features CONJUNCTION divide - and - conquer technique. O(|D| ) memory CONJUNCTION O(|D| ) time. O(|D| ) time CONJUNCTION O(|D| ) memory. statistical accuracy EVALUATE-FOR KRR. random features USED-FOR KRR. divide - and - conquer technique USED-FOR KRR. O(|D| ) time USED-FOR KRR. O(|D| ) memory USED-FOR KRR. communication strategy USED-FOR DKRR - RF. OtherScientificTerm are optimal generalization bounds, generalization bounds, and average information. Generic is theoretical bounds. ","This paper studies the statistical properties of distributed kernel ridge regression and random features (DKRR-RF). The authors provide optimal generalization bounds for KRR with O(D) memory and O(|D|) time. In particular, the authors show that under certain assumptions on the average information, KRR achieves a statistical accuracy that matches the theoretical bounds. The authors also show that KRR can be solved efficiently with the divide-and-conquer technique and the random features. Finally, they propose a new communication strategy to improve the performance of DKRR- RF. ","This paper studies the statistical properties of distributed kernel ridge regression and random features (DKRR-RF). The authors provide optimal generalization bounds for KRR with O(D) memory and O(|D|) time. In particular, the authors show that under certain assumptions on the average information, KRR achieves a statistical accuracy that matches the theoretical bounds. The authors also show that KRR can be solved efficiently with the divide-and-conquer technique and the random features. Finally, they propose a new communication strategy to improve the performance of DKRR- RF. "
1458,SP:129872706a12d89f0886c2ad0fd4083d0632343c,"search step USED-FOR architectures. validation performance EVALUATE-FOR architectures. accuracy EVALUATE-FOR weight - sharing architectures. RandomNAS USED-FOR architectures. global search space(GS ) FEATURE-OF architectures. top - performing architectures PART-OF GS. proxy search space ( PS ) USED-FOR RandomNAS. EPS HYPONYM-OF Proxy Search Space. EPS HYPONYM-OF RandomNAS - based approach. EPS COMPARE state - of - the - art. state - of - the - art COMPARE EPS. NASBench-201 EVALUATE-FOR EPS. image classification CONJUNCTION natural language processing. natural language processing CONJUNCTION image classification. DARTS - like search spaces USED-FOR tasks. EPS USED-FOR tasks. search time EVALUATE-FOR EPS. natural language processing HYPONYM-OF tasks. image classification HYPONYM-OF tasks. Method is NAS approach. Metric are achievable accuracy, and RandomNAS ’s search efficiency. OtherScientificTerm are NAS search space, and ground - truth ranking. ","This paper proposes a new NAS approach, RandomNAS, which aims to improve the achievable accuracy of weight-sharing architectures in terms of validation performance. In particular, the authors propose a new search step to select the best architectures in the global search space(GS) to be used in the next step of the NAS search space. The authors argue that the current state-of-the-art achieves high accuracy in the GS, but the current architectures are not efficient in the search step. To improve RandomNAS’s search efficiency, they propose a proxy search space (PS) called EPS, which is a RandomNAS-based approach. EPS is a Proxy Search Space where the top-performing architectures in GS are replaced by the top performing ones in GS. The paper shows that the proposed EPS outperforms the state of the art on NASBench-201 and shows that EPS can reduce the search time by a factor of 2.5 to 3.5 times. They also show that EPS performs well on two tasks (image classification and natural language processing) with DARTS-like search spaces, and that EPS is more efficient for these tasks than the state- of-the art. Finally, they show that the performance of EPS is comparable to the ground-truth ranking. ","This paper proposes a new NAS approach, RandomNAS, which aims to improve the achievable accuracy of weight-sharing architectures in terms of validation performance. In particular, the authors propose a new search step to select the best architectures in the global search space(GS) to be used in the next step of the NAS search space. The authors argue that the current state-of-the-art achieves high accuracy in the GS, but the current architectures are not efficient in the search step. To improve RandomNAS’s search efficiency, they propose a proxy search space (PS) called EPS, which is a RandomNAS-based approach. EPS is a Proxy Search Space where the top-performing architectures in GS are replaced by the top performing ones in GS. The paper shows that the proposed EPS outperforms the state of the art on NASBench-201 and shows that EPS can reduce the search time by a factor of 2.5 to 3.5 times. They also show that EPS performs well on two tasks (image classification and natural language processing) with DARTS-like search spaces, and that EPS is more efficient for these tasks than the state- of-the art. Finally, they show that the performance of EPS is comparable to the ground-truth ranking. "
1467,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"it CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION it. imitation learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION imitation learning. imitation learning CONJUNCTION meta reinforcement learning. meta reinforcement learning CONJUNCTION imitation learning. imitation learning USED-FOR method. Probabilistic Embeddings USED-FOR method. PERIL USED-FOR exploration policies. Dual inference strategies USED-FOR PERIL. imitation learning COMPARE approach. approach COMPARE imitation learning. uncertainties FEATURE-OF it. it EVALUATE-FOR approach. meta - RL USED-FOR PERIL. sparse rewards FEATURE-OF meta - RL benchmarks. Method are Imitation learning, and meta reinforcement learning ( meta - RL ). Task is exploration. Material is interaction data. Metric is adaptation rates. ","This paper proposes a meta-RL algorithm called PERIL, which is an extension of the meta-learning framework to meta-reinforcement learning (meta-RL). The main idea is to learn a probabilistic embedding of the environment, which can then be used to train an RL agent to explore the environment. The authors show that the proposed algorithm outperforms the baselines on a number of tasks, and that it can be used in combination with reinforcement learning.","This paper proposes a meta-RL algorithm called PERIL, which is an extension of the meta-learning framework to meta-reinforcement learning (meta-RL). The main idea is to learn a probabilistic embedding of the environment, which can then be used to train an RL agent to explore the environment. The authors show that the proposed algorithm outperforms the baselines on a number of tasks, and that it can be used in combination with reinforcement learning."
1476,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"learning method USED-FOR image classification. overparameterized convolutional neural networks USED-FOR image classification. overparameterized convolutional neural networks HYPONYM-OF learning method. overparameterized convolutional neural networks CONJUNCTION gradient based optimization. gradient based optimization CONJUNCTION overparameterized convolutional neural networks. 3 - layer overparameterized convolutional network CONJUNCTION stochastic gradient descent ( SGD ). stochastic gradient descent ( SGD ) CONJUNCTION 3 - layer overparameterized convolutional network. orthogonal patches PART-OF images. 3 - layer overparameterized convolutional network USED-FOR images. pattern detectors CONJUNCTION detected patterns. detected patterns CONJUNCTION pattern detectors. SGD USED-FOR setting. pattern statistics USED-FOR dot - product. learning algorithm USED-FOR PSI. learning algorithm USED-FOR setting. sample complexity EVALUATE-FOR learning algorithm. overparameterized CNNs USED-FOR MNIST. non - orthogonal patches FEATURE-OF MNIST. non - orthogonal patches USED-FOR overparameterized CNNs. Task is image classification task. OtherScientificTerm are Pattern Statistics Inductive Bias ( PSI ), filter dimension, and VC dimension lower bound. Generic is it. ","This paper proposes a new learning method for image classification based on overparameterized convolutional neural networks. The authors propose a learning method called Pattern Statistics Inductive Bias (PSI) which is a generalization of previous work on the image classification task.   The main contribution of this paper is to extend the previous work to the setting where orthogonal patches in images are represented by a 3-layer overparametersized CNN and stochastic gradient descent (SGD) is used.  The authors show that under this setting, the learning algorithm for PSI has a sample complexity of $O(\sqrt{n\log n})$, where $n$ is the filter dimension and $N$ the number of pattern detectors and $n\times n$ the detected patterns.  In the original work, the dot-product of the pattern statistics is $n^2/\sqrt{\log n}$. In this paper, it is shown that the VC dimension lower bound is $O(n^3/n)$. The authors also show that the sample complexity for this setting can be improved by using SGD.  Finally, the authors demonstrate that under the assumption that the orthogonality of the patches in the images is bounded by a constant factor, a learning algorithm can be used to reduce the PSI to $\Omega(n)$ and show that it can be shown to converge to the optimal solution.  Experiments are conducted on MNIST with non-orthogonal patch size and on the problem of learning overparametrized neural networks, and the authors also demonstrate that the overparametric version of their algorithm is able to learn a good solution for the problem in this setting.","This paper proposes a new learning method for image classification based on overparameterized convolutional neural networks. The authors propose a learning method called Pattern Statistics Inductive Bias (PSI) which is a generalization of previous work on the image classification task.   The main contribution of this paper is to extend the previous work to the setting where orthogonal patches in images are represented by a 3-layer overparametersized CNN and stochastic gradient descent (SGD) is used.  The authors show that under this setting, the learning algorithm for PSI has a sample complexity of $O(\sqrt{n\log n})$, where $n$ is the filter dimension and $N$ the number of pattern detectors and $n\times n$ the detected patterns.  In the original work, the dot-product of the pattern statistics is $n^2/\sqrt{\log n}$. In this paper, it is shown that the VC dimension lower bound is $O(n^3/n)$. The authors also show that the sample complexity for this setting can be improved by using SGD.  Finally, the authors demonstrate that under the assumption that the orthogonality of the patches in the images is bounded by a constant factor, a learning algorithm can be used to reduce the PSI to $\Omega(n)$ and show that it can be shown to converge to the optimal solution.  Experiments are conducted on MNIST with non-orthogonal patch size and on the problem of learning overparametrized neural networks, and the authors also demonstrate that the overparametric version of their algorithm is able to learn a good solution for the problem in this setting."
1485,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,approach USED-FOR representation learning. Contrastive learning USED-FOR representation learning. contrastive learning USED-FOR representation of documents. topic modeling assumptions FEATURE-OF document classification. topic posterior information FEATURE-OF representation of documents. procedure USED-FOR semi - supervised setup. linear classifiers USED-FOR document classification tasks. representations USED-FOR linear classifiers. OtherScientificTerm is embeddings of data. Method is linear models. ,"This paper proposes a new approach for representation learning based on contrastive learning. The authors show that under certain topic modeling assumptions for document classification, the representation of documents with different topic posterior information can be learned using contrastive methods. The paper also shows that the proposed procedure can be applied to a semi-supervised setup, where the embeddings of data are not fully supervised.  The authors also show that linear classifiers trained on these representations can be used to solve document classification tasks, and that linear models can be trained on the learned representations.   ","This paper proposes a new approach for representation learning based on contrastive learning. The authors show that under certain topic modeling assumptions for document classification, the representation of documents with different topic posterior information can be learned using contrastive methods. The paper also shows that the proposed procedure can be applied to a semi-supervised setup, where the embeddings of data are not fully supervised.  The authors also show that linear classifiers trained on these representations can be used to solve document classification tasks, and that linear models can be trained on the learned representations.   "
1494,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"vision CONJUNCTION language. language CONJUNCTION vision. Multimodal learning USED-FOR generative models. it USED-FOR generalisable representations. related ” multimodal data USED-FOR models. contrastive framework USED-FOR generative model learning. contrastive framework USED-FOR model. method USED-FOR multimodal learning. framework USED-FOR generative model. Task is learning generalisable representations. Method is multimodal variational autoencoder ( VAE ) models. Material is unlabeled, unpaired multimodal data. ","Multimodal learning is an important problem in learning generalisable representations, especially in the context of vision and language, where it is important for generative models to be able to generalizable to a wide range of modalities. The authors propose to use multimodal variational autoencoder (VAE) models to address this problem. They show that models trained on “related” multimodial data can generalize to unlabeled, unpaired multimodals. They propose a novel contrastive framework to improve generative model learning and show that it can be used to learn more generalizable representations. They also show that the proposed method can be applied to the problem of multi-modal learning.   The authors also propose a method to improve the generative performance of the proposed framework. The proposed model is based on the contrastive approach proposed in [1].  ","Multimodal learning is an important problem in learning generalisable representations, especially in the context of vision and language, where it is important for generative models to be able to generalizable to a wide range of modalities. The authors propose to use multimodal variational autoencoder (VAE) models to address this problem. They show that models trained on “related” multimodial data can generalize to unlabeled, unpaired multimodals. They propose a novel contrastive framework to improve generative model learning and show that it can be used to learn more generalizable representations. They also show that the proposed method can be applied to the problem of multi-modal learning.   The authors also propose a method to improve the generative performance of the proposed framework. The proposed model is based on the contrastive approach proposed in [1].  "
1503,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"Variational autoencoders ( VAEs ) HYPONYM-OF likelihood - based generative models. base prior distribution CONJUNCTION reweighting factor. reweighting factor CONJUNCTION base prior distribution. reweighting factor USED-FOR energy - based prior. base prior distribution USED-FOR energy - based prior. it USED-FOR hierarchical VAEs. latent variable groups FEATURE-OF hierarchical VAEs. noise contrastive estimation USED-FOR reweighting factor. CIFAR-10 CONJUNCTION CelebA 64. CelebA 64 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CelebA 64 CONJUNCTION CelebA HQ 256 datasets. CelebA HQ 256 datasets CONJUNCTION CelebA 64. generative EVALUATE-FOR VAEs. noise contrastive priors USED-FOR VAEs. noise contrastive priors USED-FOR generative. MNIST EVALUATE-FOR VAEs. Generic is they. OtherScientificTerm are prior, tempering, prior hole problem, prior distribution, aggregate approximate posterior, latent space, and aggregate posterior. ","This paper proposes Variational autoencoders (VAEs), a class of likelihood-based generative models. The authors argue that they can be seen as an extension of VAEs, where the prior is a prior over a set of latent variable groups, and that the prior can be regarded as an energy-based prior over the base prior distribution and a reweighting factor that is a function of the prior distribution of the latent variables in the prior.  The authors show that this prior is robust to tempering and that it can be used to train hierarchical VAEs with respect to the latent variable group, and it can also be used as a way to mitigate the prior hole problem. They also show that the reweighted factor is a result of noise contrastive estimation, and they show that it works well when the prior has a large enough gap between the aggregate approximate posterior and the true posterior in the latent space.  Experiments are conducted on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets, and show that VAEs trained with noise contrastively priors perform better in generative performance. ","This paper proposes Variational autoencoders (VAEs), a class of likelihood-based generative models. The authors argue that they can be seen as an extension of VAEs, where the prior is a prior over a set of latent variable groups, and that the prior can be regarded as an energy-based prior over the base prior distribution and a reweighting factor that is a function of the prior distribution of the latent variables in the prior.  The authors show that this prior is robust to tempering and that it can be used to train hierarchical VAEs with respect to the latent variable group, and it can also be used as a way to mitigate the prior hole problem. They also show that the reweighted factor is a result of noise contrastive estimation, and they show that it works well when the prior has a large enough gap between the aggregate approximate posterior and the true posterior in the latent space.  Experiments are conducted on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets, and show that VAEs trained with noise contrastively priors perform better in generative performance. "
1512,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"convex regularizers USED-FOR learner ’s policy. degenerate solutions HYPONYM-OF constant rewards. convex regularizers USED-FOR Regularized IRL. constant rewards USED-FOR expert ’s behavior. practical methods USED-FOR them. them USED-FOR regularized IRL. practical methods USED-FOR regularized IRL. tractable solutions CONJUNCTION practical methods. practical methods CONJUNCTION tractable solutions. maximum - entropy IRL framework USED-FOR methods. Shannon - entropy regularizers USED-FOR them. theoretical backing USED-FOR IRL method. IRL method USED-FOR discrete and continuous controls. Method is Inverse Reinforcement Learning ( IRL ). OtherScientificTerm are expert behavior, and reward functions. Generic are solutions, and tasks. ","This paper studies the problem of Inverse Reinforcement Learning (IRL) in the setting where the learner’s policy is trained with convex regularizers. Regularized IRL is based on the observation that the expert behavior can be approximated by constant rewards (i.e., degenerate solutions) that are not always the same as the expert's behavior. The authors propose two solutions to this problem: (1) tractable solutions and (2) practical methods to approximate them via Shannon-entropy regularizers, which allows them to be used in conjunction with practical methods for regularization. The proposed methods are based on a maximum-ent entropy IRL framework. The paper also provides theoretical backing for the proposed IRL method for both discrete and continuous controls. Theoretical results are provided for the case where the reward functions are non-convex, and the authors also provide theoretical results for the cases where the solutions are convex. Experiments are performed on a number of different tasks and show that the proposed method performs well.   ","This paper studies the problem of Inverse Reinforcement Learning (IRL) in the setting where the learner’s policy is trained with convex regularizers. Regularized IRL is based on the observation that the expert behavior can be approximated by constant rewards (i.e., degenerate solutions) that are not always the same as the expert's behavior. The authors propose two solutions to this problem: (1) tractable solutions and (2) practical methods to approximate them via Shannon-entropy regularizers, which allows them to be used in conjunction with practical methods for regularization. The proposed methods are based on a maximum-ent entropy IRL framework. The paper also provides theoretical backing for the proposed IRL method for both discrete and continuous controls. Theoretical results are provided for the case where the reward functions are non-convex, and the authors also provide theoretical results for the cases where the solutions are convex. Experiments are performed on a number of different tasks and show that the proposed method performs well.   "
1521,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"hard binary gates USED-FOR LS or shared paths. CLSR USED-FOR LS or shared paths. hard binary gates USED-FOR token representations. hard binary gates USED-FOR CLSR. translation signals CONJUNCTION budget constraints. budget constraints CONJUNCTION translation signals. LS capacity PART-OF MNMT. translation signals USED-FOR MNMT. CLSR COMPARE many - to - one translation. many - to - one translation COMPARE CLSR. one - to - many translation COMPARE many - to - one translation. many - to - one translation COMPARE one - to - many translation. LS computation USED-FOR top and/or bottom encoder / decoder layers. unbalanced training data USED-FOR many - to - one translation. LS modeling USED-FOR MNMT. OPUS-100 and WMT datasets EVALUATE-FOR Transformer. CLSR USED-FOR one - to - many translation. shared capacity CONJUNCTION LS capacity. LS capacity CONJUNCTION shared capacity. LS capacity USED-FOR multilingual translation. Task is multilingual neural machine translation ( MNMT ). Method are conditional language - specific routing ( CLSR ), and multilingual Transformers. Generic is gates. ","This paper studies the problem of multilingual neural machine translation (MNMT) in the context of conditional language-specific routing (CLSLR). CLSR uses hard binary gates to learn LS or shared paths between different token representations. The authors show that CLSR can be used to learn shared capacity and LS capacity in MNMT with different translation signals and budget constraints. CLSR outperforms many-to-one translation with unbalanced training data, which is a common problem in multilingual Transformers. The paper also shows that LS computation can be shared across top and/or bottom encoder/decoder layers. Experiments on the OPUS-100 and WMT datasets show that the proposed Transformer achieves state-of-the-art performance on the Transformer with CLSR, outperforming the state of the art on unbalanced and multilingual training data. The main contribution of the paper is the LS modeling for MNMT, which can be applied to any shared capacity or LS capacity.  ","This paper studies the problem of multilingual neural machine translation (MNMT) in the context of conditional language-specific routing (CLSLR). CLSR uses hard binary gates to learn LS or shared paths between different token representations. The authors show that CLSR can be used to learn shared capacity and LS capacity in MNMT with different translation signals and budget constraints. CLSR outperforms many-to-one translation with unbalanced training data, which is a common problem in multilingual Transformers. The paper also shows that LS computation can be shared across top and/or bottom encoder/decoder layers. Experiments on the OPUS-100 and WMT datasets show that the proposed Transformer achieves state-of-the-art performance on the Transformer with CLSR, outperforming the state of the art on unbalanced and multilingual training data. The main contribution of the paper is the LS modeling for MNMT, which can be applied to any shared capacity or LS capacity.  "
1530,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"Wasserstein distributional normalization ( WDN ) algorithm USED-FOR noisy labels. Wasserstein distributional normalization ( WDN ) algorithm USED-FOR accurate classification. noisy labels USED-FOR accurate classification. geometric constraints FEATURE-OF uncertain samples. Wasserstein ball USED-FOR them. WDN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE WDN. Clothing1 M and CIFAR-10/100 datasets EVALUATE-FOR state - of - the - art methods. Clothing1 M and CIFAR-10/100 datasets EVALUATE-FOR WDN. WDN COMPARE classification methods. classification methods COMPARE WDN. accuracy EVALUATE-FOR it. accuracy EVALUATE-FOR WDN. OtherScientificTerm are small loss criteria, geometric relationship, and diverse noisy labels. Generic is relation. ","This paper proposes the Wasserstein distributional normalization (WDN) algorithm for noisy labels for accurate classification. The WDN algorithm is based on the observation that under certain small loss criteria, the geometric relationship between two samples with diverse noisy labels can be broken. The paper then proposes to use this relation as a regularizer to regularize the distribution of the noisy labels. The authors propose to use geometric constraints on the uncertain samples to enforce the WDN to normalize them, and then use a modified version of the WESSERstein ball to regularise them. Experiments on the Clothing1M and CIFAR-10/100 datasets show that WDN outperforms state-of-the-art methods. In addition, WDN is shown to outperform other classification methods in terms of accuracy, and it is shown that it is more robust to the number of samples and to the presence of geometric relationship.","This paper proposes the Wasserstein distributional normalization (WDN) algorithm for noisy labels for accurate classification. The WDN algorithm is based on the observation that under certain small loss criteria, the geometric relationship between two samples with diverse noisy labels can be broken. The paper then proposes to use this relation as a regularizer to regularize the distribution of the noisy labels. The authors propose to use geometric constraints on the uncertain samples to enforce the WDN to normalize them, and then use a modified version of the WESSERstein ball to regularise them. Experiments on the Clothing1M and CIFAR-10/100 datasets show that WDN outperforms state-of-the-art methods. In addition, WDN is shown to outperform other classification methods in terms of accuracy, and it is shown that it is more robust to the number of samples and to the presence of geometric relationship."
1539,SP:e0029422e28c250dfb8c62c29a15b375030069e8,predictive accuracy EVALUATE-FOR Convolutional image classifiers. uncertainty quantification techniques USED-FOR probability estimates. uncertainty quantification techniques USED-FOR network. network USED-FOR probability estimates. Platt scaling HYPONYM-OF uncertainty quantification techniques. algorithm USED-FOR predictive set. algorithm USED-FOR classifier. user - specified probability FEATURE-OF predictive set. algorithm COMPARE Platt scaling. Platt scaling COMPARE algorithm. formal finite - sample coverage guarantee FEATURE-OF model. formal finite - sample coverage guarantee FEATURE-OF algorithm. conformal prediction algorithm USED-FOR method. scheme COMPARE approaches. approaches COMPARE scheme. scheme COMPARE stand - alone Platt scaling baseline. stand - alone Platt scaling baseline COMPARE scheme. Imagenet - V2 EVALUATE-FOR scheme. Imagenet CONJUNCTION Imagenet - V2. Imagenet - V2 CONJUNCTION Imagenet. Imagenet EVALUATE-FOR scheme. ResNet-152 CONJUNCTION classifiers. classifiers CONJUNCTION ResNet-152. classifiers EVALUATE-FOR scheme. Imagenet - V2 EVALUATE-FOR approaches. Imagenet - V2 CONJUNCTION ResNet-152. ResNet-152 CONJUNCTION Imagenet - V2. ResNet-152 EVALUATE-FOR scheme. Imagenet - V2 CONJUNCTION classifiers. classifiers CONJUNCTION Imagenet - V2. coverage EVALUATE-FOR approaches. coverage EVALUATE-FOR scheme. OtherScientificTerm is formal guarantees. ,"This paper studies the problem of improving the predictive accuracy of Convolutional image classifiers using uncertainty quantification techniques, such as Platt scaling. The authors propose a novel algorithm that quantifies the probability estimates obtained by a network based on the current state of the network, and then uses this algorithm to build a predictive set with a user-specified probability. The algorithm is similar to Platt scaled, except that the proposed algorithm has a formal finite-sample coverage guarantee for the classifier. The proposed method is based on a conformal prediction algorithm, and the authors provide formal guarantees on the number of samples required for the algorithm to converge to the true predictive set. Experiments on Imagenet-V2, ResNet-152, and classifiers on both synthetic and real-world datasets show the proposed scheme outperforms existing approaches in terms of coverage, and outperforms the stand-alone Plat scaling baseline. ","This paper studies the problem of improving the predictive accuracy of Convolutional image classifiers using uncertainty quantification techniques, such as Platt scaling. The authors propose a novel algorithm that quantifies the probability estimates obtained by a network based on the current state of the network, and then uses this algorithm to build a predictive set with a user-specified probability. The algorithm is similar to Platt scaled, except that the proposed algorithm has a formal finite-sample coverage guarantee for the classifier. The proposed method is based on a conformal prediction algorithm, and the authors provide formal guarantees on the number of samples required for the algorithm to converge to the true predictive set. Experiments on Imagenet-V2, ResNet-152, and classifiers on both synthetic and real-world datasets show the proposed scheme outperforms existing approaches in terms of coverage, and outperforms the stand-alone Plat scaling baseline. "
1548,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"algorithm USED-FOR Wasserstein-2 barycenters. input convex neural networks CONJUNCTION cycle - consistency regularization. cycle - consistency regularization CONJUNCTION input convex neural networks. entropic or quadratic regularization USED-FOR approaches. minimax optimization USED-FOR approach. low - dimensional qualitative scenarios CONJUNCTION high - dimensional quantitative experiments. high - dimensional quantitative experiments CONJUNCTION low - dimensional qualitative scenarios. high - dimensional quantitative experiments EVALUATE-FOR approach. low - dimensional qualitative scenarios EVALUATE-FOR approach. OtherScientificTerm are Wasserstein barycenters, and error bounds. Method is optimal transport. ","This paper proposes a new algorithm for finding Wasserstein-2 barycenters for input convex neural networks and cycle-consistency regularization. Previous approaches are based on entropic or quadratic regularization, but this paper proposes to use Wassersteins. The proposed approach is based on minimax optimization, and the authors provide error bounds on the number of times the optimal transport is applied. The approach is evaluated on both low-dimensional qualitative scenarios and high-dimensional quantitative experiments.","This paper proposes a new algorithm for finding Wasserstein-2 barycenters for input convex neural networks and cycle-consistency regularization. Previous approaches are based on entropic or quadratic regularization, but this paper proposes to use Wassersteins. The proposed approach is based on minimax optimization, and the authors provide error bounds on the number of times the optimal transport is applied. The approach is evaluated on both low-dimensional qualitative scenarios and high-dimensional quantitative experiments."
1557,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"multiple manifold problem HYPONYM-OF binary classification task. deep fully - connected neural network USED-FOR multiple manifold problem. randomly - initialized gradient descent USED-FOR manifolds. randomlyinitialized network CONJUNCTION gradients. gradients CONJUNCTION randomlyinitialized network. nonasymptotic framework USED-FOR generalization of networks. neural tangent kernel PART-OF deep fullyconnected ReLU networks. NTK regime FEATURE-OF generalization of networks. structured data USED-FOR generalization of networks. martingale concentration USED-FOR statistical dependencies. approach USED-FOR network architectures. Task are machine vision, and practically - motivated model problem. OtherScientificTerm are manifold configuration, network depth L, geometric and statistical properties of the data, network width n, i.i.d. samples, depth, fitting resource, class manifolds, width, and statistical resource. Method are nonasymptotic analysis of training overparameterized neural networks, and random network. ","This paper studies the nonasymptotic analysis of training overparameterized neural networks. In particular, the authors consider the binary classification task of a deep fully-connected neural network (a multiple manifold problem, i.e. a binary classification problem on a manifold configuration where the manifold configuration depends on the network depth L and the geometric and statistical properties of the data. They show that the generalization of networks trained on structured data is non-trivial in the NTK regime, and they show that under certain assumptions on the geometry of the class manifolds and the fitting resource, one can obtain a practically-motivated model problem. They also show that randomly-initialized gradient descent converges to such manifolds when the network width n is large enough, and when the number of samples is small enough.    The authors also provide a nonasymptotic framework for the generalisation of networks in this regime, which is based on the observation that the neural tangent kernel of deep fullyconnected ReLU networks can be approximated as a function of the width of the network, and that the width is a measure of the distance between a randomlyinitialized network and the gradients of the randomly initialized network. The authors show that if the width L is sufficiently large, then the martingale concentration of the statistical dependencies between the training data points and the training points of the random network can be minimized.  Finally, they apply their approach to a variety of network architectures, and show that their approach can be applied to a wide range of architectures, including ones that have been trained on i.","This paper studies the nonasymptotic analysis of training overparameterized neural networks. In particular, the authors consider the binary classification task of a deep fully-connected neural network (a multiple manifold problem, i.e. a binary classification problem on a manifold configuration where the manifold configuration depends on the network depth L and the geometric and statistical properties of the data. They show that the generalization of networks trained on structured data is non-trivial in the NTK regime, and they show that under certain assumptions on the geometry of the class manifolds and the fitting resource, one can obtain a practically-motivated model problem. They also show that randomly-initialized gradient descent converges to such manifolds when the network width n is large enough, and when the number of samples is small enough.    The authors also provide a nonasymptotic framework for the generalisation of networks in this regime, which is based on the observation that the neural tangent kernel of deep fullyconnected ReLU networks can be approximated as a function of the width of the network, and that the width is a measure of the distance between a randomlyinitialized network and the gradients of the randomly initialized network. The authors show that if the width L is sufficiently large, then the martingale concentration of the statistical dependencies between the training data points and the training points of the random network can be minimized.  Finally, they apply their approach to a variety of network architectures, and show that their approach can be applied to a wide range of architectures, including ones that have been trained on i."
1566,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"supervised learning methods USED-FOR subroutines. subroutines USED-FOR reinforcement learning algorithm. supervised learning methods USED-FOR reinforcement learning algorithm. weighted target actions USED-FOR policy. supervised learning steps PART-OF approach. one HYPONYM-OF supervised learning steps. supervised learning methods USED-FOR method. experience replay USED-FOR off - policy data. it COMPARE RL algorithms. RL algorithms COMPARE it. AWR COMPARE RL algorithms. RL algorithms COMPARE AWR. OpenAI Gym benchmark tasks EVALUATE-FOR AWR. AWR USED-FOR policies. AWR COMPARE off - policy algorithms. off - policy algorithms COMPARE AWR. environmental interactions FEATURE-OF static datasets. off - policy algorithms USED-FOR policies. complex simulated characters FEATURE-OF continuous control tasks. continuous control tasks EVALUATE-FOR algorithm. Method is advantage - weighted regression ( AWR ). OtherScientificTerm are value function, and continuous and discrete actions. ","This paper proposes a new reinforcement learning algorithm based on supervised learning methods to learn subroutines that can be used to improve the performance of the overall policy. The proposed approach, called advantage-weighted regression (AWR), consists of two supervised learning steps: one where a policy is trained on weighted target actions, and the other one where the value function is learned for both continuous and discrete actions.  The proposed method is based on the idea that supervised learning algorithms can be applied to any subroutine in a RL algorithm, and that it can be seen as a generalization of existing RL algorithms.   The approach consists of three steps: 1) learn a policy on a set of off-policy actions, 2) use experience replay to learn a new policy, and 3) use the learned policy to perform the weighted target action.  AWR is evaluated on OpenAI Gym benchmark tasks, and it is shown that AWR outperforms other RL algorithms in terms of performance on a number of continuous control tasks with complex simulated characters. The paper also shows that the proposed AWR can learn policies that are more robust to environmental interactions in static datasets, and outperforms off-policies that do not learn policies at all.","This paper proposes a new reinforcement learning algorithm based on supervised learning methods to learn subroutines that can be used to improve the performance of the overall policy. The proposed approach, called advantage-weighted regression (AWR), consists of two supervised learning steps: one where a policy is trained on weighted target actions, and the other one where the value function is learned for both continuous and discrete actions.  The proposed method is based on the idea that supervised learning algorithms can be applied to any subroutine in a RL algorithm, and that it can be seen as a generalization of existing RL algorithms.   The approach consists of three steps: 1) learn a policy on a set of off-policy actions, 2) use experience replay to learn a new policy, and 3) use the learned policy to perform the weighted target action.  AWR is evaluated on OpenAI Gym benchmark tasks, and it is shown that AWR outperforms other RL algorithms in terms of performance on a number of continuous control tasks with complex simulated characters. The paper also shows that the proposed AWR can learn policies that are more robust to environmental interactions in static datasets, and outperforms off-policies that do not learn policies at all."
1575,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,Heterogeneous assignment of bitwidths FEATURE-OF layers. parametrized sinusoidal regularizer USED-FOR WaveQ. WaveQ USED-FOR quantized weights. training USED-FOR stochastic gradient descent. sinusoidal regularizer USED-FOR stochastic gradient descent. quantized weights CONJUNCTION heterogeneous bitwidths. heterogeneous bitwidths CONJUNCTION quantized weights. WaveQ HYPONYM-OF gradient - based mechanism. gradient - based mechanism USED-FOR quantized weights. gradient - based mechanism USED-FOR heterogeneous bitwidths. ResNet-20 CONJUNCTION SVHN. SVHN CONJUNCTION ResNet-20. ResNet-18 CONJUNCTION ResNet-20. ResNet-20 CONJUNCTION ResNet-18. MobileNet CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION MobileNet. heterogeneous bitwidth assignment USED-FOR quantization. CIFAR10 CONJUNCTION MobileNet. MobileNet CONJUNCTION CIFAR10. AlexNet CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION AlexNet. SVHN CONJUNCTION VGG-11. VGG-11 CONJUNCTION SVHN. compute efficiency CONJUNCTION accuracy. accuracy CONJUNCTION compute efficiency. WaveQ USED-FOR heterogeneous bitwidth assignment. heterogeneous bitwidth assignment USED-FOR deep networks. WaveQ USED-FOR deep networks. accuracy EVALUATE-FOR WaveQ. compute efficiency EVALUATE-FOR WaveQ. VGG-11 HYPONYM-OF deep networks. AlexNet HYPONYM-OF deep networks. SVHN HYPONYM-OF deep networks. MobileNet HYPONYM-OF deep networks. ResNet-20 HYPONYM-OF deep networks. CIFAR10 HYPONYM-OF deep networks. ResNet-18 HYPONYM-OF deep networks. predetermined bitwidths USED-FOR WaveQ. DoReFa CONJUNCTION WRPN. WRPN CONJUNCTION DoReFa. accuracy EVALUATE-FOR quantized training algorithms. quantized training algorithms EVALUATE-FOR,"This paper proposes WaveQ, a parametrized sinusoidal regularizer for quantized weights and heterogeneous assignment of bitwidths in layers. The authors show that WaveQ is a gradient-based mechanism that can be used to learn quantized weight and bitwidth for training for stochastic gradient descent with a sinusoid regularizer. They also show that the heterogeneous bitwidth assignment for quantization improves the compute efficiency and accuracy of deep networks trained with WaveQ (ResNet-18, ResNet-20, SVHN, VGG-11, AlexNet, CIFAR10, MobileNet, etc.). The paper also shows that the proposed WaveQ can be applied to any quantized training algorithms.    The paper is well-written and well-motivated. WaveQ achieves state-of-the-art performance on several deep networks (CIFAR-10, Cifar-100, Mobile-Net, Resnet-18) trained with predetermined bitwidth, and achieves competitive performance with DoReFa, WRPNP, and DoReF.  The authors also show how WaveQ performs well on deep networks that are trained with heterogeneous bits. ","This paper proposes WaveQ, a parametrized sinusoidal regularizer for quantized weights and heterogeneous assignment of bitwidths in layers. The authors show that WaveQ is a gradient-based mechanism that can be used to learn quantized weight and bitwidth for training for stochastic gradient descent with a sinusoid regularizer. They also show that the heterogeneous bitwidth assignment for quantization improves the compute efficiency and accuracy of deep networks trained with WaveQ (ResNet-18, ResNet-20, SVHN, VGG-11, AlexNet, CIFAR10, MobileNet, etc.). The paper also shows that the proposed WaveQ can be applied to any quantized training algorithms.    The paper is well-written and well-motivated. WaveQ achieves state-of-the-art performance on several deep networks (CIFAR-10, Cifar-100, Mobile-Net, Resnet-18) trained with predetermined bitwidth, and achieves competitive performance with DoReFa, WRPNP, and DoReF.  The authors also show how WaveQ performs well on deep networks that are trained with heterogeneous bits. "
1584,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"data augmentation methods USED-FOR translation. monolingual data USED-FOR data augmentation methods. data augmentation methods USED-FOR neural machine translation ( NMT ). in - domain monolingual data USED-FOR it. backtranslation HYPONYM-OF data augmentation methods. data augmentation method USED-FOR neural machine translation. small and large scale datasets EVALUATE-FOR method. method COMPARE baseline models. baseline models COMPARE method. small and large scale datasets EVALUATE-FOR baseline models. Method is neural machine translation models. OtherScientificTerm are aligned word pairs, and bilingual embeddings. ","This paper proposes a new data augmentation method for neural machine translation (NMT) based on monolingual data. The idea is to augment the bilingual embeddings of the input word pairs with aligned word pairs from the source and target languages. The proposed method is based on the idea of backtranslation, which augments the embedding of the target language to the source language. The authors show that the proposed method outperforms the baseline models on both small and large scale datasets.","This paper proposes a new data augmentation method for neural machine translation (NMT) based on monolingual data. The idea is to augment the bilingual embeddings of the input word pairs with aligned word pairs from the source and target languages. The proposed method is based on the idea of backtranslation, which augments the embedding of the target language to the source language. The authors show that the proposed method outperforms the baseline models on both small and large scale datasets."
1593,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"Federated learning USED-FOR distributed data privacy. data quantity CONJUNCTION data quality. data quality CONJUNCTION data quantity. Shapley Value PART-OF game theory. Shapley Value COMPARE method. method COMPARE Shapley Value. maintaining real - time EVALUATE-FOR method. data quality EVALUATE-FOR method. data quantity EVALUATE-FOR method. Method are contribution measurement mechanism, real - time contribution measurement method, and pseudo - distributed training. Generic is mechanism. OtherScientificTerm is contribution rate. Material is Penn Treebank dataset. ","This paper studies the problem of distributed data privacy in federated learning. The authors propose a novel contribution measurement mechanism, called Shapley Value, which is a real-time contribution measurement method. The proposed mechanism is based on the idea of Shapley value in game theory, which states that the contribution rate should be the sum of the difference between the data quantity and the data quality. The paper shows that the proposed method is more efficient in maintaining real time than the existing method, while maintaining the same data quantity but with better data quality (as measured by the Penn Treebank dataset). The paper also shows that pseudo-distributed training can be used to improve the performance of the proposed mechanism. ","This paper studies the problem of distributed data privacy in federated learning. The authors propose a novel contribution measurement mechanism, called Shapley Value, which is a real-time contribution measurement method. The proposed mechanism is based on the idea of Shapley value in game theory, which states that the contribution rate should be the sum of the difference between the data quantity and the data quality. The paper shows that the proposed method is more efficient in maintaining real time than the existing method, while maintaining the same data quantity but with better data quality (as measured by the Penn Treebank dataset). The paper also shows that pseudo-distributed training can be used to improve the performance of the proposed mechanism. "
1602,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"graph structure FEATURE-OF fully - observable case. nearlylinear time algorithm USED-FOR problem. dimension - independent error guarantee FEATURE-OF nearlylinear time algorithm. error guarantees EVALUATE-FOR robust algorithms. robust learning of Bayesian networks CONJUNCTION robust mean estimation. robust mean estimation CONJUNCTION robust learning of Bayesian networks. Task is learning Bayesian networks. Method are Bayesian networks, Bayesian network, and robust mean estimation algorithm. Generic is algorithm. ","This paper considers the problem of learning Bayesian networks. In this setting, the goal is to learn a Bayesian network that is robust to perturbations in the input space. The authors consider the fully-observable case with a graph structure, and show that the problem can be solved by a nearlylinear time algorithm with a dimension-independent error guarantee. They also provide error guarantees for existing robust algorithms. Finally, the authors provide a theoretical analysis of the robust mean estimation algorithm.    The paper is well-written and well-motivated, and the proposed algorithm seems to be well motivated. The theoretical analysis is interesting and the results are well-supported by the experimental results.  I have some questions about the connection between the robust learning of Bayes networks and the robust means estimation, but I am not entirely convinced that this is a strong connection.","This paper considers the problem of learning Bayesian networks. In this setting, the goal is to learn a Bayesian network that is robust to perturbations in the input space. The authors consider the fully-observable case with a graph structure, and show that the problem can be solved by a nearlylinear time algorithm with a dimension-independent error guarantee. They also provide error guarantees for existing robust algorithms. Finally, the authors provide a theoretical analysis of the robust mean estimation algorithm.    The paper is well-written and well-motivated, and the proposed algorithm seems to be well motivated. The theoretical analysis is interesting and the results are well-supported by the experimental results.  I have some questions about the connection between the robust learning of Bayes networks and the robust means estimation, but I am not entirely convinced that this is a strong connection."
1611,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"raw high - dimensional observations USED-FOR autonomous agents. images HYPONYM-OF raw high - dimensional observations. shaped reward functions USED-FOR model - based reinforcement learning ( RL ). short - horizon reasoning USED-FOR shaped reward functions. trajectory optimization USED-FOR long - horizon reasoning. it USED-FOR image - based setting. probabilistic latent variable models USED-FOR algorithm. probabilistic latent variable models USED-FOR it. approach USED-FOR longer - horizon visual planning. latent collocation method ( LatCo ) USED-FOR approach. latent collocation method ( LatCo ) USED-FOR longer - horizon visual planning. approach COMPARE prior model - based approaches. prior model - based approaches COMPARE approach. sparse rewards CONJUNCTION long - term goals. long - term goals CONJUNCTION sparse rewards. sparse rewards FEATURE-OF visual control tasks. long - term goals FEATURE-OF visual control tasks. visual control tasks EVALUATE-FOR prior model - based approaches. visual control tasks EVALUATE-FOR approach. Method are temporally extended reasoning, myopic, short - sighted planning, and collocation - based planning. OtherScientificTerm is latent variables. ",This paper proposes a new method for long-horizon planning in RL based on a latent variable model. The proposed method is based on the idea of temporally extended reasoning. The authors show that the proposed method outperforms existing methods on a number of tasks. The paper also shows that the method can be applied to a variety of tasks with sparse rewards.,This paper proposes a new method for long-horizon planning in RL based on a latent variable model. The proposed method is based on the idea of temporally extended reasoning. The authors show that the proposed method outperforms existing methods on a number of tasks. The paper also shows that the method can be applied to a variety of tasks with sparse rewards.
1620,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"Bayesian neural networks COMPARE neural networks. neural networks COMPARE Bayesian neural networks. tempered ” or “ cold ” posterior USED-FOR uncertainty. BNNs USED-FOR image classification. CIFAR-10 HYPONYM-OF image benchmark datasets. generative model USED-FOR curation. generative model USED-FOR Bayesian account of cold posteriors. likelihood COMPARE tempered likelihoods. tempered likelihoods COMPARE likelihood. generative model USED-FOR likelihood. Method is Bayesian inference / decision theory. OtherScientificTerm are posterior, and prior. ","This paper proposes a generative model for the cold posterior of Bayesian neural networks (BNNs), which is an extension of previous work in Bayesian inference/decision theory. The authors show that the “tempered” or “cold” posterior can capture the uncertainty of a BNN for image classification. They also show that a Bayesian account of cold posteriors can be derived from the generative account of the posterior. The paper also shows that the likelihood of a given BNN trained with the generitative model is more robust to curation than that of a trained BNN without the prior, and the likelihood is also more robust than the tempered likelihoods. Experiments are conducted on image benchmark datasets such as CIFAR-10 and ImageNet. ","This paper proposes a generative model for the cold posterior of Bayesian neural networks (BNNs), which is an extension of previous work in Bayesian inference/decision theory. The authors show that the “tempered” or “cold” posterior can capture the uncertainty of a BNN for image classification. They also show that a Bayesian account of cold posteriors can be derived from the generative account of the posterior. The paper also shows that the likelihood of a given BNN trained with the generitative model is more robust to curation than that of a trained BNN without the prior, and the likelihood is also more robust than the tempered likelihoods. Experiments are conducted on image benchmark datasets such as CIFAR-10 and ImageNet. "
1629,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,non - autoregressive neural machine translation COMPARE autoregressive machine translation. autoregressive machine translation COMPARE non - autoregressive neural machine translation. GPUs USED-FOR autoregressive machine translation. latter COMPARE former. former COMPARE latter. non - autoregressive models COMPARE autoregressive baselines. autoregressive baselines COMPARE non - autoregressive models. translation quality - speed tradeoffs EVALUATE-FOR autoregressive baselines. translation quality - speed tradeoffs EVALUATE-FOR non - autoregressive models. accuracy EVALUATE-FOR autoregressive baselines. encoders USED-FOR autoregressive models. single - layer autoregressive decoder COMPARE non - autoregressive models. non - autoregressive models COMPARE single - layer autoregressive decoder. inference speed EVALUATE-FOR single - layer autoregressive decoder. inference speed EVALUATE-FOR non - autoregressive models. suboptimal layer allocation CONJUNCTION insufficient speed measurement. insufficient speed measurement CONJUNCTION suboptimal layer allocation. autoregressive baselines COMPARE non - autoregressive methods. non - autoregressive methods COMPARE autoregressive baselines. speed disadvantage EVALUATE-FOR autoregressive baselines. speed disadvantage EVALUATE-FOR non - autoregressive methods. OtherScientificTerm is knowledge distillation. Task is machine translation. ,"This paper proposes a novel non-autoregressive neural machine translation method that is more efficient than autoregressive machine translation (ARMT). The authors argue that the main difference between the two methods is that the non-auto-regressive method is able to learn the encoder and decoder in parallel, while ARMT can only learn the decoder at the end of each layer. The authors also show that the proposed method can be used for knowledge distillation, which is an important problem in machine translation.","This paper proposes a novel non-autoregressive neural machine translation method that is more efficient than autoregressive machine translation (ARMT). The authors argue that the main difference between the two methods is that the non-auto-regressive method is able to learn the encoder and decoder in parallel, while ARMT can only learn the decoder at the end of each layer. The authors also show that the proposed method can be used for knowledge distillation, which is an important problem in machine translation."
1638,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"test error EVALUATE-FOR deep neural network ( DNN ). bell - shaped variance USED-FOR model - wise double descent. test error EVALUATE-FOR DNN. test error HYPONYM-OF epoch - wise double descent. bias - variance analysis USED-FOR epoch - wise double descent. variance USED-FOR zero - one loss. metric USED-FOR diversity of model updates. stochastic gradients of random training batches USED-FOR diversity of model updates. It USED-FOR generalization ability. It USED-FOR DNN. generalization ability EVALUATE-FOR DNN. It USED-FOR early stopping. zero - one loss USED-FOR DNN. validation set USED-FOR early stopping. Method are statistical learning theory, and bias - variance decomposition. OtherScientificTerm are double descent, model complexity, U - shaped curve, OV, and unknown ) test error. Generic is descent. Metric is optimization variance ( OV ). ","This paper studies the double-descent of the test error of a deep neural network (DNN) in the presence of a bell-shaped variance, which is a well-studied phenomenon in statistical learning theory. The authors show that the bell-shaped variance is responsible for model-wise double descent in the case of a DNN with test error $O(\sqrt{T}^T)$, where $T$ is the model complexity and $T^T$ the number of training epochs. They also show that this bell-shape variance is also responsible for an epoch-wise variant of double descent.    The bias-variance analysis is based on the observation that the bias of the variance of the DNN in a training epoch can be decomposed into two terms: (1) the (unknown) test error, and (2) the optimization variance (OV) of the training epoch.  The authors use this metric to measure the diversity of model updates from the stochastic gradients of random training batches. It is shown to be a measure of the generalization ability of the trained DNN. It can be used for early stopping when the validation set is large enough, and it is shown that the zero-one loss induced by the variance is a function of the diversity in the training batch, and that this variance is correlated with the variance in training.  In addition, the authors also provide an analysis of the U-shaped curve of the OV, which shows that the descent can be seen as a function (in the sense that OV is the product of OV and OV) of training batch size, and they show that early stopping on a validation set with a large validation set leads to an early stopping. ","This paper studies the double-descent of the test error of a deep neural network (DNN) in the presence of a bell-shaped variance, which is a well-studied phenomenon in statistical learning theory. The authors show that the bell-shaped variance is responsible for model-wise double descent in the case of a DNN with test error $O(\sqrt{T}^T)$, where $T$ is the model complexity and $T^T$ the number of training epochs. They also show that this bell-shape variance is also responsible for an epoch-wise variant of double descent.    The bias-variance analysis is based on the observation that the bias of the variance of the DNN in a training epoch can be decomposed into two terms: (1) the (unknown) test error, and (2) the optimization variance (OV) of the training epoch.  The authors use this metric to measure the diversity of model updates from the stochastic gradients of random training batches. It is shown to be a measure of the generalization ability of the trained DNN. It can be used for early stopping when the validation set is large enough, and it is shown that the zero-one loss induced by the variance is a function of the diversity in the training batch, and that this variance is correlated with the variance in training.  In addition, the authors also provide an analysis of the U-shaped curve of the OV, which shows that the descent can be seen as a function (in the sense that OV is the product of OV and OV) of training batch size, and they show that early stopping on a validation set with a large validation set leads to an early stopping. "
1647,SP:8d8b738c676938952e62a6b2aea42e79518ece06,"meta - learning techniques PART-OF few - shot learning. It USED-FOR meta - initialization of model parameters. labeled training data USED-FOR It. labeled training data USED-FOR meta - initialization of model parameters. few - shot learning USED-FOR MAML. MAML USED-FOR adversarial robustness. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. robustness USED-FOR meta - model. adversarial robustness FEATURE-OF MAML. robustness USED-FOR task - specific fine - tuning stage. training protocol USED-FOR latter. robust regularization USED-FOR MAML. fast adversarial attack generation CONJUNCTION computationally - light fine - tuning. computationally - light fine - tuning CONJUNCTION fast adversarial attack generation. unlabeled data augmentation CONJUNCTION fast adversarial attack generation. fast adversarial attack generation CONJUNCTION unlabeled data augmentation. auxiliary contrastive learning task USED-FOR MAML. auxiliary contrastive learning task USED-FOR adversarial robustness. adversarial robustness FEATURE-OF MAML. methods USED-FOR robust few - shot learning. OtherScientificTerm are robustness - promoting regularization, and meta - update stage. Metric is robustness adaptation. ","This paper investigates the robustness-promoting regularization of meta-learning techniques in few-shot learning. It aims to improve the meta-initialization of model parameters on labeled training data. The paper shows that MAML is robust to adversarial attacks in the meta learning setting. The main contribution of the paper is to show that robustness of the meta model is beneficial to generalization and adversarial robustness. The authors also show that the generalization performance of a meta-model can be improved by robustness regularization.    The paper also shows that robust regularization can be used to improve robustness in meta-training. The robustness is used in the task-specific fine-tuning stage, and robustness adaptation can be applied to any training protocol.  The authors show that adversarial regularization is helpful to improve adversarial training performance of MAMM. They also propose an auxiliary contrastive learning task that improves adversarial adaptation of the adversarial-robust version of the training data to the target task.  Finally, the authors propose methods for robust few-task learning based on unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine -tuning. ","This paper investigates the robustness-promoting regularization of meta-learning techniques in few-shot learning. It aims to improve the meta-initialization of model parameters on labeled training data. The paper shows that MAML is robust to adversarial attacks in the meta learning setting. The main contribution of the paper is to show that robustness of the meta model is beneficial to generalization and adversarial robustness. The authors also show that the generalization performance of a meta-model can be improved by robustness regularization.    The paper also shows that robust regularization can be used to improve robustness in meta-training. The robustness is used in the task-specific fine-tuning stage, and robustness adaptation can be applied to any training protocol.  The authors show that adversarial regularization is helpful to improve adversarial training performance of MAMM. They also propose an auxiliary contrastive learning task that improves adversarial adaptation of the adversarial-robust version of the training data to the target task.  Finally, the authors propose methods for robust few-task learning based on unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine -tuning. "
1656,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"optimization algorithms USED-FOR optimizer. Learning - to - learn USED-FOR optimizers. optimization algorithms USED-FOR Learning - to - learn. metagradient descent USED-FOR meta - objective. trajectory USED-FOR metagradient descent. metagradient descent USED-FOR approach. step size USED-FOR quadratic loss. backpropagation USED-FOR meta - gradient. neural networks USED-FOR learned optimizers. OtherScientificTerm are metagradient explosion / vanishing problems, metagradient, and numerical issues. Method is learning - to - learn approach. Task is gradient explosion / vanishing problems. ","This paper proposes a new learning-to-learn approach to learn a meta-optimizer that is robust to metagradient explosion/vanishing problems. The authors propose to use optimization algorithms to train the optimizer and then use the learned optimizers with neural networks. The proposed approach is based on the idea that the meta-gradient of a learned optimizer can be approximated by a quadratic loss with a step size that depends on the step size of the optimization algorithm. The paper shows that this approach can be seen as an extension of the metragradient descent, which can be used to approximate a new meta-objective by using a trajectory similar to the one used in the original meta-learned approach.  The paper also shows that meta-gradients can be computed using backpropagation and that the gradient explosion / vanishing problems can be avoided if the meta gradient is computed in a way that minimizes the difference between the gradient of the original gradient and the learned gradient.   ","This paper proposes a new learning-to-learn approach to learn a meta-optimizer that is robust to metagradient explosion/vanishing problems. The authors propose to use optimization algorithms to train the optimizer and then use the learned optimizers with neural networks. The proposed approach is based on the idea that the meta-gradient of a learned optimizer can be approximated by a quadratic loss with a step size that depends on the step size of the optimization algorithm. The paper shows that this approach can be seen as an extension of the metragradient descent, which can be used to approximate a new meta-objective by using a trajectory similar to the one used in the original meta-learned approach.  The paper also shows that meta-gradients can be computed using backpropagation and that the gradient explosion / vanishing problems can be avoided if the meta gradient is computed in a way that minimizes the difference between the gradient of the original gradient and the learned gradient.   "
1665,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"methods USED-FOR large and complex graph problems. neural networks USED-FOR methods. dual views USED-FOR representations. neighborhood aggregation capability FEATURE-OF GVCLN. loss functions CONJUNCTION supervised loss. supervised loss CONJUNCTION loss functions. loss functions USED-FOR view - consistent representations. view - consistent loss USED-FOR consistent representation. view - consistent loss USED-FOR views. supervised loss CONJUNCTION view - consistent loss. view - consistent loss CONJUNCTION supervised loss. view - consistent loss CONJUNCTION pseudo - label loss. pseudo - label loss CONJUNCTION view - consistent loss. known labeled set USED-FOR supervised loss. common high - confidence predictions USED-FOR pseudo - label loss. GVCLN USED-FOR view - consistent representations. loss functions USED-FOR view - consistent representations. loss functions USED-FOR GVCLN. Citeseer CONJUNCTION PubMed. PubMed CONJUNCTION Citeseer. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. node classification tasks EVALUATE-FOR GVCLN. Task are acquisition of ground - truth labels, semisupervised learning, and classification tasks. OtherScientificTerm are viewing angles, observation objects, observation representations, and node features. ","This paper proposes a new method for learning representations that are view-consistent across different viewing angles and views of the same node. The proposed method, called GVCLN, is a generalization of existing methods for large and complex graph problems using neural networks. The key idea is to use dual views to learn representations for different views, and then use the learned representations to guide the acquisition of ground-truth labels for each pair of views. The paper also proposes a neighborhood aggregation capability to improve the performance of the proposed method.   Experiments are conducted on node classification tasks on Cora, Citeseer, and PubMed, where the paper shows that the proposed loss functions, including the standard supervised loss based on the known labeled set, the view-confident loss for learning the views for a given pair of observation objects, and the pseudo-label loss that uses common high-confidence predictions for semisupervised learning, are able to achieve state-of-the-art performance on the classification tasks. The main contribution of the paper is that the authors show that the learned loss functions can be combined with existing loss functions in order to learn view-compositional representations, and that the resulting method is able to generalize better than existing methods.  The paper is well-written and well-motivated. ","This paper proposes a new method for learning representations that are view-consistent across different viewing angles and views of the same node. The proposed method, called GVCLN, is a generalization of existing methods for large and complex graph problems using neural networks. The key idea is to use dual views to learn representations for different views, and then use the learned representations to guide the acquisition of ground-truth labels for each pair of views. The paper also proposes a neighborhood aggregation capability to improve the performance of the proposed method.   Experiments are conducted on node classification tasks on Cora, Citeseer, and PubMed, where the paper shows that the proposed loss functions, including the standard supervised loss based on the known labeled set, the view-confident loss for learning the views for a given pair of observation objects, and the pseudo-label loss that uses common high-confidence predictions for semisupervised learning, are able to achieve state-of-the-art performance on the classification tasks. The main contribution of the paper is that the authors show that the learned loss functions can be combined with existing loss functions in order to learn view-compositional representations, and that the resulting method is able to generalize better than existing methods.  The paper is well-written and well-motivated. "
1674,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"physics bias USED-FOR neural networks. neural networks USED-FOR dynamics of systems. coordinates USED-FOR conserved quantities. cyclic coordinates HYPONYM-OF coordinates. Hamiltonian dynamics USED-FOR classical systems. canonical transformations USED-FOR coordinates. Hamiltonian dynamics USED-FOR loss functions. loss functions USED-FOR coordinates. network USED-FOR conserved quantities. network COMPARE networks. networks COMPARE network. network USED-FOR dynamics of the system. Hamiltonian USED-FOR networks. classical physics systems EVALUATE-FOR method. synthetic and experimental data EVALUATE-FOR method. symmetry orbits PART-OF phase space. lower dimensional sub - spaces of phase space HYPONYM-OF phase space. lower dimensional sub - spaces of phase space HYPONYM-OF symmetry orbits. analytic formulae USED-FOR networks. conserved quantities USED-FOR networks. ( angular ) momentum HYPONYM-OF conserved quantities. OtherScientificTerm are dynamics, and symmetries. Task is description of physical systems. ","This paper studies the physics bias in the training of neural networks for learning the dynamics of systems. The authors propose to use neural networks to learn conserved quantities, i.e., coordinates that are invariant to canonical transformations (e.g. cyclic coordinates). The authors show that a network trained with such a conserved quantity is more likely to learn the dynamics than other networks trained with non-conserving quantities, e.g., (angular) momentum.  The authors also show that Hamiltonian dynamics of classical systems can be used to learn such coordinates using loss functions that are based on the learned parameters of the network.   The proposed method is tested on a number of classical physics systems and is shown to be able to learn dynamics of the system in a way that is consistent with the description of physical systems.  On synthetic and experimental data, the proposed method achieves state-of-the-art performance. The networks are trained using analytic formulae and are shown to learn networks that are able to preserve conserved values of conserved terms, and to be robust to symmetries in the system. The symmetry orbits in the phase space (i.e. the lower dimensional sub-spaces of phase space) are also conserved, and the authors also demonstrate that the networks can be trained to learn these types of invariant quantities. ","This paper studies the physics bias in the training of neural networks for learning the dynamics of systems. The authors propose to use neural networks to learn conserved quantities, i.e., coordinates that are invariant to canonical transformations (e.g. cyclic coordinates). The authors show that a network trained with such a conserved quantity is more likely to learn the dynamics than other networks trained with non-conserving quantities, e.g., (angular) momentum.  The authors also show that Hamiltonian dynamics of classical systems can be used to learn such coordinates using loss functions that are based on the learned parameters of the network.   The proposed method is tested on a number of classical physics systems and is shown to be able to learn dynamics of the system in a way that is consistent with the description of physical systems.  On synthetic and experimental data, the proposed method achieves state-of-the-art performance. The networks are trained using analytic formulae and are shown to learn networks that are able to preserve conserved values of conserved terms, and to be robust to symmetries in the system. The symmetry orbits in the phase space (i.e. the lower dimensional sub-spaces of phase space) are also conserved, and the authors also demonstrate that the networks can be trained to learn these types of invariant quantities. "
1683,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"models USED-FOR graph representation learning tasks. Graph neural networks ( GNNs ) USED-FOR graph representation learning tasks. Graph neural networks ( GNNs ) HYPONYM-OF models. gradient boosted decision trees ( GBDT ) COMPARE machine learning methods. machine learning methods COMPARE gradient boosted decision trees ( GBDT ). heterogeneous tabular data EVALUATE-FOR machine learning methods. approach USED-FOR graphs with tabular node features. GNN models USED-FOR networks. homogeneous sparse features FEATURE-OF networks. GBDT CONJUNCTION GNN. GNN CONJUNCTION GBDT. architecture USED-FOR GBDT. architecture USED-FOR GNN. GBDT model USED-FOR heterogeneous features. GNN USED-FOR graph structure. endto - end optimization USED-FOR model. Material is heterogeneous setting. OtherScientificTerm are trees, and graphs with tabular features. Method is GBDT and GNN models. ","Graph neural networks (GNNs) are one of the most popular models for graph representation learning tasks. In the heterogeneous setting, gradient boosted decision trees (GBDT) have been shown to outperform existing machine learning methods on heterogeneous tabular data. This paper proposes a new approach for graphs with tabular node features. The authors show that existing GNN models have homogeneous sparse features, and propose a new architecture for GBDT and GNN. The GBDT model is able to capture heterogeneous features, while the GNN can capture the graph structure. The model is trained end-to-end with end to-end optimization. Experiments are conducted on several datasets to validate the performance of GBDT, GNN, and other models. ","Graph neural networks (GNNs) are one of the most popular models for graph representation learning tasks. In the heterogeneous setting, gradient boosted decision trees (GBDT) have been shown to outperform existing machine learning methods on heterogeneous tabular data. This paper proposes a new approach for graphs with tabular node features. The authors show that existing GNN models have homogeneous sparse features, and propose a new architecture for GBDT and GNN. The GBDT model is able to capture heterogeneous features, while the GNN can capture the graph structure. The model is trained end-to-end with end to-end optimization. Experiments are conducted on several datasets to validate the performance of GBDT, GNN, and other models. "
1692,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"Meta - learning USED-FOR fast adaptation. train - validation split USED-FOR meta - learning. train - validation split COMPARE non - splitting method. non - splitting method COMPARE train - validation split. training EVALUATE-FOR non - splitting method. per - task data USED-FOR non - splitting method. train - validation split USED-FOR linear centroid meta - learning problem. splitting method COMPARE non - splitting method. non - splitting method COMPARE splitting method. regularization parameter CONJUNCTION split ratio. split ratio CONJUNCTION regularization parameter. split ratio USED-FOR methods. regularization parameter USED-FOR methods. asymptotic excess risk EVALUATE-FOR non - splitting method. non - splitting method COMPARE splitting method. splitting method COMPARE non - splitting method. Generic are predictor, and model. Method are linear models, splitting and non - splitting methods, and data splitting. OtherScientificTerm is data distribution. ","Meta-learning aims to achieve fast adaptation to new tasks by splitting the training data into training and validation splits. In this paper, the authors consider the linear centroid meta-learning problem, where the predictor is a linear model and the data distribution is a set of linear models. They show that the train-validation split for meta-learning is equivalent to the non-splitting method for training on per-task data. They also show that a splitting method can be more efficient than a non-blurring method on training on the same training set.   The authors also provide a theoretical analysis of the asymptotic excess risk of the splitting and non-sparing methods. The authors show that both methods can be improved by adjusting the regularization parameter and the split ratio.  They also provide empirical evidence that the splitting method is more effective than a single splitting method, and that data splitting is more efficient when the number of tasks is small.","Meta-learning aims to achieve fast adaptation to new tasks by splitting the training data into training and validation splits. In this paper, the authors consider the linear centroid meta-learning problem, where the predictor is a linear model and the data distribution is a set of linear models. They show that the train-validation split for meta-learning is equivalent to the non-splitting method for training on per-task data. They also show that a splitting method can be more efficient than a non-blurring method on training on the same training set.   The authors also provide a theoretical analysis of the asymptotic excess risk of the splitting and non-sparing methods. The authors show that both methods can be improved by adjusting the regularization parameter and the split ratio.  They also provide empirical evidence that the splitting method is more effective than a single splitting method, and that data splitting is more efficient when the number of tasks is small."
1701,SP:bb566eda95867f83a80664b2f685ad373147c87b,"noisy training data USED-FOR hard confident examples. physics USED-FOR momentum. non - simple patterns PART-OF hard confident examples. Me - Momentum USED-FOR hard confident examples. classification EVALUATE-FOR Me - Momentum. OtherScientificTerm are decision boundary, hard examples, and simple patterns. Method are classifiers, deep learning paradigm, deep neural networks, and classifier. Task are Extracting confident examples, and extracting hard confident examples. Material are noisy labels, and inaccurately labeled examples. Generic is approach. ","This paper studies the problem of extracting hard confident examples from noisy training data with noisy labels.  Extracting confident examples is an important problem in the deep learning paradigm, as the decision boundary of a deep neural networks can be highly sensitive to noise in the training data. This paper proposes Me-Momentum, a method for extracting hard confidence from noisy labels that is motivated by physics. Me-momentum is based on the observation that momentum in physics can be interpreted as a measure of how close a classifier is to the boundary of the training set.  The authors show that hard confidence examples contain non-simple patterns that are not easily identifiable by classifiers. They show that the hard examples are easy to extract from the noisy training set, and that the classifier can be trained to find hard examples that are easily distinguishable from simple patterns. The authors also show that this approach is more robust to inaccurately labeled examples.   The paper also shows that Me- Momentum can be used to extract hard confident example from noisy data, and is shown to improve classification performance. ","This paper studies the problem of extracting hard confident examples from noisy training data with noisy labels.  Extracting confident examples is an important problem in the deep learning paradigm, as the decision boundary of a deep neural networks can be highly sensitive to noise in the training data. This paper proposes Me-Momentum, a method for extracting hard confidence from noisy labels that is motivated by physics. Me-momentum is based on the observation that momentum in physics can be interpreted as a measure of how close a classifier is to the boundary of the training set.  The authors show that hard confidence examples contain non-simple patterns that are not easily identifiable by classifiers. They show that the hard examples are easy to extract from the noisy training set, and that the classifier can be trained to find hard examples that are easily distinguishable from simple patterns. The authors also show that this approach is more robust to inaccurately labeled examples.   The paper also shows that Me- Momentum can be used to extract hard confident example from noisy data, and is shown to improve classification performance. "
1710,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,certified defenses USED-FOR data poisoning attacks. certified defenses USED-FOR majority vote mechanism. k nearest neighbors ( kNN ) CONJUNCTION radius nearest neighbors ( rNN ). radius nearest neighbors ( rNN ) CONJUNCTION k nearest neighbors ( kNN ). intrinsic majority vote mechanisms FEATURE-OF Nearest neighbor algorithms. radius nearest neighbors ( rNN ) HYPONYM-OF Nearest neighbor algorithms. k nearest neighbors ( kNN ) HYPONYM-OF Nearest neighbor algorithms. kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. intrinsic majority vote mechanisms USED-FOR certified robustness guarantees. intrinsic majority vote mechanisms USED-FOR rNN. intrinsic majority vote mechanisms USED-FOR kNN. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. intrinsic certified robustness guarantees EVALUATE-FOR rNN. intrinsic certified robustness guarantees EVALUATE-FOR kNN. intrinsic certified robustness guarantees COMPARE certified defenses. certified defenses COMPARE intrinsic certified robustness guarantees. Task is Data poisoning attacks. Method is machine learning model. OtherScientificTerm is voter. ,"Data poisoning attacks are attacks in which a machine learning model is poisoned with a small subset of the voters.   The paper proposes two certified defenses for data poisoning attacks that are based on the majority vote mechanism. Nearest neighbor algorithms such as k nearest neighbors (kNN) and radius nearest neighbor (rNN) have been shown to have intrinsic majority vote mechanisms.  This paper shows that kNN and rNN can be certified robust to poisoning attacks with intrinsic majority voting mechanisms. The paper also shows that the certified robustness guarantees of kNN, rNN, and other intrinsic majorityVote mechanisms can be obtained for kNN as well as rNN.  Experiments are conducted on MNIST and CIFAR10 and show that intrinsic certified robusts guarantees of both kNN (and rNN) are better than certified defenses. ","Data poisoning attacks are attacks in which a machine learning model is poisoned with a small subset of the voters.   The paper proposes two certified defenses for data poisoning attacks that are based on the majority vote mechanism. Nearest neighbor algorithms such as k nearest neighbors (kNN) and radius nearest neighbor (rNN) have been shown to have intrinsic majority vote mechanisms.  This paper shows that kNN and rNN can be certified robust to poisoning attacks with intrinsic majority voting mechanisms. The paper also shows that the certified robustness guarantees of kNN, rNN, and other intrinsic majorityVote mechanisms can be obtained for kNN as well as rNN.  Experiments are conducted on MNIST and CIFAR10 and show that intrinsic certified robusts guarantees of both kNN (and rNN) are better than certified defenses. "
1719,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"training time CONJUNCTION model. model CONJUNCTION training time. model EVALUATE-FOR it. training time EVALUATE-FOR it. stochastic gradient decent ( SGD ) method USED-FOR deep learning models. batch size selection problem USED-FOR graph neural network ( GNN ). SGD method USED-FOR graph neural network ( GNN ). variance of gradients CONJUNCTION compute time. compute time CONJUNCTION variance of gradients. compute time FEATURE-OF mini - batch. compute time FEATURE-OF metric. variance of gradients PART-OF metric. formula USED-FOR optimal batch size. estimator USED-FOR gradients. randomness USED-FOR estimator. Ogbnarxiv CONJUNCTION Reddit. Reddit CONJUNCTION Ogbnarxiv. Ogbn - products CONJUNCTION Ogbnarxiv. Ogbnarxiv CONJUNCTION Ogbn - products. FastGCN CONJUNCTION GraphSAINT. GraphSAINT CONJUNCTION FastGCN. Reddit CONJUNCTION Pubmed. Pubmed CONJUNCTION Reddit. ClusterGCN CONJUNCTION FastGCN. FastGCN CONJUNCTION ClusterGCN. GraphSAINT HYPONYM-OF datasets. FastGCN HYPONYM-OF datasets. Pubmed HYPONYM-OF datasets. Ogbn - products HYPONYM-OF datasets. Reddit HYPONYM-OF datasets. Ogbnarxiv HYPONYM-OF datasets. deep learning models COMPARE GNNs. GNNs COMPARE deep learning models. large batch sizes USED-FOR GNNs. OtherScientificTerm are Batch size, batch - size, and batch size. Method are decent model, and GNN. ","This paper studies the batch size selection problem of a graph neural network (GNN) trained with the SGD method of the stochastic gradient decent (SGD) method for deep learning models. Batch size is defined as the difference between the training time and the final training time of a decent model. The authors propose a new metric that combines the variance of gradients and the compute time of the mini-batch, and show that it can be used to improve the performance of the model and training time. They also propose a formula for the optimal batch size, and provide an estimator for the gradients based on the randomness of the batch-size. Experiments are conducted on three datasets (ClusterGCN, FastGCN and GraphSAINT) and four datasets (Ogbn-products, Ogbnarxiv, Reddit and Pubmed). The authors show that GNNs trained with large batch sizes are more robust to batch size than other deep learning methods, and that deep models trained with larger batches are able to generalize better than GNN's trained with smaller batches. ","This paper studies the batch size selection problem of a graph neural network (GNN) trained with the SGD method of the stochastic gradient decent (SGD) method for deep learning models. Batch size is defined as the difference between the training time and the final training time of a decent model. The authors propose a new metric that combines the variance of gradients and the compute time of the mini-batch, and show that it can be used to improve the performance of the model and training time. They also propose a formula for the optimal batch size, and provide an estimator for the gradients based on the randomness of the batch-size. Experiments are conducted on three datasets (ClusterGCN, FastGCN and GraphSAINT) and four datasets (Ogbn-products, Ogbnarxiv, Reddit and Pubmed). The authors show that GNNs trained with large batch sizes are more robust to batch size than other deep learning methods, and that deep models trained with larger batches are able to generalize better than GNN's trained with smaller batches. "
1728,SP:30d97322709cd292a49f936c767099f11b0e2913,"neural network classifiers USED-FOR real - world applications. confidence scores USED-FOR detecting misclassification errors. framework USED-FOR detecting misclassification errors. framework USED-FOR confidence scores. Gaussian Processes USED-FOR calibrated confidence scores. confidence estimation methods COMPARE approach. approach COMPARE confidence estimation methods. UCI datasets EVALUATE-FOR confidence estimation methods. method USED-FOR neural network classifiers. deep learning architecture USED-FOR vision task. OtherScientificTerm are lowconfidence predictions, and classifier ’s inherent confidence indicators. Metric is confidence metrics. Method is RED. Material is out - of - distribution and adversarial samples. ","This paper proposes a method for improving the confidence of neural network classifiers for real-world applications. The authors propose a framework for calibrating confidence scores for detecting misclassification errors in the presence of lowconfidence predictions. The confidence scores are computed based on the classifier’s inherent confidence indicators, and the confidence metrics are calibrated using a framework called RED. The calibration is based on Gaussian Processes. The proposed method is shown to outperform other confidence estimation methods on the UCI datasets, and is also shown to be more robust to out-of-distribution and adversarial samples. The method can be applied to any deep learning architecture for any vision task. ","This paper proposes a method for improving the confidence of neural network classifiers for real-world applications. The authors propose a framework for calibrating confidence scores for detecting misclassification errors in the presence of lowconfidence predictions. The confidence scores are computed based on the classifier’s inherent confidence indicators, and the confidence metrics are calibrated using a framework called RED. The calibration is based on Gaussian Processes. The proposed method is shown to outperform other confidence estimation methods on the UCI datasets, and is also shown to be more robust to out-of-distribution and adversarial samples. The method can be applied to any deep learning architecture for any vision task. "
1737,SP:131b3da98f56d3af273171f496b217b90754a0a7,information retrieval PART-OF natural language processing systems. open domain question answering HYPONYM-OF natural language processing systems. methods COMPARE continuous representations. continuous representations COMPARE methods. neural networks USED-FOR continuous representations. hand - crafted features USED-FOR methods. supervised data USED-FOR retriever model. supervised data USED-FOR methods. retriever models USED-FOR downstream tasks. technique USED-FOR retriever models. technique USED-FOR downstream tasks. approach USED-FOR synthetic labels. attention scores USED-FOR task. synthetic labels USED-FOR retriever. reader model USED-FOR task. attention scores USED-FOR reader model. approach USED-FOR task. reader model USED-FOR approach. attention scores USED-FOR approach. retrieved documents USED-FOR approach. retrieved documents USED-FOR task. question answering EVALUATE-FOR method. Task is knowledge distillation. ,"This paper proposes an approach for knowledge distillation in the open-domain question answering setting, where the goal is to learn a retriever model from a large amount of supervised data. This is an important problem in the field of information retrieval in natural language processing systems, such as open domain question answering. Previous methods rely on hand-crafted features, which can be more expensive than continuous representations learned by neural networks. This paper proposes a technique to train retriever models on downstream tasks using this technique. The approach first generates synthetic labels for the retriever, and then trains a reader model using attention scores for the task using the retrieved documents. The proposed approach is evaluated on the task of question answering, and the results show that the proposed approach can learn a good retriever with the help of attention scores. ","This paper proposes an approach for knowledge distillation in the open-domain question answering setting, where the goal is to learn a retriever model from a large amount of supervised data. This is an important problem in the field of information retrieval in natural language processing systems, such as open domain question answering. Previous methods rely on hand-crafted features, which can be more expensive than continuous representations learned by neural networks. This paper proposes a technique to train retriever models on downstream tasks using this technique. The approach first generates synthetic labels for the retriever, and then trains a reader model using attention scores for the task using the retrieved documents. The proposed approach is evaluated on the task of question answering, and the results show that the proposed approach can learn a good retriever with the help of attention scores. "
1746,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"exploration USED-FOR reinforcement learned ( RL ) controllers. software engineering CONJUNCTION controller synthesis. controller synthesis CONJUNCTION software engineering. constraints FEATURE-OF constrained Markov decision process. controller synthesis USED-FOR safety methods. software engineering USED-FOR safety methods. formal languages USED-FOR them. finite automata USED-FOR constraint violations. finite automata USED-FOR constraints. Constraint states USED-FOR dense cost function. Constraint states USED-FOR MDP state. methods USED-FOR RL algorithms. constraints USED-FOR RL algorithms. Safety Gym HYPONYM-OF constraints. Atari environments HYPONYM-OF constraints. OtherScientificTerm are safety conditions, safety critical situations, and joint MDP / constraint dynamics. Method are safe controller, and learning. ","This paper proposes a method for learning a safe controller for reinforcement learning. The key idea is to learn a set of constraints on the state of the MDP, and then use these constraints to guide the learning of a safe policy. The authors show that this approach can be applied to a variety of RL problems, and that it can be combined with a number of existing safety-aware RL algorithms. The paper also shows that the proposed method can be used to train a controller that is robust to the constraints.","This paper proposes a method for learning a safe controller for reinforcement learning. The key idea is to learn a set of constraints on the state of the MDP, and then use these constraints to guide the learning of a safe policy. The authors show that this approach can be applied to a variety of RL problems, and that it can be combined with a number of existing safety-aware RL algorithms. The paper also shows that the proposed method can be used to train a controller that is robust to the constraints."
1755,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"binary classification algorithm USED-FOR models. decision tree learning HYPONYM-OF binary classification algorithm. first - class transparency FEATURE-OF models. decision tree model USED-FOR comprehensibility of classifications. Cascading Decision Trees HYPONYM-OF decision tree model. decision path CONJUNCTION explanation path. explanation path CONJUNCTION decision path. monolithic decision tree COMPARE decision subtrees. decision subtrees COMPARE monolithic decision tree. subtree USED-FOR features. subtrees USED-FOR positive classification. model COMPARE decision tree model. decision tree model COMPARE model. datasets CONJUNCTION real - world applications. real - world applications CONJUNCTION datasets. datasets EVALUATE-FOR model. real - world applications EVALUATE-FOR model. positive classifications EVALUATE-FOR model. explanation depth EVALUATE-FOR model. real - world applications EVALUATE-FOR algorithm. datasets EVALUATE-FOR algorithm. Method are decision trees, cascading decision subtrees, and cascading decision trees. OtherScientificTerm is decision paths. ","This paper proposes a new binary classification algorithm, called decision tree learning, which aims to improve models with first-class transparency in order to improve the comprehensibility of classifications. The authors propose a decision tree model called Cascading Decision Trees, which is an extension of decision trees. In particular, the authors propose cascading decision subtrees, where a monolithic decision tree is replaced by a set of individual decision paths, and each decision path corresponds to an explanation path. Each subtree is then used to extract features from the original decision tree, and the features from each subtree are concatenated to form the final decision tree.   The authors show that the features extracted from a subtree can be used to train a model with higher explanation depth than those extracted from the decision paths. They also show that a model trained with cascading explanation depth is able to achieve higher positive classifications than a standard model trained on the same datasets and real-world applications.  The algorithm is evaluated on three datasets and three real-life applications.","This paper proposes a new binary classification algorithm, called decision tree learning, which aims to improve models with first-class transparency in order to improve the comprehensibility of classifications. The authors propose a decision tree model called Cascading Decision Trees, which is an extension of decision trees. In particular, the authors propose cascading decision subtrees, where a monolithic decision tree is replaced by a set of individual decision paths, and each decision path corresponds to an explanation path. Each subtree is then used to extract features from the original decision tree, and the features from each subtree are concatenated to form the final decision tree.   The authors show that the features extracted from a subtree can be used to train a model with higher explanation depth than those extracted from the decision paths. They also show that a model trained with cascading explanation depth is able to achieve higher positive classifications than a standard model trained on the same datasets and real-world applications.  The algorithm is evaluated on three datasets and three real-life applications."
1764,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"random, static sparsity pattern FEATURE-OF weight tensors. random, static sparsity pattern USED-FOR models. training accuarcy EVALUATE-FOR model. Gaussian Process kernels USED-FOR models. sparse finite - width model kernel CONJUNCTION infinite - width kernel. infinite - width kernel CONJUNCTION sparse finite - width model kernel. Method is neural networks. OtherScientificTerm are network width, and model width. ","This paper studies the sparsity of neural networks. The authors show that models with Gaussian Process kernels have a random, static sparsity pattern on the weight tensors. They also show that the training accuarcy of a model can be approximated by a sparse finite-width model kernel and an infinite-width kernel. They further show that network width does not depend on the model width. ","This paper studies the sparsity of neural networks. The authors show that models with Gaussian Process kernels have a random, static sparsity pattern on the weight tensors. They also show that the training accuarcy of a model can be approximated by a sparse finite-width model kernel and an infinite-width kernel. They further show that network width does not depend on the model width. "
1773,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,world knowledge CONJUNCTION entities. entities CONJUNCTION world knowledge. Knowledge graphs ( KGs ) USED-FOR world knowledge. entities PART-OF Knowledge graphs ( KGs ). they COMPARE pre - trained language models. pre - trained language models COMPARE they. KG USED-FOR language modeling. joint pre - training framework USED-FOR knowledge graph. knowledge graph CONJUNCTION language. language CONJUNCTION knowledge graph. joint pre - training framework USED-FOR language. JAKET USED-FOR knowledge graph. JAKET USED-FOR language. JAKET HYPONYM-OF joint pre - training framework. knowledge module CONJUNCTION language module. language module CONJUNCTION knowledge module. knowledge module CONJUNCTION language module. language module CONJUNCTION knowledge module. knowledge module USED-FOR embeddings. language module USED-FOR context - aware initial embeddings. knowledge - aware NLP tasks EVALUATE-FOR framework. knowledge in language understanding USED-FOR framework. OtherScientificTerm is graph. Method is pre - trained model. ,"Knowledge graphs (KGs) represent world knowledge and entities in the context of a language, and they can be used to improve the performance of pre-trained language models. In this paper, the authors propose a joint pre-training framework, JAKET, to learn a knowledge graph and language from a KG for language modeling. A knowledge module is used to learn the embeddings of the knowledge graph, and a language module learns the context-aware initial embedding of the graph. The proposed framework is evaluated on a number of knowledge-aware NLP tasks, and the results show that the proposed framework performs well in terms of knowledge in language understanding.   ","Knowledge graphs (KGs) represent world knowledge and entities in the context of a language, and they can be used to improve the performance of pre-trained language models. In this paper, the authors propose a joint pre-training framework, JAKET, to learn a knowledge graph and language from a KG for language modeling. A knowledge module is used to learn the embeddings of the knowledge graph, and a language module learns the context-aware initial embedding of the graph. The proposed framework is evaluated on a number of knowledge-aware NLP tasks, and the results show that the proposed framework performs well in terms of knowledge in language understanding.   "
1782,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"hand - designed loss functions USED-FOR specific orders. domain - specific insight USED-FOR approaches. unsupervised learner USED-FOR autoregressive orders. neural network USED-FOR variational inference. learner HYPONYM-OF neural network. autoregressive ordering USED-FOR variational inference. algorithm USED-FOR end - to - end optimization. policy gradients USED-FOR algorithm. algorithm USED-FOR autoregressive orders. algorithm COMPARE fixed orders. fixed orders COMPARE algorithm. sequence modeling tasks EVALUATE-FOR algorithm. autoregressive orders COMPARE fixed orders. fixed orders COMPARE autoregressive orders. sequence modeling tasks EVALUATE-FOR solution. Task is language modeling. OtherScientificTerm are predefined ordering, insertion operations, domain - specific prior, latent variable, and variational lower bound. Metric is time complexity. ","This paper considers the problem of language modeling, where the goal is to learn a predefined ordering for a sequence of input words. The authors propose to use hand-designed loss functions to learn specific orders from a pre-defined set of predefined orders. Previous approaches rely on domain-specific insight, i.e., the order of the input words should be the same for all input words, but for each input word, the authors propose an autoregressive ordering.    The authors introduce an unsupervised learner that learns to learn to learn the order from a set of pre-specified orders. The learner is a neural network that is trained to perform variational inference over the order, and the authors introduce insertion operations that allow the learner to select the order that is most likely to be the most likely for the input word.  The algorithm is based on the observation that the autoregression of the order is a function of the prior on a latent variable, and that the order can be learned in an end-to-end manner. The algorithm uses policy gradients from the prior to learn an algorithm that is able to perform end- to-end optimization. The proposed algorithm is shown to be more efficient than the previous work, and outperforms the state-of-the-art in terms of time complexity.  Experiments on sequence modeling tasks show that the proposed solution outperforms prior work on a number of tasks, and is competitive with the best-performing approaches when the order in the set is a domain-agnostic one. The paper also shows that the algorithm can be used to learn autore progressive orders more efficiently than the fixed orders. ","This paper considers the problem of language modeling, where the goal is to learn a predefined ordering for a sequence of input words. The authors propose to use hand-designed loss functions to learn specific orders from a pre-defined set of predefined orders. Previous approaches rely on domain-specific insight, i.e., the order of the input words should be the same for all input words, but for each input word, the authors propose an autoregressive ordering.    The authors introduce an unsupervised learner that learns to learn to learn the order from a set of pre-specified orders. The learner is a neural network that is trained to perform variational inference over the order, and the authors introduce insertion operations that allow the learner to select the order that is most likely to be the most likely for the input word.  The algorithm is based on the observation that the autoregression of the order is a function of the prior on a latent variable, and that the order can be learned in an end-to-end manner. The algorithm uses policy gradients from the prior to learn an algorithm that is able to perform end- to-end optimization. The proposed algorithm is shown to be more efficient than the previous work, and outperforms the state-of-the-art in terms of time complexity.  Experiments on sequence modeling tasks show that the proposed solution outperforms prior work on a number of tasks, and is competitive with the best-performing approaches when the order in the set is a domain-agnostic one. The paper also shows that the algorithm can be used to learn autore progressive orders more efficiently than the fixed orders. "
1791,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,"Graph Convolutional Networks ( GCNs ) USED-FOR graph - related applications. large graphs USED-FOR GCNs. evolving parameters USED-FOR optimization. doubly variance reduction schema USED-FOR sampling method. O(1 / T ) convergence rate EVALUATE-FOR it. schema USED-FOR sampling methods. them USED-FOR large real - world graphs. OtherScientificTerm are computational and memory issues, nodes, memory budget, and induced variance. Method are sampling - based methods, variance of sampling methods, forward propagation, and backward propagation. Generic is works. Metric is theoretical convergence guarantees. ","Graph Convolutional Networks (GCNs) have been widely used in graph-related applications, but there are computational and memory issues due to the large graphs required to train GCNs. In particular, existing sampling-based methods suffer from the issue that the variance of sampling methods depends on the number of nodes and the memory budget. This paper proposes a doubly variance reduction scheme to reduce the variance in forward propagation and backward propagation in order to make the optimization tractable with evolving parameters. The authors propose a new sampling method based on the doubly variances reduction schema and show that it can achieve O(1/T) convergence rate. They also provide theoretical convergence guarantees for existing sampling methods based on this new schema. Finally, the authors show that their sampling methods can be applied to existing works and apply them to large real-world graphs. ","Graph Convolutional Networks (GCNs) have been widely used in graph-related applications, but there are computational and memory issues due to the large graphs required to train GCNs. In particular, existing sampling-based methods suffer from the issue that the variance of sampling methods depends on the number of nodes and the memory budget. This paper proposes a doubly variance reduction scheme to reduce the variance in forward propagation and backward propagation in order to make the optimization tractable with evolving parameters. The authors propose a new sampling method based on the doubly variances reduction schema and show that it can achieve O(1/T) convergence rate. They also provide theoretical convergence guarantees for existing sampling methods based on this new schema. Finally, the authors show that their sampling methods can be applied to existing works and apply them to large real-world graphs. "
1800,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"deep neural network methods USED-FOR image manipulation tasks. conditional adversarial generator USED-FOR complex image manipulations. edges CONJUNCTION segmentation. segmentation CONJUNCTION edges. primitive input representation USED-FOR generator. Task are Image manipulation, and single image training. Generic are task, network, method, and it. Method are deep methods, and augmentation method. ","This paper proposes a conditional adversarial generator to learn complex image manipulations using deep neural network methods for image manipulation tasks. Image manipulation is an important problem in many applications, and it is important to be able to perform complex image manipulation in an unsupervised way.  Image manipulation can be a challenging task, and the authors propose a method to augment the input images to the network to make it more robust to adversarial attacks.   The proposed method is based on the idea of augmenting the primitive input representation of the generator to make the generator more robust.  The authors show that this augmentation method can be applied to a wide range of tasks, and that it can be used to improve the performance of existing deep methods.  Experiments are conducted on single image training, where the augmentations are applied to both edges and segmentation. ","This paper proposes a conditional adversarial generator to learn complex image manipulations using deep neural network methods for image manipulation tasks. Image manipulation is an important problem in many applications, and it is important to be able to perform complex image manipulation in an unsupervised way.  Image manipulation can be a challenging task, and the authors propose a method to augment the input images to the network to make it more robust to adversarial attacks.   The proposed method is based on the idea of augmenting the primitive input representation of the generator to make the generator more robust.  The authors show that this augmentation method can be applied to a wide range of tasks, and that it can be used to improve the performance of existing deep methods.  Experiments are conducted on single image training, where the augmentations are applied to both edges and segmentation. "
1809,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"malware detection CONJUNCTION cloud computing. cloud computing CONJUNCTION malware detection. biomedical analysis CONJUNCTION malware detection. malware detection CONJUNCTION biomedical analysis. Detecting the Maximum Common Subgraph ( MCS ) USED-FOR biomedical analysis. heuristics in search USED-FOR MCS solvers. Graph Neural Network based model USED-FOR MCS detection. GLSEARCH HYPONYM-OF Graph Neural Network based model. branch and bound algorithm USED-FOR backbone search algorithm. model USED-FOR subgraphs. branch and bound algorithm USED-FOR subgraphs. branch and bound algorithm USED-FOR model. search process USED-FOR supervision. imitation learning stage USED-FOR agent. search process USED-FOR DQN. search CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION search. framework USED-FOR reinforcement learning. framework USED-FOR search. synthetic and real - world large graph pairs EVALUATE-FOR model. MCS solvers CONJUNCTION neural graph matching network models. neural graph matching network models CONJUNCTION MCS solvers. model COMPARE neural graph matching network models. neural graph matching network models COMPARE model. model COMPARE MCS solvers. MCS solvers COMPARE model. synthetic and real - world large graph pairs EVALUATE-FOR neural graph matching network models. synthetic and real - world large graph pairs EVALUATE-FOR MCS solvers. OtherScientificTerm are Maximum Common Subgraph ( MCS ), large graph pairs, limited search budget, and node selection decision. Task are drug design, extraction of common substructures, and MCS computation. Method is node selection heuristics. ","Detecting the Maximum Common Subgraph (MCS) is an important problem in biomedical analysis, malware detection and cloud computing. In this paper, the authors propose a Graph Neural Network based model, called GLSEARCH, for MCS detection. The proposed model learns subgraphs using a branch and bound algorithm that is used as a backbone search algorithm. The search process is then used as supervision during the imitation learning stage to guide the agent through the search process. The authors show that the proposed model outperforms state-of-the-art MCS solvers and neural graph matching network models on synthetic and real-world large graph pairs.    The main contribution of this paper is to propose a novel search framework that combines search and reinforcement learning. The main idea is to search for a large number of nodes in a large graph, which is called the ""maximum common subgraph"" in this paper. This is a very important problem for drug design as it is important for the extraction of common substructures in a drug design task.  The authors propose to use heuristics in search to improve the performance of existing M CS solvers. They show that existing solvers do not generalize well to large graphs due to the limited search budget. They also show that there is a trade-off between the quality of the search and the efficiency of the MCS computation. The paper also shows that the search can be done in a single step, which makes the proposed search process much more efficient. Finally, the paper shows that DQNQN can be used as an alternative search process for the proposed method. The idea is interesting and the paper is well-motivated. However, there are a few issues in the paper. For example, the node selection decision is not clear and there are some questions about the effectiveness of the proposed approach. There are also questions about how the model is able to generalize to larger graphs, and how well the model can generalize. ","Detecting the Maximum Common Subgraph (MCS) is an important problem in biomedical analysis, malware detection and cloud computing. In this paper, the authors propose a Graph Neural Network based model, called GLSEARCH, for MCS detection. The proposed model learns subgraphs using a branch and bound algorithm that is used as a backbone search algorithm. The search process is then used as supervision during the imitation learning stage to guide the agent through the search process. The authors show that the proposed model outperforms state-of-the-art MCS solvers and neural graph matching network models on synthetic and real-world large graph pairs.    The main contribution of this paper is to propose a novel search framework that combines search and reinforcement learning. The main idea is to search for a large number of nodes in a large graph, which is called the ""maximum common subgraph"" in this paper. This is a very important problem for drug design as it is important for the extraction of common substructures in a drug design task.  The authors propose to use heuristics in search to improve the performance of existing M CS solvers. They show that existing solvers do not generalize well to large graphs due to the limited search budget. They also show that there is a trade-off between the quality of the search and the efficiency of the MCS computation. The paper also shows that the search can be done in a single step, which makes the proposed search process much more efficient. Finally, the paper shows that DQNQN can be used as an alternative search process for the proposed method. The idea is interesting and the paper is well-motivated. However, there are a few issues in the paper. For example, the node selection decision is not clear and there are some questions about the effectiveness of the proposed approach. There are also questions about how the model is able to generalize to larger graphs, and how well the model can generalize. "
1818,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"PC2WF USED-FOR wireframe model. network USED-FOR wireframe. vertices CONJUNCTION edges. edges CONJUNCTION vertices. model USED-FOR architecture. it USED-FOR candidate vertices. features USED-FOR it. candidate edges USED-FOR wireframe. ground truth wireframes FEATURE-OF synthetic dataset. real - world dataset EVALUATE-FOR model. synthetic dataset EVALUATE-FOR model. model COMPARE baselines. baselines COMPARE model. model USED-FOR wireframe abstractions. OtherScientificTerm are 3D point cloud, line segments, feature vectors, and corner vertices. Task is Recovering the wireframe. Generic is It. ","This paper proposes a new wireframe model based on PC2WF. The idea is to build a 3D point cloud from a set of vertices, line segments, and edges. A network is trained to reconstruct a wireframe from the vertices and edges of the point cloud. Recovering the wireframe is done by reconstructing the feature vectors of each vertice and edge. The model is trained on a synthetic dataset of ground truth wireframes, and on a real-world dataset. The proposed model can be used to learn a new architecture, and it can also recover candidate vertices from the features. It is shown that the proposed model is able to learn wireframe abstractions that are more interpretable than baselines.   ","This paper proposes a new wireframe model based on PC2WF. The idea is to build a 3D point cloud from a set of vertices, line segments, and edges. A network is trained to reconstruct a wireframe from the vertices and edges of the point cloud. Recovering the wireframe is done by reconstructing the feature vectors of each vertice and edge. The model is trained on a synthetic dataset of ground truth wireframes, and on a real-world dataset. The proposed model can be used to learn a new architecture, and it can also recover candidate vertices from the features. It is shown that the proposed model is able to learn wireframe abstractions that are more interpretable than baselines.   "
1827,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"assumptions FEATURE-OF distribution of noise. assumptions USED-FOR stochastic optimization. uniform bound USED-FOR moments of the gradient noise. optimal convergence rates FEATURE-OF stochastic optimization. optimization algorithms USED-FOR neural network training. noise level FEATURE-OF stochastic gradients. convergence rates FEATURE-OF stochastic gradient methods. adaptive step size methods COMPARE SGD. SGD COMPARE adaptive step size methods. Method are neural networks, online estimator of the noise level, and RMSProp. OtherScientificTerm are noise, nonstationary behavior of noise, stochastic oracle, noise variation, step - size, noise variability, noise statistics, and theoretical guarantees. ","This paper studies the convergence of stochastic optimization under certain assumptions on the distribution of noise in neural networks. The authors prove a uniform bound on the moments of the gradient noise, which is a result of the nonstationary behavior of noise. They show that under this assumption, the optimal convergence rates of the so-called ""stochastic optimization"" of neural network training can be obtained. They also provide an online estimator of the noise level, which they call RMSProp.    The main contribution of this paper is to provide theoretical guarantees on the convergence rates for stochastastic gradient methods under the assumption that the noise variation is non-stationary and that the step-size of the oracle is not too large.  The authors also provide a theoretical analysis of optimization algorithms that can be applied to the case where the noise is not stationary. The main result is that adaptive step size methods can converge faster than SGD when the noise variability is small, and the authors also show that if the noise variations are non-zero, then adaptive step-sizes can converge as well.  Finally, the authors provide some numerical experiments to verify the theoretical results.","This paper studies the convergence of stochastic optimization under certain assumptions on the distribution of noise in neural networks. The authors prove a uniform bound on the moments of the gradient noise, which is a result of the nonstationary behavior of noise. They show that under this assumption, the optimal convergence rates of the so-called ""stochastic optimization"" of neural network training can be obtained. They also provide an online estimator of the noise level, which they call RMSProp.    The main contribution of this paper is to provide theoretical guarantees on the convergence rates for stochastastic gradient methods under the assumption that the noise variation is non-stationary and that the step-size of the oracle is not too large.  The authors also provide a theoretical analysis of optimization algorithms that can be applied to the case where the noise is not stationary. The main result is that adaptive step size methods can converge faster than SGD when the noise variability is small, and the authors also show that if the noise variations are non-zero, then adaptive step-sizes can converge as well.  Finally, the authors provide some numerical experiments to verify the theoretical results."
1836,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,Prior word alignment USED-FOR translation. statistical machine translation ( SMT ) models USED-FOR word alignment. method USED-FOR neural machine translation ( NMT ). prior word alignment information USED-FOR neural machine translation ( NMT ). prior word alignment information USED-FOR method. dictionaries USED-FOR approaches. decoding speed EVALUATE-FOR methods. model PART-OF neural MT model. learning model USED-FOR target information. target information USED-FOR MT input. method COMPARE baseline model. baseline model COMPARE method. English - Korean EVALUATE-FOR baseline model. English - Korean EVALUATE-FOR method. Generic is prior. Task is decoding process. Method is enhancement learning model. OtherScientificTerm is prior alignment information. ,"This paper proposes a method to incorporate prior word alignment information to improve the performance of neural machine translation (NMT) models. Prior word alignment is an important component in translation, and the authors propose to use statistical machine translation(SMT) models to improve word alignment. The authors propose two approaches to do so: (1) using dictionaries, and (2) augmenting the prior during the decoding process. Both methods are shown to improve decoding speed and improve decoding performance.   The authors also propose an enhancement learning model to incorporate the prior alignment information into the training of the model in a neural MT model. The learning model is trained to extract the target information from the MT input. The proposed method is evaluated on English-Korean and is shown to outperform a baseline model. ","This paper proposes a method to incorporate prior word alignment information to improve the performance of neural machine translation (NMT) models. Prior word alignment is an important component in translation, and the authors propose to use statistical machine translation(SMT) models to improve word alignment. The authors propose two approaches to do so: (1) using dictionaries, and (2) augmenting the prior during the decoding process. Both methods are shown to improve decoding speed and improve decoding performance.   The authors also propose an enhancement learning model to incorporate the prior alignment information into the training of the model in a neural MT model. The learning model is trained to extract the target information from the MT input. The proposed method is evaluated on English-Korean and is shown to outperform a baseline model. "
1845,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,MDP Playground HYPONYM-OF Reinforcement Learning ( RL ) algorithms. MDP Playground HYPONYM-OF benchmark. benchmark EVALUATE-FOR Reinforcement Learning ( RL ) algorithms. dimensions of hardness FEATURE-OF MDP Playground. stochasticity CONJUNCTION image representations. image representations CONJUNCTION stochasticity. image representations CONJUNCTION irrelevant features. irrelevant features CONJUNCTION image representations. sparsity of rewards CONJUNCTION stochasticity. stochasticity CONJUNCTION sparsity of rewards. time unit CONJUNCTION action range. action range CONJUNCTION time unit. irrelevant features CONJUNCTION time unit. time unit CONJUNCTION irrelevant features. delayed rewards CONJUNCTION rewardable sequences. rewardable sequences CONJUNCTION delayed rewards. rewardable sequences CONJUNCTION sparsity of rewards. sparsity of rewards CONJUNCTION rewardable sequences. action range HYPONYM-OF hardness dimensions. time unit HYPONYM-OF hardness dimensions. sparsity of rewards HYPONYM-OF hardness dimensions. stochasticity HYPONYM-OF hardness dimensions. irrelevant features HYPONYM-OF hardness dimensions. delayed rewards HYPONYM-OF hardness dimensions. image representations HYPONYM-OF hardness dimensions. rewardable sequences HYPONYM-OF hardness dimensions. benchmarks EVALUATE-FOR RL algorithms. benchmarks EVALUATE-FOR RL algorithms. fine - grained control FEATURE-OF environments ’ hardness. MDP Playground USED-FOR adaptive and intelligent RL algorithms. MDP Playground EVALUATE-FOR algorithms. OtherScientificTerm is hardness. Material is OpenAI Gym. Generic is dimensions. ,"This paper presents a new benchmark for evaluating Reinforcement Learning (RL) algorithms called MDP Playground, which is a benchmark for different dimensions of hardness. The hardness is defined as the difference between the hardness of a state in the MDP in terms of time, time unit, action range, sparsity of rewards, stochasticity, image representations, and irrelevant features.   The hardness dimensions are defined as delayed rewards, rewardable sequences, and sparsity.  The paper shows that the hardness dimensions include: (1) the time unit (2) the action range (3) the number of states, (4) the sparsity, (5) the amount of irrelevant features, (6) and (7) the total number of rewards.  In addition, the paper also shows that environments’ hardness is correlated with fine-grained control, and that adaptive and intelligent RL algorithms trained on the new benchmarks outperform existing RL algorithms on the original OpenAI Gym.  Finally, the authors show that the new algorithms are able to outperform the state-of-the-art in the original MDP playground. ","This paper presents a new benchmark for evaluating Reinforcement Learning (RL) algorithms called MDP Playground, which is a benchmark for different dimensions of hardness. The hardness is defined as the difference between the hardness of a state in the MDP in terms of time, time unit, action range, sparsity of rewards, stochasticity, image representations, and irrelevant features.   The hardness dimensions are defined as delayed rewards, rewardable sequences, and sparsity.  The paper shows that the hardness dimensions include: (1) the time unit (2) the action range (3) the number of states, (4) the sparsity, (5) the amount of irrelevant features, (6) and (7) the total number of rewards.  In addition, the paper also shows that environments’ hardness is correlated with fine-grained control, and that adaptive and intelligent RL algorithms trained on the new benchmarks outperform existing RL algorithms on the original OpenAI Gym.  Finally, the authors show that the new algorithms are able to outperform the state-of-the-art in the original MDP playground. "
1854,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"they USED-FOR overconfident predictions. approaches USED-FOR classification models. Isotonic Regression USED-FOR regression calibration. Isotonic Regression USED-FOR regression calibration. formulation USED-FOR quantile regularizer. quantile regularizer USED-FOR probabilistic regression model. approaches USED-FOR regression models. Dropout VI CONJUNCTION Deep Ensembles. Deep Ensembles CONJUNCTION Dropout VI. approach USED-FOR regression models. calibration USED-FOR regression models. architectures USED-FOR uncertainty estimates. approach USED-FOR calibration. architectures USED-FOR regression models. Deep Ensembles HYPONYM-OF uncertainty estimates. Dropout VI HYPONYM-OF uncertainty estimates. Method are Deep learning models, quantile calibration, and entropy estimation. Generic are it, model, and method. ","This paper proposes Isotonic Regression to improve regression calibration of deep learning models. Deep learning models have been shown to suffer from overconfident predictions as they are trained with quantile calibration. This paper proposes a new formulation of the quantile regularizer for a probabilistic regression model and shows that it can be used to improve the calibration of regression models trained with different approaches to calibration of classification models. The paper also shows that the proposed formulation is a generalization of the formulation of a previous method, which is based on entropy estimation. The proposed approach is applied to several regression models with different architectures and different uncertainty estimates (Dropout VI and Deep Ensembles). ","This paper proposes Isotonic Regression to improve regression calibration of deep learning models. Deep learning models have been shown to suffer from overconfident predictions as they are trained with quantile calibration. This paper proposes a new formulation of the quantile regularizer for a probabilistic regression model and shows that it can be used to improve the calibration of regression models trained with different approaches to calibration of classification models. The paper also shows that the proposed formulation is a generalization of the formulation of a previous method, which is based on entropy estimation. The proposed approach is applied to several regression models with different architectures and different uncertainty estimates (Dropout VI and Deep Ensembles). "
1863,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"6 - DoF localisation CONJUNCTION 3D dense reconstruction in spatial environments. 3D dense reconstruction in spatial environments CONJUNCTION 6 - DoF localisation. deep state - space model USED-FOR approximate Bayesian inference. approximate Bayesian inference USED-FOR 3D dense reconstruction in spatial environments. approximate Bayesian inference USED-FOR 6 - DoF localisation. multiple - view geometry CONJUNCTION rigid - body dynamics. rigid - body dynamics CONJUNCTION multiple - view geometry. rigid - body dynamics USED-FOR learning and domain knowledge. multiple - view geometry USED-FOR learning and domain knowledge. learning and domain knowledge USED-FOR approach. neural networks CONJUNCTION differentiable raycaster. differentiable raycaster CONJUNCTION neural networks. variational inference CONJUNCTION neural networks. neural networks CONJUNCTION variational inference. realistic unmanned aerial vehicle flight data EVALUATE-FOR approach. model USED-FOR generative prediction and planning. OtherScientificTerm is spatial environments. Method are visual SLAM solutions, and visual - inertial odometry systems. ","This paper proposes a deep state-space model for approximate Bayesian inference for 6-DoF localisation and 3D dense reconstruction in spatial environments. The approach is based on learning and domain knowledge based on multiple-view geometry and rigid-body dynamics. The authors combine variational inference with neural networks and a differentiable raycaster. The proposed approach is evaluated on realistic unmanned aerial vehicle flight data and shows that the proposed model can be used for generative prediction and planning.    The paper is well-written and well-motivated. The idea of using multiple views for spatial environments is novel and interesting. However, there is a lack of comparison with other visual SLAM solutions. The paper also lacks comparison with visual-inertial odometry systems. ","This paper proposes a deep state-space model for approximate Bayesian inference for 6-DoF localisation and 3D dense reconstruction in spatial environments. The approach is based on learning and domain knowledge based on multiple-view geometry and rigid-body dynamics. The authors combine variational inference with neural networks and a differentiable raycaster. The proposed approach is evaluated on realistic unmanned aerial vehicle flight data and shows that the proposed model can be used for generative prediction and planning.    The paper is well-written and well-motivated. The idea of using multiple views for spatial environments is novel and interesting. However, there is a lack of comparison with other visual SLAM solutions. The paper also lacks comparison with visual-inertial odometry systems. "
1872,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"textual descriptions USED-FOR generalization of control policies. symbol grounding CONJUNCTION control policy. control policy CONJUNCTION symbol grounding. environment rewards USED-FOR supervision. environment rewards USED-FOR EMMA. free - form natural language FEATURE-OF text manuals. framework EVALUATE-FOR model. crowd - sourcing USED-FOR free - form natural language. crowd - sourcing USED-FOR text manuals. zeroshot generalization EVALUATE-FOR EMMA. noisy descriptions FEATURE-OF grounding. EMMA USED-FOR grounding. OtherScientificTerm are prior knowledge, concrete supervision, and dynamics. Generic is policies. Method is multi - modal entity - conditioned attention module. ","This paper studies the problem of generalization of control policies from text descriptions. The authors propose EMMA, a model that learns to generalize from prior knowledge without concrete supervision. EMMA uses environment rewards to provide supervision. The model is trained in a framework where the goal is to learn policies that generalize well to unseen environments. This is achieved through a multi-modal entity-conditioned attention module, which is trained on both symbol grounding and a control policy. Experiments are conducted using free-form natural language from text manuals generated from crowd-sourcing, and the authors show that EMMA achieves zeroshot generalization. They also show that grounding with noisy descriptions can be improved by EMMA. ","This paper studies the problem of generalization of control policies from text descriptions. The authors propose EMMA, a model that learns to generalize from prior knowledge without concrete supervision. EMMA uses environment rewards to provide supervision. The model is trained in a framework where the goal is to learn policies that generalize well to unseen environments. This is achieved through a multi-modal entity-conditioned attention module, which is trained on both symbol grounding and a control policy. Experiments are conducted using free-form natural language from text manuals generated from crowd-sourcing, and the authors show that EMMA achieves zeroshot generalization. They also show that grounding with noisy descriptions can be improved by EMMA. "
1881,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,"Policy gradient algorithms USED-FOR decision making and control tasks. high sample complexity CONJUNCTION instability issues. instability issues CONJUNCTION high sample complexity. instability issues EVALUATE-FOR methods. high sample complexity EVALUATE-FOR methods. approach USED-FOR critic. actor - critic framework FEATURE-OF critic. mean value COMPARE absolute value. absolute value COMPARE mean value. continuous control tasks CONJUNCTION algorithms. algorithms CONJUNCTION continuous control tasks. sparse rewards USED-FOR method. Method are actor - critic algorithms, actor - critic, and gradient estimator. OtherScientificTerm is value function. ","Policy gradient algorithms for decision making and control tasks have been shown to suffer from high sample complexity and instability issues. This paper proposes a new approach to train a critic in the actor-critic framework, where the value function is a weighted sum of the mean value and the absolute value of the policy gradient estimator. The authors show that the proposed method can be applied to continuous control tasks with sparse rewards and outperform existing algorithms. ","Policy gradient algorithms for decision making and control tasks have been shown to suffer from high sample complexity and instability issues. This paper proposes a new approach to train a critic in the actor-critic framework, where the value function is a weighted sum of the mean value and the absolute value of the policy gradient estimator. The authors show that the proposed method can be applied to continuous control tasks with sparse rewards and outperform existing algorithms. "
1890,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"network USED-FOR overparameterization. overparameterized regime FEATURE-OF depth. locality of the relevant feature FEATURE-OF classification rule. initialization CONJUNCTION infinitesimal learning rate. infinitesimal learning rate CONJUNCTION initialization. finite networks COMPARE neural tangent kernel ( NTK ). neural tangent kernel ( NTK ) COMPARE finite networks. infinitely wide network USED-FOR neural tangent kernel ( NTK ). initialization USED-FOR infinitely wide network. infinitesimal learning rate FEATURE-OF infinitely wide network. depth dependence FEATURE-OF generalization performance. feature learning COMPARE lazy learning. lazy learning COMPARE feature learning. NTK USED-FOR depth dependence. generalization performance EVALUATE-FOR NTK. Task are generalization, and machinelearning tasks. OtherScientificTerm are local and global labels, classification rules, local labels, and global labels. ","This paper considers the problem of generalization in the overparameterized regime, where the depth of the network is highly correlated with the number of local and global labels. In this setting, the authors show that the depth dependence of a network is a function of the number and the locality of the relevant feature in the classification rule, and that a network with a large enough depth is more prone to overparametrized. The authors also show that a neural tangent kernel (NTK) on an infinitely wide network with an initialization and infinitesimal learning rate is equivalent to a finite networks with the same depth dependence as on finite networks.    The paper also shows that feature learning with the NTK is more sensitive to depth dependence than lazy learning, and shows that the generalization performance of NTK can be improved when the classification rules are differentiable.  Finally, the paper shows that for some machinelearning tasks, NTK improves the performance of feature learning when the local labels are global, but not when the global labels are not global.","This paper considers the problem of generalization in the overparameterized regime, where the depth of the network is highly correlated with the number of local and global labels. In this setting, the authors show that the depth dependence of a network is a function of the number and the locality of the relevant feature in the classification rule, and that a network with a large enough depth is more prone to overparametrized. The authors also show that a neural tangent kernel (NTK) on an infinitely wide network with an initialization and infinitesimal learning rate is equivalent to a finite networks with the same depth dependence as on finite networks.    The paper also shows that feature learning with the NTK is more sensitive to depth dependence than lazy learning, and shows that the generalization performance of NTK can be improved when the classification rules are differentiable.  Finally, the paper shows that for some machinelearning tasks, NTK improves the performance of feature learning when the local labels are global, but not when the global labels are not global."
1899,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"sample complexity EVALUATE-FOR representation. representation learning USED-FOR few - shot learning. i.i.d. task assumption USED-FOR Ω ( 1 T ) barrier. high - dimensional linear regression CONJUNCTION neural networks. neural networks CONJUNCTION high - dimensional linear regression. representation learning USED-FOR high - dimensional linear regression. representation learning USED-FOR neural networks. representation learning USED-FOR representation learning. Material is n2 ( n1 ) data. Generic is common representation. Task is sample size reduction. Metric is risk bound. OtherScientificTerm are linear representation class, and representation function class. ","This paper studies the sample complexity of learning a representation for few-shot learning with n2 (n1) data. The authors show that under the i.i.d. task assumption, there exists a common representation that can be learned with sample size reduction. They also provide a risk bound for this representation learning in the context of representation learning for the problem of few shot learning. The main contribution of this paper is to show that the Ω(1T) barrier is upper bounded under the linear representation class, and that the risk bound depends on the representation function class. The paper also shows that representation learning can be applied to high-dimensional linear regression and neural networks with representation learning. ","This paper studies the sample complexity of learning a representation for few-shot learning with n2 (n1) data. The authors show that under the i.i.d. task assumption, there exists a common representation that can be learned with sample size reduction. They also provide a risk bound for this representation learning in the context of representation learning for the problem of few shot learning. The main contribution of this paper is to show that the Ω(1T) barrier is upper bounded under the linear representation class, and that the risk bound depends on the representation function class. The paper also shows that representation learning can be applied to high-dimensional linear regression and neural networks with representation learning. "
1908,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,lowestlevel features FEATURE-OF network robustness. robustness EVALUATE-FOR networks. semantic features USED-FOR networks. black - box approach USED-FOR features. network USED-FOR features. features USED-FOR provably robust neighborhoods. provably robust neighborhoods CONJUNCTION adversarial examples. adversarial examples CONJUNCTION provably robust neighborhoods. robust features CONJUNCTION adversarial examples. adversarial examples CONJUNCTION robust features. weak features USED-FOR adversarial examples. robust features USED-FOR provably robust neighborhoods. PCA features EVALUATE-FOR approach. provably robust neighborhoods COMPARE neighborhoods. neighborhoods COMPARE provably robust neighborhoods. adversarial examples COMPARE state - of - the - art. state - of - the - art COMPARE adversarial examples. L2 distortion EVALUATE-FOR state - of - the - art. L2 distortion EVALUATE-FOR adversarial examples. attack USED-FOR ensemble adversarial training. Method is neural networks. Task is neural networks ’ robustness. OtherScientificTerm is perturbations. ,"This paper studies the problem of robustness of neural networks to perturbations in the low-level features of the network. The authors propose a black-box approach to learn features that are provably robust to adversarial attacks. They show that neural networks’ robustness can be measured using semantic features, and that the features learned by the network can be used to train a network robustly. They use these features to train provably-robust neighborhoods and adversarial neighborhoods. They also show that the robust features can be combined with adversarial examples that are based on weak features. The approach is tested on PCA features and is shown to be effective. They find that the provable robust neighborhoods are more robust than the neighborhoods, and the L2 distortion of the adversarial example is higher than the state-of-the-art. They further show that an attack can be applied to the ensemble adversarial training, and show that this attack is more effective than the original attack.   ","This paper studies the problem of robustness of neural networks to perturbations in the low-level features of the network. The authors propose a black-box approach to learn features that are provably robust to adversarial attacks. They show that neural networks’ robustness can be measured using semantic features, and that the features learned by the network can be used to train a network robustly. They use these features to train provably-robust neighborhoods and adversarial neighborhoods. They also show that the robust features can be combined with adversarial examples that are based on weak features. The approach is tested on PCA features and is shown to be effective. They find that the provable robust neighborhoods are more robust than the neighborhoods, and the L2 distortion of the adversarial example is higher than the state-of-the-art. They further show that an attack can be applied to the ensemble adversarial training, and show that this attack is more effective than the original attack.   "
1917,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"transfer learning CONJUNCTION multi - task learning. multi - task learning CONJUNCTION transfer learning. Meta - learning CONJUNCTION transfer learning. transfer learning CONJUNCTION Meta - learning. uniform similarity USED-FOR approaches. method USED-FOR clusters of related tasks. sample complexity EVALUATE-FOR these. expectation - maximization algorithm USED-FOR method. policies USED-FOR agent. expectation step EVALUATE-FOR policies. method COMPARE multi - task learning algorithms. multi - task learning algorithms COMPARE method. complex bipedal walker tasks CONJUNCTION Atari games. Atari games CONJUNCTION complex bipedal walker tasks. discrete and continuous control tasks CONJUNCTION complex bipedal walker tasks. complex bipedal walker tasks CONJUNCTION discrete and continuous control tasks. complex bipedal walker tasks EVALUATE-FOR approach. discrete and continuous control tasks EVALUATE-FOR approach. sample complexity EVALUATE-FOR approaches. Method are reinforcement learning agents, and maximization step. Task is training. OtherScientificTerm is policy. ","Meta-learning and multi-task learning are two popular methods for training reinforcement learning agents. Meta-learning is a generalization of transfer learning, transfer learning from a single task to multiple tasks, or transfer learning between different tasks. These approaches are based on uniform similarity between tasks, and these have been shown to have a lower sample complexity. This paper proposes a method for learning clusters of related tasks. The proposed method is based on an expectation-maximization algorithm, where the goal is to learn a policy that maximizes the expected return of each task. The authors show that this maximization step is equivalent to learning a set of policies that maximize the return of the agent in the expectation step. The approach is evaluated on discrete and continuous control tasks, as well as complex bipedal walker tasks and Atari games, and shows that the proposed method outperforms the state-of-the-art multi-tasks learning algorithms in terms of sample complexity in all cases. ","Meta-learning and multi-task learning are two popular methods for training reinforcement learning agents. Meta-learning is a generalization of transfer learning, transfer learning from a single task to multiple tasks, or transfer learning between different tasks. These approaches are based on uniform similarity between tasks, and these have been shown to have a lower sample complexity. This paper proposes a method for learning clusters of related tasks. The proposed method is based on an expectation-maximization algorithm, where the goal is to learn a policy that maximizes the expected return of each task. The authors show that this maximization step is equivalent to learning a set of policies that maximize the return of the agent in the expectation step. The approach is evaluated on discrete and continuous control tasks, as well as complex bipedal walker tasks and Atari games, and shows that the proposed method outperforms the state-of-the-art multi-tasks learning algorithms in terms of sample complexity in all cases. "
1926,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"self - supervised framework USED-FOR generalizable representations. generalizable representations USED-FOR non - stationary time series. local smoothness USED-FOR neighborhoods in time with stationary properties. local smoothness USED-FOR approach. debiased contrastive objective USED-FOR framework. framework USED-FOR time series representations. method COMPARE unsupervised representation learning approaches. unsupervised representation learning approaches COMPARE method. clustering and classification tasks EVALUATE-FOR multiple datasets. clustering and classification tasks EVALUATE-FOR method. Material are Time series, time series data, and labeling data. Method is Temporal Neighborhood Coding ( TNC ). OtherScientificTerm are encoding space, neighborhood, and distribution of non - neighboring signals. Task is medical field. ","This paper proposes a self-supervised framework for learning generalizable representations for non-stationary time series. Time series are nonstationary in the sense that the time series data does not have stationary properties. The approach is based on local smoothness for neighborhoods in time with stationary properties, which is called Temporal Neighborhood Coding (TNC). The framework uses a debiased contrastive objective, where the encoding space is partitioned into a neighborhood, and each neighborhood is represented as a distribution of non-neighboring signals. The paper shows that the proposed framework is able to learn time series representations that are invariant to changes in the distribution of neighboring signals. Experiments on clustering and classification tasks on multiple datasets show that the method outperforms other unsupervised representation learning approaches, and is particularly useful in the medical field where labeling data is scarce.","This paper proposes a self-supervised framework for learning generalizable representations for non-stationary time series. Time series are nonstationary in the sense that the time series data does not have stationary properties. The approach is based on local smoothness for neighborhoods in time with stationary properties, which is called Temporal Neighborhood Coding (TNC). The framework uses a debiased contrastive objective, where the encoding space is partitioned into a neighborhood, and each neighborhood is represented as a distribution of non-neighboring signals. The paper shows that the proposed framework is able to learn time series representations that are invariant to changes in the distribution of neighboring signals. Experiments on clustering and classification tasks on multiple datasets show that the method outperforms other unsupervised representation learning approaches, and is particularly useful in the medical field where labeling data is scarce."
1935,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"Conditional computation CONJUNCTION modular networks. modular networks CONJUNCTION Conditional computation. Conditional computation USED-FOR multitask learning and other problems. modular networks USED-FOR multitask learning and other problems. fully - differentiable approach USED-FOR modular networks. modules USED-FOR knowledge transfer. knowledge transfer USED-FOR tasks. soft weight sharing USED-FOR tasks. transfer learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION transfer learning. method USED-FOR self - organization of modules. transfer learning HYPONYM-OF tasks. multi - task learning CONJUNCTION transfer learning. transfer learning CONJUNCTION multi - task learning. domain adaptation HYPONYM-OF tasks. self - organization of modules USED-FOR multi - task learning. tasks EVALUATE-FOR method. it USED-FOR unsupervised multi - source domain adaptation. architectures USED-FOR image classification tasks. accuracy EVALUATE-FOR architectures. approach USED-FOR architectures. it USED-FOR adaptation. computation order USED-FOR modules. accuracy EVALUATE-FOR approach. order of pretrained modules USED-FOR adaptation. IMAGENET HYPONYM-OF image classification tasks. Task is problem solving. OtherScientificTerm are order of computation, and parameter increase. ","Conditional computation and modular networks have been used for multitask learning and other problems. This paper proposes a fully-differentiable approach to learn modular networks that can be used for problem solving. Conditional computation is used in multi-task learning, modular networks, and knowledge transfer for tasks that require soft weight sharing. The proposed method is able to learn self-organization of modules for self-organized multi-source domain adaptation, and it can also be applied for unsupervised multi- source domain adaptation. The authors show that their approach can learn architectures that achieve high accuracy on image classification tasks (e.g. IMAGENET) and multi-target learning, transfer learning, and domain adaptation and that it can be applied to adaptation based on the order of pretrained modules. They also show that the computation order of the modules can be changed to improve the accuracy of the proposed approach.   ","Conditional computation and modular networks have been used for multitask learning and other problems. This paper proposes a fully-differentiable approach to learn modular networks that can be used for problem solving. Conditional computation is used in multi-task learning, modular networks, and knowledge transfer for tasks that require soft weight sharing. The proposed method is able to learn self-organization of modules for self-organized multi-source domain adaptation, and it can also be applied for unsupervised multi- source domain adaptation. The authors show that their approach can learn architectures that achieve high accuracy on image classification tasks (e.g. IMAGENET) and multi-target learning, transfer learning, and domain adaptation and that it can be applied to adaptation based on the order of pretrained modules. They also show that the computation order of the modules can be changed to improve the accuracy of the proposed approach.   "
1944,SP:cae669c631e11fe703bf6cb511404866b19f474a,"local optima FEATURE-OF objective function. hyperparameter USED-FOR data variance. hyperparameter USED-FOR local optima. variance parameter USED-FOR VAE. variance parameter USED-FOR smoothness. gradient FEATURE-OF smoothness. It USED-FOR regularization. variance parameter USED-FOR It. variance parameter USED-FOR regularization. Fréchet inception distance ( FID ) EVALUATE-FOR Generation models. MNIST and CelebA datasets USED-FOR Fréchet inception distance ( FID ) of images. objectives USED-FOR Generation models. Method are Variational autoencoders ( VAEs ), and AR - ELBO. OtherScientificTerm are posterior collapse, latent space, oversmoothness, and linear approximated objective function. Generic are parameter, and model. ","This paper studies the problem of posterior collapse in Variational autoencoders (VAEs). The authors propose a new hyperparameter to control the data variance, which they call AR-ELBO, to prevent posterior collapse. They show that this parameter is a local optima of the objective function. It is used as a regularization for the variance parameter of a VAE, which is used to improve the smoothness of the gradient. Generation models trained with these objectives achieve the Fréchet inception distance (FID) of images on MNIST and CelebA datasets. The authors also show that the oversmoothness is due to a linear approximated objective function, which can be explained as a function of the variance in the latent space. ","This paper studies the problem of posterior collapse in Variational autoencoders (VAEs). The authors propose a new hyperparameter to control the data variance, which they call AR-ELBO, to prevent posterior collapse. They show that this parameter is a local optima of the objective function. It is used as a regularization for the variance parameter of a VAE, which is used to improve the smoothness of the gradient. Generation models trained with these objectives achieve the Fréchet inception distance (FID) of images on MNIST and CelebA datasets. The authors also show that the oversmoothness is due to a linear approximated objective function, which can be explained as a function of the variance in the latent space. "
1953,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,non i.i.d. variational autoencoders USED-FOR global dependencies. non i.i.d. variational autoencoders USED-FOR deep generative model. semi - supervised alternatives USED-FOR global modeling. mixture model CONJUNCTION global Gaussian latent variable. global Gaussian latent variable CONJUNCTION mixture model. global modeling USED-FOR deep generative models. semi - supervised alternatives USED-FOR deep generative models. semi - supervised alternatives COMPARE approach. approach COMPARE semi - supervised alternatives. local or data - dependent space FEATURE-OF mixture model. mixture model PART-OF approach. global Gaussian latent variable PART-OF approach. induced latent global space USED-FOR interpretable disentangled representations. user - defined regularization FEATURE-OF evidence lower bound. domain alignment USED-FOR model. shared attributes CONJUNCTION defined sequences of digits images. defined sequences of digits images CONJUNCTION shared attributes. face images CONJUNCTION defined sequences of digits images. defined sequences of digits images CONJUNCTION face images. shared attributes FEATURE-OF face images. face images HYPONYM-OF non - trivial underlying structures. Method is beta - VAE. OtherScientificTerm is global space. ,"This paper proposes a new deep generative model based on non i.i.d. variational autoencoders to capture global dependencies between samples. Unlike previous semi-supervised alternatives for global modeling, the proposed approach combines a mixture model in a local or data-dependent space with a global Gaussian latent variable. The proposed approach is based on beta-VAE, and the authors show that the induced latent global space can be used to learn interpretable disentangled representations. The authors also provide an evidence lower bound with user-defined regularization. The model is trained using domain alignment, and is shown to be able to disentangle non-trivial underlying structures (e.g., face images with shared attributes, defined sequences of digits images, etc.). ","This paper proposes a new deep generative model based on non i.i.d. variational autoencoders to capture global dependencies between samples. Unlike previous semi-supervised alternatives for global modeling, the proposed approach combines a mixture model in a local or data-dependent space with a global Gaussian latent variable. The proposed approach is based on beta-VAE, and the authors show that the induced latent global space can be used to learn interpretable disentangled representations. The authors also provide an evidence lower bound with user-defined regularization. The model is trained using domain alignment, and is shown to be able to disentangle non-trivial underlying structures (e.g., face images with shared attributes, defined sequences of digits images, etc.). "
1962,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"images CONJUNCTION videos. videos CONJUNCTION images. images HYPONYM-OF visual data. videos HYPONYM-OF visual data. visual data USED-FOR representation learning approaches. representations COMPARE visualonly representations. visualonly representations COMPARE representations. human interaction and attention cues USED-FOR approach. body part movements CONJUNCTION gaze. gaze CONJUNCTION body part movements. gaze FEATURE-OF human interactions. human interactions FEATURE-OF dataset. body part movements FEATURE-OF human interactions. gaze FEATURE-OF dataset. body part movements FEATURE-OF dataset. dynamics prediction ( physics ) CONJUNCTION walkable surface estimation ( affordance ). walkable surface estimation ( affordance ) CONJUNCTION dynamics prediction ( physics ). scene classification ( semantic ) CONJUNCTION action recognition ( temporal ). action recognition ( temporal ) CONJUNCTION scene classification ( semantic ). action recognition ( temporal ) CONJUNCTION depth estimation ( geometric ). depth estimation ( geometric ) CONJUNCTION action recognition ( temporal ). depth estimation ( geometric ) CONJUNCTION dynamics prediction ( physics ). dynamics prediction ( physics ) CONJUNCTION depth estimation ( geometric ). muscly - supervised ” representation USED-FOR interaction and attention cues. muscly - supervised ” representation USED-FOR target tasks. muscly - supervised ” representation COMPARE MoCo. MoCo COMPARE muscly - supervised ” representation. walkable surface estimation ( affordance ) HYPONYM-OF target tasks. scene classification ( semantic ) HYPONYM-OF target tasks. dynamics prediction ( physics ) HYPONYM-OF target tasks. depth estimation ( geometric ) HYPONYM-OF target tasks. action recognition ( temporal ) HYPONYM-OF target tasks. human ’s interactions USED-FOR representation learning. cues USED-FOR visual embedding. representation COMPARE self - supervised vision - only techniques. self - supervised vision - only techniques COMPARE representation. Task are representations of visual data, and computer vision. OtherScientificTerm is first person observations. ","This paper proposes a new approach to learning representations of visual data from human interaction and attention cues. The paper presents a dataset of human interactions (body part movements, gaze, etc.) from visual data (images and videos). The paper shows that the representations learned by the proposed approach are more interpretable than visualonly representations. The authors propose a “muscly-supervised” representation that is able to capture the interactions between humans and first person observations. The proposed approach is evaluated on a variety of different representation learning approaches using visual data, including images, videos, etc.   The authors show that the proposed “mockingly supervised” (MoCo) representation can capture the interaction and attentions more accurately than MoCo, and that the “mutually supervised’s” representations are able to generalize to more complex tasks.  The paper also shows that this representation can be used to improve the performance of a number of target tasks (depth estimation (geometric), dynamics prediction (physics), walkable surface estimation (affordance), scene classification (semantic), action recognition (temporal), etc.). The paper further shows that learning from human‘s interactions can be a useful tool for representation learning. The main contribution of the paper is that the authors propose to use these cues to learn a visual embedding that is interpretable and interpretable. They also show that their representation outperforms other self-supervision-only techniques and that do not rely on these cues.","This paper proposes a new approach to learning representations of visual data from human interaction and attention cues. The paper presents a dataset of human interactions (body part movements, gaze, etc.) from visual data (images and videos). The paper shows that the representations learned by the proposed approach are more interpretable than visualonly representations. The authors propose a “muscly-supervised” representation that is able to capture the interactions between humans and first person observations. The proposed approach is evaluated on a variety of different representation learning approaches using visual data, including images, videos, etc.   The authors show that the proposed “mockingly supervised” (MoCo) representation can capture the interaction and attentions more accurately than MoCo, and that the “mutually supervised’s” representations are able to generalize to more complex tasks.  The paper also shows that this representation can be used to improve the performance of a number of target tasks (depth estimation (geometric), dynamics prediction (physics), walkable surface estimation (affordance), scene classification (semantic), action recognition (temporal), etc.). The paper further shows that learning from human‘s interactions can be a useful tool for representation learning. The main contribution of the paper is that the authors propose to use these cues to learn a visual embedding that is interpretable and interpretable. They also show that their representation outperforms other self-supervision-only techniques and that do not rely on these cues."
1971,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"pretrained model COMPARE model. model COMPARE pretrained model. generalization EVALUATE-FOR model. generalization EVALUATE-FOR pretrained model. learning rate USED-FOR pretraining. neural network training CONJUNCTION pretraining. pretraining CONJUNCTION neural network training. OtherScientificTerm are Negative pretraining, negative pretraining effect, learning process, learning task - level, discretization of data distribution, model - level, negative pretraining effects, and negative pretraining. Method is neural networks. Generic is interventions. ","This paper studies the effect of negative pretraining on the generalization performance of neural networks. Negative pretraining is a phenomenon where a pretrained model is more likely to have a worse generalization than a model that is not pretrained at all. The authors show that this phenomenon occurs when the learning rate used during pretraining changes during the learning process, and that this change in learning rate can be attributed to the negative pre-training effect. They also show that the negative training effect can be controlled by changing the learning task-level (e.g. discretization of data distribution) and the model-level, and they show that a pre-trained model with the same learning rate has better generalization compared to a different model with a different learning rate. They further show that negative pretrainings are not always due to the same factors as in the case of neural network training and pretraining, but rather to different factors. Finally, the authors propose two interventions to mitigate the effects.","This paper studies the effect of negative pretraining on the generalization performance of neural networks. Negative pretraining is a phenomenon where a pretrained model is more likely to have a worse generalization than a model that is not pretrained at all. The authors show that this phenomenon occurs when the learning rate used during pretraining changes during the learning process, and that this change in learning rate can be attributed to the negative pre-training effect. They also show that the negative training effect can be controlled by changing the learning task-level (e.g. discretization of data distribution) and the model-level, and they show that a pre-trained model with the same learning rate has better generalization compared to a different model with a different learning rate. They further show that negative pretrainings are not always due to the same factors as in the case of neural network training and pretraining, but rather to different factors. Finally, the authors propose two interventions to mitigate the effects."
1980,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,"adversarially perturbed inputs FEATURE-OF classifier ’s robustness. bi - level optimization algorithm USED-FOR adversarially trained classifiers. bi - level optimization algorithm USED-FOR safe spots. ImageNet datasets EVALUATE-FOR adversarially trained classifiers. they USED-FOR smoothed classifiers. empirical and certified robustness EVALUATE-FOR smoothed classifiers. empirical and certified robustness EVALUATE-FOR they. safe spot inducing model training scheme CONJUNCTION safe spot generation method. safe spot generation method CONJUNCTION safe spot inducing model training scheme. out - of - distribution detection algorithm USED-FOR near - distribution outliers. safe spot generation method USED-FOR out - of - distribution detection algorithm. Task is adversarial defense. Method are classifier, and classifiers. Material is natural images. OtherScientificTerm is adversarial attacks. ","This paper proposes a bi-level optimization algorithm to find safe spots for adversarially trained classifiers on ImageNet datasets. The idea is that the classifier’s robustness to adversarial perturbed inputs is a function of the number of times a classifier has been trained on a particular input, and that if the training data points are near-distribution out of the training distribution, then they are likely to be in the safe spots. The authors show that they can find such smoothed classifiers with both empirical and certified robustness. They also propose a safe spot inducing model training scheme and safe spot generation method to find out an out-of-sample detection algorithm to detect near-data points that are not in the training set.    The paper is well-written and well-motivated, and the idea of adversarial defense is interesting. However, there are a few issues that prevent me from recommending acceptance of this paper. First, it is not clear to me that there is a clear connection between natural images and adversarial attacks. Second, the authors do not provide any theoretical analysis to support their claims. Third, there is no discussion of how the classifiers are trained and how they are trained.","This paper proposes a bi-level optimization algorithm to find safe spots for adversarially trained classifiers on ImageNet datasets. The idea is that the classifier’s robustness to adversarial perturbed inputs is a function of the number of times a classifier has been trained on a particular input, and that if the training data points are near-distribution out of the training distribution, then they are likely to be in the safe spots. The authors show that they can find such smoothed classifiers with both empirical and certified robustness. They also propose a safe spot inducing model training scheme and safe spot generation method to find out an out-of-sample detection algorithm to detect near-data points that are not in the training set.    The paper is well-written and well-motivated, and the idea of adversarial defense is interesting. However, there are a few issues that prevent me from recommending acceptance of this paper. First, it is not clear to me that there is a clear connection between natural images and adversarial attacks. Second, the authors do not provide any theoretical analysis to support their claims. Third, there is no discussion of how the classifiers are trained and how they are trained."
1989,SP:1350ab543b6a5cf579827835fb27011751cc047f,"regularities CONJUNCTION order. order CONJUNCTION regularities. regularities FEATURE-OF temporal dimension. order FEATURE-OF temporal dimension. grid based convolutions USED-FOR video processing. point spatio - temporal ( PST ) convolution USED-FOR informative representations of point cloud sequences. PST convolution USED-FOR point cloud sequences. temporal convolution USED-FOR dynamics of the spatial regions. spatial convolution USED-FOR local structure. time dimension FEATURE-OF dynamics of the spatial regions. deep network USED-FOR features of point cloud sequences. PST convolution PART-OF deep network. PST convolution USED-FOR features of point cloud sequences. hierarchical manner USED-FOR point cloud sequences. PSTNet HYPONYM-OF deep network. PSTNet USED-FOR point cloud sequences. 3D action recognition CONJUNCTION 4D semantic segmentation datasets. 4D semantic segmentation datasets CONJUNCTION 3D action recognition. 4D semantic segmentation datasets EVALUATE-FOR PSTNet. Material is Point cloud sequences. OtherScientificTerm are spatial dimension, and 3D space. ","This paper proposes a novel point spatio-temporal (PST) convolution for point cloud sequences. Point cloud sequences have a temporal dimension that depends on the time and spatial dimension. The authors propose to learn informative representations in 3D space via point spatiotemporal (PSP) convolutions, which is a generalization of grid based convolutions for video processing. PST convolution is used to learn the dynamics of the spatial regions of a point cloud sequence using a temporal convolution with regularities on the temporal dimension and order. The spatial convolution learns to capture the local structure in the time dimension, and is used in a hierarchical manner. The proposed deep network, called PSTNet, is a deep network that combines the features of point cloud samples from different regions of the time domain with the features from different spatial regions. PSTNet is evaluated on 3D action recognition and 4D semantic segmentation datasets and achieves state-of-the-art performance. ","This paper proposes a novel point spatio-temporal (PST) convolution for point cloud sequences. Point cloud sequences have a temporal dimension that depends on the time and spatial dimension. The authors propose to learn informative representations in 3D space via point spatiotemporal (PSP) convolutions, which is a generalization of grid based convolutions for video processing. PST convolution is used to learn the dynamics of the spatial regions of a point cloud sequence using a temporal convolution with regularities on the temporal dimension and order. The spatial convolution learns to capture the local structure in the time dimension, and is used in a hierarchical manner. The proposed deep network, called PSTNet, is a deep network that combines the features of point cloud samples from different regions of the time domain with the features from different spatial regions. PSTNet is evaluated on 3D action recognition and 4D semantic segmentation datasets and achieves state-of-the-art performance. "
1998,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"source TTS model USED-FOR personal voice. Custom voice HYPONYM-OF text to speech ( TTS ) service. text to speech ( TTS ) service PART-OF commercial speech platforms. Custom voice USED-FOR TTS adaptation. adaptive TTS system USED-FOR customization of new voices. AdaSpeech HYPONYM-OF adaptive TTS system. utterance and phoneme level FEATURE-OF acoustic information. acoustic encoder USED-FOR utterance - level vector. acoustic predictor USED-FOR phonemelevel vectors. one USED-FOR phoneme - level vectors. acoustic encoder CONJUNCTION one. one CONJUNCTION acoustic encoder. acoustic encoder USED-FOR phoneme - level vectors. utterance - level vector USED-FOR inference. adaptation parameters CONJUNCTION voice quality. voice quality CONJUNCTION adaptation parameters. speaker embedding USED-FOR adaptation. mel - spectrogram decoder PART-OF AdaSpeech. part CONJUNCTION speaker embedding. speaker embedding CONJUNCTION part. conditional layer normalization USED-FOR mel - spectrogram decoder. conditional layer normalization PART-OF AdaSpeech. acoustic conditions PART-OF LibriTTS. acoustic conditions FEATURE-OF VCTK and LJSpeech datasets. LibriTTS datasets USED-FOR source TTS model. VCTK and LJSpeech datasets USED-FOR it. adaptation data USED-FOR it. AdaSpeech COMPARE baseline methods. baseline methods COMPARE AdaSpeech. adaptation quality EVALUATE-FOR baseline methods. AdaSpeech USED-FOR custom voice. adaptation quality EVALUATE-FOR AdaSpeech. Method is adaptation model. Material are source speech data, and audio samples. OtherScientificTerm is memory usage. ","This paper proposes AdaSpeech, a text to speech (TTS) model that adapts a source TTS model to a personal voice. Custom voice is a text-to-speech (TTTS) service that is widely used in commercial speech platforms. The authors propose an adaptive TTS system, called AdaSpech, that allows for the customization of new voices by adapting the TTS adaptation model to the source speech data. Specifically, the authors propose to use acoustic information at both utterance and phoneme level to encode the acoustic information in the source and target speech. The acoustic encoder and acoustic predictor are used to learn phonemelevel vectors, and the utterance-level vector is used for inference. A mel-spectrogram decoder is used to decouple the acoustic conditions of the source from the target speech, and a mel-speech decoder uses conditional layer normalization to normalize the output of the mel-sphere decoder. The adaptation parameters and voice quality are learned using speaker embedding, and it uses the VCTK and LJSpeech datasets with the same acoustic conditions as LibriTTS.   Experiments show that the adaptation model is able to adapt to a new source speech without any additional audio samples, and that it can adapt to the adaptation data in a way that does not require any additional adaptation data, and does not increase memory usage. In addition, the adaptation quality is improved over the baseline methods, with the authors also showing that the adaptor of the custom voice using AdaSpeak achieves better adaptation quality compared to the original source. ","This paper proposes AdaSpeech, a text to speech (TTS) model that adapts a source TTS model to a personal voice. Custom voice is a text-to-speech (TTTS) service that is widely used in commercial speech platforms. The authors propose an adaptive TTS system, called AdaSpech, that allows for the customization of new voices by adapting the TTS adaptation model to the source speech data. Specifically, the authors propose to use acoustic information at both utterance and phoneme level to encode the acoustic information in the source and target speech. The acoustic encoder and acoustic predictor are used to learn phonemelevel vectors, and the utterance-level vector is used for inference. A mel-spectrogram decoder is used to decouple the acoustic conditions of the source from the target speech, and a mel-speech decoder uses conditional layer normalization to normalize the output of the mel-sphere decoder. The adaptation parameters and voice quality are learned using speaker embedding, and it uses the VCTK and LJSpeech datasets with the same acoustic conditions as LibriTTS.   Experiments show that the adaptation model is able to adapt to a new source speech without any additional audio samples, and that it can adapt to the adaptation data in a way that does not require any additional adaptation data, and does not increase memory usage. In addition, the adaptation quality is improved over the baseline methods, with the authors also showing that the adaptor of the custom voice using AdaSpeak achieves better adaptation quality compared to the original source. "
2007,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,sparse networks COMPARE dense neural architectures. dense neural architectures COMPARE sparse networks. regularizers USED-FOR dense networks. activation functions CONJUNCTION regularizers. regularizers CONJUNCTION activation functions. optimizers CONJUNCTION activation functions. activation functions CONJUNCTION optimizers. activation functions USED-FOR dense networks. regularizers USED-FOR sparse networks. gradient flow USED-FOR sparse networks. training regime USED-FOR gradient flow. tailoring optimization USED-FOR sparse networks. OtherScientificTerm is initialization. Task is training sparse networks. ,"This paper shows that sparse networks are more robust to regularizers than dense neural architectures. The authors show that the gradient flow of sparse networks in the training regime is a function of the initialization, the optimizers, the activation functions, and the regularizers used to train dense networks. They also show that training sparse networks is more efficient than training dense networks with different activation functions and regularizers. Finally, they show that by tailoring optimization to sparse networks, they can achieve better performance.","This paper shows that sparse networks are more robust to regularizers than dense neural architectures. The authors show that the gradient flow of sparse networks in the training regime is a function of the initialization, the optimizers, the activation functions, and the regularizers used to train dense networks. They also show that training sparse networks is more efficient than training dense networks with different activation functions and regularizers. Finally, they show that by tailoring optimization to sparse networks, they can achieve better performance."
2016,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"Graph Convolutional Neural Networks ( GCN ) HYPONYM-OF message passing algorithms. Label Propagation ( LPA ) CONJUNCTION Graph Convolutional Neural Networks ( GCN ). Graph Convolutional Neural Networks ( GCN ) CONJUNCTION Label Propagation ( LPA ). Label Propagation ( LPA ) HYPONYM-OF message passing algorithms. graphs USED-FOR message passing algorithms. GCN USED-FOR node feature information. LPA USED-FOR node label information. LPA CONJUNCTION GCN. GCN CONJUNCTION LPA. LPA CONJUNCTION GCN. GCN CONJUNCTION LPA. LPA USED-FOR node classification. GCN CONJUNCTION LPA. LPA CONJUNCTION GCN. end - to - end model USED-FOR node classification. GCN USED-FOR node classification. GCN PART-OF end - to - end model. LPA PART-OF end - to - end model. LPA USED-FOR GCN. LPA USED-FOR regularization. model COMPARE feature - based attention models. feature - based attention models COMPARE model. attention weights USED-FOR model. node labels USED-FOR attention weights. real - world graphs EVALUATE-FOR model. model COMPARE GCN - based methods. GCN - based methods COMPARE model. real - world graphs EVALUATE-FOR GCN - based methods. node classification accuracy EVALUATE-FOR GCN - based methods. node classification accuracy EVALUATE-FOR model. OtherScientificTerm are edges of the graph, feature / label, feature / label influence, and edge weights. Task are feature / label smoothing, and classification. Method is unified model. ","Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are popular message passing algorithms on graphs. In this paper, the authors propose an end-to-end model that combines LPA, GCN, and LPA for node label information and node classification. LPA is used as a regularization, while GCN is used to preserve node feature information. The proposed model is evaluated on real-world graphs and shows superior node classification accuracy compared to other GCN-based methods. The authors also show that the proposed model outperforms feature-based attention models in terms of attention weights based on node labels.    The main contribution of this paper is that it proposes a unified model for node classification that can be applied to all edges of the graph. The idea is to use feature/label smoothing to ensure that each node has the same influence on the final classification, and that the feature /label influence is shared across all nodes. The paper also shows that the attention weights of the model do not depend on the number of edges in the graph, but only on the influence of the edge weights. ","Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are popular message passing algorithms on graphs. In this paper, the authors propose an end-to-end model that combines LPA, GCN, and LPA for node label information and node classification. LPA is used as a regularization, while GCN is used to preserve node feature information. The proposed model is evaluated on real-world graphs and shows superior node classification accuracy compared to other GCN-based methods. The authors also show that the proposed model outperforms feature-based attention models in terms of attention weights based on node labels.    The main contribution of this paper is that it proposes a unified model for node classification that can be applied to all edges of the graph. The idea is to use feature/label smoothing to ensure that each node has the same influence on the final classification, and that the feature /label influence is shared across all nodes. The paper also shows that the attention weights of the model do not depend on the number of edges in the graph, but only on the influence of the edge weights. "
2025,SP:c5883e3a59e6575eff044251b38175a6ed024034,"VC - dimension CONJUNCTION Rademacher complexity ( R - Complexity ). Rademacher complexity ( R - Complexity ) CONJUNCTION VC - dimension. complexity EVALUATE-FOR classifier ’s function space. complexity FEATURE-OF generalization gap. generalization gap EVALUATE-FOR classifier ’s function space. classifier CONJUNCTION generator function spaces. generator function spaces CONJUNCTION classifier. R - Complexity EVALUATE-FOR generator function spaces. R - Complexity EVALUATE-FOR classifier. generalization performance EVALUATE-FOR generator space. invariances CONJUNCTION local smoothness. local smoothness CONJUNCTION invariances. generator space USED-FOR constraints. invariances HYPONYM-OF constraints. local smoothness HYPONYM-OF constraints. classifier CONJUNCTION generator. generator CONJUNCTION classifier. invariance co - complexity term CONJUNCTION dissociation co - complexity term. dissociation co - complexity term CONJUNCTION invariance co - complexity term. classifier USED-FOR generator. invariant transformations FEATURE-OF generator. invariance co - complexity term PART-OF It. invariant transformations FEATURE-OF classifier. dissociation co - complexity term PART-OF It. invariance co - complexity FEATURE-OF classifier. CNN architecture CONJUNCTION transformation - equivariant extensions. transformation - equivariant extensions CONJUNCTION CNN architecture. Co - complexity USED-FOR classifiers. Metric are generalization error bounds, generalization error, co - complexity, dissociation co - complexity, and training error. OtherScientificTerm are ground truth label generating function ( LGF ), LGF, ground truth labels, and function space. Generic is it. ","This paper studies the generalization error bounds of the ground truth label generating function (LGF). The authors show that the VC-dimension and Rademacher complexity (R-Complexity) of the classifier’s function space are related to the complexity of a classifier's function space, and that the R-complexity of a given classifier and generator function spaces is a function of both the number of ground truth labels and the complexity in the function space. The authors also show that a generalization gap of $O(\sqrt{n\epsilon})$ for any classifier with R-complexity of $\mathcal{O}(n\log n)$ is upper bounded by $O(n^2)$, where $n$ is the size of the training set.    The generalization performance of the generator space is also studied, and it is shown that the generalisation performance of a generator function space can be improved by introducing constraints such as invariances and local smoothness.  It is also shown that for a certain CNN architecture and transformation-equivariant extensions, the co-computation between the invariance of the learned classifier to invariant transformations in the generator and the dissociation of the generated classifier from the original classifier can be reduced to $\log n$. Co-comcomplexity can be used to improve the performance of classifiers in a variety of settings.  The paper also shows that the training error of a ground truth classifier is bounded by a co-consistency term, which is a measure of the distance between the classifiers' function space and the generator's space. This term is defined as the distance from the ground-truth label of a point in the space to the point in which the generator has been trained on. The paper shows that this co-compassibility term is independent of $n$.  ","This paper studies the generalization error bounds of the ground truth label generating function (LGF). The authors show that the VC-dimension and Rademacher complexity (R-Complexity) of the classifier’s function space are related to the complexity of a classifier's function space, and that the R-complexity of a given classifier and generator function spaces is a function of both the number of ground truth labels and the complexity in the function space. The authors also show that a generalization gap of $O(\sqrt{n\epsilon})$ for any classifier with R-complexity of $\mathcal{O}(n\log n)$ is upper bounded by $O(n^2)$, where $n$ is the size of the training set.    The generalization performance of the generator space is also studied, and it is shown that the generalisation performance of a generator function space can be improved by introducing constraints such as invariances and local smoothness.  It is also shown that for a certain CNN architecture and transformation-equivariant extensions, the co-computation between the invariance of the learned classifier to invariant transformations in the generator and the dissociation of the generated classifier from the original classifier can be reduced to $\log n$. Co-comcomplexity can be used to improve the performance of classifiers in a variety of settings.  The paper also shows that the training error of a ground truth classifier is bounded by a co-consistency term, which is a measure of the distance between the classifiers' function space and the generator's space. This term is defined as the distance from the ground-truth label of a point in the space to the point in which the generator has been trained on. The paper shows that this co-compassibility term is independent of $n$.  "
2034,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"neural network quantization USED-FOR task. end - toend retraining USED-FOR neural network quantization. Post - training Quantization ( PTQ ) HYPONYM-OF neural network quantization. PTQ USED-FOR quantized models. PTQ COMPARE Quantization - Aware Training ( QAT ). Quantization - Aware Training ( QAT ) COMPARE PTQ. Quantization - Aware Training ( QAT ) USED-FOR quantized models. bitwidth FEATURE-OF PTQ. BRECQ HYPONYM-OF PTQ framework. neural networks USED-FOR BRECQ. crosslayer dependency CONJUNCTION generalization error. generalization error CONJUNCTION crosslayer dependency. crosslayer dependency EVALUATE-FOR BRECQ. generalization error EVALUATE-FOR BRECQ. mixed precision technique PART-OF framework. handcrafted and searched neural architectures USED-FOR image classification and object detection tasks. 4 - bit ResNet CONJUNCTION MobileNetV2. MobileNetV2 CONJUNCTION 4 - bit ResNet. PTQ COMPARE QAT. QAT COMPARE PTQ. MobileNetV2 COMPARE QAT. QAT COMPARE MobileNetV2. MobileNetV2 EVALUATE-FOR PTQ. 4 - bit ResNet EVALUATE-FOR PTQ. OtherScientificTerm are INT2, quantization, and inter - layer and intra - layer sensitivity. Metric is second - order error. ","This paper proposes Post-training Quantization (PTQ), a method for post-training quantization for neural network quantization on a new task. PTQ is based on end-to-end retraining, where each layer of the network is trained with INT2. The authors show that PTQ outperforms Quantization-Aware Training (QAT) for training quantized models, and that the second-order error of PTQ depends on the bitwidth of the quantized model.  The authors propose a new PTQ framework called BRECQ, where the neural networks are trained with a mixed-precision technique, and the crosslayer dependency and generalization error of the model are considered. The proposed framework incorporates a mixed precision technique, which allows the quantization to be applied to both inter-layer and intra-layer sensitivity. Experiments are conducted on image classification and object detection tasks with handcrafted and searched neural architectures, and on 4-bit ResNet and MobileNetV2, where PTQ performs better than QAT. ","This paper proposes Post-training Quantization (PTQ), a method for post-training quantization for neural network quantization on a new task. PTQ is based on end-to-end retraining, where each layer of the network is trained with INT2. The authors show that PTQ outperforms Quantization-Aware Training (QAT) for training quantized models, and that the second-order error of PTQ depends on the bitwidth of the quantized model.  The authors propose a new PTQ framework called BRECQ, where the neural networks are trained with a mixed-precision technique, and the crosslayer dependency and generalization error of the model are considered. The proposed framework incorporates a mixed precision technique, which allows the quantization to be applied to both inter-layer and intra-layer sensitivity. Experiments are conducted on image classification and object detection tasks with handcrafted and searched neural architectures, and on 4-bit ResNet and MobileNetV2, where PTQ performs better than QAT. "
2043,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"large labeled datasets USED-FOR deep learning deployment. distributional shift CONJUNCTION labeling cost. labeling cost CONJUNCTION distributional shift. medicine HYPONYM-OF real - world applications. dataset properties USED-FOR calibration. dataset properties COMPARE architecture. architecture COMPARE dataset properties. common strategies USED-FOR class imbalance. dataset properties USED-FOR calibration. dataset curation USED-FOR calibration. Method are Neural networks, and neural networks. Metric are downstream prediction accuracy, accuracy, and calibration error. OtherScientificTerm are model uncertainty, label quality, label noise, small dataset sizes, and network expressivity. Generic is complementary approach. Task is dataset imbalance. ","This paper studies the problem of calibration of deep learning deployment on large labeled datasets. Neural networks are trained on a large number of labeled datasets, and the goal is to achieve a downstream prediction accuracy that is at least as good as the accuracy on the original dataset. This is a challenging problem due to distributional shift, labeling cost, and model uncertainty. In real-world applications, such as medicine, this is an important problem.    This paper proposes a complementary approach to mitigate the issue of dataset imbalance. The idea is to use dataset curation as a way to calibrate the accuracy of neural networks. The authors show that common strategies to mitigate class imbalance (i.e., label quality vs. label noise) are not effective when there are small dataset sizes. They also show that dataset properties (e.g., architecture) are more important for calibration than dataset properties for calibration, and that the calibration error is lower when the dataset curations are more sensitive to the network expressivity. ","This paper studies the problem of calibration of deep learning deployment on large labeled datasets. Neural networks are trained on a large number of labeled datasets, and the goal is to achieve a downstream prediction accuracy that is at least as good as the accuracy on the original dataset. This is a challenging problem due to distributional shift, labeling cost, and model uncertainty. In real-world applications, such as medicine, this is an important problem.    This paper proposes a complementary approach to mitigate the issue of dataset imbalance. The idea is to use dataset curation as a way to calibrate the accuracy of neural networks. The authors show that common strategies to mitigate class imbalance (i.e., label quality vs. label noise) are not effective when there are small dataset sizes. They also show that dataset properties (e.g., architecture) are more important for calibration than dataset properties for calibration, and that the calibration error is lower when the dataset curations are more sensitive to the network expressivity. "
2052,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"information exchange CONJUNCTION cooperation. cooperation CONJUNCTION information exchange. Effective communication USED-FOR information exchange. Effective communication USED-FOR cooperation. symbolic channels USED-FOR emergent communication. 3D environment FEATURE-OF joints. non - uniform distribution of intents CONJUNCTION commonknowledge energy cost. commonknowledge energy cost CONJUNCTION non - uniform distribution of intents. agents USED-FOR protocols. OtherScientificTerm are discrete cheap - talk channels, and latent feature. Method is emergent protocols. Generic is modality. Material is training curricula. ","This paper studies the problem of emergent communication via symbolic channels. Effective communication is an important technique for information exchange and cooperation. The paper proposes to use discrete cheap-talk channels, where each agent has access to a latent feature of the other agent’s intent, and the goal is to establish emergent protocols. The idea is to use a non-uniform distribution of intents and a commonknowledge energy cost between agents to establish protocols. Experiments are conducted on joints in a 3D environment, where the modality is differentiable and the agents are able to communicate across different joints. The training curricula is well-motivated and the experiments are well-designed. ","This paper studies the problem of emergent communication via symbolic channels. Effective communication is an important technique for information exchange and cooperation. The paper proposes to use discrete cheap-talk channels, where each agent has access to a latent feature of the other agent’s intent, and the goal is to establish emergent protocols. The idea is to use a non-uniform distribution of intents and a commonknowledge energy cost between agents to establish protocols. Experiments are conducted on joints in a 3D environment, where the modality is differentiable and the agents are able to communicate across different joints. The training curricula is well-motivated and the experiments are well-designed. "
2061,SP:5ba686e2eef369fa49b10ba3f41f102740836859,Generating high quality uncertainty estimates USED-FOR sequential regression. deep recurrent networks HYPONYM-OF sequential regression. real world non - stationary signals CONJUNCTION drift. drift CONJUNCTION real world non - stationary signals. method USED-FOR symmetric and asymmetric uncertainty estimates. method COMPARE baselines. baselines COMPARE method. drift and non drift scenarios EVALUATE-FOR baselines. sequential regression USED-FOR real - world applications. modeling toolbox USED-FOR sequential uncertainty quantification. Generic is approaches. OtherScientificTerm is stationarity. ,"This paper proposes a new method for generating high quality uncertainty estimates for sequential regression (i.e. deep recurrent networks). The authors show that the proposed method can be used to obtain symmetric and asymmetric uncertainty estimates. The proposed method outperforms several baselines in both the drift and non drift scenarios. The authors also show that their method is applicable to real-world applications where real world non-stationary signals and/or drift are present, and that existing approaches do not account for the stationarity of the data. Finally, the authors propose a new modeling toolbox for sequential uncertainty quantification. ","This paper proposes a new method for generating high quality uncertainty estimates for sequential regression (i.e. deep recurrent networks). The authors show that the proposed method can be used to obtain symmetric and asymmetric uncertainty estimates. The proposed method outperforms several baselines in both the drift and non drift scenarios. The authors also show that their method is applicable to real-world applications where real world non-stationary signals and/or drift are present, and that existing approaches do not account for the stationarity of the data. Finally, the authors propose a new modeling toolbox for sequential uncertainty quantification. "
2070,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,metric measure spaces USED-FOR machine learning problems. metric space HYPONYM-OF Comparing metric measure spaces. probability distribution FEATURE-OF metric space. Gromov - Wasserstein ( GW ) distance HYPONYM-OF metric measure spaces. probability distribution FEATURE-OF metric measure spaces. distance CONJUNCTION upper - bounding relaxation. upper - bounding relaxation CONJUNCTION distance. upper - bounding relaxation HYPONYM-OF Unbalanced Gromov - Wasserstein formulations. distance HYPONYM-OF Unbalanced Gromov - Wasserstein formulations. They USED-FOR metric spaces. isometries FEATURE-OF positive measures. positive measures FEATURE-OF metric spaces. formulation USED-FOR positive and definite divergence. relaxation of the mass conservation constraint USED-FOR positive and definite divergence. quadratically - homogeneous divergence USED-FOR relaxation of the mass conservation constraint. quadratically - homogeneous divergence USED-FOR positive and definite divergence. entropic regularization approach USED-FOR large scale optimal transport problems. entropic regularization approach USED-FOR divergence. parallelizable and GPU - friendly iterative scheme USED-FOR non - convex optimization problem. distance FEATURE-OF mm - spaces. distance USED-FOR formulation. isometries FEATURE-OF distance. isometries FEATURE-OF mm - spaces. conic lifting USED-FOR distance. synthetic examples CONJUNCTION domain adaptation data. domain adaptation data CONJUNCTION synthetic examples. unbalanced divergence USED-FOR ML. domain adaptation data CONJUNCTION Positive - Unlabeled learning task. Positive - Unlabeled learning task CONJUNCTION domain adaptation data. synthetic examples CONJUNCTION Positive - Unlabeled learning task. Positive - Unlabeled learning task CONJUNCTION synthetic examples. Task is quadratic assignment problem. OtherScientificTerm is GW distance. ,"Comparing metric measure spaces for machine learning problems is a quadratic assignment problem. Comparing the probability distribution of the metric space (i.e., metric space) is a well-studied problem, and the Gromov-Wasserstein (GW) distance is one of the most commonly used metrics for this problem.    Unbalanced GromOVMs are a popular metric measure space, and have been widely used in the literature. They can be seen as a generalization of existing metric spaces, e.g., the standard GW distance.  This paper considers two variants of the GW distance: (1) the distance and upper-bounding relaxation, and (2) the number of positive measures.  The authors propose a new formulation for the positive and definite divergence based on the relaxation of the mass conservation constraint based on quadratically-homogeneous divergence. They also propose an entropic regularization approach for the divergence, which can be applied to large scale optimal transport problems. The authors also propose a parallelizable and GPU-friendly iterative scheme for solving the non-convex optimization problem. They show that the distance of mm-spaces with different isometries of the positive measures can be approximated by a simple formulation based on conic lifting.  Experiments are conducted on synthetic examples, domain adaptation data, a Positive-Unlabeled learning task, and a domain adaptation task. The results show that unbalanced divergence is a useful metric for ML. ","Comparing metric measure spaces for machine learning problems is a quadratic assignment problem. Comparing the probability distribution of the metric space (i.e., metric space) is a well-studied problem, and the Gromov-Wasserstein (GW) distance is one of the most commonly used metrics for this problem.    Unbalanced GromOVMs are a popular metric measure space, and have been widely used in the literature. They can be seen as a generalization of existing metric spaces, e.g., the standard GW distance.  This paper considers two variants of the GW distance: (1) the distance and upper-bounding relaxation, and (2) the number of positive measures.  The authors propose a new formulation for the positive and definite divergence based on the relaxation of the mass conservation constraint based on quadratically-homogeneous divergence. They also propose an entropic regularization approach for the divergence, which can be applied to large scale optimal transport problems. The authors also propose a parallelizable and GPU-friendly iterative scheme for solving the non-convex optimization problem. They show that the distance of mm-spaces with different isometries of the positive measures can be approximated by a simple formulation based on conic lifting.  Experiments are conducted on synthetic examples, domain adaptation data, a Positive-Unlabeled learning task, and a domain adaptation task. The results show that unbalanced divergence is a useful metric for ML. "
2079,SP:47dcefd5515e772f29e03219c01713e2403643ce,"computational cost CONJUNCTION memory consumption. memory consumption CONJUNCTION computational cost. Network pruning USED-FOR memory consumption. compression ratios EVALUATE-FOR saliencybased pruning. sparse parameters FEATURE-OF well - trainable networks. pruning method USED-FOR pruned networks. all - alive pruning ( AAP ) HYPONYM-OF pruning method. trainable weights USED-FOR pruned networks. AAP USED-FOR saliency - based pruning methods. AAP USED-FOR model architectures. saliency - based pruning methods CONJUNCTION model architectures. model architectures CONJUNCTION saliency - based pruning methods. one - shot pruning CONJUNCTION dynamic pruning. dynamic pruning CONJUNCTION one - shot pruning. iterative pruning CONJUNCTION one - shot pruning. one - shot pruning CONJUNCTION iterative pruning. pruning methods USED-FOR AAP. dynamic pruning USED-FOR AAP. accuracy EVALUATE-FOR AAP. benchmark datasets EVALUATE-FOR AAP. dynamic pruning HYPONYM-OF pruning methods. iterative pruning HYPONYM-OF pruning methods. one - shot pruning HYPONYM-OF pruning methods. Material is low - resource devices. Metric is accuracy loss. Method is network pruning. OtherScientificTerm are model capacity, and dead connections. ","Network pruning aims to reduce the computational cost and memory consumption, especially on low-resource devices. Network pruning has been shown to be effective in reducing the memory consumption and the accuracy loss. However, the compression ratios of saliencybased pruning can be very expensive, and well-trainable networks with sparse parameters tend to have sparse parameters. This paper proposes a new pruning method, called all-alive pruning (AAP), which can be applied to pruned networks with trainable weights. The authors show that the performance of AAP is comparable to saliency-based and iterative pruning methods on standard benchmark datasets. They also show that AAP can be used to prune different model architectures and prune the model capacity more efficiently. The paper also shows that the accuracy of AAP can improve when the number of dead connections is small.   The authors also provide a comprehensive comparison of different pruning algorithms, including iterative, one-shot pruning, dynamic, and dynamic pruning. The results show that a variety of network pruning techniques are effective. ","Network pruning aims to reduce the computational cost and memory consumption, especially on low-resource devices. Network pruning has been shown to be effective in reducing the memory consumption and the accuracy loss. However, the compression ratios of saliencybased pruning can be very expensive, and well-trainable networks with sparse parameters tend to have sparse parameters. This paper proposes a new pruning method, called all-alive pruning (AAP), which can be applied to pruned networks with trainable weights. The authors show that the performance of AAP is comparable to saliency-based and iterative pruning methods on standard benchmark datasets. They also show that AAP can be used to prune different model architectures and prune the model capacity more efficiently. The paper also shows that the accuracy of AAP can improve when the number of dead connections is small.   The authors also provide a comprehensive comparison of different pruning algorithms, including iterative, one-shot pruning, dynamic, and dynamic pruning. The results show that a variety of network pruning techniques are effective. "
2088,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"Generative Adversarial Net ( GAN ) USED-FOR latent space. approaches USED-FOR task. Generative Adversarial Net ( GAN ) USED-FOR latent - space transformations. latent space CONJUNCTION latent - space transformations. latent - space transformations CONJUNCTION latent space. Generative Adversarial Net ( GAN ) USED-FOR approaches. Generative Adversarial Net ( GAN ) USED-FOR task. global image identity CONJUNCTION diminished photo - realism. diminished photo - realism CONJUNCTION global image identity. attribute edits CONJUNCTION global image identity. global image identity CONJUNCTION attribute edits. content loss CONJUNCTION adversarial loss. adversarial loss CONJUNCTION content loss. maintenance of image identity CONJUNCTION photo - realism. photo - realism CONJUNCTION maintenance of image identity. attribute regression USED-FOR transformation functions. adversarial loss USED-FOR maintenance of image identity. content loss USED-FOR maintenance of image identity. quantitative evaluation strategies EVALUATE-FOR controllable editing. image identity CONJUNCTION realism. realism CONJUNCTION image identity. model USED-FOR singleand multipleattribute editing. model USED-FOR targeted image manipulation. natural and synthetic images EVALUATE-FOR model. Task are Controllable semantic image editing, and qualitative evaluation. OtherScientificTerm are image attributes, and multiple attribute transformations. ","Controllable semantic image editing is an important problem that is important for both qualitative evaluation and quantitative evaluation. This paper proposes two approaches to this task using Generative Adversarial Net (GAN) to learn latent space and latent-space transformations. The key idea is to use attribute regression to learn the transformation functions and use content loss and adversarial loss for the maintenance of image identity and photo-realism. Experiments show that attribute edits, global image identity, and diminished photo-risqueness can be controlled with controllable editing using two quantitative evaluation strategies. The model can be used for singleand multiple attribute editing, and is shown to be effective for targeted image manipulation on both natural and synthetic images.","Controllable semantic image editing is an important problem that is important for both qualitative evaluation and quantitative evaluation. This paper proposes two approaches to this task using Generative Adversarial Net (GAN) to learn latent space and latent-space transformations. The key idea is to use attribute regression to learn the transformation functions and use content loss and adversarial loss for the maintenance of image identity and photo-realism. Experiments show that attribute edits, global image identity, and diminished photo-risqueness can be controlled with controllable editing using two quantitative evaluation strategies. The model can be used for singleand multiple attribute editing, and is shown to be effective for targeted image manipulation on both natural and synthetic images."
2097,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"mode dropping CONJUNCTION unstable training. unstable training CONJUNCTION mode dropping. Generative Adversarial Networks ( GAN ) USED-FOR synthesizing sequences of discrete elements. unstable training HYPONYM-OF synthesizing sequences of discrete elements. mode dropping HYPONYM-OF synthesizing sequences of discrete elements. binary classifier PART-OF discriminator. Feature Statistics Alignment ( FSA ) paradigm USED-FOR fine - grained signals. latent high - dimensional representation space FEATURE-OF fine - grained signals. FSA USED-FOR mean statistics. finite - dimensional feature space FEATURE-OF real data. approach USED-FOR discrete sequence generation. synthetic and real benchmark datasets EVALUATE-FOR approach. quantitative evaluation EVALUATE-FOR approach. Gumbel - Softmax based GAN framework USED-FOR sequence generation. feature alignment regularization USED-FOR Gumbel - Softmax based GAN framework. OtherScientificTerm are learning signals, and binary classification feedback. Task is adversarial training. ","This paper proposes a novel approach for discrete sequence generation based on Generative Adversarial Networks (GAN) for synthesizing sequences of discrete elements (e.g. mode dropping, unstable training, etc.). The authors propose to use the Feature Statistics Alignment (FSA) paradigm to learn fine-grained signals in a latent high-dimensional representation space. The discriminator consists of a binary classifier and a discriminator that is trained to distinguish between learning signals that are generated by the discriminator and those generated by a binary classification feedback. The authors show that the mean statistics generated by FSA are more robust to adversarial training. They also show that real data in a finite-dimensional feature space can be generated using FSA. The proposed approach is evaluated on both synthetic and real benchmark datasets, and the proposed approach achieves state-of-the-art performance on both quantitative evaluation and qualitative evaluation.  The authors also propose a Gumbel-Softmax based GAN framework for sequence generation with feature alignment regularization. ","This paper proposes a novel approach for discrete sequence generation based on Generative Adversarial Networks (GAN) for synthesizing sequences of discrete elements (e.g. mode dropping, unstable training, etc.). The authors propose to use the Feature Statistics Alignment (FSA) paradigm to learn fine-grained signals in a latent high-dimensional representation space. The discriminator consists of a binary classifier and a discriminator that is trained to distinguish between learning signals that are generated by the discriminator and those generated by a binary classification feedback. The authors show that the mean statistics generated by FSA are more robust to adversarial training. They also show that real data in a finite-dimensional feature space can be generated using FSA. The proposed approach is evaluated on both synthetic and real benchmark datasets, and the proposed approach achieves state-of-the-art performance on both quantitative evaluation and qualitative evaluation.  The authors also propose a Gumbel-Softmax based GAN framework for sequence generation with feature alignment regularization. "
2106,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"progressive rewards FEATURE-OF reinforcement learning tasks. tasks HYPONYM-OF reinforcement learning tasks. Spectral DQN USED-FOR reward. it COMPARE benchmarks. benchmarks COMPARE it. it COMPARE benchmarks. benchmarks COMPARE it. it COMPARE it. it COMPARE it. Spectral DQN USED-FOR reward progressivity. OtherScientificTerm are rewards, training loss, and extreme reward progressivity. Method are value - based deep reinforcement learning agents, and value - based methods. Generic are agent, and approach. Material is Atari games. ","This paper studies the problem of reward progressivity in reinforcement learning, specifically in the setting of reinforcement learning with progressive rewards. The authors propose a new approach to improve the performance of value-based deep reinforcement learning agents. They show that Spectral DQN is able to increase the reward of an agent when the training loss is large enough, and that it outperforms other benchmarks. They also show that this approach can be applied to Atari games.  ","This paper studies the problem of reward progressivity in reinforcement learning, specifically in the setting of reinforcement learning with progressive rewards. The authors propose a new approach to improve the performance of value-based deep reinforcement learning agents. They show that Spectral DQN is able to increase the reward of an agent when the training loss is large enough, and that it outperforms other benchmarks. They also show that this approach can be applied to Atari games.  "
2115,SP:bff215c695b302ce31311f2dd105dace06307cfc,representations USED-FOR task. usable information FEATURE-OF representation. deep network USED-FOR representation. minimal sufficient representations USED-FOR task. learning - rate CONJUNCTION small batch size. small batch size CONJUNCTION learning - rate. learning - rate FEATURE-OF Stochastic Gradient Descent. Stochastic Gradient Descent USED-FOR implicit regularization. neuroscience literature USED-FOR perceptual decision - making tasks. Generic is it. Method is minimal sufficient representation. OtherScientificTerm is learning dynamics. Task is image classification tasks. ,"This paper studies the problem of learning representations that are sufficient for a given task. The authors show that the representation learned by a deep network is sufficient for the task, and that it can be seen as a minimal sufficient representation. They show that minimal sufficient representations can be learned for any task. They also show that representations learned for a particular task can contain usable information that can be used to guide the learning dynamics of the task.    The authors propose an implicit regularization based on Stochastic Gradient Descent with learning-rate and small batch size. This is a well-studied topic in neuroscience literature for perceptual decision-making tasks. The paper is well-written, well-motivated, and well-organized. The experiments on image classification tasks are well-designed. ","This paper studies the problem of learning representations that are sufficient for a given task. The authors show that the representation learned by a deep network is sufficient for the task, and that it can be seen as a minimal sufficient representation. They show that minimal sufficient representations can be learned for any task. They also show that representations learned for a particular task can contain usable information that can be used to guide the learning dynamics of the task.    The authors propose an implicit regularization based on Stochastic Gradient Descent with learning-rate and small batch size. This is a well-studied topic in neuroscience literature for perceptual decision-making tasks. The paper is well-written, well-motivated, and well-organized. The experiments on image classification tasks are well-designed. "
2124,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"robust adversarial learning CONJUNCTION inverse reinforcement learning. inverse reinforcement learning CONJUNCTION robust adversarial learning. Min - max optimization USED-FOR machine learning problems. nonconvexstrongly - concave min - max optimization HYPONYM-OF machine learning problems. robust adversarial learning HYPONYM-OF machine learning problems. inverse reinforcement learning HYPONYM-OF machine learning problems. variance reduction algorithm SREDA USED-FOR problem. accuracy level FEATURE-OF optimal complexity dependence. initialization accuracy CONJUNCTION -dependent stepsize. -dependent stepsize CONJUNCTION initialization accuracy. -dependent stepsize USED-FOR per - iteration progress. convergence guarantee EVALUATE-FOR SREDA. initialization accuracy USED-FOR convergence guarantee. analytical framework USED-FOR SREDA. SREDA - Boost COMPARE SREDA. SREDA COMPARE SREDA - Boost. SREDA - Boost USED-FOR zeroth - order variance reduction algorithm. ZO - SREDA - Boost COMPARE complexity dependence on. complexity dependence on COMPARE ZO - SREDA - Boost. ZO - SREDA - Boost HYPONYM-OF zeroth - order variance reduction algorithm. variance reduction technique USED-FOR zeroth - order algorithm. zeroth - order algorithm USED-FOR min - max optimization problems. variance reduction technique USED-FOR min - max optimization problems. OtherScientificTerm are restrictive initialization requirement, and gradients. ","Min-max optimization is an important problem in many machine learning problems such as robust adversarial learning and inverse reinforcement learning. In this paper, the authors consider the nonconvexstrongly-concave min-min optimization problem and propose a variance reduction algorithm SREDA to solve this problem. The authors provide an analytical framework for the convergence guarantee of SREDa under a restrictive initialization requirement. They show that the optimal complexity dependence on the accuracy level and the per-iteration progress depends on the initialization accuracy and the -dependent stepsize. They then propose a zeroth-order variance reduction method called ZO-SREDA-Boost, which is a variant of the original SRED a-boost, and show that ZO -SREDa-Boost outperforms SRedA in terms of convergence guarantee. They also show that this variance reduction technique can be applied to other min-MAX optimization problems, and that the zerth-order algorithm can also be used as a generalization of the existing state-of-the-art variance reduction techniques. ","Min-max optimization is an important problem in many machine learning problems such as robust adversarial learning and inverse reinforcement learning. In this paper, the authors consider the nonconvexstrongly-concave min-min optimization problem and propose a variance reduction algorithm SREDA to solve this problem. The authors provide an analytical framework for the convergence guarantee of SREDa under a restrictive initialization requirement. They show that the optimal complexity dependence on the accuracy level and the per-iteration progress depends on the initialization accuracy and the -dependent stepsize. They then propose a zeroth-order variance reduction method called ZO-SREDA-Boost, which is a variant of the original SRED a-boost, and show that ZO -SREDa-Boost outperforms SRedA in terms of convergence guarantee. They also show that this variance reduction technique can be applied to other min-MAX optimization problems, and that the zerth-order algorithm can also be used as a generalization of the existing state-of-the-art variance reduction techniques. "
2133,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"COCO EVALUATE-FOR state - of - the - art. it USED-FOR models. Example based object detection USED-FOR Detection of Novel Objects. Task are one - shot object detection, few - shot learning, and data annotation. OtherScientificTerm are generalization gap, Object categories, and object categories. Metric is generalization. Method are few - shot detection models, and metric learning approaches. ","This paper studies the problem of one-shot object detection in the context of few-shot learning. The authors propose a new metric called generalization gap, which measures the gap between the generalization performance of a classifier and the performance of the classifier on a set of classes. Object categories are defined as the number of classes for which the model is able to generalize well. The paper also proposes a new method called Detection of Novel Objects based on Example based object detection, and shows that it can be used to train models that generalize better than the state-of-the-art on COCO.    The paper is well-written, well-motivated, and well-structured. The idea of generalization is interesting, and the paper is clearly written. However, there are a few issues with the paper: (1) Object categories can be arbitrary, (2) Few-shot detection models are not generalizable, (3) the paper does not provide any data annotation, and (4) there is no comparison between different metric learning approaches. ","This paper studies the problem of one-shot object detection in the context of few-shot learning. The authors propose a new metric called generalization gap, which measures the gap between the generalization performance of a classifier and the performance of the classifier on a set of classes. Object categories are defined as the number of classes for which the model is able to generalize well. The paper also proposes a new method called Detection of Novel Objects based on Example based object detection, and shows that it can be used to train models that generalize better than the state-of-the-art on COCO.    The paper is well-written, well-motivated, and well-structured. The idea of generalization is interesting, and the paper is clearly written. However, there are a few issues with the paper: (1) Object categories can be arbitrary, (2) Few-shot detection models are not generalizable, (3) the paper does not provide any data annotation, and (4) there is no comparison between different metric learning approaches. "
2149,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"3D representations USED-FOR modeling clean mesh surfaces. occupancy fields CONJUNCTION signed distance functions ( SDF ). signed distance functions ( SDF ) CONJUNCTION occupancy fields. Implicit neural shape functions USED-FOR 3D representations. signed distance functions ( SDF ) HYPONYM-OF Implicit neural shape functions. occupancy fields HYPONYM-OF Implicit neural shape functions. representations USED-FOR single - view object reconstruction. Existing approaches USED-FOR single - view object reconstruction. representations USED-FOR Existing approaches. supervision signals USED-FOR Existing approaches. spatial gradient FEATURE-OF implicit field. supervision USED-FOR single - view reconstruction. feature map USED-FOR spatial gradient. real - world scenes USED-FOR single view implicit surface reconstructions. scanned dataset USED-FOR single view implicit surface reconstructions. ShapeNet CONJUNCTION ScannetV2. ScannetV2 CONJUNCTION ShapeNet. ShapeNet HYPONYM-OF datasets. ScannetV2 HYPONYM-OF datasets. model USED-FOR 3D implicit surface reconstruction. RGB image USED-FOR model. Task are real - world scenarios, and training on large - scale scenes. Material are ideal watertight geometric training data, large - scale scenes, internet, Internet, and pix3d dataset. OtherScientificTerm are training signal, spatial gradients, feature maps, and dense 3D supervision. Generic is this. Method are Pix3D, and DGS module. ","This paper proposes a method to learn 3D representations for modeling clean mesh surfaces. Implicit neural shape functions, such as occupancy fields and signed distance functions (SDF), are commonly used in 3D representation learning. Existing approaches for single-view object reconstruction are based on these representations, but in many real-world scenarios, there is no ideal watertight geometric training data available (e.g. large-scale scenes are not available in the real world), and existing approaches rely on supervision signals. This paper proposes to address this issue by learning representations that are watertight to the training signal, and thus can be used for single view implicit surface reconstructions on real-life scenes. Existence approaches are trained using supervision signals, but this is not watertight in the sense that the spatial gradients of the implicit field are not captured by the feature maps. The authors propose to use dense 3D supervision, where the spatial gradient of an implicit field is modeled as a feature map, and the feature map is used as a spatial gradient for the feature of the input image.   The authors train a model for 3D implicit surface reconstruction on an RGB image from Pix3D, and train it on two datasets, ShapeNet and ScannetV2. They also train on a scanned dataset from the Internet, and show that their model is able to learn to reconstruct a 3D surface from a single view image. The paper also shows that the model can be trained on a pix3d dataset, and that the DGS module works well.  The paper is well-written and well-motivated, and is easy to follow. However, the paper suffers from a lack of novelty, and there is not enough experiments to demonstrate the effectiveness of the proposed method. ","This paper proposes a method to learn 3D representations for modeling clean mesh surfaces. Implicit neural shape functions, such as occupancy fields and signed distance functions (SDF), are commonly used in 3D representation learning. Existing approaches for single-view object reconstruction are based on these representations, but in many real-world scenarios, there is no ideal watertight geometric training data available (e.g. large-scale scenes are not available in the real world), and existing approaches rely on supervision signals. This paper proposes to address this issue by learning representations that are watertight to the training signal, and thus can be used for single view implicit surface reconstructions on real-life scenes. Existence approaches are trained using supervision signals, but this is not watertight in the sense that the spatial gradients of the implicit field are not captured by the feature maps. The authors propose to use dense 3D supervision, where the spatial gradient of an implicit field is modeled as a feature map, and the feature map is used as a spatial gradient for the feature of the input image.   The authors train a model for 3D implicit surface reconstruction on an RGB image from Pix3D, and train it on two datasets, ShapeNet and ScannetV2. They also train on a scanned dataset from the Internet, and show that their model is able to learn to reconstruct a 3D surface from a single view image. The paper also shows that the model can be trained on a pix3d dataset, and that the DGS module works well.  The paper is well-written and well-motivated, and is easy to follow. However, the paper suffers from a lack of novelty, and there is not enough experiments to demonstrate the effectiveness of the proposed method. "
2165,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"deep learning algorithms CONJUNCTION distributed training. distributed training CONJUNCTION deep learning algorithms. distributed training CONJUNCTION hardware design. hardware design CONJUNCTION distributed training. hardware design USED-FOR large models. GPT-3 CONJUNCTION Switch Transformer. Switch Transformer CONJUNCTION GPT-3. distributed training USED-FOR large models. Switch Transformer HYPONYM-OF extreme - scale models. GPT-3 HYPONYM-OF extreme - scale models. limited resources USED-FOR extreme - scale model training. memory footprint USED-FOR extreme - scale model training. Pseudo - to - Real USED-FOR high - memoryfootprint - required large models. training strategy USED-FOR high - memoryfootprint - required large models. Pseudo - to - Real HYPONYM-OF training strategy. Pseudo - to - Real CONJUNCTION large models. large models CONJUNCTION Pseudo - to - Real. architecture of sequential layers USED-FOR large models. GPUs USED-FOR state - of - the - art. Granular CPU offloading USED-FOR CPU memory. technique USED-FOR CPU memory. Granular CPU offloading HYPONYM-OF technique. Metric are model convergence, and carbon footprint. Method is large model. OtherScientificTerm is GPU utilities. Task is greener AI. ","This paper considers the problem of extreme-scale model training with limited resources with a memory footprint. The authors consider deep learning algorithms, distributed training, hardware design, and hardware design for large models (e.g., GPT-3 and Switch Transformer). The authors propose a training strategy called Pseudo-to-Real, which aims to reduce the memory footprint of high-memoryfootprint-required large models with distributed training. They also propose a technique called ""Granular CPU offloading"" to reduce CPU memory, which is a technique that can be applied to any large model.   The authors show that the proposed technique is able to significantly reduce the CPU memory of large models trained on GPUs, and that the model convergence is faster than the state-of-the-art. The paper also shows that the large models can be trained with an architecture of sequential layers, which can be used to reduce GPU utilities.  The paper is well-written and well-motivated, and the paper is clearly written. It is a good contribution to the field of greener AI. ","This paper considers the problem of extreme-scale model training with limited resources with a memory footprint. The authors consider deep learning algorithms, distributed training, hardware design, and hardware design for large models (e.g., GPT-3 and Switch Transformer). The authors propose a training strategy called Pseudo-to-Real, which aims to reduce the memory footprint of high-memoryfootprint-required large models with distributed training. They also propose a technique called ""Granular CPU offloading"" to reduce CPU memory, which is a technique that can be applied to any large model.   The authors show that the proposed technique is able to significantly reduce the CPU memory of large models trained on GPUs, and that the model convergence is faster than the state-of-the-art. The paper also shows that the large models can be trained with an architecture of sequential layers, which can be used to reduce GPU utilities.  The paper is well-written and well-motivated, and the paper is clearly written. It is a good contribution to the field of greener AI. "
2181,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"Energy - based models ( EBMs ) HYPONYM-OF generative models. maximum likelihood estimation USED-FOR generative models. Gibbs distribution USED-FOR energy. Fenchel duality USED-FOR variational principles. variational principles USED-FOR maximum likelihood EBMs. shallow overparametrized neural network energies USED-FOR maximum likelihood EBMs. shallow overparametrized neural network energies USED-FOR variational principles. dual formulation USED-FOR training algorithm. particles CONJUNCTION neurons. neurons CONJUNCTION particles. particles PART-OF sample space. parameter space FEATURE-OF neurons. dual formulation USED-FOR active regime. maximum likelihood CONJUNCTION score matching training. score matching training CONJUNCTION maximum likelihood. intermediate parameter setups USED-FOR dual algorithm. Generic are approach, and algorithm. ","This paper proposes a new class of generative models, called Energy-based models (EBMs), which is based on the Fenchel duality of maximum likelihood estimation. The authors show that maximum likelihood EBMs can be approximated by variational principles based on shallow overparametrized neural network energies. The energy is modeled as a Gibbs distribution, and the authors propose a dual formulation of the training algorithm based on this dual formulation. This dual formulation can be used as a training algorithm in the active regime, where particles in the sample space and neurons in the parameter space share the same parameter space. The proposed dual algorithm can be applied to both maximum likelihood and score matching training under different intermediate parameter setups. Experiments are conducted to demonstrate the effectiveness of the proposed approach. ","This paper proposes a new class of generative models, called Energy-based models (EBMs), which is based on the Fenchel duality of maximum likelihood estimation. The authors show that maximum likelihood EBMs can be approximated by variational principles based on shallow overparametrized neural network energies. The energy is modeled as a Gibbs distribution, and the authors propose a dual formulation of the training algorithm based on this dual formulation. This dual formulation can be used as a training algorithm in the active regime, where particles in the sample space and neurons in the parameter space share the same parameter space. The proposed dual algorithm can be applied to both maximum likelihood and score matching training under different intermediate parameter setups. Experiments are conducted to demonstrate the effectiveness of the proposed approach. "
2197,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"differentially private ERM USED-FOR convex functions. lower bounds FEATURE-OF convex functions. lower bounds FEATURE-OF differentially private ERM. logarithmic terms USED-FOR lower bounds. biased mean property USED-FOR fingerprinting codes. ` 2 loss function COMPARE linear functions. linear functions COMPARE ` 2 loss function. ` 2 loss function USED-FOR pure - DP. Method are approximate - DP, and DP - ERM. Material is constrained case. OtherScientificTerm are unconstrained case, auxiliary dimension, ` 2 loss, and one - way marginals. Generic is it. ","This paper studies the problem of differentially private ERM with lower bounds on the logarithmic terms of the convex functions. In particular, the authors consider the constrained case and the unconstrained case, and show that the lower bounds of the differentially privacy ERM can be obtained for convex function of the form $f(x, x') = f(x) + \epsilon(x')$. They also show that under certain assumptions on the auxiliary dimension of the function, the `2 loss function for pure-DP is asymptotically optimal as for linear functions. Finally, they prove that the biased mean property of the fingerprinting codes is satisfied for the case of approximate-DP, and that it holds for DP-ERM.  ","This paper studies the problem of differentially private ERM with lower bounds on the logarithmic terms of the convex functions. In particular, the authors consider the constrained case and the unconstrained case, and show that the lower bounds of the differentially privacy ERM can be obtained for convex function of the form $f(x, x') = f(x) + \epsilon(x')$. They also show that under certain assumptions on the auxiliary dimension of the function, the `2 loss function for pure-DP is asymptotically optimal as for linear functions. Finally, they prove that the biased mean property of the fingerprinting codes is satisfied for the case of approximate-DP, and that it holds for DP-ERM.  "
2213,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"gradient flow USED-FOR machine learning applications. Wasserstein metric FEATURE-OF space of probability densities. approach USED-FOR Wasserstein gradient flow. finite difference USED-FOR approach. finite difference USED-FOR Wasserstein gradient flow. scalable proximal gradient type algorithm USED-FOR Wasserstein gradient flow. variational formulation of the objective function USED-FOR JKO proximal map. variational formulation of the objective function USED-FOR method. primal - dual optimization USED-FOR JKO proximal map. heat equation CONJUNCTION porous medium equation. porous medium equation CONJUNCTION heat equation. framework USED-FOR Wasserstein gradient flows. porous medium equation HYPONYM-OF Wasserstein gradient flows. heat equation HYPONYM-OF Wasserstein gradient flows. OtherScientificTerm are grid, and inner and outer loops. Task is primal - dual problem. Generic is algorithm. ","This paper considers the problem of gradient flow for machine learning applications. The authors propose an approach to Wasserstein gradient flow based on a finite difference between the space of probability densities on a grid and the Wassersteine metric of the underlying space. They propose a scalable proximal gradient type algorithm to approximate the gradient flow. The method is based on the variational formulation of the objective function of the JKO proximal map from primal-dual optimization, which is a variant of the primal-penal dual problem. The algorithm is shown to be computationally efficient. The paper also proposes a new framework for approximating the gradient flows of two special cases, the heat equation and the porous medium equation, and shows that the inner and outer loops of the proposed algorithm converge to a stationary point.","This paper considers the problem of gradient flow for machine learning applications. The authors propose an approach to Wasserstein gradient flow based on a finite difference between the space of probability densities on a grid and the Wassersteine metric of the underlying space. They propose a scalable proximal gradient type algorithm to approximate the gradient flow. The method is based on the variational formulation of the objective function of the JKO proximal map from primal-dual optimization, which is a variant of the primal-penal dual problem. The algorithm is shown to be computationally efficient. The paper also proposes a new framework for approximating the gradient flows of two special cases, the heat equation and the porous medium equation, and shows that the inner and outer loops of the proposed algorithm converge to a stationary point."
2229,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,ML algorithm CONJUNCTION hyper - parameter configuration. hyper - parameter configuration CONJUNCTION ML algorithm. ML algorithm USED-FOR AutoML problem. approach USED-FOR meta - features. Optimal Transport procedure USED-FOR meta - features. MetaBu meta - features USED-FOR topology. hyper - parameter configurations USED-FOR AutoML. MetaBu meta - features USED-FOR AutoML systems. AutoSkLearn CONJUNCTION Probabilistic Matrix Factorization. Probabilistic Matrix Factorization CONJUNCTION AutoSkLearn. OpenML CC-18 benchmark EVALUATE-FOR AutoML systems. OpenML CC-18 benchmark EVALUATE-FOR MetaBu meta - features. AutoSkLearn HYPONYM-OF AutoML systems. Probabilistic Matrix Factorization HYPONYM-OF AutoML systems. topology USED-FOR intrinsic dimensionality. intrinsic dimensionality FEATURE-OF OpenML benchmark. MetaBu meta - features USED-FOR intrinsic dimensionality. MetaBu meta - features USED-FOR topology. Method is MetaBu. OtherScientificTerm is manually designed meta - features. ,"This paper proposes MetaBu, a new approach to learn meta-features for AutoML problem by combining the benefits of both the ML algorithm and hyper-parameter configuration. MetaBu is an extension of the Optimal Transport procedure, which is used to learn a meta-feature for each hyper parameter. The authors show that MetaBu meta-distributions can be used to improve the intrinsic dimensionality of AutoML systems on the OpenML CC-18 benchmark. They also show that the MetaBu topology can improve AutoML topology on the topology of AutoSkLearn and Probabilistic Matrix Factorization.    The authors also demonstrate that the learned topology is a good way to improve Auto-learning performance on the open-source OpenML benchmark. The paper also shows that the meta features learned by MetaBu are more robust to manually designed meta-resets, and that AutoML can be trained with different hyper parameter configurations.","This paper proposes MetaBu, a new approach to learn meta-features for AutoML problem by combining the benefits of both the ML algorithm and hyper-parameter configuration. MetaBu is an extension of the Optimal Transport procedure, which is used to learn a meta-feature for each hyper parameter. The authors show that MetaBu meta-distributions can be used to improve the intrinsic dimensionality of AutoML systems on the OpenML CC-18 benchmark. They also show that the MetaBu topology can improve AutoML topology on the topology of AutoSkLearn and Probabilistic Matrix Factorization.    The authors also demonstrate that the learned topology is a good way to improve Auto-learning performance on the open-source OpenML benchmark. The paper also shows that the meta features learned by MetaBu are more robust to manually designed meta-resets, and that AutoML can be trained with different hyper parameter configurations."
2245,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"Federated learning ( FL ) USED-FOR distributed learning framework. robustness FEATURE-OF models. Split - Mix FL strategy USED-FOR heterogeneous participants. base sub - networks USED-FOR customization. communication CONJUNCTION storage. storage CONJUNCTION communication. storage CONJUNCTION inference. inference CONJUNCTION storage. split - mix strategy USED-FOR customization. method COMPARE heterogeneous - architecture FL methods. heterogeneous - architecture FL methods COMPARE method. in - situ customization EVALUATE-FOR heterogeneous - architecture FL methods. in - situ customization EVALUATE-FOR method. Task is FL scenarios. OtherScientificTerm are hardware and inference dynamics, and inference requirements. Method are FL approaches, and FL. ","This paper studies the problem of federated learning (FL) in a distributed learning framework. The authors propose a Split-Mix FL strategy to handle heterogeneous participants and improve robustness of models in FL scenarios. In particular, the authors propose to split the base sub-networks into two subsets to allow for customization of different base subnetworks, and then use the split-mix strategy to achieve this customization.  The authors show that the proposed method outperforms other heterogeneous-architecture FL methods in terms of in-situ customization in a variety of hardware and inference dynamics, including communication, storage, inference, and inference requirements. The paper also shows that FL approaches that do not take into account the heterogeneous nature of the hardware and the inference requirements do not perform well in practice. ","This paper studies the problem of federated learning (FL) in a distributed learning framework. The authors propose a Split-Mix FL strategy to handle heterogeneous participants and improve robustness of models in FL scenarios. In particular, the authors propose to split the base sub-networks into two subsets to allow for customization of different base subnetworks, and then use the split-mix strategy to achieve this customization.  The authors show that the proposed method outperforms other heterogeneous-architecture FL methods in terms of in-situ customization in a variety of hardware and inference dynamics, including communication, storage, inference, and inference requirements. The paper also shows that FL approaches that do not take into account the heterogeneous nature of the hardware and the inference requirements do not perform well in practice. "
2261,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"extragradient - type algorithm USED-FOR nonconvex - nonconcave minimax problems. local solution USED-FOR general minimax problems. first order methods USED-FOR variational inequalities. problem class USED-FOR non - trivial structures. algorithms USED-FOR limit cycles. algorithm USED-FOR constrained and regularized problems. adaptive stepsize USED-FOR stepsizes. adaptive stepsize PART-OF algorithm. limit cycles FEATURE-OF operator. it USED-FOR training of generative adversarial networks. variant USED-FOR training of generative adversarial networks. stochastic oracles USED-FOR variant. monotone setting FEATURE-OF it. OtherScientificTerm are weak Minty variational inequality ( MVI ), and weak MVI. Generic is scheme. Method are generative adversarial networks, and stochastic algorithm. ","This paper proposes an extension of the nonconvex-nonconcave minimax minimax problem to non-monotone minimax problems. The main idea is to use the weak Minty variational inequality (MVI) as a local solution for the non-trivial case. The authors show that the proposed algorithm can be used for both constrained and regularized problems. In particular, it is shown that the algorithm is monotone in the monotonic case and that it can be applied to training of generative adversarial networks.","This paper proposes an extension of the nonconvex-nonconcave minimax minimax problem to non-monotone minimax problems. The main idea is to use the weak Minty variational inequality (MVI) as a local solution for the non-trivial case. The authors show that the proposed algorithm can be used for both constrained and regularized problems. In particular, it is shown that the algorithm is monotone in the monotonic case and that it can be applied to training of generative adversarial networks."
2277,SP:af22742091277b726f67e7155b412dd35f29e804,"neural contextual bandits HYPONYM-OF contextual bandits. learning algorithm USED-FOR raw feature vector. upper confidence bound ( UCB ) approach USED-FOR last linear layer ( shallow exploration ). upper confidence bound ( UCB ) approach USED-FOR learning algorithm. finitetime regret EVALUATE-FOR algorithm. neural contextual bandit algorithms COMPARE approach. approach COMPARE neural contextual bandit algorithms. deep neural network USED-FOR it. OtherScientificTerm are reward generating function, and learning time horizon. ","This paper studies the problem of neural contextual bandits, a class of contextual bandits where the goal is to learn a reward generating function. The authors propose a learning algorithm for learning the raw feature vector, which is based on the upper confidence bound (UCB) approach for the last linear layer (shallow exploration). The authors show that the proposed algorithm achieves a finitetime regret of $O(\sqrt{T})$, where $T$ is the learning time horizon. Compared to other neural contextual bandit algorithms, the proposed approach is more computationally efficient as it uses a deep neural network.","This paper studies the problem of neural contextual bandits, a class of contextual bandits where the goal is to learn a reward generating function. The authors propose a learning algorithm for learning the raw feature vector, which is based on the upper confidence bound (UCB) approach for the last linear layer (shallow exploration). The authors show that the proposed algorithm achieves a finitetime regret of $O(\sqrt{T})$, where $T$ is the learning time horizon. Compared to other neural contextual bandit algorithms, the proposed approach is more computationally efficient as it uses a deep neural network."
2293,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"Training action space selection USED-FOR reinforcement learning ( RL ). Shapley - inspired methodology USED-FOR action space categorization. Monte Carlo simulation USED-FOR unnecessary explorations. Monte Carlo simulation PART-OF methodology. cloud infrastructure resource tuning case study EVALUATE-FOR methodology. It USED-FOR search space. it USED-FOR RL model design. data - driven methodology USED-FOR reinforcement learning algorithms. OtherScientificTerm are complex state - action relationships, and exponential - time shapley computations. ",Training action space selection in reinforcement learning (RL) is an important problem with complex state-action relationships. This paper proposes a Shapley-inspired methodology for action space categorization. The methodology combines Monte Carlo simulation to avoid unnecessary explorations and exponential-time shapley computations. The authors demonstrate the effectiveness of the methodology on a cloud infrastructure resource tuning case study and show that it can be applied to RL model design using a data-driven methodology. ,Training action space selection in reinforcement learning (RL) is an important problem with complex state-action relationships. This paper proposes a Shapley-inspired methodology for action space categorization. The methodology combines Monte Carlo simulation to avoid unnecessary explorations and exponential-time shapley computations. The authors demonstrate the effectiveness of the methodology on a cloud infrastructure resource tuning case study and show that it can be applied to RL model design using a data-driven methodology. 
2309,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"approach USED-FOR probably approximately correct ( PAC ) prediction sets. importance weights USED-FOR algorithm. confidence intervals FEATURE-OF importance weights. DomainNet CONJUNCTION ImageNet. ImageNet CONJUNCTION DomainNet. approach USED-FOR covariate shifts. DomainNet USED-FOR approach. ImageNet USED-FOR approach. PAC constraint FEATURE-OF approaches. PAC constraint FEATURE-OF algorithm. Method are machine learning, predictive model, and uncertainty quantification algorithms. OtherScientificTerm are data distribution, and covariate shift. Generic is shifts. Metric is average normalized size. ",This paper proposes a new approach to estimate probably approximately correct (PAC) prediction sets for machine learning. The proposed algorithm uses importance weights with confidence intervals as a proxy for the uncertainty of a given predictive model. The approach is tested on DomainNet and ImageNet and is shown to be robust to covariate shifts in the data distribution. The paper also shows that the proposed approach can be used to estimate the average normalized size of the covariate shift. The algorithm is also shown to satisfy the PAC constraint of the proposed algorithm. ,This paper proposes a new approach to estimate probably approximately correct (PAC) prediction sets for machine learning. The proposed algorithm uses importance weights with confidence intervals as a proxy for the uncertainty of a given predictive model. The approach is tested on DomainNet and ImageNet and is shown to be robust to covariate shifts in the data distribution. The paper also shows that the proposed approach can be used to estimate the average normalized size of the covariate shift. The algorithm is also shown to satisfy the PAC constraint of the proposed algorithm. 
2325,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"generalization error EVALUATE-FOR iterative SSL algorithms. information - theoretic principles USED-FOR generalization error. binary Gaussian mixture model HYPONYM-OF model. benchmark datasets EVALUATE-FOR model. MNIST and CIFAR datasets HYPONYM-OF benchmark datasets. OtherScientificTerm are model parameters, class conditional variances, and pseudo - labelling iterations. ","This paper studies the generalization error of iterative SSL algorithms based on information-theoretic principles. The authors propose a new model, the binary Gaussian mixture model, which is based on the assumption that the model parameters are independent of class conditional variances. The proposed model is tested on two benchmark datasets, the MNIST and CIFAR datasets, and is shown to outperform existing methods. The paper also shows that pseudo-labelling iterations can be used to improve generalization performance. ","This paper studies the generalization error of iterative SSL algorithms based on information-theoretic principles. The authors propose a new model, the binary Gaussian mixture model, which is based on the assumption that the model parameters are independent of class conditional variances. The proposed model is tested on two benchmark datasets, the MNIST and CIFAR datasets, and is shown to outperform existing methods. The paper also shows that pseudo-labelling iterations can be used to improve generalization performance. "
2341,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"model - free reinforcement learning algorithms USED-FOR policy. intentional action sequences USED-FOR high value regions. intentional action sequences USED-FOR plans. it USED-FOR plans. multi - step plans USED-FOR temporally coordinated exploration. GPM USED-FOR temporally coordinated exploration. multi - step plans USED-FOR GPM. GPM USED-FOR it. crude initial plan generator USED-FOR GPM. benchmark environments EVALUATE-FOR baseline methods. OtherScientificTerm are inefficient exploration, single step nature, single step level, consistent movement, and multi - step plan. Method are Generative Planning method ( GPM ), generative planning, and actionrepeat strategy. ","This paper proposes a new generative planning method (GPM) to tackle the problem of inefficient exploration in RL. The authors argue that due to the single step nature of RL, existing model-free reinforcement learning algorithms are unable to learn a good policy that can efficiently explore the entire environment. To address this issue, the authors propose the Generative Planning method (GP) which learns a single step level plan that can be used to generate multiple multi-step plans. These plans can be learned from intentional action sequences that aim to explore high value regions of the environment. GPM uses a crude initial plan generator to generate the initial plan, and then it uses GPM to generate plans that are temporally coordinated through multi-stage plans, which are then used to train a policy that is able to explore the whole environment at once. This is achieved by training a single policy at each step, and using a single-step level plan at each time step. The paper also proposes an actionrepeat strategy, where the policy is repeated until it reaches a consistent movement. Experiments on several benchmark environments show that GPM outperforms several baseline methods.","This paper proposes a new generative planning method (GPM) to tackle the problem of inefficient exploration in RL. The authors argue that due to the single step nature of RL, existing model-free reinforcement learning algorithms are unable to learn a good policy that can efficiently explore the entire environment. To address this issue, the authors propose the Generative Planning method (GP) which learns a single step level plan that can be used to generate multiple multi-step plans. These plans can be learned from intentional action sequences that aim to explore high value regions of the environment. GPM uses a crude initial plan generator to generate the initial plan, and then it uses GPM to generate plans that are temporally coordinated through multi-stage plans, which are then used to train a policy that is able to explore the whole environment at once. This is achieved by training a single policy at each step, and using a single-step level plan at each time step. The paper also proposes an actionrepeat strategy, where the policy is repeated until it reaches a consistent movement. Experiments on several benchmark environments show that GPM outperforms several baseline methods."
2357,SP:ce6a93847209a0926ed0be5190378a3f61db1935,"deep linear and nonlinear matrix factorizations USED-FOR machine learning. deep learning CONJUNCTION tensor decomposition. tensor decomposition CONJUNCTION deep learning. matrices CONJUNCTION tensors. tensors CONJUNCTION matrices. factorization methods USED-FOR matrix and tensor completion problems. methods COMPARE matrix and tensor factorization methods. matrix and tensor factorization methods COMPARE methods. generalization error bounds EVALUATE-FOR matrix and tensor factorization methods. generalization error bounds EVALUATE-FOR methods. synthetic data and real datasets EVALUATE-FOR methods. recovery accuracy EVALUATE-FOR baselines. methods COMPARE baselines. baselines COMPARE methods. synthetic data and real datasets EVALUATE-FOR baselines. recovery accuracy EVALUATE-FOR methods. Method are deep nonlinear matrix factorization methods, and multi - mode deep matrix and tensor factorizations. ","This paper studies deep linear and nonlinear matrix factorizations for machine learning. In particular, the authors focus on the problem of matrix factorization in the context of deep learning and tensor decomposition. The authors propose two new factorization methods for matrix and tensors, which are based on the recent work of [1]. The authors show that the proposed methods achieve better generalization error bounds than the existing state-of-the-art Matrix and Tensor Factorization methods.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]   [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [28] [29] [30] [34] [35] [36] [37] [38] ","This paper studies deep linear and nonlinear matrix factorizations for machine learning. In particular, the authors focus on the problem of matrix factorization in the context of deep learning and tensor decomposition. The authors propose two new factorization methods for matrix and tensors, which are based on the recent work of [1]. The authors show that the proposed methods achieve better generalization error bounds than the existing state-of-the-art Matrix and Tensor Factorization methods.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]   [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [28] [29] [30] [34] [35] [36] [37] [38] "
2373,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"interpretation technique USED-FOR structured output models. features USED-FOR structured model. function USED-FOR interpreter. energy - based training process USED-FOR interpreter function. structural information PART-OF model. simulated and real data sets EVALUATE-FOR method. OtherScientificTerm are output variables, computational path of output variables, feature, output variable, and input space. Method are structured models, and structured output model. ","This paper proposes a new interpretation technique for structured output models, where the output variables of the model are structured and the computational path of output variables can be interpreted as a feature of the input variable. The structured model uses the features extracted from the features to train a structured model. The authors propose an energy-based training process to learn the interpreter function, which is a function that maps the input feature to the output variable in the input space. The proposed method is evaluated on simulated and real data sets, and shows that the proposed method can incorporate structural information into the model. ","This paper proposes a new interpretation technique for structured output models, where the output variables of the model are structured and the computational path of output variables can be interpreted as a feature of the input variable. The structured model uses the features extracted from the features to train a structured model. The authors propose an energy-based training process to learn the interpreter function, which is a function that maps the input feature to the output variable in the input space. The proposed method is evaluated on simulated and real data sets, and shows that the proposed method can incorporate structural information into the model. "
2389,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"approaches USED-FOR distributional DRL. reward function USED-FOR agent behavior. variance reduction measures USED-FOR on - policy learning. asymptotically consistent estimate of the policy gradient USED-FOR CDF - based objectives. sampling USED-FOR asymptotically consistent estimate of the policy gradient. sampling USED-FOR CDF - based objectives. algorithm USED-FOR agents. risk profiles FEATURE-OF penalty - based formulations. accumulation of positive rewards CONJUNCTION frequency of incurred penalties. frequency of incurred penalties CONJUNCTION accumulation of positive rewards. OpenAI Safety Gym environments FEATURE-OF penalty - based formulations. penalty - based formulations USED-FOR agents. risk profiles FEATURE-OF agents. risk profile COMPARE Proximal Policy Optimization ( PPO ). Proximal Policy Optimization ( PPO ) COMPARE risk profile. risk profile COMPARE PPO. PPO COMPARE risk profile. Proximal Policy Optimization ( PPO ) COMPARE PPO. PPO COMPARE Proximal Policy Optimization ( PPO ). Proximal Policy Optimization ( PPO ) CONJUNCTION positive reward. positive reward CONJUNCTION Proximal Policy Optimization ( PPO ). positive reward COMPARE PPO. PPO COMPARE positive reward. Lagrangians USED-FOR cost levels. positive reward EVALUATE-FOR risk profile. Lagrangians USED-FOR PPO. Method is deep reinforcement learning ( DRL ) agents. Generic are policy, approach, and technique. Task is human decision - making. OtherScientificTerm are distributional context, projected distribution of returns, distribution of full - episode outcomes, cumulative distribution function ( CDF ), relative quality, continuous and discrete action spaces, and policy gradient. ","This paper studies the problem of training deep reinforcement learning (DRL) agents in a distributional setting, where the reward function is a cumulative distribution over a set of trajectories and the agent behavior depends on the distributional context. The authors propose two approaches for distributional DRL: (1) learning a policy that maximizes the expected return of the agent under the projected distribution of returns, (2) using variance reduction measures for on-policy learning, and (3) learning an asymptotically consistent estimate of the policy gradient for CDF-based objectives based on sampling.    The authors show that the proposed approach (called Proximal Policy Optimization (PPO) in the paper) is a generalization of existing approaches to the distribution of full-episode outcomes in DRL. The main difference is that the authors consider the cumulative distribution function (CDF) to be a function of the relative quality of the trajectories, which is a measure of how well the agent is able to learn a policy. They show that, in continuous and discrete action spaces, their algorithm can learn agents with risk profiles that match the risk profiles of penalty-based formulations in the OpenAI Safety Gym environments. They also show that their risk profile matches the risk profile of PPO, and that PPO is more robust to the accumulation of positive rewards and to the frequency of incurred penalties.  In addition, they show that Lagrangians can be used to estimate the cost levels of their algorithm, and compare the risk of their agents with PPO and with a positive reward.  The technique is also shown to be applicable to human decision-making. ","This paper studies the problem of training deep reinforcement learning (DRL) agents in a distributional setting, where the reward function is a cumulative distribution over a set of trajectories and the agent behavior depends on the distributional context. The authors propose two approaches for distributional DRL: (1) learning a policy that maximizes the expected return of the agent under the projected distribution of returns, (2) using variance reduction measures for on-policy learning, and (3) learning an asymptotically consistent estimate of the policy gradient for CDF-based objectives based on sampling.    The authors show that the proposed approach (called Proximal Policy Optimization (PPO) in the paper) is a generalization of existing approaches to the distribution of full-episode outcomes in DRL. The main difference is that the authors consider the cumulative distribution function (CDF) to be a function of the relative quality of the trajectories, which is a measure of how well the agent is able to learn a policy. They show that, in continuous and discrete action spaces, their algorithm can learn agents with risk profiles that match the risk profiles of penalty-based formulations in the OpenAI Safety Gym environments. They also show that their risk profile matches the risk profile of PPO, and that PPO is more robust to the accumulation of positive rewards and to the frequency of incurred penalties.  In addition, they show that Lagrangians can be used to estimate the cost levels of their algorithm, and compare the risk of their agents with PPO and with a positive reward.  The technique is also shown to be applicable to human decision-making. "
2405,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"Interactive Neural Process ( INP ) HYPONYM-OF Bayesian active learning framework. Bayesian active learning framework USED-FOR deep learning surrogate model. Interactive Neural Process ( INP ) USED-FOR simulation. deep sequence model CONJUNCTION active learning. active learning CONJUNCTION deep sequence model. neural process CONJUNCTION deep sequence model. deep sequence model CONJUNCTION neural process. deep sequence model USED-FOR framework. neural process USED-FOR framework. active learning USED-FOR framework. spatiotemporal neural process model USED-FOR simulator dynamics. model USED-FOR latent process. latent process USED-FOR intrinsic uncertainty. latent information gain USED-FOR acquisition function. Bayesian active learning algorithms USED-FOR simulator. approach COMPARE random sampling. random sampling COMPARE approach. sample complexity EVALUATE-FOR random sampling. high dimension FEATURE-OF random sampling. sample complexity EVALUATE-FOR approach. framework USED-FOR rapid simulation and scenario exploration. framework USED-FOR complex infectious disease simulator. Task is Stochastic simulations. OtherScientificTerm are fine - grained resolution, and theoretical analysis. ","This paper proposes a deep learning surrogate model based on a Bayesian active learning framework called Interactive Neural Process (INP) for simulation. The proposed framework combines a neural process, a deep sequence model, and active learning. Stochastic simulations are used to model the simulator dynamics using a spatiotemporal neural process model. The model is used to learn a latent process that captures the intrinsic uncertainty. The acquisition function is based on the latent information gain. The authors show that the proposed approach achieves better sample complexity compared to random sampling in high dimension (i.e. fine-grained resolution). The authors also provide a theoretical analysis that shows that the latent process is able to capture intrinsic uncertainty, and that the model can learn a good acquisition function.   The authors apply the proposed framework to the problem of rapid simulation and scenario exploration in a complex infectious disease simulator. The simulator is trained using a number of Bayesian action spaces, and the authors also show that their approach can be used to train a simulator in a more efficient way compared to previous work.","This paper proposes a deep learning surrogate model based on a Bayesian active learning framework called Interactive Neural Process (INP) for simulation. The proposed framework combines a neural process, a deep sequence model, and active learning. Stochastic simulations are used to model the simulator dynamics using a spatiotemporal neural process model. The model is used to learn a latent process that captures the intrinsic uncertainty. The acquisition function is based on the latent information gain. The authors show that the proposed approach achieves better sample complexity compared to random sampling in high dimension (i.e. fine-grained resolution). The authors also provide a theoretical analysis that shows that the latent process is able to capture intrinsic uncertainty, and that the model can learn a good acquisition function.   The authors apply the proposed framework to the problem of rapid simulation and scenario exploration in a complex infectious disease simulator. The simulator is trained using a number of Bayesian action spaces, and the authors also show that their approach can be used to train a simulator in a more efficient way compared to previous work."
2421,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"Differentially Private ( DP ) learning USED-FOR large deep learning models of text. hyperparameters USED-FOR DP optimization. hyperparameters CONJUNCTION fine - tuning objectives. fine - tuning objectives CONJUNCTION hyperparameters. pretraining procedure USED-FOR fine - tuning objectives. private NLP models COMPARE private training approaches. private training approaches COMPARE private NLP models. private training approaches CONJUNCTION nonprivate baselines. nonprivate baselines CONJUNCTION private training approaches. private NLP models COMPARE nonprivate baselines. nonprivate baselines COMPARE private NLP models. moderately - sized corpora USED-FOR DP optimization. DP optimization USED-FOR pretrained models. moderately - sized corpora USED-FOR pretrained models. linear layer PART-OF model. per - example gradients USED-FOR linear layer. memory saving technique USED-FOR clipping. clipping PART-OF DP - SGD. large Transformers USED-FOR DP - SGD. memory saving technique USED-FOR DP - SGD. privately training Transformers COMPARE non - private training. non - private training COMPARE privately training Transformers. technique USED-FOR privately training Transformers. memory cost EVALUATE-FOR non - private training. memory cost EVALUATE-FOR privately training Transformers. modest run - time overhead EVALUATE-FOR non - private training. DP optimization USED-FOR high - dimensional models. pretrained models USED-FOR private learning. Task is NLP tasks. Metric is computational overhead. Method is large pretrained models. OtherScientificTerm are noise, and dimension - dependent performance degradation. ","Differentially Private (DP) learning is a popular technique for training large deep learning models of text for NLP tasks. However, DP optimization of pretrained models trained on moderately-sized corpora is expensive due to the computational overhead. This paper proposes a new DP optimization method, DP-SGD, which uses hyperparameters from DP optimization and fine-tuning objectives learned during the pretraining procedure. The authors show that private NLP models trained with DP optimization outperform existing private training approaches and nonprivate baselines. They also propose a memory saving technique to reduce the clipping in the training of DP- SGD on large Transformers. The main idea is to add a linear layer to the model, which is based on the per-example gradients of the previous layer, and to use this linear layer as a regularizer for DP optimization. The paper shows that privately training Transformers with this technique is more memory efficient than non-private training with modest run-time overhead, and that DP optimization can be applied to high-dimensional models.   The paper also shows that DP optimized models are more robust to noise in the input data, and can be used to avoid dimension-dependent performance degradation.  Finally, the paper shows empirically that the proposed technique is effective for privately training Transformer models, and shows that the performance of privately learning Transformers is comparable to that of publicly trained models.","Differentially Private (DP) learning is a popular technique for training large deep learning models of text for NLP tasks. However, DP optimization of pretrained models trained on moderately-sized corpora is expensive due to the computational overhead. This paper proposes a new DP optimization method, DP-SGD, which uses hyperparameters from DP optimization and fine-tuning objectives learned during the pretraining procedure. The authors show that private NLP models trained with DP optimization outperform existing private training approaches and nonprivate baselines. They also propose a memory saving technique to reduce the clipping in the training of DP- SGD on large Transformers. The main idea is to add a linear layer to the model, which is based on the per-example gradients of the previous layer, and to use this linear layer as a regularizer for DP optimization. The paper shows that privately training Transformers with this technique is more memory efficient than non-private training with modest run-time overhead, and that DP optimization can be applied to high-dimensional models.   The paper also shows that DP optimized models are more robust to noise in the input data, and can be used to avoid dimension-dependent performance degradation.  Finally, the paper shows empirically that the proposed technique is effective for privately training Transformer models, and shows that the performance of privately learning Transformers is comparable to that of publicly trained models."
2437,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"skeletal structure CONJUNCTION joint attributes. joint attributes CONJUNCTION skeletal structure. length CONJUNCTION size. size CONJUNCTION length. size CONJUNCTION strength. strength CONJUNCTION size. strength HYPONYM-OF joint attributes. length HYPONYM-OF joint attributes. size HYPONYM-OF joint attributes. design procedure PART-OF decision - making process. agent PART-OF decision - making process. design procedure USED-FOR agent. skeletal structure CONJUNCTION joint attributes. joint attributes CONJUNCTION skeletal structure. transform actions USED-FOR joint attributes. transform actions USED-FOR skeletal structure. control actions USED-FOR design. message passing USED-FOR joint - specific actions. policy gradient methods USED-FOR approach. approach USED-FOR joint optimization of agent design and control. joint optimization of agent design and control CONJUNCTION experience sharing. experience sharing CONJUNCTION joint optimization of agent design and control. experience sharing USED-FOR approach. approach COMPARE prior methods. prior methods COMPARE approach. Transform2Act COMPARE prior methods. prior methods COMPARE Transform2Act. Transform2Act HYPONYM-OF approach. convergence speed EVALUATE-FOR approach. convergence speed EVALUATE-FOR prior methods. giraffes CONJUNCTION squids. squids CONJUNCTION giraffes. squids CONJUNCTION spiders. spiders CONJUNCTION squids. OtherScientificTerm are agent ’s functionality, and design space. Generic is function. Method are optimal controller, conditional policy, and graph - based policy. Metric is sample efficiency. ","This paper proposes a new approach to joint optimization of agent design and control, where the goal is to learn an agent’s functionality that maximizes the expected return of the optimal controller. The agent is trained with a design procedure that is part of the decision-making process, and the design procedure consists of two steps: (1) learning a function that maps the joint attributes (skeletal structure, joint attributes such as length, size, strength, etc.) to the design space, and (2) learning an optimal controller that optimizes this function. The design space is partitioned into two parts: a skeletal structure, and joint attributes, which are learned via transform actions. The joint attributes are learned using transform actions, and control actions are used to guide the design. The authors propose a conditional policy gradient method to learn the joint-specific actions using message passing. They also propose a graph-based policy, which is trained using policy gradient methods. They show that their approach, Transform2Act, outperforms prior methods in terms of convergence speed and sample efficiency. The approach is also able to be combined with experience sharing to improve the performance of joint optimization. Experiments are conducted on giraffes, squids, and spiders. ","This paper proposes a new approach to joint optimization of agent design and control, where the goal is to learn an agent’s functionality that maximizes the expected return of the optimal controller. The agent is trained with a design procedure that is part of the decision-making process, and the design procedure consists of two steps: (1) learning a function that maps the joint attributes (skeletal structure, joint attributes such as length, size, strength, etc.) to the design space, and (2) learning an optimal controller that optimizes this function. The design space is partitioned into two parts: a skeletal structure, and joint attributes, which are learned via transform actions. The joint attributes are learned using transform actions, and control actions are used to guide the design. The authors propose a conditional policy gradient method to learn the joint-specific actions using message passing. They also propose a graph-based policy, which is trained using policy gradient methods. They show that their approach, Transform2Act, outperforms prior methods in terms of convergence speed and sample efficiency. The approach is also able to be combined with experience sharing to improve the performance of joint optimization. Experiments are conducted on giraffes, squids, and spiders. "
2453,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"view synthesis CONJUNCTION 3D object representation and rendering. 3D object representation and rendering CONJUNCTION view synthesis. Implicit neural representations USED-FOR tasks. multi - layer perceptrons ( MLPs ) USED-FOR Implicit neural representations. 3D object representation and rendering HYPONYM-OF tasks. view synthesis HYPONYM-OF tasks. MLP USED-FOR image, video, or 3D object. MLP USED-FOR training. coordinate - based MLPs USED-FOR implicit neural representations. inference CONJUNCTION training. training CONJUNCTION inference. CoordX USED-FOR initial layers. coordinate - based MLPs USED-FOR inference. coordinate - based MLPs USED-FOR training. layers USED-FOR intermediate features. accuracy EVALUATE-FOR baseline MLP. training CONJUNCTION inference. inference CONJUNCTION training. architecture USED-FOR implicit neural representation tasks. speedup EVALUATE-FOR baseline model. Generic are representations, approach, and them. Method is split MLP architecture. OtherScientificTerm is memory overheads. ","implicit neural representations based on multi-layer perceptrons (MLPs) have been shown to perform well on a variety of tasks such as view synthesis, 3D object representation and rendering. The authors propose a split MLP architecture, where each MLP is split into a set of initial layers, and each initial layer is used to learn a representation of the input image, video, or 3d object. These representations are then used to train a separate MLP for each of these tasks. The approach is based on the idea that a single MLP can be used to encode an image, a video, and a 3D scene into a single representation, and then the MLP will be able to be used for training and inference.   The authors show that coordinate-based MLPs are able to learn implicit neural representations, and that the initial layers of CoordX can be split into multiple initial layers. These additional layers are used to extract intermediate features, and the intermediate features are used as input to the final MLP.  They show that the accuracy of a baseline MLP trained with this architecture is comparable to a baseline model trained with the same number of layers, but with much smaller memory overheads. They also show that this architecture can be applied to other implicit neural representation tasks and achieve a speedup. ","implicit neural representations based on multi-layer perceptrons (MLPs) have been shown to perform well on a variety of tasks such as view synthesis, 3D object representation and rendering. The authors propose a split MLP architecture, where each MLP is split into a set of initial layers, and each initial layer is used to learn a representation of the input image, video, or 3d object. These representations are then used to train a separate MLP for each of these tasks. The approach is based on the idea that a single MLP can be used to encode an image, a video, and a 3D scene into a single representation, and then the MLP will be able to be used for training and inference.   The authors show that coordinate-based MLPs are able to learn implicit neural representations, and that the initial layers of CoordX can be split into multiple initial layers. These additional layers are used to extract intermediate features, and the intermediate features are used as input to the final MLP.  They show that the accuracy of a baseline MLP trained with this architecture is comparable to a baseline model trained with the same number of layers, but with much smaller memory overheads. They also show that this architecture can be applied to other implicit neural representation tasks and achieve a speedup. "
2469,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"method USED-FOR object - centric representations of visual scenes. appearance CONJUNCTION 3D pose. 3D pose CONJUNCTION appearance. structured representation USED-FOR shape. shape CONJUNCTION appearance. appearance CONJUNCTION shape. structured representation USED-FOR appearance. localized neural radiance field USED-FOR 2D views of the scene. object representation USED-FOR localized neural radiance field. differentiable rendering process USED-FOR localized neural radiance field. differentiable rendering process USED-FOR 2D views of the scene. reconstruction loss USED-FOR model. inferred scenes USED-FOR representations. 3D object representations USED-FOR visual reasoning task. CATER dataset USED-FOR 3D object representations. Method are INFERNO, and neural 3D rendering. OtherScientificTerm are annotations, rendered scenes, and supervision. ","This paper proposes a method for learning object-centric representations of visual scenes. The proposed method, INFERNO, learns a structured representation for shape, appearance, and 3D pose, and uses a differentiable rendering process to generate a localized neural radiance field from the object representation to produce 2D views of the scene. The model is trained with a reconstruction loss to ensure that the representations from inferred scenes are similar to the original scene, and that the annotations are consistent with the rendered scenes. Experiments on the CATER dataset show that the learned 3D object representations can be used to solve a visual reasoning task, and can be combined with neural 3D rendering. However, there is no additional supervision, and it is not clear how well the learned representations are transferable.","This paper proposes a method for learning object-centric representations of visual scenes. The proposed method, INFERNO, learns a structured representation for shape, appearance, and 3D pose, and uses a differentiable rendering process to generate a localized neural radiance field from the object representation to produce 2D views of the scene. The model is trained with a reconstruction loss to ensure that the representations from inferred scenes are similar to the original scene, and that the annotations are consistent with the rendered scenes. Experiments on the CATER dataset show that the learned 3D object representations can be used to solve a visual reasoning task, and can be combined with neural 3D rendering. However, there is no additional supervision, and it is not clear how well the learned representations are transferable."
2485,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"feature attribution framework USED-FOR GNN ’s prediction. features USED-FOR GNN ’s prediction. edges HYPONYM-OF features. subgraph USED-FOR model prediction. distribution shift USED-FOR out - ofdistribution problem. surrogate variable USED-FOR subgraphs. generative model USED-FOR unbiased estimation of subgraph importance. explanation fidelity EVALUATE-FOR DSE. Method are graph neural networks ( GNNs ), GNN, in - depth causal analysis, Deconfounded Subgraph Evaluation ( DSE ), and front - door adjustment. OtherScientificTerm are influential subgraph, subgraph importance, OOD effect, explanatory subgraph, and data distribution. Task is evaluation. ","This paper proposes a feature attribution framework for graph neural networks (GNNs) that aims to improve the performance of GNN’s prediction under an out-of-distribution (OOD) setting. The authors propose Deconfounded Subgraph Evaluation (DSE) that uses features (e.g., edges) from a subgraph to evaluate the model prediction under a distribution shift that is caused by an influential subgraph. They show that the distribution shift causes an OOD effect and that the subgraph importance of each subgraph is correlated with the importance of a surrogate variable. They then propose an in-depth causal analysis that shows that a GNN can be seen as a generative model that can be decoupled from the underlying subgraphs. They also show that DSE is unbiased in the sense that it can be used to perform an unbiased estimation of subgraph importantness and provide an explanation fidelity that is independent of the data distribution. Finally, the authors propose a front-door adjustment that allows for the evaluation to be performed in a way that is more robust to OOD effects. ","This paper proposes a feature attribution framework for graph neural networks (GNNs) that aims to improve the performance of GNN’s prediction under an out-of-distribution (OOD) setting. The authors propose Deconfounded Subgraph Evaluation (DSE) that uses features (e.g., edges) from a subgraph to evaluate the model prediction under a distribution shift that is caused by an influential subgraph. They show that the distribution shift causes an OOD effect and that the subgraph importance of each subgraph is correlated with the importance of a surrogate variable. They then propose an in-depth causal analysis that shows that a GNN can be seen as a generative model that can be decoupled from the underlying subgraphs. They also show that DSE is unbiased in the sense that it can be used to perform an unbiased estimation of subgraph importantness and provide an explanation fidelity that is independent of the data distribution. Finally, the authors propose a front-door adjustment that allows for the evaluation to be performed in a way that is more robust to OOD effects. "
2501,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"pretrained models COMPARE active learners. active learners COMPARE pretrained models. spurious correlations CONJUNCTION latent minority groups. latent minority groups CONJUNCTION spurious correlations. latent minority groups CONJUNCTION domain shifts. domain shifts CONJUNCTION latent minority groups. pretrained models COMPARE random sampling. random sampling COMPARE pretrained models. spurious correlations FEATURE-OF image and text datasets. data USED-FOR pretrained models. uncertainty sampling USED-FOR data. accuracy EVALUATE-FOR pretrained models. uncertainty sampling USED-FOR pretrained models. minority classes CONJUNCTION informative examples. informative examples CONJUNCTION minority classes. spurious feature CONJUNCTION class label. class label CONJUNCTION spurious feature. active learning COMPARE unpretrained models. unpretrained models COMPARE active learning. Active learning USED-FOR task ambiguity. Pretraining USED-FOR models. Pretraining USED-FOR task ambiguity. Pretraining USED-FOR active learners. active learners USED-FOR task ambiguity. disambiguating examples USED-FOR active learners. Method are machine learning systems, and pretraining process. Material is few - shot settings. OtherScientificTerm is shape. ","This paper studies the problem of disambiguation in few-shot learning, where the goal is to learn a classifier that disentangles the task-specific and task-invariant aspects of the data. The authors consider the setting where there is a large amount of data for which the classifier is trained on, but only a small amount of it is available for active learning. In this setting, the authors show that active learning outperforms unpretrained models in terms of accuracy, but not performance. They also show that the performance gap between pretrained models and active learners is smaller than that of random sampling.","This paper studies the problem of disambiguation in few-shot learning, where the goal is to learn a classifier that disentangles the task-specific and task-invariant aspects of the data. The authors consider the setting where there is a large amount of data for which the classifier is trained on, but only a small amount of it is available for active learning. In this setting, the authors show that active learning outperforms unpretrained models in terms of accuracy, but not performance. They also show that the performance gap between pretrained models and active learners is smaller than that of random sampling."
2517,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,GRAPHIX HYPONYM-OF pre - trained graph edit model. automatically detecting and fixing bugs CONJUNCTION code quality issues. code quality issues CONJUNCTION automatically detecting and fixing bugs. pre - trained graph edit model USED-FOR automatically detecting and fixing bugs. code quality issues FEATURE-OF Java programs. pre - trained graph edit model USED-FOR code quality issues. sequence - tosequence models COMPARE GRAPHIX. GRAPHIX COMPARE sequence - tosequence models. abstract syntax structure of code USED-FOR GRAPHIX. multi - head graph encoder USED-FOR GRAPHIX. model USED-FOR graph edit actions. graph edit actions USED-FOR automated program repair. model USED-FOR automated program repair. autoregressive tree decoder PART-OF model. pre - training strategy USED-FOR GRAPHIX. pre - training strategy USED-FOR model. implicit knowledge of program structures USED-FOR model. deleted sub - tree reconstruction HYPONYM-OF pre - training strategy. unlabeled source code USED-FOR implicit knowledge of program structures. bug fixing task USED-FOR downstream learning. pre - training objective CONJUNCTION bug fixing task. bug fixing task CONJUNCTION pre - training objective. pre - training objective USED-FOR downstream learning. abstract and concrete code USED-FOR GRAPHIX. Wild Java benchmark EVALUATE-FOR GRAPHIX. CodeBERT CONJUNCTION BART. BART CONJUNCTION CodeBERT. GRAPHIX COMPARE pre - trained Transformer models. pre - trained Transformer models COMPARE GRAPHIX. GRAPHIX COMPARE baselines. baselines COMPARE GRAPHIX. baselines COMPARE pre - trained Transformer models. pre - trained Transformer models COMPARE baselines. GRAPHIX COMPARE BART. BART COMPARE GRAPHIX. GRAPHIX COMPARE CodeBERT. CodeBERT COMPARE GRAPHIX. BART HYPONYM-OF baselines. CodeBERT HYPONYM-OF baselines. GRAPHIX USED-FOR structural and semantic code patterns. Material is abstract and concrete source code. ,"This paper proposes GRAPHIX, a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. Unlike previous sequence-tosequence models, GRAPHix is able to leverage the abstract syntax structure of code, which allows the model to learn graph edit actions that can be used for automated program repair. The model consists of an autoregressive tree decoder and a multi-head graph encoder. The pre-training strategy (deleted sub-tree reconstruction) is used to improve the model’s implicit knowledge of program structures from unlabeled source code. The paper also proposes a new downstream learning objective that combines the pre-learning objective with the bug fixing task. The authors evaluate GRAPHX on the Wild Java benchmark and show that the model can identify structural and semantic code patterns in both abstract and concrete source code, outperforming baselines such as CodeBERT and BART.","This paper proposes GRAPHIX, a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. Unlike previous sequence-tosequence models, GRAPHix is able to leverage the abstract syntax structure of code, which allows the model to learn graph edit actions that can be used for automated program repair. The model consists of an autoregressive tree decoder and a multi-head graph encoder. The pre-training strategy (deleted sub-tree reconstruction) is used to improve the model’s implicit knowledge of program structures from unlabeled source code. The paper also proposes a new downstream learning objective that combines the pre-learning objective with the bug fixing task. The authors evaluate GRAPHX on the Wild Java benchmark and show that the model can identify structural and semantic code patterns in both abstract and concrete source code, outperforming baselines such as CodeBERT and BART."
2533,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,Federated Adversarial Training ( FAT ) USED-FOR data privacy and governance issues. adversarial attack FEATURE-OF model robustness. inner - maximization optimization of Adversarial Training USED-FOR data heterogeneity. lower bound USED-FOR Federated Learning. convergence FEATURE-OF FAT. convergence EVALUATE-FOR α - weighted mechanism. adversarial learning methods CONJUNCTION federated optimization methods. federated optimization methods CONJUNCTION adversarial learning methods. α - WFAT COMPARE FAT. FAT COMPARE α - WFAT. benchmark datasets EVALUATE-FOR α - WFAT. benchmark datasets EVALUATE-FOR FAT. adversarial learning methods USED-FOR FAT. adversarial learning methods USED-FOR α - WFAT. Method is inner - maximization of Adversarial Training. ,"This paper proposes Federated Adversarial Training (FAT) to address the data privacy and governance issues of adversarial attack on model robustness in federated learning. Specifically, the authors propose a new lower bound for Federated Learning based on the inner-maximization optimization of Advertsarial Training. The convergence of FAT is shown to be non-trivial under the proposed α-weighted mechanism. The authors also show that α-WFAT outperforms FAT on a number of benchmark datasets. The paper also shows that the convergence of the proposed adversarial learning methods and federated optimization methods can be improved with the addition of α-wFAT.  ","This paper proposes Federated Adversarial Training (FAT) to address the data privacy and governance issues of adversarial attack on model robustness in federated learning. Specifically, the authors propose a new lower bound for Federated Learning based on the inner-maximization optimization of Advertsarial Training. The convergence of FAT is shown to be non-trivial under the proposed α-weighted mechanism. The authors also show that α-WFAT outperforms FAT on a number of benchmark datasets. The paper also shows that the convergence of the proposed adversarial learning methods and federated optimization methods can be improved with the addition of α-wFAT.  "
2549,SP:ff3c787512035e2af20778d53586752852196be9,"LML USED-FOR supervised learning. models USED-FOR semi - supervised continual learning exceptions. Mako HYPONYM-OF wrapper tool. wrapper tool PART-OF supervised LML frameworks. data programming USED-FOR wrapper tool. data programming USED-FOR Mako. Mako USED-FOR continual semi - supervised learning. labeled data USED-FOR continual semi - supervised learning. tool COMPARE fully labeled data. fully labeled data COMPARE tool. per - task accuracy CONJUNCTION resistance. resistance CONJUNCTION per - task accuracy. resistance EVALUATE-FOR catastrophic forgetting. resistance EVALUATE-FOR tool. per - task accuracy EVALUATE-FOR tool. CIFAR-10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. accuracy CONJUNCTION catastrophic forgetting prevention. catastrophic forgetting prevention CONJUNCTION accuracy. Mako USED-FOR unlabeled data. Mako USED-FOR LML tools. LML tools USED-FOR supervised learning. fully labeled data USED-FOR supervised learning. accuracy EVALUATE-FOR supervised learning. image classification data sets USED-FOR LML task sequences. CIFAR100 HYPONYM-OF image classification data sets. CIFAR-10 HYPONYM-OF image classification data sets. MNIST HYPONYM-OF image classification data sets. ORDisCo CONJUNCTION DistillMatch. DistillMatch CONJUNCTION ORDisCo. CNNL CONJUNCTION ORDisCo. ORDisCo CONJUNCTION CNNL. Mako COMPARE them. them COMPARE Mako. baseline semi - supervised LML tools COMPARE Mako. Mako COMPARE baseline semi - supervised LML tools. DistillMatch HYPONYM-OF baseline semi - supervised LML tools. CNNL HYPONYM-OF baseline semi - supervised LML tools. accuracy EVALUATE-FOR Mako. ORDisCo HYPONYM-OF baseline semi - supervised LML tools. Task are Lifelong machine learning ( LML ), and human learning process. Method is LML methods. OtherScientificTerm is knowledge base overhead. ","This paper proposes a new continual learning method called Mako, which is an extension of the Lifelong machine learning (LML) framework. Lifelong learning is an important problem in machine learning as it allows for lifelong learning in the presence of catastrophic forgetting. The authors propose a new method for continual semi-supervised continual learning (SSL) which is based on the idea of continual learning in a continual learning framework. The paper shows that the proposed method is able to achieve state-of-the-art performance in terms of per-task accuracy, resistance to catastrophic forgetting, and generalization to new tasks.","This paper proposes a new continual learning method called Mako, which is an extension of the Lifelong machine learning (LML) framework. Lifelong learning is an important problem in machine learning as it allows for lifelong learning in the presence of catastrophic forgetting. The authors propose a new method for continual semi-supervised continual learning (SSL) which is based on the idea of continual learning in a continual learning framework. The paper shows that the proposed method is able to achieve state-of-the-art performance in terms of per-task accuracy, resistance to catastrophic forgetting, and generalization to new tasks."
2565,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"adversarial examples USED-FOR Evading adversarial example detection defenses. attack techniques USED-FOR adversarial examples. Selective Projected Gradient Descent CONJUNCTION Orthogonal Projected Gradient Descent. Orthogonal Projected Gradient Descent CONJUNCTION Selective Projected Gradient Descent. Selective Projected Gradient Descent CONJUNCTION attack techniques. attack techniques CONJUNCTION Selective Projected Gradient Descent. technique USED-FOR detection defenses. accuracy EVALUATE-FOR technique. Generic are model, and attacks. OtherScientificTerm is gradients. Method is gradient - based attacks. Metric is detection rate. ","Evading adversarial example detection defenses using adversarial examples is an important problem. This paper proposes a new attack technique that uses gradient-based attacks to attack the model. The authors combine existing attack techniques such as Selective Projected Gradient Descent (SGP) and Orthogonal projected gradient descent (OGD) and attack techniques to generate adversarial attacks. The proposed technique is shown to improve detection defenses in terms of accuracy, detection rate, and the number of gradients used. ","Evading adversarial example detection defenses using adversarial examples is an important problem. This paper proposes a new attack technique that uses gradient-based attacks to attack the model. The authors combine existing attack techniques such as Selective Projected Gradient Descent (SGP) and Orthogonal projected gradient descent (OGD) and attack techniques to generate adversarial attacks. The proposed technique is shown to improve detection defenses in terms of accuracy, detection rate, and the number of gradients used. "
2581,SP:5eef907024017849303477eed92f317438c87a69,data valuation CONJUNCTION model valuation. model valuation CONJUNCTION data valuation. feature interpretation CONJUNCTION data valuation. data valuation CONJUNCTION feature interpretation. model valuation USED-FOR ensembles. Valuation problems PART-OF machine learning applications. model valuation HYPONYM-OF Valuation problems. data valuation HYPONYM-OF Valuation problems. feature interpretation HYPONYM-OF Valuation problems. Shapley value CONJUNCTION Banzhaf value. Banzhaf value CONJUNCTION Shapley value. game - theoretic criteria USED-FOR problems. Banzhaf value HYPONYM-OF game - theoretic criteria. Shapley value HYPONYM-OF game - theoretic criteria. energy - based treatment USED-FOR cooperative games. maximum entropy principle USED-FOR energy - based treatment. one - step fixed point iteration USED-FOR ELBO objective. mean - field variational inference USED-FOR classical game - theoretic valuation criteria. mean - field variational inference USED-FOR energy - based model. one - step fixed point iteration USED-FOR classical game - theoretic valuation criteria. uniform initializations USED-FOR variational valuations. game - theoretic axioms FEATURE-OF variational valuations. decoupling error CONJUNCTION valuation. valuation CONJUNCTION decoupling error. valuation FEATURE-OF synthetic and real - world valuation problems. valuation EVALUATE-FOR Variational Index. decoupling error EVALUATE-FOR Variational Index. synthetic and real - world valuation problems EVALUATE-FOR Variational Index. Generic is criteria. Method is fixed point iteration. ,"Valuation problems in machine learning applications such as feature interpretation, data valuation, model valuation for ensembles, etc.  Valuation problems can be decomposed into Shapley value, Banzhaf value, and other game-theoretic criteria.    The paper proposes two new problems based on the game-thruristics of game theoretic criteria:   1. A new energy-based treatment for cooperative games based on maximum entropy principle.  2. A one-step fixed point iteration for the ELBO objective based on mean-field variational inference for classical game-tertiary valuation criteria. The paper shows that variational valuations with uniform initializations can be derived from game-iterative axioms.  3. Experiments on synthetic and real-world valuation problems show that the Variational Index has a decoupling error and a good trade-off between decoupled error and valuation.","Valuation problems in machine learning applications such as feature interpretation, data valuation, model valuation for ensembles, etc.  Valuation problems can be decomposed into Shapley value, Banzhaf value, and other game-theoretic criteria.    The paper proposes two new problems based on the game-thruristics of game theoretic criteria:   1. A new energy-based treatment for cooperative games based on maximum entropy principle.  2. A one-step fixed point iteration for the ELBO objective based on mean-field variational inference for classical game-tertiary valuation criteria. The paper shows that variational valuations with uniform initializations can be derived from game-iterative axioms.  3. Experiments on synthetic and real-world valuation problems show that the Variational Index has a decoupling error and a good trade-off between decoupled error and valuation."
2597,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"out - of - sample prediction error FEATURE-OF Epistemic uncertainty. approach USED-FOR epistemic uncertainty. intrinsic unpredictability HYPONYM-OF estimate of aleatoric uncertainty. active learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION active learning. estimator USED-FOR interactive learning environments. estimator USED-FOR epistemic uncertainty. active learning USED-FOR interactive learning environments. reinforcement learning USED-FOR interactive learning environments. sequential model optimization CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION sequential model optimization. methods USED-FOR uncertainty estimation. methods USED-FOR tasks. uncertainty estimation USED-FOR tasks. sequential model optimization HYPONYM-OF tasks. reinforcement learning HYPONYM-OF tasks. uncertainty estimates USED-FOR estimating uncertainty. DEUP USED-FOR probabilistic classification of images. uncertainty estimates USED-FOR probabilistic classification of images. synergistic drug combinations FEATURE-OF estimating uncertainty. DEUP USED-FOR uncertainty estimates. OtherScientificTerm are model variance, and generalization error. Method is Direct Epistemic Uncertainty Prediction ( DEUP ). ","Epistemic uncertainty is a measure of the out-of-sample prediction error, which is the difference between the model variance and the generalization error. This paper proposes a new approach to measure epistemic uncertainty, i.e., the estimate of aleatoric uncertainty (i.e. intrinsic unpredictability). The proposed estimator, Direct Epistemic Uncertainty Prediction (DEUP), is applied to interactive learning environments with active learning and reinforcement learning. The paper shows that DEUP outperforms existing methods for uncertainty estimation on a variety of tasks including sequential model optimization, reinforcement learning, and active learning. DEUP is also applied to probabilistic classification of images using uncertainty estimates for estimating uncertainty in the presence of synergistic drug combinations.","Epistemic uncertainty is a measure of the out-of-sample prediction error, which is the difference between the model variance and the generalization error. This paper proposes a new approach to measure epistemic uncertainty, i.e., the estimate of aleatoric uncertainty (i.e. intrinsic unpredictability). The proposed estimator, Direct Epistemic Uncertainty Prediction (DEUP), is applied to interactive learning environments with active learning and reinforcement learning. The paper shows that DEUP outperforms existing methods for uncertainty estimation on a variety of tasks including sequential model optimization, reinforcement learning, and active learning. DEUP is also applied to probabilistic classification of images using uncertainty estimates for estimating uncertainty in the presence of synergistic drug combinations."
2613,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,Product quantization ( PQ ) USED-FOR approximate nearest neighbor ( ANN ) search systems. Product quantization ( PQ ) CONJUNCTION space rotation. space rotation CONJUNCTION Product quantization ( PQ ). disk storage USED-FOR embeddings. rotation learning methods USED-FOR quantization distortion. quantization distortion FEATURE-OF fixed embeddings. block Givens coordinate descent algorithms USED-FOR rotation matrix. geometric intuitions USED-FOR block Givens coordinate descent algorithms. Lie group theory USED-FOR geometric intuitions. special orthogonal group SO(n ) HYPONYM-OF geometric intuitions. special orthogonal group SO(n ) HYPONYM-OF Lie group theory. SVD method COMPARE Givens algorithms. Givens algorithms COMPARE SVD method. runtime EVALUATE-FOR GPUs. GPUs USED-FOR Givens algorithms. runtime EVALUATE-FOR Givens algorithms. vanilla product quantization USED-FOR end - to - end training scenario. vanilla product quantization USED-FOR They. end - to - end training scenario EVALUATE-FOR They. Task is inner product computation. OtherScientificTerm is convex objectives. ,"Product quantization (PQ) and space rotation are popular techniques for approximate nearest neighbor (ANN) search systems. Product quantization has been shown to reduce the quantization distortion of fixed embeddings on disk storage, but the inner product computation is computationally intractable due to the need to compute the embedding matrix. This paper proposes to use rotation learning methods to alleviate the quantisation distortion. The authors propose block Givens coordinate descent algorithms to learn the rotation matrix, based on geometric intuitions from Lie group theory (e.g., special orthogonal group SO(n)) and convex objectives. They show that the SVD method is more computationally efficient than existing GivENS algorithms on GPUs with much smaller runtime. They also show that vanilla product quantization can be used in an end-to-end training scenario, which is a nice contribution to the literature.","Product quantization (PQ) and space rotation are popular techniques for approximate nearest neighbor (ANN) search systems. Product quantization has been shown to reduce the quantization distortion of fixed embeddings on disk storage, but the inner product computation is computationally intractable due to the need to compute the embedding matrix. This paper proposes to use rotation learning methods to alleviate the quantisation distortion. The authors propose block Givens coordinate descent algorithms to learn the rotation matrix, based on geometric intuitions from Lie group theory (e.g., special orthogonal group SO(n)) and convex objectives. They show that the SVD method is more computationally efficient than existing GivENS algorithms on GPUs with much smaller runtime. They also show that vanilla product quantization can be used in an end-to-end training scenario, which is a nice contribution to the literature."
2629,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"sensory information USED-FOR conceptual abstractions. Structure Mapping Theory USED-FOR human analogical reasoning. ( higher - order ) relations USED-FOR analogical mappings. two - stage neural framework USED-FOR visual analogies. Raven ’s Progressive Matrices HYPONYM-OF abstract visual reasoning test of fluid intelligence. Neural Structure Mapping ( NSM ) HYPONYM-OF two - stage neural framework. Raven ’s Progressive Matrices USED-FOR visual analogies. multi - task visual relationship encoder CONJUNCTION neural module net - based analogy inference engine. neural module net - based analogy inference engine CONJUNCTION multi - task visual relationship encoder. neural module net - based analogy inference engine USED-FOR framework. raw visual input USED-FOR multi - task visual relationship encoder. multi - task visual relationship encoder USED-FOR framework. structure USED-FOR analogical reasoning. NSM approach USED-FOR relational structure. Generic is them. Task are human intelligence, and Abstract reasoning. OtherScientificTerm are analogies, and novel domains. Material is known domains. Method is machine learning ( ML ) models. ","This paper proposes a two-stage neural framework called Neural Structure Mapping (NSM) to learn visual analogies based on Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence, which uses sensory information to learn conceptual abstractions. The authors draw inspiration from the structure Mapping Theory for human analogical reasoning, and show that (higher-order) relations can be used to learn analogical mappings between two images. The proposed framework uses a multi-task visual relationship encoder with raw visual input and a neural module net-based analogy inference engine to learn the framework. The paper shows that the NSM approach is able to learn a relational structure that can be transferred to novel domains, and that it can be applied to existing machine learning (ML) models as well.  Abstract reasoning is an important problem in many fields, and it is important to be aware that existing works on this topic are limited to known domains. It is also important to note that it is not clear that this structure is useful for the problem of learning analogial reasoning.  ","This paper proposes a two-stage neural framework called Neural Structure Mapping (NSM) to learn visual analogies based on Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence, which uses sensory information to learn conceptual abstractions. The authors draw inspiration from the structure Mapping Theory for human analogical reasoning, and show that (higher-order) relations can be used to learn analogical mappings between two images. The proposed framework uses a multi-task visual relationship encoder with raw visual input and a neural module net-based analogy inference engine to learn the framework. The paper shows that the NSM approach is able to learn a relational structure that can be transferred to novel domains, and that it can be applied to existing machine learning (ML) models as well.  Abstract reasoning is an important problem in many fields, and it is important to be aware that existing works on this topic are limited to known domains. It is also important to note that it is not clear that this structure is useful for the problem of learning analogial reasoning.  "
2645,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,contrastive self - supervised method USED-FOR nucleotide genome representation learning. Self - GenomeNet HYPONYM-OF contrastive self - supervised method. Self - GenomeNet HYPONYM-OF self - supervised framework. method USED-FOR latent space. reverse - complement of genomic sequences USED-FOR method. reverse - complement of genomic sequences USED-FOR latent space. framework USED-FOR semantic representations. features USED-FOR context network. context network USED-FOR framework. encoder network USED-FOR features. context network USED-FOR semantic representations. unsupervised contrastive loss USED-FOR network. method COMPARE deep learning methods. deep learning methods COMPARE method. self - supervised and semi - supervised settings USED-FOR deep learning methods. self - supervised and semi - supervised settings USED-FOR method. representations USED-FOR datasets. Material is nucleotide genomic data. OtherScientificTerm is domain - specific characteristics. ,"This paper proposes a contrastive self-supervised method for nucleotide genome representation learning, called Self-GenomeNet, which is an extension of the popular self supervised framework SelfGenome. The proposed method learns a latent space based on the reverse-computation of genomic sequences. The authors propose a framework to learn semantic representations using a context network and an encoder network. The network is trained with an unsupervised contrastive loss, and the proposed method is shown to outperform state-of-the-art deep learning methods in both self -supervised and semi-supervision settings. Experiments are conducted on two datasets with different types of nucleotide genomic data, and show that the proposed representations are more robust to domain-specific characteristics. ","This paper proposes a contrastive self-supervised method for nucleotide genome representation learning, called Self-GenomeNet, which is an extension of the popular self supervised framework SelfGenome. The proposed method learns a latent space based on the reverse-computation of genomic sequences. The authors propose a framework to learn semantic representations using a context network and an encoder network. The network is trained with an unsupervised contrastive loss, and the proposed method is shown to outperform state-of-the-art deep learning methods in both self -supervised and semi-supervision settings. Experiments are conducted on two datasets with different types of nucleotide genomic data, and show that the proposed representations are more robust to domain-specific characteristics. "
2661,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"low - level vision theory USED-FOR steerable filters. steerable feed - forward learning - based approach USED-FOR point clouds. spherical decision surfaces PART-OF steerable feed - forward learning - based approach. 3D steerability constraint USED-FOR hypersphere neurons. conformal embedding of Euclidean space USED-FOR 3D steerability constraint. conformal embedding of Euclidean space USED-FOR hypersphere neurons. synthetic point set CONJUNCTION real - world 3D skeleton data. real - world 3D skeleton data CONJUNCTION synthetic point set. spherical filter banks USED-FOR invariant class predictions. online optimization USED-FOR invariant class predictions. invariant class predictions USED-FOR known point sets. online optimization USED-FOR spherical filter banks. unknown orientations FEATURE-OF invariant class predictions. unknown orientations FEATURE-OF known point sets. Method is steerable convolutional neural networks. OtherScientificTerm are 3D geometry, rotational equivariance, and model parameters. Task is learning representations of point sets. ","This paper proposes steerable convolutional neural networks that are steerable in 3D geometry. The authors draw inspiration from steerable filters in low-level vision theory and propose a steerable feed-forward learning-based approach for point clouds with spherical decision surfaces. The 3D steerability constraint is imposed on the hypersphere neurons via a conformal embedding of Euclidean space, and the authors show that the 3D steerability constraint can be used to train hyperspheres neurons that are invariant to rotational equivariance. Experiments are conducted on a synthetic point set and real-world 3D skeleton data. The results show that online optimization of spherical filter banks is able to obtain invariant class predictions for known point sets with unknown orientations, and that the model parameters are rotational invariant. The paper also shows that learning representations of point sets is computationally efficient. ","This paper proposes steerable convolutional neural networks that are steerable in 3D geometry. The authors draw inspiration from steerable filters in low-level vision theory and propose a steerable feed-forward learning-based approach for point clouds with spherical decision surfaces. The 3D steerability constraint is imposed on the hypersphere neurons via a conformal embedding of Euclidean space, and the authors show that the 3D steerability constraint can be used to train hyperspheres neurons that are invariant to rotational equivariance. Experiments are conducted on a synthetic point set and real-world 3D skeleton data. The results show that online optimization of spherical filter banks is able to obtain invariant class predictions for known point sets with unknown orientations, and that the model parameters are rotational invariant. The paper also shows that learning representations of point sets is computationally efficient. "
2677,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,Pre - trained language models ( PLMs ) USED-FOR continual learning. continual learning USED-FOR natural language problems. continual learning methods CONJUNCTION PLMs. PLMs CONJUNCTION continual learning methods. PLMs CONJUNCTION CL approaches. CL approaches CONJUNCTION PLMs. benchmarks EVALUATE-FOR continual learning. benchmarks EVALUATE-FOR CL approaches. PLMs CONJUNCTION CL methods. CL methods CONJUNCTION PLMs. representativeness probing analyses USED-FOR PLMs ’ performance characteristics. representativeness probing analyses USED-FOR layer - wise and task - wise manner. layer - wise and task - wise manner FEATURE-OF PLMs ’ performance characteristics. Task is Continual learning ( CL ). Generic is model. OtherScientificTerm is forgetting. Method is continual learning techniques. ,"This paper studies the problem of continual learning (CL) in the context of pre-trained language models (PLMs) for continual learning on natural language problems. Continual learning methods, PLMs, and PLMs and CL approaches are evaluated on a number of benchmarks that are commonly used in continual learning. The paper provides representativeness probing analyses of PLMs’ performance characteristics in a layer-wise and task-wise manner, and compares the performance of different PLMs with different CL methods on a variety of benchmarks. The authors find that the model performs well on most of the benchmarks, and that PLMs are able to generalize to new tasks without forgetting. They also find that existing continual learning techniques do not generalize well.","This paper studies the problem of continual learning (CL) in the context of pre-trained language models (PLMs) for continual learning on natural language problems. Continual learning methods, PLMs, and PLMs and CL approaches are evaluated on a number of benchmarks that are commonly used in continual learning. The paper provides representativeness probing analyses of PLMs’ performance characteristics in a layer-wise and task-wise manner, and compares the performance of different PLMs with different CL methods on a variety of benchmarks. The authors find that the model performs well on most of the benchmarks, and that PLMs are able to generalize to new tasks without forgetting. They also find that existing continual learning techniques do not generalize well."
2693,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"model poisoning attacks COMPARE centralized learning. centralized learning COMPARE model poisoning attacks. Federated learning COMPARE centralized learning. centralized learning COMPARE Federated learning. Federated learning USED-FOR model poisoning attacks. multi - party, distributed learning HYPONYM-OF Federated learning. model updates USED-FOR global model. Bulyan CONJUNCTION FABA. FABA CONJUNCTION Bulyan. FABA CONJUNCTION FoolsGold. FoolsGold CONJUNCTION FABA. Krum CONJUNCTION Bulyan. Bulyan CONJUNCTION Krum. FoolsGold HYPONYM-OF Byzantine - resilient federated learning algorithms. Krum HYPONYM-OF Byzantine - resilient federated learning algorithms. FABA HYPONYM-OF Byzantine - resilient federated learning algorithms. Bulyan HYPONYM-OF Byzantine - resilient federated learning algorithms. defense USED-FOR directed deviation attack. TESSERACT HYPONYM-OF defense. TESSERACT USED-FOR directed deviation attack. learning algorithms CONJUNCTION models. models CONJUNCTION learning algorithms. reputation scores USED-FOR TESSERACT. TESSERACT USED-FOR attack. robustness EVALUATE-FOR TESSERACT. Method are untargeted model poisoning attack, and federated learning. OtherScientificTerm are gradient updates, and gradient flips. Metric is test error rate. Task is model poisoning attack. ","This paper studies the problem of untargeted model poisoning attack in federated learning. Federated learning (i.e., multi-party, distributed learning) has been shown to be more vulnerable to model poisoning attacks than centralized learning. The authors argue that this is due to the fact that the global model updates for each client are distributed across multiple clients, which makes it difficult for the attacker to directly target the gradient updates of each client. To mitigate this problem, the authors propose a new Byzantine-resilient federated training algorithm, called TESSERACT, which is able to defend against a directed deviation attack. The proposed defense is based on the observation that the test error rate of the attacker is higher when the gradient flips are larger than the gradient of the local model. They also show that TESSerACT is robust to the attack based on reputation scores.   The authors evaluate the performance of Byzantine-Resilient Federated Learning algorithms such as Krum, Bulyan, FABA, and FoolsGold on three datasets. They show that the proposed defense TessERACT is more robust to directed deviation attacks than the baselines. They further show that their defense improves the robustness of the attack against a variety of different learning algorithms and models.  The paper also shows that the attack is more difficult to defend when gradient flips happen at the local level. The paper concludes with a discussion on the effect of gradient flips on the model performance in the presence of a model poisoning threat.","This paper studies the problem of untargeted model poisoning attack in federated learning. Federated learning (i.e., multi-party, distributed learning) has been shown to be more vulnerable to model poisoning attacks than centralized learning. The authors argue that this is due to the fact that the global model updates for each client are distributed across multiple clients, which makes it difficult for the attacker to directly target the gradient updates of each client. To mitigate this problem, the authors propose a new Byzantine-resilient federated training algorithm, called TESSERACT, which is able to defend against a directed deviation attack. The proposed defense is based on the observation that the test error rate of the attacker is higher when the gradient flips are larger than the gradient of the local model. They also show that TESSerACT is robust to the attack based on reputation scores.   The authors evaluate the performance of Byzantine-Resilient Federated Learning algorithms such as Krum, Bulyan, FABA, and FoolsGold on three datasets. They show that the proposed defense TessERACT is more robust to directed deviation attacks than the baselines. They further show that their defense improves the robustness of the attack against a variety of different learning algorithms and models.  The paper also shows that the attack is more difficult to defend when gradient flips happen at the local level. The paper concludes with a discussion on the effect of gradient flips on the model performance in the presence of a model poisoning threat."
2709,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"regularization CONJUNCTION model selection. model selection CONJUNCTION regularization. debiasing USED-FOR regularization. debiasing USED-FOR model selection. debiasing USED-FOR √ n - consistent and asymptotically normal estimation. double robustness CONJUNCTION Neyman orthogonality. Neyman orthogonality CONJUNCTION double robustness. functional - specific theoretical derivation USED-FOR influence function. correction term PART-OF plug - in estimator of the functional. Neural Nets CONJUNCTION Random Forests. Random Forests CONJUNCTION Neural Nets. Neural Nets USED-FOR Riesz representation of the linear functional. Random Forests USED-FOR Riesz representation of the linear functional. Neural Nets USED-FOR automatic debiasing procedure. Random Forests USED-FOR automatic debiasing procedure. value query oracle access USED-FOR linear functional. value query oracle access USED-FOR method. representation layers USED-FOR functions. stochastic gradient descent minimization USED-FOR Riesz representer and regression loss. stochastic gradient descent minimization USED-FOR multi - tasking Neural Net debiasing method. Random Forest method USED-FOR locally linear representation of the Riesz function. methodology USED-FOR arbitrary functionals. neural net based estimator USED-FOR average treatment effect functional. it COMPARE neural net based estimator. neural net based estimator COMPARE it. method USED-FOR estimating average marginal effects. gasoline demand FEATURE-OF semi - synthetic data of gasoline price changes. semi - synthetic data of gasoline price changes EVALUATE-FOR method. continuous treatments USED-FOR estimating average marginal effects. Task are causal and policy effects of interest, and Debiasing. ","This paper considers the problem of causal and policy effects of interest, i.e., the effect of an intervention on the outcome of a particular treatment on the treatment effect of interest. Debiasing is a popular technique for regularization and model selection, and debiasing has been shown to improve the performance of the regularization as well as model selection. This paper proposes to use debiased neural networks to achieve the equivalence between the√ n-consistent and asymptotically normal estimation. The authors propose a functional-specific theoretical derivation of the influence function, and propose a plug-in estimator of the functional based on a correction term. They use Neural Nets and Random Forests to learn the Riesz representation of the linear functional, and then use the Neural Nets to perform an automatic debiase procedure. The proposed method is based on value query oracle access to a linear functional and uses stochastic gradient descent minimization to minimize the difference between the Riedz representer and regression loss. They also propose a multi-tasking Neural Net debiusing method based on the multi-tasking and multi-objective nature of the proposed method.   The authors show that the proposed methodology can be applied to arbitrary functionals, and that the representation layers can be used to learn functions with double robustness and Neyman orthogonality. They show that their method outperforms a neural net based estimator for the average treatment effect functional and show that it is more robust than the neural net-based estimator.  They also show that they can learn a locally linear representation of a function using the Random Forest method, which is a variant of their method. Finally, the method is applied to estimating average marginal effects of continuous treatments, and is shown to perform well on semi-synthetic data of gasoline demand. ","This paper considers the problem of causal and policy effects of interest, i.e., the effect of an intervention on the outcome of a particular treatment on the treatment effect of interest. Debiasing is a popular technique for regularization and model selection, and debiasing has been shown to improve the performance of the regularization as well as model selection. This paper proposes to use debiased neural networks to achieve the equivalence between the√ n-consistent and asymptotically normal estimation. The authors propose a functional-specific theoretical derivation of the influence function, and propose a plug-in estimator of the functional based on a correction term. They use Neural Nets and Random Forests to learn the Riesz representation of the linear functional, and then use the Neural Nets to perform an automatic debiase procedure. The proposed method is based on value query oracle access to a linear functional and uses stochastic gradient descent minimization to minimize the difference between the Riedz representer and regression loss. They also propose a multi-tasking Neural Net debiusing method based on the multi-tasking and multi-objective nature of the proposed method.   The authors show that the proposed methodology can be applied to arbitrary functionals, and that the representation layers can be used to learn functions with double robustness and Neyman orthogonality. They show that their method outperforms a neural net based estimator for the average treatment effect functional and show that it is more robust than the neural net-based estimator.  They also show that they can learn a locally linear representation of a function using the Random Forest method, which is a variant of their method. Finally, the method is applied to estimating average marginal effects of continuous treatments, and is shown to perform well on semi-synthetic data of gasoline demand. "
2725,SP:96e1da163020441f9724985ae15674233e0cfe0d,"finite - time convergence CONJUNCTION sample complexity. sample complexity CONJUNCTION finite - time convergence. sample complexity EVALUATE-FOR algorithm. single - agent actorcritic algorithms USED-FOR reinforcement learning. sample complexity bound EVALUATE-FOR single - agent actorcritic algorithms. Method is actor - critic algorithm. OtherScientificTerm are average reward, global average reward, and communication network. Generic is problem. Task is MARL setting. ","This paper studies the problem of learning an actor-critic algorithm in a MARL setting, where the agent is given an average reward and the critic is given a global average reward. The problem is formulated as learning a communication network between two agents, and the goal is to learn a policy that maximizes the average of the two agents' rewards. The authors prove a finite-time convergence and sample complexity for the proposed algorithm. They also provide a sample complexity bound for single-agent actorcritic algorithms for reinforcement learning. ","This paper studies the problem of learning an actor-critic algorithm in a MARL setting, where the agent is given an average reward and the critic is given a global average reward. The problem is formulated as learning a communication network between two agents, and the goal is to learn a policy that maximizes the average of the two agents' rewards. The authors prove a finite-time convergence and sample complexity for the proposed algorithm. They also provide a sample complexity bound for single-agent actorcritic algorithms for reinforcement learning. "
2741,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"contrastive learning USED-FOR large - scale self - supervised learning. approach USED-FOR large - scale self - supervised learning. contrastive learning HYPONYM-OF approach. conditional independence assumption USED-FOR guarantee. theory COMPARE contrastive methods. contrastive methods COMPARE theory. synthetic and real - world datasets EVALUATE-FOR contrastive methods. synthetic and real - world datasets EVALUATE-FOR theory. contrastive learning USED-FOR class - separated representations. Generic is it. OtherScientificTerm are positive samples, and intra - class samples. Task is surrogate task. ","This paper proposes a new approach, called contrastive learning, for large-scale self-supervised learning, where the goal is to learn class-separated representations from positive samples and negative samples. The main contribution of the paper is the guarantee that the learned representations are independent of each other under the conditional independence assumption. The theory is tested on both synthetic and real-world datasets, and compared with other contrastive methods, and it is shown that the proposed method outperforms them both in terms of performance. The paper also provides a surrogate task that can be used to compare the performance of different classes.   ","This paper proposes a new approach, called contrastive learning, for large-scale self-supervised learning, where the goal is to learn class-separated representations from positive samples and negative samples. The main contribution of the paper is the guarantee that the learned representations are independent of each other under the conditional independence assumption. The theory is tested on both synthetic and real-world datasets, and compared with other contrastive methods, and it is shown that the proposed method outperforms them both in terms of performance. The paper also provides a surrogate task that can be used to compare the performance of different classes.   "
2757,SP:b491314336c503b276e34e410cf461cb81294890,"Speech restoration USED-FOR distortions in speech signals. speech denoising CONJUNCTION speech declipping. speech declipping CONJUNCTION speech denoising. Prior methods USED-FOR single - task speech restoration ( SSR ). speech declipping HYPONYM-OF single - task speech restoration ( SSR ). speech denoising HYPONYM-OF single - task speech restoration ( SSR ). SSR systems USED-FOR speech restoration tasks. SSR systems USED-FOR speech restoration problem. speech super - resolution HYPONYM-OF speech restoration tasks. generative framework USED-FOR GSR task. VoiceFixer1 HYPONYM-OF generative framework. VoiceFixer1 USED-FOR GSR task. analysis stage CONJUNCTION synthesis stage. synthesis stage CONJUNCTION analysis stage. VoiceFixer USED-FOR speech analysis. synthesis stage USED-FOR speech analysis. synthesis stage PART-OF VoiceFixer. analysis stage PART-OF VoiceFixer. ResUNet USED-FOR analysis stage. neural vocoder USED-FOR synthesis stage. ResUNet CONJUNCTION neural vocoder. neural vocoder CONJUNCTION ResUNet. neural vocoder USED-FOR analysis stage. additive noise CONJUNCTION room reverberation. room reverberation CONJUNCTION additive noise. room reverberation CONJUNCTION low - resolution, and clipping distortions. low - resolution, and clipping distortions CONJUNCTION room reverberation. additive noise USED-FOR VoiceFixer. room reverberation FEATURE-OF VoiceFixer. low - resolution, and clipping distortions EVALUATE-FOR VoiceFixer. baseline GSR model COMPARE speech denoising SSR model. speech denoising SSR model COMPARE baseline GSR model. mean opinion score ( MOS ) EVALUATE-FOR speech denoising SSR model. mean opinion score ( MOS ) EVALUATE-FOR baseline GSR model. VoiceFixer COMPARE GSR baseline model. GSR baseline model COMPARE VoiceFixer. MOS score EVALUATE-FOR GSR baseline model. MOS score EVALUATE-FOR VoiceFixer. old movies CONJUNCTION historical speeches. historical speeches CONJUNCTION old movies. VoiceFixer USED-FOR historical speeches.","Speech restoration aims to remove distortions in speech signals. Prior methods have focused on single-task speech restoration (SSR) such as speech denoising and speech declipping. However, existing SSR systems are limited to speech restoration tasks such as ""speech super-resolution"". To tackle the speech restoration problem, this paper proposes a generative framework called VoiceFixer1 to tackle the GSR task.    The proposed voice fixer consists of an analysis stage based on ResUNet and a neural vocoder for the synthesis stage for speech analysis.  The authors show that the Voice fixer is robust to additive noise, room reverberation, low-resolution, and clipping distortions.  Experiments are conducted on old movies and historical speeches, and the Voicefixer outperforms the baseline GSR model in terms of mean opinion score (MOS) and outperforms a standard GSR baseline model in the presence of clipping distortions and low-resolution. ","Speech restoration aims to remove distortions in speech signals. Prior methods have focused on single-task speech restoration (SSR) such as speech denoising and speech declipping. However, existing SSR systems are limited to speech restoration tasks such as ""speech super-resolution"". To tackle the speech restoration problem, this paper proposes a generative framework called VoiceFixer1 to tackle the GSR task.    The proposed voice fixer consists of an analysis stage based on ResUNet and a neural vocoder for the synthesis stage for speech analysis.  The authors show that the Voice fixer is robust to additive noise, room reverberation, low-resolution, and clipping distortions.  Experiments are conducted on old movies and historical speeches, and the Voicefixer outperforms the baseline GSR model in terms of mean opinion score (MOS) and outperforms a standard GSR baseline model in the presence of clipping distortions and low-resolution. "
2773,SP:c80a7392ec6147395a664734601fb389a1eb4470,"Residual Tensor Networks ( MVSRTN ) USED-FOR multivariate time series. Residual Tensor Networks ( MVSRTN ) USED-FOR Variable Space. tensor network USED-FOR variable space. low - rank approximation USED-FOR variable space. low - rank approximation USED-FOR tensor network. translation invariance FEATURE-OF network. tensor components USED-FOR translation invariance. tensor components USED-FOR network. it USED-FOR space - approximated tensor network. seriesvariable encoder USED-FOR variable space. skip - connection layer USED-FOR dissemination of information. scale HYPONYM-OF dissemination of information. multivariate time series forecasting benchmark datasets EVALUATE-FOR method. Material is Multivariate time series. OtherScientificTerm are latent space, time window, and long - term sequences. Generic is framework. Method is N - order residual connection approach. ","Multivariate time series is a challenging problem in which the latent space is multivariate. This paper proposes Residual Tensor Networks (MVSRTN) for Multivariate Time series to learn a Variable Space, which is based on the idea that the tensor network in the variable space is a low-rank approximation of the original latent space. The proposed framework is a generalization of the N-order residual connection approach. The key idea is to use tensor components of the network to ensure translation invariance, and then use it to train a space-approximated tensornet. The seriesvariable encoder is used to learn the variable representation of the time series, and a skip-connection layer is added to encourage the dissemination of information (in terms of scale) across the time window. The method is evaluated on multdimensional time series forecasting benchmark datasets, and is shown to outperform the previous state-of-the-art method.  ","Multivariate time series is a challenging problem in which the latent space is multivariate. This paper proposes Residual Tensor Networks (MVSRTN) for Multivariate Time series to learn a Variable Space, which is based on the idea that the tensor network in the variable space is a low-rank approximation of the original latent space. The proposed framework is a generalization of the N-order residual connection approach. The key idea is to use tensor components of the network to ensure translation invariance, and then use it to train a space-approximated tensornet. The seriesvariable encoder is used to learn the variable representation of the time series, and a skip-connection layer is added to encourage the dissemination of information (in terms of scale) across the time window. The method is evaluated on multdimensional time series forecasting benchmark datasets, and is shown to outperform the previous state-of-the-art method.  "
2789,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"Neighbor sampling USED-FOR Graph Neural Networks ( GNNs ). large graphs USED-FOR Graph Neural Networks ( GNNs ). Stochastic Compositional Optimization ( SCO ) problems USED-FOR samplingbased GNN training. SCO algorithms USED-FOR samplingbased GNN training. SCO algorithms USED-FOR GNNs. moving averages USED-FOR aggregated features. moving averages USED-FOR they. large graphs USED-FOR GNNs. GPU memory limit CONJUNCTION CPU memory limit. CPU memory limit CONJUNCTION GPU memory limit. GPU memory limit FEATURE-OF moving averages. SCO algorithms USED-FOR GNN training. sparse moving averages USED-FOR SCO algorithms. moving averages USED-FOR algorithm. fixed size buffer USED-FOR algorithm. convergence rate EVALUATE-FOR SCO algorithm. algorithm USED-FOR SCO algorithm. convergence rate EVALUATE-FOR algorithm. Adam SGD USED-FOR GNN training. algorithm COMPARE Adam SGD. Adam SGD COMPARE algorithm. algorithm USED-FOR GNN training. memory overhead EVALUATE-FOR algorithm. OtherScientificTerm are graph, graph size, and buffer size. ","Neighbor sampling is a popular technique for training Graph Neural Networks (GNNs) on large graphs. In this paper, the authors consider the Stochastic Compositional Optimization (SCO) problems for samplingbased GNN training. They show that existing SCO algorithms for sampling based GNNs are computationally expensive to compute, and they propose to use moving averages to compute aggregated features instead. The authors also show that the moving averages have a GPU memory limit and a CPU memory limit.    The main contribution of this paper is that the authors propose a new algorithm that uses moving averages instead of a fixed size buffer, which is more efficient for graph size and memory. They also provide a convergence rate for the proposed SCO algorithm that matches the convergence rate of Adam SGD for the same size of the graph.  The paper also shows that the proposed algorithm can be used to improve the performance of the existing GNN algorithms that use sparse moving averages for GNN learning. The algorithm is shown to be more computationally efficient compared to the existing algorithm for SGD in terms of memory overhead, and it is shown that the algorithm can also be used as a regularizer to the standard GNN algorithm for larger graphs.","Neighbor sampling is a popular technique for training Graph Neural Networks (GNNs) on large graphs. In this paper, the authors consider the Stochastic Compositional Optimization (SCO) problems for samplingbased GNN training. They show that existing SCO algorithms for sampling based GNNs are computationally expensive to compute, and they propose to use moving averages to compute aggregated features instead. The authors also show that the moving averages have a GPU memory limit and a CPU memory limit.    The main contribution of this paper is that the authors propose a new algorithm that uses moving averages instead of a fixed size buffer, which is more efficient for graph size and memory. They also provide a convergence rate for the proposed SCO algorithm that matches the convergence rate of Adam SGD for the same size of the graph.  The paper also shows that the proposed algorithm can be used to improve the performance of the existing GNN algorithms that use sparse moving averages for GNN learning. The algorithm is shown to be more computationally efficient compared to the existing algorithm for SGD in terms of memory overhead, and it is shown that the algorithm can also be used as a regularizer to the standard GNN algorithm for larger graphs."
2805,SP:72e0cac289dce803582053614ec9ee93e783c838,"random hashes USED-FOR Jaccard ( resemblance ) similarity. Minwise hashing ( MinHash ) USED-FOR random hashes. massive binary ( 0/1 ) data USED-FOR Jaccard ( resemblance ) similarity. large - scale learning models CONJUNCTION approximate near neighbor search. approximate near neighbor search CONJUNCTION large - scale learning models. massive data USED-FOR approximate near neighbor search. independent random permutations USED-FOR MinHash. that COMPARE classical MinHash. classical MinHash COMPARE that. Jaccard estimation variance EVALUATE-FOR classical MinHash. circulant manner FEATURE-OF independent random permutations. permutations USED-FOR it. estimation accuracy EVALUATE-FOR it. Method are Circulant MinHash ( C - MinHash ), and C - MinHash variant. Generic is method. ","This paper proposes Circulant MinHash (C-MinHash), a variant of Minwise hashing (MinHash) for random hashes for Jaccard (similarity) similarity on massive binary (0/1) data. The authors show that MinHash can be regarded as an extension of MinHash that uses independent random permutations in a circulant manner, which can be used to train large-scale learning models and approximate near neighbor search on massive data. They show that that improves the estimation accuracy over the classical MinHash in terms of JACCard estimation variance, and that it is more robust to permutations that are not permuted in the same way as those permutations used in the original MinHash. They also propose a C-minHash variant that is more computationally efficient.   ","This paper proposes Circulant MinHash (C-MinHash), a variant of Minwise hashing (MinHash) for random hashes for Jaccard (similarity) similarity on massive binary (0/1) data. The authors show that MinHash can be regarded as an extension of MinHash that uses independent random permutations in a circulant manner, which can be used to train large-scale learning models and approximate near neighbor search on massive data. They show that that improves the estimation accuracy over the classical MinHash in terms of JACCard estimation variance, and that it is more robust to permutations that are not permuted in the same way as those permutations used in the original MinHash. They also propose a C-minHash variant that is more computationally efficient.   "
2821,SP:d254b38331b6b6f30de398bae09380cd5c951698,Adversarial training ( AT ) USED-FOR adversarial robustness. adversarial robustness EVALUATE-FOR single lpthreat models. lp - threat models USED-FOR adversarial robustness. training scheme USED-FOR adversarial robustness. training scheme USED-FOR union of lp - threat models. adversarial training USED-FOR lp - threat model. E - AT scheme USED-FOR lp - robust model. multiple - norm robustness EVALUATE-FOR state - of - the - art. multiple norm robustness FEATURE-OF ImageNet models. CIFAR-10 EVALUATE-FOR multiple - norm robustness. CIFAR-10 EVALUATE-FOR state - of - the - art. adversarial robustness FEATURE-OF threat models. CIFAR-10 EVALUATE-FOR SOTA l1 - robustness. Task is safety - critical systems. OtherScientificTerm is lp - balls. Metric is multiple norm adversarial robustness. ,"Adversarial training (AT) has been shown to improve the adversarial robustness of single lpthreat models. However, it is not clear whether this is the case for safety-critical systems. In this paper, the authors propose a new training scheme for adversarial training on union of lp-threat models, which is called E-AT.    The main idea is to train a lp - threat model with adversarial learning on top of existing lp training, and then use the training scheme to train the union of adversarial threat models.  The authors show that the proposed E -AT scheme is able to train an Lp-robust model that is robust to multiple norm adversarial perturbations.  Empirically, the paper shows that multiple norm robustness on CIFAR-10 is comparable to the state-of-the-art on SOTA l1 - robustness, and the paper also shows that the multiple-norm robustness for ImageNet models can be improved. ","Adversarial training (AT) has been shown to improve the adversarial robustness of single lpthreat models. However, it is not clear whether this is the case for safety-critical systems. In this paper, the authors propose a new training scheme for adversarial training on union of lp-threat models, which is called E-AT.    The main idea is to train a lp - threat model with adversarial learning on top of existing lp training, and then use the training scheme to train the union of adversarial threat models.  The authors show that the proposed E -AT scheme is able to train an Lp-robust model that is robust to multiple norm adversarial perturbations.  Empirically, the paper shows that multiple norm robustness on CIFAR-10 is comparable to the state-of-the-art on SOTA l1 - robustness, and the paper also shows that the multiple-norm robustness for ImageNet models can be improved. "
2837,SP:4c2928f6772664d63c02c29f913b476e1c932983,MTL models COMPARE single - task counterpart. single - task counterpart COMPARE MTL models. private encoders CONJUNCTION gates. gates CONJUNCTION private encoders. gates CONJUNCTION private decoders. private decoders CONJUNCTION gates. public encoder CONJUNCTION private encoders. private encoders CONJUNCTION public encoder. private encoder CONJUNCTION gate. gate CONJUNCTION private encoder. gate CONJUNCTION private decoder. private decoder CONJUNCTION gate. public encoder CONJUNCTION private encoder. private encoder CONJUNCTION public encoder. storage cost FEATURE-OF inference stage. SMTL USED-FOR gate. SMTL USED-FOR gates. benchmark datasets EVALUATE-FOR methods. Method is Multi - Task Learning ( MTL ). Generic is problem. OtherScientificTerm is negative sharing. Task is safe multi - task learning. ,"This paper studies the problem of Multi-Task Learning (MTL) where the goal is to learn a good multi-task model without negative sharing. In this problem, there is a large number of tasks that share the same public encoder, but different private encoders, gates, and private decoders. The paper shows that MTL models trained with SMTL outperform their single-task counterpart. The main contribution of the paper is that SMTL can be used to train a gate, a private decoder, and a gate and a private encoder in parallel, and that the gate can be shared across all tasks. The authors also show that the storage cost at the inference stage can be reduced by using SMTL to share the gates between all the tasks. Experiments on several benchmark datasets are conducted to show the effectiveness of the proposed methods. In addition, the authors also provide a theoretical analysis of the effect of SMTL on the performance of the gate and the gate in terms of the negative sharing, which is a relevant issue in the context of safe multi-tasks learning. ","This paper studies the problem of Multi-Task Learning (MTL) where the goal is to learn a good multi-task model without negative sharing. In this problem, there is a large number of tasks that share the same public encoder, but different private encoders, gates, and private decoders. The paper shows that MTL models trained with SMTL outperform their single-task counterpart. The main contribution of the paper is that SMTL can be used to train a gate, a private decoder, and a gate and a private encoder in parallel, and that the gate can be shared across all tasks. The authors also show that the storage cost at the inference stage can be reduced by using SMTL to share the gates between all the tasks. Experiments on several benchmark datasets are conducted to show the effectiveness of the proposed methods. In addition, the authors also provide a theoretical analysis of the effect of SMTL on the performance of the gate and the gate in terms of the negative sharing, which is a relevant issue in the context of safe multi-tasks learning. "
2853,SP:c4cee0d44198559c417750ec4729d26b41061929,"energy - based sequence models USED-FOR partition functions. expressive parametric families USED-FOR energy - based sequence models. model selection CONJUNCTION learning model parameters. learning model parameters CONJUNCTION model selection. partition functions FEATURE-OF sequence model families. asymptotic guarantees FEATURE-OF statistical procedures. OtherScientificTerm are model parameters, partition function, rational number, reduced expressiveness, and computability concerns. Generic is they. Task is sequence modeling. Method is model parametrizations. ","This paper studies the expressive parametric families of energy-based sequence models for partition functions. The authors show that the partition functions of two sequence model families with different partition functions can be expressed as a function of the model parameters, and that they can be represented as functions of a rational number. They also provide asymptotic guarantees for a number of statistical procedures that can be used for model selection and learning model parameters.    The authors also show that for any partition function of a sequence model, the rational number of the partition function can be written in terms of the number of parameters of the sequence model. This is an interesting result, and it is an important contribution to the field of sequence modeling. However, the authors also point out that the reduced expressiveness is not due to model parametrizations, but rather to computability concerns that arise from model selection, model selection in general, and model selection with respect to model selection. ","This paper studies the expressive parametric families of energy-based sequence models for partition functions. The authors show that the partition functions of two sequence model families with different partition functions can be expressed as a function of the model parameters, and that they can be represented as functions of a rational number. They also provide asymptotic guarantees for a number of statistical procedures that can be used for model selection and learning model parameters.    The authors also show that for any partition function of a sequence model, the rational number of the partition function can be written in terms of the number of parameters of the sequence model. This is an interesting result, and it is an important contribution to the field of sequence modeling. However, the authors also point out that the reduced expressiveness is not due to model parametrizations, but rather to computability concerns that arise from model selection, model selection in general, and model selection with respect to model selection. "
2869,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"Wasserstein distance USED-FOR large - scale machine learning problems. random projection USED-FOR sliced Wasserstein distance. computational efficiency EVALUATE-FOR sliced Wasserstein distance. augmented sliced Wasserstein distances ( ASWDs ) HYPONYM-OF distance metrics. neural networks USED-FOR higher - dimensional hypersurfaces. they USED-FOR complex structures of the data distribution. gradient ascent USED-FOR hypersurfaces. ASWD COMPARE Wasserstein variants. Wasserstein variants COMPARE ASWD. Wasserstein variants USED-FOR synthetic and real - world problems. synthetic and real - world problems EVALUATE-FOR ASWD. Metric is computational cost. OtherScientificTerm are projections, ( random ) linear projections, and nonlinear projections. Method is injective neural network architecture. ","This paper proposes a new Wasserstein distance for large-scale machine learning problems, which is based on a random projection of the data distribution. The authors argue that the computational cost is prohibitively high for non-linear projections, so they propose to use (random) linear projections instead. They also propose two new distance metrics, called augmented sliced WASSERSTEIN distances (ASWDs), to improve the computational efficiency of the sliced WESSERstein distance. The main contribution of the paper is the introduction of neural networks for higher-dimensional hypersurfaces, where they are able to capture the complex structures in the data, and the use of gradient ascent to map hypersurfacing to nonlinear projections. They show that ASWD outperforms the existing Wasso variants on both synthetic and real-world problems, and they also propose an injective neural network architecture that can be applied to more complex data.","This paper proposes a new Wasserstein distance for large-scale machine learning problems, which is based on a random projection of the data distribution. The authors argue that the computational cost is prohibitively high for non-linear projections, so they propose to use (random) linear projections instead. They also propose two new distance metrics, called augmented sliced WASSERSTEIN distances (ASWDs), to improve the computational efficiency of the sliced WESSERstein distance. The main contribution of the paper is the introduction of neural networks for higher-dimensional hypersurfaces, where they are able to capture the complex structures in the data, and the use of gradient ascent to map hypersurfacing to nonlinear projections. They show that ASWD outperforms the existing Wasso variants on both synthetic and real-world problems, and they also propose an injective neural network architecture that can be applied to more complex data."
2885,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,coordinated exploration and behaviour PART-OF multi - agent systems. framework USED-FOR multi - agent reinforcement learners ( MARL ). reinforcement learning ( RL ) CONJUNCTION switching controls. switching controls CONJUNCTION reinforcement learning ( RL ). switching controls USED-FOR LIGS. LIGS USED-FOR learning process. intrinsic rewards USED-FOR learning process. LIGS USED-FOR intrinsic rewards. reinforcement learning ( RL ) USED-FOR LIGS. LIGS USED-FOR systems of RL agents. sparse rewards USED-FOR systems of RL agents. it USED-FOR joint policies. multi - agent RL algorithms USED-FOR LIGS. Foraging CONJUNCTION StarCraft. StarCraft CONJUNCTION Foraging. LIGS framework USED-FOR Foraging. LIGS framework USED-FOR StarCraft. Method is reinforcement learners ( RL ). ,"This paper proposes a framework for multi-agent reinforcement learners (MARL) that combines the benefits of reinforcement learning (RL) and switching controls in order to encourage coordinated exploration and behaviour among agents in multiple-agent systems. Specifically, LIGS is based on the observation that RL agents can learn to learn systems of RL agents with sparse rewards from sparse rewards, and that the learning process can be improved by learning with intrinsic rewards. The authors show that the proposed framework can be applied to a variety of RL algorithms and that it can be used to learn joint policies across multiple agents. They also demonstrate the effectiveness of the proposed LigS framework on Foraging and StarCraft. ","This paper proposes a framework for multi-agent reinforcement learners (MARL) that combines the benefits of reinforcement learning (RL) and switching controls in order to encourage coordinated exploration and behaviour among agents in multiple-agent systems. Specifically, LIGS is based on the observation that RL agents can learn to learn systems of RL agents with sparse rewards from sparse rewards, and that the learning process can be improved by learning with intrinsic rewards. The authors show that the proposed framework can be applied to a variety of RL algorithms and that it can be used to learn joint policies across multiple agents. They also demonstrate the effectiveness of the proposed LigS framework on Foraging and StarCraft. "
2901,SP:9eadc19f7f712c488cf50d091f372092f6352930,multi - hop QA systems COMPARE DOCHOPPER. DOCHOPPER COMPARE multi - hop QA systems. document information CONJUNCTION q. q CONJUNCTION document information. QA tasks EVALUATE-FOR DOCHOPPER. datasets EVALUATE-FOR DOCHOPPER. inference time EVALUATE-FOR DOCHOPPER. Generic is model. Method is compact neural representation of q. ,"This paper proposes a new model for multi-hop QA, called DOCHOPPER. Compared to other multi-hops QA systems, DOChOPPER is able to learn a compact neural representation of q and document information in a single step. The authors conduct experiments on three different QA tasks and show that DOChopPER outperforms existing multi-hitter QA methods on all three datasets. They also show that the inference time is significantly faster than existing methods. ","This paper proposes a new model for multi-hop QA, called DOCHOPPER. Compared to other multi-hops QA systems, DOChOPPER is able to learn a compact neural representation of q and document information in a single step. The authors conduct experiments on three different QA tasks and show that DOChopPER outperforms existing multi-hitter QA methods on all three datasets. They also show that the inference time is significantly faster than existing methods. "
2917,SP:4e79b326bbda5d1509e88869dde9886764366d41,"modalities USED-FOR voice search request. voice casting USED-FOR audiovisual productions. it USED-FOR modalities. it USED-FOR voice recommendation system. it USED-FOR voice search request. characteristic extraction USED-FOR voice casting. taxonomy USED-FOR comedian voices. taxonomy USED-FOR annotation protocol. Label Refining HYPONYM-OF semi - supervised learning method. vocal characteristics HYPONYM-OF refined labels. clustering algorithm USED-FOR refined representation extractor. refined labels USED-FOR refined representation extractor. representation extractor USED-FOR method. clustering algorithm USED-FOR refined labels. Label Refining USED-FOR method. subsidiary corpus USED-FOR voice characteristics. OtherScientificTerm are characteristic, and priori knowledge. Material is MassEffect 3 video game. ","This paper proposes a semi-supervised learning method called Label Refining, which is a method to refine the modalities of a voice search request from multiple modalities. The authors propose it for voice casting for audiovisual productions, and it can be used for voice recommendation system. The key idea is to use the characteristic extraction for the voice casting as a pre-processing step in voice casting. The taxonomy of comedian voices is used as an annotation protocol, and the taxonomy is used to train a self-attention mechanism. The proposed method is based on the idea of Label Refined, a recent semi-Supervised learning approach that uses refined labels (e.g., vocal characteristics) to refine a representation extractor based on a clustering algorithm. The method is evaluated on the MassEffect 3 video game, where the voice characteristics are extracted from a subsidiary corpus. The results show that the proposed method outperforms the state-of-the-art methods.   ","This paper proposes a semi-supervised learning method called Label Refining, which is a method to refine the modalities of a voice search request from multiple modalities. The authors propose it for voice casting for audiovisual productions, and it can be used for voice recommendation system. The key idea is to use the characteristic extraction for the voice casting as a pre-processing step in voice casting. The taxonomy of comedian voices is used as an annotation protocol, and the taxonomy is used to train a self-attention mechanism. The proposed method is based on the idea of Label Refined, a recent semi-Supervised learning approach that uses refined labels (e.g., vocal characteristics) to refine a representation extractor based on a clustering algorithm. The method is evaluated on the MassEffect 3 video game, where the voice characteristics are extracted from a subsidiary corpus. The results show that the proposed method outperforms the state-of-the-art methods.   "
2933,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"federated and split learning HYPONYM-OF Distributed collaborative learning approaches. Vision Transformer ( ViT ) USED-FOR common representation. Vision Transformer ( ViT ) USED-FOR computer vision applications. global attention USED-FOR common representation. distributed learning framework USED-FOR image processing tasks. ViT USED-FOR distributed learning framework. task - agnostic Vision Transformer CONJUNCTION task - specific head / tail. task - specific head / tail CONJUNCTION task - agnostic Vision Transformer. task - specific heads and tails CONJUNCTION task - agnostic Transformer body. task - agnostic Transformer body CONJUNCTION task - specific heads and tails. features PART-OF representation. global attention USED-FOR Transformer body. task - agnostic learning USED-FOR Transformer. task - specific learning USED-FOR heads. alternating training strategy USED-FOR task - specific learning. method USED-FOR task - specific network. multi - task learning EVALUATE-FOR method. Method is neural networks. Generic are they, applications, and translation. OtherScientificTerm is customer - specific head and tail. Material is medical image data. ","Distributed collaborative learning approaches, such as federated and split learning, have been a popular technique for training neural networks. However, they are expensive to train as they require multiple workers to share information across multiple clients. This paper proposes a new distributed learning framework based on Vision Transformer (ViT) for computer vision applications, which uses global attention to learn a common representation across all clients.    The paper proposes to use ViT in a distributed manner for image processing tasks, where each client has a customer-specific head and tail, and the goal is to train a Transformer body with global attention on all features in the representation. The authors propose a task-agnostic learning for the Transformer, where the heads are trained using task-specific learning. The heads are split into multiple heads, and each head uses an alternating training strategy, where one head is trained on all clients, while the other heads are only trained on a subset of the clients. The proposed method is evaluated on multi-task learning, where it is shown that the proposed method can learn a task agnostic network, which can be used for multiple tasks, and is shown to perform well on medical image data. The paper also shows that the method can be applied to other applications such as translation, image classification, etc.","Distributed collaborative learning approaches, such as federated and split learning, have been a popular technique for training neural networks. However, they are expensive to train as they require multiple workers to share information across multiple clients. This paper proposes a new distributed learning framework based on Vision Transformer (ViT) for computer vision applications, which uses global attention to learn a common representation across all clients.    The paper proposes to use ViT in a distributed manner for image processing tasks, where each client has a customer-specific head and tail, and the goal is to train a Transformer body with global attention on all features in the representation. The authors propose a task-agnostic learning for the Transformer, where the heads are trained using task-specific learning. The heads are split into multiple heads, and each head uses an alternating training strategy, where one head is trained on all clients, while the other heads are only trained on a subset of the clients. The proposed method is evaluated on multi-task learning, where it is shown that the proposed method can learn a task agnostic network, which can be used for multiple tasks, and is shown to perform well on medical image data. The paper also shows that the method can be applied to other applications such as translation, image classification, etc."
2949,SP:249a72ef4e9cf02221243428174bb749068af6b2,"misspecified reward functions USED-FOR RL agents. misspecified rewards FEATURE-OF RL environments. action space resolution CONJUNCTION observation space noise. observation space noise CONJUNCTION action space resolution. model capacity CONJUNCTION action space resolution. action space resolution CONJUNCTION model capacity. observation space noise CONJUNCTION training time. training time CONJUNCTION observation space noise. agent capabilities USED-FOR reward hacking. training time HYPONYM-OF agent capabilities. model capacity HYPONYM-OF agent capabilities. observation space noise HYPONYM-OF agent capabilities. action space resolution HYPONYM-OF agent capabilities. proxy reward CONJUNCTION true reward. true reward CONJUNCTION proxy reward. capability thresholds HYPONYM-OF phase transitions. anomaly detection task USED-FOR aberrant policies. Task are Reward hacking, and ML systems. OtherScientificTerm is reward misspecifications. Method is baseline detectors. ","This paper studies the problem of reward hacking in reinforcement learning. The authors show that RL agents can be trained with misspecified reward functions, and that reward hacking can be performed in RL environments with missspecified rewards. Reward hacking is a problem that has been explored in the literature for a long time, but this paper is the first time that it has been studied in the context of RL agents.    The authors consider three agent capabilities (model capacity, action space resolution, observation space noise, and training time) that can be exploited for reward hacking: (1) proxy reward, (2) true reward, and (3) phase transitions (e.g., capability thresholds).   They show that the reward misspecifications can be used to fool baseline detectors. They also propose an anomaly detection task to detect aberrant policies, which is a useful tool for ML systems.","This paper studies the problem of reward hacking in reinforcement learning. The authors show that RL agents can be trained with misspecified reward functions, and that reward hacking can be performed in RL environments with missspecified rewards. Reward hacking is a problem that has been explored in the literature for a long time, but this paper is the first time that it has been studied in the context of RL agents.    The authors consider three agent capabilities (model capacity, action space resolution, observation space noise, and training time) that can be exploited for reward hacking: (1) proxy reward, (2) true reward, and (3) phase transitions (e.g., capability thresholds).   They show that the reward misspecifications can be used to fool baseline detectors. They also propose an anomaly detection task to detect aberrant policies, which is a useful tool for ML systems."
2965,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,Kullback – Leibler ( KL ) divergence CONJUNCTION arbitary differeitiable f divergence. arbitary differeitiable f divergence CONJUNCTION Kullback – Leibler ( KL ) divergence. f -TVO USED-FOR Thermodynamic Variational Objective ( TVO ). f -TVO USED-FOR dual function of model evidence f∗(p(x ) ). log model evidence PART-OF TVO. dual function of model evidence f∗(p(x ) ) COMPARE log model evidence. log model evidence COMPARE dual function of model evidence f∗(p(x ) ). deformed χ - geometry perspective USED-FOR f -TVO. variational posterior distribution CONJUNCTION true posterior distribution. true posterior distribution CONJUNCTION variational posterior distribution. χ - exponential family exponential USED-FOR f -TVO. χ - path USED-FOR f -TVO. reparameterization trick CONJUNCTION Monte Carlo approximation. Monte Carlo approximation CONJUNCTION reparameterization trick. reparameterization trick PART-OF f -TVO. Monte Carlo approximation PART-OF f -TVO. VAE CONJUNCTION Bayesian neural network. Bayesian neural network CONJUNCTION VAE. f -TVO COMPARE cooresponding baseline f -divergence variational inference. cooresponding baseline f -divergence variational inference COMPARE f -TVO. Bayesian neural network EVALUATE-FOR f -TVO. VAE EVALUATE-FOR f -TVO. Bayesian neural network EVALUATE-FOR cooresponding baseline f -divergence variational inference. OtherScientificTerm is deformed geodesic. ,"This paper proposes f-TVO, which is a generalization of the Thermodynamic Variational Objective (TVO) based on the deformed χ-geometry perspective. The main idea is to replace the log model evidence in TVO with the dual function of model evidence f∗(p(x)), which is the Kullback–Leibler (KL) divergence and the arbitary differeitiable f divergence.    The main contribution of the paper is to use the χ exponential family exponential to define f -TVO.  The authors also introduce a deformed geodesic, which allows to use a variational posterior distribution instead of the true posterior distribution.  In addition, the authors propose to use an extension of the �ck-path of f- TVO to the case where the data is drawn from a deformation of the data distribution. The authors show that the f---TVO can be seen as a reparameterization trick and a Monte Carlo approximation.  Experiments are conducted on VAE and a Bayesian neural network, and the authors demonstrate that f--TVO outperforms the cooresponding baseline f-divergence variational inference. ","This paper proposes f-TVO, which is a generalization of the Thermodynamic Variational Objective (TVO) based on the deformed χ-geometry perspective. The main idea is to replace the log model evidence in TVO with the dual function of model evidence f∗(p(x)), which is the Kullback–Leibler (KL) divergence and the arbitary differeitiable f divergence.    The main contribution of the paper is to use the χ exponential family exponential to define f -TVO.  The authors also introduce a deformed geodesic, which allows to use a variational posterior distribution instead of the true posterior distribution.  In addition, the authors propose to use an extension of the �ck-path of f- TVO to the case where the data is drawn from a deformation of the data distribution. The authors show that the f---TVO can be seen as a reparameterization trick and a Monte Carlo approximation.  Experiments are conducted on VAE and a Bayesian neural network, and the authors demonstrate that f--TVO outperforms the cooresponding baseline f-divergence variational inference. "
2981,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"critics ’ initialization USED-FOR ensemble - based actor - critic exploration. approximated UCB CONJUNCTION weighted Bellman backup. weighted Bellman backup CONJUNCTION approximated UCB. strategy COMPARE approximated UCB. approximated UCB COMPARE strategy. weighted Bellman backup COMPARE clipped double Q - Learning. clipped double Q - Learning COMPARE weighted Bellman backup. additive action noise USED-FOR exploration. weighted Bellman backup USED-FOR strategy. actors ’ initialization USED-FOR training. posterior sampling USED-FOR strategy. methods USED-FOR policies. Method are deep reinforcement learning ( RL ), RL toolbox, and ED2. Generic are task, and tools. Material is continuous control setting. OtherScientificTerm is evaluation runs. Task is continuous control tasks. ","This paper proposes an ensemble-based actor-critic exploration based on the critics’ initialization for deep reinforcement learning (RL). The authors propose a new RL toolbox, called ED2, which is designed for the continuous control setting. The authors show that the proposed strategy outperforms approximated UCB and weighted Bellman backup, and clipped double Q-Learning with additive action noise for exploration. They also show that ED2 is more robust to the number of evaluation runs than ED1 and ED2 with the same number of evaluations.  The authors also propose two methods to learn policies based on posterior sampling, which can be applied to continuous control tasks.   ","This paper proposes an ensemble-based actor-critic exploration based on the critics’ initialization for deep reinforcement learning (RL). The authors propose a new RL toolbox, called ED2, which is designed for the continuous control setting. The authors show that the proposed strategy outperforms approximated UCB and weighted Bellman backup, and clipped double Q-Learning with additive action noise for exploration. They also show that ED2 is more robust to the number of evaluation runs than ED1 and ED2 with the same number of evaluations.  The authors also propose two methods to learn policies based on posterior sampling, which can be applied to continuous control tasks.   "
2997,SP:21819b54433fa274657d9fe418f66407eee83eeb,"natural language processing CONJUNCTION face recognition. face recognition CONJUNCTION natural language processing. lending CONJUNCTION college admission. college admission CONJUNCTION lending. Supervised learning models USED-FOR domains. college admission CONJUNCTION natural language processing. natural language processing CONJUNCTION college admission. lending HYPONYM-OF domains. face recognition HYPONYM-OF domains. college admission HYPONYM-OF domains. natural language processing HYPONYM-OF domains. fairness notions USED-FOR fairness issues. fair predictor USED-FOR constrained optimization problem. Equalized Loss ( EL ) HYPONYM-OF fairness notion. prediction error / loss USED-FOR fairness notion. algorithms USED-FOR global optimum. algorithms USED-FOR non - convex problem. global optimum USED-FOR non - convex problem. convex programming tools USED-FOR algorithms. ELminimizer algorithm USED-FOR EL fair predictor. non - convex optimization problem USED-FOR EL fair predictor. convex constrained optimizations USED-FOR non - convex optimization problem. algorithm USED-FOR sub - optimal EL fair predictor. algorithm COMPARE ELminimizer. ELminimizer COMPARE algorithm. unconstrained convex programming tools USED-FOR algorithm. unconstrained convex programming tools USED-FOR sub - optimal EL fair predictor. real - world data EVALUATE-FOR algorithms. Generic are models, it, and constraint. OtherScientificTerm are protected social groups, and loss function. Method is learning process. ","This paper studies the problem of fair learning for supervised learning in the setting where there are protected social groups. In this setting, the goal is to train a model that is fair in the sense that it is fair for all groups to have the same level of discrimination. The authors propose a new fairness notion, Equalized Loss (EL), which is a generalization of Equalized Risk Minimization (ERM) and Equalized Fairness (EFRM). The authors show that under certain conditions, the proposed EL can be a global optimum for a non-convex optimization problem.    The authors also show that the proposed algorithm is a sub-optimal EL fair predictor.  The main contribution of the paper is that the authors propose an algorithm that uses convex programming tools to approximate the global optimum of an EL fair function. ","This paper studies the problem of fair learning for supervised learning in the setting where there are protected social groups. In this setting, the goal is to train a model that is fair in the sense that it is fair for all groups to have the same level of discrimination. The authors propose a new fairness notion, Equalized Loss (EL), which is a generalization of Equalized Risk Minimization (ERM) and Equalized Fairness (EFRM). The authors show that under certain conditions, the proposed EL can be a global optimum for a non-convex optimization problem.    The authors also show that the proposed algorithm is a sub-optimal EL fair predictor.  The main contribution of the paper is that the authors propose an algorithm that uses convex programming tools to approximate the global optimum of an EL fair function. "
3013,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"neural networks USED-FOR cognitive capacity. meaningful learning USED-FOR systematic generalization. compositional skills FEATURE-OF models. semantic connections USED-FOR models. semantic connections USED-FOR compositional skills. semantic links USED-FOR models. RNNs CONJUNCTION CNNs. CNNs CONJUNCTION RNNs. CNNs CONJUNCTION Transformers. Transformers CONJUNCTION CNNs. SCAN CONJUNCTION real - world datasets. real - world datasets CONJUNCTION SCAN. real - world datasets USED-FOR semantic parsing. semantic linking USED-FOR sequenceto - sequence models. RNNs HYPONYM-OF sequenceto - sequence models. CNNs HYPONYM-OF sequenceto - sequence models. Transformers PART-OF sequenceto - sequence models. prior knowledge CONJUNCTION semantic linking. semantic linking CONJUNCTION prior knowledge. prior knowledge USED-FOR systematic generalization. semantic linking USED-FOR systematic generalization. inductive learning COMPARE deductive learning. deductive learning COMPARE inductive learning. neural networks USED-FOR systematic generalization. learning schemes USED-FOR neural networks. learning schemes USED-FOR systematic generalization. Material is SCAN dataset. Method are meaningful learning principle, and data augmentation techniques. OtherScientificTerm is inductive or deductive manner. Generic is them. ","This paper studies systematic generalization of neural networks in the context of cognitive capacity. The authors propose a meaningful learning principle, i.e., that meaningful learning can be used to improve systematic generalisation. They show that models with semantic connections are able to learn compositional skills, and that models trained with semantic links can learn models that can generalize to unseen tasks. They demonstrate this on the SCAN dataset, and on two real-world datasets for semantic parsing, SCAN and CIFAR-10. They also show that sequenceto-sequence models (RNNs, CNNs, Transformers, etc.) with semantic linking can learn to generalize well, and they show that the learning schemes used to train neural networks to learn these kinds of skills are more effective for systematic generalizability than inductive or deductive learning.    The authors also propose two data augmentation techniques to improve the generalization performance of the models. The first is to augment the prior knowledge with prior knowledge of the task and the semantic linking, and the second is to use inductive learning to augment a sequence of sequences of sequences to make them more similar.  The paper is well-written and well-motivated, and is easy to follow. ","This paper studies systematic generalization of neural networks in the context of cognitive capacity. The authors propose a meaningful learning principle, i.e., that meaningful learning can be used to improve systematic generalisation. They show that models with semantic connections are able to learn compositional skills, and that models trained with semantic links can learn models that can generalize to unseen tasks. They demonstrate this on the SCAN dataset, and on two real-world datasets for semantic parsing, SCAN and CIFAR-10. They also show that sequenceto-sequence models (RNNs, CNNs, Transformers, etc.) with semantic linking can learn to generalize well, and they show that the learning schemes used to train neural networks to learn these kinds of skills are more effective for systematic generalizability than inductive or deductive learning.    The authors also propose two data augmentation techniques to improve the generalization performance of the models. The first is to augment the prior knowledge with prior knowledge of the task and the semantic linking, and the second is to use inductive learning to augment a sequence of sequences of sequences to make them more similar.  The paper is well-written and well-motivated, and is easy to follow. "
3029,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"method USED-FOR 3D shape representation. multi - scale wavelet decomposition USED-FOR method. up / down - sampling USED-FOR hierarchies. sub - bands components PART-OF 3D shapes. lifting scheme USED-FOR high or low sub - bands components. Transformers USED-FOR AWT - Net. shape features USED-FOR them. 3D shape classification and segmentation benchmarks EVALUATE-FOR AWT - Net. OtherScientificTerm are decomposition tree, approximation or detail wavelet coefficients, features, and wavelet coefficients. Method are multi - resolution wavelet analysis, and holistic representations. ","This paper proposes a novel method for learning a 3D shape representation based on multi-scale wavelet decomposition. The proposed method, AWT-Net, is based on a decomposition tree, where sub-bands components of 3D shapes are decomposable into sub-bands, and hierarchies can be learned via up/down-sampling. A lifting scheme is used to select high or low sub-band components, and a multi-resolution wavelet analysis is used. Transformers are used to train AWT - Net, and the authors use Transformers to learn the decompositions of the subbands, and use them as shape features. The authors show that the approximation or detail wavelet coefficients can be used as features, and that the learned features are holistic representations that can be applied to any shape. The paper also shows that the proposed method is able to achieve state-of-the-art performance on 3D object detection and segmentation tasks.   ","This paper proposes a novel method for learning a 3D shape representation based on multi-scale wavelet decomposition. The proposed method, AWT-Net, is based on a decomposition tree, where sub-bands components of 3D shapes are decomposable into sub-bands, and hierarchies can be learned via up/down-sampling. A lifting scheme is used to select high or low sub-band components, and a multi-resolution wavelet analysis is used. Transformers are used to train AWT - Net, and the authors use Transformers to learn the decompositions of the subbands, and use them as shape features. The authors show that the approximation or detail wavelet coefficients can be used as features, and that the learned features are holistic representations that can be applied to any shape. The paper also shows that the proposed method is able to achieve state-of-the-art performance on 3D object detection and segmentation tasks.   "
3045,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"pretrained language model USED-FOR natural language generation tasks. prefixtuning CONJUNCTION adapters. adapters CONJUNCTION prefixtuning. Lightweight finetuning COMPARE full finetuning. full finetuning COMPARE Lightweight finetuning. adapters HYPONYM-OF Lightweight finetuning. prefixtuning HYPONYM-OF Lightweight finetuning. lightweight finetuning COMPARE full finetuning in - distribution ( ID ). full finetuning in - distribution ( ID ) COMPARE lightweight finetuning. ID CONJUNCTION OOD. OOD CONJUNCTION ID. full and lightweight finetuning USED-FOR methods. ID CONJUNCTION OOD. OOD CONJUNCTION ID. full and lightweight finetuning USED-FOR ID. full and lightweight finetuning USED-FOR OOD. cocktail finetuning USED-FOR full finetuning. model CONJUNCTION cocktail finetuning. cocktail finetuning CONJUNCTION model. distillation USED-FOR lightweight model. lightweight model USED-FOR full finetuning. distillation USED-FOR full finetuning. distillation USED-FOR OOD behavior. distillation USED-FOR model. distillation USED-FOR ID data. OOD behavior FEATURE-OF model. Method are pretrained model, and lightweight and full finetuning models. Task is multiclass logistic regression setting. ","This paper studies the problem of fine-tuning a pretrained language model for natural language generation tasks. Lightweight finetuning (prefixtuning, adapters, and adapters) is proposed as a way to improve the performance of the pretrained model, and is shown to outperform full finetune in-distribution (ID) as well as the full-finetune (FDF) method in the multiclass logistic regression setting. The authors also show that both methods can be used to improve both ID and OOD performance using both full and lightweight finetuned models.    The main contribution of this paper is that the authors show that the performance gain from using lightweightfinetuning is more significant than that of full finETune in the ID setting. They show that a lightweight model trained with distillation improves the OOD behavior of a model trained on ID data, and that a full model trained using the same distillation performs better on OOD data. Finally, the authors demonstrate that a combination of distillation and full finetching can improve the model’s performance in both cases. ","This paper studies the problem of fine-tuning a pretrained language model for natural language generation tasks. Lightweight finetuning (prefixtuning, adapters, and adapters) is proposed as a way to improve the performance of the pretrained model, and is shown to outperform full finetune in-distribution (ID) as well as the full-finetune (FDF) method in the multiclass logistic regression setting. The authors also show that both methods can be used to improve both ID and OOD performance using both full and lightweight finetuned models.    The main contribution of this paper is that the authors show that the performance gain from using lightweightfinetuning is more significant than that of full finETune in the ID setting. They show that a lightweight model trained with distillation improves the OOD behavior of a model trained on ID data, and that a full model trained using the same distillation performs better on OOD data. Finally, the authors demonstrate that a combination of distillation and full finetching can improve the model’s performance in both cases. "
3061,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"health CONJUNCTION governance. governance CONJUNCTION health. pointillistically labeled data USED-FOR data - hungry ML algorithms. Data programming USED-FOR probabilistic training labels. domain experts USED-FOR labelling functions. approach USED-FOR iterative and interactive improvement of weakly supervised models. WARM HYPONYM-OF Active Refinement of Weakly Supervised Models. active learning USED-FOR approach. active learning USED-FOR weakly supervised models. probabilistic accuracy EVALUATE-FOR label model. expert labelling functions PART-OF weak supervision model. probabilistic labels USED-FOR downstream classifiers. real - world medical classification datasets EVALUATE-FOR WARM. WARM USED-FOR probabilistic labels. accuracy EVALUATE-FOR probabilistic labels. accuracy EVALUATE-FOR WARM. domain shift CONJUNCTION artificial noise. artificial noise CONJUNCTION domain shift. population characteristics CONJUNCTION noisy initial labelling functions. noisy initial labelling functions CONJUNCTION population characteristics. noisy initial labelling functions FEATURE-OF WARM. population characteristics FEATURE-OF WARM. WARM USED-FOR weakly supervised systems. Method are Supervised machine learning ( ML ), and ML methods. Task are clinical research, and data collection. Generic is framework. OtherScientificTerm are weak supervision, and Gradient updates. ","Supervised machine learning (ML) is an important problem in clinical research, especially in the context of data-hungry ML algorithms that rely on pointillistically labeled data. Data programming is used to generate probabilistic training labels, and domain experts are used to train the labelling functions. This paper proposes an approach to iterative and interactive improvement of weakly supervised models, called WARM, which is called Active Refinement of Weakly Supervised Models. The approach is based on active learning, where a weakly-supervised model is trained with expert labelling function updates, and the goal is to improve the probablistic accuracy of the label model. The authors show that the proposed framework can be applied to a wide range of weak supervision settings, including health and governance, where there is no data collection. They show that WARM can improve the accuracy of a few downstream classifiers with the help of the proposed approach. They evaluate WARM on two real-world medical classification datasets, and show that it improves the performance of a number of existing ML methods. They also show that using the proposed WARM improves the accuracy on the accuracy for the probabilistically labels generated by the proposed method. They further show that their approach is robust to domain shift, artificial noise, domain shift and domain noise. Finally, they show that WarM can be used to improve a variety of existing weaklysupervised systems, and that it can be combined with existing methods to improve their performance.    The paper is well-written and well-motivated. The idea of WARM is interesting, and it is an interesting. However, the paper suffers from a lack of comparison with existing work. The paper also suffers from lack of discussion about the relationship between population characteristics, noisy initial labelling, and population characteristics. Gradient updates are not explained clearly enough. ","Supervised machine learning (ML) is an important problem in clinical research, especially in the context of data-hungry ML algorithms that rely on pointillistically labeled data. Data programming is used to generate probabilistic training labels, and domain experts are used to train the labelling functions. This paper proposes an approach to iterative and interactive improvement of weakly supervised models, called WARM, which is called Active Refinement of Weakly Supervised Models. The approach is based on active learning, where a weakly-supervised model is trained with expert labelling function updates, and the goal is to improve the probablistic accuracy of the label model. The authors show that the proposed framework can be applied to a wide range of weak supervision settings, including health and governance, where there is no data collection. They show that WARM can improve the accuracy of a few downstream classifiers with the help of the proposed approach. They evaluate WARM on two real-world medical classification datasets, and show that it improves the performance of a number of existing ML methods. They also show that using the proposed WARM improves the accuracy on the accuracy for the probabilistically labels generated by the proposed method. They further show that their approach is robust to domain shift, artificial noise, domain shift and domain noise. Finally, they show that WarM can be used to improve a variety of existing weaklysupervised systems, and that it can be combined with existing methods to improve their performance.    The paper is well-written and well-motivated. The idea of WARM is interesting, and it is an interesting. However, the paper suffers from a lack of comparison with existing work. The paper also suffers from lack of discussion about the relationship between population characteristics, noisy initial labelling, and population characteristics. Gradient updates are not explained clearly enough. "
3077,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"group annotated training data USED-FOR classification model. empirical risk minimization ( ERM ) objective USED-FOR models. it COMPARE ERM. ERM COMPARE it. Group - DRO COMPARE ERM. ERM COMPARE Group - DRO. ERM USED-FOR minority groups. Group - DRO USED-FOR minority groups. algorithm USED-FOR learning of features. algorithm COMPARE baselines. baselines COMPARE algorithm. ERM CONJUNCTION Group - DRO. Group - DRO CONJUNCTION ERM. minority groups FEATURE-OF benchmarks. Group - DRO HYPONYM-OF baselines. benchmarks EVALUATE-FOR Group - DRO. ERM HYPONYM-OF baselines. benchmarks EVALUATE-FOR baselines. benchmarks EVALUATE-FOR algorithm. algorithm USED-FOR smooth nonconvex functions. descent method USED-FOR algorithm. OtherScientificTerm are distribution shift, and learning of shared / common features. Task is domain generalization. Metric is regularized loss. ","This paper studies the problem of domain generalization from group annotated training data, where the goal is to learn a classification model that generalizes well to distribution shift. The authors propose a new empirical risk minimization (ERM) objective for training such models, and show that it outperforms ERM and Group-DRO for minority groups. They also propose an algorithm for the learning of features that is smooth nonconvex, which they call Group DRO. The algorithm is tested on several benchmarks with minority groups and outperforms baselines such as ERM, Group- DRO, etc.    The authors also propose a descent method for the algorithm, which is based on learning of shared/common features, and a regularized loss that encourages the learned features to be smooth. ","This paper studies the problem of domain generalization from group annotated training data, where the goal is to learn a classification model that generalizes well to distribution shift. The authors propose a new empirical risk minimization (ERM) objective for training such models, and show that it outperforms ERM and Group-DRO for minority groups. They also propose an algorithm for the learning of features that is smooth nonconvex, which they call Group DRO. The algorithm is tested on several benchmarks with minority groups and outperforms baselines such as ERM, Group- DRO, etc.    The authors also propose a descent method for the algorithm, which is based on learning of shared/common features, and a regularized loss that encourages the learned features to be smooth. "
3093,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"influential features USED-FOR prediction. bivariate methods USED-FOR feature interactions. bivariate methods USED-FOR black - box models. univariate explanation USED-FOR higher - order. feature interactions PART-OF black - box models. univariate explanation USED-FOR explainability. directionality USED-FOR influential features. directional explanations USED-FOR feature interactions. Shapley value explanations USED-FOR bivariate method. IMDB CONJUNCTION Census. Census CONJUNCTION IMDB. CIFAR10 CONJUNCTION IMDB. IMDB CONJUNCTION CIFAR10. method COMPARE state - of - the - art. state - of - the - art COMPARE method. Drug, and gene data EVALUATE-FOR method. Drug, and gene data EVALUATE-FOR state - of - the - art. IMDB EVALUATE-FOR state - of - the - art. IMDB EVALUATE-FOR method. Census EVALUATE-FOR state - of - the - art. Census EVALUATE-FOR method. CIFAR10 EVALUATE-FOR method. CIFAR10 EVALUATE-FOR state - of - the - art. Method are machine learning algorithms, and explanation methods. Generic are they, and graph. OtherScientificTerm are directed graph, and features. ","This paper proposes a new explanation method for black-box machine learning algorithms. The authors argue that existing bivariate methods for explaining feature interactions in black- box models are not interpretable as they are based on Shapley value explanations. To improve explainability, the authors propose to use univariate explanation for a higher-order, and use directional explanations for feature interactions. The idea is to use the directionality of the influential features for prediction, and then use a directed graph to map the features to the underlying graph. Experiments on CIFAR10, IMDB, Census, and Drug, and gene data show that the proposed method outperforms the state-of-the-art on all three datasets.   ","This paper proposes a new explanation method for black-box machine learning algorithms. The authors argue that existing bivariate methods for explaining feature interactions in black- box models are not interpretable as they are based on Shapley value explanations. To improve explainability, the authors propose to use univariate explanation for a higher-order, and use directional explanations for feature interactions. The idea is to use the directionality of the influential features for prediction, and then use a directed graph to map the features to the underlying graph. Experiments on CIFAR10, IMDB, Census, and Drug, and gene data show that the proposed method outperforms the state-of-the-art on all three datasets.   "
3109,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"observed behaviour USED-FOR models of human decision - making. clinical care HYPONYM-OF real - world policies. framework USED-FOR interpretable policy learning. probabilistic tree policies USED-FOR physician actions. Policy Extraction USED-FOR interpretable policy learning. decision Trees ( POETREE ) USED-FOR Policy Extraction. medical history USED-FOR probabilistic tree policies. representation of patient history USED-FOR decision tree policies. complexity USED-FOR modelling task. Fullydifferentiable tree architectures USED-FOR modelling task. recurrence USED-FOR representation of patient history. patient information USED-FOR decision tree policies. policy learning method COMPARE stateof - the - art. stateof - the - art COMPARE policy learning method. policy learning method USED-FOR decision support systems. it USED-FOR decision support systems. real and synthetic medical datasets EVALUATE-FOR stateof - the - art. real and synthetic medical datasets EVALUATE-FOR policy learning method. Method is policy learning approaches. Generic is they. Task are decision - making process, and optimization. ","This paper proposes a new framework for interpretable policy learning from observed behaviour for models of human decision-making. The authors propose Policy Extraction with decision Trees (POETREE) which extends the framework of policy learning approaches to a setting where real-world policies (e.g. clinical care) are available. They use probabilistic tree policies from medical history to model physician actions and show that they are interpretable. They also show that the complexity of the modelling task can be reduced by using Fullydifferentiable tree architectures. They further show that decision tree policies can be learned from a representation of patient history via recurrence, and that the representation of the patient history can be used to learn a decision tree based on patient information. They show that their policy learning method outperforms the stateof-the-art on both real and synthetic medical datasets and that it can be applied to decision support systems.   ","This paper proposes a new framework for interpretable policy learning from observed behaviour for models of human decision-making. The authors propose Policy Extraction with decision Trees (POETREE) which extends the framework of policy learning approaches to a setting where real-world policies (e.g. clinical care) are available. They use probabilistic tree policies from medical history to model physician actions and show that they are interpretable. They also show that the complexity of the modelling task can be reduced by using Fullydifferentiable tree architectures. They further show that decision tree policies can be learned from a representation of patient history via recurrence, and that the representation of the patient history can be used to learn a decision tree based on patient information. They show that their policy learning method outperforms the stateof-the-art on both real and synthetic medical datasets and that it can be applied to decision support systems.   "
3125,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"Data augmentation ( DA ) USED-FOR deep learning models. search USED-FOR automated DA methods. image level FEATURE-OF search. joint optimal augmentation policies USED-FOR patches. Patch AutoAugment HYPONYM-OF fine - grained automated DA approach. Stanford Cars CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION Stanford Cars. FGVC - Aircraft CONJUNCTION Pascal VOC 2007. Pascal VOC 2007 CONJUNCTION FGVC - Aircraft. CUB-200 - 2011 CONJUNCTION Stanford Cars. Stanford Cars CONJUNCTION CUB-200 - 2011. image classification CONJUNCTION fine - grained image recognition. fine - grained image recognition CONJUNCTION image classification. ImageNet CONJUNCTION CUB-200 - 2011. CUB-200 - 2011 CONJUNCTION ImageNet. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. fine - grained image recognition CONJUNCTION object detection. object detection CONJUNCTION fine - grained image recognition. CUB-200 - 2011 CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION CUB-200 - 2011. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-100 HYPONYM-OF object detection. CIFAR-10 HYPONYM-OF object detection. object detection EVALUATE-FOR method. fine - grained image recognition EVALUATE-FOR method. image classification EVALUATE-FOR method. method COMPARE DA methods. DA methods COMPARE method. computational resources EVALUATE-FOR method. computational resources EVALUATE-FOR DA methods. OtherScientificTerm are DA policies, grid of patches, augmentation policy, semantics, and team reward. Task is exploration of diversity in local regions. Generic are it, and agents. ","Data augmentation (DA) is a popular technique for training deep learning models. However, the search for automated DA methods is expensive due to the need to search at the image level. This paper proposes a fine-grained automated DA approach called Patch AutoAugment, where two DA policies are trained on a grid of patches. The authors propose joint optimal augmentation policies for these patches, where each augmentation policy is trained to maximize the mutual information between the input image and the output of the other policy. The idea is that the exploration of diversity in local regions is important for efficient search, but it is not always possible to find the optimal policy for each patch. To solve this problem, the authors propose to learn the semantics of each patch, which is then used to train a team of agents. The proposed method is evaluated on image classification (CUB-200-2011, Stanford Cars, FGVC-Airlines, Pascal VOC 2007, CIFAR-100, ImageNet, and ImageNet) for image classification as well as fine-rigid image recognition and object detection (cifar-10, cifar100, and image-level object detection). The authors show that the proposed method outperforms the state-of-the-art DA methods in terms of computational resources. ","Data augmentation (DA) is a popular technique for training deep learning models. However, the search for automated DA methods is expensive due to the need to search at the image level. This paper proposes a fine-grained automated DA approach called Patch AutoAugment, where two DA policies are trained on a grid of patches. The authors propose joint optimal augmentation policies for these patches, where each augmentation policy is trained to maximize the mutual information between the input image and the output of the other policy. The idea is that the exploration of diversity in local regions is important for efficient search, but it is not always possible to find the optimal policy for each patch. To solve this problem, the authors propose to learn the semantics of each patch, which is then used to train a team of agents. The proposed method is evaluated on image classification (CUB-200-2011, Stanford Cars, FGVC-Airlines, Pascal VOC 2007, CIFAR-100, ImageNet, and ImageNet) for image classification as well as fine-rigid image recognition and object detection (cifar-10, cifar100, and image-level object detection). The authors show that the proposed method outperforms the state-of-the-art DA methods in terms of computational resources. "
3141,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"adversarial vulnerability FEATURE-OF deep neural networks. deep neural networks PART-OF machine learning. machine learning USED-FOR adversarial vulnerability. causality USED-FOR distribution change. causal reasoning USED-FOR distribution change. adversarial attacks FEATURE-OF distribution change. causal formulations USED-FOR intuition of adversarial attacks. causal formulations USED-FOR robust DNNs. causal graph USED-FOR generation process of adversarial examples. adversarial distribution USED-FOR intuition of adversarial attacks. models USED-FOR origin of adversarial vulnerability. spurious correlations USED-FOR origin of adversarial vulnerability. spurious correlations USED-FOR adversarial distribution alignment method. causality USED-FOR adversarial vulnerability. OtherScientificTerm are causal perspective, natural and adversarial distribution, and natural and adversarial distributions. Method is causal understanding. Generic is method. ","This paper studies the adversarial vulnerability of deep neural networks in the context of machine learning. The authors consider the problem from a causal perspective. They show that causality is a powerful tool for understanding the distribution change induced by adversarial attacks, and that causal reasoning can be used to identify the distribution of distribution change. They also show that existing causal formulations for robust DNNs can be seen as causal formulations of the intuition of adversarial vulnerabilities induced by a given adversarial distribution. Finally, the authors propose a method to align the natural and adversarial distributions in order to improve the causal understanding. They use a causal graph to model the generation process of an adversarial examples, and show that the natural distribution is more likely to be adversarial than the natural one. They further show that models trained with spurious correlations to the natural distributions can be identified as the source of the origin of an adversary’s attack, and propose a new method to mitigate this issue. ","This paper studies the adversarial vulnerability of deep neural networks in the context of machine learning. The authors consider the problem from a causal perspective. They show that causality is a powerful tool for understanding the distribution change induced by adversarial attacks, and that causal reasoning can be used to identify the distribution of distribution change. They also show that existing causal formulations for robust DNNs can be seen as causal formulations of the intuition of adversarial vulnerabilities induced by a given adversarial distribution. Finally, the authors propose a method to align the natural and adversarial distributions in order to improve the causal understanding. They use a causal graph to model the generation process of an adversarial examples, and show that the natural distribution is more likely to be adversarial than the natural one. They further show that models trained with spurious correlations to the natural distributions can be identified as the source of the origin of an adversary’s attack, and propose a new method to mitigate this issue. "
3157,SP:9f09449a47464efb5458d0732df7664865558e6f,"Continual learning USED-FOR catastrophic forgetting of deep neural networks. network layer FEATURE-OF convolutional filters. filter atoms USED-FOR convolutional filters. filter atom swapping USED-FOR continual learning. filter subspace FEATURE-OF convolutional layer. models USED-FOR forgetting. scheme USED-FOR continual learning. atom swapping framework USED-FOR model ensemble. optimization schemes CONJUNCTION convolutional network structures. convolutional network structures CONJUNCTION optimization schemes. method USED-FOR optimization schemes. method USED-FOR convolutional network structures. benchmark datasets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. accuracy CONJUNCTION scalability. scalability CONJUNCTION accuracy. benchmark datasets EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR state - of - the - art methods. scalability EVALUATE-FOR state - of - the - art methods. scalability EVALUATE-FOR method. accuracy EVALUATE-FOR method. Method is deep neural networks. OtherScientificTerm are low - rank filter subspace, subspace coefficients, and continual learning settings. ","Continual learning is an important problem in the field of catastrophic forgetting of deep neural networks. In this paper, the authors propose a new scheme for continual learning in which the convolutional filters of each network layer are replaced with a set of filter atoms. This is done by swapping filter atoms in the low-rank filter subspace of each layer. The authors show that this scheme can be applied to continual learning by filter atom swapping in the subspace coefficients of the filters. They also show that models trained with this scheme are more robust to forgetting in the continual learning settings.  The authors also propose a novel atom swapping framework to train a model ensemble. The proposed method can be used to improve existing optimization schemes and convolutionic network structures. The method is evaluated on several benchmark datasets and shows improved accuracy and scalability over state-of-the-art methods. ","Continual learning is an important problem in the field of catastrophic forgetting of deep neural networks. In this paper, the authors propose a new scheme for continual learning in which the convolutional filters of each network layer are replaced with a set of filter atoms. This is done by swapping filter atoms in the low-rank filter subspace of each layer. The authors show that this scheme can be applied to continual learning by filter atom swapping in the subspace coefficients of the filters. They also show that models trained with this scheme are more robust to forgetting in the continual learning settings.  The authors also propose a novel atom swapping framework to train a model ensemble. The proposed method can be used to improve existing optimization schemes and convolutionic network structures. The method is evaluated on several benchmark datasets and shows improved accuracy and scalability over state-of-the-art methods. "
3173,SP:b806dd540708b39c10d3c165ea7d394a02376805,"Stein variational gradient descent ( SVGD ) HYPONYM-OF deterministic inference algorithm. variance collapse FEATURE-OF SVGD. SVGD update COMPARE gradient descent. gradient descent COMPARE SVGD update. maximum mean discrepancy ( MMD ) objective EVALUATE-FOR SVGD update. maximum mean discrepancy ( MMD ) objective EVALUATE-FOR gradient descent. proportional asymptotic limit FEATURE-OF variance collapse. SVGD USED-FOR variance collapse. SVGD CONJUNCTION MMD - descent. MMD - descent CONJUNCTION SVGD. equilibrium variance USED-FOR SVGD. equilibrium variance USED-FOR MMD - descent. equilibrium variance USED-FOR learning high - dimensional isotropic Gaussians. Task are variance collapse phenomenon, and variance estimation. Method are deterministic updates, and high - dimensional isotropic Gaussians. OtherScientificTerm is near - orthogonality condition. ","This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD), a deterministic inference algorithm. The authors show that SVGD has a proportional asymptotic limit to variance collapse in deterministic updates. They also show that the maximum mean discrepancy (MMD) objective of the SVGD update is the same as that of gradient descent, and that the variance of SVGD and gradient descent converges to a near-orthogonality condition. Finally, they show that for high-dimensional isotropic Gaussians, SVGD with the equilibrium variance is equivalent to MMD-descent when the variance estimation is orthogonal. ","This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD), a deterministic inference algorithm. The authors show that SVGD has a proportional asymptotic limit to variance collapse in deterministic updates. They also show that the maximum mean discrepancy (MMD) objective of the SVGD update is the same as that of gradient descent, and that the variance of SVGD and gradient descent converges to a near-orthogonality condition. Finally, they show that for high-dimensional isotropic Gaussians, SVGD with the equilibrium variance is equivalent to MMD-descent when the variance estimation is orthogonal. "
3189,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"Noisy labels ( NL ) CONJUNCTION adversarial examples. adversarial examples CONJUNCTION Noisy labels ( NL ). measure USED-FOR intrinsic geometric property. AT COMPARE NL. NL COMPARE AT. sample selection USED-FOR NL. PGD steps USED-FOR sample selection. AT COMPARE training. training COMPARE AT. NL COMPARE training. training COMPARE NL. AT HYPONYM-OF NL correction. AT USED-FOR NL. smoothing effects FEATURE-OF AT. AT USED-FOR general - purpose robust learning criterion. NL USED-FOR AT. natural accuracy EVALUATE-FOR AT. Generic are models, and they. OtherScientificTerm are projected gradient descent ( PGD ) steps, adversarial example, class boundary, noisy - class boundary, and NL corrections. Metric is robustness. Material is natural data. ","This paper proposes a new measure to measure the intrinsic geometric property of robustness to noisy labels (NL) and adversarial examples. The authors show that models trained with projected gradient descent (PGD) steps are more robust to adversarial attacks when the class boundary of the adversarial example is closer to the noisy-class boundary than when it is further away from the original class boundary. They also show that the robustness of a model trained with PGD steps is enhanced when the noisy labels are close to the original label boundary.    The authors also propose a new NL correction, called AT, which is a general-purpose robust learning criterion. They show that AT is more robust than training with NL, and that sample selection for NL is more stable when AT is used in conjunction with training.  The paper also shows that AT improves the natural accuracy of NL when trained with NL. The main contribution of the paper is that AT has smooth smoothing effects, which means that AT can be used to improve the performance of NL on natural data. The paper further shows that the smoothing effect of AT is independent of the noise level of the training data. ","This paper proposes a new measure to measure the intrinsic geometric property of robustness to noisy labels (NL) and adversarial examples. The authors show that models trained with projected gradient descent (PGD) steps are more robust to adversarial attacks when the class boundary of the adversarial example is closer to the noisy-class boundary than when it is further away from the original class boundary. They also show that the robustness of a model trained with PGD steps is enhanced when the noisy labels are close to the original label boundary.    The authors also propose a new NL correction, called AT, which is a general-purpose robust learning criterion. They show that AT is more robust than training with NL, and that sample selection for NL is more stable when AT is used in conjunction with training.  The paper also shows that AT improves the natural accuracy of NL when trained with NL. The main contribution of the paper is that AT has smooth smoothing effects, which means that AT can be used to improve the performance of NL on natural data. The paper further shows that the smoothing effect of AT is independent of the noise level of the training data. "
3205,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"language processing CONJUNCTION protein folding. protein folding CONJUNCTION language processing. classification CONJUNCTION language processing. language processing CONJUNCTION classification. Neural network models USED-FOR tasks. classification HYPONYM-OF tasks. language processing HYPONYM-OF tasks. protein folding HYPONYM-OF tasks. adversarial inputs FEATURE-OF reliability. small input perturbations HYPONYM-OF adversarial inputs. neural networks USED-FOR critical systems. expected robustness EVALUATE-FOR neural network model. statistical method EVALUATE-FOR neural network model. statistical method USED-FOR expected robustness. random input perturbation USED-FOR misclassification. robustness EVALUATE-FOR models. neural network certification USED-FOR safety - critical applications. risk and robustness assessments USED-FOR risk mitigation. risk mitigation USED-FOR neural network certification. categorial basis USED-FOR risk mitigation. categorial basis USED-FOR risk and robustness assessments. Generic are model, method, and approach. OtherScientificTerm is Adversarial inputs. Method are Robustness Measurement and Assessment ( RoMA ), RoMA, verification methods, and classification network. Metric are model ’s robustness, robustness levels, and categorial robustness. ","This paper studies the robustness of neural network models for a variety of tasks (classification, language processing, protein folding, etc.). Neural network models have been shown to be robust to adversarial inputs on a number of tasks. Adversarial inputs are defined as inputs that can be perturbed in such a way that the model’s robustness drops significantly. This paper proposes a new method called Robustness Measurement and Assessment (RoMA) to measure the reliability of a model on adversarial input. The authors show that robustness levels are highly correlated with the number of perturbations that are used to train the model. They also show that the reliability is highly correlated across different types of robustness.    The authors propose a statistical method for measuring the expected robustness to perturbation of a neural network model. The method is based on the observation that the adversarial robustness on a small number of inputs (i.e. small input perturbings) can be measured as a function of the classification network's robustness, and the authors propose to use this statistical method to evaluate the expected risk of a given neural network for a given critical systems. They show that under certain conditions, models with high robustness are more likely to be certified to be a critical system, and that models with low robustness have a higher probability of being certified.  The paper also shows that a random inputperturbation can lead to misclassification.  Finally, the authors demonstrate that neural network certification for safety-critical applications can be achieved through risk and robustness assessments on a categorial basis to perform risk mitigation for risk mitigation, which can be used to certify that a model is robust.  RoMA is evaluated on three different verification methods, and it is shown that the proposed approach is more robust than existing methods. ","This paper studies the robustness of neural network models for a variety of tasks (classification, language processing, protein folding, etc.). Neural network models have been shown to be robust to adversarial inputs on a number of tasks. Adversarial inputs are defined as inputs that can be perturbed in such a way that the model’s robustness drops significantly. This paper proposes a new method called Robustness Measurement and Assessment (RoMA) to measure the reliability of a model on adversarial input. The authors show that robustness levels are highly correlated with the number of perturbations that are used to train the model. They also show that the reliability is highly correlated across different types of robustness.    The authors propose a statistical method for measuring the expected robustness to perturbation of a neural network model. The method is based on the observation that the adversarial robustness on a small number of inputs (i.e. small input perturbings) can be measured as a function of the classification network's robustness, and the authors propose to use this statistical method to evaluate the expected risk of a given neural network for a given critical systems. They show that under certain conditions, models with high robustness are more likely to be certified to be a critical system, and that models with low robustness have a higher probability of being certified.  The paper also shows that a random inputperturbation can lead to misclassification.  Finally, the authors demonstrate that neural network certification for safety-critical applications can be achieved through risk and robustness assessments on a categorial basis to perform risk mitigation for risk mitigation, which can be used to certify that a model is robust.  RoMA is evaluated on three different verification methods, and it is shown that the proposed approach is more robust than existing methods. "
3221,SP:6ba17dd4b31a39478abd995df894447675f2f974,"chunking USED-FOR cognitive science. HCM USED-FOR representations. non - i.i.d sequential data USED-FOR HCM. non - i.i.d sequential data USED-FOR representations. learning guarantees USED-FOR HCM. approaches USED-FOR representation learning. cognitive science CONJUNCTION theories of chunking. theories of chunking CONJUNCTION cognitive science. theories of chunking USED-FOR approaches. OtherScientificTerm are proximity, minimal atomic sequential units, sequential dependence, and partial representational structure. Method are hierarchical chunking model ( HCM ), and hierarchy of chunk representation. ","This paper proposes a hierarchical chunking model (HCM) for learning representations from non-i.i.d sequential data. The idea of chunking is an important topic in cognitive science, where the goal is to learn representations that have minimal atomic sequential units (i.e. units that share the same proximity to minimal atomic units). The authors show that HCM learns representations that are invariant to the sequential dependence on the number of atomic units and to the hierarchy of chunk representation. The authors also provide learning guarantees for HCM under the assumption that there is a partial representational structure. The paper also proposes two approaches for representation learning that are based on cognitive science and theories of chunked.","This paper proposes a hierarchical chunking model (HCM) for learning representations from non-i.i.d sequential data. The idea of chunking is an important topic in cognitive science, where the goal is to learn representations that have minimal atomic sequential units (i.e. units that share the same proximity to minimal atomic units). The authors show that HCM learns representations that are invariant to the sequential dependence on the number of atomic units and to the hierarchy of chunk representation. The authors also provide learning guarantees for HCM under the assumption that there is a partial representational structure. The paper also proposes two approaches for representation learning that are based on cognitive science and theories of chunked."
3237,SP:625e3908502fd5be949bb915116ed7569ba84298,"gradient flow FEATURE-OF neural reparametrization. graph convolutional network ( GCN ) USED-FOR neural network architecture. GCN USED-FOR aggregation function. gradients of the loss function USED-FOR aggregation function. network synchronization CONJUNCTION persistent homology optimization. persistent homology optimization CONJUNCTION network synchronization. method USED-FOR optimization problems. persistent homology optimization HYPONYM-OF optimization problems. network synchronization HYPONYM-OF optimization problems. OtherScientificTerm are optimization variables, maximum speed up, and Hessian. Method is neural network. Task is optimization. ","This paper proposes a new neural network architecture based on graph convolutional network (GCN). The authors show that the neural reparametrization of the gradient flow of a neural network can be decomposed into two optimization variables: (1) the number of parameters of the neural network, and (2) the size of the aggregation function of a GCN. The authors propose to use the gradients of the loss function of the GCN as an aggregation function to achieve a maximum speed up in the optimization. The proposed method is applied to two optimization problems: network synchronization and persistent homology optimization. ","This paper proposes a new neural network architecture based on graph convolutional network (GCN). The authors show that the neural reparametrization of the gradient flow of a neural network can be decomposed into two optimization variables: (1) the number of parameters of the neural network, and (2) the size of the aggregation function of a GCN. The authors propose to use the gradients of the loss function of the GCN as an aggregation function to achieve a maximum speed up in the optimization. The proposed method is applied to two optimization problems: network synchronization and persistent homology optimization. "
3253,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,Deep convolutional neural networks ( DCNNs ) USED-FOR image data. DCNNs USED-FOR supervised learning of image data. pre - labeled images USED-FOR real - world problems. SVMnet USED-FOR non - parametric image classification. SVMnet HYPONYM-OF method. method COMPARE DCNNs. DCNNs COMPARE method. accuracy EVALUATE-FOR DCNNs. SVMs COMPARE neural networks. neural networks COMPARE SVMs. accuracy EVALUATE-FOR method. DCNN architectures COMPARE SVMnet. SVMnet COMPARE DCNN architectures. DCNN architectures COMPARE SVMnet. SVMnet COMPARE DCNN architectures. ResNet-50 COMPARE SVMnet. SVMnet COMPARE ResNet-50. accuracy EVALUATE-FOR SVMnet. ResNet-50 HYPONYM-OF DCNN architectures. Material is labeled “ ground truth ” images. OtherScientificTerm is real - world cases. ,"This paper proposes a new method called SVMnet, which is a generalization of Deep convolutional neural networks (DCNNs) for image data. DCNNs are commonly used for supervised learning of image data, but not for real-world problems where the labeled “ground truth” images are not available. The authors propose to use pre-labeled images for this purpose. The proposed method, called SVMs, is a simple method that can be applied to non-parametric image classification, where SVMs are trained in a similar way as neural networks. The method is shown to achieve similar accuracy as DCNN architectures such as ResNet-50, but with a much smaller number of parameters. The paper also shows that SVMs can be used for nonparametric tasks, and that SVMnets are able to achieve better accuracy than DCNNions.    The authors also show that the proposed method is able to generalize well to real world cases. ","This paper proposes a new method called SVMnet, which is a generalization of Deep convolutional neural networks (DCNNs) for image data. DCNNs are commonly used for supervised learning of image data, but not for real-world problems where the labeled “ground truth” images are not available. The authors propose to use pre-labeled images for this purpose. The proposed method, called SVMs, is a simple method that can be applied to non-parametric image classification, where SVMs are trained in a similar way as neural networks. The method is shown to achieve similar accuracy as DCNN architectures such as ResNet-50, but with a much smaller number of parameters. The paper also shows that SVMs can be used for nonparametric tasks, and that SVMnets are able to achieve better accuracy than DCNNions.    The authors also show that the proposed method is able to generalize well to real world cases. "
3269,SP:a18f4697f350a864866dac871f581b8fc67e8088,"large graphs USED-FOR GNNs. distributed algorithm USED-FOR GNN training. centralized storage CONJUNCTION model learning. model learning CONJUNCTION centralized storage. excessive communication costs CONJUNCTION large memory overheads. large memory overheads CONJUNCTION excessive communication costs. excessive communication costs FEATURE-OF distributed GNN training methods. Learn Locally, Correct Globally ( LLCG ) HYPONYM-OF distributed GNN training technique. local machine USED-FOR GNN. local machine PART-OF LLCG. local data USED-FOR GNN. Global Server Corrections USED-FOR locally learned models. Global Server Corrections USED-FOR server. distributed methods USED-FOR GNNs. periodic model averaging USED-FOR distributed methods. global corrections USED-FOR fast convergence rate. global corrections USED-FOR residual error. real - world datasets EVALUATE-FOR LLCG. efficiency EVALUATE-FOR LLCG. Method are Graph Neural Networks ( GNNs ), and locally trained model. OtherScientificTerm are graph, privacy concern, dependency between nodes, node dependency, and irreducible residual error. Metric are scalability, and communication and memory overhead. ","Graph Neural Networks (GNNs) have been shown to be effective at learning GNNs on large graphs. However, distributed GNN training methods suffer from excessive communication costs and large memory overheads due to the need for centralized storage and model learning. This paper proposes a distributed algorithm for GNN learning, called Learn Locally, Correct Globally (LLCG), which is an extension of the distributed algorithm to GNN. In particular, the authors propose a new distributed version of the widely used Learn Local, Correct Global (LGCG) training technique. In LGCG, a local machine is used to train a GNN on the local data, and the server uses Global Server Corrections to update the locally learned models.   In the paper, the main idea of LLCG is to train the local machine in a distributed way, where each node in the graph has its own local machine, and each server uses a different server to update its local machine. The local machine updates the locally trained model, and then the server updates the global machine.  The authors argue that this is a good way to reduce the communication and memory overhead of GNN in distributed setting, and also to reduce privacy concern due to dependency between nodes. The authors also argue that existing distributed methods for training GNN for large graphs (e.g. periodic model averaging) suffer from scalability issues due to large number of nodes and high communication costs. To address this issue, they propose a variant of LGC, called Global Server Correction (GSC). In GSC, the server is updated on the server and the local machines, and updates are made to the server based on the updated local data. The server is then updated to the updated locally trained models. In addition, they also propose to use global corrections to improve the fast convergence rate and reduce the residual error due to irreducible residual error.  In experiments on several real-world datasets, they show that LLCG achieves state-of-the-art efficiency in terms of accuracy and efficiency. They also show that the global corrections are effective in reducing the communication cost and reduce node dependency. ","Graph Neural Networks (GNNs) have been shown to be effective at learning GNNs on large graphs. However, distributed GNN training methods suffer from excessive communication costs and large memory overheads due to the need for centralized storage and model learning. This paper proposes a distributed algorithm for GNN learning, called Learn Locally, Correct Globally (LLCG), which is an extension of the distributed algorithm to GNN. In particular, the authors propose a new distributed version of the widely used Learn Local, Correct Global (LGCG) training technique. In LGCG, a local machine is used to train a GNN on the local data, and the server uses Global Server Corrections to update the locally learned models.   In the paper, the main idea of LLCG is to train the local machine in a distributed way, where each node in the graph has its own local machine, and each server uses a different server to update its local machine. The local machine updates the locally trained model, and then the server updates the global machine.  The authors argue that this is a good way to reduce the communication and memory overhead of GNN in distributed setting, and also to reduce privacy concern due to dependency between nodes. The authors also argue that existing distributed methods for training GNN for large graphs (e.g. periodic model averaging) suffer from scalability issues due to large number of nodes and high communication costs. To address this issue, they propose a variant of LGC, called Global Server Correction (GSC). In GSC, the server is updated on the server and the local machines, and updates are made to the server based on the updated local data. The server is then updated to the updated locally trained models. In addition, they also propose to use global corrections to improve the fast convergence rate and reduce the residual error due to irreducible residual error.  In experiments on several real-world datasets, they show that LLCG achieves state-of-the-art efficiency in terms of accuracy and efficiency. They also show that the global corrections are effective in reducing the communication cost and reduce node dependency. "
3285,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"model USED-FOR Anytime inference. image classification PART-OF anytime visual recognition. unified and end - toend model approach USED-FOR anytime pixel - level recognition. depth and spatial resolution FEATURE-OF features. redesigned exit architecture CONJUNCTION spatial adaptivity. spatial adaptivity CONJUNCTION redesigned exit architecture. full model USED-FOR anytime inference. spatial adaptivity USED-FOR anytime inference. spatial adaptivity USED-FOR full model. redesigned exit architecture USED-FOR full model. accuracy EVALUATE-FOR full model. semantic segmentation EVALUATE-FOR approach. approach USED-FOR anytime inference. Cityscapes semantic segmentation CONJUNCTION MPII human pose estimation. MPII human pose estimation CONJUNCTION Cityscapes semantic segmentation. Cityscapes semantic segmentation EVALUATE-FOR approach. MPII human pose estimation EVALUATE-FOR approach. total FLOPs EVALUATE-FOR models. total FLOPs EVALUATE-FOR approach. accuracy - computation curve EVALUATE-FOR method. deep equilibrium networks CONJUNCTION feature - based stochastic sampling approach. feature - based stochastic sampling approach CONJUNCTION deep equilibrium networks. method COMPARE them. them COMPARE method. accuracy - computation curve EVALUATE-FOR them. method COMPARE deep equilibrium networks. deep equilibrium networks COMPARE method. method COMPARE feature - based stochastic sampling approach. feature - based stochastic sampling approach COMPARE method. OtherScientificTerm are exits, and prior predictions. Metric is total computation. Method is spatially adaptive approach. Task is human pose estimation. ","This paper proposes a unified and end-to-end model approach for anytime pixel-level recognition. Anytime inference is performed by learning a model that can be applied to any image classification in anytime visual recognition. The authors propose a spatially adaptive approach, where features are learned at both depth and spatial resolution, and exits are adjusted based on prior predictions. The full model is trained with a redesigned exit architecture and spatial adaptivity, and the final accuracy of the full model can be used for anytime inference. The approach is evaluated on Cityscapes semantic segmentation and MPII human pose estimation, where the authors show that the approach can achieve anytime inference performance comparable to the state-of-the-art in terms of total FLOPs for both models. They also show that their method outperforms deep equilibrium networks and a feature-based stochastic sampling approach, and shows an improved accuracy-computation curve compared to them. ","This paper proposes a unified and end-to-end model approach for anytime pixel-level recognition. Anytime inference is performed by learning a model that can be applied to any image classification in anytime visual recognition. The authors propose a spatially adaptive approach, where features are learned at both depth and spatial resolution, and exits are adjusted based on prior predictions. The full model is trained with a redesigned exit architecture and spatial adaptivity, and the final accuracy of the full model can be used for anytime inference. The approach is evaluated on Cityscapes semantic segmentation and MPII human pose estimation, where the authors show that the approach can achieve anytime inference performance comparable to the state-of-the-art in terms of total FLOPs for both models. They also show that their method outperforms deep equilibrium networks and a feature-based stochastic sampling approach, and shows an improved accuracy-computation curve compared to them. "
3301,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,Neural Processes USED-FOR stochastic processes. neural networks USED-FOR stochastic processes. Modeling functional uncertainty PART-OF learning stochastic processes. bootstrap method USED-FOR functional uncertainty. Gaussian assumption FEATURE-OF latent variable. bootstrap method USED-FOR Bootstrapping Neural Processes ( B(A)NP ). B(A)NP USED-FOR bootstrapping. ANP CONJUNCTION BANP. BANP CONJUNCTION ANP. NeuBANP USED-FOR bootstrap distribution of random functions. encoder CONJUNCTION loss function. loss function CONJUNCTION encoder. Bayesian optimization CONJUNCTION contextual multi - armed bandit. contextual multi - armed bandit CONJUNCTION Bayesian optimization. Bayesian optimization EVALUATE-FOR models. sequential decision - making tasks EVALUATE-FOR NP methods. NeuBANP COMPARE NP methods. NP methods COMPARE NeuBANP. functional uncertainty modeling EVALUATE-FOR NeuBANP. functional uncertainty modeling EVALUATE-FOR method. sequential decision - making tasks EVALUATE-FOR NeuBANP. Generic is approach. Method is Neural Bootstrapping Attentive Neural Processes ( NeuBANP ). ,"This paper proposes Neural Bootstrapping Attentive Neural Processes (NeuBANP), a new approach to learning stochastic processes using neural networks. Modeling functional uncertainty in learning is an important problem, and the authors propose a bootstrap method for learning functional uncertainty under a Gaussian assumption on the latent variable. Bootstrapped neural processes (B(A)NP) is a variant of the bootstrapping neural Processes, which uses a different bootstrap algorithm to bootstrap the bootstrap distribution of the neural process. The authors show that B(A(A))NP can be used as a bootstrapped alternative to ANP, BANP, and B(B)NP, and that the proposed approach is more robust to perturbations in the encoder and loss function. Experiments on Bayesian optimization, contextual multi-armed bandit, and sequential decision-making tasks show that the models outperform the previous state-of-the-art in all cases. The proposed method is also shown to perform well in functional uncertainty modeling, outperforming other NP methods. ","This paper proposes Neural Bootstrapping Attentive Neural Processes (NeuBANP), a new approach to learning stochastic processes using neural networks. Modeling functional uncertainty in learning is an important problem, and the authors propose a bootstrap method for learning functional uncertainty under a Gaussian assumption on the latent variable. Bootstrapped neural processes (B(A)NP) is a variant of the bootstrapping neural Processes, which uses a different bootstrap algorithm to bootstrap the bootstrap distribution of the neural process. The authors show that B(A(A))NP can be used as a bootstrapped alternative to ANP, BANP, and B(B)NP, and that the proposed approach is more robust to perturbations in the encoder and loss function. Experiments on Bayesian optimization, contextual multi-armed bandit, and sequential decision-making tasks show that the models outperform the previous state-of-the-art in all cases. The proposed method is also shown to perform well in functional uncertainty modeling, outperforming other NP methods. "
3317,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"promoter classification CONJUNCTION transaction factor binding sites prediction. transaction factor binding sites prediction CONJUNCTION promoter classification. regulatory genome modeling USED-FOR regulatory downstream tasks. regulatory genome modeling PART-OF genome biology research. promoter classification HYPONYM-OF regulatory downstream tasks. transaction factor binding sites prediction HYPONYM-OF regulatory downstream tasks. deep learning methods USED-FOR genome sequences. approach USED-FOR pre - training genome data. genome data USED-FOR multi - modal and self - supervised manner. multi - modal and self - supervised manner USED-FOR pre - training genome data. robustness EVALUATE-FOR model. pre - training tasks USED-FOR model. pre - training tasks USED-FOR robustness. genome sequences USED-FOR ATAC - seq dataset. ATAC - seq dataset EVALUATE-FOR model. transaction factor binding sites prediction CONJUNCTION disease risk estimation. disease risk estimation CONJUNCTION transaction factor binding sites prediction. disease risk estimation CONJUNCTION splicing sites prediction. splicing sites prediction CONJUNCTION disease risk estimation. promoter classification CONJUNCTION transaction factor binding sites prediction. transaction factor binding sites prediction CONJUNCTION promoter classification. regulatory downstream tasks EVALUATE-FOR GeneBERT. splicing sites prediction HYPONYM-OF regulatory downstream tasks. promoter classification HYPONYM-OF regulatory downstream tasks. transaction factor binding sites prediction HYPONYM-OF regulatory downstream tasks. disease risk estimation HYPONYM-OF regulatory downstream tasks. OtherScientificTerm is regulatory elements. Generic is them. Task is biological applications. Material are 1d sequence of genome data, and large - scale regulatory genomics data. ","This paper proposes GeneBERT, an approach to pre-training genome data using multi-modal and self-supervised manner on genome data generated from multiple modalities of regulatory elements. This is an interesting direction in genome biology research, where regulatory genome modeling has been an important problem in many regulatory downstream tasks such as promoter classification, transaction factor binding sites prediction, and disease risk estimation. The authors propose to use deep learning methods to learn genome sequences from a 1d sequence of genome data, and then use them for a variety of biological applications. They show that the proposed approach, GeneberT, achieves state-of-the-art robustness on the ATAC-seq dataset using genome sequences generated from different modalities. They also show that their model is able to improve the robustness to adversarial attacks on pre-trained tasks.    The authors also demonstrate the effectiveness of their approach on several regulatory downstream task, including: (1) promoting classification, (2) splicing sites prediction (3) and (4) regulatory factor binding site prediction (5).   This paper is well-written and well-motivated, and the authors have done a good job of explaining their approach.  However, there is a lack of comparison to other approaches, and there is no comparison to large-scale regulatory genomics data, which makes it difficult to judge the quality of the contribution of the paper.","This paper proposes GeneBERT, an approach to pre-training genome data using multi-modal and self-supervised manner on genome data generated from multiple modalities of regulatory elements. This is an interesting direction in genome biology research, where regulatory genome modeling has been an important problem in many regulatory downstream tasks such as promoter classification, transaction factor binding sites prediction, and disease risk estimation. The authors propose to use deep learning methods to learn genome sequences from a 1d sequence of genome data, and then use them for a variety of biological applications. They show that the proposed approach, GeneberT, achieves state-of-the-art robustness on the ATAC-seq dataset using genome sequences generated from different modalities. They also show that their model is able to improve the robustness to adversarial attacks on pre-trained tasks.    The authors also demonstrate the effectiveness of their approach on several regulatory downstream task, including: (1) promoting classification, (2) splicing sites prediction (3) and (4) regulatory factor binding site prediction (5).   This paper is well-written and well-motivated, and the authors have done a good job of explaining their approach.  However, there is a lack of comparison to other approaches, and there is no comparison to large-scale regulatory genomics data, which makes it difficult to judge the quality of the contribution of the paper."
3333,SP:841b12443d0274e34b78940f220b17d36798899b,"method USED-FOR detecting OOD samples. IGEOOD USED-FOR detecting OOD samples. IGEOOD HYPONYM-OF method. IGEOOD USED-FOR pre - trained neural network. geodesic ( FisherRao ) distance USED-FOR discriminator. confidence scores CONJUNCTION features. features CONJUNCTION confidence scores. features PART-OF deep neural network. confidence scores USED-FOR discriminator. logits outputs USED-FOR confidence scores. features USED-FOR discriminator. deep neural network USED-FOR discriminator. IGEOOD COMPARE state - of - the - art methods. state - of - the - art methods COMPARE IGEOOD. Method are machine learning ( ML ) systems, and ML model. OtherScientificTerm are OOD samples, and data distributions. Material is OOD data. ",This paper proposes a new method called IGEOOD for detecting OOD samples in machine learning (ML) systems. The proposed method is based on the observation that a pre-trained neural network trained on OOD data may not be able to distinguish between OOD and non-OD samples. The authors propose to use the geodesic (FisherRao) distance to train a discriminator that can distinguish between the two data distributions. The discriminator is trained using confidence scores from the logits outputs and features from a deep neural network. Experiments are conducted on a variety of datasets to show the effectiveness of the proposed method. The results show that IGEOLD outperforms other state-of-the-art methods.   ,This paper proposes a new method called IGEOOD for detecting OOD samples in machine learning (ML) systems. The proposed method is based on the observation that a pre-trained neural network trained on OOD data may not be able to distinguish between OOD and non-OD samples. The authors propose to use the geodesic (FisherRao) distance to train a discriminator that can distinguish between the two data distributions. The discriminator is trained using confidence scores from the logits outputs and features from a deep neural network. Experiments are conducted on a variety of datasets to show the effectiveness of the proposed method. The results show that IGEOLD outperforms other state-of-the-art methods.   
3349,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"translations CONJUNCTION rotations. rotations CONJUNCTION translations. group FEATURE-OF identity - preserving transformations. identity - preserving transformations USED-FOR representations of objects. translations HYPONYM-OF group. translations HYPONYM-OF identity - preserving transformations. group equivariance USED-FOR representation. Cover ’s Function Counting Theorem USED-FOR linearly separable and group - invariant binary dichotomies. linearly separable and group - invariant binary dichotomies USED-FOR equivariant representations of objects. element - wise nonlinearities CONJUNCTION global and local pooling. global and local pooling CONJUNCTION element - wise nonlinearities. convolutions CONJUNCTION element - wise nonlinearities. element - wise nonlinearities CONJUNCTION convolutions. relation USED-FOR operations. global and local pooling HYPONYM-OF operations. convolutions HYPONYM-OF operations. element - wise nonlinearities HYPONYM-OF operations. OtherScientificTerm are Equivariance, separable dichotomies, group action, and local pooling. Generic is theory. ","This paper studies equivariant representations of objects that are invariant to identity-preserving transformations in a group (e.g., translations, rotations, etc.). Equivariance is defined as the equivalence between separable dichotomies of a group action and the group equivariance of a representation. The paper builds on Cover’s Function Counting Theorem to prove linearly separable and group-invariant binary dichotomyies for the representation of an object. The theory relies on a relation between operations such as convolutions, element-wise nonlinearities, global and local pooling, etc. ","This paper studies equivariant representations of objects that are invariant to identity-preserving transformations in a group (e.g., translations, rotations, etc.). Equivariance is defined as the equivalence between separable dichotomies of a group action and the group equivariance of a representation. The paper builds on Cover’s Function Counting Theorem to prove linearly separable and group-invariant binary dichotomyies for the representation of an object. The theory relies on a relation between operations such as convolutions, element-wise nonlinearities, global and local pooling, etc. "
3365,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"models USED-FOR machine learning objectives. concept drift CONJUNCTION mitigating biases. mitigating biases CONJUNCTION concept drift. robustness HYPONYM-OF machine learning objectives. mitigating biases HYPONYM-OF machine learning objectives. concept drift HYPONYM-OF machine learning objectives. counterfactual explanations CONJUNCTION concept activation vectors. concept activation vectors CONJUNCTION counterfactual explanations. it USED-FOR models. pretrained models USED-FOR approach. prior ideas USED-FOR CCE. counterfactual explanations HYPONYM-OF prior ideas. concept activation vectors HYPONYM-OF prior ideas. CCE USED-FOR spurious correlation. spurious correlations USED-FOR models. CCE USED-FOR models. data USED-FOR models. medical applications EVALUATE-FOR CCE. Generic are model, and systematic approach. Method are conceptual counterfactual explanations ( CCE ), and classifier. OtherScientificTerm are human - understandable concepts, faint stripes, and model mistakes. ","This paper proposes a systematic approach to learning counterfactual explanations for human-interpretable concepts, i.e., explanations that explain why a model is misclassified. The approach is based on the idea that models trained on certain machine learning objectives (e.g., robustness, concept drift, mitigating biases, etc.) may not be able to explain human-understandable concepts. The authors propose to use a new systematic approach called Concept Counterfactual Explanations (CCE), which is a generalization of the idea of concept drift and mitigate biases. CCE is a simple systematic approach where a classifier is trained on a subset of the training data, and it is used to train models that are robust to spurious correlations between the training and test data. The paper shows that CCE can be used to identify spurious correlation between a model’s classifier and a set of prior ideas (counterfactual explanation, concept activation vectors, etc.). The paper also shows that the CCE method is able to identify models that have spurious correlations in the data, which is called “faint stripes”. The method is tested on medical applications and is shown to be effective at identifying spurious correlations.   ","This paper proposes a systematic approach to learning counterfactual explanations for human-interpretable concepts, i.e., explanations that explain why a model is misclassified. The approach is based on the idea that models trained on certain machine learning objectives (e.g., robustness, concept drift, mitigating biases, etc.) may not be able to explain human-understandable concepts. The authors propose to use a new systematic approach called Concept Counterfactual Explanations (CCE), which is a generalization of the idea of concept drift and mitigate biases. CCE is a simple systematic approach where a classifier is trained on a subset of the training data, and it is used to train models that are robust to spurious correlations between the training and test data. The paper shows that CCE can be used to identify spurious correlation between a model’s classifier and a set of prior ideas (counterfactual explanation, concept activation vectors, etc.). The paper also shows that the CCE method is able to identify models that have spurious correlations in the data, which is called “faint stripes”. The method is tested on medical applications and is shown to be effective at identifying spurious correlations.   "
3381,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"3D point cloud applications EVALUATE-FOR Kernel Point Convolution ( KPConv ). KPConv network USED-FOR mobile scenarios. KPConv USED-FOR neighbor - kernel correlation. Euclidean distance USED-FOR neighbor - kernel correlation. module USED-FOR KPConv. efficiency EVALUATE-FOR KPConv. Mobile Attention Kernel Point Convolution ( MAKPConv ) USED-FOR KPConv. Mobile Attention Kernel Point Convolution ( MAKPConv ) HYPONYM-OF module. efficiency EVALUATE-FOR module. depthwise kernel USED-FOR resource consumption. depthwise kernel USED-FOR MAKPConv. Inverted Residual Bottleneck ( IRB ) USED-FOR design space. MAKPConv USED-FOR 3D networks. Wide & Deep Predictor USED-FOR dense and sparse neural architecture representations. carrying feature engineering USED-FOR neural architecture representations. Wide & Deep Predictor USED-FOR error in performance prediction. searchable features USED-FOR carrying feature engineering. predictor USED-FOR design space. 3D point cloud classification and segmentation benchmarks EVALUATE-FOR NAS - crafted MAKPConv network. NAScrafted model SPVNAS COMPARE NAS - crafted MAKPConv network. NAS - crafted MAKPConv network COMPARE NAScrafted model SPVNAS. Multiply - Accumulates EVALUATE-FOR NAS - crafted MAKPConv network. mIOU EVALUATE-FOR NAS - crafted MAKPConv network. OtherScientificTerm are kernel relationship, and weak representation power. Method is Neighbor - Kernel attention. Metric is representation power. ","This paper proposes a new Kernel Point Convolution (KPKCConv) for 3D point cloud applications. The proposed KPConv network aims to improve the efficiency of the neighbor-kernel correlation between two points in Euclidean distance. The authors propose a module called Mobile Attention Kernel Point Convolution (MAKPConv), which is a module that improves the efficiency and performance of the original kernel point convolutional network in mobile scenarios. The key idea is to use a depthwise kernel to reduce the resource consumption, which is similar to Neighbor-Kernel attention.  The authors also propose a Wide & Deep Predictor to learn dense and sparse neural architecture representations, which can be used for carrying feature engineering to learn neural architectures representations with searchable features. The design space is optimized using Inverted Residual Bottleneck (IRB) and the predictor is used to guide the design space.  Experiments show that the NAS-crafted MAKPconv network outperforms the NAScrafted model SPVNAS on mIOU and mIOCP datasets on a number of standard 3d point cloud classification and segmentation benchmarks.    The main contribution of the paper is that the authors show that MAKCpConv can be applied to 3D networks with a variety of architectures. The paper also shows that the wide & deep Predictor is able to learn a good error in performance prediction, and that the proposed MAKCPConv has a good representation power.","This paper proposes a new Kernel Point Convolution (KPKCConv) for 3D point cloud applications. The proposed KPConv network aims to improve the efficiency of the neighbor-kernel correlation between two points in Euclidean distance. The authors propose a module called Mobile Attention Kernel Point Convolution (MAKPConv), which is a module that improves the efficiency and performance of the original kernel point convolutional network in mobile scenarios. The key idea is to use a depthwise kernel to reduce the resource consumption, which is similar to Neighbor-Kernel attention.  The authors also propose a Wide & Deep Predictor to learn dense and sparse neural architecture representations, which can be used for carrying feature engineering to learn neural architectures representations with searchable features. The design space is optimized using Inverted Residual Bottleneck (IRB) and the predictor is used to guide the design space.  Experiments show that the NAS-crafted MAKPconv network outperforms the NAScrafted model SPVNAS on mIOU and mIOCP datasets on a number of standard 3d point cloud classification and segmentation benchmarks.    The main contribution of the paper is that the authors show that MAKCpConv can be applied to 3D networks with a variety of architectures. The paper also shows that the wide & deep Predictor is able to learn a good error in performance prediction, and that the proposed MAKCPConv has a good representation power."
3397,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. robustness overestimation CONJUNCTION robustness - accuracy trade - off. robustness - accuracy trade - off CONJUNCTION robustness overestimation. problems PART-OF adversarial training. robustness - accuracy trade - off HYPONYM-OF problems. robust overfitting HYPONYM-OF problems. robustness overestimation HYPONYM-OF problems. lowquality samples PART-OF dataset. strategy USED-FOR data quality. learning behaviors USED-FOR strategy. learning behaviors USED-FOR data quality. data quality CONJUNCTION problems. problems CONJUNCTION data quality. problems FEATURE-OF adversarial training. data quality CONJUNCTION adversarial training. adversarial training CONJUNCTION data quality. robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. Material is low - quality data. Metric is adversarial robustness. ,"This paper studies the problem of robust overfitting, robustness overestimation, and robustness-accuracy trade-off in the context of adversarial training. The authors propose a new strategy to improve the data quality of a dataset with low-quality samples. The proposed strategy is based on the observation that adversarial robustness can be improved by learning behaviors that improve data quality. Experiments are conducted to demonstrate the effectiveness of the proposed strategy.    The paper is well-written, well-motivated, and well-structured. The data quality and problems of robust training are well-understood, and the paper is clearly written. ","This paper studies the problem of robust overfitting, robustness overestimation, and robustness-accuracy trade-off in the context of adversarial training. The authors propose a new strategy to improve the data quality of a dataset with low-quality samples. The proposed strategy is based on the observation that adversarial robustness can be improved by learning behaviors that improve data quality. Experiments are conducted to demonstrate the effectiveness of the proposed strategy.    The paper is well-written, well-motivated, and well-structured. The data quality and problems of robust training are well-understood, and the paper is clearly written. "
3413,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"neural network USED-FOR multivariate functions of bounded second mixed derivatives. Korobov functions HYPONYM-OF multivariate functions of bounded second mixed derivatives. upper bounds USED-FOR shallow and deep neural networks. quantities USED-FOR shallow and deep neural networks. upper bounds USED-FOR quantities. bounds FEATURE-OF activation functions. ReLU HYPONYM-OF activation functions. continuous function approximator USED-FOR Korobov functions. neural networks HYPONYM-OF near - optimal function approximators. OtherScientificTerm are training parameters, and curse of dimensionality. ","This paper considers the problem of learning multivariate functions of bounded second mixed derivatives of a neural network, called Korobov functions. The authors provide upper bounds on the generalization performance of shallow and deep neural networks based on these quantities for both the case of ReLU and ReLU activation functions. They show that for any continuous function approximator, the number of training parameters is bounded. They also show that the curse of dimensionality can be alleviated by the use of neural networks, and that the bounds for activation functions are tight for ReLU. Finally, they show that neural networks can be considered as near-optimators of the above function.  ","This paper considers the problem of learning multivariate functions of bounded second mixed derivatives of a neural network, called Korobov functions. The authors provide upper bounds on the generalization performance of shallow and deep neural networks based on these quantities for both the case of ReLU and ReLU activation functions. They show that for any continuous function approximator, the number of training parameters is bounded. They also show that the curse of dimensionality can be alleviated by the use of neural networks, and that the bounds for activation functions are tight for ReLU. Finally, they show that neural networks can be considered as near-optimators of the above function.  "
3429,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"Populations USED-FOR language. agent population size FEATURE-OF speaker - listener Lewis Game. agent population size USED-FOR emergent language properties. training speed CONJUNCTION network capacity. network capacity CONJUNCTION training speed. speaker - listener asymmetry USED-FOR language structure. network capacity HYPONYM-OF diversity factors. training speed HYPONYM-OF diversity factors. relative difference of factors USED-FOR emergent language properties. Material are sociolinguistic literature, and structured languages. Method is neural agents. OtherScientificTerm are agent community, population heterogeneity, confounding factors, training speed heterogeneities, and simulated communities. ","This paper studies the emergent language properties of a population of agents in the Lewis Game setting, where the agent population size in the speaker-listener Lewis Game depends on the population size of the agent community. Populations of agents that share the same language tend to share similar language, but differ in the diversity factors (e.g., training speed, network capacity, etc.). The authors show that the diversity of the population is related to the relative difference of factors in the population, and that diversity factors such as training speed and network capacity are related to language structure. The authors also show that there is a speaker-l listener asymmetry in the language structure, which is a common phenomenon in the sociolinguistic literature. They show that neural agents are able to learn to communicate in structured languages, but not in languages where the population heterogeneity is high.    The authors further show that a number of confounding factors, e.g. training speed heterogeneities, population size, etc., are responsible for population heterogeneity. They also propose to use simulated communities to study the impact of these factors. ","This paper studies the emergent language properties of a population of agents in the Lewis Game setting, where the agent population size in the speaker-listener Lewis Game depends on the population size of the agent community. Populations of agents that share the same language tend to share similar language, but differ in the diversity factors (e.g., training speed, network capacity, etc.). The authors show that the diversity of the population is related to the relative difference of factors in the population, and that diversity factors such as training speed and network capacity are related to language structure. The authors also show that there is a speaker-l listener asymmetry in the language structure, which is a common phenomenon in the sociolinguistic literature. They show that neural agents are able to learn to communicate in structured languages, but not in languages where the population heterogeneity is high.    The authors further show that a number of confounding factors, e.g. training speed heterogeneities, population size, etc., are responsible for population heterogeneity. They also propose to use simulated communities to study the impact of these factors. "
3445,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"Graph Neural Networks ( GNNs ) USED-FOR node classification task. node features CONJUNCTION graph topology. graph topology CONJUNCTION node features. node features USED-FOR Graph Neural Networks ( GNNs ). heterophilic graphs EVALUATE-FOR models. homophily FEATURE-OF GNNs. polynomial graph filters USED-FOR models. models USED-FOR polynomials. spectrum FEATURE-OF adaptive polynomial filters. model USED-FOR filter. classification accuracy EVALUATE-FOR model. node classification task EVALUATE-FOR polynomials. model COMPARE polynomial filter - based approaches. polynomial filter - based approaches COMPARE model. model COMPARE models. models COMPARE model. models COMPARE polynomial filter - based approaches. polynomial filter - based approaches COMPARE models. OtherScientificTerm are connected nodes, overdetermined system of equations, and eigencomponents. Method are polynomial graph filter models, eigendecomposition of the graph, and latent polynomial filters. Material is anonymized code. ","Graph Neural Networks (GNNs) for node classification task are based on node features and graph topology. The authors study the homophily of GNNs on heterophilic graphs and propose two models based on polynomial graph filters. They show that polynomials are more homophilic than other models, and that models trained with polynomorphic graph filters are more robust to changes in the number of connected nodes and the eigendecomposition of the graph. They also show that the spectrum of the adaptive polynometric filters is the same as that of an overdetermined system of equations, which is a result of the over-parameterization of the eigencomponents of the filters. Finally, they show that a model trained with the proposed model is able to learn a filter that maximizes the classification accuracy on a given node.   The authors show that their proposed model outperforms polynomic filter-based approaches and other models that do not use latent polynomorphisms. They demonstrate that their model is more robust than polynomatic filters on the node classification problem.  They also provide some experiments on anonymized code, showing that the model can be trained on a dataset with heterophily.","Graph Neural Networks (GNNs) for node classification task are based on node features and graph topology. The authors study the homophily of GNNs on heterophilic graphs and propose two models based on polynomial graph filters. They show that polynomials are more homophilic than other models, and that models trained with polynomorphic graph filters are more robust to changes in the number of connected nodes and the eigendecomposition of the graph. They also show that the spectrum of the adaptive polynometric filters is the same as that of an overdetermined system of equations, which is a result of the over-parameterization of the eigencomponents of the filters. Finally, they show that a model trained with the proposed model is able to learn a filter that maximizes the classification accuracy on a given node.   The authors show that their proposed model outperforms polynomic filter-based approaches and other models that do not use latent polynomorphisms. They demonstrate that their model is more robust than polynomatic filters on the node classification problem.  They also provide some experiments on anonymized code, showing that the model can be trained on a dataset with heterophily."
3461,SP:903545b1b340ec5c13070e0f25f550c444de4124,biomedical structure prediction CONJUNCTION social relationship analysis. social relationship analysis CONJUNCTION biomedical structure prediction. graphs FEATURE-OF Shortest Distance Queries ( SDQs ). Shortest Distance Queries ( SDQs ) HYPONYM-OF network analysis. social relationship analysis HYPONYM-OF network analysis. biomedical structure prediction HYPONYM-OF network analysis. Approximate algorithms of SDQs USED-FOR complex graph applications. reduced complexity FEATURE-OF Approximate algorithms of SDQs. approaches USED-FOR embedding - based distance prediction. accuracy EVALUATE-FOR embedding - based distance prediction. efficiency EVALUATE-FOR embedding - based distance prediction. truncated random walk CONJUNCTION Pointwise Mutual Information ( PMI)-based optimization. Pointwise Mutual Information ( PMI)-based optimization CONJUNCTION truncated random walk. predictor USED-FOR global extraction of nodes ’ mutual shortest distance. Pointwise Mutual Information ( PMI)-based optimization USED-FOR Embedding - based distance prediction. truncated random walk USED-FOR Embedding - based distance prediction. Random walk HYPONYM-OF unstrained node sequence. limited distance exploration FEATURE-OF Random walk. graph domain FEATURE-OF intrinsic metric. distance range FEATURE-OF Betweenness Centrality(BC)-based random walk. intrinsic metric USED-FOR distance range. intrinsic metric EVALUATE-FOR Betweenness Centrality(BC)-based random walk. steering optimization objective USED-FOR global distance matrix. strategy USED-FOR global distance matrix. maximum likelihood optimization COMPARE PMI - based optimization. PMI - based optimization COMPARE maximum likelihood optimization. strategy USED-FOR distance relation. Distance Resampling ( DR ) COMPARE PMI - based optimization. PMI - based optimization COMPARE Distance Resampling ( DR ). maximum likelihood optimization USED-FOR Distance Resampling ( DR ). walk paths USED-FOR Distance Resampling ( DR ). steering optimization objective USED-FOR strategy. steering optimization objective USED-FOR distance relation. method COMPARE methods. methods COMPARE method. realworld graph datasets USED-FOR SDQ problems. SDQ problems EVALUATE-FOR method. SDQ problems EVALUATE-FOR methods. realworld graph datasets EVALUATE-FOR method. realworld graph datasets,"Shortest Distance Queries (SDQs) on graphs are an important problem in network analysis, particularly in biomedical structure prediction and social relationship analysis. Approximate algorithms of SDQs with reduced complexity have been proposed for many complex graph applications. This paper proposes two approaches for embedding-based distance prediction with improved accuracy and efficiency, based on truncated random walk and Pointwise Mutual Information (PMI)-based optimization. Embedding - based distance prediction is performed by truncated Random walk on an unstrained node sequence (random walk with limited distance exploration). Random walk is an intrinsic metric in the graph domain that measures the distance range of a pair of nodes. The paper proposes to use a predictor to guide the global extraction of nodes’ mutual shortest distance. The proposed strategy is based on a steering optimization objective to optimize the global distance matrix. The authors show that Distance Resampling (DR) using walk paths is more efficient than maximum likelihood optimization and PMI-based optimization, and that the proposed strategy can learn a global distance relation using the same strategy. The method is evaluated on two realworld graph datasets for two SDQ problems, and compared to other methods. ","Shortest Distance Queries (SDQs) on graphs are an important problem in network analysis, particularly in biomedical structure prediction and social relationship analysis. Approximate algorithms of SDQs with reduced complexity have been proposed for many complex graph applications. This paper proposes two approaches for embedding-based distance prediction with improved accuracy and efficiency, based on truncated random walk and Pointwise Mutual Information (PMI)-based optimization. Embedding - based distance prediction is performed by truncated Random walk on an unstrained node sequence (random walk with limited distance exploration). Random walk is an intrinsic metric in the graph domain that measures the distance range of a pair of nodes. The paper proposes to use a predictor to guide the global extraction of nodes’ mutual shortest distance. The proposed strategy is based on a steering optimization objective to optimize the global distance matrix. The authors show that Distance Resampling (DR) using walk paths is more efficient than maximum likelihood optimization and PMI-based optimization, and that the proposed strategy can learn a global distance relation using the same strategy. The method is evaluated on two realworld graph datasets for two SDQ problems, and compared to other methods. "
3477,SP:13db440061fed785f05bb41d0767225403ecf7a1,"fact - checking CONJUNCTION open dialogue. open dialogue CONJUNCTION fact - checking. question answering CONJUNCTION fact - checking. fact - checking CONJUNCTION question answering. web corpus USED-FOR knowledge - dependent downstream tasks. Large Language Models ( LMs ) USED-FOR world knowledge. open dialogue HYPONYM-OF knowledge - dependent downstream tasks. question answering HYPONYM-OF knowledge - dependent downstream tasks. fact - checking HYPONYM-OF knowledge - dependent downstream tasks. world knowledge PART-OF LMs. Continual Knowledge Learning ( CKL ) HYPONYM-OF continual learning ( CL ) problem. retention of time - invariant world knowledge CONJUNCTION update of outdated knowledge. update of outdated knowledge CONJUNCTION retention of time - invariant world knowledge. update of outdated knowledge CONJUNCTION acquisition of new knowledge. acquisition of new knowledge CONJUNCTION update of outdated knowledge. metric USED-FOR retention of time - invariant world knowledge. metric USED-FOR acquisition of new knowledge. metric USED-FOR update of outdated knowledge. benchmark USED-FOR retention of time - invariant world knowledge. benchmark CONJUNCTION metric. metric CONJUNCTION benchmark. CKL USED-FOR ever - changing LMs1. OtherScientificTerm are real - world scenarios, catastrophic forgetting, invariant knowledge, and knowledge forgetting. Task is maintenance of ever - changing LMs. Generic are baselines, and CL setups. Method is parameter expansion. ","This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL) which aims to address the issue of catastrophic forgetting in the maintenance of ever-changing LMs. The authors propose a new benchmark for knowledge-dependent downstream tasks (e.g. question answering, fact-checking, open dialogue) from a web corpus, and show that existing Large Language Models (LMs) are unable to maintain world knowledge in the context of knowledge-dependant downstream tasks such as question answering and fact-testing. The paper also shows that existing baselines are not robust to catastrophic forgetting and that the problem of invariant knowledge is a major cause of knowledge forgetting in real-world scenarios. To address this problem, the authors introduce a benchmark and a new metric for the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. Experiments show that the proposed CKL is able to learn invariant and continual knowledge in a way that does not require any parameter expansion, which is a common problem in existing CL setups. ","This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL) which aims to address the issue of catastrophic forgetting in the maintenance of ever-changing LMs. The authors propose a new benchmark for knowledge-dependent downstream tasks (e.g. question answering, fact-checking, open dialogue) from a web corpus, and show that existing Large Language Models (LMs) are unable to maintain world knowledge in the context of knowledge-dependant downstream tasks such as question answering and fact-testing. The paper also shows that existing baselines are not robust to catastrophic forgetting and that the problem of invariant knowledge is a major cause of knowledge forgetting in real-world scenarios. To address this problem, the authors introduce a benchmark and a new metric for the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. Experiments show that the proposed CKL is able to learn invariant and continual knowledge in a way that does not require any parameter expansion, which is a common problem in existing CL setups. "
3493,SP:639fd88482330389019fb5be7446a909b99a8609,"approach USED-FOR supervised learning task. Decision trees USED-FOR supervised learning task. Decision trees USED-FOR applications. medical imaging CONJUNCTION computer vision. computer vision CONJUNCTION medical imaging. computer vision HYPONYM-OF applications. medical imaging HYPONYM-OF applications. feature CONJUNCTION threshold. threshold CONJUNCTION feature. exhaustive search algorithm USED-FOR criterion minimization problem. stochastic approach USED-FOR criterion minimization. algorithm COMPARE exhaustive search. exhaustive search COMPARE algorithm. algorithm COMPARE decision tree learning methods. decision tree learning methods COMPARE algorithm. algorithm COMPARE baseline non - stochastic approach. baseline non - stochastic approach COMPARE algorithm. baseline non - stochastic approach HYPONYM-OF decision tree learning methods. algorithm COMPARE baseline algorithm. baseline algorithm COMPARE algorithm. accuracy CONJUNCTION computational cost. computational cost CONJUNCTION accuracy. computational cost EVALUATE-FOR algorithm. accuracy EVALUATE-FOR algorithm. computational cost EVALUATE-FOR baseline algorithm. accuracy EVALUATE-FOR baseline algorithm. algorithm USED-FOR Haar tree. MNIST dataset FEATURE-OF Haar tree. tree COMPARE axis - aligned tree. axis - aligned tree COMPARE tree. MNIST COMPARE axis - aligned tree. axis - aligned tree COMPARE MNIST. MNIST EVALUATE-FOR tree. test accuracy EVALUATE-FOR tree. OtherScientificTerm are leaf nodes, stopping criterion, node, features, and criterion. Method is oblique trees. Metric is training times. ","This paper proposes a novel approach to tackle the supervised learning task using decision trees. Decision trees are widely used in many applications, such as medical imaging and computer vision, where the goal is to find a set of leaf nodes that minimizes a stopping criterion, i.e., the feature and a threshold. In this paper, the authors propose a stochastic approach to the criterion minimization problem, where each node in the tree is a node that has the same feature and threshold, and oblique trees are nodes that have different features and thresholds. The authors propose an exhaustive search algorithm to solve the problem, and show that their algorithm outperforms the exhaustive search in terms of accuracy and computational cost. They also compare their algorithm with other decision tree learning methods, including a baseline non-stochastic approach, and a baseline algorithm that uses the Haar tree from the MNIST dataset. They show that the algorithm is able to find the optimal solution to the problem faster than the baseline algorithm, and that the tree on MNIST has better test accuracy than an axis-aligned tree.    The authors also show that a similar algorithm can also be used to find an algorithm that can find the best solution for a specific node in a tree. The algorithm is based on the idea of finding a tree that maximizes the test accuracy of a node with the same criterion, and the authors show that this algorithm can find a tree with the best test accuracy in a single step. The paper also shows that the proposed algorithm can be applied to the case where the node is in an oblique tree and the criterion is the same as the one, and shows that it can also find a good solution for the same.  Finally, they show that they can also train their algorithm on the same number of training times as the baseline. ","This paper proposes a novel approach to tackle the supervised learning task using decision trees. Decision trees are widely used in many applications, such as medical imaging and computer vision, where the goal is to find a set of leaf nodes that minimizes a stopping criterion, i.e., the feature and a threshold. In this paper, the authors propose a stochastic approach to the criterion minimization problem, where each node in the tree is a node that has the same feature and threshold, and oblique trees are nodes that have different features and thresholds. The authors propose an exhaustive search algorithm to solve the problem, and show that their algorithm outperforms the exhaustive search in terms of accuracy and computational cost. They also compare their algorithm with other decision tree learning methods, including a baseline non-stochastic approach, and a baseline algorithm that uses the Haar tree from the MNIST dataset. They show that the algorithm is able to find the optimal solution to the problem faster than the baseline algorithm, and that the tree on MNIST has better test accuracy than an axis-aligned tree.    The authors also show that a similar algorithm can also be used to find an algorithm that can find the best solution for a specific node in a tree. The algorithm is based on the idea of finding a tree that maximizes the test accuracy of a node with the same criterion, and the authors show that this algorithm can find a tree with the best test accuracy in a single step. The paper also shows that the proposed algorithm can be applied to the case where the node is in an oblique tree and the criterion is the same as the one, and shows that it can also find a good solution for the same.  Finally, they show that they can also train their algorithm on the same number of training times as the baseline. "
3509,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,Learning rate schedulers USED-FOR deep neural networks. SGD USED-FOR problems. optimizing quadratic objectives HYPONYM-OF problems. eigenvalue distribution FEATURE-OF Hessian matrix. Eigencurve HYPONYM-OF learning rate schedules. SGD USED-FOR quadratic objectives. Eigencurve USED-FOR SGD. learning rate schedules USED-FOR SGD. minimax optimal convergence rates EVALUATE-FOR learning rate schedules. Eigencurve COMPARE step decay. step decay COMPARE Eigencurve. step decay USED-FOR image classification tasks. image classification tasks EVALUATE-FOR Eigencurve. CIFAR-10 USED-FOR image classification tasks. learning rate schedulers USED-FOR eigencurve. theory USED-FOR learning rate schedulers. schedulers COMPARE cosine decay. cosine decay COMPARE schedulers. schedulers USED-FOR optimal shape. cosine decay USED-FOR optimal shape. schedulers COMPARE cosine decay. cosine decay COMPARE schedulers. ,"This paper studies learning rate schedulers for deep neural networks. In particular, the authors consider two problems: (1) optimizing quadratic objectives, and (2) optimizing the eigenvalue distribution of the Hessian matrix. The authors propose two learning rate schedules, called Eigencurve and Eigendecay, for these two problems, and show that SGD converges to the optimal solution of these two sets of SGD with different learning rates. They also provide minimax optimal convergence rates for the proposed learning rate schedule.  Empirical results show that the proposed EigenCurve outperforms step decay on image classification tasks on CIFAR-10, and is competitive with SGD for the same number of iterations for the quadratically objectives.  The authors also provide a theory that shows that the learned eigencurves of the two types of learning rate Schedules converge to the same solution.  Finally, they also show that their theory shows that learning rate scheduling achieves better results than cosine decay in finding the optimal shape.   ","This paper studies learning rate schedulers for deep neural networks. In particular, the authors consider two problems: (1) optimizing quadratic objectives, and (2) optimizing the eigenvalue distribution of the Hessian matrix. The authors propose two learning rate schedules, called Eigencurve and Eigendecay, for these two problems, and show that SGD converges to the optimal solution of these two sets of SGD with different learning rates. They also provide minimax optimal convergence rates for the proposed learning rate schedule.  Empirical results show that the proposed EigenCurve outperforms step decay on image classification tasks on CIFAR-10, and is competitive with SGD for the same number of iterations for the quadratically objectives.  The authors also provide a theory that shows that the learned eigencurves of the two types of learning rate Schedules converge to the same solution.  Finally, they also show that their theory shows that learning rate scheduling achieves better results than cosine decay in finding the optimal shape.   "
3525,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"Offline reinforcement learning USED-FOR control policies. total variation distance FEATURE-OF model. imaginary rollout horizon HYPONYM-OF hyperparameters. Bayesian Optimization USED-FOR hyperparameters. Material is online data collection. Method are offline model - based reinforcement learning, dynamics model, probabilistic model, and uncertainty heuristics. OtherScientificTerm are model uncertainty, pessimistic MDP, MDP, pessimistic return, and estimated model uncertainty. Generic are Existing methods, heuristics, and protocols. ","Offline reinforcement learning for learning control policies in the presence of online data collection is an important problem in the context of offline model-based reinforcement learning. Existing methods are based on the assumption that the dynamics model is a probabilistic model and that the total variation distance between the model and the true dynamics is a function of the model uncertainty. This paper proposes a pessimistic MDP, which is a variant of the original MDP. The authors propose two heuristics: (1) a pessimistic return, where the pessimistic return depends on the estimated model uncertainty, and (2) an imaginary rollout horizon, where hyperparameters (e.g., the imaginary horizon) are learned via Bayesian Optimization. The paper also proposes a number of other protocols to improve the performance of the proposed methods. ","Offline reinforcement learning for learning control policies in the presence of online data collection is an important problem in the context of offline model-based reinforcement learning. Existing methods are based on the assumption that the dynamics model is a probabilistic model and that the total variation distance between the model and the true dynamics is a function of the model uncertainty. This paper proposes a pessimistic MDP, which is a variant of the original MDP. The authors propose two heuristics: (1) a pessimistic return, where the pessimistic return depends on the estimated model uncertainty, and (2) an imaginary rollout horizon, where hyperparameters (e.g., the imaginary horizon) are learned via Bayesian Optimization. The paper also proposes a number of other protocols to improve the performance of the proposed methods. "
3541,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"methods USED-FOR sampling. critic networks USED-FOR policy learning. TD - error HYPONYM-OF features. TD - error USED-FOR sampled experiences. sampled experiences USED-FOR Q - values. auxiliary features USED-FOR TD - error. auxiliary features USED-FOR sampling. learnable features USED-FOR experience replay method. curriculum learning USED-FOR predicting Q - values. MaPER USED-FOR curriculum learning. MaPER COMPARE vanilla PER. vanilla PER COMPARE MaPER. computational overhead COMPARE vanilla PER. vanilla PER COMPARE computational overhead. critic network USED-FOR curriculum learning. critic network USED-FOR predicting Q - values. tasks EVALUATE-FOR MaPER. offpolicy MfRL CONJUNCTION MbRL. MbRL CONJUNCTION offpolicy MfRL. MaPER USED-FOR MbRL. off - policy MfRL algorithms USED-FOR policy optimization procedure. MaPER USED-FOR offpolicy MfRL. off - policy MfRL algorithms PART-OF MbRL. Method are Experience replay, and model - based RL ( MbRL ). ","This paper proposes a new experience replay method based on learnable features. Experience replay is an important problem in RL, but existing methods for sampling are computationally expensive. This paper proposes to use auxiliary features (e.g., TD-error) to improve the quality of sampled experiences for Q-values. The authors also propose a new curriculum learning method, called MaPER, which uses critic networks for policy learning. The idea is to learn a critic network for curriculum learning for the task of Q-value prediction from sampled experiences. The proposed MaPER method is evaluated on a number of tasks, and is shown to outperform vanilla PER in terms of computational overhead. MaPER is also applied to offpolicy MfRL and model-based RL (MbRL). The authors show that MaPER improves the performance of offpolicy and MbRL using MaPER for both. The main contribution of the paper is the use of off-policy and model based RL algorithms in the policy optimization procedure. ","This paper proposes a new experience replay method based on learnable features. Experience replay is an important problem in RL, but existing methods for sampling are computationally expensive. This paper proposes to use auxiliary features (e.g., TD-error) to improve the quality of sampled experiences for Q-values. The authors also propose a new curriculum learning method, called MaPER, which uses critic networks for policy learning. The idea is to learn a critic network for curriculum learning for the task of Q-value prediction from sampled experiences. The proposed MaPER method is evaluated on a number of tasks, and is shown to outperform vanilla PER in terms of computational overhead. MaPER is also applied to offpolicy MfRL and model-based RL (MbRL). The authors show that MaPER improves the performance of offpolicy and MbRL using MaPER for both. The main contribution of the paper is the use of off-policy and model based RL algorithms in the policy optimization procedure. "
3557,SP:0db83e057c21ac10fe91624876498d8456797492,"fast learning CONJUNCTION training safety. training safety CONJUNCTION fast learning. human knowledge PART-OF reinforcement learning. Human intervention USED-FOR human knowledge. trial - and - error exploration CONJUNCTION human ’s partial demonstration. human ’s partial demonstration CONJUNCTION trial - and - error exploration. human ’s partial demonstration USED-FOR HACO. agent USED-FOR proxy values. HACO USED-FOR agent. HACO USED-FOR proxy state - action values. environmental reward USED-FOR HACO. safe driving benchmark EVALUATE-FOR sample efficiency. sample efficiency EVALUATE-FOR HACO. safe driving benchmark EVALUATE-FOR HACO. reinforcement learning CONJUNCTION imitation learning baselines. imitation learning baselines CONJUNCTION reinforcement learning. Method are learning agent, and Human - AI Copilot Optimization ( HACO ). OtherScientificTerm are human expert, trivial behaviors, human interventions, and human intervention budget. Generic is It. ","This paper proposes a new learning agent, called Human-AI Copilot Optimization (HACO), which aims to balance fast learning and training safety by incorporating human knowledge into reinforcement learning. Human intervention is used to guide the agent to learn human knowledge in the presence of a human expert. HACO uses a combination of trial-and-error exploration and a human’s partial demonstration to learn a proxy state-action values. The agent learns the proxy values by learning an agent that is able to learn to imitate the human expert in a safe way. The paper also proposes an environmental reward to encourage the agent not to learn trivial behaviors that are harmful to human interventions. It is shown that the agent learns to imitate human interventions in an environment where the human intervention budget is limited. Experiments on the safe driving benchmark show that HACE improves sample efficiency and is competitive with reinforcement learning and imitation learning baselines. ","This paper proposes a new learning agent, called Human-AI Copilot Optimization (HACO), which aims to balance fast learning and training safety by incorporating human knowledge into reinforcement learning. Human intervention is used to guide the agent to learn human knowledge in the presence of a human expert. HACO uses a combination of trial-and-error exploration and a human’s partial demonstration to learn a proxy state-action values. The agent learns the proxy values by learning an agent that is able to learn to imitate the human expert in a safe way. The paper also proposes an environmental reward to encourage the agent not to learn trivial behaviors that are harmful to human interventions. It is shown that the agent learns to imitate human interventions in an environment where the human intervention budget is limited. Experiments on the safe driving benchmark show that HACE improves sample efficiency and is competitive with reinforcement learning and imitation learning baselines. "
3573,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"long - horizon unsegmented demonstrations USED-FOR subskills. hierarchical structure USED-FOR Transferring and reorganizing modular sub - skills. Dual Meta Imitation Learning ( DMIL ) HYPONYM-OF hierarchical meta imitation learning method. highlevel network USED-FOR sub - skill adaptation. likelihood of state - action pairs USED-FOR supervision. likelihood of state - action pairs USED-FOR sub - skill. high - level network adaptation USED-FOR DMIL. highlevel network USED-FOR DMIL. likelihood of state - action pairs USED-FOR DMIL. DMIL CONJUNCTION Expectation - Maximization algorithm. Expectation - Maximization algorithm CONJUNCTION DMIL. iterative training process USED-FOR DMIL. Kitchen environment EVALUATE-FOR few - shot imitation learning. Method are Hierarchical Imitation learning ( HIL ), and model - agnostic meta - learning. OtherScientificTerm is high - level network. ","This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta-imitation learning method for few-shot imitation learning. The core idea of DMIL is to learn a hierarchical hierarchy of sub-skills from long-horizon unsegmented demonstrations, which is called Hierarchical Imitation learning (HIL) in the paper. Transferring and reorganizing modular sub -skills with a hierarchical structure is performed by a high-level network. A highlevel network is used for sub-skill adaptation, and a sub-task is learned by maximizing the likelihood of state-action pairs for supervision. DMIL uses a novel iterative training process, where the high level network is trained to adapt to a new task, and the low-level one adapts to the new task. The paper also proposes an Expectation-Maximization algorithm to improve the performance of the DMIL. Empirical results on the Kitchen environment show that DMIL outperforms the state-of-the-art model-agnostic meta-learning. ","This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta-imitation learning method for few-shot imitation learning. The core idea of DMIL is to learn a hierarchical hierarchy of sub-skills from long-horizon unsegmented demonstrations, which is called Hierarchical Imitation learning (HIL) in the paper. Transferring and reorganizing modular sub -skills with a hierarchical structure is performed by a high-level network. A highlevel network is used for sub-skill adaptation, and a sub-task is learned by maximizing the likelihood of state-action pairs for supervision. DMIL uses a novel iterative training process, where the high level network is trained to adapt to a new task, and the low-level one adapts to the new task. The paper also proposes an Expectation-Maximization algorithm to improve the performance of the DMIL. Empirical results on the Kitchen environment show that DMIL outperforms the state-of-the-art model-agnostic meta-learning. "
3589,SP:fb0efa670729796471a7a562b231172103bb8749,Graph neural networks ( GNNs ) HYPONYM-OF deep learning models. deep learning models USED-FOR graph data. node features USED-FOR they. graph without node feature USED-FOR networks. number of degrees HYPONYM-OF graph - based node features. embeddings HYPONYM-OF input node representation. approach USED-FOR node embeddings. node embeddings CONJUNCTION GNNs. GNNs CONJUNCTION node embeddings. graphics processing unit ( GPU ) memory FEATURE-OF GNNs. embedding compression methods USED-FOR natural language processing ( NLP ) models. bit vector COMPARE float - point vector. float - point vector COMPARE bit vector. bit vector USED-FOR node. parameters USED-FOR compression method. GNNs USED-FOR parameters. node embedding compression method COMPARE alternatives. alternatives COMPARE node embedding compression method. Generic is network. Material is industrial scale graph data. ,"Graph neural networks (GNNs) are one of the most popular deep learning models for graph data. However, they rely on node features (e.g. number of degrees) to encode the input node representation, which can be expensive to compress. This paper proposes a new approach to compress node embeddings and GNNs in the presence of graphics processing unit (GPU) memory. Specifically, the networks are trained on a graph without node feature, and then the input graph-based node features are used as input to the network. To compress a node, a bit vector is used instead of a float-point vector. The authors show that the proposed node embedding compression method outperforms existing alternatives on industrial scale graph data, and can be applied to other natural language processing (NLP) models. The compression method can also be used to compress additional parameters of the GNN. ","Graph neural networks (GNNs) are one of the most popular deep learning models for graph data. However, they rely on node features (e.g. number of degrees) to encode the input node representation, which can be expensive to compress. This paper proposes a new approach to compress node embeddings and GNNs in the presence of graphics processing unit (GPU) memory. Specifically, the networks are trained on a graph without node feature, and then the input graph-based node features are used as input to the network. To compress a node, a bit vector is used instead of a float-point vector. The authors show that the proposed node embedding compression method outperforms existing alternatives on industrial scale graph data, and can be applied to other natural language processing (NLP) models. The compression method can also be used to compress additional parameters of the GNN. "
3605,SP:15c243829ed3b2505ed1e122bd499089f8a862da,domain adaptation USED-FOR learning invariant representations. domain - adversarial training USED-FOR learning invariant representations. asymptotic convergence guarantees FEATURE-OF optimizer. gradient descent USED-FOR domain - adversarial training. optimal solutions PART-OF domain - adversarial training. local Nash equilibria USED-FOR optimal solutions. gradient descent CONJUNCTION high - order ODE solvers. high - order ODE solvers CONJUNCTION gradient descent. Runge – Kutta HYPONYM-OF high - order ODE solvers. optimizers COMPARE optimizers. optimizers COMPARE optimizers. drop - in replacement COMPARE optimizers. optimizers COMPARE drop - in replacement. optimizers USED-FOR drop - in replacement. learning rates FEATURE-OF optimizers. optimizers PART-OF domain - adversarial framework. Generic is approach. Method is domain - adversarial methods. ,"This paper studies the problem of domain adaptation for learning invariant representations in the context of domain-adversarial training. In particular, the authors consider the setting where the optimizer has asymptotic convergence guarantees for the optimal solution of the optimization problem. They show that the optimal solutions of the optimizers used in the domain adaptation are local Nash equilibria, and show that gradient descent and high-order ODE solvers (e.g., Runge–Kutta) can be used to find optimal solutions for the domain adversarial training using gradient descent. They also show that drop-in replacement can be performed with these optimizers at higher learning rates than existing optimizers.  The authors also provide a theoretical analysis of the proposed approach.   The paper is well-written and well-motivated, and the authors have made a good effort to provide a detailed analysis of their results. The authors have also conducted extensive experiments to show the effectiveness of their proposed optimizers in the proposed domain-additively adversarial framework. The results show that their optimizers outperform existing state-of-the-art optimizers, and are competitive with existing state of the art optimizers on a variety of datasets. The paper also shows that the proposed optimizer is more robust to the choice of learning rates, which is a nice contribution of the authors.  Overall, the paper is clearly written and easy to follow. However, there is a lack of discussion of the theoretical analysis, and there is no comparison with other domain-adaptive domain-agnostic methods.","This paper studies the problem of domain adaptation for learning invariant representations in the context of domain-adversarial training. In particular, the authors consider the setting where the optimizer has asymptotic convergence guarantees for the optimal solution of the optimization problem. They show that the optimal solutions of the optimizers used in the domain adaptation are local Nash equilibria, and show that gradient descent and high-order ODE solvers (e.g., Runge–Kutta) can be used to find optimal solutions for the domain adversarial training using gradient descent. They also show that drop-in replacement can be performed with these optimizers at higher learning rates than existing optimizers.  The authors also provide a theoretical analysis of the proposed approach.   The paper is well-written and well-motivated, and the authors have made a good effort to provide a detailed analysis of their results. The authors have also conducted extensive experiments to show the effectiveness of their proposed optimizers in the proposed domain-additively adversarial framework. The results show that their optimizers outperform existing state-of-the-art optimizers, and are competitive with existing state of the art optimizers on a variety of datasets. The paper also shows that the proposed optimizer is more robust to the choice of learning rates, which is a nice contribution of the authors.  Overall, the paper is clearly written and easy to follow. However, there is a lack of discussion of the theoretical analysis, and there is no comparison with other domain-adaptive domain-agnostic methods."
3621,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"regularization methods USED-FOR machine learning models. loss function USED-FOR Flooding. individual Flood HYPONYM-OF regularizer. iFlood USED-FOR models. instance - level constraints FEATURE-OF training loss. instance - level constraints USED-FOR iFlood. it USED-FOR applications. it USED-FOR models. iFlood COMPARE regularizers. regularizers COMPARE iFlood. image classification and language understanding tasks EVALUATE-FOR models. iFlood USED-FOR models. Generic is one. OtherScientificTerm are under - fitted instances, inductive biases, and instance - level. Metric is generalization ability. ","This paper proposes a new regularization method, called iFlood, to improve the generalization performance of machine learning models. The authors argue that existing regularization methods are not robust enough to deal with under-fitted instances due to inductive biases. To this end, the authors propose an additional regularizer called individual Flood, which is a regularizer that encourages the loss function to be invariant to Flooding. The paper shows that iFlOOD can be used to train models with models with different instance-level constraints on the training loss, and that it can be applied to a variety of applications. Experiments on image classification and language understanding tasks show that models trained with iFl Flood outperform existing regularizers in terms of generalization ability.   ","This paper proposes a new regularization method, called iFlood, to improve the generalization performance of machine learning models. The authors argue that existing regularization methods are not robust enough to deal with under-fitted instances due to inductive biases. To this end, the authors propose an additional regularizer called individual Flood, which is a regularizer that encourages the loss function to be invariant to Flooding. The paper shows that iFlOOD can be used to train models with models with different instance-level constraints on the training loss, and that it can be applied to a variety of applications. Experiments on image classification and language understanding tasks show that models trained with iFl Flood outperform existing regularizers in terms of generalization ability.   "
3637,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"Reinforcement learning USED-FOR complex tasks. Reinforcement learning USED-FOR policies. policies USED-FOR complex tasks. methods USED-FOR long - horizon tasks. Hierarchical reinforcement learning USED-FOR lowlevel skills. Hierarchical reinforcement learning USED-FOR action abstractions. action abstractions USED-FOR lowlevel skills. lower - level policies USED-FOR state abstraction. approach USED-FOR representation. value functions USED-FOR lower - level skill. value functions USED-FOR approach. value functions USED-FOR representation. value functions USED-FOR representation. approach COMPARE model - free and model - based methods. model - free and model - based methods COMPARE approach. maze - solving and robotic manipulation tasks EVALUATE-FOR approach. zero - shot generalization EVALUATE-FOR model - free and model - based methods. zero - shot generalization EVALUATE-FOR approach. OtherScientificTerm are horizon, lower - level skills, Hierarchies, and space states. Method is Value Function Spaces. ","This paper proposes a new method for learning long-horizon policies that generalize well to unseen tasks. The proposed method is based on Hierarchical Reinforcement Learning (HRL), which learns a hierarchy of low-level skills that can be used to learn long horizon policies. The key idea of HRL is to learn a set of value functions for each skill in the hierarchy, and then use these value functions to guide the learning of a new skill. The paper shows that the proposed method outperforms existing methods on a number of tasks. ","This paper proposes a new method for learning long-horizon policies that generalize well to unseen tasks. The proposed method is based on Hierarchical Reinforcement Learning (HRL), which learns a hierarchy of low-level skills that can be used to learn long horizon policies. The key idea of HRL is to learn a set of value functions for each skill in the hierarchy, and then use these value functions to guide the learning of a new skill. The paper shows that the proposed method outperforms existing methods on a number of tasks. "
3653,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"one - shot probabilistic decoders USED-FOR vector - shaped prior. generative adversarial networks ( GAN ) CONJUNCTION normalizing flows. normalizing flows CONJUNCTION generative adversarial networks ( GAN ). variational autoencoders ( VAE ) CONJUNCTION generative adversarial networks ( GAN ). generative adversarial networks ( GAN ) CONJUNCTION variational autoencoders ( VAE ). functions PART-OF variational autoencoders ( VAE ). functions USED-FOR drug discovery. Transformer layers CONJUNCTION graph neural networks. graph neural networks CONJUNCTION Transformer layers. them CONJUNCTION prior vector. prior vector CONJUNCTION them. Transformer layers USED-FOR prior vector. Transformer layers USED-FOR them. graph neural networks USED-FOR them. architecture USED-FOR exchangeable distributions. VAEs CONJUNCTION GANs. GANs CONJUNCTION VAEs. exchangeability USED-FOR VAEs. exchangeability USED-FOR GANs. Top - n HYPONYM-OF deterministic, non - exchangeable set creation mechanism. VAE CONJUNCTION GAN. GAN CONJUNCTION VAE. Top - n USED-FOR VAE. i.i.d. generation USED-FOR VAE. i.i.d. generation USED-FOR GAN. Top - n COMPARE i.i.d. generation. i.i.d. generation COMPARE Top - n. Top - n USED-FOR complex dependencies. Top - n COMPARE i.i.d. generation. i.i.d. generation COMPARE Top - n. SetMNIST reconstruction EVALUATE-FOR Top - n. SetMNIST reconstruction EVALUATE-FOR i.i.d. generation. algorithm USED-FOR molecule generation methods. algorithm USED-FOR one - shot generation. Task is Set and graph generation. OtherScientificTerm are normal distribution, and equivariance. Material are synthetic molecule - like dataset, and QM9 dataset. ","This paper proposes a new generative autoencoder (GAN) architecture for one-shot generative modeling of molecules. The proposed architecture is based on a VAE-based generative adversarial network (GAN) architecture. The authors show that the generative model is equivariant to exchangeability, which is an important property of VAEs and GANs. The paper also shows that the GAN-based VAE can be used to generate molecules from a set of molecules in a single step. ","This paper proposes a new generative autoencoder (GAN) architecture for one-shot generative modeling of molecules. The proposed architecture is based on a VAE-based generative adversarial network (GAN) architecture. The authors show that the generative model is equivariant to exchangeability, which is an important property of VAEs and GANs. The paper also shows that the GAN-based VAE can be used to generate molecules from a set of molecules in a single step. "
3669,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,Deep Ritz Method ( DRM ) CONJUNCTION Physics - Informed Neural Networks ( PINNs ). Physics - Informed Neural Networks ( PINNs ) CONJUNCTION Deep Ritz Method ( DRM ). deep learning techniques USED-FOR elliptic partial differential equations ( PDEs ). Deep Ritz Method ( DRM ) USED-FOR deep learning techniques. random samples USED-FOR deep learning techniques. Physics - Informed Neural Networks ( PINNs ) USED-FOR deep learning techniques. hypercube FEATURE-OF Schrödinger equation. Schrödinger equation HYPONYM-OF prototype elliptic PDE. upper bounds USED-FOR problem. upper and lower bounds USED-FOR methods. upper bounds USED-FOR upper and lower bounds. rate generalization bound USED-FOR upper bounds. rate generalization bound USED-FOR problem. PINN CONJUNCTION DRM. DRM CONJUNCTION PINN. DRM USED-FOR minimax optimal bounds. PINN USED-FOR minimax optimal bounds. Sobolev spaces FEATURE-OF minimax optimal bounds. dimension dependent power law USED-FOR deep PDE solvers. power law USED-FOR deep model accuracy. OtherScientificTerm is zero Dirichlet boundary condition. Task is quantummechanical systems. Method is Deep Ritz Method. Generic is it. ,"This paper studies deep learning techniques for solving elliptic partial differential equations (PDEs) based on Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). The authors consider the problem of solving quantummechanical systems, where the Schrödinger equation on the hypercube is a prototype elliptic PDE, and the zero Dirichlet boundary condition is not satisfied.    The authors propose two methods to solve the problem, based on upper and lower bounds on the generalization error of these methods. The upper bounds for the problem are based on the rate generalization bound of the rate of convergence of the algorithm to the solution of the problem. The lower bound is based on a bound on the dimension dependent power law of deep PDE solvers. The authors show that both PINN and DRM provide minimax optimal bounds in Sobolev spaces, and that the power law for deep model accuracy depends on the number of samples and the dimension of the data.","This paper studies deep learning techniques for solving elliptic partial differential equations (PDEs) based on Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). The authors consider the problem of solving quantummechanical systems, where the Schrödinger equation on the hypercube is a prototype elliptic PDE, and the zero Dirichlet boundary condition is not satisfied.    The authors propose two methods to solve the problem, based on upper and lower bounds on the generalization error of these methods. The upper bounds for the problem are based on the rate generalization bound of the rate of convergence of the algorithm to the solution of the problem. The lower bound is based on a bound on the dimension dependent power law of deep PDE solvers. The authors show that both PINN and DRM provide minimax optimal bounds in Sobolev spaces, and that the power law for deep model accuracy depends on the number of samples and the dimension of the data."
3685,SP:80614db60d27a48c3c1b1882844e298666b798d4,Machine learning ( ML ) robustness CONJUNCTION generalization. generalization CONJUNCTION Machine learning ( ML ) robustness. data distribution shift FEATURE-OF they. adversarial and natural settings FEATURE-OF data distribution shift. other USED-FOR one. norm of the last layer CONJUNCTION Jacobian norm. Jacobian norm CONJUNCTION norm of the last layer. Jacobian norm CONJUNCTION data augmentations ( DA ). data augmentations ( DA ) CONJUNCTION Jacobian norm. function class regularization process USED-FOR domain generalization. adversarial training USED-FOR robustness. function class regularization USED-FOR robustness. DA USED-FOR generalization. DA USED-FOR regularization. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. Generic is theoretical framework. OtherScientificTerm is sufficient conditions. ,"This paper studies the relationship between Machine learning (ML) robustness and generalization under data distribution shift in both adversarial and natural settings. The authors propose a theoretical framework and provide two main results: (1) they show that adversarial training improves robustness when the norm of the last layer, Jacobian norm, and data augmentations (DA) are combined, and (2) the function class regularization process improves domain generalization when they are not. They also provide sufficient conditions under which DA can be used as regularization.","This paper studies the relationship between Machine learning (ML) robustness and generalization under data distribution shift in both adversarial and natural settings. The authors propose a theoretical framework and provide two main results: (1) they show that adversarial training improves robustness when the norm of the last layer, Jacobian norm, and data augmentations (DA) are combined, and (2) the function class regularization process improves domain generalization when they are not. They also provide sufficient conditions under which DA can be used as regularization."
3701,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"Meta - learning USED-FOR quick learning of few - shot tasks. meta - training tasks USED-FOR meta - knowledge. Wellgeneralized meta - knowledge USED-FOR fast adaptation. methods PART-OF framework. deconfounder approaches USED-FOR methods. Dropout USED-FOR meta - knowledge. deconfounder algorithms USED-FOR memorization. causal perspective USED-FOR memorization. causal perspective USED-FOR deconfounder algorithms. benchmark datasets USED-FOR memorization. benchmark datasets EVALUATE-FOR deconfounder algorithms. Task is task - specific adaptation. Method are regularizer - based and augmentation - based methods, meta - learning, and front - door adjustment. OtherScientificTerm are causality, and universal label space. ","Meta-learning is a popular technique for the quick learning of few-shot tasks. However, the task-specific adaptation can be problematic due to the lack of causality between tasks. To address this issue, the authors propose a meta-learning approach that learns a set of meta-training tasks that can be used for fast adaptation to new tasks. Wellgeneralized meta-knowledge is used to guide the fast adaptation. The authors propose two methods in this framework that are based on deconfounder approaches: 1) regularizer-based and augmentation-based methods. 2) Dropout is used for meta-knowing in order to improve the generalization ability of the meta-learners.  The authors show that the deconfounder algorithms are able to avoid memorization using a causal perspective, which is based on the observation that meta-learner learns a universal label space. They also show that their proposed methods are more robust to front-door adjustment, and that their algorithms outperform the state-of-the-art on standard benchmark datasets.","Meta-learning is a popular technique for the quick learning of few-shot tasks. However, the task-specific adaptation can be problematic due to the lack of causality between tasks. To address this issue, the authors propose a meta-learning approach that learns a set of meta-training tasks that can be used for fast adaptation to new tasks. Wellgeneralized meta-knowledge is used to guide the fast adaptation. The authors propose two methods in this framework that are based on deconfounder approaches: 1) regularizer-based and augmentation-based methods. 2) Dropout is used for meta-knowing in order to improve the generalization ability of the meta-learners.  The authors show that the deconfounder algorithms are able to avoid memorization using a causal perspective, which is based on the observation that meta-learner learns a universal label space. They also show that their proposed methods are more robust to front-door adjustment, and that their algorithms outperform the state-of-the-art on standard benchmark datasets."
3717,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"ad hoc teamwork HYPONYM-OF problem. full observability CONJUNCTION fixed and predefined teammates ’ types. fixed and predefined teammates ’ types CONJUNCTION full observability. fixed and predefined teammates ’ types HYPONYM-OF assumptions. full observability HYPONYM-OF assumptions. reinforcement learning framework USED-FOR autonomous agent. ODITS HYPONYM-OF reinforcement learning framework. information - based regularizer USED-FOR proxy representations of the learned variables. local observations USED-FOR information - based regularizer. local observations USED-FOR proxy representations of the learned variables. ODITS COMPARE baselines. baselines COMPARE ODITS. ad hoc teamwork tasks EVALUATE-FOR ODITS. ad hoc teamwork tasks EVALUATE-FOR baselines. Task is Autonomous agents. OtherScientificTerm are teammates, and partial observability. ","This paper studies the problem of ad hoc teamwork, i.e., the problem where a team of autonomous agents are asked to work together to solve a task together with a limited number of teammates. Autonomous agents are trained with two assumptions: full observability and fixed and predefined teammates’ types. The authors propose a reinforcement learning framework called ODITS to train an autonomous agent with these two assumptions. ODITS uses an information-based regularizer based on local observations to learn proxy representations of the learned variables that are robust to partial observability. Experiments show that ODITS outperforms several baselines on a number of standard (and some non-standard) ad hoc teamsitization tasks.","This paper studies the problem of ad hoc teamwork, i.e., the problem where a team of autonomous agents are asked to work together to solve a task together with a limited number of teammates. Autonomous agents are trained with two assumptions: full observability and fixed and predefined teammates’ types. The authors propose a reinforcement learning framework called ODITS to train an autonomous agent with these two assumptions. ODITS uses an information-based regularizer based on local observations to learn proxy representations of the learned variables that are robust to partial observability. Experiments show that ODITS outperforms several baselines on a number of standard (and some non-standard) ad hoc teamsitization tasks."
3733,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"missing values PART-OF high - dimensional data. approach USED-FOR down - stream analysis. imputation CONJUNCTION model estimation. model estimation CONJUNCTION imputation. model estimation USED-FOR down - stream analysis. imputation PART-OF approach. model estimation PART-OF approach. algorithm USED-FOR imputation. normalizing flow ( NF ) model USED-FOR data space. latent space FEATURE-OF imputation. normalizing flow ( NF ) model USED-FOR algorithm. Expectation - Maximization ( EM ) algorithm USED-FOR imputation. EMFlow COMPARE methods. methods COMPARE EMFlow. predictive accuracy CONJUNCTION speed of algorithmic convergence. speed of algorithmic convergence CONJUNCTION predictive accuracy. high - dimensional multivariate and image datasets EVALUATE-FOR EMFlow. speed of algorithmic convergence EVALUATE-FOR EMFlow. predictive accuracy EVALUATE-FOR EMFlow. speed of algorithmic convergence EVALUATE-FOR methods. predictive accuracy EVALUATE-FOR methods. Method are data mining and machine learning methods, EMFlow algorithm, and NF alternatively. ",This paper proposes a new approach to perform down-stream analysis of missing values in high-dimensional data. The approach combines imputation and model estimation for both data mining and machine learning methods. The authors propose an algorithm for imputation based on an Expectation-Maximization (EM) algorithm. The algorithm is based on a normalizing flow (NF) model to model the data space and imputation in the latent space. The paper shows that EMFlow outperforms existing methods in terms of predictive accuracy and speed of algorithmic convergence on high-dimensionality multivariate and image datasets. The EMFlow algorithm is also shown to outperform NF alternatively. ,This paper proposes a new approach to perform down-stream analysis of missing values in high-dimensional data. The approach combines imputation and model estimation for both data mining and machine learning methods. The authors propose an algorithm for imputation based on an Expectation-Maximization (EM) algorithm. The algorithm is based on a normalizing flow (NF) model to model the data space and imputation in the latent space. The paper shows that EMFlow outperforms existing methods in terms of predictive accuracy and speed of algorithmic convergence on high-dimensionality multivariate and image datasets. The EMFlow algorithm is also shown to outperform NF alternatively. 
3749,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,rectified linear units ( ReLUs ) USED-FOR DNNs. gates USED-FOR neural path kernel ( NPK ). rotational invariance CONJUNCTION ensemble structure. ensemble structure CONJUNCTION rotational invariance. global pooling CONJUNCTION skip connection. skip connection CONJUNCTION global pooling. convolution USED-FOR NPK. ensemble structure USED-FOR NPK. convolution USED-FOR ensemble structure. convolution USED-FOR rotational invariance. skip connection FEATURE-OF convolution. global pooling FEATURE-OF convolution. gates USED-FOR weights. external masks USED-FOR weights. weights PART-OF network. gates USED-FOR external masks. ReLUs FEATURE-OF DNNs. deep linear network USED-FOR pre - activations. DNNs USED-FOR ‘ black box’-ness. disentanglement CONJUNCTION interpretable re - arrangement of the computations. interpretable re - arrangement of the computations CONJUNCTION disentanglement. interpretable re - arrangement of the computations PART-OF DNN. ReLUs USED-FOR interpretable re - arrangement of the computations. ReLUs USED-FOR DNN. path space FEATURE-OF weights network. DLGN USED-FOR computations. primal ’ linearity CONJUNCTION dual ’ linearity. dual ’ linearity CONJUNCTION primal ’ linearity. path space FEATURE-OF dual ’ linearity. ‘ mathematically ’ interpretable linearities PART-OF DLGN. dual ’ linearity HYPONYM-OF ‘ mathematically ’ interpretable linearities. primal ’ linearity HYPONYM-OF ‘ mathematically ’ interpretable linearities. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. DGN CONJUNCTION DLGN. DLGN CONJUNCTION DGN. DNN CONJUNCTION DGN. DGN CONJUNCTION DNN. DLGN COMPARE DNNs. DNNs COMPARE DLGN. ‘ disentangled and interpretable ’ computations PART-OF DLGN. entang,"This paper studies the use of rectified linear units (ReLUs) in DNNs. Specifically, the authors show that ReLUs can be used as gates for the neural path kernel (NPK) of a DNN, and that convolution with global pooling and skip connection is sufficient to preserve the rotational invariance and ensemble structure of the NPK. The authors also show that the weights of the network can be re-arranged by using external masks. The main contribution of the paper is to show that a deep linear network is able to learn pre-activations that preserve the disentanglement and interpretable re-approximation of the computations in the DNN. The paper also shows that DNN’s are able to achieve ‘black box’-ness by re-ordering the weights in the path space of the weights network.  The authors further show that DLGN contains ‘mathematically’ interpretable linearities (primal’ linearity in path space and dual’linearity) and ‘disentangled’ computations that can be applied to any DNN (DNN, DGN, DLGN). The authors show experimentally on CIFAR-10, CifAR-100, and ImageNet that the DLGN achieves better disentangling and interpretability compared to other recent works. ","This paper studies the use of rectified linear units (ReLUs) in DNNs. Specifically, the authors show that ReLUs can be used as gates for the neural path kernel (NPK) of a DNN, and that convolution with global pooling and skip connection is sufficient to preserve the rotational invariance and ensemble structure of the NPK. The authors also show that the weights of the network can be re-arranged by using external masks. The main contribution of the paper is to show that a deep linear network is able to learn pre-activations that preserve the disentanglement and interpretable re-approximation of the computations in the DNN. The paper also shows that DNN’s are able to achieve ‘black box’-ness by re-ordering the weights in the path space of the weights network.  The authors further show that DLGN contains ‘mathematically’ interpretable linearities (primal’ linearity in path space and dual’linearity) and ‘disentangled’ computations that can be applied to any DNN (DNN, DGN, DLGN). The authors show experimentally on CIFAR-10, CifAR-100, and ImageNet that the DLGN achieves better disentangling and interpretability compared to other recent works. "
3765,SP:5676944f4983676b5ad843fdb190bf029ad647bb,Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. Vision Transformer ( ViT ) USED-FOR computer vision tasks. PVT HYPONYM-OF Vision Transformer ( ViT ). Swin HYPONYM-OF Vision Transformer ( ViT ). Layer Normalization ( LN ) USED-FOR models. Transformers USED-FOR inductive bias. LN USED-FOR positional context. positional context HYPONYM-OF inductive bias. Dynamic Token Normalization ( DTN ) HYPONYM-OF normalizer. it USED-FOR normalization methods. unified formulation USED-FOR it. global contextual information CONJUNCTION local positional context. local positional context CONJUNCTION global contextual information. Transformers USED-FOR local positional context. Transformers USED-FOR global contextual information. DTN USED-FOR Transformers. DTN USED-FOR intra - token and inter - token manners. PVT CONJUNCTION LeViT. LeViT CONJUNCTION PVT. BigBird CONJUNCTION Reformer. Reformer CONJUNCTION BigBird. Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. LeViT CONJUNCTION T2T - ViT. T2T - ViT CONJUNCTION LeViT. T2T - ViT CONJUNCTION BigBird. BigBird CONJUNCTION T2T - ViT. ViT CONJUNCTION Swin. Swin CONJUNCTION ViT. DTN USED-FOR vision transformers. ViT HYPONYM-OF vision transformers. LeViT HYPONYM-OF vision transformers. Swin HYPONYM-OF vision transformers. T2T - ViT HYPONYM-OF vision transformers. PVT HYPONYM-OF vision transformers. Reformer HYPONYM-OF vision transformers. BigBird HYPONYM-OF vision transformers. transformer COMPARE baseline model. baseline model COMPARE transformer. DTN USED-FOR transformer. computational overhead EVALUATE-FOR baseline model. DTN COMPARE LN. LN COMPARE DTN. accuracy EVALUATE-FOR Long ListOps. Long - Range Arena FEATURE-OF,"This paper proposes a new normalization method for vision transformers, called Dynamic Token Normalization (DTN). The idea is to normalize the tokens of a Transformer (ViT, Swin, PVT, LeViT) based on the inductive bias (i.e., positional context) induced by Layer Normalization. The authors show that the proposed method is more robust than LN in the sense that it can be applied to any normalization methods. They also show that DTN can be used in both intra-token and inter-token manners, and that it is compatible with any unified formulation of normalization. Experiments are conducted on a variety of Vision Transformer models, and DTN is shown to outperform LN for most of the models. ","This paper proposes a new normalization method for vision transformers, called Dynamic Token Normalization (DTN). The idea is to normalize the tokens of a Transformer (ViT, Swin, PVT, LeViT) based on the inductive bias (i.e., positional context) induced by Layer Normalization. The authors show that the proposed method is more robust than LN in the sense that it can be applied to any normalization methods. They also show that DTN can be used in both intra-token and inter-token manners, and that it is compatible with any unified formulation of normalization. Experiments are conducted on a variety of Vision Transformer models, and DTN is shown to outperform LN for most of the models. "
3781,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,deep learning models USED-FOR expressive functions. SGD USED-FOR deep learning models. theoretical models USED-FOR spectral bias. methodologies USED-FOR spectral bias. spectral bias FEATURE-OF image classification networks. interventions USED-FOR generalization. spectral bias FEATURE-OF networks. regularization USED-FOR learning of high frequencies. models COMPARE ones. ones COMPARE models. models USED-FOR high frequencies. function frequency CONJUNCTION image frequency. image frequency CONJUNCTION function frequency. low frequencies PART-OF natural images. low frequencies USED-FOR spectral bias. natural images USED-FOR spectral bias. neural networks USED-FOR image classification. OtherScientificTerm is Spectral bias. Method is deep models. ,"This paper studies the spectral bias of deep learning models trained with SGD for expressive functions. Spectral bias is a phenomenon that occurs when the function frequency and the image frequency of deep models are close to each other. The authors propose two theoretical models to explain the phenomenon and propose two new methodologies to mitigate the effects of spectral bias in the training of image classification networks. They also propose interventions to improve the generalization performance of the networks with spectral bias. They show that regularization helps the learning of high frequencies and that models with high frequencies are more sensitive to high frequencies than ones with low frequencies. Finally, they show that low frequencies in natural images are responsible for the observed spectral bias, and that neural networks trained for image classification with neural networks with high frequency are sensitive to low frequencies as well.","This paper studies the spectral bias of deep learning models trained with SGD for expressive functions. Spectral bias is a phenomenon that occurs when the function frequency and the image frequency of deep models are close to each other. The authors propose two theoretical models to explain the phenomenon and propose two new methodologies to mitigate the effects of spectral bias in the training of image classification networks. They also propose interventions to improve the generalization performance of the networks with spectral bias. They show that regularization helps the learning of high frequencies and that models with high frequencies are more sensitive to high frequencies than ones with low frequencies. Finally, they show that low frequencies in natural images are responsible for the observed spectral bias, and that neural networks trained for image classification with neural networks with high frequency are sensitive to low frequencies as well."
3797,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"Exploration PART-OF reinforcement learning ( RL ). monolithic behaviour policy USED-FOR methods. nonmonolithic exploration USED-FOR RL. algorithmic components USED-FOR switching mechanism. two - mode exploration CONJUNCTION switching. switching CONJUNCTION two - mode exploration. sub - episodic time - scales FEATURE-OF switching. sub - episodic time - scales FEATURE-OF two - mode exploration. switching USED-FOR Atari. two - mode exploration USED-FOR Atari. OtherScientificTerm are exploratory behaviours, switching triggers, and hyper - parameter - tuning burden. ","Exploration in reinforcement learning (RL) is an important component of many existing methods. However, existing methods rely on a monolithic behaviour policy, which can be problematic when there is nonmonolithic exploration in RL. To address this issue, this paper proposes two algorithmic components: (1) a switching mechanism that switches between two different exploratory behaviours, and (2) a set of switching triggers. The switching mechanism is based on a combination of the two existing algorithms. Experiments on Atari show that two-mode exploration and switching on sub-episodic time-scales are effective at Atari. The authors also show that switching triggers can be used to reduce the hyper-parameter-tuning burden. ","Exploration in reinforcement learning (RL) is an important component of many existing methods. However, existing methods rely on a monolithic behaviour policy, which can be problematic when there is nonmonolithic exploration in RL. To address this issue, this paper proposes two algorithmic components: (1) a switching mechanism that switches between two different exploratory behaviours, and (2) a set of switching triggers. The switching mechanism is based on a combination of the two existing algorithms. Experiments on Atari show that two-mode exploration and switching on sub-episodic time-scales are effective at Atari. The authors also show that switching triggers can be used to reduce the hyper-parameter-tuning burden. "
3813,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,initialization scheme USED-FOR k - median problem. metric space FEATURE-OF k - median problem. metric embedding tree structure USED-FOR initialization scheme. discrete space HYPONYM-OF metric space. search algorithm USED-FOR initial centers. search algorithm USED-FOR local search algorithm. differential privacy ( DP ) USED-FOR private initial centers. HST initialization HYPONYM-OF method. k - median++ USED-FOR non - DP setting. initialization method USED-FOR non - DP setting. initialization method USED-FOR initial centers. HST initialization USED-FOR initial centers. k - median++ HYPONYM-OF initialization method. DP local search CONJUNCTION private HST initialization. private HST initialization CONJUNCTION DP local search. Method is clustering algorithms. Task is construction of metric embedding tree structure. OtherScientificTerm is privacy constraint. Generic is methods. ,"This paper proposes a new initialization scheme for the k-median problem in the metric space (i.e., discrete space) based on the metric embedding tree structure. The authors propose a local search algorithm based on a search algorithm to find initial centers that satisfy differential privacy (DP) for clustering algorithms. They also propose a method called HST initialization, which is an initialization method for the non-DP setting based on k-Median++. The main contribution of this paper is the construction of metric embeddings tree structure that satisfy the privacy constraint. They show that the proposed DP local search and private HST initialized initial centers outperform the existing methods. ","This paper proposes a new initialization scheme for the k-median problem in the metric space (i.e., discrete space) based on the metric embedding tree structure. The authors propose a local search algorithm based on a search algorithm to find initial centers that satisfy differential privacy (DP) for clustering algorithms. They also propose a method called HST initialization, which is an initialization method for the non-DP setting based on k-Median++. The main contribution of this paper is the construction of metric embeddings tree structure that satisfy the privacy constraint. They show that the proposed DP local search and private HST initialized initial centers outperform the existing methods. "
3829,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"planning USED-FOR agent. representation USED-FOR visual perception tasks. agent USED-FOR complex dynamics of the real - world. agent USED-FOR representation. complicated dynamics CONJUNCTION broader domain. broader domain CONJUNCTION complicated dynamics. broader domain FEATURE-OF real - life datasets. complicated dynamics FEATURE-OF real - life datasets. narrow benchmarks EVALUATE-FOR video prediction models. underfitting USED-FOR low quality predictions. FitVid HYPONYM-OF architecture. image augmentation techniques USED-FOR it. FitVid COMPARE models. models COMPARE FitVid. video prediction benchmarks EVALUATE-FOR models. video prediction benchmarks EVALUATE-FOR FitVid. metrics EVALUATE-FOR models. metrics EVALUATE-FOR FitVid. Generic are task, they, and state - of - theart models. Method is video models. OtherScientificTerm is overfitting. ","This paper proposes a new architecture, called FitVid, to improve the performance of video prediction models on narrow benchmarks. The key idea is to use planning to guide the agent to learn a representation for visual perception tasks. The agent is trained to capture the complex dynamics of the real-world, and the goal is to learn such a representation without overfitting to the task at hand. The paper shows that underfitting to low quality predictions is a major cause of video models underfitting, and that they can be trained to be more robust to overfitting. The authors also show that real-life datasets with complicated dynamics and a broader domain are more likely to be over-fitted to a single model. The proposed architecture is a simple architecture, and it uses image augmentation techniques to make it more robust.   The authors show that Fitvid outperforms existing models on standard video prediction benchmarks on a variety of metrics, and outperforms state-of-theart models on a number of datasets.","This paper proposes a new architecture, called FitVid, to improve the performance of video prediction models on narrow benchmarks. The key idea is to use planning to guide the agent to learn a representation for visual perception tasks. The agent is trained to capture the complex dynamics of the real-world, and the goal is to learn such a representation without overfitting to the task at hand. The paper shows that underfitting to low quality predictions is a major cause of video models underfitting, and that they can be trained to be more robust to overfitting. The authors also show that real-life datasets with complicated dynamics and a broader domain are more likely to be over-fitted to a single model. The proposed architecture is a simple architecture, and it uses image augmentation techniques to make it more robust.   The authors show that Fitvid outperforms existing models on standard video prediction benchmarks on a variety of metrics, and outperforms state-of-theart models on a number of datasets."
3845,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,generalization EVALUATE-FOR machine learning algorithm. neural network HYPONYM-OF machine learning algorithm. model USED-FOR test loss. model USED-FOR stochastic gradient descent ( SGD ). data structure USED-FOR test loss dynamics. arbitrary covariance structure FEATURE-OF features. Gaussian features CONJUNCTION arbitrary features. arbitrary features CONJUNCTION Gaussian features. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. Gaussian model USED-FOR test loss. Gaussian model USED-FOR nonlinear random - feature models. nonlinear random - feature models CONJUNCTION deep neural networks. deep neural networks CONJUNCTION nonlinear random - feature models. theory USED-FOR Gaussian features. Gaussian model USED-FOR deep neural networks. real datasets EVALUATE-FOR deep neural networks. SGD USED-FOR deep neural networks. MNIST HYPONYM-OF real datasets. CIFAR-10 HYPONYM-OF real datasets. fixed compute budget FEATURE-OF optimal batch size. feature correlation structure USED-FOR optimal batch size. small batch sizes USED-FOR SGD. theory USED-FOR stochastic gradient descent. framework USED-FOR training and test error. real data EVALUATE-FOR framework. fixed subsampled training set USED-FOR stochastic gradient descent. OtherScientificTerm is structure of the data distribution. ,"This paper studies the generalization performance of a machine learning algorithm (e.g., neural network) when the data structure of the test loss dynamics depends on the training data. The authors show that stochastic gradient descent (SGD) converges to a test loss that depends on a specific data structure. They show that SGD converges when the structure of data distribution is Gaussian, but not when the features have an arbitrary covariance structure. In particular, they show that for nonlinear random-feature models and deep neural networks trained with SGD with a Gaussian model, the optimal batch size with a fixed compute budget is a function of the feature correlation structure. The theory is applied to both Gaussian features and arbitrary features. They also show that training and test error of SGD under small batch sizes can be approximated by the theory. Finally, they demonstrate that the proposed framework can be used to estimate the training error on real data (i.e., MNIST and CIFAR-10).  ","This paper studies the generalization performance of a machine learning algorithm (e.g., neural network) when the data structure of the test loss dynamics depends on the training data. The authors show that stochastic gradient descent (SGD) converges to a test loss that depends on a specific data structure. They show that SGD converges when the structure of data distribution is Gaussian, but not when the features have an arbitrary covariance structure. In particular, they show that for nonlinear random-feature models and deep neural networks trained with SGD with a Gaussian model, the optimal batch size with a fixed compute budget is a function of the feature correlation structure. The theory is applied to both Gaussian features and arbitrary features. They also show that training and test error of SGD under small batch sizes can be approximated by the theory. Finally, they demonstrate that the proposed framework can be used to estimate the training error on real data (i.e., MNIST and CIFAR-10).  "
3861,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"Stochastic gradient descent ( SGD ) USED-FOR nonlinear, nonconvex problem. learning rate CONJUNCTION model. model CONJUNCTION learning rate. AMSGrad USED-FOR local maxima. sharp minima USED-FOR SGD. Method are deep neural networks, and minimal neural network - like construction. Task is optimization problems. ","This paper considers the problem of learning deep neural networks. Stochastic gradient descent (SGD) is used to solve a nonlinear, nonconvex problem. The authors propose a minimal neural network-like construction, where the learning rate and the model are the same. They show that AMSGrad is able to find local maxima of SGD with sharp minima. They also show that the optimization problems can be solved in a similar way. ","This paper considers the problem of learning deep neural networks. Stochastic gradient descent (SGD) is used to solve a nonlinear, nonconvex problem. The authors propose a minimal neural network-like construction, where the learning rate and the model are the same. They show that AMSGrad is able to find local maxima of SGD with sharp minima. They also show that the optimization problems can be solved in a similar way. "
3877,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"gradientbased hyperparameter optimization ( HO ) methods USED-FOR hyperparameters. Implicit Function Theorem ( IFT ) based methods USED-FOR online optimization. high - dimensional hyperparameters CONJUNCTION horizon length. horizon length CONJUNCTION high - dimensional hyperparameters. short horizon bias FEATURE-OF short horizon approximations. knowledge distillation USED-FOR second - order term. Jacobian - vector product ( JVP ) USED-FOR HO step. hyperparameter dimension CONJUNCTION horizon length. horizon length CONJUNCTION hyperparameter dimension. method USED-FOR online optimization. hyperparameter dimension USED-FOR method. meta - learning methods CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION meta - learning methods. meta - learning methods EVALUATE-FOR method. benchmark datasets EVALUATE-FOR method. Method are gradient - based meta - learning methods, inner - optimization, Unrolled differentiation methods, and HO method. ",This paper proposes a new meta-learning method for hyperparameter optimization. The proposed method is based on the implicit function theorem (IFT) and is a generalization of existing hyperparameters optimization methods. The main idea is to use the Jacobian-vector product (JVP) as a second-order term in the inner-optimization step. The authors also propose a knowledge distillation method to further improve the performance of the proposed method.,This paper proposes a new meta-learning method for hyperparameter optimization. The proposed method is based on the implicit function theorem (IFT) and is a generalization of existing hyperparameters optimization methods. The main idea is to use the Jacobian-vector product (JVP) as a second-order term in the inner-optimization step. The authors also propose a knowledge distillation method to further improve the performance of the proposed method.
3893,SP:a64b26faef315c3ece590322291bab198932c604,"tasks USED-FOR meta - learning. meta - learning USED-FOR learning of new tasks. globally shared metalearner USED-FOR tasks. globally shared metalearner USED-FOR meta - learning. customization CONJUNCTION generalization. generalization CONJUNCTION customization. task clustering USED-FOR task - aware modulation. methods USED-FOR task representation. baselearner model USED-FOR task - specific optimization process. features USED-FOR task representation. features CONJUNCTION learning path. learning path CONJUNCTION features. features USED-FOR task representation. learning path USED-FOR task representation. geometric quantities USED-FOR learning path. path representation USED-FOR downstream clustering and modulation. meta path learner USED-FOR path representation. shortcut tunnel USED-FOR feature cluster assignments. shortcut tunnel USED-FOR path. path CONJUNCTION feature cluster assignments. feature cluster assignments CONJUNCTION path. few - shot image classification CONJUNCTION cold - start recommendation. cold - start recommendation CONJUNCTION few - shot image classification. CTML COMPARE baselines. baselines COMPARE CTML. real - world application domains EVALUATE-FOR CTML. real - world application domains EVALUATE-FOR baselines. cold - start recommendation EVALUATE-FOR CTML. cold - start recommendation HYPONYM-OF real - world application domains. few - shot image classification HYPONYM-OF real - world application domains. OtherScientificTerm is task heterogeneity. Method are global meta - learner, rehearsed task learning, and rehearsed learning. ","This paper proposes a meta-learning framework for learning new tasks in the presence of task heterogeneity. The key idea is to train a globally shared metalearner for all tasks, and then use the global meta-learner to guide the learning of new tasks. The authors argue that this is an effective way to tackle the problem of task-specific customization and generalization, and that there is no need for rehearsed task learning. They propose two methods to learn the task representation: (1) a task-aware modulation based on task clustering, and (2) a learning path based on geometric quantities. The baselearner model is trained to learn a task specific optimization process. The task representation is learned using features from the base learner and the learning path. The path representation is used for downstream clustering and modulation, and the path representation from the meta path learner is used to train the path and the feature cluster assignments using a shortcut tunnel. The proposed CTML is evaluated on two real-world application domains: few-shot image classification and cold-start recommendation, and compared with several baselines. ","This paper proposes a meta-learning framework for learning new tasks in the presence of task heterogeneity. The key idea is to train a globally shared metalearner for all tasks, and then use the global meta-learner to guide the learning of new tasks. The authors argue that this is an effective way to tackle the problem of task-specific customization and generalization, and that there is no need for rehearsed task learning. They propose two methods to learn the task representation: (1) a task-aware modulation based on task clustering, and (2) a learning path based on geometric quantities. The baselearner model is trained to learn a task specific optimization process. The task representation is learned using features from the base learner and the learning path. The path representation is used for downstream clustering and modulation, and the path representation from the meta path learner is used to train the path and the feature cluster assignments using a shortcut tunnel. The proposed CTML is evaluated on two real-world application domains: few-shot image classification and cold-start recommendation, and compared with several baselines. "
3909,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"in - distribution ( ID ) data EVALUATE-FOR deep neural networks. methods USED-FOR near OOD samples. labeled data USED-FOR near OOD samples. ensemble - based procedure USED-FOR semi - supervised novelty detection ( SSND ). ensemble - based procedure USED-FOR detection. unlabeled ID and OOD samples USED-FOR ensemble - based procedure. regularization USED-FOR OOD data. regularization USED-FOR It. approach COMPARE SSND methods. SSND methods COMPARE approach. image data sets CONJUNCTION medical image data sets. medical image data sets CONJUNCTION image data sets. medical image data sets EVALUATE-FOR SSND methods. image data sets EVALUATE-FOR SSND methods. medical image data sets EVALUATE-FOR approach. image data sets EVALUATE-FOR approach. Task is expert evaluation. Method is OOD detection algorithms. Material are near OOD data, and ID data. Metric is computational cost. ","This paper proposes an ensemble-based procedure for semi-supervised novelty detection (SSND) for deep neural networks trained on in-distribution (ID) data. The authors propose two methods to detect near OOD samples from labeled data, which is a common problem in OOD detection algorithms. In particular, the authors propose to use an ensemble of OOD and ID samples to perform the detection, and then use a regularization to encourage OOD data to be classified correctly. The paper also proposes a new ensemble- based procedure for the detection based on unlabeled ID and OLD samples. The proposed approach is evaluated on two image data sets and two medical image datasets, and compared with other SSND methods. The results show that the proposed approach outperforms the state-of-the-art methods.  ","This paper proposes an ensemble-based procedure for semi-supervised novelty detection (SSND) for deep neural networks trained on in-distribution (ID) data. The authors propose two methods to detect near OOD samples from labeled data, which is a common problem in OOD detection algorithms. In particular, the authors propose to use an ensemble of OOD and ID samples to perform the detection, and then use a regularization to encourage OOD data to be classified correctly. The paper also proposes a new ensemble- based procedure for the detection based on unlabeled ID and OLD samples. The proposed approach is evaluated on two image data sets and two medical image datasets, and compared with other SSND methods. The results show that the proposed approach outperforms the state-of-the-art methods.  "
3925,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"multi - agent trajectory prediction USED-FOR safe control of robotic systems. representation USED-FOR planning. encoder - decoder architectures USED-FOR scene - consistent multi - agent trajectories. Latent Variable Sequential Set Transformers HYPONYM-OF encoder - decoder architectures. Latent Variable Sequential Set Transformers USED-FOR scene - consistent multi - agent trajectories. AutoBots ” HYPONYM-OF architectures. temporal and social dimensions FEATURE-OF equivariant processing. model USED-FOR single - agent prediction case. Argoverse vehicle prediction challenge EVALUATE-FOR model. global nuScenes vehicle motion prediction leaderboard EVALUATE-FOR model. synthetic partition of TrajNet++ dataset EVALUATE-FOR model. model USED-FOR socially - consistent predictions. multi - agent setting EVALUATE-FOR model. synthetic partition of TrajNet++ dataset EVALUATE-FOR socially - consistent predictions. synthetic partition of TrajNet++ dataset EVALUATE-FOR multi - agent setting. desktop GPU ( 1080 Ti ) USED-FOR models. Method are encoder, and decoder. OtherScientificTerm is sequential structure. Material is Omniglot data. Generic is method. ","This paper proposes a new method for multi-agent trajectory prediction for safe control of robotic systems. The authors propose two encoder-decoder architectures, “Latent Variable Sequential Set Transformers” and “AutoBots”, which are scene-consistent multimodal decoders that learn a representation for planning that is equivariant to both temporal and social dimensions. The proposed model is trained on Omniglot data and tested on the Argoverse vehicle prediction challenge, where it is shown to outperform a model trained on a single-agent prediction case. The model is also tested on a global nuScenes vehicle motion prediction leaderboard, where the model outperforms the model trained for the single agent setting on a synthetic partition of TrajNet++ dataset.  The authors also show that the proposed model can make socially-consistency predictions on the synthetic partition, which is particularly useful for the multi- agent setting. The models are trained using a desktop GPU (1080 Ti) and the encoder and decoder are trained on top of each other, and the sequential structure of the data is used to train the decoder. The method is evaluated on a number of datasets and shows that the method is able to generalize to unseen trajectories. ","This paper proposes a new method for multi-agent trajectory prediction for safe control of robotic systems. The authors propose two encoder-decoder architectures, “Latent Variable Sequential Set Transformers” and “AutoBots”, which are scene-consistent multimodal decoders that learn a representation for planning that is equivariant to both temporal and social dimensions. The proposed model is trained on Omniglot data and tested on the Argoverse vehicle prediction challenge, where it is shown to outperform a model trained on a single-agent prediction case. The model is also tested on a global nuScenes vehicle motion prediction leaderboard, where the model outperforms the model trained for the single agent setting on a synthetic partition of TrajNet++ dataset.  The authors also show that the proposed model can make socially-consistency predictions on the synthetic partition, which is particularly useful for the multi- agent setting. The models are trained using a desktop GPU (1080 Ti) and the encoder and decoder are trained on top of each other, and the sequential structure of the data is used to train the decoder. The method is evaluated on a number of datasets and shows that the method is able to generalize to unseen trajectories. "
3941,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"methods USED-FOR image classification models. baseline explanation technique COMPARE concept - based and counterfactual explanations. concept - based and counterfactual explanations COMPARE baseline explanation technique. baseline COMPARE concept - based explanations. concept - based explanations COMPARE baseline. Counterfactual explanations COMPARE baseline. baseline COMPARE Counterfactual explanations. invertible neural network USED-FOR Counterfactual explanations. technical evaluations CONJUNCTION proxy tasks. proxy tasks CONJUNCTION technical evaluations. Generic are they, and model. Method is synthetic dataset generator. ","This paper proposes two new methods to explain the performance of image classification models. The baseline explanation technique is compared to concept-based and counterfactual explanations, and the results show that the proposed baseline is more interpretable than the previous baseline and the existing concept -based explanations. Counterfactual explanation is based on an invertible neural network, and they are shown to be interpretable as well. The paper also introduces a synthetic dataset generator, which can be used as a proxy for the model. Experiments are conducted on both technical evaluations and proxy tasks. ","This paper proposes two new methods to explain the performance of image classification models. The baseline explanation technique is compared to concept-based and counterfactual explanations, and the results show that the proposed baseline is more interpretable than the previous baseline and the existing concept -based explanations. Counterfactual explanation is based on an invertible neural network, and they are shown to be interpretable as well. The paper also introduces a synthetic dataset generator, which can be used as a proxy for the model. Experiments are conducted on both technical evaluations and proxy tasks. "
3957,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,deep networks USED-FOR backdoor data poisoning attacks. model USED-FOR inference. iterative training procedure USED-FOR poisoned data. boosting framework USED-FOR clean data. boosting framework USED-FOR poisoned data. bootstrapped measure of generalization USED-FOR algorithm. method USED-FOR dirty label backdoor attack. approach COMPARE defenses. defenses COMPARE approach. OtherScientificTerm is malicious data. Method is ensemble of weak learners. ,"This paper studies the problem of backdoor data poisoning attacks on deep networks. In particular, the authors consider the setting where the malicious data is not available but the model is trained on it during inference. The authors propose an iterative training procedure to generate poisoned data, where the poisoned data is generated by an ensemble of weak learners. They also propose a boosting framework to generate clean data from poisoned data. Finally, they propose an algorithm based on a bootstrapped measure of generalization. Experiments show that the proposed method can successfully defend against a dirty label backdoor attack. The proposed approach is shown to be more robust than existing defenses.","This paper studies the problem of backdoor data poisoning attacks on deep networks. In particular, the authors consider the setting where the malicious data is not available but the model is trained on it during inference. The authors propose an iterative training procedure to generate poisoned data, where the poisoned data is generated by an ensemble of weak learners. They also propose a boosting framework to generate clean data from poisoned data. Finally, they propose an algorithm based on a bootstrapped measure of generalization. Experiments show that the proposed method can successfully defend against a dirty label backdoor attack. The proposed approach is shown to be more robust than existing defenses."
3973,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"MLTC USED-FOR modeling label correlations. single - label text classification methods COMPARE MLTC. MLTC COMPARE single - label text classification methods. document representation learning USED-FOR single - label text classification methods. label - correlation simplification CONJUNCTION sequencing label sets. sequencing label sets CONJUNCTION label - correlation simplification. sequencing label sets CONJUNCTION label - correlation overload. label - correlation overload CONJUNCTION sequencing label sets. It USED-FOR inductive bias. sequencing label sets HYPONYM-OF inductive bias. label - correlation simplification HYPONYM-OF inductive bias. latent label representations USED-FOR label correlations. latent labels USED-FOR contextual encodings. benchmark datasets EVALUATE-FOR It. label - correlation utilization CONJUNCTION document representation. document representation CONJUNCTION label - correlation utilization. token embeddings COMPARE latent labels. latent labels COMPARE token embeddings. embeddings FEATURE-OF latent labels. task information FEATURE-OF they. Task is Multi - label text classification ( MLTC ). OtherScientificTerm are complex label dependencies, and text tokens. Generic are method, and BERT. Method are latent - label encodings, latent and distributed correlation modeling, and latent label embeddings. ","This paper proposes Multi-label text classification (MLTC), a new method for multi-label classification, where multiple labels have complex label dependencies. The authors propose MLTC for modeling label correlations between labels and document representation learning, which is different from existing single-labels text classification methods. The proposed method is based on BERT. It alleviates inductive bias such as label-correlation simplification and sequencing label sets, which are common problems in single-label texts. It is evaluated on several benchmark datasets and shows that the latent label representations of label correlations are useful for learning label correlations, and that the contextual encodings of latent labels can be used to learn contextual embeddings of contextualized text tokens as well.    The authors also show that latent-label encoders can be shared across multiple labels, and the latent and distributed correlation modeling can be combined to improve the performance of the proposed method.  The paper also shows that latent label embedding of text tokens is more powerful than the corresponding token embedding from the latent labels. This is due to the fact that they are more sensitive to task information, and thus can capture the trade-off between label-relation utilization and the document representation. ","This paper proposes Multi-label text classification (MLTC), a new method for multi-label classification, where multiple labels have complex label dependencies. The authors propose MLTC for modeling label correlations between labels and document representation learning, which is different from existing single-labels text classification methods. The proposed method is based on BERT. It alleviates inductive bias such as label-correlation simplification and sequencing label sets, which are common problems in single-label texts. It is evaluated on several benchmark datasets and shows that the latent label representations of label correlations are useful for learning label correlations, and that the contextual encodings of latent labels can be used to learn contextual embeddings of contextualized text tokens as well.    The authors also show that latent-label encoders can be shared across multiple labels, and the latent and distributed correlation modeling can be combined to improve the performance of the proposed method.  The paper also shows that latent label embedding of text tokens is more powerful than the corresponding token embedding from the latent labels. This is due to the fact that they are more sensitive to task information, and thus can capture the trade-off between label-relation utilization and the document representation. "
3989,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"images CONJUNCTION audio. audio CONJUNCTION images. deep convolutional networks USED-FOR tasks. deep convolutional networks USED-FOR highdimensional data. highdimensional data USED-FOR tasks. images HYPONYM-OF highdimensional data. audio HYPONYM-OF highdimensional data. convolution and pooling layers FEATURE-OF hierarchical kernels. convolutional kernel networks USED-FOR hierarchical kernels. norm USED-FOR spatial similarities. pooling layers USED-FOR norm. additive models of interaction terms PART-OF RKHS. pooling layers USED-FOR spatial similarities. pooling CONJUNCTION patches. patches CONJUNCTION pooling. sample complexity guarantees EVALUATE-FOR patches. Generic are they, and terms. Method is kernel methods. Material is vision datasets. OtherScientificTerm are functional space, inductive bias, generalization bounds, and regularities. ","This paper studies the generalization of deep convolutional networks on highdimensional data (e.g., images and audio). The authors consider hierarchical kernels with convolution and pooling layers, and show that hierarchical kernels can be approximated by convolution kernel networks. In particular, they show that the norm of the spatial similarities induced by pooling and convolution layers is a function of the functional space. They also show that additive models of interaction terms in the RKHS are inductive biases, and that they can be used to derive generalization bounds. Finally, the authors provide sample complexity guarantees for patches and convolutions on vision datasets.  ","This paper studies the generalization of deep convolutional networks on highdimensional data (e.g., images and audio). The authors consider hierarchical kernels with convolution and pooling layers, and show that hierarchical kernels can be approximated by convolution kernel networks. In particular, they show that the norm of the spatial similarities induced by pooling and convolution layers is a function of the functional space. They also show that additive models of interaction terms in the RKHS are inductive biases, and that they can be used to derive generalization bounds. Finally, the authors provide sample complexity guarantees for patches and convolutions on vision datasets.  "
4005,SP:7bee8d65c68765cbfe38767743fec27981879d34,"Neural Tangent Kernel ( NTK ) PART-OF deep learning. NTK USED-FOR training and generalization of NN architectures. infinite width limit FEATURE-OF NTK. NTK USED-FOR NNs. architecture search CONJUNCTION meta - learning. meta - learning CONJUNCTION architecture search. NTK USED-FOR finite widths. compute and memory requirements FEATURE-OF NTK computation. NTK computation PART-OF finite width networks. compute and memory requirements FEATURE-OF finite width NTK. neural networks USED-FOR algorithms. algorithms USED-FOR finite width NTK. compute and memory requirements EVALUATE-FOR algorithms. attention CONJUNCTION recurrence. recurrence CONJUNCTION attention. convolutions CONJUNCTION attention. attention CONJUNCTION convolutions. general - purpose JAX function transformations USED-FOR differentiable computation. algorithms USED-FOR differentiable computation. convolutions CONJUNCTION recurrence. recurrence CONJUNCTION convolutions. general - purpose JAX function transformations USED-FOR algorithms. recurrence HYPONYM-OF algorithms. convolutions HYPONYM-OF general - purpose JAX function transformations. attention HYPONYM-OF algorithms. recurrence HYPONYM-OF general - purpose JAX function transformations. attention HYPONYM-OF general - purpose JAX function transformations. recurrence HYPONYM-OF differentiable computation. convolutions HYPONYM-OF differentiable computation. attention HYPONYM-OF differentiable computation. OtherScientificTerm are neural network ( NN ) Jacobians, and hyper - parameters. Method is NN architectures. ","This paper studies the role of Neural Tangent Kernel (NTK) in deep learning. The authors show that the neural network (NN) Jacobians of the NTK can be seen as a special case of the infinite width limit of NTK, and that NTK plays a key role in the training and generalization of NN architectures. In particular, the authors prove that the infinite-width limit of NNs can be approximated by NTK for finite widths, and they show that this can be used for architecture search and meta-learning. In addition, they also show that there exists a finite width NTK that has compute and memory requirements for NTK computation in finite width networks. Finally, they propose three algorithms for approximating the finite width of neural networks based on neural networks. The algorithms are based on general-purpose JAX function transformations (convolutions, attention, recurrence) for differentiable computation such as convolutions and attention with different hyper-parameters.  ","This paper studies the role of Neural Tangent Kernel (NTK) in deep learning. The authors show that the neural network (NN) Jacobians of the NTK can be seen as a special case of the infinite width limit of NTK, and that NTK plays a key role in the training and generalization of NN architectures. In particular, the authors prove that the infinite-width limit of NNs can be approximated by NTK for finite widths, and they show that this can be used for architecture search and meta-learning. In addition, they also show that there exists a finite width NTK that has compute and memory requirements for NTK computation in finite width networks. Finally, they propose three algorithms for approximating the finite width of neural networks based on neural networks. The algorithms are based on general-purpose JAX function transformations (convolutions, attention, recurrence) for differentiable computation such as convolutions and attention with different hyper-parameters.  "
4021,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"policy USED-FOR expected return. problem setting USED-FOR real - world scenarios. safety constraints FEATURE-OF policy. policy USED-FOR offline RL setting. estimation error FEATURE-OF offpolicy evaluation. offline constrained RL algorithm USED-FOR policy. stationary distribution FEATURE-OF policy. stationary distribution corrections FEATURE-OF optimal policy. algorithm USED-FOR stationary distribution corrections. algorithm USED-FOR cost - conservative policy. returns FEATURE-OF optimal policy. constraint satisfaction CONJUNCTION return - maximization. return - maximization CONJUNCTION constraint satisfaction. COptiDICE COMPARE baseline algorithms. baseline algorithms COMPARE COptiDICE. COptiDICE USED-FOR policies. return - maximization FEATURE-OF policies. constraint satisfaction FEATURE-OF policies. constraint satisfaction EVALUATE-FOR COptiDICE. return - maximization EVALUATE-FOR COptiDICE. Task is offline constrained reinforcement learning ( RL ) problem. OtherScientificTerm are cost constraints, and cost upper bound. Material is pre - collected dataset. ","This paper considers the offline constrained reinforcement learning (RL) problem, where the cost constraints are imposed on a pre-collected dataset and the goal is to learn a policy that maximizes the expected return under a set of safety constraints. This problem setting is relevant to many real-world scenarios, and the authors propose a new offline RL algorithm called COptiDICE to learn such a policy in the offline RL setting. The key idea is to use an offline constrained RL algorithm to learn the policy that minimizes the estimation error of the offpolicy evaluation. The cost upper bound is based on the assumption that the optimal policy satisfies a stationary distribution over the stationary distribution of the offline dataset. The authors propose an algorithm that learns a cost-conservative policy that obtains stationary distribution corrections on the returns of an optimal policy that satisfies this stationary distribution. The proposed algorithm COptIDICE outperforms baseline algorithms in terms of both constraint satisfaction and return-maximization of the learned policies.   ","This paper considers the offline constrained reinforcement learning (RL) problem, where the cost constraints are imposed on a pre-collected dataset and the goal is to learn a policy that maximizes the expected return under a set of safety constraints. This problem setting is relevant to many real-world scenarios, and the authors propose a new offline RL algorithm called COptiDICE to learn such a policy in the offline RL setting. The key idea is to use an offline constrained RL algorithm to learn the policy that minimizes the estimation error of the offpolicy evaluation. The cost upper bound is based on the assumption that the optimal policy satisfies a stationary distribution over the stationary distribution of the offline dataset. The authors propose an algorithm that learns a cost-conservative policy that obtains stationary distribution corrections on the returns of an optimal policy that satisfies this stationary distribution. The proposed algorithm COptIDICE outperforms baseline algorithms in terms of both constraint satisfaction and return-maximization of the learned policies.   "
4037,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,"dataparallel CONJUNCTION model - parallel training algorithms. model - parallel training algorithms CONJUNCTION dataparallel. parallelization strategies USED-FOR GRU. model - parallel training algorithms HYPONYM-OF parallelization strategies. dataparallel HYPONYM-OF parallelization strategies. training time EVALUATE-FOR approaches. parallel training scheme USED-FOR GRU. parallel - in - time HYPONYM-OF parallel training scheme. multigrid reduction in time ( MGRIT ) solver USED-FOR parallel training scheme. hierarchical correction of the hidden state USED-FOR end - to - end communication. parallel training scheme COMPARE serial approach. serial approach COMPARE parallel training scheme. HMDB51 dataset EVALUATE-FOR parallel training scheme. speedup EVALUATE-FOR serial approach. speedup EVALUATE-FOR parallel training scheme. parallelization strategy COMPARE parallel GRU algorithm. parallel GRU algorithm COMPARE parallelization strategy. sequence length FEATURE-OF parallelization strategy. Task is Parallelizing Gated Recurrent Unit ( GRU ) networks. Method are MGRIT, and gradient descent. OtherScientificTerm is processors. Material is image sequence. ","This paper considers the problem of parallelizing Gated Recurrent Unit (GRU) networks. The authors consider two parallelization strategies for GRU: dataparallel and model-parallel training algorithms. Both approaches have been shown to significantly reduce the training time of a GRU. The paper proposes a parallel training scheme called parallel-in-time, which is a variant of the multigrid reduction in time (MGRIT) solver. The main idea of MGRIT is to train multiple processors on the same image sequence, but each processor is trained independently. The parallelization strategy is based on the hierarchical correction of the hidden state, which allows end-to-end communication between the processors.  The paper shows that the parallelization scheme outperforms the serial approach in terms of speedup on the HMDB51 dataset. In addition, the paper also shows that a parallelized strategy with sequence length is more efficient than a parallel GRU algorithm based on gradient descent.","This paper considers the problem of parallelizing Gated Recurrent Unit (GRU) networks. The authors consider two parallelization strategies for GRU: dataparallel and model-parallel training algorithms. Both approaches have been shown to significantly reduce the training time of a GRU. The paper proposes a parallel training scheme called parallel-in-time, which is a variant of the multigrid reduction in time (MGRIT) solver. The main idea of MGRIT is to train multiple processors on the same image sequence, but each processor is trained independently. The parallelization strategy is based on the hierarchical correction of the hidden state, which allows end-to-end communication between the processors.  The paper shows that the parallelization scheme outperforms the serial approach in terms of speedup on the HMDB51 dataset. In addition, the paper also shows that a parallelized strategy with sequence length is more efficient than a parallel GRU algorithm based on gradient descent."
4053,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"Functional magnetic resonance imaging ( fMRI ) HYPONYM-OF noisy measurement of brain activity. measurement resolution USED-FOR spatiotemporal averaging. PCA CONJUNCTION shared response modeling ( SRM ). shared response modeling ( SRM ) CONJUNCTION PCA. linear methods USED-FOR they. shared response modeling ( SRM ) HYPONYM-OF linear methods. PCA HYPONYM-OF linear methods. neural network USED-FOR common embedding. common space USED-FOR extensible manifold. classification accuracy EVALUATE-FOR stimulus features. framework USED-FOR applications. OtherScientificTerm are environmental differences, intrinsic dimension, brain activity, intrinsic structure, and noise. Generic is approaches. Material is raw fMRI signals. Task is cross - subject translation of fMRI signals. ","Functional magnetic resonance imaging (fMRI) is a noisy measurement of brain activity that is sensitive to environmental differences between subjects. The paper proposes two approaches to improve the performance of fMRI signals. The first approach is based on spatiotemporal averaging at different measurement resolution, where the intrinsic dimension of the input is the same for all subjects but the measurement resolution is different for each subject. The second approach is to use linear methods such as PCA and shared response modeling (SRM) to learn a common embedding between the input and the output of a neural network. The idea is to learn an extensible manifold over a common space that is shared across all subjects.  The authors show that this framework can be applied to a variety of applications, including cross-subject translation (cross-domain adaptation) and cross-source adaptation (translational adaptation) to improve classification accuracy for different stimulus features. The authors also show that the intrinsic structure of the brain activity is similar across subjects, which is an important property that can be exploited to improve performance.   ","Functional magnetic resonance imaging (fMRI) is a noisy measurement of brain activity that is sensitive to environmental differences between subjects. The paper proposes two approaches to improve the performance of fMRI signals. The first approach is based on spatiotemporal averaging at different measurement resolution, where the intrinsic dimension of the input is the same for all subjects but the measurement resolution is different for each subject. The second approach is to use linear methods such as PCA and shared response modeling (SRM) to learn a common embedding between the input and the output of a neural network. The idea is to learn an extensible manifold over a common space that is shared across all subjects.  The authors show that this framework can be applied to a variety of applications, including cross-subject translation (cross-domain adaptation) and cross-source adaptation (translational adaptation) to improve classification accuracy for different stimulus features. The authors also show that the intrinsic structure of the brain activity is similar across subjects, which is an important property that can be exploited to improve performance.   "
4069,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"biological phenomena CONJUNCTION self - driving cars. self - driving cars CONJUNCTION biological phenomena. Detecting out - of - distribution examples USED-FOR safety - critical machine learning applications. detecting novel biological phenomena HYPONYM-OF safety - critical machine learning applications. self - driving cars HYPONYM-OF safety - critical machine learning applications. benchmarks USED-FOR large - scale settings. ImageNet-21 K USED-FOR PASCAL VOC and COCO multilabel anomaly detectors. benchmark USED-FOR anomaly segmentation. road anomalies FEATURE-OF segmentation benchmark. segmentation benchmark USED-FOR benchmark. detector COMPARE prior methods. prior methods COMPARE detector. maximum logit USED-FOR detector. OtherScientificTerm are small - scale settings, and real - world settings. Task is out - of - distribution detection. Material is high - resolution images. Method is ImageNet multiclass anomaly detectors. ","Detecting out-of-distribution examples is an important problem in safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. This paper proposes two benchmarks for large-scale settings, ImageNet-21K for PASCAL VOC and COCO multilabel anomaly detectors, and a new benchmark for anomaly segmentation based on a segmentation benchmark on road anomalies. The authors show that the proposed benchmark is a good benchmark for out- of-sample detection, and that the performance of ImageNet multiclass anomaly detectors is comparable to the state of the art. They also show that a detector trained with maximum logit is able to outperform prior methods in the small-scale setting.    The authors also provide a theoretical analysis of the performance in real-world settings, and show that in high-resolution images, the performance is similar to that of the standard detector. ","Detecting out-of-distribution examples is an important problem in safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. This paper proposes two benchmarks for large-scale settings, ImageNet-21K for PASCAL VOC and COCO multilabel anomaly detectors, and a new benchmark for anomaly segmentation based on a segmentation benchmark on road anomalies. The authors show that the proposed benchmark is a good benchmark for out- of-sample detection, and that the performance of ImageNet multiclass anomaly detectors is comparable to the state of the art. They also show that a detector trained with maximum logit is able to outperform prior methods in the small-scale setting.    The authors also provide a theoretical analysis of the performance in real-world settings, and show that in high-resolution images, the performance is similar to that of the standard detector. "
4085,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"parametric models USED-FOR intransitive tournaments. d dimensional node representations USED-FOR parametric models. d dimensional representations USED-FOR class of tournaments. theory USED-FOR parametric tournament representations. d dimensional representations USED-FOR class of tournaments. forbidden configurations FEATURE-OF tournament classes. tournaments PART-OF forbidden flip class. rank 2 tournaments CONJUNCTION locally - transitive tournaments. locally - transitive tournaments CONJUNCTION rank 2 tournaments. tournament class USED-FOR minimum feedback arc set problem. Quicksort procedure USED-FOR minimum feedback arc set problem. coned - doubly regular tournament FEATURE-OF flip class. forbidden configuration FEATURE-OF flip class. minimum dimension USED-FOR tournaments. upper bound USED-FOR smallest representation dimension. flip class FEATURE-OF tournament. flip class FEATURE-OF feedback arc set. OtherScientificTerm are Real world tournaments, union of flip classes, rank d tournament class, and sign - rank of matrices. ","This paper studies the problem of learning parametric models for intransitive tournaments with d dimensional node representations. Real world tournaments can be seen as a special case of this problem. The authors propose a theory for parametric tournament representations based on d dimensional representations for a class of tournaments. They show that there are two types of tournament classes with forbidden configurations: (1) tournaments in the forbidden flip class, where the union of flip classes is a rank d tournament class, and (2) tournaments that are rank 2 tournaments or locally-transitive tournaments.  The authors show that for any tournament class that has a minimum feedback arc set problem that can be solved by the Quicksort procedure, the minimum dimension of the smallest representation dimension for all tournaments in this class is O(1/n). The authors also provide an upper bound on the minimum representation dimension of a tournament in the flip class with a forbidden configuration. They also show that in the case of a coned-doubly regular tournament, there exists a flip class that is the only one with a guaranteed forbidden configuration, and that this flip class can be represented as a rank-rank of matrices.","This paper studies the problem of learning parametric models for intransitive tournaments with d dimensional node representations. Real world tournaments can be seen as a special case of this problem. The authors propose a theory for parametric tournament representations based on d dimensional representations for a class of tournaments. They show that there are two types of tournament classes with forbidden configurations: (1) tournaments in the forbidden flip class, where the union of flip classes is a rank d tournament class, and (2) tournaments that are rank 2 tournaments or locally-transitive tournaments.  The authors show that for any tournament class that has a minimum feedback arc set problem that can be solved by the Quicksort procedure, the minimum dimension of the smallest representation dimension for all tournaments in this class is O(1/n). The authors also provide an upper bound on the minimum representation dimension of a tournament in the flip class with a forbidden configuration. They also show that in the case of a coned-doubly regular tournament, there exists a flip class that is the only one with a guaranteed forbidden configuration, and that this flip class can be represented as a rank-rank of matrices."
4101,SP:d39765dcc8950d4fc1d43e4c167208736578882e,context dataset USED-FOR Neural processes ( NPs ). identifier USED-FOR task. context representation USED-FOR identifier. dataset USED-FOR context representation. NPs USED-FOR identifier. context representation USED-FOR NPs. dataset USED-FOR NPs. network architectures CONJUNCTION aggregation functions. aggregation functions CONJUNCTION network architectures. NPs USED-FOR context embedding approaches. prediction accuracy EVALUATE-FOR NPs. permutation invariant FEATURE-OF aggregation functions. stochastic attention mechanism USED-FOR context information. stochastic attention mechanism USED-FOR NPs. NPs USED-FOR context information. method USED-FOR context embedding. method USED-FOR NPs. NPs USED-FOR context embedding. information theory USED-FOR method. features USED-FOR NPs. method USED-FOR context embedding. noisy data sets CONJUNCTION restricted task distributions. restricted task distributions CONJUNCTION noisy data sets. noisy data sets USED-FOR method. context embeddings USED-FOR NPs. predator - prey model CONJUNCTION image completion. image completion CONJUNCTION predator - prey model. 1D regression CONJUNCTION predator - prey model. predator - prey model CONJUNCTION 1D regression. approach COMPARE NPs. NPs COMPARE approach. 1D regression USED-FOR NPs. predator - prey model USED-FOR NPs. predator - prey model USED-FOR approach. image completion USED-FOR approach. 1D regression EVALUATE-FOR approach. MovieLens-10k dataset HYPONYM-OF real - world problem. real - world problem EVALUATE-FOR method. MovieLens-10k dataset EVALUATE-FOR method. ,"This paper proposes a new context dataset for Neural processes (NPs). NPs learn an identifier for each task based on the context representation of the dataset. The dataset is used to train NPs to learn the identifier for a given task. The authors show that NPs trained on this dataset can learn a context representation that can be used to improve the performance of NPs in terms of prediction accuracy. They also show that existing context embedding approaches based on NPs are permutation invariant to changes in network architectures and aggregation functions.   The authors propose a new method for learning context embeddings for NPs based on information theory. They use a stochastic attention mechanism to encode the context information in NPs. They show that the proposed method is able to learn a good contextual embedding using NPs using features extracted from the original dataset. They demonstrate that their method can be applied to learn both noisy data sets and restricted task distributions. They evaluate their method on 1D regression, predator-prey model, and image completion on a real-world problem (the MovieLens-10k dataset). Their approach is shown to outperform existing NPs on all three tasks.","This paper proposes a new context dataset for Neural processes (NPs). NPs learn an identifier for each task based on the context representation of the dataset. The dataset is used to train NPs to learn the identifier for a given task. The authors show that NPs trained on this dataset can learn a context representation that can be used to improve the performance of NPs in terms of prediction accuracy. They also show that existing context embedding approaches based on NPs are permutation invariant to changes in network architectures and aggregation functions.   The authors propose a new method for learning context embeddings for NPs based on information theory. They use a stochastic attention mechanism to encode the context information in NPs. They show that the proposed method is able to learn a good contextual embedding using NPs using features extracted from the original dataset. They demonstrate that their method can be applied to learn both noisy data sets and restricted task distributions. They evaluate their method on 1D regression, predator-prey model, and image completion on a real-world problem (the MovieLens-10k dataset). Their approach is shown to outperform existing NPs on all three tasks."
4117,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"Transformer language models USED-FOR NLP tasks. prototype networks PART-OF model architecture. architecture COMPARE language models. language models COMPARE architecture. user interactions USED-FOR it. Metric is interpretability. Generic are black - box models, and network. OtherScientificTerm is human capabilities. Method is data - driven approaches. ","This paper investigates the interpretability of Transformer language models for NLP tasks. The authors propose a new model architecture that incorporates prototype networks to improve interpretability. They show that the proposed architecture is more interpretable than existing language models, and that it is able to learn from user interactions. They also show that black-box models are interpretable and that human capabilities are transferrable across different tasks.    The paper is well-written, well-motivated, and well-structured. However, there is a lack of comparison with existing data-driven approaches, and it is not clear to me that this paper is a significant contribution to the field.  The authors also do not provide a detailed analysis of the network, or a detailed comparison to existing work.","This paper investigates the interpretability of Transformer language models for NLP tasks. The authors propose a new model architecture that incorporates prototype networks to improve interpretability. They show that the proposed architecture is more interpretable than existing language models, and that it is able to learn from user interactions. They also show that black-box models are interpretable and that human capabilities are transferrable across different tasks.    The paper is well-written, well-motivated, and well-structured. However, there is a lack of comparison with existing data-driven approaches, and it is not clear to me that this paper is a significant contribution to the field.  The authors also do not provide a detailed analysis of the network, or a detailed comparison to existing work."
4133,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"Catastrophic forgetting PART-OF continual learning. Trust Region Gradient Projection ( TRGP ) USED-FOR continual learning. Trust Region Gradient Projection ( TRGP ) USED-FOR forward knowledge transfer. continual learning USED-FOR forward knowledge transfer. scaled weight projection USED-FOR frozen weights. frozen weights PART-OF trust region. layer - wise scaling matrix USED-FOR scaled weight projection. layer - wise scaling matrix USED-FOR frozen weights. TRGP USED-FOR knowledge transfer. scaling matrices CONJUNCTION model. model CONJUNCTION scaling matrices. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Generic are methods, and task. OtherScientificTerm are optimization space, task correlation, layer - wise and single - shot manner, and subspaces of old tasks. Method is norm of gradient projection. ","This paper proposes Trust Region Gradient Projection (TRGP) for continual learning to address the issue of catastrophic forgetting in continual learning. Previous methods rely on the assumption that the optimization space is a linear function of the current task and the previous task. However, this assumption does not account for task correlation. To address this issue, the authors propose to use a layer-wise and single-shot manner, where each layer is partitioned into a trust region and the frozen weights in the trust region are projected to a layer -wise scaling matrix, and the scaled weight projection is applied to these frozen weights. The authors show that the norm of gradient projection can be used as a proxy for the task correlation, and that the TRGP is able to perform knowledge transfer between different subspaces of old tasks. They also show that scaling matrices and the model can be learned in a single shot manner. Experiments show that this approach outperforms previous state-of-the-art methods.","This paper proposes Trust Region Gradient Projection (TRGP) for continual learning to address the issue of catastrophic forgetting in continual learning. Previous methods rely on the assumption that the optimization space is a linear function of the current task and the previous task. However, this assumption does not account for task correlation. To address this issue, the authors propose to use a layer-wise and single-shot manner, where each layer is partitioned into a trust region and the frozen weights in the trust region are projected to a layer -wise scaling matrix, and the scaled weight projection is applied to these frozen weights. The authors show that the norm of gradient projection can be used as a proxy for the task correlation, and that the TRGP is able to perform knowledge transfer between different subspaces of old tasks. They also show that scaling matrices and the model can be learned in a single shot manner. Experiments show that this approach outperforms previous state-of-the-art methods."
4149,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,Optimization CONJUNCTION generalization. generalization CONJUNCTION Optimization. generalization PART-OF machine learning. Optimization PART-OF machine learning. framework USED-FOR generalization. framework USED-FOR optimization. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. gradient flow algorithm USED-FOR length of optimization trajectory. length of optimization trajectory USED-FOR generalization error. initialization USED-FOR gradient flow. estimate USED-FOR length - based generalization bound. short optimization paths USED-FOR generalization. it USED-FOR generalization estimates. kernel regression CONJUNCTION overparameterized two - layer ReLU neural networks. overparameterized two - layer ReLU neural networks CONJUNCTION kernel regression. underdetermined lp linear regression CONJUNCTION kernel regression. kernel regression CONJUNCTION underdetermined lp linear regression. it USED-FOR machine learning models. generalization estimates USED-FOR machine learning models. overparameterized two - layer ReLU neural networks HYPONYM-OF machine learning models. underdetermined lp linear regression HYPONYM-OF machine learning models. kernel regression HYPONYM-OF machine learning models. Generic is approach. OtherScientificTerm is explicit length estimate. ,"Optimization and generalization in machine learning is an important topic. This paper proposes a new framework to study the generalization of optimization and generalisation in the context of short optimization paths. The authors propose a gradient flow algorithm to estimate the length of optimization trajectory, which is then used to compute a generalization error based on the length. This approach is based on an explicit length estimate of the gradient flow, and the authors show that this estimate can be used to derive a length-based generalization bound. The paper also shows that the length is a function of the initialization of gradient flow and that it can be applied to generalization estimates for various machine learning models such as underdetermined lp linear regression, kernel regression, and overparameterized two-layer ReLU neural networks.","Optimization and generalization in machine learning is an important topic. This paper proposes a new framework to study the generalization of optimization and generalisation in the context of short optimization paths. The authors propose a gradient flow algorithm to estimate the length of optimization trajectory, which is then used to compute a generalization error based on the length. This approach is based on an explicit length estimate of the gradient flow, and the authors show that this estimate can be used to derive a length-based generalization bound. The paper also shows that the length is a function of the initialization of gradient flow and that it can be applied to generalization estimates for various machine learning models such as underdetermined lp linear regression, kernel regression, and overparameterized two-layer ReLU neural networks."
4165,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"Adversarial examples USED-FOR deep learning systems. high - frequency noise FEATURE-OF adversarial examples. CIFAR-10 CONJUNCTION ImageNet - derived datasets. ImageNet - derived datasets CONJUNCTION CIFAR-10. ImageNet - derived datasets USED-FOR models. CIFAR-10 USED-FOR models. frequency constraints USED-FOR robust models. Task is attacks. Generic are examples, and framework. Method is frequency - based understanding of adversarial examples. OtherScientificTerm is frequency - based explanation. ","Adversarial examples for deep learning systems can be classified as high-frequency noise, and attacks can be seen as examples where the frequency of the examples is high. This paper proposes a frequency-based understanding of adversarial examples and proposes a framework to identify such examples. The authors show that models trained on CIFAR-10 and ImageNet-derived datasets are robust to such attacks. They also show that robust models trained with frequency constraints are more robust than models trained without frequency constraints.    The authors also show how to use frequency constraints in the context of robust models. ","Adversarial examples for deep learning systems can be classified as high-frequency noise, and attacks can be seen as examples where the frequency of the examples is high. This paper proposes a frequency-based understanding of adversarial examples and proposes a framework to identify such examples. The authors show that models trained on CIFAR-10 and ImageNet-derived datasets are robust to such attacks. They also show that robust models trained with frequency constraints are more robust than models trained without frequency constraints.    The authors also show how to use frequency constraints in the context of robust models. "
4181,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"relational inductive bias ( homophily assumption ) USED-FOR graph structures. graph structures USED-FOR Graph Neural Networks ( GNNs ). GNNs COMPARE NNs. NNs COMPARE GNNs. GNNs COMPARE graph - agnostic NNs. graph - agnostic NNs COMPARE GNNs. NNs USED-FOR real - world tasks. real - world tasks EVALUATE-FOR GNNs. aggregation operation USED-FOR GNNs. similarity matrix USED-FOR GNNs. graph structure CONJUNCTION features. features CONJUNCTION graph structure. features USED-FOR GNNs. similarity matrix USED-FOR metrics. metrics COMPARE homophily metrics. homophily metrics COMPARE metrics. synthetic graphs EVALUATE-FOR homophily metrics. diversification operation USED-FOR harmful heterophily. diversification CONJUNCTION identity channels. identity channels CONJUNCTION diversification. aggregation CONJUNCTION diversification. diversification CONJUNCTION aggregation. Adaptive Channel Mixing ( ACM ) framework USED-FOR aggregation. identity channels USED-FOR harmful heterophily. Adaptive Channel Mixing ( ACM ) framework USED-FOR diversification. Adaptive Channel Mixing ( ACM ) framework USED-FOR harmful heterophily. identity channels USED-FOR GNN layer. Adaptive Channel Mixing ( ACM ) framework USED-FOR identity channels. diversification USED-FOR GNN layer. GNN layer USED-FOR harmful heterophily. realworld node classification tasks EVALUATE-FOR ACM - augmented baselines. They COMPARE GNNs. GNNs COMPARE They. tasks EVALUATE-FOR They. tasks EVALUATE-FOR GNNs. OtherScientificTerm are Heterophily, and heterophily. Method is filterbanks. ","Graph Neural Networks (GNNs) have been shown to have a relational inductive bias (homophily assumption) towards graph structures. Heterophily is defined as the ability of GNNs to generalize better than NNs on real-world tasks when the graph structure and features are similar.   This paper shows that under the same aggregation operation, the graph-agnostic NNs are more homophily than graph-agnostic NNs.  The authors propose two metrics for measuring heterophily based on the similarity matrix between two graphs. The authors show that these metrics are more robust to heterophilty metrics on synthetic graphs. They also show that the diversification operation is able to mitigate harmfulheterophily.  Finally, the authors propose an Adaptive Channel Mixing (ACM) framework to combine aggregation, diversification, and identity channels to mitigate the harmful heterophiliness of a GNN layer.  They show that their ACM-augmented baselines outperform the state-of-the-art on a number of realworld node classification tasks. They are also shown to outperform GNN's on a variety of tasks. ","Graph Neural Networks (GNNs) have been shown to have a relational inductive bias (homophily assumption) towards graph structures. Heterophily is defined as the ability of GNNs to generalize better than NNs on real-world tasks when the graph structure and features are similar.   This paper shows that under the same aggregation operation, the graph-agnostic NNs are more homophily than graph-agnostic NNs.  The authors propose two metrics for measuring heterophily based on the similarity matrix between two graphs. The authors show that these metrics are more robust to heterophilty metrics on synthetic graphs. They also show that the diversification operation is able to mitigate harmfulheterophily.  Finally, the authors propose an Adaptive Channel Mixing (ACM) framework to combine aggregation, diversification, and identity channels to mitigate the harmful heterophiliness of a GNN layer.  They show that their ACM-augmented baselines outperform the state-of-the-art on a number of realworld node classification tasks. They are also shown to outperform GNN's on a variety of tasks. "
4197,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,generalizability FEATURE-OF deep RL approach. RL training techniques USED-FOR deep learning architecture. deep learning architecture PART-OF proposition. equivariance USED-FOR training. local search heuristics USED-FOR value landscape. local search heuristics CONJUNCTION RL training. RL training CONJUNCTION local search heuristics. RL training USED-FOR value landscape. proposition COMPARE deep RL methods. deep RL methods COMPARE proposition. random and realistic TSP problems EVALUATE-FOR deep RL methods. random and realistic TSP problems EVALUATE-FOR proposition. Method is Deep reinforcement learning ( RL ). Material is larger - sized instances. Generic is approach. Task is ablation study. ,"Deep reinforcement learning (RL) has been shown to generalize well to larger-sized instances. However, the generalizability of a deep RL approach is not well-studied. This paper proposes a new proposition that combines existing RL training techniques to learn a deep learning architecture that is equivariant to changes in the training environment. The approach is based on the observation that local search heuristics and RL training converge to the same value landscape. The proposition is evaluated on random and realistic TSP problems and shows that the proposed proposition outperforms existing deep RL methods. An ablation study is also performed to show the effectiveness of the proposed approach.","Deep reinforcement learning (RL) has been shown to generalize well to larger-sized instances. However, the generalizability of a deep RL approach is not well-studied. This paper proposes a new proposition that combines existing RL training techniques to learn a deep learning architecture that is equivariant to changes in the training environment. The approach is based on the observation that local search heuristics and RL training converge to the same value landscape. The proposition is evaluated on random and realistic TSP problems and shows that the proposed proposition outperforms existing deep RL methods. An ablation study is also performed to show the effectiveness of the proposed approach."
4213,SP:8aa471b92e2671d471107c087164378f45fb204f,"Federated learning ( FL ) HYPONYM-OF privacy - preserving collaborative learning paradigm. framework USED-FOR non - IID issue. local generative adversarial network ( GAN ) USED-FOR synthetic data. parameter server ( PS ) USED-FOR global shared synthetic dataset. confident threshold USED-FOR pseudo labeling. pseudo labeling USED-FOR PS. local private dataset CONJUNCTION labeled synthetic dataset. labeled synthetic dataset CONJUNCTION local private dataset. artificial noise USED-FOR local model gradients. local GANs USED-FOR privacy. differential privacy USED-FOR local GANs. artificial noise USED-FOR local GANs. framework COMPARE baseline methods. baseline methods COMPARE framework. supervised and semi - supervised settings FEATURE-OF benchmark datasets. supervised and semi - supervised settings EVALUATE-FOR framework. supervised and semi - supervised settings EVALUATE-FOR baseline methods. benchmark datasets EVALUATE-FOR framework. benchmark datasets EVALUATE-FOR baseline methods. Generic is it. OtherScientificTerm are IID ( independent and identically distributed ) data, and data distributions. Material are differentially private synthetic data, and global dataset. Task is global aggregation. Method is local models. ","This paper proposes a new privacy-preserving collaborative learning paradigm, called Federated learning (FL) that is differentially private synthetic data, and it addresses the non-IID issue. The framework is based on the observation that IID (independent and identically distributed) data is not always IID. To address this issue, the authors propose a new framework, called Differentially Private Federated Generative Adversarial Networks (DPGANs), which learns a local generative adversarial network (GAN) to generate synthetic data from a global shared synthetic dataset. A parameter server (PS) aggregates the global shared dataset to a local shared dataset, and then a local GAN is trained on the global dataset. The PS is trained with pseudo labeling from a confident threshold, which is used to train a local model on the local private dataset and a labeled synthetic dataset from the PS. Local GANs are trained with differential privacy by adding artificial noise to the local model gradients. The authors also propose a global aggregation, where the local models are trained on both the local and the global private dataset. Experiments show that the proposed framework outperforms baseline methods on several benchmark datasets in both supervised and semi-supervised settings. ","This paper proposes a new privacy-preserving collaborative learning paradigm, called Federated learning (FL) that is differentially private synthetic data, and it addresses the non-IID issue. The framework is based on the observation that IID (independent and identically distributed) data is not always IID. To address this issue, the authors propose a new framework, called Differentially Private Federated Generative Adversarial Networks (DPGANs), which learns a local generative adversarial network (GAN) to generate synthetic data from a global shared synthetic dataset. A parameter server (PS) aggregates the global shared dataset to a local shared dataset, and then a local GAN is trained on the global dataset. The PS is trained with pseudo labeling from a confident threshold, which is used to train a local model on the local private dataset and a labeled synthetic dataset from the PS. Local GANs are trained with differential privacy by adding artificial noise to the local model gradients. The authors also propose a global aggregation, where the local models are trained on both the local and the global private dataset. Experiments show that the proposed framework outperforms baseline methods on several benchmark datasets in both supervised and semi-supervised settings. "
4229,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"classifier USED-FOR classifier. Gaussian noise USED-FOR classifier. robustness EVALUATE-FOR classifier. accuracy CONJUNCTION ( adversarial ) robustness. ( adversarial ) robustness CONJUNCTION accuracy. training method USED-FOR smoothed classifiers. sample - wise control of robustness USED-FOR training method. robustness CONJUNCTION prediction confidence. prediction confidence CONJUNCTION robustness. robustness EVALUATE-FOR smoothed classifiers. prediction confidence FEATURE-OF smoothed classifiers. certified robustness EVALUATE-FOR training methods. method COMPARE training methods. training methods COMPARE method. certified robustness EVALUATE-FOR method. OtherScientificTerm are ` 2 - adversarial perturbations, noise, adversarial robustness, training objective, and worst - case ( adversarial ) objective. Method is randomized smoothing. Generic is control. ","This paper studies the problem of training a classifier with Gaussian noise under `2-adversarial perturbations' (i.e., when the classifier is trained with the same robustness as the original classifier). The authors propose a training method to improve the robustness of smoothed classifiers. The training method is based on sample-wise control of robustness, i.e. the training objective is to minimize the difference between the true robustness and the worst-case (adversarially) robustness under the noise. The authors show that randomized smoothing can be used to achieve this goal. They also show that this control can be extended to the case that the noise is generated from the worst case. The paper also shows that the training method can be applied to any training objective that minimizes robustness to adversarial robustness. Finally, the paper shows that this method achieves better certified robustness compared to other training methods. ","This paper studies the problem of training a classifier with Gaussian noise under `2-adversarial perturbations' (i.e., when the classifier is trained with the same robustness as the original classifier). The authors propose a training method to improve the robustness of smoothed classifiers. The training method is based on sample-wise control of robustness, i.e. the training objective is to minimize the difference between the true robustness and the worst-case (adversarially) robustness under the noise. The authors show that randomized smoothing can be used to achieve this goal. They also show that this control can be extended to the case that the noise is generated from the worst case. The paper also shows that the training method can be applied to any training objective that minimizes robustness to adversarial robustness. Finally, the paper shows that this method achieves better certified robustness compared to other training methods. "
4245,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"Wikipedia dataset USED-FOR pretraining BERT. histogram of sequence lengths USED-FOR packing. linear complexity EVALUATE-FOR algorithms. packing order FEATURE-OF Wikipedia dataset. model USED-FOR dataset. OtherScientificTerm are padding tokens, padding, near optimal packing, and 2x speed - up. Method is packing algorithms. Material is packed dataset. Metric is convergence. ","This paper studies the problem of pretraining BERT on the Wikipedia dataset with padding tokens. The authors propose two packing algorithms, one based on a histogram of sequence lengths, and the other based on the packing order. The algorithms are shown to have linear complexity with respect to the number of padding tokens, which implies that the size of the packing is not too large. They also show that the near optimal packing results in a 2x speed-up.    The authors also propose a new model for the dataset, which they call ""Packed BERT"". The idea is to train a model on a packed dataset, and then use the model to predict the order in which the packing tokens should be placed in the dataset. The packing order of a Wikipedia dataset is known to be highly correlated with the order of the padding tokens and the authors show that if the packing happens in a particular order, then the algorithm will converge to the optimal packing. However, if the order is not known, the algorithms will not converge.","This paper studies the problem of pretraining BERT on the Wikipedia dataset with padding tokens. The authors propose two packing algorithms, one based on a histogram of sequence lengths, and the other based on the packing order. The algorithms are shown to have linear complexity with respect to the number of padding tokens, which implies that the size of the packing is not too large. They also show that the near optimal packing results in a 2x speed-up.    The authors also propose a new model for the dataset, which they call ""Packed BERT"". The idea is to train a model on a packed dataset, and then use the model to predict the order in which the packing tokens should be placed in the dataset. The packing order of a Wikipedia dataset is known to be highly correlated with the order of the padding tokens and the authors show that if the packing happens in a particular order, then the algorithm will converge to the optimal packing. However, if the order is not known, the algorithms will not converge."
4261,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,adaptive tree search algorithm USED-FOR high - scoring outputs. adaptive tree search algorithm HYPONYM-OF Monte Carlo tree search. translation models USED-FOR high - scoring outputs. algorithm USED-FOR models. autoregressivity CONJUNCTION conditional independence assumptions. conditional independence assumptions CONJUNCTION autoregressivity. algorithm COMPARE beam search. beam search COMPARE algorithm. decoding bias USED-FOR autoregressive models. algorithm USED-FOR autoregressive models. adaptive tree search algorithm COMPARE beam search. beam search COMPARE adaptive tree search algorithm. reranking techniques USED-FOR models. beam search USED-FOR autoregressive models. adaptive tree search algorithm COMPARE reranking techniques. reranking techniques COMPARE adaptive tree search algorithm. model scores EVALUATE-FOR adaptive tree search algorithm. BLEU EVALUATE-FOR translation model objectives. noisy channel model CONJUNCTION objective. objective CONJUNCTION noisy channel model. autoregressive models CONJUNCTION noisy channel model. noisy channel model CONJUNCTION autoregressive models. expected automatic metric scores CONJUNCTION noisy channel model. noisy channel model CONJUNCTION expected automatic metric scores. autoregressive models USED-FOR expected automatic metric scores. decoder USED-FOR search. objective HYPONYM-OF models. autoregressive models HYPONYM-OF models. beam search bias USED-FOR models. noisy channel model HYPONYM-OF models. search USED-FOR models. beam search CONJUNCTION reranking based methods. reranking based methods CONJUNCTION beam search. OtherScientificTerm is search objective. Task is decoding. Generic is objectives. ,"This paper proposes an adaptive tree search algorithm, which is a variant of Monte Carlo tree search, to find high-scoring outputs from translation models. The authors show that the algorithm can be used to train models with autoregressivity and conditional independence assumptions. The algorithm is shown to outperform beam search and reranking techniques for training autoregressive models.  The authors also show that their algorithm outperforms beam search in terms of model scores, and that the decoding bias of autore progressive models is not the same as beam search, but that the search objective is biased towards the decoder.   The paper also shows that the BLEU of the translation model objectives is a function of the number of times the decoding is performed, and the search is biased in favor of models that have a high decoding bias (e.g. expected automatic metric scores, a noisy channel model, and an objective).   Finally, the paper shows that for models with a beam search bias, the search can be biased in favour of models with low decoding bias, but not for models that do not have a search bias.  This is an interesting finding, but the paper is not very well-written, and it is not clear to me that this is a novel contribution to the field. The paper is clearly written and well-motivated, but there is a lot of missing information in the paper, which makes it difficult to understand the contributions of the paper.  In particular, the authors do not provide any theoretical analysis of their search objective, and do not discuss the relationship between their search and beam search or reranking based methods. They also do not show that adaptive tree tree search improves the model scores.  It is also not clear that their search algorithm is able to find the best decoder for a given model, which could be a limitation of beam search.  They do not compare their search with beam search to other reranking methods, and they do not evaluate the performance of their algorithm on a variety of models, including autoregression models, noisy channel models, and autorespectral models.","This paper proposes an adaptive tree search algorithm, which is a variant of Monte Carlo tree search, to find high-scoring outputs from translation models. The authors show that the algorithm can be used to train models with autoregressivity and conditional independence assumptions. The algorithm is shown to outperform beam search and reranking techniques for training autoregressive models.  The authors also show that their algorithm outperforms beam search in terms of model scores, and that the decoding bias of autore progressive models is not the same as beam search, but that the search objective is biased towards the decoder.   The paper also shows that the BLEU of the translation model objectives is a function of the number of times the decoding is performed, and the search is biased in favor of models that have a high decoding bias (e.g. expected automatic metric scores, a noisy channel model, and an objective).   Finally, the paper shows that for models with a beam search bias, the search can be biased in favour of models with low decoding bias, but not for models that do not have a search bias.  This is an interesting finding, but the paper is not very well-written, and it is not clear to me that this is a novel contribution to the field. The paper is clearly written and well-motivated, but there is a lot of missing information in the paper, which makes it difficult to understand the contributions of the paper.  In particular, the authors do not provide any theoretical analysis of their search objective, and do not discuss the relationship between their search and beam search or reranking based methods. They also do not show that adaptive tree tree search improves the model scores.  It is also not clear that their search algorithm is able to find the best decoder for a given model, which could be a limitation of beam search.  They do not compare their search with beam search to other reranking methods, and they do not evaluate the performance of their algorithm on a variety of models, including autoregression models, noisy channel models, and autorespectral models."
4277,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"normal distribution FEATURE-OF task. Energy Based Model ( EBM ) USED-FOR intractability of abnormal distribution. iterative optimization procedure USED-FOR Langevin Dynamics ( LD ). Langevin Dynamics ( LD ) USED-FOR EBM. iterative optimization procedure USED-FOR EBM. anomaly detector USED-FOR task. adaptive sparse coding layer USED-FOR anomaly detector. OtherScientificTerm are anomaly, normal population, plug and play feature, and sparse coding layer. Method are AI solutions, EBMs, and meta learning scheme. ","This paper proposes an Energy Based Model (EBM) to address the intractability of abnormal distribution in the normal distribution of a task. The authors propose an iterative optimization procedure based on Langevin Dynamics (LD) to learn an EBM that is robust to anomalies in the data. The anomaly detector for the task is trained with an adaptive sparse coding layer, where the anomaly is sampled from the normal population and the plug and play feature is used as an input to the EBM. The paper also proposes a meta learning scheme, where EBMs are used to learn a meta-learning algorithm that can be applied to other AI solutions. Experiments show the effectiveness of the proposed method. ","This paper proposes an Energy Based Model (EBM) to address the intractability of abnormal distribution in the normal distribution of a task. The authors propose an iterative optimization procedure based on Langevin Dynamics (LD) to learn an EBM that is robust to anomalies in the data. The anomaly detector for the task is trained with an adaptive sparse coding layer, where the anomaly is sampled from the normal population and the plug and play feature is used as an input to the EBM. The paper also proposes a meta learning scheme, where EBMs are used to learn a meta-learning algorithm that can be applied to other AI solutions. Experiments show the effectiveness of the proposed method. "
4293,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,propaganda CONJUNCTION news. news CONJUNCTION propaganda. news CONJUNCTION social media. social media CONJUNCTION news. QA systems USED-FOR misinformation. misinformation FEATURE-OF QA models. large - scale dataset USED-FOR problem. CONTRAQA HYPONYM-OF large - scale dataset. contradicting contexts USED-FOR QA models. question answering CONJUNCTION misinformation detection. misinformation detection CONJUNCTION question answering. counter - measure USED-FOR misinformation - aware QA system. misinformation detection PART-OF counter - measure. question answering PART-OF counter - measure. misinformation detection PART-OF misinformation - aware QA system. Method is QA model. OtherScientificTerm is real and fake information. ,"This paper studies the problem of misinformation in QA systems. The authors propose a new large-scale dataset, CONTRAQA, to study this problem. The paper proposes a counter-measure that combines question answering and misinformation detection in a misinformation-aware QA system. The counter measure is based on the fact that a QA model can be trained to distinguish between real and fake information, and the authors show that QA models are susceptible to misinformation in the presence of contradicting contexts (propaganda and news). ","This paper studies the problem of misinformation in QA systems. The authors propose a new large-scale dataset, CONTRAQA, to study this problem. The paper proposes a counter-measure that combines question answering and misinformation detection in a misinformation-aware QA system. The counter measure is based on the fact that a QA model can be trained to distinguish between real and fake information, and the authors show that QA models are susceptible to misinformation in the presence of contradicting contexts (propaganda and news). "
4309,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"embodiment CONJUNCTION morphology. morphology CONJUNCTION embodiment. expert demonstrations USED-FOR imitation agent. embodiment USED-FOR imitation agent. morphology USED-FOR imitation agent. method USED-FOR cross - domain imitation. GromovWasserstein distance USED-FOR method. GWIL USED-FOR optimality. rigid transformation of the expert domain CONJUNCTION arbitrary transformation of the state - action space. arbitrary transformation of the state - action space CONJUNCTION rigid transformation of the expert domain. GWIL USED-FOR continuous control domains. Task is Cross - domain imitation learning. OtherScientificTerm is stationary distributions. Generic are they, and theory. Method is Gromov - Wasserstein Imitation Learning ( GWIL ). ","This paper considers the problem of cross-domain imitation learning, where an imitation agent is trained on expert demonstrations conditioned on both the agent’s embodiment and morphology, and the goal is to learn a good imitation agent that can learn to imitate the behavior of an expert agent in a different domain. Cross-domain imitiation learning has been a hot topic in the recent years, but the authors point out that existing methods are limited to stationary distributions, and that they do not generalize well to new domains. To address this problem, the authors propose Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domains imitation based on the GomovWassersteier distance. GWIL is shown to achieve optimality in the case of rigid transformation of the expert domain, and arbitrary transformation in the state-action space. The authors also show that GWIL can be applied to continuous control domains, which is a nice contribution to the theory.","This paper considers the problem of cross-domain imitation learning, where an imitation agent is trained on expert demonstrations conditioned on both the agent’s embodiment and morphology, and the goal is to learn a good imitation agent that can learn to imitate the behavior of an expert agent in a different domain. Cross-domain imitiation learning has been a hot topic in the recent years, but the authors point out that existing methods are limited to stationary distributions, and that they do not generalize well to new domains. To address this problem, the authors propose Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domains imitation based on the GomovWassersteier distance. GWIL is shown to achieve optimality in the case of rigid transformation of the expert domain, and arbitrary transformation in the state-action space. The authors also show that GWIL can be applied to continuous control domains, which is a nice contribution to the theory."
4325,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"self - supervised learning ( SSL ) USED-FOR computer vision. labeling cost FEATURE-OF computer vision. labeling cost EVALUATE-FOR self - supervised learning ( SSL ). SSL USED-FOR invariant visual representations. contrastive loss EVALUATE-FOR representation invariant. hidden layer PART-OF projection head. hierarchical projection head USED-FOR raw representations of the backbone. hierarchical projection head USED-FOR HCCL. cross - level contrastive learning USED-FOR HCCL. generalization ability EVALUATE-FOR visual representations. generalization ability EVALUATE-FOR HCCL. HCCL USED-FOR SSL frameworks. detection CONJUNCTION segmentation. segmentation CONJUNCTION detection. classification CONJUNCTION detection. detection CONJUNCTION classification. segmentation CONJUNCTION few - shot learning tasks. few - shot learning tasks CONJUNCTION segmentation. HCCL USED-FOR detection. HCCL USED-FOR segmentation. few - shot learning tasks EVALUATE-FOR HCCL. classification EVALUATE-FOR HCCL. HCCL COMPARE methods. methods COMPARE HCCL. benchmark datasets EVALUATE-FOR HCCL. benchmark datasets EVALUATE-FOR methods. Method are SSL methods, and Hierarchical Cross Contrastive Learning(HCCL ). Generic is approach. OtherScientificTerm are latent spaces, and latent features. ","This paper proposes a new self-supervised learning (SSL) method called Hierarchical Cross Contrastive Learning (HCCL) to reduce the labeling cost in computer vision. The key idea of HCCL is to learn invariant visual representations that are invariant to the contrastive loss. Previous SSL methods have shown that the representation invariance can be achieved by learning a hierarchical projection head over the raw representations of the backbone, where each hidden layer of the projection head corresponds to a different representation of the latent spaces. The authors propose to use cross-level contrastive learning to learn a representation that is invariant across layers. They show that the proposed approach can be applied to any SSL framework, and that the generalization ability of such visual representations can be improved. They also show that HccL outperforms existing SSL frameworks on classification, detection, segmentation, and few-shot learning tasks.    The paper also shows that the performance of the proposed method is comparable to the state-of-the-art methods on several benchmark datasets.  The authors also provide an ablation study that shows the effect of the choice of latent features. ","This paper proposes a new self-supervised learning (SSL) method called Hierarchical Cross Contrastive Learning (HCCL) to reduce the labeling cost in computer vision. The key idea of HCCL is to learn invariant visual representations that are invariant to the contrastive loss. Previous SSL methods have shown that the representation invariance can be achieved by learning a hierarchical projection head over the raw representations of the backbone, where each hidden layer of the projection head corresponds to a different representation of the latent spaces. The authors propose to use cross-level contrastive learning to learn a representation that is invariant across layers. They show that the proposed approach can be applied to any SSL framework, and that the generalization ability of such visual representations can be improved. They also show that HccL outperforms existing SSL frameworks on classification, detection, segmentation, and few-shot learning tasks.    The paper also shows that the performance of the proposed method is comparable to the state-of-the-art methods on several benchmark datasets.  The authors also provide an ablation study that shows the effect of the choice of latent features. "
4341,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"Real economies HYPONYM-OF sequential imperfect - information game. heterogeneous, interacting strategic agents PART-OF sequential imperfect - information game. Dynamic general equilibrium models USED-FOR economic activity. Dynamic general equilibrium models USED-FOR interactions. economic activity CONJUNCTION interactions. interactions CONJUNCTION economic activity. Dynamic general equilibrium models USED-FOR systems. analytical and computational methods USED-FOR explicit equilibria. structured learning curricula CONJUNCTION GPU - only simulation and training. GPU - only simulation and training CONJUNCTION structured learning curricula. market clearing HYPONYM-OF unrealistic assumptions. GPU implementation USED-FOR training and analyzing economies. real - business - cycle models HYPONYM-OF DGE models. approach USED-FOR real - business - cycle models. RL policies CONJUNCTION economic intuitions. economic intuitions CONJUNCTION RL policies. meta - game -Nash equilibria PART-OF open RBC models. approximate best - response analyses USED-FOR meta - game -Nash equilibria. Method are joint learning, and meta - game. OtherScientificTerm are reward function, consumer ’s expendable income, -Nash equilibria, analytical tractability, and worker - consumers. Task is economic simulations. ","Real economies are a sequential imperfect-information game with heterogeneous, interacting strategic agents. Dynamic general equilibrium models are used to model these systems, which can be seen as a meta-game where the reward function is a function of the consumer’s expendable income and the agents’ ability to maximize their own returns.    The paper proposes a new class of systems called “Dynamic General Equilibrium models” (DGE) that can model both economic activity and interactions between agents.  The authors propose two analytical and computational methods for computing explicit equilibria, which are based on joint learning and Nash equilibrium.  They show that under certain assumptions (e.g. market clearing, unrealistic assumptions such as market clearing), DGE models are able to capture the dynamics of the system.  In addition, the authors show that the approach can be used to train real-business-cycle models, i.e. real-Business Cycle Models (RBC), which are real-world real-economy models that can be trained with a GPU implementation for training and analyzing economies.  Finally, they show that RL policies and economic intuitions can be integrated with open RBC models, and that meta-Game-Nash equilibriums can be found using approximate best-response analyses.  This is an interesting and important contribution of the paper.  However, the paper suffers from a lack of analytical tractability, which makes it difficult to compare with state-of-the-art work on economic simulations. The paper also suffers from the lack of structured learning curricula and GPU-only simulation and training, which is a limitation of the authors claim to address. ","Real economies are a sequential imperfect-information game with heterogeneous, interacting strategic agents. Dynamic general equilibrium models are used to model these systems, which can be seen as a meta-game where the reward function is a function of the consumer’s expendable income and the agents’ ability to maximize their own returns.    The paper proposes a new class of systems called “Dynamic General Equilibrium models” (DGE) that can model both economic activity and interactions between agents.  The authors propose two analytical and computational methods for computing explicit equilibria, which are based on joint learning and Nash equilibrium.  They show that under certain assumptions (e.g. market clearing, unrealistic assumptions such as market clearing), DGE models are able to capture the dynamics of the system.  In addition, the authors show that the approach can be used to train real-business-cycle models, i.e. real-Business Cycle Models (RBC), which are real-world real-economy models that can be trained with a GPU implementation for training and analyzing economies.  Finally, they show that RL policies and economic intuitions can be integrated with open RBC models, and that meta-Game-Nash equilibriums can be found using approximate best-response analyses.  This is an interesting and important contribution of the paper.  However, the paper suffers from a lack of analytical tractability, which makes it difficult to compare with state-of-the-art work on economic simulations. The paper also suffers from the lack of structured learning curricula and GPU-only simulation and training, which is a limitation of the authors claim to address. "
4357,SP:f885c992df9c685f806a653398736432ba38bd80,public API USED-FOR machine learning model. robustness CONJUNCTION model utility. model utility CONJUNCTION robustness. defenses USED-FOR model stealing. query access USED-FOR model extraction. differential privacy USED-FOR calibration. victim model USED-FOR method. Task is model extraction attacks. Generic is model. Metric is computational effort. OtherScientificTerm is proof - of - work. Method is machine learning practitioners. ,"This paper studies the problem of model extraction attacks, where a machine learning model is queried from a public API, and the goal is to steal the model. The authors propose two defenses against model stealing: (1) protect the robustness and model utility of the model, and (2) calibrate the model to be more robust to model stealing attacks.  The authors show that existing defenses are ineffective against model extraction when query access to the model is available, and propose a new method based on a victim model. They also show that the method is robust to the computational effort required to compute the proof-of-work, and that the proposed method can be applied to any machine learning practitioners.  They also propose to use differential privacy for calibration and show that their method is more robust than existing defenses.  ","This paper studies the problem of model extraction attacks, where a machine learning model is queried from a public API, and the goal is to steal the model. The authors propose two defenses against model stealing: (1) protect the robustness and model utility of the model, and (2) calibrate the model to be more robust to model stealing attacks.  The authors show that existing defenses are ineffective against model extraction when query access to the model is available, and propose a new method based on a victim model. They also show that the method is robust to the computational effort required to compute the proof-of-work, and that the proposed method can be applied to any machine learning practitioners.  They also propose to use differential privacy for calibration and show that their method is more robust than existing defenses.  "
4373,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"Neural Ordinary Differential Equations ( ODEs ) USED-FOR generative models of images. Continuous Normalizing Flows ( CNFs ) USED-FOR generative models of images. models USED-FOR exact likelihood calculation. exact likelihood calculation CONJUNCTION invertible generation / density estimation. invertible generation / density estimation CONJUNCTION exact likelihood calculation. models USED-FOR invertible generation / density estimation. approach COMPARE prior methods. prior methods COMPARE approach. likelihood values FEATURE-OF image datasets. likelihood values EVALUATE-FOR approach. GPU USED-FOR prior methods. image datasets EVALUATE-FOR approach. training time EVALUATE-FOR prior methods. Method is MRCNF ). OtherScientificTerm are conditional distribution, fine image, coarse image, and log likelihood. ","This paper proposes to use Neural Ordinary Differential Equations (ODEs) to train generative models of images based on Continuous Normalizing Flows (CNFs). The authors propose two models for exact likelihood calculation and invertible generation/density estimation. The proposed approach (MRCNF) is based on the observation that the conditional distribution of a fine image can be decomposed into two parts: (1) a coarse image, and (2) a continuous distribution. The authors show that their approach outperforms prior methods in terms of training time and training time on a GPU. They also show that the proposed approach can improve the likelihood values of image datasets. ","This paper proposes to use Neural Ordinary Differential Equations (ODEs) to train generative models of images based on Continuous Normalizing Flows (CNFs). The authors propose two models for exact likelihood calculation and invertible generation/density estimation. The proposed approach (MRCNF) is based on the observation that the conditional distribution of a fine image can be decomposed into two parts: (1) a coarse image, and (2) a continuous distribution. The authors show that their approach outperforms prior methods in terms of training time and training time on a GPU. They also show that the proposed approach can improve the likelihood values of image datasets. "
4389,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"Label noise PART-OF real - world datasets. robust training techniques USED-FOR DNNs. DNNs USED-FOR corrupted patterns. overfitting USED-FOR corrupted patterns. noisy supervisions USED-FOR model. and training - free solution USED-FOR detect noisy labels. neighborhood information USED-FOR methods. nearby representations USED-FOR local voting. noisy label consensuses USED-FOR local voting. one HYPONYM-OF methods. local voting USED-FOR one. ranking - based approach USED-FOR one. representations USED-FOR ranking - based approach. worst - case error bound EVALUATE-FOR ranking - based method. training - free solutions COMPARE training - based baselines. training - based baselines COMPARE training - free solutions. synthetic and real - world label noise EVALUATE-FOR training - free solutions. synthetic and real - world label noise EVALUATE-FOR training - based baselines. Method is generalization of deep neural networks ( DNNs ). Generic is approach. OtherScientificTerm are noisy labels, and clean label. ","Label noise is a common problem in real-world datasets, but it is also present in the generalization of deep neural networks (DNNs). In this paper, the authors consider the problem of robust training techniques for DNNs to detect corrupted patterns caused by overfitting to noisy labels. The approach is to train a model with noisy supervisions and then use a training-free solution to detect noisy labels in the presence of a clean label. Two methods are proposed based on neighborhood information, one based on local voting based on nearby representations and the other based on a ranking-based approach based on representations. The authors provide a worst-case error bound for the first one, and show that the second one achieves a similar result. The paper also shows that the two training-based solutions are more robust than training -based baselines under both synthetic and real world label noise. ","Label noise is a common problem in real-world datasets, but it is also present in the generalization of deep neural networks (DNNs). In this paper, the authors consider the problem of robust training techniques for DNNs to detect corrupted patterns caused by overfitting to noisy labels. The approach is to train a model with noisy supervisions and then use a training-free solution to detect noisy labels in the presence of a clean label. Two methods are proposed based on neighborhood information, one based on local voting based on nearby representations and the other based on a ranking-based approach based on representations. The authors provide a worst-case error bound for the first one, and show that the second one achieves a similar result. The paper also shows that the two training-based solutions are more robust than training -based baselines under both synthetic and real world label noise. "
4405,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"robustness EVALUATE-FOR RL agents. state observations USED-FOR strongest / optimal adversarial perturbations. strongest / optimal adversarial perturbations USED-FOR reinforcement learning ( RL ) agent. Existing works USED-FOR adversarial RL. heuristics - based methods USED-FOR Existing works. attacking method USED-FOR optimal attacks. designed function CONJUNCTION RL - based learner. RL - based learner CONJUNCTION designed function. algorithm COMPARE RL - based works. RL - based works COMPARE algorithm. PA - AD COMPARE RL - based works. RL - based works COMPARE PA - AD. PA - AD HYPONYM-OF algorithm. PA - AD COMPARE attacking methods. attacking methods COMPARE PA - AD. attacking methods USED-FOR Atari and MuJoCo environments. PA - AD USED-FOR Atari and MuJoCo environments. PA - AD USED-FOR adversarial training. empirical robustness EVALUATE-FOR PA - AD. OtherScientificTerm are optimal adversary, optimal attack, agent, large state space, policy perturbation direction, policy perturbation directions, large state spaces, and strong adversaries. Method is RL - based adversary. ","This paper studies the robustness of RL agents to adversarial attacks. The authors propose a new algorithm called PA-AD, which is based on the observation that the optimal adversary can be found in a large state space. They show that the strongest/optimal adversarial perturbations from state observations can be used to fool a reinforcement learning (RL) agent. Existing works for adversarial RL are based on heuristics-based methods.    The authors show that an optimal attack can be obtained by perturbing the state space of an agent that is trained on the same environment as the optimal attack, but with a different policy perturbation direction. They also provide an attacking method that can be applied to any optimal attacks.  The key idea of the algorithm is to train an RL-based adversary that is able to fool an agent trained on a given environment. The algorithm is a simple combination of a designed function and a RL -based learner. The agent is trained in a way that the RL learner is trained to be able to learn a policy that is robust to the optimal attacks, and the RL-learner can be trained to fool the RL adversary.  In order to achieve this goal, the authors propose an algorithm that is a generalization of a previous algorithm (PA-AD) that was proposed in [1]. The authors compare the proposed algorithm with RL - based works, and show that PA - AD outperforms RL- based works in terms of empirical robustness.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11]","This paper studies the robustness of RL agents to adversarial attacks. The authors propose a new algorithm called PA-AD, which is based on the observation that the optimal adversary can be found in a large state space. They show that the strongest/optimal adversarial perturbations from state observations can be used to fool a reinforcement learning (RL) agent. Existing works for adversarial RL are based on heuristics-based methods.    The authors show that an optimal attack can be obtained by perturbing the state space of an agent that is trained on the same environment as the optimal attack, but with a different policy perturbation direction. They also provide an attacking method that can be applied to any optimal attacks.  The key idea of the algorithm is to train an RL-based adversary that is able to fool an agent trained on a given environment. The algorithm is a simple combination of a designed function and a RL -based learner. The agent is trained in a way that the RL learner is trained to be able to learn a policy that is robust to the optimal attacks, and the RL-learner can be trained to fool the RL adversary.  In order to achieve this goal, the authors propose an algorithm that is a generalization of a previous algorithm (PA-AD) that was proposed in [1]. The authors compare the proposed algorithm with RL - based works, and show that PA - AD outperforms RL- based works in terms of empirical robustness.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11]"
4421,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"cumulative reward FEATURE-OF monotonous policies. diversity CONJUNCTION novelty. novelty CONJUNCTION diversity. novelty FEATURE-OF policies. diversity FEATURE-OF policies. policy generation workflow USED-FOR diverse and well - performing policies. novelty metric USED-FOR novelty - seeking problem. behavioral novelty FEATURE-OF multi - objective optimization approaches. constrained optimization literature FEATURE-OF interior point method. interior point method USED-FOR Interior Policy Differentiation ( IPD ). Interior Policy Differentiation ( IPD ) HYPONYM-OF policy seeking algorithm. constrained optimization USED-FOR novelty - seeking problem. IPD COMPARE novelty - seeking methods. novelty - seeking methods COMPARE IPD. benchmark environments EVALUATE-FOR IPD. benchmark environments EVALUATE-FOR novelty - seeking methods. Task is problem - solving. Generic are problem, and metric. Method are reinforcement learning algorithms, and learning algorithms. OtherScientificTerm is novelty of generated policies. ","This paper studies the problem-solving problem of learning monotonous policies with a cumulative reward. In this problem, the goal is to learn a policy that maximizes the cumulative reward of a set of policies that satisfy certain criteria (diversity, novelty, etc.). This problem is challenging for many reinforcement learning algorithms, as the novelty of generated policies can be highly correlated with the diversity and novelty of the policies. This paper proposes a novel policy seeking algorithm called Interior Policy Differentiation (IPD), which is an extension of the policy generation workflow that aims to learn diverse and well-performing policies. The novelty-seeking problem is formulated as a constrained optimization, and the novelty metric is used to solve the novelty-finding problem. The authors show that multi-objective optimization approaches that rely on behavioral novelty are more likely to find policies that achieve high performance. They also show that the interior point method in the constrained optimization literature is a variant of the proposed interior policy differentiation (IPD). The authors evaluate IPD on several benchmark environments, and show that IPD outperforms existing novelty-sourcing methods.  ","This paper studies the problem-solving problem of learning monotonous policies with a cumulative reward. In this problem, the goal is to learn a policy that maximizes the cumulative reward of a set of policies that satisfy certain criteria (diversity, novelty, etc.). This problem is challenging for many reinforcement learning algorithms, as the novelty of generated policies can be highly correlated with the diversity and novelty of the policies. This paper proposes a novel policy seeking algorithm called Interior Policy Differentiation (IPD), which is an extension of the policy generation workflow that aims to learn diverse and well-performing policies. The novelty-seeking problem is formulated as a constrained optimization, and the novelty metric is used to solve the novelty-finding problem. The authors show that multi-objective optimization approaches that rely on behavioral novelty are more likely to find policies that achieve high performance. They also show that the interior point method in the constrained optimization literature is a variant of the proposed interior policy differentiation (IPD). The authors evaluate IPD on several benchmark environments, and show that IPD outperforms existing novelty-sourcing methods.  "
4437,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"accuracy EVALUATE-FOR automatic speech recognition. quality of speech USED-FOR human perception. Reverberation FEATURE-OF audio reflecting off surfaces. audio modality USED-FOR reverberation. reverberation effects FEATURE-OF audio stream. real - world 3D scans of homes FEATURE-OF realistic acoustic renderings of speech. large - scale dataset EVALUATE-FOR task. realistic acoustic renderings of speech USED-FOR large - scale dataset. speech enhancement CONJUNCTION speech recognition. speech recognition CONJUNCTION speech enhancement. speech recognition CONJUNCTION speaker identification. speaker identification CONJUNCTION speech recognition. it COMPARE audio - only methods. audio - only methods COMPARE it. simulated and real imagery USED-FOR speech enhancement. approach USED-FOR speech enhancement. approach USED-FOR speech recognition. simulated and real imagery USED-FOR approach. OtherScientificTerm are audio - visual observations, visual environment, room geometry, speaker location, visual scene, and room acoustics. Method is end - to - end approach. ","This paper proposes an end-to-end approach to improve the accuracy of automatic speech recognition. The key idea is that the quality of speech is important for human perception, and the reverberation of audio reflecting off surfaces in a room is an important component of audio-visual observations. Reverberation in the audio modality can be caused by reverberation in a visual environment. The authors propose to use the room geometry, speaker location, and reverberation effects of the audio stream as input to a neural network, which is trained to reconstruct the audio from the room acoustics. The task is evaluated on a large-scale dataset with realistic acoustic renderings of speech from real-world 3D scans of homes. The approach is able to leverage both simulated and real imagery for speech enhancement, speech recognition, and speaker identification, and it outperforms audio-only methods. ","This paper proposes an end-to-end approach to improve the accuracy of automatic speech recognition. The key idea is that the quality of speech is important for human perception, and the reverberation of audio reflecting off surfaces in a room is an important component of audio-visual observations. Reverberation in the audio modality can be caused by reverberation in a visual environment. The authors propose to use the room geometry, speaker location, and reverberation effects of the audio stream as input to a neural network, which is trained to reconstruct the audio from the room acoustics. The task is evaluated on a large-scale dataset with realistic acoustic renderings of speech from real-world 3D scans of homes. The approach is able to leverage both simulated and real imagery for speech enhancement, speech recognition, and speaker identification, and it outperforms audio-only methods. "
4453,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"model USED-FOR extrapolation. methods USED-FOR extrapolation. position representation method USED-FOR extrapolation. Attention with Linear Biases ( ALiBi ) HYPONYM-OF position method. it USED-FOR query - key attention scores. positional embeddings USED-FOR word embeddings. perplexity EVALUATE-FOR sinusoidal position embedding model. method COMPARE sinusoidal position embedding model. sinusoidal position embedding model COMPARE method. perplexity EVALUATE-FOR method. it COMPARE position methods. position methods COMPARE it. WikiText-103 benchmark EVALUATE-FOR it. WikiText-103 benchmark EVALUATE-FOR position methods. Method are transformer model, and ALiBi. OtherScientificTerm are memory, and recency. ","This paper proposes a novel position representation method, Attention with Linear Biases (ALiBi), which is a position method that extends previous methods for extrapolation. The proposed model is based on the transformer model. The key idea of ALiBi is that it learns query-key attention scores for each word in the input sequence, and then uses positional embeddings to learn word embedding. The paper shows that the proposed method outperforms the sinusoidal position embedding model in perplexity and memory, and it outperforms other position methods on the WikiText-103 benchmark. The authors also show that ALiBI is more memory-efficient than previous methods, as it does not require recency. ","This paper proposes a novel position representation method, Attention with Linear Biases (ALiBi), which is a position method that extends previous methods for extrapolation. The proposed model is based on the transformer model. The key idea of ALiBi is that it learns query-key attention scores for each word in the input sequence, and then uses positional embeddings to learn word embedding. The paper shows that the proposed method outperforms the sinusoidal position embedding model in perplexity and memory, and it outperforms other position methods on the WikiText-103 benchmark. The authors also show that ALiBI is more memory-efficient than previous methods, as it does not require recency. "
4469,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,unconstrained max - min form FEATURE-OF multi - objective dynamic regret. multi - objective dynamic regret PART-OF Multi - Objective Online Convex Optimization. regret USED-FOR zero - order multi - objective bandit setting. it COMPARE regret. regret COMPARE it. vanilla min - norm solver CONJUNCTION L1 - regularized min - norm solver. L1 - regularized min - norm solver CONJUNCTION vanilla min - norm solver. variants USED-FOR composite gradient. Online Mirror Multiple Descent algorithm USED-FOR composite gradient. variants USED-FOR Online Mirror Multiple Descent algorithm. L1 - regularized min - norm solver USED-FOR composite gradient. vanilla min - norm solver USED-FOR composite gradient. regret bounds FEATURE-OF variants. lower bound FEATURE-OF L1 - regularized variant. Task is multi - objective online learning. Method is first - order gradient - based methods. Generic is algorithm. ,"This paper studies the multi-objective online learning in the setting of multi-observational online learning. In particular, the authors consider Multi-Objective Online Convex Optimization (MOCO) with multi- objective dynamic regret in the unconstrained max-min form, which is a generalization of the zero-order multi-order bandit setting. In this setting, it is known that the regret of the algorithm in this setting is asymptotically optimal. However, the regret in this case is not optimal as it is in the case of first-order gradient-based methods. The authors propose two variants of the Online Mirror Multiple Descent algorithm to compute the composite gradient using a vanilla min-norm solver and an L1-regularized min-net solver, and provide regret bounds for both variants. They also provide a lower bound on the regret for the L1 -regularized variant.   ","This paper studies the multi-objective online learning in the setting of multi-observational online learning. In particular, the authors consider Multi-Objective Online Convex Optimization (MOCO) with multi- objective dynamic regret in the unconstrained max-min form, which is a generalization of the zero-order multi-order bandit setting. In this setting, it is known that the regret of the algorithm in this setting is asymptotically optimal. However, the regret in this case is not optimal as it is in the case of first-order gradient-based methods. The authors propose two variants of the Online Mirror Multiple Descent algorithm to compute the composite gradient using a vanilla min-norm solver and an L1-regularized min-net solver, and provide regret bounds for both variants. They also provide a lower bound on the regret for the L1 -regularized variant.   "
4485,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"intelligence USED-FOR real - world problems. Learning continually PART-OF intelligence. simple scenarios CONJUNCTION low - dimensional benchmarks. low - dimensional benchmarks CONJUNCTION simple scenarios. generative models USED-FOR replay patterns. simplified assumptions USED-FOR it. generative models USED-FOR Generative replay. simple scenarios HYPONYM-OF simplified assumptions. low - dimensional benchmarks HYPONYM-OF simplified assumptions. OtherScientificTerm are catastrophic forgetting, and learning experiences. Method are continual learning, replay approach, and generative replay approaches. Metric is classification accuracy. Generic are they, and approach. Material are ImageNet-1000, and high - dimensional data. ","This paper studies the problem of continual learning. Learning continually in intelligence is an important problem in real-world problems, but there is a lack of research in the area of catastrophic forgetting. In this paper, the authors propose a new replay approach, called generative replay. Generative replay uses generative models to learn replay patterns, and it is based on simplified assumptions, such as simple scenarios and low-dimensional benchmarks. The authors show that the generative model is able to improve classification accuracy on ImageNet-1000, and they also show that they are able to do so even on high-dimensional data. The paper also shows that their approach can be applied to a wide range of different learning experiences.    Overall, the paper is well-written and well-motivated. However, there are a few issues that need to be addressed in order for the paper to be accepted as a new research paper. ","This paper studies the problem of continual learning. Learning continually in intelligence is an important problem in real-world problems, but there is a lack of research in the area of catastrophic forgetting. In this paper, the authors propose a new replay approach, called generative replay. Generative replay uses generative models to learn replay patterns, and it is based on simplified assumptions, such as simple scenarios and low-dimensional benchmarks. The authors show that the generative model is able to improve classification accuracy on ImageNet-1000, and they also show that they are able to do so even on high-dimensional data. The paper also shows that their approach can be applied to a wide range of different learning experiences.    Overall, the paper is well-written and well-motivated. However, there are a few issues that need to be addressed in order for the paper to be accepted as a new research paper. "
4501,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"modularity maximization CONJUNCTION NCut minimization. NCut minimization CONJUNCTION modularity maximization. graph partitioning ( GP ) FEATURE-OF NP - hard combinatorial optimization problems. NP - hard combinatorial optimization problems USED-FOR network systems. NCut minimization HYPONYM-OF NP - hard combinatorial optimization problems. NCut minimization HYPONYM-OF graph partitioning ( GP ). modularity maximization HYPONYM-OF NP - hard combinatorial optimization problems. modularity maximization HYPONYM-OF graph partitioning ( GP ). machine learning techniques USED-FOR Existing methods. heuristic strategies USED-FOR GP methods. inductive graph partitioning ( IGP ) framework USED-FOR NP - hard challenge. transductive GP methods COMPARE inductive graph partitioning ( IGP ) framework. inductive graph partitioning ( IGP ) framework COMPARE transductive GP methods. inductive graph partitioning ( IGP ) framework USED-FOR graphs. dual graph neural network USED-FOR IGP. historical graph snapshots USED-FOR dual graph neural network. model USED-FOR graphs. model USED-FOR online GP. quality CONJUNCTION efficiency. efficiency CONJUNCTION quality. graphs USED-FOR online GP. IGP USED-FOR online GP. IGP HYPONYM-OF framework. graphs USED-FOR online GP. benchmarks EVALUATE-FOR IGP. efficiency EVALUATE-FOR state - of - the - art baselines. IGP COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE IGP. benchmarks EVALUATE-FOR state - of - the - art baselines. efficiency EVALUATE-FOR IGP. OtherScientificTerm are NP - hardness, quality degradation, and optimization. Method is GP. Metric is complexity. Generic is system. ","This paper studies graph partitioning (GP) for NP-hard combinatorial optimization problems such as modularity maximization and NCut minimization for network systems. Existing methods are based on machine learning techniques, and the authors show that existing GP methods suffer from heuristic strategies that do not scale well when the number of nodes is large. The authors propose a novel inductive graph partitionation (IGP) framework to tackle the NP-Hard challenge. Unlike previous transductive GP methods, the authors propose to partition the graphs using a dual graph neural network based on historical graph snapshots. They show that the proposed model is able to partition graphs in an online GP, and that the quality and efficiency of the proposed IGP is comparable to existing state-of-the-art baselines on standard benchmarks.   The main contribution of this paper is to propose a new framework called IGP, which addresses the problem of NP- hardness. The main idea is that the complexity of a GP can be reduced by partitioning the graphs in a way that minimizes the quality of the solution, and thus avoids the quality degradation due to the optimization.  The authors also show that IGP can be used as a model to learn a model for graphs that can be efficiently partitioned. The paper also shows that the IGP framework can be applied to any system that has a large number of graphs. ","This paper studies graph partitioning (GP) for NP-hard combinatorial optimization problems such as modularity maximization and NCut minimization for network systems. Existing methods are based on machine learning techniques, and the authors show that existing GP methods suffer from heuristic strategies that do not scale well when the number of nodes is large. The authors propose a novel inductive graph partitionation (IGP) framework to tackle the NP-Hard challenge. Unlike previous transductive GP methods, the authors propose to partition the graphs using a dual graph neural network based on historical graph snapshots. They show that the proposed model is able to partition graphs in an online GP, and that the quality and efficiency of the proposed IGP is comparable to existing state-of-the-art baselines on standard benchmarks.   The main contribution of this paper is to propose a new framework called IGP, which addresses the problem of NP- hardness. The main idea is that the complexity of a GP can be reduced by partitioning the graphs in a way that minimizes the quality of the solution, and thus avoids the quality degradation due to the optimization.  The authors also show that IGP can be used as a model to learn a model for graphs that can be efficiently partitioned. The paper also shows that the IGP framework can be applied to any system that has a large number of graphs. "
4517,SP:ad28c185efd966eea1f44a6ff474900812b4705a,Multiresolution Equivariant Graph Variational Autoencoders ( MGVAE ) HYPONYM-OF hierarchical generative model. hierarchical generative model USED-FOR graphs. multiresolution and equivariant manner USED-FOR graphs. higher order message passing USED-FOR graph. MGVAE USED-FOR graph. MGVAE USED-FOR resolution level. higher order message passing USED-FOR resolution level. higher order message passing USED-FOR MGVAE. MGVAE USED-FOR hierarchical generative model. hierarchical generative model USED-FOR hierarchy of coarsened graphs. node ordering FEATURE-OF framework. general graph generation CONJUNCTION molecular generation. molecular generation CONJUNCTION general graph generation. unsupervised molecular representation learning USED-FOR molecular properties. molecular generation CONJUNCTION unsupervised molecular representation learning. unsupervised molecular representation learning CONJUNCTION molecular generation. link prediction CONJUNCTION graph - based image generation. graph - based image generation CONJUNCTION link prediction. MGVAE COMPARE generative tasks. generative tasks COMPARE MGVAE. unsupervised molecular representation learning CONJUNCTION link prediction. link prediction CONJUNCTION unsupervised molecular representation learning. citation graphs USED-FOR link prediction. unsupervised molecular representation learning HYPONYM-OF generative tasks. general graph generation HYPONYM-OF generative tasks. graph - based image generation HYPONYM-OF generative tasks. link prediction HYPONYM-OF generative tasks. molecular generation HYPONYM-OF generative tasks. Generic is it. OtherScientificTerm is hierarchy of latent distributions. ,"This paper proposes Multiresolution Equivariant Graph Variational Autoencoders (MGVAE), a hierarchical generative model for graphs that is trained in a multiresolution and equivariant manner. MGVAE uses higher order message passing at each resolution level of the graph to learn a hierarchy of latent distributions. The framework is based on node ordering, and it can be seen as a generalization of the previous work (Zhang et al. 2017). MGVAEs are trained to learn the hierarchy of coarsened graphs, and the authors show that MGVAes can be used to learn general graph generation, molecular generation, unsupervised molecular representation learning for molecular properties, and link prediction on citation graphs. Experiments show that the proposed MGVAe outperforms other generative tasks (general graph generation and graph-based image generation) and outperforms existing generative models. ","This paper proposes Multiresolution Equivariant Graph Variational Autoencoders (MGVAE), a hierarchical generative model for graphs that is trained in a multiresolution and equivariant manner. MGVAE uses higher order message passing at each resolution level of the graph to learn a hierarchy of latent distributions. The framework is based on node ordering, and it can be seen as a generalization of the previous work (Zhang et al. 2017). MGVAEs are trained to learn the hierarchy of coarsened graphs, and the authors show that MGVAes can be used to learn general graph generation, molecular generation, unsupervised molecular representation learning for molecular properties, and link prediction on citation graphs. Experiments show that the proposed MGVAe outperforms other generative tasks (general graph generation and graph-based image generation) and outperforms existing generative models. "
4533,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"Nonlinear ICA HYPONYM-OF machine learning. framework USED-FOR nonlinear ICA. volume - preserving transformation FEATURE-OF mixing function. artificial data CONJUNCTION synthesized images. synthesized images CONJUNCTION artificial data. synthesized images EVALUATE-FOR theory. volume - preserving flow - based models USED-FOR framework. artificial data EVALUATE-FOR theory. framework USED-FOR interpretable features. real - world images EVALUATE-FOR framework. OtherScientificTerm are independent components ( sources ), and temporal structure. Generic are sources, and methods. ","Nonlinear ICA is an important problem in machine learning, where there are multiple independent components (source) and each component has its own temporal structure. This paper proposes a new framework for nonlinear I CA, which is based on volume-preserving flow-based models. The key idea is to learn a mixing function that is invariant to the volume preserving transformation of the mixing function. Experiments on artificial data and synthesized images demonstrate the effectiveness of the theory on both synthetic data and real-world images. The paper also shows that the proposed framework can be used to learn interpretable features that can be applied to unseen sources.   ","Nonlinear ICA is an important problem in machine learning, where there are multiple independent components (source) and each component has its own temporal structure. This paper proposes a new framework for nonlinear I CA, which is based on volume-preserving flow-based models. The key idea is to learn a mixing function that is invariant to the volume preserving transformation of the mixing function. Experiments on artificial data and synthesized images demonstrate the effectiveness of the theory on both synthetic data and real-world images. The paper also shows that the proposed framework can be used to learn interpretable features that can be applied to unseen sources.   "
4549,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"Graph convolutional networks USED-FOR representation learning of networked data. single message passing strategy USED-FOR they. sharing scheme USED-FOR filters. sharing scheme USED-FOR spectral graph convolutional operators. sharing scheme USED-FOR filters. BankGCN HYPONYM-OF graph convolution operator. sharing scheme USED-FOR spectral methods. BankGCN USED-FOR multi - channel signals. adaptive filters USED-FOR BankGCN. graphs USED-FOR multi - channel signals. filters PART-OF filter bank. filters PART-OF subspaces. frequency response FEATURE-OF filters. filter bank CONJUNCTION signal decomposition. signal decomposition CONJUNCTION filter bank. filter bank USED-FOR spectral characteristics. signal decomposition USED-FOR spectral characteristics. spectral characteristics FEATURE-OF graph data. benchmark graph datasets EVALUATE-FOR BankGCN. Method are message passing graph convolutional networks ( MPGCNs ), and MPGCNs. OtherScientificTerm are low - frequency information, graph features, and single ‘ low - pass ’ features. Task is overfitting problems. Generic is compact architecture. ","Graph convolutional networks (GCNs) have been a popular choice for representation learning of networked data. However, they are typically based on a single message passing strategy where low-frequency information is shared across all layers. In this paper, the authors propose a sharing scheme to share the filters among different spectral graph convolution neural operators. Specifically, they propose BankGCN, which is an extension of the message passing Graph Convolutional Networks (MPGCNs).    First, they show that MPGCNs suffer from overfitting problems when the number of filters in the filter bank is large. To address this issue, they introduce a shared pool of filters across different subspaces that share the same frequency response. They also propose a novel sharing scheme for sharing filters between different spectral graphs, which can be used to improve the performance of spectral methods.   Second, they present a new type of Graph convolution operator, called BankGCNs, that is based on the idea of sharing the graph features across different filters. They propose a compact architecture where each filter in a filter bank corresponds to a single ‘low-pass’ features, and each filter is shared among all filters in that filter bank. The authors show that this sharing scheme can be applied to all filters across all filters.  Third, they demonstrate that the proposed bankGCN is able to learn multi-channel signals on graphs with adaptive filters, and that it can be trained on a large number of graphs.  The authors also show that the filters in their filter bank and signal decomposition are able to capture the spectral characteristics of the graph data.  Finally, they evaluate BankGCNN on several benchmark graph datasets, and show that their proposed method, Bank GCN, achieves state-of-the-art performance. ","Graph convolutional networks (GCNs) have been a popular choice for representation learning of networked data. However, they are typically based on a single message passing strategy where low-frequency information is shared across all layers. In this paper, the authors propose a sharing scheme to share the filters among different spectral graph convolution neural operators. Specifically, they propose BankGCN, which is an extension of the message passing Graph Convolutional Networks (MPGCNs).    First, they show that MPGCNs suffer from overfitting problems when the number of filters in the filter bank is large. To address this issue, they introduce a shared pool of filters across different subspaces that share the same frequency response. They also propose a novel sharing scheme for sharing filters between different spectral graphs, which can be used to improve the performance of spectral methods.   Second, they present a new type of Graph convolution operator, called BankGCNs, that is based on the idea of sharing the graph features across different filters. They propose a compact architecture where each filter in a filter bank corresponds to a single ‘low-pass’ features, and each filter is shared among all filters in that filter bank. The authors show that this sharing scheme can be applied to all filters across all filters.  Third, they demonstrate that the proposed bankGCN is able to learn multi-channel signals on graphs with adaptive filters, and that it can be trained on a large number of graphs.  The authors also show that the filters in their filter bank and signal decomposition are able to capture the spectral characteristics of the graph data.  Finally, they evaluate BankGCNN on several benchmark graph datasets, and show that their proposed method, Bank GCN, achieves state-of-the-art performance. "
4565,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"pretrain - finetune paradigm USED-FOR deep learning. model USED-FOR downstream tasks. self - supervised pre - training COMPARE supervised pre - training. supervised pre - training COMPARE self - supervised pre - training. transferability EVALUATE-FOR self - supervised pre - training. supervised methods USED-FOR pre - training stage. supervised pretraining model USED-FOR downstream tasks. transferability EVALUATE-FOR supervised pre - training methods. It USED-FOR overfitting upstream tasks. method USED-FOR large datasets. state - of - the - art methods USED-FOR supervised and self - supervised pre - training. LOOK COMPARE state - of - the - art methods. state - of - the - art methods COMPARE LOOK. LOOK USED-FOR supervised and self - supervised pre - training. multiple downstream tasks EVALUATE-FOR LOOK. Material is ImageNet. OtherScientificTerm are negligence of valuable intra - class semantic difference, visual contents, multi - mode distribution, and intra - class difference. Generic is methods. Task is overfit of upstream tasks. Method is supervised pre - training method. ","This paper proposes a new pretrain-finetune paradigm for deep learning. The authors show that self-supervised pre-training improves the transferability of downstream tasks compared to supervised pre-trains. They also show that supervised pretraining model can be used to improve the performance of the downstream tasks.    The paper is well-written and well-motivated, and the paper is clearly written.  However, there are a few issues with the paper: (1) the authors do not address the issue of overfitting of upstream tasks due to the negligence of valuable intra-class semantic difference, (2) the lack of comparison between supervised methods during the pre-train stage, and (3) the overfit of the model to different downstream tasks is not well-explained.  In addition to these issues, the paper also points out that existing methods do not account for the multi-mode distribution of the visual contents, which is problematic.  The authors also point out that the current supervised pretrained method, Look, does not work well on large datasets. It does not prevent overfitting upstream tasks, and it does not perform well on ImageNet.  Finally, the authors propose a new method, called Look, to address these issues. They show that Look outperforms previous state-of-the-art methods for both supervised and self -supervised pretraining on multiple downstream tasks, showing that Look is able to achieve better transferability compared to previous supervised pre -training methods. The paper also shows that the proposed method can be applied to large datasets with a multi-modal distribution, and shows that it is more robust to overfit to multi-class differences in the dataset. ","This paper proposes a new pretrain-finetune paradigm for deep learning. The authors show that self-supervised pre-training improves the transferability of downstream tasks compared to supervised pre-trains. They also show that supervised pretraining model can be used to improve the performance of the downstream tasks.    The paper is well-written and well-motivated, and the paper is clearly written.  However, there are a few issues with the paper: (1) the authors do not address the issue of overfitting of upstream tasks due to the negligence of valuable intra-class semantic difference, (2) the lack of comparison between supervised methods during the pre-train stage, and (3) the overfit of the model to different downstream tasks is not well-explained.  In addition to these issues, the paper also points out that existing methods do not account for the multi-mode distribution of the visual contents, which is problematic.  The authors also point out that the current supervised pretrained method, Look, does not work well on large datasets. It does not prevent overfitting upstream tasks, and it does not perform well on ImageNet.  Finally, the authors propose a new method, called Look, to address these issues. They show that Look outperforms previous state-of-the-art methods for both supervised and self -supervised pretraining on multiple downstream tasks, showing that Look is able to achieve better transferability compared to previous supervised pre -training methods. The paper also shows that the proposed method can be applied to large datasets with a multi-modal distribution, and shows that it is more robust to overfit to multi-class differences in the dataset. "
4581,SP:2b3916ba24094c286117126e11032820f8c7c50a,"wrinkling of cheeks CONJUNCTION formation of dimples. formation of dimples CONJUNCTION wrinkling of cheeks. Morphable Models ( 3DMMs ) USED-FOR fine details. PCA - based representations USED-FOR fine details. FaceDet3D HYPONYM-OF method. single image geometric facial details USED-FOR method. vertex displacement map USED-FOR facial details. method USED-FOR facial geometric details. FDS USED-FOR detailed geometry. hallucinated details PART-OF smooth proxy geometry. facial details FEATURE-OF detailed geometry. Neural Rendering USED-FOR detailed geometry. zoom - in FEATURE-OF predicted facial details. Predicted Facial Detail CONJUNCTION Render of Predicted Facial Detail. Render of Predicted Facial Detail CONJUNCTION Predicted Facial Detail. OtherScientificTerm are Facial Expressions, 3D face geometry, smile, edit expressions, and Proxy Shading. Method is Neural Renderer. Task is Facial detail hallucination and rendering. ","This paper proposes a method called FaceDet3D, which uses Morphable Models (3DMMs) to extract fine details from 3D face geometry (e.g. wrinkling of cheeks or formation of dimples). The method is based on single image geometric facial details. The method first extracts facial geometric details from a single image using a Neural Renderer, and then applies PCA-based representations to these fine details. Then, facial details are extracted from a vertex displacement map, and the method uses FDS to render the detailed geometry from the hallucinated details. Facial detail hallucination and rendering is performed using Neural Rendering, where the hallucination is applied to a smooth proxy geometry, which is then used to edit expressions. The proposed method is evaluated on a number of datasets, and is shown to achieve state-of-the-art performance. The paper also shows that the predicted facial details from zoom-in and zoom-out can be rendered in a single pass, and that the proposed method can be combined with Proxy Shading to achieve better results.    The paper is well-written and well-motivated. However, there are a few issues:","This paper proposes a method called FaceDet3D, which uses Morphable Models (3DMMs) to extract fine details from 3D face geometry (e.g. wrinkling of cheeks or formation of dimples). The method is based on single image geometric facial details. The method first extracts facial geometric details from a single image using a Neural Renderer, and then applies PCA-based representations to these fine details. Then, facial details are extracted from a vertex displacement map, and the method uses FDS to render the detailed geometry from the hallucinated details. Facial detail hallucination and rendering is performed using Neural Rendering, where the hallucination is applied to a smooth proxy geometry, which is then used to edit expressions. The proposed method is evaluated on a number of datasets, and is shown to achieve state-of-the-art performance. The paper also shows that the predicted facial details from zoom-in and zoom-out can be rendered in a single pass, and that the proposed method can be combined with Proxy Shading to achieve better results.    The paper is well-written and well-motivated. However, there are a few issues:"
4597,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"neural representations USED-FOR NLP models. neural representations CONJUNCTION linguistic factors. linguistic factors CONJUNCTION neural representations. syntactic roles PART-OF factors. latent variables CONJUNCTION realizations of syntactic roles. realizations of syntactic roles CONJUNCTION latent variables. attention USED-FOR deep probabilistic generative model. Attention - Driven Variational Autoencoder ( ADVAE ) HYPONYM-OF probabilistic model. attention USED-FOR ADVAEs. evaluation protocol EVALUATE-FOR disentanglement. disentanglement FEATURE-OF realizations of syntactic roles. evaluation protocol USED-FOR realizations of syntactic roles. attention maxima CONJUNCTION latent variable perturbations. latent variable perturbations CONJUNCTION attention maxima. latent variable perturbations USED-FOR decoder. attention maxima USED-FOR encoder. latent variable perturbations USED-FOR protocol. attention maxima USED-FOR protocol. ADVAE USED-FOR syntactic roles. sequence VAEs CONJUNCTION Transformer VAEs. Transformer VAEs CONJUNCTION sequence VAEs. ADVAE COMPARE Transformer VAEs. Transformer VAEs COMPARE ADVAE. ADVAE COMPARE sequence VAEs. sequence VAEs COMPARE ADVAE. raw English text PART-OF SNLI dataset. latent variables USED-FOR realizations of syntactic roles. Generic is they. OtherScientificTerm are decomposition of predicative structures, and supervision. Method is Transformer - based machine translation models. Task are disentanglement of syntactic roles, and unsupervised controllable content generation. ","This paper proposes an attention-driven variational autoencoder (ADVAE) model for the problem of disentangling syntactic roles in machine translation. The authors propose a deep probabilistic generative model based on attention, which is able to disentangle neural representations in NLP models from neural representations and linguistic factors. These factors are decomposed into two factors: (1) the decomposition of predicative structures, and (2) the role of the decoder. ADVAEs are trained with attention, and the authors show that they are able to achieve disentanglement in the sense that they do not require any supervision.   The authors also propose a new evaluation protocol for disentangled realizations of syntactical roles using attention maxima and latent variable perturbations. They show that ADVAE disentangles latent variables and realizations in the context of Transformer-based machine translation models. They also show that the proposed protocol can be applied to any protocol that uses attention maxime in the encoder as well as the attentionmaxima in the decoders, and that this protocol does not require supervision. Finally, the authors demonstrate that the performance of ADVAe is comparable to sequence VAEs and Transformer VAEs on the SNLI dataset, which contains raw English text. The paper also shows that the representation of syntactic role disentangler can be learned from latent variables, which can be used to learn disentanged representations of the role.  The paper concludes that the disentained representations can be useful for unsupervised controllable content generation and that can be transferred to other tasks.","This paper proposes an attention-driven variational autoencoder (ADVAE) model for the problem of disentangling syntactic roles in machine translation. The authors propose a deep probabilistic generative model based on attention, which is able to disentangle neural representations in NLP models from neural representations and linguistic factors. These factors are decomposed into two factors: (1) the decomposition of predicative structures, and (2) the role of the decoder. ADVAEs are trained with attention, and the authors show that they are able to achieve disentanglement in the sense that they do not require any supervision.   The authors also propose a new evaluation protocol for disentangled realizations of syntactical roles using attention maxima and latent variable perturbations. They show that ADVAE disentangles latent variables and realizations in the context of Transformer-based machine translation models. They also show that the proposed protocol can be applied to any protocol that uses attention maxime in the encoder as well as the attentionmaxima in the decoders, and that this protocol does not require supervision. Finally, the authors demonstrate that the performance of ADVAe is comparable to sequence VAEs and Transformer VAEs on the SNLI dataset, which contains raw English text. The paper also shows that the representation of syntactic role disentangler can be learned from latent variables, which can be used to learn disentanged representations of the role.  The paper concludes that the disentained representations can be useful for unsupervised controllable content generation and that can be transferred to other tasks."
4613,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"Discovering successful coordinated behaviors PART-OF Multi - Agent Reinforcement Learning ( MARL ). joint action space USED-FOR it. mechanism USED-FOR sufficient exploration and coordination. framework USED-FOR coordination protocols. sparse rewards CONJUNCTION partial observability. partial observability CONJUNCTION sparse rewards. StarCraft Multi - Agent Challenge HYPONYM-OF exploration scheme. exploration scheme USED-FOR complex cooperative strategies. methods COMPARE baselines. baselines COMPARE methods. Method is intrinsic motivation functions. OtherScientificTerm are agents ’ interactions, and counterfactual rollouts. Generic is approach. ","This paper proposes a new exploration strategy for cooperative multi-agent reinforcement learning (MARL). The proposed method is based on the observation that the agent’s intrinsic motivation functions are highly dependent on the agents’ interactions. The authors propose a mechanism to encourage sufficient exploration and coordination between agents in MARL. They show that it is possible to learn a joint action space that maximally maximizes the mutual information between agents. They also show that the proposed framework can be used to learn coordination protocols that are robust to sparse rewards and partial observability. Finally, they show that their exploration scheme, StarCraft Multi-Agent Challenge, is able to learn complex cooperative strategies in the presence of sparse rewards. They demonstrate that their methods outperform several baselines, and that their approach is more robust to counterfactual rollouts. ","This paper proposes a new exploration strategy for cooperative multi-agent reinforcement learning (MARL). The proposed method is based on the observation that the agent’s intrinsic motivation functions are highly dependent on the agents’ interactions. The authors propose a mechanism to encourage sufficient exploration and coordination between agents in MARL. They show that it is possible to learn a joint action space that maximally maximizes the mutual information between agents. They also show that the proposed framework can be used to learn coordination protocols that are robust to sparse rewards and partial observability. Finally, they show that their exploration scheme, StarCraft Multi-Agent Challenge, is able to learn complex cooperative strategies in the presence of sparse rewards. They demonstrate that their methods outperform several baselines, and that their approach is more robust to counterfactual rollouts. "
4629,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"fine - tuning approaches COMPARE editing algorithms. editing algorithms COMPARE fine - tuning approaches. Gradient Decomposition ( MEND ) USED-FOR Model Editor Networks. Model Editor Networks USED-FOR post - hoc editing. MEND USED-FOR gradient. low - rank decomposition of the gradient USED-FOR transformation. low - rank decomposition of the gradient USED-FOR gradient. fine - tuning USED-FOR gradient. low - rank decomposition of the gradient USED-FOR MEND. MEND USED-FOR edits. edits USED-FOR pre - trained model. GPU USED-FOR MEND. MEND USED-FOR model editing. approach USED-FOR model editing. Method are large pre - trained models, large neural networks, and small auxiliary editing networks. Generic are models, and model. OtherScientificTerm are targeted edits, and pre - trained model ’s behavior. ","This paper proposes Gradient Decomposition (MEND) for Model Editor Networks for post-hoc editing of large pre-trained models. Compared to other fine-tuning approaches, the proposed editing algorithms are more robust to targeted edits and can be applied to large neural networks. MEND uses a low-rank decomposition of the gradient for each transformation in a pre-training model to compute the gradient of the final output of the model with the help of small auxiliary editing networks. The paper shows that MEND is able to generate edits that can be used to fine-tune the pre-train model on a small number of targeted edits. The authors also show that the proposed approach can be extended to model editing on a GPU. ","This paper proposes Gradient Decomposition (MEND) for Model Editor Networks for post-hoc editing of large pre-trained models. Compared to other fine-tuning approaches, the proposed editing algorithms are more robust to targeted edits and can be applied to large neural networks. MEND uses a low-rank decomposition of the gradient for each transformation in a pre-training model to compute the gradient of the final output of the model with the help of small auxiliary editing networks. The paper shows that MEND is able to generate edits that can be used to fine-tune the pre-train model on a small number of targeted edits. The authors also show that the proposed approach can be extended to model editing on a GPU. "
4645,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,learning abilities CONJUNCTION physical and structural knowledge. physical and structural knowledge CONJUNCTION learning abilities. artificial neural networks CONJUNCTION physical and structural knowledge. physical and structural knowledge CONJUNCTION artificial neural networks. learning abilities FEATURE-OF artificial neural networks. FINN USED-FOR constituents of partial differential equations ( PDEs ). numerical simulation USED-FOR physical and structural knowledge. artificial neural networks USED-FOR FINN. learning abilities PART-OF FINN. oneand two - dimensional PDEs EVALUATE-FOR FINN. modeling accuracy CONJUNCTION outof - distribution generalization ability. outof - distribution generalization ability CONJUNCTION modeling accuracy. initial and boundary conditions FEATURE-OF outof - distribution generalization ability. modeling accuracy EVALUATE-FOR FINN. outof - distribution generalization ability EVALUATE-FOR FINN. diffusion - sorption HYPONYM-OF oneand two - dimensional PDEs. FINN COMPARE physics - aware models. physics - aware models COMPARE FINN. FINN COMPARE machine learning. machine learning COMPARE FINN. machine learning CONJUNCTION physics - aware models. physics - aware models CONJUNCTION machine learning. FINN COMPARE calibrated physical model. calibrated physical model COMPARE FINN. calibrated physical model USED-FOR sparse real - world data. sparse real - world data USED-FOR FINN. generalization abilities EVALUATE-FOR FINN. diffusionsorption scenario FEATURE-OF sparse real - world data. Task is spatiotemporal advection - diffusion processes. OtherScientificTerm is unknown retardation factor. ,"This paper studies the problem of learning spatiotemporal advection-diffusion processes (SDPs) in the presence of an unknown retardation factor. The authors propose a method called Finetuned Neural Network (finetuned-finetune) to learn constituents of partial differential equations (PDEs). The authors use artificial neural networks (ANNs) to model the learning abilities of artificial Neural Networks (ANNs) and show that finetuned ANNs can generalize better than the state-of-the-art physics-aware models.  The authors also show that Finetune can be used to model constituents of two-dimensional PDEs (diffusion-sorption and diffusion-sporption) in numerical simulation.   The main contribution of the paper is that the authors show that FINN is able to generalize to constituents of oneand two-dimensions of constituents of PDEs, and that it can generalise better than machine learning. The paper also shows that the modeling accuracy and outof-distribution generalization ability under certain initial and boundary conditions can be improved by finetune. Finally, the authors demonstrate that the generalization abilities of FINN can be further improved by using sparse real-world data in the diffusionsorption scenario, where a calibrated physical model is used to train a sparse physical model and then fine-tune it on sparse data. ","This paper studies the problem of learning spatiotemporal advection-diffusion processes (SDPs) in the presence of an unknown retardation factor. The authors propose a method called Finetuned Neural Network (finetuned-finetune) to learn constituents of partial differential equations (PDEs). The authors use artificial neural networks (ANNs) to model the learning abilities of artificial Neural Networks (ANNs) and show that finetuned ANNs can generalize better than the state-of-the-art physics-aware models.  The authors also show that Finetune can be used to model constituents of two-dimensional PDEs (diffusion-sorption and diffusion-sporption) in numerical simulation.   The main contribution of the paper is that the authors show that FINN is able to generalize to constituents of oneand two-dimensions of constituents of PDEs, and that it can generalise better than machine learning. The paper also shows that the modeling accuracy and outof-distribution generalization ability under certain initial and boundary conditions can be improved by finetune. Finally, the authors demonstrate that the generalization abilities of FINN can be further improved by using sparse real-world data in the diffusionsorption scenario, where a calibrated physical model is used to train a sparse physical model and then fine-tune it on sparse data. "
4661,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"unsupervised machine learning ( ML ) models CONJUNCTION representations of computer programs. representations of computer programs CONJUNCTION unsupervised machine learning ( ML ) models. representations of computer programs USED-FOR representations of computer programs. unsupervised machine learning ( ML ) models USED-FOR representations of computer programs. abstract syntax tree ( AST)-related information CONJUNCTION runtime information. runtime information CONJUNCTION abstract syntax tree ( AST)-related information. brain representations USED-FOR static and dynamic properties of code. runtime information HYPONYM-OF static and dynamic properties of code. abstract syntax tree ( AST)-related information HYPONYM-OF static and dynamic properties of code. brain representations USED-FOR representations. representations USED-FOR ML models. Material is Python code. Metric is complexity. Method are Multiple Demand system, and machine learned representations of code. OtherScientificTerm is code. ","This paper investigates the relationship between representations learned by unsupervised machine learning (ML) models and representations of computer programs. Specifically, the authors focus on Python code. They show that brain representations of code are able to capture both static and dynamic properties of code, such as abstract syntax tree (AST)-related information and runtime information. They also show that these representations can be used to train ML models.    The paper is well-written and well-motivated, and the complexity of the problem is well described. The authors also provide a detailed analysis of the Multiple Demand system, which is a well-known problem in machine learning. The paper also shows that the machine learned representations of codes are not necessarily static, and that the code is able to change over time.","This paper investigates the relationship between representations learned by unsupervised machine learning (ML) models and representations of computer programs. Specifically, the authors focus on Python code. They show that brain representations of code are able to capture both static and dynamic properties of code, such as abstract syntax tree (AST)-related information and runtime information. They also show that these representations can be used to train ML models.    The paper is well-written and well-motivated, and the complexity of the problem is well described. The authors also provide a detailed analysis of the Multiple Demand system, which is a well-known problem in machine learning. The paper also shows that the machine learned representations of codes are not necessarily static, and that the code is able to change over time."
4677,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,phoneme decoder CONJUNCTION mel - spectrogram synthesizer. mel - spectrogram synthesizer CONJUNCTION phoneme decoder. speech encoder CONJUNCTION phoneme decoder. phoneme decoder CONJUNCTION speech encoder. mel - spectrogram synthesizer CONJUNCTION attention module. attention module CONJUNCTION mel - spectrogram synthesizer. attention module PART-OF Translatotron 2. mel - spectrogram synthesizer PART-OF Translatotron 2. phoneme decoder PART-OF Translatotron 2. speech encoder PART-OF Translatotron 2. Translatotron 2 COMPARE Translatotron. Translatotron COMPARE Translatotron 2. babbling CONJUNCTION long pause. long pause CONJUNCTION babbling. translation quality CONJUNCTION predicted speech naturalness. predicted speech naturalness CONJUNCTION translation quality. robustness EVALUATE-FOR predicted speech. long pause HYPONYM-OF over - generation. babbling HYPONYM-OF over - generation. predicted speech naturalness EVALUATE-FOR Translatotron 2. robustness EVALUATE-FOR Translatotron 2. translation quality EVALUATE-FOR Translatotron. translation quality EVALUATE-FOR Translatotron 2. model USED-FOR production deployment. Translatotron COMPARE it. it COMPARE Translatotron. method CONJUNCTION concatenation - based data augmentation. concatenation - based data augmentation CONJUNCTION method. Material is translated speech. Method is Translatotron 2 model. ,"This paper proposes Translatotron 2, a speech encoder, phoneme decoder, mel-spectrogram synthesizer, and attention module to improve the performance of Translatron 2. The authors show that Translattron 2 achieves better translation quality, robustness to over-generation (e.g., babbling, long pause), and predicted speech naturalness. The paper also shows that the model can be used for production deployment, and that the translated speech is more likely to be natural than the original speech.   The authors also show that the Translatrotron 2 model is more robust to noise in the input speech, and can be trained to produce speech that is more natural. The proposed method is based on concatenation-based data augmentation, and it is shown that the proposed method improves the performance over the original model. ","This paper proposes Translatotron 2, a speech encoder, phoneme decoder, mel-spectrogram synthesizer, and attention module to improve the performance of Translatron 2. The authors show that Translattron 2 achieves better translation quality, robustness to over-generation (e.g., babbling, long pause), and predicted speech naturalness. The paper also shows that the model can be used for production deployment, and that the translated speech is more likely to be natural than the original speech.   The authors also show that the Translatrotron 2 model is more robust to noise in the input speech, and can be trained to produce speech that is more natural. The proposed method is based on concatenation-based data augmentation, and it is shown that the proposed method improves the performance over the original model. "
4693,SP:296102e60b842923c94f579f524fa1147328ee4b,"attribute - based representations USED-FOR concept learning. zeroshot learning HYPONYM-OF attribute - based learning paradigms. supervised learning COMPARE selfsupervised pre - training. selfsupervised pre - training COMPARE supervised learning. predictability of test attributes USED-FOR model. predictability of test attributes USED-FOR generalization ability. generalization ability EVALUATE-FOR model. OtherScientificTerm are Semantic concepts, mappings, and random splits of the attribute space. Task is rapid learning of attributes. Method is few - shot learning of semantic classes. ","This paper studies the problem of concept learning with attribute-based representations. Semantic concepts are represented as a set of attributes, and the goal is to learn the mappings between attributes. The authors propose two attribute- based learning paradigms, i.e., zeroshot learning and self-supervised pre-training. They show that supervised learning outperforms selfsupervised pretraining in few-shot learning of semantic classes. They also show that the predictability of test attributes helps improve the generalization ability of the model. ","This paper studies the problem of concept learning with attribute-based representations. Semantic concepts are represented as a set of attributes, and the goal is to learn the mappings between attributes. The authors propose two attribute- based learning paradigms, i.e., zeroshot learning and self-supervised pre-training. They show that supervised learning outperforms selfsupervised pretraining in few-shot learning of semantic classes. They also show that the predictability of test attributes helps improve the generalization ability of the model. "
4709,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,relative entropy gradient sampler ( REGS ) USED-FOR sampling from unnormalized distributions. particle method USED-FOR nonlinear transforms. REGS HYPONYM-OF particle method. gradient flow USED-FOR path of probability distributions. path of probability distributions USED-FOR reference distribution. density of evolving particles CONJUNCTION unnormalized target density. unnormalized target density CONJUNCTION density of evolving particles. density ratios FEATURE-OF density of evolving particles. density ratios CONJUNCTION unnormalized target density. unnormalized target density CONJUNCTION density ratios. density ratios FEATURE-OF velocity fields. velocity fields FEATURE-OF ODE system. ODE system USED-FOR It. particle evolution USED-FOR ODE system. nonparametric approach USED-FOR logarithmic density ratio. neural networks USED-FOR nonparametric approach. REGS COMPARE sampling methods. sampling methods COMPARE REGS. multimodal 1D and 2D mixture distributions CONJUNCTION Bayesian logistic regression. Bayesian logistic regression CONJUNCTION multimodal 1D and 2D mixture distributions. real datasets EVALUATE-FOR Bayesian logistic regression. Method is Wasserstein gradient flow of relative entropy. ,"This paper proposes a relative entropy gradient sampler (REGS) for sampling from unnormalized distributions. REGS is a particle method for nonlinear transforms, which is based on the Wasserstein gradient flow of relative entropy. It uses the ODE system of particle evolution as the reference distribution, and uses the path of probability distributions generated by the gradient flow to sample a reference distribution. The density ratios of the density of evolving particles and the density ratios between velocity fields are used to compute density ratios for both density ratios and for density of the target density and the unnormalised target density. A nonparametric approach is also proposed to compute the logarithmic density ratio using neural networks. Experiments on multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets show that REGS outperforms other sampling methods. ","This paper proposes a relative entropy gradient sampler (REGS) for sampling from unnormalized distributions. REGS is a particle method for nonlinear transforms, which is based on the Wasserstein gradient flow of relative entropy. It uses the ODE system of particle evolution as the reference distribution, and uses the path of probability distributions generated by the gradient flow to sample a reference distribution. The density ratios of the density of evolving particles and the density ratios between velocity fields are used to compute density ratios for both density ratios and for density of the target density and the unnormalised target density. A nonparametric approach is also proposed to compute the logarithmic density ratio using neural networks. Experiments on multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets show that REGS outperforms other sampling methods. "
4725,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,quantum machine learning techniques USED-FOR classical image classification. quantum neural network USED-FOR inference. qubits USED-FOR encoding schemes. quantum systems USED-FOR framework. encoding mechanism USED-FOR approach. accuracy EVALUATE-FOR classical neural networks. framework COMPARE classical neural networks. classical neural networks COMPARE framework. personal laptop FEATURE-OF MNIST dataset. accuracy EVALUATE-FOR framework. quantum computers CONJUNCTION classical simulation. classical simulation CONJUNCTION quantum computers. work USED-FOR quantum machine learning and classification. classical datasets USED-FOR quantum machine learning and classification. OtherScientificTerm is quantum states. Generic is technique. ,"This paper proposes a new quantum machine learning techniques for classical image classification. The proposed framework is based on quantum systems, where a quantum neural network is used for inference, and the encoding schemes are based on qubits. The authors propose a new encoding mechanism for the proposed approach, and show that the proposed framework can achieve comparable accuracy to classical neural networks. The paper also shows that the MNIST dataset on a personal laptop can be used as an example of the proposed technique.  The authors also show that their work can be applied to quantum machines, quantum computers, and classical simulation. They show that this work can achieve state-of-the-art performance on classical datasets, and can also be used for quantum machines and classification. ","This paper proposes a new quantum machine learning techniques for classical image classification. The proposed framework is based on quantum systems, where a quantum neural network is used for inference, and the encoding schemes are based on qubits. The authors propose a new encoding mechanism for the proposed approach, and show that the proposed framework can achieve comparable accuracy to classical neural networks. The paper also shows that the MNIST dataset on a personal laptop can be used as an example of the proposed technique.  The authors also show that their work can be applied to quantum machines, quantum computers, and classical simulation. They show that this work can achieve state-of-the-art performance on classical datasets, and can also be used for quantum machines and classification. "
4741,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"data privacy FEATURE-OF face recognition. framework USED-FOR federated learning face recognition. communicating auxiliary and privacy - agnostic information USED-FOR framework. communicating auxiliary and privacy - agnostic information USED-FOR federated learning face recognition. Differentially Private Local Clustering ( DPLC ) mechanism USED-FOR sanitized clusters. local class centers USED-FOR sanitized clusters. consensus - aware recognition loss USED-FOR global consensuses. IJB - B CONJUNCTION IJB - C. IJB - C CONJUNCTION IJB - B. large - scale dataset EVALUATE-FOR method. Method are federated learning ( FL ) paradigm, FL methods, and PrivacyFace. Generic is task. Task is recognition. OtherScientificTerm are privacy leakage, privacy - utility paradox, discriminative features, and lightweight overhead. "," of the federated learning (FL) paradigm. This paper proposes a new framework for federated face recognition based on communicating auxiliary and privacy-agnostic information to ensure data privacy for face recognition. The authors propose a novel Differentially Private Local Clustering (DPLC) mechanism to create sanitized clusters based on the local class centers. They also propose a consensus-aware recognition loss to improve the global consensuses. The proposed method is evaluated on a large-scale dataset and shows competitive performance compared to other FL methods.    PrivacyFace:   The paper addresses the privacy-utility paradox, which is a problem that arises when the task is distributed across multiple clients and the goal is to ensure that each client does not have too much privacy leakage. This is achieved by sanitizing the discriminative features of each client, which has a lightweight overhead.  IJB-B and IJB -C are two variants of the proposed method."," of the federated learning (FL) paradigm. This paper proposes a new framework for federated face recognition based on communicating auxiliary and privacy-agnostic information to ensure data privacy for face recognition. The authors propose a novel Differentially Private Local Clustering (DPLC) mechanism to create sanitized clusters based on the local class centers. They also propose a consensus-aware recognition loss to improve the global consensuses. The proposed method is evaluated on a large-scale dataset and shows competitive performance compared to other FL methods.    PrivacyFace:   The paper addresses the privacy-utility paradox, which is a problem that arises when the task is distributed across multiple clients and the goal is to ensure that each client does not have too much privacy leakage. This is achieved by sanitizing the discriminative features of each client, which has a lightweight overhead.  IJB-B and IJB -C are two variants of the proposed method."
4757,SP:408d9e1299ee05b89855df9742b608626692b40d,Transfer - learning methods USED-FOR data - scarce target domain. data - rich source domain USED-FOR model. model USED-FOR Transfer - learning methods. strategy COMPARE method. method COMPARE strategy. fine - tuning USED-FOR model. linear probing USED-FOR intermediate layers. classification head USED-FOR target - domain. features USED-FOR classification head. Visual Task Adaptation Benchmark ( VTAB ) EVALUATE-FOR Head2Toe. Head2Toe COMPARE fine - tuning. fine - tuning COMPARE Head2Toe. Visual Task Adaptation Benchmark ( VTAB ) EVALUATE-FOR Head2Toe. Head2Toe COMPARE Head2Toe. Head2Toe COMPARE Head2Toe. fine - tuning USED-FOR Head2Toe. Head2Toe USED-FOR out - of - distribution transfer. Generic is source model. OtherScientificTerm is pretrained layers. ,"Transfer-learning methods for data-scarce target domain can be seen as an extension of existing methods for learning a model from a data-rich source domain to a data scarce target domain, where the source model is not available in the target domain but is available to the model in the source domain. This paper proposes a new strategy, called Head2Toe, to adapt a model to a target domain from a model that is available in a source domain but not in a target-distribution. The proposed strategy is similar to the previous method, but instead of fine-tuning the intermediate layers, the authors propose to use linear probing to find the best intermediate layers. The classification head is trained on the features from the target-domain and the classification head on the source-domain. The authors evaluate the performance of the proposed method on the Visual Task Adaptation Benchmark (VTAB) and show that, compared to the original method, the proposed Head2One achieves better performance on the VTAB, and is more robust to out of distribution transfer. Moreover, it is shown that, when the number of pretrained layers is small enough, the performance is comparable to that of the original Head2Tune. Finally, the paper also shows that, in the case of a large number of samples, Head2TOe performs better than the original head-to-head version of the method, which is called HeadT2T.    Overall, this paper is well-written and well-motivated. The paper is clearly written. The experimental results show that the proposed by the authors show that Head2toe outperforms the state-of-the-art method, HeadTune, in terms of performance, and can be used to improve the performance in the presence of large amounts of data.  The paper also provides a thorough ablation study that shows the effect of the amount of data on the performance. ","Transfer-learning methods for data-scarce target domain can be seen as an extension of existing methods for learning a model from a data-rich source domain to a data scarce target domain, where the source model is not available in the target domain but is available to the model in the source domain. This paper proposes a new strategy, called Head2Toe, to adapt a model to a target domain from a model that is available in a source domain but not in a target-distribution. The proposed strategy is similar to the previous method, but instead of fine-tuning the intermediate layers, the authors propose to use linear probing to find the best intermediate layers. The classification head is trained on the features from the target-domain and the classification head on the source-domain. The authors evaluate the performance of the proposed method on the Visual Task Adaptation Benchmark (VTAB) and show that, compared to the original method, the proposed Head2One achieves better performance on the VTAB, and is more robust to out of distribution transfer. Moreover, it is shown that, when the number of pretrained layers is small enough, the performance is comparable to that of the original Head2Tune. Finally, the paper also shows that, in the case of a large number of samples, Head2TOe performs better than the original head-to-head version of the method, which is called HeadT2T.    Overall, this paper is well-written and well-motivated. The paper is clearly written. The experimental results show that the proposed by the authors show that Head2toe outperforms the state-of-the-art method, HeadTune, in terms of performance, and can be used to improve the performance in the presence of large amounts of data.  The paper also provides a thorough ablation study that shows the effect of the amount of data on the performance. "
4773,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,dynamics models USED-FOR model - based planning. Dynamics models USED-FOR image - based games. fully observable states FEATURE-OF image - based games. models USED-FOR Text - Based Games ( TBGs ). noisy text observations FEATURE-OF partially observable states. planning algorithms USED-FOR decision - making problems. planning algorithms USED-FOR text domains. text domains USED-FOR decision - making problems. OOTD USED-FOR memory graph. OOTD model USED-FOR beliefs of object states. OOTD model USED-FOR dynamics. independently parameterized transition layers USED-FOR beliefs of object states. variational objectives USED-FOR stochasticity of predicted dynamics. object - supervised and self - supervised settings USED-FOR variational objectives. OOTD - based planner COMPARE model - free baselines. model - free baselines COMPARE OOTD - based planner. sample efficiency CONJUNCTION running scores. running scores CONJUNCTION sample efficiency. running scores EVALUATE-FOR OOTD - based planner. sample efficiency EVALUATE-FOR OOTD - based planner. running scores EVALUATE-FOR model - free baselines. sample efficiency EVALUATE-FOR model - free baselines. OtherScientificTerm is object - irrelevant information. ,"This paper proposes a new model-based planning based on dynamics models for image-based games with fully observable states and partially observable states with noisy text observations. Dynamics models are commonly used for decision-making problems with planning algorithms in text domains, and the authors extend these models to Text-Based Games (TBGs). The authors propose to use OOTD to learn a memory graph, which is then used to train a model-free planning algorithm. The authors show that the learned dynamics can be used to learn the dynamics of the environment, and that the dynamics learned by the OotD model can capture the beliefs of object states, which are then used as independently parameterized transition layers. They also show that variational objectives are used to capture the stochasticity of predicted dynamics in both object-supervised and self-supervision settings, which helps to capture object-irrelevant information. The paper shows that the OOTd-based planner outperforms the state-of-the-art in terms of sample efficiency and running scores, and outperforms other model free baselines in sample efficiency.","This paper proposes a new model-based planning based on dynamics models for image-based games with fully observable states and partially observable states with noisy text observations. Dynamics models are commonly used for decision-making problems with planning algorithms in text domains, and the authors extend these models to Text-Based Games (TBGs). The authors propose to use OOTD to learn a memory graph, which is then used to train a model-free planning algorithm. The authors show that the learned dynamics can be used to learn the dynamics of the environment, and that the dynamics learned by the OotD model can capture the beliefs of object states, which are then used as independently parameterized transition layers. They also show that variational objectives are used to capture the stochasticity of predicted dynamics in both object-supervised and self-supervision settings, which helps to capture object-irrelevant information. The paper shows that the OOTd-based planner outperforms the state-of-the-art in terms of sample efficiency and running scores, and outperforms other model free baselines in sample efficiency."
4789,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"cost - sensitive loss FEATURE-OF label taxonomy. tractable method USED-FOR hierarchical learning problem. hierarchical cost - sensitive loss CONJUNCTION layer - wise abstaining losses. layer - wise abstaining losses CONJUNCTION hierarchical cost - sensitive loss. bijective mapping USED-FOR hierarchical cost - sensitive loss. symmetry assumptions USED-FOR bijective mapping. distributionally robust learning framework USED-FOR learningto - abstain problems. large - scale bird dataset CONJUNCTION cell classification problems. cell classification problems CONJUNCTION large - scale bird dataset. LAM COMPARE methods. methods COMPARE LAM. high accuracy regions FEATURE-OF hierarchical cost - sensitive loss. hierarchical cost - sensitive loss EVALUATE-FOR LAM. high accuracy regions EVALUATE-FOR LAM. perclass loss - adjustment heuristic USED-FOR performance profile. perclass loss - adjustment heuristic USED-FOR LAM. cost design USED-FOR user requirements. cost design USED-FOR optimizable cost functions. user requirements CONJUNCTION optimizable cost functions. optimizable cost functions CONJUNCTION user requirements. Task is cost - sensitive hierarchical classification. OtherScientificTerm are hierarchy, cost - sensitive hierarchical loss, non - convexity, and taxonomy. Metric is accuracy. ","This paper studies the problem of cost-sensitive hierarchical classification. The authors propose a tractable method for solving the hierarchical learning problem, where the goal is to learn a label taxonomy under the cost-sensitive loss. The hierarchy is represented as a bijective mapping, and the authors propose to use a bijection between the hierarchical cost-ensitive loss and layer-wise abstaining losses. Under symmetry assumptions, the authors show that under certain conditions, the bijection is non-convex. They also propose a distributionally robust learning framework for the learningto-abstain problems. They show that LAM outperforms existing methods in high accuracy regions of the hierarchical loss, and outperforms LAM in low accuracy regions. Finally, they show that the perclass loss-adjustment heuristic can be used to improve the performance profile of LAM, and that the cost design can be optimized to meet user requirements and optimizable cost functions.   The paper is well-written, well-motivated, and easy to follow. The experiments are conducted on a large-scale bird dataset, and cell classification problems. The results are promising. The paper also shows that the proposed LAM can outperform LAM when the taxonomy is not well-structured, and LAM is able to learn to abstain in regions with high accuracy. ","This paper studies the problem of cost-sensitive hierarchical classification. The authors propose a tractable method for solving the hierarchical learning problem, where the goal is to learn a label taxonomy under the cost-sensitive loss. The hierarchy is represented as a bijective mapping, and the authors propose to use a bijection between the hierarchical cost-ensitive loss and layer-wise abstaining losses. Under symmetry assumptions, the authors show that under certain conditions, the bijection is non-convex. They also propose a distributionally robust learning framework for the learningto-abstain problems. They show that LAM outperforms existing methods in high accuracy regions of the hierarchical loss, and outperforms LAM in low accuracy regions. Finally, they show that the perclass loss-adjustment heuristic can be used to improve the performance profile of LAM, and that the cost design can be optimized to meet user requirements and optimizable cost functions.   The paper is well-written, well-motivated, and easy to follow. The experiments are conducted on a large-scale bird dataset, and cell classification problems. The results are promising. The paper also shows that the proposed LAM can outperform LAM when the taxonomy is not well-structured, and LAM is able to learn to abstain in regions with high accuracy. "
4805,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"spatial location CONJUNCTION semantic identity label. semantic identity label CONJUNCTION spatial location. estimating sound sources ’ temporal location CONJUNCTION spatial location. spatial location CONJUNCTION estimating sound sources ’ temporal location. multi - channel sound raw waveforms USED-FOR semantic identity label. multi - channel sound raw waveforms USED-FOR estimating sound sources ’ temporal location. single - scale filter bank USED-FOR sound waveforms. STFT CONJUNCTION LogMel. LogMel CONJUNCTION STFT. they USED-FOR hand - engineered features. limited time - frequency resolution capability FEATURE-OF sound waveforms. parameter tuning USED-FOR they. STFT HYPONYM-OF hand - engineered features. LogMel HYPONYM-OF hand - engineered features. parameter tuning USED-FOR hand - engineered features. frequency resolution FEATURE-OF synperiodic filter. time - frequency resolution trade - off EVALUATE-FOR synperiodic filter. synperiodic filter bank USED-FOR multi - scale perception. synperiodic filter bank USED-FOR downsampled waveform. synperiodic filter bank group USED-FOR dynamic multi - scale time - frequency representation. multi - scale perception USED-FOR synperiodic filter bank group. time and frequency domain advantage FEATURE-OF multi - scale perception. semantic identity label CONJUNCTION spatial location representation. spatial location representation CONJUNCTION semantic identity label. Transformer - like backbone USED-FOR semantic identity label. Transformer - like backbone USED-FOR spatial location representation. parallel soft - stitched branches PART-OF Transformer - like backbone. Transformer - like backbone PART-OF synperiodic filter bank group front - end. direction of arrival estimation task CONJUNCTION physical location estimation task. physical location estimation task CONJUNCTION direction of arrival estimation task. direction of arrival estimation task EVALUATE-FOR framework. physical location estimation task EVALUATE-FOR framework. OtherScientificTerm are complex waveform mixture, periodicity term, and temporal length. Generic are representation, and Existing methods. Method are parameterized synperiodic filter banks, and synperiodic filter banks. Material is raw waveform. ","This paper proposes a method for multi-scale time-frequency representation learning from multi-channel sound raw waveforms. The proposed method is based on the idea of a multi-frequency filter bank, which is used to encode the temporal and spatial information in the waveform. The authors show that the proposed method outperforms the state-of-the-art methods on the arrival estimation task and the physical location estimation task.  ","This paper proposes a method for multi-scale time-frequency representation learning from multi-channel sound raw waveforms. The proposed method is based on the idea of a multi-frequency filter bank, which is used to encode the temporal and spatial information in the waveform. The authors show that the proposed method outperforms the state-of-the-art methods on the arrival estimation task and the physical location estimation task.  "
4821,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"self - training USED-FOR model. iterative self - training USED-FOR model. implicit curriculum USED-FOR iterative self - training. method USED-FOR virtual samples. intermediate distributions USED-FOR virtual samples. iterative self - training CONJUNCTION GIFT. GIFT CONJUNCTION iterative self - training. self - training CONJUNCTION iterative self - training. iterative self - training CONJUNCTION self - training. GIFT USED-FOR model. domain adaptation methods USED-FOR GIFT. natural distribution shifts FEATURE-OF benchmarks. benchmarks EVALUATE-FOR iterative self - training. benchmarks EVALUATE-FOR self - training. benchmarks EVALUATE-FOR GIFT. Task is domain adaptation. Method are learning domain invariant representations, and iterative selftraining. Generic is assumptions. OtherScientificTerm is synthetic distribution shifts. ","This paper studies the problem of domain adaptation. The authors propose a method for learning domain invariant representations by learning from virtual samples generated from intermediate distributions. They show that iterative self-training with an implicit curriculum is able to learn a model that is robust to synthetic distribution shifts in the target domain. They also show that under certain assumptions, iterative selftraining can learn a good representation that is invariant to the synthetic distribution shift. They further show that the proposed method can be used to generate virtual samples that are robust to natural distribution shifts. They evaluate the performance of self-learning and iterative learning on three benchmarks with natural distribution shift and show that their method outperforms the state-of-the-art self-trained models on all three benchmarks. Finally, they show that a model trained with GIFT, which is one of the most commonly used domain adaptation methods, can be trained to generalize to new domains.","This paper studies the problem of domain adaptation. The authors propose a method for learning domain invariant representations by learning from virtual samples generated from intermediate distributions. They show that iterative self-training with an implicit curriculum is able to learn a model that is robust to synthetic distribution shifts in the target domain. They also show that under certain assumptions, iterative selftraining can learn a good representation that is invariant to the synthetic distribution shift. They further show that the proposed method can be used to generate virtual samples that are robust to natural distribution shifts. They evaluate the performance of self-learning and iterative learning on three benchmarks with natural distribution shift and show that their method outperforms the state-of-the-art self-trained models on all three benchmarks. Finally, they show that a model trained with GIFT, which is one of the most commonly used domain adaptation methods, can be trained to generalize to new domains."
4837,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"representations USED-FOR multi - modal retrieval. recommendation CONJUNCTION search. search CONJUNCTION recommendation. representations USED-FOR applications. search HYPONYM-OF applications. recommendation HYPONYM-OF applications. video titles or audio transcripts USED-FOR video - text retrieval literature. method USED-FOR representations. videos USED-FOR representations. attention - based mechanism USED-FOR model. comments USED-FOR method. video - text retrieval benchmarks EVALUATE-FOR method. Generic is benchmarks. OtherScientificTerm are modalities, and user comments. ","This paper proposes a method to learn representations for multi-modal retrieval, which is an important problem in applications such as recommendation and search. The paper builds on the video-text retrieval literature, which typically uses video titles or audio transcripts. The proposed method learns representations from videos using an attention-based mechanism, where the modalities are modalities from user comments. The method is evaluated on a number of video-Text retrieval benchmarks, and the results show that the proposed method can learn representations from multiple modalities. ","This paper proposes a method to learn representations for multi-modal retrieval, which is an important problem in applications such as recommendation and search. The paper builds on the video-text retrieval literature, which typically uses video titles or audio transcripts. The proposed method learns representations from videos using an attention-based mechanism, where the modalities are modalities from user comments. The method is evaluated on a number of video-Text retrieval benchmarks, and the results show that the proposed method can learn representations from multiple modalities. "
4853,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"policy USED-FOR latent - conditioned trajectories. discriminator USED-FOR distinguishability. trajectories USED-FOR latents. penalization USED-FOR exploration. objective USED-FOR epistemic uncertainty. objective USED-FOR intrinsic reward. intrinsic reward COMPARE pseudocount - based methods. pseudocount - based methods COMPARE intrinsic reward. objective COMPARE pseudocount - based methods. pseudocount - based methods COMPARE objective. DISDAIN USED-FOR skill learning. tabular grid world EVALUATE-FOR DISDAIN. DISDAIN USED-FOR pessimism. OtherScientificTerm are Unsupervised skill learning objectives, extrinsic rewards, inherent pessimism, information gain auxiliary objective, and discriminators. ","This paper studies the problem of unsupervised skill learning objectives. Unsupervised skills learning objectives can be seen as intrinsic rewards or extrinsic rewards. In this paper, the authors propose a policy that learns latent-conditioned trajectories conditioned on a discriminator that aims at distinguishability. The discriminator is trained to distinguish between two latents conditioned on the same trajectories. The authors also propose an information gain auxiliary objective that penalizes the inherent pessimism of the discriminator. This objective is designed to capture the epistemic uncertainty and penalization for exploration. Experiments show that the proposed objective improves intrinsic reward over pseudocount-based methods and is more robust to intrinsic pessimism. The proposed objective, DISDAIN, is tested on a tabular grid world and is shown to improve the performance of skill learning. It is also shown that the pessimism induced by DISDAINE is more sensitive to the discriminators.  ","This paper studies the problem of unsupervised skill learning objectives. Unsupervised skills learning objectives can be seen as intrinsic rewards or extrinsic rewards. In this paper, the authors propose a policy that learns latent-conditioned trajectories conditioned on a discriminator that aims at distinguishability. The discriminator is trained to distinguish between two latents conditioned on the same trajectories. The authors also propose an information gain auxiliary objective that penalizes the inherent pessimism of the discriminator. This objective is designed to capture the epistemic uncertainty and penalization for exploration. Experiments show that the proposed objective improves intrinsic reward over pseudocount-based methods and is more robust to intrinsic pessimism. The proposed objective, DISDAIN, is tested on a tabular grid world and is shown to improve the performance of skill learning. It is also shown that the pessimism induced by DISDAINE is more sensitive to the discriminators.  "
4869,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,deep neural networks USED-FOR chemistry. deep neural networks USED-FOR generating molecules. spanning tree CONJUNCTION residual edges. residual edges CONJUNCTION spanning tree. spanning tree USED-FOR molecular graph generation. tree - constructive operations USED-FOR molecular graph connectivity. formulation USED-FOR sparsity of molecular graphs. intermediate graph structure USED-FOR framework. intermediate graph structure USED-FOR construction process. chemical valence rules FEATURE-OF molecular graphs. Transformer architecture USED-FOR tree construction procedure. tree - based relative positional encodings USED-FOR tree construction procedure. tree - based relative positional encodings USED-FOR Transformer architecture. Fréchet ChemNet distance CONJUNCTION fragment similarity. fragment similarity CONJUNCTION Fréchet ChemNet distance. validity CONJUNCTION Fréchet ChemNet distance. Fréchet ChemNet distance CONJUNCTION validity. QM9 CONJUNCTION ZINC250k. ZINC250k CONJUNCTION QM9. ZINC250k CONJUNCTION MOSES benchmarks. MOSES benchmarks CONJUNCTION ZINC250k. QM9 EVALUATE-FOR framework. MOSES benchmarks EVALUATE-FOR framework. metrics EVALUATE-FOR framework. fragment similarity EVALUATE-FOR framework. validity EVALUATE-FOR framework. validity HYPONYM-OF metrics. fragment similarity HYPONYM-OF metrics. Fréchet ChemNet distance HYPONYM-OF metrics. STGG USED-FOR penalized LogP value of molecules. Task is maximizing penalized LogP value of molecules. ,"This paper proposes a novel approach for molecular graph generation based on deep neural networks for chemistry. In particular, the authors propose a novel way of generating molecules from molecules by constructing a spanning tree and residual edges. The authors also introduce tree-constructive operations to improve the molecular graph connectivity.   The authors claim that this formulation is more robust to sparsity of molecular graphs, and that the construction process does not require the intermediate graph structure of the molecule, which is more suitable for a more general construction process.  The proposed Transformer architecture is based on tree-based relative positional encodings for the tree construction procedure, and is able to capture chemical valence rules for molecular graphs. The proposed framework is evaluated on QM9, ZINC250k, and MOSES benchmarks using three metrics: validity, Fréchet ChemNet distance, and fragment similarity. STGG is also applied to maximize the penalized LogP value of molecules generated by STGG. ","This paper proposes a novel approach for molecular graph generation based on deep neural networks for chemistry. In particular, the authors propose a novel way of generating molecules from molecules by constructing a spanning tree and residual edges. The authors also introduce tree-constructive operations to improve the molecular graph connectivity.   The authors claim that this formulation is more robust to sparsity of molecular graphs, and that the construction process does not require the intermediate graph structure of the molecule, which is more suitable for a more general construction process.  The proposed Transformer architecture is based on tree-based relative positional encodings for the tree construction procedure, and is able to capture chemical valence rules for molecular graphs. The proposed framework is evaluated on QM9, ZINC250k, and MOSES benchmarks using three metrics: validity, Fréchet ChemNet distance, and fragment similarity. STGG is also applied to maximize the penalized LogP value of molecules generated by STGG. "
4885,SP:3a19340d6af65e3f949dda839a6d233369891c46,"image generation CONJUNCTION face recognition. face recognition CONJUNCTION image generation. Polynomial neural networks ( PNNs ) USED-FOR image generation. Polynomial neural networks ( PNNs ) USED-FOR face recognition. spectral bias FEATURE-OF low - frequency functions. spectral bias FEATURE-OF neural networks. spectral analysis USED-FOR Neural Tangent Kernel ( NTK ). Neural Tangent Kernel ( NTK ) USED-FOR PNNs. spectral analysis USED-FOR PNNs. parametrization of PNNs USED-FOR learning. Π - Net family USED-FOR learning. parametrization of PNNs HYPONYM-OF Π - Net family. polynomials USED-FOR multiplicative interactions. OtherScientificTerm are high - frequency information, and theoretical bias. Task is training. ","Polynomial neural networks (PNNs) have been shown to have a spectral bias towards low-frequency functions, which is a well-known phenomenon in image generation and face recognition. This paper extends the spectral bias of neural networks to neural networks with polynomials. The authors use spectral analysis of the Neural Tangent Kernel (NTK) of PNNs to show that the spectral analysis is biased towards high-frequency information. They also show that a parametrization of the Π-Net family, i.e., parametrized by polynomial interactions between polynmials, is sufficient to explain the theoretical bias. The paper also shows that the multiplicative interactions between two polynoms can be understood as a function of the number of polynometries of the input, which explains why the training can be done efficiently.","Polynomial neural networks (PNNs) have been shown to have a spectral bias towards low-frequency functions, which is a well-known phenomenon in image generation and face recognition. This paper extends the spectral bias of neural networks to neural networks with polynomials. The authors use spectral analysis of the Neural Tangent Kernel (NTK) of PNNs to show that the spectral analysis is biased towards high-frequency information. They also show that a parametrization of the Π-Net family, i.e., parametrized by polynomial interactions between polynmials, is sufficient to explain the theoretical bias. The paper also shows that the multiplicative interactions between two polynoms can be understood as a function of the number of polynometries of the input, which explains why the training can be done efficiently."
4901,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"memory CONJUNCTION computational costs. computational costs CONJUNCTION memory. hidden subnetworks PART-OF randomly initialized NNs. edge - popup algorithm USED-FOR hidden subnetworks. subnetworks PART-OF randomly initialized NNs. disguised subnetworks HYPONYM-OF subnetworks. disguised subnetworks COMPARE hidden counterparts. hidden counterparts COMPARE disguised subnetworks. unmasking process USED-FOR subnetworks. unmasking process USED-FOR sparse subnetwork mask. two - stage algorithm USED-FOR disguised subnetworks. operations USED-FOR two - stage algorithm. random initialization USED-FOR subnetwork. ResNet-18 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-18. ResNet-50 CONJUNCTION WideResNet-28. WideResNet-28 CONJUNCTION ResNet-50. PaB COMPARE counterparts. counterparts COMPARE PaB. PaB COMPARE edge - popup. edge - popup COMPARE PaB. CIFAR-10/100 datasets EVALUATE-FOR PaB. edge - popup COMPARE counterparts. counterparts COMPARE edge - popup. large - scale models EVALUATE-FOR PaB. CIFAR-10/100 datasets EVALUATE-FOR large - scale models. WideResNet-28 HYPONYM-OF large - scale models. ResNet-18 HYPONYM-OF large - scale models. ResNet-50 HYPONYM-OF large - scale models. Method are Sparse neural networks ( NNs ), pruningand - finetuning pipeline, random networks, and training algorithm. OtherScientificTerm are disguise, latent weights, and approximated gradients. ","Sparse neural networks (NNs) have been a popular topic of interest in recent years. However, the pruningand-finetuning pipeline has been slow and expensive due to memory and computational costs. This paper proposes an edge-popup algorithm to prune hidden subnetworks in randomly initialized NNs, called ""subnetworks"" (i.e. ""subnets"").   The paper proposes two types of subnets: ""concealed subnets"" and ""disguised subnets"". The paper shows that the disguised subnets can be pruned and finetuned in a similar way as the random networks.   In particular, the paper proposes a two-stage algorithm that first prunes the hidden subnetwork mask, and then uses an unmasking process to unmask the subnets. The masked subnets are then pruned in an end-to-end manner. The paper claims that the unmasked subnets have better performance than the original subnets in terms of accuracy.  In addition to this, the authors also propose an unmasking process that can be applied to any subnetwork with a random initialization.  The authors show that the masked subnetwork masks are more effective at masking out hidden subnets than their original counterparts. The authors also show that a simple modification to the training algorithm is sufficient to achieve this.","Sparse neural networks (NNs) have been a popular topic of interest in recent years. However, the pruningand-finetuning pipeline has been slow and expensive due to memory and computational costs. This paper proposes an edge-popup algorithm to prune hidden subnetworks in randomly initialized NNs, called ""subnetworks"" (i.e. ""subnets"").   The paper proposes two types of subnets: ""concealed subnets"" and ""disguised subnets"". The paper shows that the disguised subnets can be pruned and finetuned in a similar way as the random networks.   In particular, the paper proposes a two-stage algorithm that first prunes the hidden subnetwork mask, and then uses an unmasking process to unmask the subnets. The masked subnets are then pruned in an end-to-end manner. The paper claims that the unmasked subnets have better performance than the original subnets in terms of accuracy.  In addition to this, the authors also propose an unmasking process that can be applied to any subnetwork with a random initialization.  The authors show that the masked subnetwork masks are more effective at masking out hidden subnets than their original counterparts. The authors also show that a simple modification to the training algorithm is sufficient to achieve this."
4917,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"it USED-FOR risk - sensitive domains. robust GNN model USED-FOR adversarial attacks. Unified Graph Neural Network ( GUGNN ) framework USED-FOR graph. graph CONJUNCTION features. features CONJUNCTION graph. robust GNN model(R - GUGNN ) USED-FOR adversarial attacks. operations USED-FOR robust GNN model(R - GUGNN ). operations USED-FOR it. similarity of two adjacent nodes ’ features CONJUNCTION sparsity of real - world graphs. sparsity of real - world graphs CONJUNCTION similarity of two adjacent nodes ’ features. small eigenvalues FEATURE-OF perturbed graphs. operation USED-FOR graph. convolution operation USED-FOR features. Laplacian smoothness CONJUNCTION prior knowledge. prior knowledge CONJUNCTION Laplacian smoothness. Laplacian smoothness USED-FOR convolution operation. real - world datasets EVALUATE-FOR R - GUGNN. real - world datasets EVALUATE-FOR baselines. R - GUGNN COMPARE baselines. baselines COMPARE R - GUGNN. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are graphs, and denoising features. Generic is they. Task is cleaning perturbed graph structure. ","Graph Neural Networks (GNNs) have been shown to be robust to perturbations to graphs, but they are not robust to adversarial attacks. This paper proposes a new robust GNN model (R-GUGNN) based on the Unified Graph Neural Network (UGNN), which can be seen as an extension of Graph Neural Networks [1]. The authors propose a new set of operations to improve the robustness of R-GNN model in the context of risk-sensitive domains, and show that it can be applied to a number of different types of perturbed graphs with small eigenvalues. The authors also show that the proposed R-UGNN can be used for cleaning perturbed graph structure by cleaning the similarity of two adjacent nodes’ features, sparsity of real-world graphs, and denoising features.   The authors show that R-R-UGN is able to learn a robust graph in a unified way, and that it is robust to a variety of attacks. In particular, the authors propose an operation to clean the graph and the features of the graph, and then apply a convolution operation based on Laplacian smoothness and prior knowledge. Experiments on several real-life datasets show that this R-ugNN outperforms several baselines.","Graph Neural Networks (GNNs) have been shown to be robust to perturbations to graphs, but they are not robust to adversarial attacks. This paper proposes a new robust GNN model (R-GUGNN) based on the Unified Graph Neural Network (UGNN), which can be seen as an extension of Graph Neural Networks [1]. The authors propose a new set of operations to improve the robustness of R-GNN model in the context of risk-sensitive domains, and show that it can be applied to a number of different types of perturbed graphs with small eigenvalues. The authors also show that the proposed R-UGNN can be used for cleaning perturbed graph structure by cleaning the similarity of two adjacent nodes’ features, sparsity of real-world graphs, and denoising features.   The authors show that R-R-UGN is able to learn a robust graph in a unified way, and that it is robust to a variety of attacks. In particular, the authors propose an operation to clean the graph and the features of the graph, and then apply a convolution operation based on Laplacian smoothness and prior knowledge. Experiments on several real-life datasets show that this R-ugNN outperforms several baselines."
4933,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,approach USED-FOR texture mapping. it USED-FOR document image unwarping. texture mapping USED-FOR 3D surface. method USED-FOR surface parameterization. 3D surface positions CONJUNCTION 2D texture - space coordinates. 2D texture - space coordinates CONJUNCTION 3D surface positions. continuous bijective mapping USED-FOR 3D surface positions. 2D texture - space coordinates FEATURE-OF continuous bijective mapping. continuous bijective mapping USED-FOR method. multi - view images CONJUNCTION rendering loss. rendering loss CONJUNCTION multi - view images. surface parameterization network PART-OF differentiable rendering pipeline. rendering loss USED-FOR surface parameterization network. multi - view images USED-FOR surface parameterization network. differentiable rendering techniques USED-FOR implicit surfaces. 3D scene reconstruction CONJUNCTION view synthesis. view synthesis CONJUNCTION 3D scene reconstruction. differentiable rendering techniques USED-FOR 3D scene reconstruction. methods USED-FOR appearance color. texture map extraction CONJUNCTION texture editing. texture editing CONJUNCTION texture map extraction. differentiable renderer USED-FOR implicit surfaces. texture extraction USED-FOR document - unwarping. approach USED-FOR high - frequency textures. high - frequency textures FEATURE-OF arbitrary document shapes. synthetic and real scenarios FEATURE-OF arbitrary document shapes. it USED-FOR document texture editing. system USED-FOR document texture editing. it USED-FOR system. Method is explicit surface parameterization. Generic is they. ,"This paper proposes a new approach to learn a texture mapping between a 3D surface and its corresponding texture mapping, and applies it to document image unwarping. The method is based on a continuous bijective mapping between 3D surfaces and their 2D texture-space coordinates, which can be expressed as a function of explicit surface parameterization. The proposed method is able to learn the surface paramization from multi-view images and a single rendering loss, and it can also be used for document texture editing.   The method consists of two steps: (1) learning a surface parametrization network that is trained using multi-views and a rendering loss (2) using differentiable rendering techniques to learn implicit surfaces from a differentiable renderer.  The paper shows that the proposed approach can learn high-frequency textures for arbitrary document shapes in both synthetic and real scenarios, and that it can be used to perform document-unwarping through texture extraction and texture editing, and can be applied to 3D scene reconstruction and view synthesis. The paper also shows that existing methods can learn appearance color, but they are limited to a single view, and the proposed method can learn the appearance color from multiple views.  Finally, the paper shows how to use the learned texture map extraction and the texture editing to improve the performance of the system in the context of document textures.","This paper proposes a new approach to learn a texture mapping between a 3D surface and its corresponding texture mapping, and applies it to document image unwarping. The method is based on a continuous bijective mapping between 3D surfaces and their 2D texture-space coordinates, which can be expressed as a function of explicit surface parameterization. The proposed method is able to learn the surface paramization from multi-view images and a single rendering loss, and it can also be used for document texture editing.   The method consists of two steps: (1) learning a surface parametrization network that is trained using multi-views and a rendering loss (2) using differentiable rendering techniques to learn implicit surfaces from a differentiable renderer.  The paper shows that the proposed approach can learn high-frequency textures for arbitrary document shapes in both synthetic and real scenarios, and that it can be used to perform document-unwarping through texture extraction and texture editing, and can be applied to 3D scene reconstruction and view synthesis. The paper also shows that existing methods can learn appearance color, but they are limited to a single view, and the proposed method can learn the appearance color from multiple views.  Finally, the paper shows how to use the learned texture map extraction and the texture editing to improve the performance of the system in the context of document textures."
4949,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"subtle regions USED-FOR fine - grained features. strided operations USED-FOR representation. Convolutional Neural Network USED-FOR representation. strided operations USED-FOR Convolutional Neural Network. downsampling algorithm USED-FOR network. scale of receptive field CONJUNCTION granularity of feature. granularity of feature CONJUNCTION scale of receptive field. trade - off mechanism USED-FOR ARP. ARP USED-FOR network. image classification CONJUNCTION image retrieval. image retrieval CONJUNCTION image classification. pooling operation COMPARE state - of - the - arts. state - of - the - arts COMPARE pooling operation. image classification EVALUATE-FOR state - of - the - arts. Task is Fine - grained recognition. OtherScientificTerm are feature resolution, fine - grained information, resolution of sub - sampled feature, and learning - based parameters. ","Fine-grained recognition is an important problem in many applications where feature resolution is very important. This paper proposes to use a Convolutional Neural Network with strided operations to learn a representation that is more robust to sub-regions of the input image. The idea is to use subtle regions of the image to capture fine-graining features. The network is trained using a downsampling algorithm, where each layer of the convolutional neural network is partitioned into a set of sub-resolutions, and each sub-resolution is used as a pooling operation. The paper proposes a trade-off between the scale of receptive field and the granularity of feature, and proposes an ARP to optimize the trade-offs. Experiments on image classification and image retrieval show that the proposed pooling method outperforms the state-of-the-arts in terms of performance in image classification, and in image retrieval. The main contribution of the paper is that the pooling is able to capture the fine- grained information in the sub-region, and that the network can be trained with ARP in a way that does not require the resolution of sub - sampled feature to be the same as the learning-based parameters.   ","Fine-grained recognition is an important problem in many applications where feature resolution is very important. This paper proposes to use a Convolutional Neural Network with strided operations to learn a representation that is more robust to sub-regions of the input image. The idea is to use subtle regions of the image to capture fine-graining features. The network is trained using a downsampling algorithm, where each layer of the convolutional neural network is partitioned into a set of sub-resolutions, and each sub-resolution is used as a pooling operation. The paper proposes a trade-off between the scale of receptive field and the granularity of feature, and proposes an ARP to optimize the trade-offs. Experiments on image classification and image retrieval show that the proposed pooling method outperforms the state-of-the-arts in terms of performance in image classification, and in image retrieval. The main contribution of the paper is that the pooling is able to capture the fine- grained information in the sub-region, and that the network can be trained with ARP in a way that does not require the resolution of sub - sampled feature to be the same as the learning-based parameters.   "
4965,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"distribution shifts FEATURE-OF neural networks. structural information USED-FOR prediction. structural information FEATURE-OF graph. OOD problem USED-FOR node - level prediction. domain - invariant learning approach USED-FOR GNNs. invariant graph features USED-FOR prediction. invariant graph features USED-FOR GNNs. graphs USED-FOR OOD problem. graphs USED-FOR node - level prediction. Explore - to - Extrapolate Risk Minimization HYPONYM-OF domain - invariant learning approach. graph editers HYPONYM-OF multiple context explorers. cross - domain transfers CONJUNCTION dynamic graph evolution. dynamic graph evolution CONJUNCTION cross - domain transfers. artificial spurious features CONJUNCTION cross - domain transfers. cross - domain transfers CONJUNCTION artificial spurious features. real - world datasets USED-FOR distribution shifts. method USED-FOR distribution shifts. real - world datasets EVALUATE-FOR method. Material are Euclidean data, and graph - structured data. Method are invariant models, and OOD solution. OtherScientificTerm is virtual environments. Generic is model. ","This paper proposes a domain-invariant learning approach to tackle the OOD problem for node-level prediction based on invariant graph features for neural networks under distribution shifts between Euclidean data and graph-structured data. In particular, the authors propose an Explore-to-extrapolate Risk Minimization (Explore-T2E) domain-irrelevance learning approach, which is a domain invariant learning method for learning invariant models in the presence of out-of-distribution (OOD) distribution shifts in neural networks. The authors show that invariant GNNs can be trained on graphs that are invariant to OOD distribution shifts, and that the invariance of the model to distribution shifts can be achieved through the use of graph editers, which are multiple context explorers (i.e., graph editsers) that are trained on different virtual environments. The proposed method is tested on three real-world datasets for distribution shifts and is shown to be robust to artificial spurious features, cross-domain transfers and dynamic graph evolution. The paper also shows that the proposed OOD solution can be applied to a number of existing methods.","This paper proposes a domain-invariant learning approach to tackle the OOD problem for node-level prediction based on invariant graph features for neural networks under distribution shifts between Euclidean data and graph-structured data. In particular, the authors propose an Explore-to-extrapolate Risk Minimization (Explore-T2E) domain-irrelevance learning approach, which is a domain invariant learning method for learning invariant models in the presence of out-of-distribution (OOD) distribution shifts in neural networks. The authors show that invariant GNNs can be trained on graphs that are invariant to OOD distribution shifts, and that the invariance of the model to distribution shifts can be achieved through the use of graph editers, which are multiple context explorers (i.e., graph editsers) that are trained on different virtual environments. The proposed method is tested on three real-world datasets for distribution shifts and is shown to be robust to artificial spurious features, cross-domain transfers and dynamic graph evolution. The paper also shows that the proposed OOD solution can be applied to a number of existing methods."
4981,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"contrastive learning USED-FOR time series data. encoder USED-FOR robust and discriminative representations. augmentations USED-FOR contrastive learning. ad - hoc manual selection EVALUATE-FOR time series augmentations. prefabricated human priors USED-FOR rule of thumb. rule of thumb USED-FOR augmented samples. augmentations of time series data USED-FOR contrastive learning tasks. meta - learning mechanism USED-FOR information - aware approach. InfoTS HYPONYM-OF information - aware approach. meta - learner CONJUNCTION encoder. encoder CONJUNCTION meta - learner. classification task EVALUATE-FOR leading baselines. accuracy EVALUATE-FOR classification task. MSE EVALUATE-FOR forecasting task. accuracy EVALUATE-FOR leading baselines. datasets EVALUATE-FOR forecasting task. forecasting task EVALUATE-FOR leading baselines. datasets EVALUATE-FOR leading baselines. classification task EVALUATE-FOR forecasting task. datasets EVALUATE-FOR classification task. Method are contrastive learning approaches, information theory, and contrastive representation learning. Material is image and language domains. OtherScientificTerm is sub - optimal solutions. ","This paper studies the problem of contrastive learning for time series data. In contrast to previous work, the authors focus on augmentations to augmentations, which are commonly used in contrastive approaches. The authors propose a meta-learning mechanism, InfoTS, which is an information-aware approach that learns an encoder to learn robust and discriminative representations from augmentations.  The authors show that augmentations can be used to improve the generalization performance of existing time series augmentations by ad-hoc manual selection. They also show that the augmented samples can be selected using a rule of thumb based on prefabricated human priors.    The main contribution of the paper is that InfoTS is a novel approach to augmenting augmentations of time-series data to improve performance on contrastive representation learning. The paper shows that the meta-learner and the encoder can be trained in an end-to-end manner. The leading baselines are evaluated on three datasets for a classification task, a forecasting task, and a MSE task, where InfoTS achieves better accuracy than the best of the best. In the image and language domains, it is shown that the augmentations are more robust to sub-optimal solutions.","This paper studies the problem of contrastive learning for time series data. In contrast to previous work, the authors focus on augmentations to augmentations, which are commonly used in contrastive approaches. The authors propose a meta-learning mechanism, InfoTS, which is an information-aware approach that learns an encoder to learn robust and discriminative representations from augmentations.  The authors show that augmentations can be used to improve the generalization performance of existing time series augmentations by ad-hoc manual selection. They also show that the augmented samples can be selected using a rule of thumb based on prefabricated human priors.    The main contribution of the paper is that InfoTS is a novel approach to augmenting augmentations of time-series data to improve performance on contrastive representation learning. The paper shows that the meta-learner and the encoder can be trained in an end-to-end manner. The leading baselines are evaluated on three datasets for a classification task, a forecasting task, and a MSE task, where InfoTS achieves better accuracy than the best of the best. In the image and language domains, it is shown that the augmentations are more robust to sub-optimal solutions."
4997,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"winning tickets PART-OF task. theoretical physics FEATURE-OF renormalization group theory. iterative magnitude pruning USED-FOR winning tickets. method USED-FOR winning tickets. iterative magnitude pruning HYPONYM-OF method. iterative magnitude pruning HYPONYM-OF renormalization group scheme. numerical and theoretical tools USED-FOR winning ticket universality. winning ticket universality FEATURE-OF large scale lottery ticket experiments. Task are Lottery Ticket Hypothesis, and machine learning. Generic is tasks. ","This paper studies the Lottery Ticket Hypothesis, which claims that winning tickets in a task can be universal across tasks. The authors draw inspiration from renormalization group theory in theoretical physics and propose a novel method, iterative magnitude pruning, to prune winning tickets from a given task. They also provide numerical and theoretical tools to study the winning ticket universality in large scale lottery ticket experiments.    The paper is well-written, well-motivated, and well-structured. It is a well-designed paper that draws inspiration from recent advances in machine learning. The main contribution of the paper is to propose a simple and effective method to find winning tickets, which is based on a renormalized group scheme, namely iterative magnitudes pruning. The paper also provides a theoretical analysis of this method.","This paper studies the Lottery Ticket Hypothesis, which claims that winning tickets in a task can be universal across tasks. The authors draw inspiration from renormalization group theory in theoretical physics and propose a novel method, iterative magnitude pruning, to prune winning tickets from a given task. They also provide numerical and theoretical tools to study the winning ticket universality in large scale lottery ticket experiments.    The paper is well-written, well-motivated, and well-structured. It is a well-designed paper that draws inspiration from recent advances in machine learning. The main contribution of the paper is to propose a simple and effective method to find winning tickets, which is based on a renormalized group scheme, namely iterative magnitudes pruning. The paper also provides a theoretical analysis of this method."
5013,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"Transformer - based models USED-FOR information retrieval problem. BERT USED-FOR information retrieval problem. BERT HYPONYM-OF Transformer - based models. joint embedding USED-FOR cross - attention ( CA ) models. separate embeddings USED-FOR dual - encoder ( DE ) models. cross - attention ( CA ) models HYPONYM-OF models. DE models USED-FOR scores. real - world problems EVALUATE-FOR DE models. CA COMPARE DE models. DE models COMPARE CA. real - world problems EVALUATE-FOR CA. benchmark neural re - ranking datasets EVALUATE-FOR distillation strategy. Method are CA models, and cross - attention. ","This paper studies the information retrieval problem in Transformer-based models such as BERT. The authors propose two models, cross-attention (CA) models and dual-encoder (DE) models, where the joint embedding is shared across all layers and the separate embeddings are used for each layer. They show that CA models are able to achieve better scores than DE models on a number of real-world problems. They also propose a distillation strategy to improve the performance of CA compared to DE models. Experiments are conducted on benchmark neural re-ranking datasets and show that the proposed CA outperforms DE models in most of the cases. ","This paper studies the information retrieval problem in Transformer-based models such as BERT. The authors propose two models, cross-attention (CA) models and dual-encoder (DE) models, where the joint embedding is shared across all layers and the separate embeddings are used for each layer. They show that CA models are able to achieve better scores than DE models on a number of real-world problems. They also propose a distillation strategy to improve the performance of CA compared to DE models. Experiments are conducted on benchmark neural re-ranking datasets and show that the proposed CA outperforms DE models in most of the cases. "
5029,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"Off - policy methods USED-FOR Policy Optimization ( PO ) algorithms. IS PART-OF Monte Carlo simulation. variance minimization approach USED-FOR IS. variance minimization approach PART-OF Monte Carlo simulation. behavioral distribution USED-FOR sampling. variance minimization USED-FOR performance improvement tool. variance minimization COMPARE off - policy learning. off - policy learning COMPARE variance minimization. PO algorithm USED-FOR Policy Optimization. variance minimization USED-FOR Policy Optimization. Optimal Policy Evaluation ( POPE ) USED-FOR Policy Optimization. small batch sizes FEATURE-OF robustness. Method is Importance Sampling ( IS ). OtherScientificTerm are behavioral policy, and trust region. Material is continuous RL benchmarks. ","This paper proposes a new off-policy methods for Policy Optimization (PO) algorithms based on Importance Sampling (IS). IS is a variance minimization approach in Monte Carlo simulation where a behavioral policy is sampled from a behavioral distribution, and the sampling is based on a trust region of the behavioral distribution. The paper shows that variance minimisation can be used as a performance improvement tool to improve the performance of the PO algorithm. The authors also show that the proposed policy optimizer, Policy Optimized with Optimal Policy Evaluation (POPE), is more robust to robustness to small batch sizes on continuous RL benchmarks.   ","This paper proposes a new off-policy methods for Policy Optimization (PO) algorithms based on Importance Sampling (IS). IS is a variance minimization approach in Monte Carlo simulation where a behavioral policy is sampled from a behavioral distribution, and the sampling is based on a trust region of the behavioral distribution. The paper shows that variance minimisation can be used as a performance improvement tool to improve the performance of the PO algorithm. The authors also show that the proposed policy optimizer, Policy Optimized with Optimal Policy Evaluation (POPE), is more robust to robustness to small batch sizes on continuous RL benchmarks.   "
5045,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"Graph Neural Networks ( GNNs ) USED-FOR atomic simulations. Graph Neural Networks ( GNNs ) USED-FOR catalyst discovery. GNNs USED-FOR task. triplets CONJUNCTION quadruplets of atoms. quadruplets of atoms CONJUNCTION triplets. they USED-FOR higher - order interactions. higher - order interactions PART-OF graphs. method USED-FOR GNNs. method USED-FOR graphs. GPUs USED-FOR graphs. force MAE metric EVALUATE-FOR S2EF task. AFbT metric EVALUATE-FOR IS2RS task. force MAE metric EVALUATE-FOR graph - parallelized models. OtherScientificTerm is climate change. Generic is models. Method are Graph Parallelism, and DimeNet++ and GemNet models. ","Graph Neural Networks (GNNs) have been widely used for atomic simulations, and the authors propose Graph Parallelism, a method to parallelize GNNs for the task of catalyst discovery, which is an important problem in the context of climate change. The authors propose a method for parallelizing graphs on GPUs, and show that they are able to capture higher-order interactions in the graphs, such as triplets and quadruplets of atoms. They also show that graph-parallelized models can achieve a better force MAE metric on the S2EF task, and improve the AFbT metric for the IS2RS task. The paper also shows that the DimeNet++ and GemNet models can be parallelized in a similar way. ","Graph Neural Networks (GNNs) have been widely used for atomic simulations, and the authors propose Graph Parallelism, a method to parallelize GNNs for the task of catalyst discovery, which is an important problem in the context of climate change. The authors propose a method for parallelizing graphs on GPUs, and show that they are able to capture higher-order interactions in the graphs, such as triplets and quadruplets of atoms. They also show that graph-parallelized models can achieve a better force MAE metric on the S2EF task, and improve the AFbT metric for the IS2RS task. The paper also shows that the DimeNet++ and GemNet models can be parallelized in a similar way. "
5061,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"language models USED-FOR text generation tasks. meandering CONJUNCTION incoherent content. incoherent content CONJUNCTION meandering. incoherent content HYPONYM-OF structural flaws. meandering HYPONYM-OF structural flaws. Time Control ( TC ) HYPONYM-OF language model. latent stochastic process USED-FOR language model. representation USED-FOR TC. representation USED-FOR language model. stochastic process USED-FOR document plan. text infilling CONJUNCTION discourse coherence. discourse coherence CONJUNCTION text infilling. domain - specific methods COMPARE TC. TC COMPARE domain - specific methods. domain - specific methods CONJUNCTION fine - tuning GPT2. fine - tuning GPT2 CONJUNCTION domain - specific methods. fine - tuning GPT2 COMPARE TC. TC COMPARE fine - tuning GPT2. text domains EVALUATE-FOR fine - tuning GPT2. discourse coherence EVALUATE-FOR TC. text infilling EVALUATE-FOR TC. ordering CONJUNCTION text length consistency. text length consistency CONJUNCTION ordering. long text generation settings EVALUATE-FOR TC. TC USED-FOR text structure. OtherScientificTerm are stochastic process of interest, and latent plan. ","This paper proposes a new language model called Time Control (TC) that learns a latent stochastic process of interest, which can be used to train language models for text generation tasks with structural flaws such as meandering or incoherent content. The authors show that the language model trained with this representation is able to learn a language model that can be fine-tuned to a specific task. The key idea is to use the latent process as a document plan to guide the learning of the latent plan, and then use this representation as a representation to fine-tune TC. Experiments on text infilling, discourse coherence, and text length consistency show that TC outperforms other domain-specific methods, and fine -tuning GPT2 on a number of text domains. TC is also able to improve text structure in long text generation settings by improving ordering, length consistency, etc.  ","This paper proposes a new language model called Time Control (TC) that learns a latent stochastic process of interest, which can be used to train language models for text generation tasks with structural flaws such as meandering or incoherent content. The authors show that the language model trained with this representation is able to learn a language model that can be fine-tuned to a specific task. The key idea is to use the latent process as a document plan to guide the learning of the latent plan, and then use this representation as a representation to fine-tune TC. Experiments on text infilling, discourse coherence, and text length consistency show that TC outperforms other domain-specific methods, and fine -tuning GPT2 on a number of text domains. TC is also able to improve text structure in long text generation settings by improving ordering, length consistency, etc.  "
5077,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"representation of objects USED-FOR higher - level concepts. framework USED-FOR object - centric representation. single 2D images USED-FOR object - centric representation. object - centric models COMPARE model. model COMPARE object - centric models. model USED-FOR segmenting objects. network USED-FOR latent code space. geometric shape CONJUNCTION texture / color. texture / color CONJUNCTION geometric shape. supervision USED-FOR model. specificity FEATURE-OF object segmentation. predictive learning USED-FOR models. approach USED-FOR symbolic representation. OtherScientificTerm are prediction error of future sensory input, moving objects, latent causes, 3D environment, and clustering colors. Material is synthetic dataset. ","This paper proposes a framework to learn an object-centric representation from single 2D images. The key idea is to learn a representation of objects that captures higher-level concepts such as geometric shape, texture/color, etc. The paper shows that the representation learned by the proposed framework is more interpretable than existing object-centric models, and can be used for segmenting objects in a 3D environment. The model is trained without any supervision on the prediction error of future sensory input, and is able to generalize well to moving objects.    The paper presents a synthetic dataset where objects are represented as latent causes, and a network is trained to learn the latent code space for each object in the scene, which is then used to train a model that can be applied to a sequence of objects. The proposed approach learns a symbolic representation that is invariant to changes in geometric shape and texture/colors, and that is interpretable. The authors also show that the proposed approach can be combined with predictive learning to learn models that can generalize to unseen objects. They show that their approach is more robust to changes of geometric shape/color than existing models. They also demonstrate that their model can generalise to a more complex scene (e.g. from geometric shape to clustering colors).  ","This paper proposes a framework to learn an object-centric representation from single 2D images. The key idea is to learn a representation of objects that captures higher-level concepts such as geometric shape, texture/color, etc. The paper shows that the representation learned by the proposed framework is more interpretable than existing object-centric models, and can be used for segmenting objects in a 3D environment. The model is trained without any supervision on the prediction error of future sensory input, and is able to generalize well to moving objects.    The paper presents a synthetic dataset where objects are represented as latent causes, and a network is trained to learn the latent code space for each object in the scene, which is then used to train a model that can be applied to a sequence of objects. The proposed approach learns a symbolic representation that is invariant to changes in geometric shape and texture/colors, and that is interpretable. The authors also show that the proposed approach can be combined with predictive learning to learn models that can generalize to unseen objects. They show that their approach is more robust to changes of geometric shape/color than existing models. They also demonstrate that their model can generalise to a more complex scene (e.g. from geometric shape to clustering colors).  "
5093,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,"traffic management CONJUNCTION public safety. public safety CONJUNCTION traffic management. Spatio - temporal ( ST ) prediction task USED-FOR traffic management. Spatio - temporal ( ST ) prediction task USED-FOR public safety. mobility forecasting USED-FOR traffic management. mobility forecasting HYPONYM-OF Spatio - temporal ( ST ) prediction task. spatial and temporal domains FEATURE-OF features. independent variables PART-OF latent representation. semantic factors FEATURE-OF independent variables. It USED-FOR mobility forecasting models. It USED-FOR spatial and temporal features. VAE - based architecture USED-FOR disentangled representation. real spatio - temporal data USED-FOR mobility forecasting. real spatio - temporal data USED-FOR disentangled representation. deep generative model USED-FOR latent representation. temporal dynamics CONJUNCTION spatially varying component. spatially varying component CONJUNCTION temporal dynamics. deep generative model USED-FOR reconstructions. non - informative features USED-FOR method. Task is mobility forecasting problems. Generic are they, and models. Method are dynamic and static components, and Disentangled representation learning. Material is spatio - temporal datasets. ","This paper proposes a new method for learning a disentangled representation of spatial and temporal features for mobility forecasting problems. Spatio-temporal (ST) prediction task in mobility forecasting is an important problem in both traffic management and public safety, and mobility forecasting for traffic forecasting is a special case of this problem. The paper proposes to disentangle the features in both spatial and time-series domains. The authors propose a VAE-based architecture to learn the disentangling representation. It disentangles the dynamic and static components of a latent representation, which is composed of independent variables that depend on the semantic factors of the data. It can be applied to existing mobility forecasting models, and is shown to improve their performance when they are trained on real spatio-temporally data. Disentangled representations are also shown to be useful for reconstructions from a deep generative model, which can be used to learn from real-world data. The proposed method is evaluated on a number of different datasets, and the results show that the proposed method can learn from non-informative features that are not present in existing models. The results also show that this method is able to learn a good disentanglement between the temporal dynamics and the spatially varying component. The experiments are conducted on two different types of datasets, both of which have been designed for the purpose of mobility forecasting.    The paper is well-written and well-motivated. The idea is interesting and the experiments are well-designed. However, there are some issues that need to be addressed in the paper, and it would be good to see more experiments on more realistic and more realistic spatiotemporal datasets (e.g. more realistic data). ","This paper proposes a new method for learning a disentangled representation of spatial and temporal features for mobility forecasting problems. Spatio-temporal (ST) prediction task in mobility forecasting is an important problem in both traffic management and public safety, and mobility forecasting for traffic forecasting is a special case of this problem. The paper proposes to disentangle the features in both spatial and time-series domains. The authors propose a VAE-based architecture to learn the disentangling representation. It disentangles the dynamic and static components of a latent representation, which is composed of independent variables that depend on the semantic factors of the data. It can be applied to existing mobility forecasting models, and is shown to improve their performance when they are trained on real spatio-temporally data. Disentangled representations are also shown to be useful for reconstructions from a deep generative model, which can be used to learn from real-world data. The proposed method is evaluated on a number of different datasets, and the results show that the proposed method can learn from non-informative features that are not present in existing models. The results also show that this method is able to learn a good disentanglement between the temporal dynamics and the spatially varying component. The experiments are conducted on two different types of datasets, both of which have been designed for the purpose of mobility forecasting.    The paper is well-written and well-motivated. The idea is interesting and the experiments are well-designed. However, there are some issues that need to be addressed in the paper, and it would be good to see more experiments on more realistic and more realistic spatiotemporal datasets (e.g. more realistic data). "
5109,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"deep learning framework USED-FOR probabilistic interpolation of irregularly sampled time series. temporal VAE architecture USED-FOR uncertainty. heteroscedastic output layer USED-FOR variable uncertainty. input layer CONJUNCTION temporal VAE architecture. temporal VAE architecture CONJUNCTION input layer. temporal VAE architecture CONJUNCTION heteroscedastic output layer. heteroscedastic output layer CONJUNCTION temporal VAE architecture. heteroscedastic output layer USED-FOR output interpolations. variable uncertainty FEATURE-OF output interpolations. input layer USED-FOR input observation sparsity. temporal VAE architecture PART-OF HeTVAE. input layer PART-OF HeTVAE. heteroscedastic output layer PART-OF HeTVAE. architecture COMPARE deep latent variable models. deep latent variable models COMPARE architecture. homoscedastic output layers USED-FOR deep latent variable models. Material is Irregularly sampled time series. Method are deep learning models, Heteroscedastic Temporal Variational Autoencoder ( HeTVAE ), and sparse and irregular sampling. OtherScientificTerm is input sparsity. ","This paper proposes a new deep learning framework for probabilistic interpolation of irregularly sampled time series. Irregularized time series are a common problem in many deep learning models. The authors propose a Heteroscedastic Temporal Variational Autoencoder (HeTVAE) that is able to handle the problem of sparse and irregular sampling. HeTVAE consists of an input layer, a temporal VAE architecture to capture the uncertainty of the input, and a heteroscedatic output layer to capture variable uncertainty in the output interpolations. The input layer is used to mitigate input observation sparsity, and the temporalVAE architecture is used for the input sparsity. Experiments show that the proposed architecture outperforms other deep latent variable models with homoscedrict output layers.","This paper proposes a new deep learning framework for probabilistic interpolation of irregularly sampled time series. Irregularized time series are a common problem in many deep learning models. The authors propose a Heteroscedastic Temporal Variational Autoencoder (HeTVAE) that is able to handle the problem of sparse and irregular sampling. HeTVAE consists of an input layer, a temporal VAE architecture to capture the uncertainty of the input, and a heteroscedatic output layer to capture variable uncertainty in the output interpolations. The input layer is used to mitigate input observation sparsity, and the temporalVAE architecture is used for the input sparsity. Experiments show that the proposed architecture outperforms other deep latent variable models with homoscedrict output layers."
5125,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"structure HYPONYM-OF computational graph. importance CONJUNCTION coherence. coherence CONJUNCTION importance. coherence HYPONYM-OF proxies. importance HYPONYM-OF proxies. statistical methods USED-FOR proxies. proxies USED-FOR partitionings. network weights CONJUNCTION correlations of activations. correlations of activations CONJUNCTION network weights. correlations of activations USED-FOR edges. spectrally clustering USED-FOR partitionings. network weights USED-FOR edges. ones HYPONYM-OF partitionings. weights USED-FOR partitionings. weights USED-FOR ones. graph - based partitioning USED-FOR modularity. graph - based partitioning USED-FOR deep neural networks. Method is neural network. OtherScientificTerm are functionality, and neurons. Task is non - runtime analysis. ","This paper considers the problem of partitioning a neural network into sub-networks that share the same functionality but differ in their structure (i.e., the computational graph). The authors propose two proxies, importance and coherence, which are based on statistical methods, to partitionings. The partitionings are obtained by spectrally clustering the weights of the sub-network and the edges of the network weights and the correlations of activations between the neurons. The authors show that the partitionings can be partitioned into two types of partitionings, the ones based on the weights and on the correlations between the weights. They also show that graph-based partitioning can be used to improve modularity in deep neural networks and provide a non-runtime analysis. ","This paper considers the problem of partitioning a neural network into sub-networks that share the same functionality but differ in their structure (i.e., the computational graph). The authors propose two proxies, importance and coherence, which are based on statistical methods, to partitionings. The partitionings are obtained by spectrally clustering the weights of the sub-network and the edges of the network weights and the correlations of activations between the neurons. The authors show that the partitionings can be partitioned into two types of partitionings, the ones based on the weights and on the correlations between the weights. They also show that graph-based partitioning can be used to improve modularity in deep neural networks and provide a non-runtime analysis. "
5141,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"mobile devices FEATURE-OF speech - interactive features. lottery ticket hypothesis USED-FOR sparse subnetworks. lottery ticket hypothesis USED-FOR lightweight speech recognition models. noise FEATURE-OF speech. CTC CONJUNCTION RNN - Transducer, and Transformer models. RNN - Transducer, and Transformer models CONJUNCTION CTC. winning tickets COMPARE full models. full models COMPARE winning tickets. sparsity USED-FOR noise robustness. Method are Lightweight speech recognition models, and speech models. Generic are systems, and full model. Task is open - world personalization. OtherScientificTerm are structured sparsity, backbones, full model weights, and background noises. ","Lightweight speech recognition models are becoming more and more popular in mobile devices due to their ability to capture speech-interactive features on mobile devices. However, these systems suffer from a lack of structured sparsity. This paper investigates the lottery ticket hypothesis for sparse subnetworks, which is a well-studied problem in open-world personalization. The authors show that lottery tickets can be used to train lightweight speech recognition systems, and that winning tickets are more likely to be used than full models. The paper also shows that the lottery tickets are not always won by the full model, but by a smaller number of winning tickets.   The authors also show that the winning tickets tend to be larger than the full models, which may be due to the fact that the systems are trained with different backbones.  The paper concludes with a series of experiments on CTC, RNN-Transducer, and Transformer models, where the authors find that the sparsity helps noise robustness to background noises, and the noise in speech is more sensitive to the amount of noise in the input speech. They also find that full model weights are more robust to noise in background noises.","Lightweight speech recognition models are becoming more and more popular in mobile devices due to their ability to capture speech-interactive features on mobile devices. However, these systems suffer from a lack of structured sparsity. This paper investigates the lottery ticket hypothesis for sparse subnetworks, which is a well-studied problem in open-world personalization. The authors show that lottery tickets can be used to train lightweight speech recognition systems, and that winning tickets are more likely to be used than full models. The paper also shows that the lottery tickets are not always won by the full model, but by a smaller number of winning tickets.   The authors also show that the winning tickets tend to be larger than the full models, which may be due to the fact that the systems are trained with different backbones.  The paper concludes with a series of experiments on CTC, RNN-Transducer, and Transformer models, where the authors find that the sparsity helps noise robustness to background noises, and the noise in speech is more sensitive to the amount of noise in the input speech. They also find that full model weights are more robust to noise in background noises."
5157,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"random weights USED-FOR Deep neural networks. skip connections CONJUNCTION Hadamard transforms. Hadamard transforms CONJUNCTION skip connections. skip connections USED-FOR ResNet architectures. Hadamard transforms USED-FOR ResNet architectures. batch normalization USED-FOR network training. random weights USED-FOR network initialization. image classification datasets EVALUATE-FOR ZerO. ImageNet HYPONYM-OF image classification datasets. OtherScientificTerm is stable signal propagation. Metric are variance, and reproducibility. Method are random weight initialization, and residual networks. ","Deep neural networks with random weights have been shown to suffer from stable signal propagation, and the variance of the random weight initialization can be very high. Deep neural networks trained with skip connections and Hadamard transforms have shown to be particularly vulnerable to this issue. This paper proposes ZerO, a method to improve the reproducibility of residual networks by using random weights for network initialization. Specifically, the authors propose to use skip connections or HadamARD transforms to train ResNet architectures, and then apply batch normalization during the network training. Experiments on several image classification datasets (e.g. ImageNet, CIFAR-10, ImageNet) show that ZerO performs well on average. ","Deep neural networks with random weights have been shown to suffer from stable signal propagation, and the variance of the random weight initialization can be very high. Deep neural networks trained with skip connections and Hadamard transforms have shown to be particularly vulnerable to this issue. This paper proposes ZerO, a method to improve the reproducibility of residual networks by using random weights for network initialization. Specifically, the authors propose to use skip connections or HadamARD transforms to train ResNet architectures, and then apply batch normalization during the network training. Experiments on several image classification datasets (e.g. ImageNet, CIFAR-10, ImageNet) show that ZerO performs well on average. "
5173,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,"minimax formulation USED-FOR backdoors. backdoors PART-OF poisoned model. minimax formulation USED-FOR poisoned model. clean data USED-FOR minimax formulation. backdoor removal PART-OF formulation. implicit hypergradient USED-FOR algorithm. robustness EVALUATE-FOR minimax. clean data USED-FOR minimax. I - BAU COMPARE state - ofart backdoor defenses. state - ofart backdoor defenses COMPARE I - BAU. state - ofart backdoor defenses USED-FOR backdoor attacks. I - BAU USED-FOR backdoor attacks. I - BAU COMPARE baseline. baseline COMPARE I - BAU. attack settings CONJUNCTION poison ratio. poison ratio CONJUNCTION attack settings. poison ratio CONJUNCTION clean data size. clean data size CONJUNCTION poison ratio. it COMPARE baseline. baseline COMPARE it. single - target attack setting EVALUATE-FOR baseline. single - target attack setting EVALUATE-FOR it. computation USED-FOR I - BAU. OtherScientificTerm is inner and outer optimization. Metric is convergence. Generic are its, and baselines. ","This paper proposes a minimax formulation for backdoors in a poisoned model, where the poisoned model is trained on clean data. The authors propose an algorithm, called I-BAU, that combines backdoor removal with backdoor removal in the formulation. The algorithm is based on implicit hypergradient, and the authors show that the minimax with clean data improves the robustness against backdoor attacks. They also show that I- BAU outperforms state-ofart backdoor defenses in the single-target attack setting, and outperforms a baseline in the multi-target setting.    The authors also provide a theoretical analysis of the convergence of the algorithm, showing that the inner and outer optimization converges to a stationary point, and that its convergence is guaranteed to be tight. The paper also shows that under certain attack settings, poison ratio, and clean data size, the algorithm converges faster than its baselines. Finally, the paper shows that the computation is also faster than the baseline, and it is more robust to backdoor attacks under a few attack settings. ","This paper proposes a minimax formulation for backdoors in a poisoned model, where the poisoned model is trained on clean data. The authors propose an algorithm, called I-BAU, that combines backdoor removal with backdoor removal in the formulation. The algorithm is based on implicit hypergradient, and the authors show that the minimax with clean data improves the robustness against backdoor attacks. They also show that I- BAU outperforms state-ofart backdoor defenses in the single-target attack setting, and outperforms a baseline in the multi-target setting.    The authors also provide a theoretical analysis of the convergence of the algorithm, showing that the inner and outer optimization converges to a stationary point, and that its convergence is guaranteed to be tight. The paper also shows that under certain attack settings, poison ratio, and clean data size, the algorithm converges faster than its baselines. Finally, the paper shows that the computation is also faster than the baseline, and it is more robust to backdoor attacks under a few attack settings. "
5189,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"random permutations COMPARE with - replacement sampling. with - replacement sampling COMPARE random permutations. permutations COMPARE random. random COMPARE permutations. smooth second derivatives FEATURE-OF 1 - dimensional strongly convex functions. random permutations USED-FOR strongly convex functions. easy - to - construct permutations USED-FOR accelerated convergence. easy - to - construct permutations COMPARE random. random COMPARE easy - to - construct permutations. easy - to - construct permutations USED-FOR quadratic, strongly - convex functions. convergence characterization USED-FOR optimal permutations. Method is permutation - based SGD. Metric is convergence gap. ","This paper studies the convergence of permutation-based SGD. The authors show that for 1-dimensional strongly convex functions with smooth second derivatives, random permutations are faster than random with-replacement sampling. They also show that, for quadratic, strongly-convex functions, easy-to-construct permutations can lead to accelerated convergence. Finally, the authors provide a convergence characterization for optimal permutations. ","This paper studies the convergence of permutation-based SGD. The authors show that for 1-dimensional strongly convex functions with smooth second derivatives, random permutations are faster than random with-replacement sampling. They also show that, for quadratic, strongly-convex functions, easy-to-construct permutations can lead to accelerated convergence. Finally, the authors provide a convergence characterization for optimal permutations. "
5205,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,transformations USED-FOR flow models. method USED-FOR transformation layers. unique transformations USED-FOR transformation layers. mixed distribution USED-FOR architecture optimization. invertibility FEATURE-OF NF architecture. discrete space USED-FOR architecture optimization. global minimum FEATURE-OF approximate upper bound. approximate upper bound USED-FOR mixture NF. block - wise alternating optimization algorithm USED-FOR architecture optimization. block - wise alternating optimization algorithm USED-FOR deep flow models. architecture optimization USED-FOR deep flow models. Metric is performance - cost trade - offs. Method is flow architecture. ,"This paper proposes a method for learning transformation layers that are unique to flow models with different types of transformations. The key idea of the method is to learn transformation layers based on unique transformations, and then use architecture optimization on a mixed distribution to optimize the performance-cost trade-offs. The authors show that the invertibility of the NF architecture can be guaranteed in the discrete space. They also provide an approximate upper bound on the global minimum of the mixture NF, and show that a block-wise alternating optimization algorithm can be used for architecture optimization for deep flow models. ","This paper proposes a method for learning transformation layers that are unique to flow models with different types of transformations. The key idea of the method is to learn transformation layers based on unique transformations, and then use architecture optimization on a mixed distribution to optimize the performance-cost trade-offs. The authors show that the invertibility of the NF architecture can be guaranteed in the discrete space. They also provide an approximate upper bound on the global minimum of the mixture NF, and show that a block-wise alternating optimization algorithm can be used for architecture optimization for deep flow models. "
5221,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,self - supervised learning ( SSL ) methods USED-FOR representations. Intrinsic Dimension ( ID ) USED-FOR representation space. Intrinsic Dimension ( ID ) USED-FOR expressiveness. KNN classifier USED-FOR Kmeans cluster labels. Kmeans cluster labels USED-FOR held - out representations. learning speed EVALUATE-FOR KNN classifier. KNN classifier USED-FOR Cluster Learnability ( CL ). learning speed EVALUATE-FOR Cluster Learnability ( CL ). contrastive losses CONJUNCTION pretext tasks. pretext tasks CONJUNCTION contrastive losses. ID CONJUNCTION CL. CL CONJUNCTION ID. model architecture CONJUNCTION human labels. human labels CONJUNCTION model architecture. data augmentation CONJUNCTION model architecture. model architecture CONJUNCTION data augmentation. ID USED-FOR downstream classification. CL USED-FOR downstream classification. CL COMPARE techniques. techniques COMPARE CL. ID COMPARE techniques. techniques COMPARE ID. contrastive losses USED-FOR techniques. pretext tasks USED-FOR techniques. DeepCluster USED-FOR representations. modification USED-FOR DeepCluster. ImageNet benchmarks EVALUATE-FOR DeepCluster. intermediate checkpoints USED-FOR SSL algorithms. framework USED-FOR SSL algorithms. framework USED-FOR intermediate checkpoints. Generic is architectures. ,"This paper proposes Cluster Learnability (CL) to improve the learning speed of self-supervised learning (SSL) methods to learn representations. The core idea is to use Intrinsic Dimension (ID) to define the representation space and to use a KNN classifier to learn Kmeans cluster labels for the held-out representations. This modification is called DeepCluster. The paper shows that CL outperforms existing techniques based on contrastive losses and pretext tasks in terms of learning speed and expressiveness.   The paper also shows that the proposed CL is more robust to data augmentation, model architecture, and human labels than existing SSL algorithms based on ID, CL, and ID for downstream classification. Finally, the paper proposes a modification to the existing architectures to further improve the performance of CL and ID. The authors also propose a framework to train SSL algorithms with intermediate checkpoints. The experiments on the ImageNet benchmarks show that DeepCLuster is able to learn more expressive representations than existing methods based on the proposed modification. ","This paper proposes Cluster Learnability (CL) to improve the learning speed of self-supervised learning (SSL) methods to learn representations. The core idea is to use Intrinsic Dimension (ID) to define the representation space and to use a KNN classifier to learn Kmeans cluster labels for the held-out representations. This modification is called DeepCluster. The paper shows that CL outperforms existing techniques based on contrastive losses and pretext tasks in terms of learning speed and expressiveness.   The paper also shows that the proposed CL is more robust to data augmentation, model architecture, and human labels than existing SSL algorithms based on ID, CL, and ID for downstream classification. Finally, the paper proposes a modification to the existing architectures to further improve the performance of CL and ID. The authors also propose a framework to train SSL algorithms with intermediate checkpoints. The experiments on the ImageNet benchmarks show that DeepCLuster is able to learn more expressive representations than existing methods based on the proposed modification. "
5237,SP:4f5c00469e4425751db5efbc355085a5e8709def,"Deep neural networks USED-FOR adversarial examples. query efficiency EVALUATE-FOR black - box attacks. segmentation priors USED-FOR black - box attacks. salient region FEATURE-OF perturbations. query efficiency CONJUNCTION success rate. success rate CONJUNCTION query efficiency. imperceptibility performance EVALUATE-FOR blackbox attacks. segmentation priors USED-FOR blackbox attacks. Saliency Attack HYPONYM-OF gradient - free black - box attack. attacks USED-FOR perturbations. approach USED-FOR perturbations. approach USED-FOR detection - based defense. Task are blackbox setting, and adversarial attacks. Metric is imperceptibility. ","Deep neural networks can be used to generate adversarial examples that are imperceptible to humans in the blackbox setting, but imperceptibility performance of blackbox attacks on segmentation priors is not well studied. This paper proposes a gradient-free black-box attack, called the Saliency Attack, that aims to improve the query efficiency and the success rate of existing attacks on perturbations in the salient region. The authors also propose a detection-based defense based on the proposed approach, which is shown to be effective at detecting the perturbation. ","Deep neural networks can be used to generate adversarial examples that are imperceptible to humans in the blackbox setting, but imperceptibility performance of blackbox attacks on segmentation priors is not well studied. This paper proposes a gradient-free black-box attack, called the Saliency Attack, that aims to improve the query efficiency and the success rate of existing attacks on perturbations in the salient region. The authors also propose a detection-based defense based on the proposed approach, which is shown to be effective at detecting the perturbation. "
5253,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"Synthesis planning CONJUNCTION reaction outcome prediction. reaction outcome prediction CONJUNCTION Synthesis planning. Synthesis planning HYPONYM-OF computer - aided organic chemistry. reaction outcome prediction HYPONYM-OF computer - aided organic chemistry. Natural language approaches USED-FOR problem. Natural language approaches USED-FOR end - to - end formulation. Natural language approaches USED-FOR SMILES - to - SMILES translation. Transformer models USED-FOR text generation. Transformer models CONJUNCTION molecular graph encoders. molecular graph encoders CONJUNCTION Transformer models. molecular graph encoders USED-FOR Graph2SMILES model. Transformer models USED-FOR Graph2SMILES model. Graph2SMILES USED-FOR task. Transformer USED-FOR task. Graph2SMILES USED-FOR Transformer. molecule(s)-to - molecule(s ) transformations USED-FOR task. Graph2SMILES USED-FOR end - to - end architecture. global attention encoder USED-FOR long - range and intermolecular interactions. graph - aware positional embedding USED-FOR global attention encoder. top-1 accuracy EVALUATE-FOR Transformer baselines. Graph2SMILES COMPARE Transformer baselines. Transformer baselines COMPARE Graph2SMILES. reaction outcome prediction CONJUNCTION one - step retrosynthesis. one - step retrosynthesis CONJUNCTION reaction outcome prediction. reaction outcome prediction EVALUATE-FOR Transformer baselines. USPTO_480k and USPTO_STEREO datasets USED-FOR reaction outcome prediction. reaction outcome prediction EVALUATE-FOR Graph2SMILES. USPTO_50k dataset USED-FOR one - step retrosynthesis. top-1 accuracy EVALUATE-FOR Graph2SMILES. Method are data - driven approaches, machine translation model architectures, SMILES representations, SMILES augmentation, and encoder. Task is data preprocessing. OtherScientificTerm are molecular structures, input data augmentation, and local chemical environments. ","Synthesis planning and reaction outcome prediction is an important problem in computer-aided organic chemistry. Natural language approaches to this problem have been used in the SMILES-to-SMILES translation. However, data-driven approaches are expensive and time-consuming to train. This paper tackles this problem by proposing a novel end- to-end formulation.   Synthesis planning is a well-studied problem in the field of machine translation model architectures, and this paper proposes a new task of molecule(s)-to-molecule(s) transformations, where the goal is to translate molecular structures to molecular structures.  The authors use Transformer models for text generation and molecular graph encoders to train a Graph2SMILESS model, which is a generalization of the Transformer model.  In the paper, the authors propose a new end-of-the-art architecture based on the graph-aware positional embedding in Graph2SmILES.  They propose a global attention encoder to capture long-range and intermolecular interactions, and a new global attention embedding to capture the global information in the input data augmentation.  This paper also proposes a way to augment input data to improve the quality of SMILes representations.  To do so, they augment the input to the encoder with the input of the molecule (s) to be transformed into the target molecule.  Experiments are conducted on the USPTO_480k and USPto_STEREO datasets for the task of chemical synthesis, and the authors compare the performance of Transformer baselines on the top-1 accuracy and top-2 accuracy on the task to the performance improvement of Graph2smILES compared to the state of the art Transformer based baselines. The authors also conduct experiments on a one-step retrosynthesis task on a modified version of the USpTO_50k dataset, and show that Graph2 SMILESS achieves better performance than the state-of the-art Transformer on the tasks of chemical synthesizing molecules from a single molecule and a single reaction. They also perform experiments on two local chemical environments, where they show that their Graph2SILES model is able to perform better than Transformer. ","Synthesis planning and reaction outcome prediction is an important problem in computer-aided organic chemistry. Natural language approaches to this problem have been used in the SMILES-to-SMILES translation. However, data-driven approaches are expensive and time-consuming to train. This paper tackles this problem by proposing a novel end- to-end formulation.   Synthesis planning is a well-studied problem in the field of machine translation model architectures, and this paper proposes a new task of molecule(s)-to-molecule(s) transformations, where the goal is to translate molecular structures to molecular structures.  The authors use Transformer models for text generation and molecular graph encoders to train a Graph2SMILESS model, which is a generalization of the Transformer model.  In the paper, the authors propose a new end-of-the-art architecture based on the graph-aware positional embedding in Graph2SmILES.  They propose a global attention encoder to capture long-range and intermolecular interactions, and a new global attention embedding to capture the global information in the input data augmentation.  This paper also proposes a way to augment input data to improve the quality of SMILes representations.  To do so, they augment the input to the encoder with the input of the molecule (s) to be transformed into the target molecule.  Experiments are conducted on the USPTO_480k and USPto_STEREO datasets for the task of chemical synthesis, and the authors compare the performance of Transformer baselines on the top-1 accuracy and top-2 accuracy on the task to the performance improvement of Graph2smILES compared to the state of the art Transformer based baselines. The authors also conduct experiments on a one-step retrosynthesis task on a modified version of the USpTO_50k dataset, and show that Graph2 SMILESS achieves better performance than the state-of the-art Transformer on the tasks of chemical synthesizing molecules from a single molecule and a single reaction. They also perform experiments on two local chemical environments, where they show that their Graph2SILES model is able to perform better than Transformer. "
5269,SP:ce3cde67564679a8d9a0539f1e12551390b91475,reinforcement learning ( RL ) methods USED-FOR task - oriented dialogues setting. task - oriented dialogues setting USED-FOR automatic disease diagnosis. reinforcement learning ( RL ) methods USED-FOR automatic disease diagnosis. RL tasks COMPARE action space. action space COMPARE RL tasks. action space FEATURE-OF disease diagnosis. approaches USED-FOR problem. hierarchical policy PART-OF dialogue policy learning. symptom checkers CONJUNCTION disease classifier. disease classifier CONJUNCTION symptom checkers. master model USED-FOR low level model. low level policy PART-OF high level policy. disease classifier PART-OF low level policy. symptom checkers PART-OF low level policy. master model PART-OF high level policy. hierarchical framework COMPARE systems. systems COMPARE hierarchical framework. accuracy CONJUNCTION symptom recall. symptom recall CONJUNCTION accuracy. symptom recall EVALUATE-FOR systems. accuracy EVALUATE-FOR systems. disease diagnosis EVALUATE-FOR systems. hierarchical framework USED-FOR disease diagnosis. symptom recall EVALUATE-FOR hierarchical framework. accuracy EVALUATE-FOR hierarchical framework. Task is offline consultation process. ,"This paper proposes a hierarchical approach to tackle the problem of automatic disease diagnosis using reinforcement learning (RL) methods in the task-oriented dialogues setting. The authors argue that the offline consultation process can be problematic due to the lack of action space in the RL tasks compared to the action space of disease diagnosis. To address this problem, the authors propose two approaches to tackle this problem. The first approach is to learn a hierarchical policy in dialogue policy learning, where the high level policy is a master model, and the low level policy consists of a set of symptom checkers and a disease classifier. The second approach is a hierarchical framework where the master model is used to train a low level model. The hierarchical framework is compared to other systems on accuracy and symptom recall on disease diagnosis and shows that the proposed hierarchical framework achieves better accuracy and better symptom recall. ","This paper proposes a hierarchical approach to tackle the problem of automatic disease diagnosis using reinforcement learning (RL) methods in the task-oriented dialogues setting. The authors argue that the offline consultation process can be problematic due to the lack of action space in the RL tasks compared to the action space of disease diagnosis. To address this problem, the authors propose two approaches to tackle this problem. The first approach is to learn a hierarchical policy in dialogue policy learning, where the high level policy is a master model, and the low level policy consists of a set of symptom checkers and a disease classifier. The second approach is a hierarchical framework where the master model is used to train a low level model. The hierarchical framework is compared to other systems on accuracy and symptom recall on disease diagnosis and shows that the proposed hierarchical framework achieves better accuracy and better symptom recall. "
5285,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,Federated Learning ( FL ) USED-FOR ML training ecosystem. distributed training USED-FOR data privacy. edge devices USED-FOR distributed training. FL COMPARE centralized training. centralized training COMPARE FL. Addressing label deficiency PART-OF FL. framework USED-FOR algorithms. SSFL ) HYPONYM-OF self - supervised and personalized federated learning framework. centralized self - supervised learning methods USED-FOR FL setting. SimSiam networks COMPARE FedAvg algorithm. FedAvg algorithm COMPARE SimSiam networks. Ditto CONJUNCTION local fine - tuning. local fine - tuning CONJUNCTION Ditto. perFedAvg CONJUNCTION Ditto. Ditto CONJUNCTION perFedAvg. algorithms USED-FOR supervised personalization algorithms. algorithms USED-FOR self - supervised learning. supervised personalization algorithms USED-FOR self - supervised learning. local fine - tuning HYPONYM-OF self - supervised learning. perFedAvg HYPONYM-OF self - supervised learning. Ditto HYPONYM-OF self - supervised learning. personalization CONJUNCTION consensus. consensus CONJUNCTION personalization. Per - SSFL USED-FOR personalization. Per - SSFL HYPONYM-OF personalized federated self - supervised learning algorithm. distributed training system USED-FOR SSFL. distributed training system CONJUNCTION evaluation protocol. evaluation protocol CONJUNCTION distributed training system. evaluation protocol USED-FOR SSFL. supervised learning CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION supervised learning. unsupervised learning USED-FOR FL. evaluation accuracy EVALUATE-FOR unsupervised learning. synthetic non - I.I.D. dataset CONJUNCTION intrinsically non - I.I.D. dataset. intrinsically non - I.I.D. dataset CONJUNCTION synthetic non - I.I.D. dataset. training system USED-FOR synthetic non - I.I.D. dataset. evaluation accuracy EVALUATE-FOR supervised learning. supervised learning USED-FOR FL. CIFAR-10 USED-FOR synthetic non - I.I.D. dataset. batch size CONJUNCTION non-I.I.D.ness. non-I.I.D.ness CONJUNCTION batch,"This paper proposes a new federated learning framework, called SSFL, which is a federated self-supervised learning (FL) framework that aims to address the problem of label deficiency in the ML training landscape. The authors propose a new algorithm, called Per-SSFL, to address this problem. The proposed algorithm is based on the SimSiam network, and the authors show that the proposed algorithm outperforms the state-of-the-art FedAvg and Ditto algorithms in terms of accuracy and privacy. The paper also shows that SSFL can be applied to both centralized and federated settings.","This paper proposes a new federated learning framework, called SSFL, which is a federated self-supervised learning (FL) framework that aims to address the problem of label deficiency in the ML training landscape. The authors propose a new algorithm, called Per-SSFL, to address this problem. The proposed algorithm is based on the SimSiam network, and the authors show that the proposed algorithm outperforms the state-of-the-art FedAvg and Ditto algorithms in terms of accuracy and privacy. The paper also shows that SSFL can be applied to both centralized and federated settings."
5301,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"deep neural networks ( DNN ) CONJUNCTION partial differential equations ( PDEs ). partial differential equations ( PDEs ) CONJUNCTION deep neural networks ( DNN ). PDEs USED-FOR DNN architectures. PDEs USED-FOR ResNet - like DNN. adjustment operator USED-FOR DNN. adjustment operator USED-FOR ResNet - like DNN. adjustment operator USED-FOR PDEs. training method USED-FOR DNN models. PDEs theory USED-FOR training method. robustness EVALUATE-FOR training method. training method USED-FOR networks. PDEs USED-FOR networks. training method USED-FOR DNN. robustness EVALUATE-FOR DNN. generalization gap FEATURE-OF DNN. training method USED-FOR generalization gap. method USED-FOR DNN. DNN COMPARE baseline model. baseline model COMPARE DNN. method USED-FOR DNN. generalization EVALUATE-FOR DNN. OtherScientificTerm are neural network design space, overfitting, and adversarial perturbations. Method is neural network structures. ","This paper studies the relationship between deep neural networks (DNNs) and partial differential equations (PDEs). The authors propose a training method based on PDEs theory to improve the robustness of DNN models to adversarial perturbations in the neural network design space. Specifically, the authors propose an adjustment operator for DNNs that can be applied to any PDE to adjust the weights of a ResNet-like DNN. The authors show that this training method can be used to train networks that are robust to adversarially perturbed inputs, and that the training method reduces the generalization gap of a DNN trained with the proposed training method.  The authors also show that the proposed method is able to reduce the gap between the trained DNN and a baseline model in terms of generalization performance.   The main contribution of the paper is that the authors provide a theoretical analysis that shows that the generalisation gap of the trained networks can be reduced by applying the adjustment operator. The paper also shows that, under some conditions, the networks trained with PDE’s can be trained with networks that have the same weights as those trained with neural network structures. ","This paper studies the relationship between deep neural networks (DNNs) and partial differential equations (PDEs). The authors propose a training method based on PDEs theory to improve the robustness of DNN models to adversarial perturbations in the neural network design space. Specifically, the authors propose an adjustment operator for DNNs that can be applied to any PDE to adjust the weights of a ResNet-like DNN. The authors show that this training method can be used to train networks that are robust to adversarially perturbed inputs, and that the training method reduces the generalization gap of a DNN trained with the proposed training method.  The authors also show that the proposed method is able to reduce the gap between the trained DNN and a baseline model in terms of generalization performance.   The main contribution of the paper is that the authors provide a theoretical analysis that shows that the generalisation gap of the trained networks can be reduced by applying the adjustment operator. The paper also shows that, under some conditions, the networks trained with PDE’s can be trained with networks that have the same weights as those trained with neural network structures. "
5317,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,deep learning models USED-FOR emergence of language. emergent language USED-FOR task. simulated agents USED-FOR emergent language. language games FEATURE-OF emergence of language. expressivity FEATURE-OF emergent languages. expressivity FEATURE-OF emergent languages. contrastive loss COMPARE referential loss. referential loss COMPARE contrastive loss. Generic is languages. Metric is complexity. OtherScientificTerm is message type collapse. ,This paper studies the emergence of language in language games using deep learning models. The authors show that emergent languages with high expressivity (i.e. languages that are able to express complex messages) are more likely to be learned by contrastive loss (compared to referential loss) than languages with low expressivity. They also show that the emergent language learned by simulated agents can be used to learn a task in which the complexity of the language is lower than the message type collapse.   ,This paper studies the emergence of language in language games using deep learning models. The authors show that emergent languages with high expressivity (i.e. languages that are able to express complex messages) are more likely to be learned by contrastive loss (compared to referential loss) than languages with low expressivity. They also show that the emergent language learned by simulated agents can be used to learn a task in which the complexity of the language is lower than the message type collapse.   
5333,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"exploration - exploitation dilemma PART-OF reinforcement learning. them USED-FOR reinforcement learning setting. exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. bandit setting FEATURE-OF Uncertainty - based exploration strategies. method USED-FOR exploration. Sample Average Uncertainty ( SAU ) USED-FOR exploration. Sample Average Uncertainty ( SAU ) USED-FOR method. exploration USED-FOR bandit problems. value predictions USED-FOR it. δ - exploration HYPONYM-OF exploration strategy. SAU USED-FOR sequential Reinforcement Learning scenario. bandits USED-FOR SAU. Deep Q - learning case EVALUATE-FOR δ - exploration. OtherScientificTerm are posterior distributions, and model posterior distributions. Task is generic sequential decision tasks. Method are reward models, and Reinforcement Learning. ","This paper studies the exploration-exploitation dilemma in reinforcement learning. Uncertainty-based exploration strategies are well known in the bandit setting, but it is difficult to apply them in the reinforcement learning setting. This paper proposes a method based on Sample Average Uncertainity (SAU) to improve exploration and exploitation in the context of generic sequential decision tasks.    The main idea is to use SAU in the sequential Reinforcement Learning scenario where the posterior distributions of the reward models are known and the model posterior distributions are not. In particular, the authors show that SAU can be used in the setting of bandits, and that it can be combined with value predictions to improve the performance of exploration in bandit problems. The authors also propose a new exploration strategy called δ-exploration, which is based on the observation that in the Deep Q-learning case, it is possible to learn a good exploration strategy.  The paper is well-written and well-motivated. ","This paper studies the exploration-exploitation dilemma in reinforcement learning. Uncertainty-based exploration strategies are well known in the bandit setting, but it is difficult to apply them in the reinforcement learning setting. This paper proposes a method based on Sample Average Uncertainity (SAU) to improve exploration and exploitation in the context of generic sequential decision tasks.    The main idea is to use SAU in the sequential Reinforcement Learning scenario where the posterior distributions of the reward models are known and the model posterior distributions are not. In particular, the authors show that SAU can be used in the setting of bandits, and that it can be combined with value predictions to improve the performance of exploration in bandit problems. The authors also propose a new exploration strategy called δ-exploration, which is based on the observation that in the Deep Q-learning case, it is possible to learn a good exploration strategy.  The paper is well-written and well-motivated. "
5349,SP:2f6e266b03939c96434834579999707d3268c5d6,"spatio - temporal complexity CONJUNCTION continuity of videos. continuity of videos CONJUNCTION spatio - temporal complexity. deep learning era USED-FOR long video generation. implicit neural representations ( INRs ) USED-FOR continuous signal. generative adversarial network USED-FOR video generation. motion discriminator USED-FOR unnatural motions. INR - based video generator USED-FOR motion dynamics. video extrapolation CONJUNCTION non - autoregressive video generation. non - autoregressive video generation CONJUNCTION video extrapolation. long video synthesis CONJUNCTION video extrapolation. video extrapolation CONJUNCTION long video synthesis. datasets EVALUATE-FOR DIGAN. long video synthesis HYPONYM-OF datasets. video extrapolation HYPONYM-OF datasets. UCF-101 EVALUATE-FOR FVD score. 128×128 resolution FEATURE-OF 128 frame videos. FVD score EVALUATE-FOR DIGAN. 128 frame videos USED-FOR DIGAN. UCF-101 EVALUATE-FOR DIGAN. OtherScientificTerm are video distribution, 3D grids of RGB values, scale of generated videos, continuous dynamics, INRs of video, and space and time coordinates. Method is parameterized neural network. Material is long frame sequences. ","This paper proposes a new method for long-range video generation based on implicit neural representations (INR). In particular, the authors propose a generative adversarial adversarial network (GAN) approach for long video generation. The proposed method, DIGAN, is based on the idea of implicit neural networks (INRs) that can be used to learn a continuous representation of the video distribution. The authors show that the proposed method is able to achieve state-of-the-art performance on three datasets: long video synthesis, video extrapolation, and non-autoregressive video generation, and achieves FVD score on UCF-101. ","This paper proposes a new method for long-range video generation based on implicit neural representations (INR). In particular, the authors propose a generative adversarial adversarial network (GAN) approach for long video generation. The proposed method, DIGAN, is based on the idea of implicit neural networks (INRs) that can be used to learn a continuous representation of the video distribution. The authors show that the proposed method is able to achieve state-of-the-art performance on three datasets: long video synthesis, video extrapolation, and non-autoregressive video generation, and achieves FVD score on UCF-101. "
5365,SP:878325384328c885ced7af0ebf31bbf79287c169,Private multi - winner voting USED-FOR revealing k - hot binary vectors. bounded differential privacy guarantee FEATURE-OF revealing k - hot binary vectors. task PART-OF machine learning literature. Binary HYPONYM-OF privacy - preserving multi - label mechanisms. Powerset voting HYPONYM-OF privacy - preserving multi - label mechanisms. composition USED-FOR Binary voting. ` 2 norm FEATURE-OF τ voting. binary vector USED-FOR Powerset voting. Powerset voting COMPARE Binary voting. Binary voting COMPARE Powerset voting. mechanisms USED-FOR privacy - preserving multi - label learning. canonical single - label technique USED-FOR mechanisms. PATE HYPONYM-OF canonical single - label technique. canonical single - label technique USED-FOR privacy - preserving multi - label learning. large real - world healthcare data CONJUNCTION multi - label benchmarks. multi - label benchmarks CONJUNCTION large real - world healthcare data. techniques COMPARE DPSGD. DPSGD COMPARE techniques. large real - world healthcare data EVALUATE-FOR DPSGD. multi - label benchmarks EVALUATE-FOR DPSGD. large real - world healthcare data EVALUATE-FOR techniques. multi - label benchmarks EVALUATE-FOR techniques. centralized setting EVALUATE-FOR techniques. mechanisms USED-FOR models. mechanisms USED-FOR multi - site ( distributed ) setting. multi - site ( distributed ) setting FEATURE-OF models. Material is healthcare. OtherScientificTerm is power set. Method is multi - label CaPC. ,"This paper studies the problem of privacy-preserving multi-label multi-winner voting in healthcare. Private multi-winning voting aims at revealing k-hot binary vectors with a bounded differential privacy guarantee. This is a well-studied task in the machine learning literature. Binary and Powerset voting are two popular and well-known algorithms for this task. Binary voting is based on the composition of a power set of candidates, and powerset voting uses the `2 norm of the power set to compute the winner. The authors propose two new mechanisms, namely, PATE and PATE+, that are based on a canonical single-label technique, to improve the performance of the two existing algorithms. They show that the proposed techniques outperform DPSGD on large real-world healthcare data and multi-labels benchmarks. They also show that these two mechanisms can be used to train models in a multi-site (distributed) setting, and that models trained with these mechanisms outperform those trained with DPSGD in a centralized setting.    The paper is well-written, well-motivated, and easy to follow. The paper also has a nice summary of the related work.  I have a few questions:   1. Is it possible to use the binary vector as a proxy for the winner in Powerset Voting?  2. What is the benefit of the proposed mechanisms?  3. How does the proposed mechanism compare to the state-of-the-art methods?   4. Is the proposed method (PATE) differentiable?  5. Can the proposed by the authors be used in a more general setting? ","This paper studies the problem of privacy-preserving multi-label multi-winner voting in healthcare. Private multi-winning voting aims at revealing k-hot binary vectors with a bounded differential privacy guarantee. This is a well-studied task in the machine learning literature. Binary and Powerset voting are two popular and well-known algorithms for this task. Binary voting is based on the composition of a power set of candidates, and powerset voting uses the `2 norm of the power set to compute the winner. The authors propose two new mechanisms, namely, PATE and PATE+, that are based on a canonical single-label technique, to improve the performance of the two existing algorithms. They show that the proposed techniques outperform DPSGD on large real-world healthcare data and multi-labels benchmarks. They also show that these two mechanisms can be used to train models in a multi-site (distributed) setting, and that models trained with these mechanisms outperform those trained with DPSGD in a centralized setting.    The paper is well-written, well-motivated, and easy to follow. The paper also has a nice summary of the related work.  I have a few questions:   1. Is it possible to use the binary vector as a proxy for the winner in Powerset Voting?  2. What is the benefit of the proposed mechanisms?  3. How does the proposed mechanism compare to the state-of-the-art methods?   4. Is the proposed method (PATE) differentiable?  5. Can the proposed by the authors be used in a more general setting? "
5381,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"dataset CONJUNCTION regularization. regularization CONJUNCTION dataset. architecture CONJUNCTION optimizer. optimizer CONJUNCTION architecture. batch size CONJUNCTION dataset. dataset CONJUNCTION batch size. optimizer CONJUNCTION batch size. batch size CONJUNCTION optimizer. optimizer CONJUNCTION learning rate schedule. learning rate schedule CONJUNCTION optimizer. tuned optimizer COMPARE optimizer. optimizer COMPARE tuned optimizer. grafting USED-FOR non - adaptive learning rate correction. non - adaptive learning rate correction USED-FOR SGD. non - adaptive learning rate correction USED-FOR BERT model. Method are large neural networks, optimizer grafting, optimizer hyperparameter search, and deep learning. OtherScientificTerm are implicit step size schedule, and empirical performance. Task is optimizer comparisons. ","This paper studies the problem of learning large neural networks. The authors propose to use optimizer grafting, which is an extension of optimizer hyperparameter search to the implicit step size schedule. The main idea is to use the empirical performance of a tuned optimizer as a proxy for the performance of the optimizer in terms of batch size, dataset, optimizer and learning rate schedule, and regularization. The paper shows that the non-adaptive learning rate correction of SGD with the help of the proposed grafting can be used to improve the BERT model performance.   ","This paper studies the problem of learning large neural networks. The authors propose to use optimizer grafting, which is an extension of optimizer hyperparameter search to the implicit step size schedule. The main idea is to use the empirical performance of a tuned optimizer as a proxy for the performance of the optimizer in terms of batch size, dataset, optimizer and learning rate schedule, and regularization. The paper shows that the non-adaptive learning rate correction of SGD with the help of the proposed grafting can be used to improve the BERT model performance.   "
5397,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"RL algorithms USED-FOR policy. algorithm USED-FOR online RL. algorithm USED-FOR offline demonstration data. sub - optimal behavior policy USED-FOR offline demonstration data. sparse reward settings FEATURE-OF online RL. policy improvement step CONJUNCTION policy guidance step. policy guidance step CONJUNCTION policy improvement step. offline demonstration data USED-FOR policy guidance step. LOGO USED-FOR policy. algorithm USED-FOR incomplete observation setting. censored version of the true state observation PART-OF demonstration data. sparse rewards CONJUNCTION censored state. censored state CONJUNCTION sparse rewards. algorithm COMPARE approaches. approaches COMPARE algorithm. censored state FEATURE-OF benchmark environments. sparse rewards FEATURE-OF benchmark environments. benchmark environments EVALUATE-FOR approaches. benchmark environments EVALUATE-FOR algorithm. LOGO USED-FOR obstacle avoidance. LOGO USED-FOR trajectory tracking. trajectory tracking CONJUNCTION obstacle avoidance. obstacle avoidance CONJUNCTION trajectory tracking. mobile robot USED-FOR trajectory tracking. mobile robot USED-FOR obstacle avoidance. mobile robot USED-FOR LOGO. LOGO USED-FOR approach. Task is real - world reinforcement learning ( RL ). OtherScientificTerm are sparsity of reward feedback, sparse reward function, fine grain feedback, exploration actions, feedback, guidance, sub - optimal policy, and learning episode. Material is offline data. Generic is it. ","This paper considers the problem of offline RL in sparse reward settings, where offline demonstrations are sparse and the reward function is sparse. The authors propose a new algorithm, called LOGO, to learn a policy from offline demonstrations that is robust to the sparsity of reward feedback. The algorithm is based on the idea that RL algorithms can learn a good policy when the offline demonstration data comes from a sub-optimal behavior policy, but not when there is a sparse reward function.    The paper proposes an algorithm that learns a policy that is both robust and robust to offline demonstrations. The key idea of the algorithm is to use offline demonstrations as guidance to guide the learning of a policy in the online RL setting. The policy is trained using a policy improvement step and a policy guidance step, where the policy is learned from offline demonstration observations. The paper shows that the policy learned by LOGO outperforms existing RL algorithms on a number of benchmark environments with sparse rewards and a censored state.  The authors also show that the algorithm can be applied to the incomplete observation setting, where there is no offline data, but there is offline demonstration. ","This paper considers the problem of offline RL in sparse reward settings, where offline demonstrations are sparse and the reward function is sparse. The authors propose a new algorithm, called LOGO, to learn a policy from offline demonstrations that is robust to the sparsity of reward feedback. The algorithm is based on the idea that RL algorithms can learn a good policy when the offline demonstration data comes from a sub-optimal behavior policy, but not when there is a sparse reward function.    The paper proposes an algorithm that learns a policy that is both robust and robust to offline demonstrations. The key idea of the algorithm is to use offline demonstrations as guidance to guide the learning of a policy in the online RL setting. The policy is trained using a policy improvement step and a policy guidance step, where the policy is learned from offline demonstration observations. The paper shows that the policy learned by LOGO outperforms existing RL algorithms on a number of benchmark environments with sparse rewards and a censored state.  The authors also show that the algorithm can be applied to the incomplete observation setting, where there is no offline data, but there is offline demonstration. "
5413,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"scalarization problem USED-FOR Pareto front. Linear Scalarization CONJUNCTION domain decomposition. domain decomposition CONJUNCTION Linear Scalarization. Multi - Task Learning ( MTL ) solvers USED-FOR Pareto solutions. Linear Scalarization PART-OF Multi - Task Learning ( MTL ) solvers. domain decomposition PART-OF Multi - Task Learning ( MTL ) solvers. Linear Scalarization USED-FOR Pareto solutions. MTL solvers USED-FOR real - world applications. non - convex functions CONJUNCTION constraints. constraints CONJUNCTION non - convex functions. Hybrid Neural Pareto Front ( HNPF ) USED-FOR non - convex functions. Hybrid Neural Pareto Front ( HNPF ) USED-FOR constraints. Hybrid Neural Pareto Front ( HNPF ) HYPONYM-OF two stage Pareto framework. Stage-1 neural network USED-FOR weak Pareto front. Fritz - John Conditions ( FJC ) USED-FOR Stage-1 neural network. FJC guided diffusive manifold USED-FOR weak Pareto front. low - cost Pareto filter USED-FOR strong Pareto subset. strong Pareto subset PART-OF weak front. low - cost Pareto filter USED-FOR weak front. Method is Fixed - point iterative strategies. OtherScientificTerm are convexity assumptions, Pareto definition, and convexity. Task is benchmarking and verification. Generic is approach. ","This paper considers the scalarization problem of the Pareto front, which is a generalization of the Multi-Task Learning (MTL) solvers that combines Linear Scalarization and domain decomposition. The authors propose a two-stage approach to solve the problem. In the first stage, a Stage-1 neural network is trained under the Fritz-John Conditions (FJC) to approximate the weak Pareta front, and in the second stage, the authors propose Hybrid Neural Paretto Front (HNPF), which combines non-convex functions and constraints, and is a variant of the two stage Paretopo framework. The proposed approach is evaluated on benchmarking and verification tasks, and the authors show that the proposed approach outperforms existing methods.  ","This paper considers the scalarization problem of the Pareto front, which is a generalization of the Multi-Task Learning (MTL) solvers that combines Linear Scalarization and domain decomposition. The authors propose a two-stage approach to solve the problem. In the first stage, a Stage-1 neural network is trained under the Fritz-John Conditions (FJC) to approximate the weak Pareta front, and in the second stage, the authors propose Hybrid Neural Paretto Front (HNPF), which combines non-convex functions and constraints, and is a variant of the two stage Paretopo framework. The proposed approach is evaluated on benchmarking and verification tasks, and the authors show that the proposed approach outperforms existing methods.  "
5429,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"library of diverse expert models COMPARE single generalist model. single generalist model COMPARE library of diverse expert models. library of diverse expert models USED-FOR task. single generalist model USED-FOR task. related task - specific teachers USED-FOR recognition tasks. related task - specific teachers USED-FOR consolidated image feature representation. student model PART-OF knowledge distillation. downstream transferability EVALUATE-FOR task - agnostic generalist model. distillation of task - specific teachers COMPARE task - agnostic generalist model. task - agnostic generalist model COMPARE distillation of task - specific teachers. teacher representations USED-FOR distillation of task - specific teachers. generalist teacher USED-FOR representations. generalist teacher PART-OF task - specific teacher(s ). unlabeled proxy dataset USED-FOR multihead, multi - task distillation method. task - specific teacher(s ) USED-FOR representations. multi - task joint training oracle EVALUATE-FOR method. Generic is library. OtherScientificTerm are teacher, and ImageNet pre - trained features. ","This paper proposes a library of diverse expert models that can be used to improve the downstream transferability of a single generalist model to a new task. The library consists of a collection of related task-specific teachers for different recognition tasks, and a consolidated image feature representation is learned by distilling the knowledge from the related task -specific teachers. The student model in knowledge distillation is trained in a similar way as the teacher. The authors show that the distillation of the teacher representations from a task-agnostic generalist teacher is more effective than distillation from the distilled representations from the task specific teacher. They also propose a multihead, multi-task distillation method based on an unlabeled proxy dataset. The proposed method is evaluated on a multi-tasks joint training oracle, where the teacher is a generalist and the student is a student. The paper also shows that the proposed method can be applied to ImageNet pre-trained features. ","This paper proposes a library of diverse expert models that can be used to improve the downstream transferability of a single generalist model to a new task. The library consists of a collection of related task-specific teachers for different recognition tasks, and a consolidated image feature representation is learned by distilling the knowledge from the related task -specific teachers. The student model in knowledge distillation is trained in a similar way as the teacher. The authors show that the distillation of the teacher representations from a task-agnostic generalist teacher is more effective than distillation from the distilled representations from the task specific teacher. They also propose a multihead, multi-task distillation method based on an unlabeled proxy dataset. The proposed method is evaluated on a multi-tasks joint training oracle, where the teacher is a generalist and the student is a student. The paper also shows that the proposed method can be applied to ImageNet pre-trained features. "
5445,SP:ab0d024d4060235df45182dab584c36db16d8e31,"Quantifying the data uncertainty USED-FOR learning tasks. valid coverage CONJUNCTION efficiency. efficiency CONJUNCTION valid coverage. valid coverage FEATURE-OF prediction sets. low length HYPONYM-OF efficiency. constrained empirical risk minimization ( ERM ) problem USED-FOR prediction set. empirical coverage FEATURE-OF prediction set. approximate valid population coverage CONJUNCTION near - optimal efficiency. near - optimal efficiency CONJUNCTION approximate valid population coverage. function class PART-OF conformalization step. meta - algorithm USED-FOR conformal prediction algorithms. near - optimal efficiency EVALUATE-FOR it. approximate valid population coverage EVALUATE-FOR it. it USED-FOR ERM problem. non - differentiable coverage constraint PART-OF it. differentiable surrogate losses CONJUNCTION Lagrangians. Lagrangians CONJUNCTION differentiable surrogate losses. gradient - based algorithm USED-FOR it. constrained ERM USED-FOR gradient - based algorithm. Lagrangians USED-FOR gradient - based algorithm. Lagrangians USED-FOR constrained ERM. differentiable surrogate losses USED-FOR gradient - based algorithm. differentiable surrogate losses USED-FOR constrained ERM. minimum - volume prediction sets CONJUNCTION label prediction sets. label prediction sets CONJUNCTION minimum - volume prediction sets. prediction intervals CONJUNCTION minimum - volume prediction sets. minimum - volume prediction sets CONJUNCTION prediction intervals. algorithm COMPARE approaches. approaches COMPARE algorithm. label prediction sets USED-FOR image classification. efficiency EVALUATE-FOR approaches. minimum - volume prediction sets USED-FOR multi - output regression. algorithm USED-FOR applications. approaches USED-FOR applications. image classification HYPONYM-OF applications. efficiency EVALUATE-FOR algorithm. prediction intervals HYPONYM-OF applications. minimum - volume prediction sets HYPONYM-OF applications. label prediction sets HYPONYM-OF applications. OtherScientificTerm is prediction interval. Method are Conformal prediction, and conformal prediction. ","This paper studies the problem of quantifying the data uncertainty for learning tasks. Conformal prediction is a popular approach to quantify the uncertainty of a set of data points for which the prediction interval is a function of the function class of the data points. The paper proposes a meta-algorithm for learning conformal prediction algorithms, where the prediction set is a constrained empirical risk minimization (ERM) problem, and the goal is to achieve both valid coverage and efficiency (e.g., low length). Conformally, it solves the ERM problem by adding a non-differentiable coverage constraint to the original problem, so that the empirical coverage of a prediction set satisfies the constraint. The conformalization step consists of adding a function class to each prediction set, and then the paper shows that it can achieve approximate valid population coverage and near-optimal efficiency.  The paper also proposes a gradient-based algorithm that uses differentiable surrogate losses and Lagrangians to solve the constrained ERM, and shows that the proposed algorithm outperforms existing approaches in three applications: (1) multi-output regression with minimum-volume prediction sets for image classification, (2) prediction intervals, (3) and (4) for learning with label prediction sets.","This paper studies the problem of quantifying the data uncertainty for learning tasks. Conformal prediction is a popular approach to quantify the uncertainty of a set of data points for which the prediction interval is a function of the function class of the data points. The paper proposes a meta-algorithm for learning conformal prediction algorithms, where the prediction set is a constrained empirical risk minimization (ERM) problem, and the goal is to achieve both valid coverage and efficiency (e.g., low length). Conformally, it solves the ERM problem by adding a non-differentiable coverage constraint to the original problem, so that the empirical coverage of a prediction set satisfies the constraint. The conformalization step consists of adding a function class to each prediction set, and then the paper shows that it can achieve approximate valid population coverage and near-optimal efficiency.  The paper also proposes a gradient-based algorithm that uses differentiable surrogate losses and Lagrangians to solve the constrained ERM, and shows that the proposed algorithm outperforms existing approaches in three applications: (1) multi-output regression with minimum-volume prediction sets for image classification, (2) prediction intervals, (3) and (4) for learning with label prediction sets."
5461,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"reinforcement learning based approach USED-FOR query object localization. ordinal metric learning USED-FOR exemplary set. exemplary set USED-FOR transferable reward signal. ordinal metric learning USED-FOR transferable reward signal. method COMPARE fine - tuning approaches. fine - tuning approaches COMPARE method. method USED-FOR test - time policy adaptation. annotated images USED-FOR fine - tuning approaches. corrupted MNIST CONJUNCTION CU - Birds. CU - Birds CONJUNCTION corrupted MNIST. CU - Birds CONJUNCTION COCO datasets. COCO datasets CONJUNCTION CU - Birds. COCO datasets EVALUATE-FOR approach. corrupted MNIST EVALUATE-FOR approach. OtherScientificTerm are reward signals, and transferable reward. ","This paper proposes a reinforcement learning based approach for query object localization. The key idea is to learn a transferable reward signal based on ordinal metric learning on the exemplary set. The proposed method is shown to outperform other fine-tuning approaches that use annotated images and reward signals. Experiments on corrupted MNIST, CU-Birds, and COCO datasets show that the proposed method can be used for test-time policy adaptation. ","This paper proposes a reinforcement learning based approach for query object localization. The key idea is to learn a transferable reward signal based on ordinal metric learning on the exemplary set. The proposed method is shown to outperform other fine-tuning approaches that use annotated images and reward signals. Experiments on corrupted MNIST, CU-Birds, and COCO datasets show that the proposed method can be used for test-time policy adaptation. "
5477,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"Transformers USED-FOR vision tasks. feature matching CONJUNCTION stereo. stereo CONJUNCTION feature matching. object detection CONJUNCTION feature matching. feature matching CONJUNCTION object detection. them USED-FOR vision tasks. dense predictions USED-FOR vision tasks. object detection HYPONYM-OF vision tasks. feature matching HYPONYM-OF vision tasks. quadtree transformer USED-FOR attention. token pyramids USED-FOR quadtree transformer. flops reduction EVALUATE-FOR stereo matching. quadtree attention USED-FOR vision tasks. top-1 accuracy EVALUATE-FOR ImageNet classification. ScanNet USED-FOR feature matching. feature matching EVALUATE-FOR quadtree attention. flops reduction EVALUATE-FOR quadtree attention. feature matching HYPONYM-OF vision tasks. Metric are quadratic computational complexity, and computational complexity. Method is QuadTree Attention. OtherScientificTerm is attention scores. Task is COCO object detection. ","Transformers have been shown to be effective for vision tasks with quadratic computational complexity. This paper proposes to reduce the computational complexity of a quadtree transformer by using token pyramids. The authors show that the quadtree transformers are able to achieve attention scores of $O(1/\sqrt{n})$ and $O(\log n)$ for different vision tasks, and then use them for a variety of vision tasks that require dense predictions, including object detection, feature matching, stereo, etc.  The authors also show that quadtree attention achieves flops reduction for stereo matching, and achieves top-1 accuracy for ImageNet classification. They also show quadtree attentions can be used for other vision tasks such as feature matching on ScanNet and feature matching for COCO object detection.  ","Transformers have been shown to be effective for vision tasks with quadratic computational complexity. This paper proposes to reduce the computational complexity of a quadtree transformer by using token pyramids. The authors show that the quadtree transformers are able to achieve attention scores of $O(1/\sqrt{n})$ and $O(\log n)$ for different vision tasks, and then use them for a variety of vision tasks that require dense predictions, including object detection, feature matching, stereo, etc.  The authors also show that quadtree attention achieves flops reduction for stereo matching, and achieves top-1 accuracy for ImageNet classification. They also show quadtree attentions can be used for other vision tasks such as feature matching on ScanNet and feature matching for COCO object detection.  "
5493,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"reusable options USED-FOR unknown task distribution. options USED-FOR transfer learning. options CONJUNCTION state transitions. state transitions CONJUNCTION options. MI USED-FOR method. scalable approximation USED-FOR MI maximization. scalable approximation USED-FOR InfoMax Termination Critic ( IMTC ) algorithm. gradient ascent USED-FOR scalable approximation. gradient ascent USED-FOR MI maximization. extrinsic rewards CONJUNCTION intrinsic rewards. intrinsic rewards CONJUNCTION extrinsic rewards. IMTC USED-FOR diversity of learned options. IMTC USED-FOR quick adaptation. IMTC USED-FOR complex domains. Method are reinforcement learning, and mutual information ( MI ) based skill learning. OtherScientificTerm is reusable building blocks. Task is learning reusable options. ","This paper studies the problem of learning reusable options for transfer learning in reinforcement learning. In particular, the authors focus on learning options for an unknown task distribution, where options are available for the task at hand and the goal is to maximize the mutual information (MI) based skill learning between the options and the state transitions. The proposed method is based on the idea of using MI to learn a set of reusable building blocks for each task. The authors propose the InfoMax Termination Critic (IMTC) algorithm, which uses a scalable approximation to MI maximization via gradient ascent. They show that IMTC can learn a diversity of learned options for complex domains, and that it can be used for quick adaptation. They also show that the learned options can be combined with extrinsic rewards and intrinsic rewards. ","This paper studies the problem of learning reusable options for transfer learning in reinforcement learning. In particular, the authors focus on learning options for an unknown task distribution, where options are available for the task at hand and the goal is to maximize the mutual information (MI) based skill learning between the options and the state transitions. The proposed method is based on the idea of using MI to learn a set of reusable building blocks for each task. The authors propose the InfoMax Termination Critic (IMTC) algorithm, which uses a scalable approximation to MI maximization via gradient ascent. They show that IMTC can learn a diversity of learned options for complex domains, and that it can be used for quick adaptation. They also show that the learned options can be combined with extrinsic rewards and intrinsic rewards. "
5509,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"Open - World Object Detection HYPONYM-OF object detection paradigm. openworld object detector USED-FOR unknown objects. independent modules USED-FOR unknown categories. independent modules USED-FOR incremental learning. node PART-OF semantic topology. discriminative feature representations USED-FOR constraint. semantic topology COMPARE open - world object detectors. open - world object detectors COMPARE semantic topology. semantic topology USED-FOR open - world object detection. well - trained language model USED-FOR semantic topology. OtherScientificTerm are Semantic Topology, and features. Method are open - world object detector, and detector. Metric is absolute open - set error. ","This paper proposes a new object detection paradigm called Open-World Object Detection, which is an openworld object detector for unknown objects. Semantic Topology is defined as the topology of a node in the semantic topology, and the open-world object detection is performed by a trained detector. Two independent modules are used for the unknown categories and for incremental learning. The paper shows that the absolute open-set error is O(1/\sqrt{n}) when the feature space is large enough, and O(n^2) when the features are small enough. This constraint is enforced through discriminative feature representations. Experiments show that semantic topologies learned by a well-trained language model are more robust to the presence of unknown categories, and can be used to improve the performance of the detector.   ","This paper proposes a new object detection paradigm called Open-World Object Detection, which is an openworld object detector for unknown objects. Semantic Topology is defined as the topology of a node in the semantic topology, and the open-world object detection is performed by a trained detector. Two independent modules are used for the unknown categories and for incremental learning. The paper shows that the absolute open-set error is O(1/\sqrt{n}) when the feature space is large enough, and O(n^2) when the features are small enough. This constraint is enforced through discriminative feature representations. Experiments show that semantic topologies learned by a well-trained language model are more robust to the presence of unknown categories, and can be used to improve the performance of the detector.   "
5525,SP:97f618558f4add834e5930fd177f012a753247dc,Deep learning USED-FOR vision and natural language processing. computation CONJUNCTION human labeling effort. human labeling effort CONJUNCTION computation. deep learning models COMPARE ones. ones COMPARE deep learning models. dataset USED-FOR ones. Prior methods USED-FOR submodular objective functions. predicted class labels CONJUNCTION decision boundaries. decision boundaries CONJUNCTION predicted class labels. balancing constraints FEATURE-OF predicted class labels. balancing constraints FEATURE-OF decision boundaries. matroids HYPONYM-OF algebraic structure. algebraic structure USED-FOR linear independence. vector spaces FEATURE-OF linear independence. constant approximation guarantees FEATURE-OF greedy algorithm. matroids USED-FOR constraints. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR10. ImageNet CONJUNCTION long - tailed datasets. long - tailed datasets CONJUNCTION ImageNet. long - tailed datasets HYPONYM-OF classification datasets. CIFAR-100 - LT HYPONYM-OF long - tailed datasets. long - tailed datasets EVALUATE-FOR baselines. ImageNet HYPONYM-OF classification datasets. CIFAR10 HYPONYM-OF classification datasets. classification datasets EVALUATE-FOR baselines. CIFAR-100 HYPONYM-OF classification datasets. Generic is models. ,"This paper proposes a new algorithm for learning submodular objective functions for deep learning for vision and natural language processing. Prior methods have been shown to be non-convex, and the authors argue that deep learning models trained on the same dataset as the ones trained on a different dataset may not be able to achieve the same level of computation and human labeling effort. The authors propose a greedy algorithm that uses the algebraic structure of matroids, i.e. linear independence in vector spaces, to learn a sub-modular function that satisfies the balancing constraints between predicted class labels and decision boundaries. They also provide constant approximation guarantees for the proposed greedy algorithm.  The authors evaluate their proposed algorithms on three classification datasets (CIFAR10, CIFAR-10-LT, CifAR-100, and ImageNet), long-tailed datasets, and two classification datasets with long-tail datasets. They show that the proposed algorithms outperform the baselines on all three of these datasets. In addition, the authors show that their models are able to learn constraints that are non-trivial in the case of matrosids, and that the constraints can be learned in a more efficient way.","This paper proposes a new algorithm for learning submodular objective functions for deep learning for vision and natural language processing. Prior methods have been shown to be non-convex, and the authors argue that deep learning models trained on the same dataset as the ones trained on a different dataset may not be able to achieve the same level of computation and human labeling effort. The authors propose a greedy algorithm that uses the algebraic structure of matroids, i.e. linear independence in vector spaces, to learn a sub-modular function that satisfies the balancing constraints between predicted class labels and decision boundaries. They also provide constant approximation guarantees for the proposed greedy algorithm.  The authors evaluate their proposed algorithms on three classification datasets (CIFAR10, CIFAR-10-LT, CifAR-100, and ImageNet), long-tailed datasets, and two classification datasets with long-tail datasets. They show that the proposed algorithms outperform the baselines on all three of these datasets. In addition, the authors show that their models are able to learn constraints that are non-trivial in the case of matrosids, and that the constraints can be learned in a more efficient way."
5541,SP:e0432ff922708c6c6e59124d27c1386605930346,"models USED-FOR semantic segmentation. adaptive inference strategy USED-FOR semantic segmentation. Instance - adaptive Batch Normalization ( IaBN ) USED-FOR normalization layers. feature statistics USED-FOR normalization layers. test - time training ( TTT ) approach USED-FOR semantic segmentation. Seg - TTT HYPONYM-OF test - time training ( TTT ) approach. self - supervised loss USED-FOR Seg - TTT. self - supervised loss USED-FOR model parameters. techniques COMPARE baseline. baseline COMPARE techniques. accuracy EVALUATE-FOR generalization methods. techniques COMPARE generalization methods. generalization methods COMPARE techniques. generalization USED-FOR semantic segmentation. accuracy EVALUATE-FOR techniques. Metric is Out - of - distribution robustness. Generic are model, and complementary techniques. ","This paper proposes a new adaptive inference strategy for semantic segmentation based on the observation that existing models are not robust to out-of-distribution (OOD) attacks. The authors propose Instance-adaptive Batch Normalization (IaBN) to regularize the normalization layers based on feature statistics. They also propose a test-time training (TTT) approach called Seg-TTT, which is a self-supervised loss to optimize the model parameters. They show that the proposed techniques outperform the baseline in terms of accuracy and generalization to OOD data.    The authors also propose two complementary techniques to improve the generalization performance. ","This paper proposes a new adaptive inference strategy for semantic segmentation based on the observation that existing models are not robust to out-of-distribution (OOD) attacks. The authors propose Instance-adaptive Batch Normalization (IaBN) to regularize the normalization layers based on feature statistics. They also propose a test-time training (TTT) approach called Seg-TTT, which is a self-supervised loss to optimize the model parameters. They show that the proposed techniques outperform the baseline in terms of accuracy and generalization to OOD data.    The authors also propose two complementary techniques to improve the generalization performance. "
5557,SP:427100edad574722a6525ca917e84f817ff60d7e,contrastive loss USED-FOR mappings. mappings USED-FOR contrastive loss. Material is tabular data. Generic is method. Method is default set rule of hyperparameters selection. ,"This paper proposes a new contrastive loss for tabular data. The key idea of the proposed method is to learn a set of mappings between two mappings, which are then used to train a contrastive learning algorithm. The method is based on the default set rule of hyperparameters selection. Experiments are conducted to validate the effectiveness of the method.","This paper proposes a new contrastive loss for tabular data. The key idea of the proposed method is to learn a set of mappings between two mappings, which are then used to train a contrastive learning algorithm. The method is based on the default set rule of hyperparameters selection. Experiments are conducted to validate the effectiveness of the method."
5573,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,dimensional approach USED-FOR psychiatric classification. mining corresponded nosological relations USED-FOR low - dimensional embedding space. diagnostic information USED-FOR optimal embedding space. dual utilisation of diagnostic information PART-OF conditional variational auto - encoder. major depressive disorder CONJUNCTION schizophrenia. schizophrenia CONJUNCTION major depressive disorder. autism spectrum disorder CONJUNCTION major depressive disorder. major depressive disorder CONJUNCTION autism spectrum disorder. approaches USED-FOR synthetic functional connectivity features. autism spectrum disorder HYPONYM-OF nosological relation. major depressive disorder HYPONYM-OF nosological relation. empirical neuropsychiatric neuroimaging datasets EVALUATE-FOR approach. OtherScientificTerm is neuropsychiatric disorders. ,"This paper proposes a novel dimensional approach for psychiatric classification based on mining corresponded nosological relations to learn a low-dimensional embedding space. The authors propose a conditional variational auto-encoder that incorporates a dual utilisation of diagnostic information to learn an optimal embedding based on the corresponding neuropsychiatric disorders. The proposed approach is evaluated on a number of standard datasets and is shown to outperform existing approaches for learning synthetic functional connectivity features.   The authors also show that the proposed approach can be applied to a number  of empirical neurobiological neuroimaging datasets.  The paper is well-written and well-motivated, and the paper is clearly written and easy to follow.  However, there are a few issues that need to be addressed. ","This paper proposes a novel dimensional approach for psychiatric classification based on mining corresponded nosological relations to learn a low-dimensional embedding space. The authors propose a conditional variational auto-encoder that incorporates a dual utilisation of diagnostic information to learn an optimal embedding based on the corresponding neuropsychiatric disorders. The proposed approach is evaluated on a number of standard datasets and is shown to outperform existing approaches for learning synthetic functional connectivity features.   The authors also show that the proposed approach can be applied to a number  of empirical neurobiological neuroimaging datasets.  The paper is well-written and well-motivated, and the paper is clearly written and easy to follow.  However, there are a few issues that need to be addressed. "
5589,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,quantum neural networks USED-FOR quantum learning tasks. trainable quantum tensor network ( QTN ) USED-FOR quantum embedding. variational quantum circuit ( VQC ) USED-FOR quantum embedding. QTN - VQC HYPONYM-OF end - to - end learning framework. parametric tensor - train network CONJUNCTION tensor product encoding. tensor product encoding CONJUNCTION parametric tensor - train network. tensor product encoding USED-FOR quantum embedding. parametric tensor - train network USED-FOR feature extraction. architecture USED-FOR QTN. parametric tensor - train network PART-OF architecture. parametric tensor - train network PART-OF QTN. tensor product encoding PART-OF QTN. tensor product encoding PART-OF architecture. QTN USED-FOR quantum embedding. QTN USED-FOR end - to - end parametric model pipeline. QTN - VQC HYPONYM-OF end - to - end parametric model pipeline. QTN USED-FOR quantum embedding. MNIST dataset EVALUATE-FOR QTN. QTN COMPARE quantum embedding approaches. quantum embedding approaches COMPARE QTN. ,"This paper proposes a trainable quantum tensor network (QTN) to learn quantum embedding using quantum neural networks for quantum learning tasks. The architecture of QTN consists of a parametric tensor-train network for feature extraction, and a tensor product encoding to encode the information of the quantum embeddings. The authors propose an end-to-end learning framework called QTN-VQC, which is an extension of the variational quantum circuit (VIQC) to train the QTN. Experiments on the MNIST dataset show that QTN is able to learn the quantum encoding of a quantum image, and can be used to train a QTN to learn a more general QTN, which can then be used as an alternative to the standard QTN in an end to end parametric model pipeline such as QTN - VQC. The paper also shows that the proposed QTN outperforms the state-of-the-art state of the art in terms of performance on MNIST datasets.   ","This paper proposes a trainable quantum tensor network (QTN) to learn quantum embedding using quantum neural networks for quantum learning tasks. The architecture of QTN consists of a parametric tensor-train network for feature extraction, and a tensor product encoding to encode the information of the quantum embeddings. The authors propose an end-to-end learning framework called QTN-VQC, which is an extension of the variational quantum circuit (VIQC) to train the QTN. Experiments on the MNIST dataset show that QTN is able to learn the quantum encoding of a quantum image, and can be used to train a QTN to learn a more general QTN, which can then be used as an alternative to the standard QTN in an end to end parametric model pipeline such as QTN - VQC. The paper also shows that the proposed QTN outperforms the state-of-the-art state of the art in terms of performance on MNIST datasets.   "
5605,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"networks USED-FOR task. networks USED-FOR algorithms. neural networks USED-FOR high - level computational processes. algorithm USED-FOR low - dimensional manifolds. meta - model USED-FOR hidden states. meta - model USED-FOR DYNAMO. model PART-OF meta - model. pre - trained neural networks USED-FOR DYNAMO. model embedding vectors USED-FOR manifold. model embedding vector USED-FOR model. RNNs CONJUNCTION CNNs. CNNs CONJUNCTION RNNs. DYNAMO USED-FOR RNNs. DYNAMO USED-FOR CNNs. model embedding spaces USED-FOR applications. neural networks USED-FOR operable neural network. model embedding spaces USED-FOR clustering of neural networks. neural networks USED-FOR task. high - level computational processes USED-FOR clustering of neural networks. semi - supervised learning HYPONYM-OF applications. clustering of neural networks HYPONYM-OF applications. optimization USED-FOR semi - supervised learning. high - level computational processes FEATURE-OF topology of RNN dynamics. RNNs USED-FOR fixed - point analysis of meta - models. Method are deep neural networks, and neural network model. Generic is models. OtherScientificTerm are reparameterization, and model embedding space. ","This paper studies the problem of learning deep neural networks. The authors propose two algorithms based on networks trained to solve a specific task, where neural networks are used to model high-level computational processes. The first algorithm, DYNAMO, is designed for learning low-dimensional manifolds. The algorithm uses a meta-model to model the hidden states of a neural network model, where the model embedding vector of the model is the sum of the embedding vectors of all the layers of the network on the manifold. The paper shows that the model of the neural network can be decomposed into two parts: (1) a model in which a single model is used to represent the entire network, and (2) a set of models in which each model is reparameterized to a different manifold.   The authors show that the algorithm can be applied to RNNs and CNNs, and that the proposed algorithm is able to learn the topology of RNN dynamics.  The paper also shows that, for pre-trained neural networks, the algorithm is capable of learning an operable neural network.  Finally, the authors demonstrate that, in a number of applications, the proposed algorithms can be used to learn a model that can be reparametrized to a new manifold. In particular, they show that, when the model in the new manifold is a fixed-point analysis of meta-models, they can learn an RNN with a fixed number of layers, and can learn a network that is a single layer deep. They also show that their algorithm can also be used for semi-supervised learning with optimization.  In addition, the paper shows how to learn model embeddings of neural networks for a new task, and how to train neural networks to learn to solve the clustering of neural network for a specific problem. The idea is to learn an embedding space that is similar to that of the original neural network, but for a different task. This is done by training a model on a new dataset, and then using the learned embedding spaces for two different applications, namely, clustering neural networks and optimization.","This paper studies the problem of learning deep neural networks. The authors propose two algorithms based on networks trained to solve a specific task, where neural networks are used to model high-level computational processes. The first algorithm, DYNAMO, is designed for learning low-dimensional manifolds. The algorithm uses a meta-model to model the hidden states of a neural network model, where the model embedding vector of the model is the sum of the embedding vectors of all the layers of the network on the manifold. The paper shows that the model of the neural network can be decomposed into two parts: (1) a model in which a single model is used to represent the entire network, and (2) a set of models in which each model is reparameterized to a different manifold.   The authors show that the algorithm can be applied to RNNs and CNNs, and that the proposed algorithm is able to learn the topology of RNN dynamics.  The paper also shows that, for pre-trained neural networks, the algorithm is capable of learning an operable neural network.  Finally, the authors demonstrate that, in a number of applications, the proposed algorithms can be used to learn a model that can be reparametrized to a new manifold. In particular, they show that, when the model in the new manifold is a fixed-point analysis of meta-models, they can learn an RNN with a fixed number of layers, and can learn a network that is a single layer deep. They also show that their algorithm can also be used for semi-supervised learning with optimization.  In addition, the paper shows how to learn model embeddings of neural networks for a new task, and how to train neural networks to learn to solve the clustering of neural network for a specific problem. The idea is to learn an embedding space that is similar to that of the original neural network, but for a different task. This is done by training a model on a new dataset, and then using the learned embedding spaces for two different applications, namely, clustering neural networks and optimization."
5621,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"constraint - based approach COMPARE direct prediction. direct prediction COMPARE constraint - based approach. direct prediction USED-FOR simulation engines. constraint - based approach USED-FOR simulation engines. framework USED-FOR constraint - based learned simulation. trainable function approximator USED-FOR scalar constraint function. gradient descent USED-FOR constraint solver. graph neural network USED-FOR constraint function. graph neural network CONJUNCTION gradient descent. gradient descent CONJUNCTION graph neural network. gradient descent USED-FOR method. graph neural network USED-FOR method. backpropagation USED-FOR architecture. colliding irregular shapes CONJUNCTION splashing fluids. splashing fluids CONJUNCTION colliding irregular shapes. bouncing balls CONJUNCTION colliding irregular shapes. colliding irregular shapes CONJUNCTION bouncing balls. simulated ropes CONJUNCTION bouncing balls. bouncing balls CONJUNCTION simulated ropes. physical domains EVALUATE-FOR model. colliding irregular shapes HYPONYM-OF physical domains. splashing fluids HYPONYM-OF physical domains. simulated ropes HYPONYM-OF physical domains. bouncing balls HYPONYM-OF physical domains. model COMPARE simulators. simulators COMPARE model. forward learned simulators USED-FOR constraint - based framework. numerical methods USED-FOR learned models. Method are physical simulators, forward model, and forward approaches. Task is constraint satisfaction problem. Metric is simulation accuracy. OtherScientificTerm is hand - designed constraints. ","This paper proposes a framework for constraint-based learned simulation, where the goal is to improve the performance of existing physical simulators. The authors argue that the constraint satisfaction problem is important for improving the simulation accuracy of a forward model, and that direct prediction is the most common way to train simulation engines, but that the current state-of-the-art simulation engines rely on direct prediction. To solve this problem, the authors propose a framework that learns a scalar constraint function, which is then used as a trainable function approximator to learn a new constraint function. The proposed method uses graph neural network and gradient descent to train the constraint solver. The architecture is trained with backpropagation, and the method is evaluated on three physical domains (simulated ropes, bouncing balls, and splashing fluids). The authors show that the proposed model outperforms existing simulators, and is able to achieve better simulation accuracy than existing forward learned simulators that do not use any hand-designed constraints. They also show that numerical methods can be used to train learned models.   ","This paper proposes a framework for constraint-based learned simulation, where the goal is to improve the performance of existing physical simulators. The authors argue that the constraint satisfaction problem is important for improving the simulation accuracy of a forward model, and that direct prediction is the most common way to train simulation engines, but that the current state-of-the-art simulation engines rely on direct prediction. To solve this problem, the authors propose a framework that learns a scalar constraint function, which is then used as a trainable function approximator to learn a new constraint function. The proposed method uses graph neural network and gradient descent to train the constraint solver. The architecture is trained with backpropagation, and the method is evaluated on three physical domains (simulated ropes, bouncing balls, and splashing fluids). The authors show that the proposed model outperforms existing simulators, and is able to achieve better simulation accuracy than existing forward learned simulators that do not use any hand-designed constraints. They also show that numerical methods can be used to train learned models.   "
5637,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"exploration CONJUNCTION few - shot adaptation. few - shot adaptation CONJUNCTION exploration. policies USED-FOR exploration. diverse behaviors FEATURE-OF policies. controlling robots HYPONYM-OF real - world scenarios. iterative reproduction CONJUNCTION selection of policies. selection of policies CONJUNCTION iterative reproduction. iterative reproduction PART-OF evolutionary techniques. selection of policies USED-FOR methods. iterative reproduction USED-FOR methods. evolutionary techniques USED-FOR methods. EDO - CS HYPONYM-OF Evolutionary Diversity Optimization algorithm. Clusteringbased Selection USED-FOR Evolutionary Diversity Optimization algorithm. Clusteringbased Selection USED-FOR EDO - CS. EDO - CS COMPARE methods. methods COMPARE EDO - CS. EDO - CS USED-FOR policies. continuous control tasks EVALUATE-FOR EDO - CS. EDO - CS COMPARE EDO - CS. EDO - CS COMPARE EDO - CS. Method is Reinforcement Learning ( RL ). Generic is task. OtherScientificTerm are selection mechanisms, and clusters. Task are reproduction, and reproduction process. ","This paper studies the problem of Reinforcement Learning (RL) where the goal is to learn policies with diverse behaviors for a given task. The authors consider two real-world scenarios: exploration and few-shot adaptation. They propose two methods based on evolutionary techniques that combine iterative reproduction and selection of policies with the goal of encouraging diverse behaviors in the learned policies.    The authors propose EDO-CS, an Evolutionary Diversity Optimization algorithm based on Clusteringbased Selection. The selection mechanisms are based on the observation that, in RL, there is a trade-off between diversity and reproducibility between different policies, and the authors propose to use iterative Reproducing Clusters (i.e., clusters of policies that have similar behaviors) to encourage diversity in the selection of the policies for exploration.  The paper shows that this can be done in the context of a number of tasks, including controlling robots, where reproducing different policies is a common problem in RL. The paper also shows that EDO - CS outperforms existing methods that do not consider iterative reproducing the policies, which does not consider the diversity of the selection process.  Experiments are conducted on continuous control tasks and show that EDo-CS outperforms other methods. ","This paper studies the problem of Reinforcement Learning (RL) where the goal is to learn policies with diverse behaviors for a given task. The authors consider two real-world scenarios: exploration and few-shot adaptation. They propose two methods based on evolutionary techniques that combine iterative reproduction and selection of policies with the goal of encouraging diverse behaviors in the learned policies.    The authors propose EDO-CS, an Evolutionary Diversity Optimization algorithm based on Clusteringbased Selection. The selection mechanisms are based on the observation that, in RL, there is a trade-off between diversity and reproducibility between different policies, and the authors propose to use iterative Reproducing Clusters (i.e., clusters of policies that have similar behaviors) to encourage diversity in the selection of the policies for exploration.  The paper shows that this can be done in the context of a number of tasks, including controlling robots, where reproducing different policies is a common problem in RL. The paper also shows that EDO - CS outperforms existing methods that do not consider iterative reproducing the policies, which does not consider the diversity of the selection process.  Experiments are conducted on continuous control tasks and show that EDo-CS outperforms other methods. "
5653,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"Markovian noise CONJUNCTION general consensus - type interaction. general consensus - type interaction CONJUNCTION Markovian noise. general consensus - type interaction USED-FOR multi - agent linear stochastic approximation algorithm. Markovian noise USED-FOR multi - agent linear stochastic approximation algorithm. time - varying directed graph USED-FOR interconnection structure. doubly stochastic matrices USED-FOR interconnection. finite - time bounds USED-FOR mean - square error. interaction matrices FEATURE-OF uniformly strongly connected graph sequences. stochastic matrices USED-FOR consensus - type algorithm. OtherScientificTerm are local stochastic approximation process, interconnection matrix, ordinary differential equation, interconnection matrices, local equilibria, communication, constant and time - varying step - sizes, and convex combination. Method are consensus - based stochastic approximation algorithms, and push - type distributed stochastic approximation algorithm. Generic is algorithm. ","This paper studies a multi-agent linear stochastic approximation algorithm with Markovian noise and a general consensus-type interaction between agents. The authors consider the interconnection structure of a time-varying directed graph, where each agent has its own local stochastically approximation process and the agents share the same interconnection matrix. The interconnection matrices are assumed to be a convex combination of doubly-stochastic matrices. They show that under certain assumptions, they can obtain finite-time bounds on the mean-square error of the algorithm.    The authors also provide an algorithm that is a generalization of the work of [1]. In particular, they consider the case where the agent has access to a local equilibria, and the agent is not allowed to communicate with the other agents. In this setting, the authors show that the consensus-based linear approximation algorithms are non-convex, which means that the communication between agents is non-differentiable.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]   [16] [17]  [18] [19] [20] ","This paper studies a multi-agent linear stochastic approximation algorithm with Markovian noise and a general consensus-type interaction between agents. The authors consider the interconnection structure of a time-varying directed graph, where each agent has its own local stochastically approximation process and the agents share the same interconnection matrix. The interconnection matrices are assumed to be a convex combination of doubly-stochastic matrices. They show that under certain assumptions, they can obtain finite-time bounds on the mean-square error of the algorithm.    The authors also provide an algorithm that is a generalization of the work of [1]. In particular, they consider the case where the agent has access to a local equilibria, and the agent is not allowed to communicate with the other agents. In this setting, the authors show that the consensus-based linear approximation algorithms are non-convex, which means that the communication between agents is non-differentiable.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]   [16] [17]  [18] [19] [20] "
5669,SP:f7f96d545a907887396393aba310974f4d3f75ff,"ones HYPONYM-OF methods. equivariant Graph Neural Networks ( GNNs ) USED-FOR methods. equivariant Graph Neural Networks ( GNNs ) USED-FOR ones. forward kinematics information FEATURE-OF structural object. it USED-FOR forward kinematics information. generalized coordinates USED-FOR forward kinematics information. generalized coordinates USED-FOR it. forward kinematics USED-FOR geometrical constraints. dynamics of constrained systems COMPARE unconstrained counterparts. unconstrained counterparts COMPARE dynamics of constrained systems. equivariant message passing USED-FOR GMN. orthogonality - equivariant functions USED-FOR equivariant message passing. molecular dynamics prediction CONJUNCTION human motion capture. human motion capture CONJUNCTION molecular dynamics prediction. sticks CONJUNCTION hinges. hinges CONJUNCTION sticks. particles CONJUNCTION sticks. sticks CONJUNCTION particles. prediction accuracy CONJUNCTION constraint satisfaction. constraint satisfaction CONJUNCTION prediction accuracy. GMN COMPARE GNNs. GNNs COMPARE GMN. constraint satisfaction CONJUNCTION data efficiency. data efficiency CONJUNCTION constraint satisfaction. real - world datasets USED-FOR molecular dynamics prediction. real - world datasets USED-FOR human motion capture. particles PART-OF simulated systems. sticks PART-OF simulated systems. simulated systems EVALUATE-FOR GMN. constraint satisfaction EVALUATE-FOR GMN. constraint satisfaction EVALUATE-FOR GNNs. data efficiency EVALUATE-FOR GMN. data efficiency EVALUATE-FOR GNNs. prediction accuracy EVALUATE-FOR GNNs. prediction accuracy EVALUATE-FOR GMN. Task are machine learning, and interacting systems. OtherScientificTerm is constrained systems. Method are Graph Mechanics Network ( GMN ), and equivariant formulation. ","This paper proposes a Graph Mechanics Network (GMs) that is equivariant to geometrical constraints. The authors show that the GMs can be used to learn geometrically-equivariant graph neural networks (GNNs). The authors also show that GMs are able to learn the geometries of a system in an end-to-end manner.  The authors demonstrate that the proposed GMs outperform existing GNNs on a number of tasks, including molecular dynamics prediction and human motion capture.","This paper proposes a Graph Mechanics Network (GMs) that is equivariant to geometrical constraints. The authors show that the GMs can be used to learn geometrically-equivariant graph neural networks (GNNs). The authors also show that GMs are able to learn the geometries of a system in an end-to-end manner.  The authors demonstrate that the proposed GMs outperform existing GNNs on a number of tasks, including molecular dynamics prediction and human motion capture."
5685,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"framework USED-FOR federated learning. partial model personalization USED-FOR federated learning. partial model personalization USED-FOR framework. full model personalization COMPARE partial model personalization. partial model personalization COMPARE full model personalization. domain knowledge USED-FOR model. domain knowledge USED-FOR partial model personalization. federated optimization algorithms USED-FOR partially personalized models. them USED-FOR deep learning models. algorithms USED-FOR minimizing smooth nonconvex functions. alternating update algorithm COMPARE simultaneous update algorithm. simultaneous update algorithm COMPARE alternating update algorithm. partial model personalization USED-FOR full model personalization. personalized parameters USED-FOR partial model personalization. personalized parameters USED-FOR full model personalization. OtherScientificTerm are on - device memory footprint, shared and personal parameters, shared parameters, and smooth nonconvex functions. Material is real - world image and text datasets. ","This paper proposes a new framework for federated learning based on partial model personalization. The authors argue that the on-device memory footprint is a limitation of the existing federated optimization algorithms for partially personalized models. They show that the difference between full model personalisation (i.e. the shared and personal parameters are updated on the same device) and partial modelpersonalization (where the model is personalized based on the domain knowledge of the clients) is that the partial model personalizedization can be done with domain knowledge that is shared across all clients.  The authors propose two algorithms for minimizing smooth nonconvex functions and apply them to several deep learning models, and show that their algorithms outperform existing algorithms. They also show that an alternating update algorithm is more efficient than the standard simultaneous update algorithm. Finally, the authors show that by using personalized parameters for partial model privateization, they can achieve the state-of-the-art performance on real-world image and text datasets.   ","This paper proposes a new framework for federated learning based on partial model personalization. The authors argue that the on-device memory footprint is a limitation of the existing federated optimization algorithms for partially personalized models. They show that the difference between full model personalisation (i.e. the shared and personal parameters are updated on the same device) and partial modelpersonalization (where the model is personalized based on the domain knowledge of the clients) is that the partial model personalizedization can be done with domain knowledge that is shared across all clients.  The authors propose two algorithms for minimizing smooth nonconvex functions and apply them to several deep learning models, and show that their algorithms outperform existing algorithms. They also show that an alternating update algorithm is more efficient than the standard simultaneous update algorithm. Finally, the authors show that by using personalized parameters for partial model privateization, they can achieve the state-of-the-art performance on real-world image and text datasets.   "
5701,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"contrastive learning HYPONYM-OF unsupervised representation learning. contrastive learning PART-OF machine learning. augmentation operations USED-FOR representations. framework USED-FOR unsupervised learning of object representations. CLTT USED-FOR viewing sequences. CLTT USED-FOR supervised learning. ThreeDWorld ( TDW ) USED-FOR near - photorealistic training environment. ThreeDWorld ( TDW ) USED-FOR data set. near - photorealistic training environment USED-FOR data set. CLTT USED-FOR linear classification. OtherScientificTerm are supervision, positive pairs, temporal structure, and representational similarity. Material are biology, and natural videos. Method are object representations, Contrastive Learning Through Time ( CLTT ), contrastive learning methods, and fully supervised setting. Generic is data sets. ","This paper proposes a new unsupervised representation learning method, Contrastive Learning Through Time (CLTT), which aims to improve the performance of contrastive learning (CLT) in the setting where there is no supervision. The authors propose a new framework for unsupervision of object representations, which is inspired by biology. The key idea of CLTT is to learn object representations that are invariant to different augmentation operations, and to learn positive pairs that have a similar temporal structure.  The authors show that CLTT can be used to learn viewing sequences that are similar to natural videos, and that the representation learned by CLTT improves supervised learning performance. The paper also shows that the proposed data set can be trained using a near-photorealistic training environment based on ThreeDWorld (TDW), which is a popular data set for learning from data sets.  Experiments are conducted on three datasets, and CLTT achieves state-of-the-art performance for linear classification, and outperforms other contrastive methods in the fully supervised setting.  ","This paper proposes a new unsupervised representation learning method, Contrastive Learning Through Time (CLTT), which aims to improve the performance of contrastive learning (CLT) in the setting where there is no supervision. The authors propose a new framework for unsupervision of object representations, which is inspired by biology. The key idea of CLTT is to learn object representations that are invariant to different augmentation operations, and to learn positive pairs that have a similar temporal structure.  The authors show that CLTT can be used to learn viewing sequences that are similar to natural videos, and that the representation learned by CLTT improves supervised learning performance. The paper also shows that the proposed data set can be trained using a near-photorealistic training environment based on ThreeDWorld (TDW), which is a popular data set for learning from data sets.  Experiments are conducted on three datasets, and CLTT achieves state-of-the-art performance for linear classification, and outperforms other contrastive methods in the fully supervised setting.  "
5717,SP:2fb4af247b5022710b681037faca2420207a507a,deterministic transition model USED-FOR goal - directed planning. Monte Carlo Tree Search USED-FOR deterministic control problems. function approximators USED-FOR MCTS. MCTS USED-FOR continuous domains. MCTS USED-FOR AlphaZero family of algorithms. function approximators USED-FOR tree. algorithms USED-FOR control problems. sparse rewards FEATURE-OF control problems. goal - directed domains HYPONYM-OF sparse rewards. AlphaZero USED-FOR goal - directed planning tasks. Hindsight Experience Replay USED-FOR AlphaZero. application USED-FOR quantum compiling domain. application HYPONYM-OF simulated domains. quantum compiling domain HYPONYM-OF simulated domains. simulated domains EVALUATE-FOR approach. OtherScientificTerm is positive reward. ,"This paper proposes a deterministic transition model for goal-directed planning based on Monte Carlo Tree Search to solve deterministic control problems with sparse rewards. The authors propose the AlphaZero family of algorithms based on MCTS for continuous domains, which uses function approximators to search the tree. They show that the algorithms can be applied to a variety of control problems, including sparse rewards (goal-directed domains) and continuous domains (continuous domains). They also show that AlphaZero can be used to solve goal-directed planning tasks based on Hindsight Experience Replay, where the goal is to reach a goal with a positive reward. The approach is evaluated on two simulated domains, the quantum compiling domain, and an application to the quantum compilation domain.","This paper proposes a deterministic transition model for goal-directed planning based on Monte Carlo Tree Search to solve deterministic control problems with sparse rewards. The authors propose the AlphaZero family of algorithms based on MCTS for continuous domains, which uses function approximators to search the tree. They show that the algorithms can be applied to a variety of control problems, including sparse rewards (goal-directed domains) and continuous domains (continuous domains). They also show that AlphaZero can be used to solve goal-directed planning tasks based on Hindsight Experience Replay, where the goal is to reach a goal with a positive reward. The approach is evaluated on two simulated domains, the quantum compiling domain, and an application to the quantum compilation domain."
5733,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"network capacity CONJUNCTION data replay. data replay CONJUNCTION network capacity. network capacity USED-FOR methods. data replay USED-FOR methods. virtual Feature Encoding Layer ( FEL ) USED-FOR network structures. iteratively updated optimizer CONJUNCTION virtual Feature Encoding Layer ( FEL ). virtual Feature Encoding Layer ( FEL ) CONJUNCTION iteratively updated optimizer. iteratively updated optimizer USED-FOR gradient. task descriptors USED-FOR virtual Feature Encoding Layer ( FEL ). iteratively updated optimizer PART-OF RGO. virtual Feature Encoding Layer ( FEL ) PART-OF RGO. task descriptors USED-FOR network structures. 20 - split - CIFAR100 CONJUNCTION 20 - split - miniImageNet. 20 - split - miniImageNet CONJUNCTION 20 - split - CIFAR100. RGO COMPARE baselines. baselines COMPARE RGO. continual classification benchmarks EVALUATE-FOR baselines. 20 - split - miniImageNet EVALUATE-FOR RGO. 20 - split - CIFAR100 EVALUATE-FOR RGO. continual classification benchmarks EVALUATE-FOR RGO. method USED-FOR continual learning capabilities. average accuracy EVALUATE-FOR Single - Task Learning ( STL ). Single - Task Learning ( STL ) COMPARE method. method COMPARE Single - Task Learning ( STL ). average accuracy EVALUATE-FOR method. continual learning capabilities USED-FOR learning models. gradient descent USED-FOR learning models. Method are Continual Learning ( CL ), neural networks, and Recursive Gradient Optimization ( RGO ). Generic is approach. ","This paper proposes a new continual learning method called Recursive gradient optimization (RGO) for continual learning (CL). RGO is based on the idea that existing continual learning methods (CL) are limited by network capacity and data replay. To address this issue, RGO introduces a virtual feature encoding layer (FEL) and an iteratively updated optimizer. The authors show that RGO outperforms baselines on continual classification benchmarks on CIFAR-10, 20-split-CIFAR100, and 20-split-miniImageNet. They also show that the proposed method improves the average accuracy over Single-Task Learning (STL) and outperforms the method proposed in [1] on continual learning capabilities for learning models trained with gradient descent. ","This paper proposes a new continual learning method called Recursive gradient optimization (RGO) for continual learning (CL). RGO is based on the idea that existing continual learning methods (CL) are limited by network capacity and data replay. To address this issue, RGO introduces a virtual feature encoding layer (FEL) and an iteratively updated optimizer. The authors show that RGO outperforms baselines on continual classification benchmarks on CIFAR-10, 20-split-CIFAR100, and 20-split-miniImageNet. They also show that the proposed method improves the average accuracy over Single-Task Learning (STL) and outperforms the method proposed in [1] on continual learning capabilities for learning models trained with gradient descent. "
5749,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"fine - tuning USED-FOR them. aligned StyleGAN models USED-FOR image - to - image translation. human faces CONJUNCTION churches. churches CONJUNCTION human faces. latent spaces FEATURE-OF child model. churches HYPONYM-OF distant data domains. human faces HYPONYM-OF distant data domains. aligned models USED-FOR tasks. image translation CONJUNCTION automatic cross - domain image morphing. automatic cross - domain image morphing CONJUNCTION image translation. child domain USED-FOR zero - shot vision tasks. fine - tuning CONJUNCTION inversion. inversion CONJUNCTION fine - tuning. fine - tuning USED-FOR approach. inversion USED-FOR approach. Method are aligned generative models, and StyleGAN. Generic are models, and they. OtherScientificTerm are architecture, and supervision. Task are transfer learning, and model alignment. ","This paper proposes to use aligned generative models for zero-shot image-to-image translation and cross-domain image morphing. The idea is that aligned StyleGAN models can be used for both image to image translation and image to text translation. The paper shows that the aligned models are able to perform well on both tasks, and that fine-tuning can help them to generalize better.   The paper also shows that aligned models can generalize well to distant data domains (e.g. human faces and churches). The paper further shows that aligning the latent spaces of the child model and the parent model can be beneficial for transfer learning.  The authors also show that the alignment of the two models is beneficial to transfer learning when the architecture of the parent and the child models share the same latent spaces.  In addition, the paper shows how to align the latent space of a child model with the latent of a parent model. The proposed approach is based on the idea that the model alignment between the child and parent is beneficial in the sense that the training of the model on the child domain can be done without any additional supervision. The authors further show that this approach can be combined with fine -tuning and inversion to improve the performance of StyleGAN.  Experiments are conducted on image translation, and automatic cross-domains image morph, where the goal is to learn a single model that can be applied to multiple domains. The approach is evaluated on two different domains, and is shown to perform better than using only one model on each domain. ","This paper proposes to use aligned generative models for zero-shot image-to-image translation and cross-domain image morphing. The idea is that aligned StyleGAN models can be used for both image to image translation and image to text translation. The paper shows that the aligned models are able to perform well on both tasks, and that fine-tuning can help them to generalize better.   The paper also shows that aligned models can generalize well to distant data domains (e.g. human faces and churches). The paper further shows that aligning the latent spaces of the child model and the parent model can be beneficial for transfer learning.  The authors also show that the alignment of the two models is beneficial to transfer learning when the architecture of the parent and the child models share the same latent spaces.  In addition, the paper shows how to align the latent space of a child model with the latent of a parent model. The proposed approach is based on the idea that the model alignment between the child and parent is beneficial in the sense that the training of the model on the child domain can be done without any additional supervision. The authors further show that this approach can be combined with fine -tuning and inversion to improve the performance of StyleGAN.  Experiments are conducted on image translation, and automatic cross-domains image morph, where the goal is to learn a single model that can be applied to multiple domains. The approach is evaluated on two different domains, and is shown to perform better than using only one model on each domain. "
5765,SP:0e13f831c211626195c118487f2fff36a6e293f6,Comparing structured objects PART-OF learning tasks. graphs HYPONYM-OF Comparing structured objects. Optimal Transport ( OT ) USED-FOR Gromov - Wasserstein ( GW ) distance. nodes connectivity relations USED-FOR GW. GW USED-FOR graphs. probability measures USED-FOR GW. conservation of mass USED-FOR OT. graph dictionary CONJUNCTION partition learning. partition learning CONJUNCTION graph dictionary. semi - relaxed Gromov - Wasserstein divergence USED-FOR it. partition learning HYPONYM-OF tasks. graph dictionary HYPONYM-OF tasks. it USED-FOR graph dictionary learning algorithm. partitioning CONJUNCTION clustering. clustering CONJUNCTION partitioning. clustering CONJUNCTION completion. completion CONJUNCTION clustering. graphs USED-FOR complex tasks. completion HYPONYM-OF complex tasks. completion HYPONYM-OF graphs. partitioning HYPONYM-OF graphs. partitioning HYPONYM-OF complex tasks. clustering HYPONYM-OF graphs. clustering HYPONYM-OF complex tasks. OtherScientificTerm is nodes. ,"Comparing structured objects in learning tasks, such as graphs, is an important problem. In this paper, the authors propose to use Optimal Transport (OT) to compute the Gromov-Wasserstein (GW) distance between two graphs based on nodes connectivity relations. The GW is based on the observation that the probability measures of the two graphs are highly correlated. The authors use the conservation of mass in OT to motivate the use of OT. They also propose a graph dictionary learning algorithm based on it, which uses a semi-relaxed version of the GW.  The authors show that the GW can be used to learn graphs for complex tasks such as graph dictionary, partitioning, clustering, and completion.  ","Comparing structured objects in learning tasks, such as graphs, is an important problem. In this paper, the authors propose to use Optimal Transport (OT) to compute the Gromov-Wasserstein (GW) distance between two graphs based on nodes connectivity relations. The GW is based on the observation that the probability measures of the two graphs are highly correlated. The authors use the conservation of mass in OT to motivate the use of OT. They also propose a graph dictionary learning algorithm based on it, which uses a semi-relaxed version of the GW.  The authors show that the GW can be used to learn graphs for complex tasks such as graph dictionary, partitioning, clustering, and completion.  "
5781,SP:d6d144be11230070ae9395db70b7c7743540bad4,"prediction CONJUNCTION collaboration. collaboration CONJUNCTION prediction. Models USED-FOR prediction. Models USED-FOR collaboration. ones HYPONYM-OF categories. ones HYPONYM-OF categories. imitation learning USED-FOR ones. predicting policies USED-FOR systematic suboptimality. Bayesian inference USED-FOR systematic deviations. Boltzmann policy distribution ( BPD ) USED-FOR human policies. sampling CONJUNCTION inference. inference CONJUNCTION sampling. generative and sequence models USED-FOR sampling. generative and sequence models USED-FOR inference. high - dimensional continuous space FEATURE-OF policies. BPD USED-FOR prediction of human behavior. prediction of human behavior CONJUNCTION human - AI collaboration. human - AI collaboration CONJUNCTION prediction of human behavior. BPD USED-FOR human - AI collaboration. human - AI collaboration CONJUNCTION imitation learning - based human models. imitation learning - based human models CONJUNCTION human - AI collaboration. prediction of human behavior CONJUNCTION imitation learning - based human models. imitation learning - based human models CONJUNCTION prediction of human behavior. BPD USED-FOR imitation learning - based human models. OtherScientificTerm are human behavior, reward function, Boltzmann rationality, and trajectories. Generic are former, and models. Material is human data. ","This paper considers the problem of systematic suboptimality in learning from human behavior. Models for both prediction and collaboration are considered. Two categories of models are considered: ones based on imitation learning and ones that are based on Boltzmann rationality. In the former, the goal is to learn a reward function that maximizes the probability that the reward function is maximized, while in the latter, the aim is to maximize the probability of the reward being maximized.    The authors propose to learn human policies from Boltzman policy distribution (BPD) by predicting policies in a high-dimensional continuous space, and then using Bayesian inference to identify systematic deviations between the two trajectories. The inference is performed using generative and sequence models for sampling and inference respectively. The authors show that BPD can be used to improve the prediction of human behavior, human-AI collaboration, and imitation learning-based human models based on BPD. They also show that their models are able to learn from human data. ","This paper considers the problem of systematic suboptimality in learning from human behavior. Models for both prediction and collaboration are considered. Two categories of models are considered: ones based on imitation learning and ones that are based on Boltzmann rationality. In the former, the goal is to learn a reward function that maximizes the probability that the reward function is maximized, while in the latter, the aim is to maximize the probability of the reward being maximized.    The authors propose to learn human policies from Boltzman policy distribution (BPD) by predicting policies in a high-dimensional continuous space, and then using Bayesian inference to identify systematic deviations between the two trajectories. The inference is performed using generative and sequence models for sampling and inference respectively. The authors show that BPD can be used to improve the prediction of human behavior, human-AI collaboration, and imitation learning-based human models based on BPD. They also show that their models are able to learn from human data. "
5797,SP:401ef5fe2022e926b0321258efac1f369f186ace,"Quantization of deep neural networks ( DNN ) USED-FOR compressing and accelerating DNN models. Data - free quantization ( DFQ ) HYPONYM-OF approach. accuracy EVALUATE-FOR DFQ solutions. sub - second quantization time FEATURE-OF on - the - fly DFQ framework. inference - only devices USED-FOR networks. SQuant HYPONYM-OF on - the - fly DFQ framework. discrete domain FEATURE-OF data - free optimization objective. computation complexity EVALUATE-FOR objective solver. algorithm USED-FOR objective solver. back - propagation USED-FOR algorithm. computation complexity EVALUATE-FOR algorithm. fine - tuning CONJUNCTION synthetic datasets. synthetic datasets CONJUNCTION fine - tuning. SQuant COMPARE data - free post - training quantization. data - free post - training quantization COMPARE SQuant. SQuant USED-FOR data - free quantization process. SQuant USED-FOR sub - second level. synthetic datasets EVALUATE-FOR SQuant. accuracy EVALUATE-FOR models. sub - second level FEATURE-OF data - free quantization process. 4 - bit quantization FEATURE-OF models. accuracy EVALUATE-FOR SQuant. Method are DNN models, and network architecture. OtherScientificTerm are privacy - sensitive and confidential scenarios, DNN task loss, Hessian - based optimization objective, diagonal sub - items, and weight tensor. Material is synthetic data. Metric is computation and memory requirements. ","Quantization of deep neural networks (DNN) is an important problem in compressing and accelerating DNN models, especially in privacy-sensitive and confidential scenarios. Data-free quantization (DFQ) is a well-known approach to reduce the computational cost of quantization of DNNs. However, the accuracy of DFQ solutions can be slow and expensive to compute. This paper proposes an on-the-fly DFQ framework called SQuant, which can achieve sub-second quantization time on inference-only devices for training networks with 4-bit quantization. SQuant is based on the observation that the data-free optimization objective in the discrete domain is a Hessian-based optimization objective, and that the computation complexity of the objective solver can be significantly reduced by back-propagating through the DNN task loss. The authors propose an algorithm that uses this algorithm to speed up the computation and memory requirements. They show that SQuant can be applied to the sub-2nd level of the training process, where diagonal sub-items of the weight tensor can be removed. They also show that the performance of SQuant on fine-tuning and synthetic datasets is comparable to that of the original SQuant in terms of accuracy, and SQuant outperforms the state-of-the art on the accuracy for models with 4 bit quantization on synthetic data.   ","Quantization of deep neural networks (DNN) is an important problem in compressing and accelerating DNN models, especially in privacy-sensitive and confidential scenarios. Data-free quantization (DFQ) is a well-known approach to reduce the computational cost of quantization of DNNs. However, the accuracy of DFQ solutions can be slow and expensive to compute. This paper proposes an on-the-fly DFQ framework called SQuant, which can achieve sub-second quantization time on inference-only devices for training networks with 4-bit quantization. SQuant is based on the observation that the data-free optimization objective in the discrete domain is a Hessian-based optimization objective, and that the computation complexity of the objective solver can be significantly reduced by back-propagating through the DNN task loss. The authors propose an algorithm that uses this algorithm to speed up the computation and memory requirements. They show that SQuant can be applied to the sub-2nd level of the training process, where diagonal sub-items of the weight tensor can be removed. They also show that the performance of SQuant on fine-tuning and synthetic datasets is comparable to that of the original SQuant in terms of accuracy, and SQuant outperforms the state-of-the art on the accuracy for models with 4 bit quantization on synthetic data.   "
5813,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"stock market partitioning CONJUNCTION sleep stage labelling. sleep stage labelling CONJUNCTION stock market partitioning. sleep stage labelling CONJUNCTION human activity recognition. human activity recognition CONJUNCTION sleep stage labelling. Time series USED-FOR tasks. human activity recognition HYPONYM-OF tasks. stock market partitioning HYPONYM-OF tasks. sleep stage labelling HYPONYM-OF tasks. sliding window USED-FOR sub - sequences. overlapping stride FEATURE-OF sliding window. sliding window USED-FOR time series. accuracy EVALUATE-FOR segmentation. approach USED-FOR approximate breakpoints. it USED-FOR long - term dependencies. bi - pass architecture USED-FOR SegTime. Task are time series segmentation, and classification. OtherScientificTerm are precise breakpoints, sliding windows, and label changing frequency. ","This paper proposes SegTime, a new approach for time series segmentation. Time series are used for several tasks such as stock market partitioning, sleep stage labelling, and human activity recognition. The paper proposes a sliding window for each time series to represent sub-sequences. The sliding window has an overlapping stride, which allows for precise breakpoints to be defined.  The paper shows that SegTime is able to segment a time series with high accuracy while maintaining high accuracy for segmentation and classification. SegTime uses a bi-pass architecture, which is a generalization of SegTime to sliding windows. This approach can be applied to approximate breakpoints, and it can be used to capture long-term dependencies between time series.  SegTime also shows that the label changing frequency can be controlled by adjusting the size of the sliding window.  ","This paper proposes SegTime, a new approach for time series segmentation. Time series are used for several tasks such as stock market partitioning, sleep stage labelling, and human activity recognition. The paper proposes a sliding window for each time series to represent sub-sequences. The sliding window has an overlapping stride, which allows for precise breakpoints to be defined.  The paper shows that SegTime is able to segment a time series with high accuracy while maintaining high accuracy for segmentation and classification. SegTime uses a bi-pass architecture, which is a generalization of SegTime to sliding windows. This approach can be applied to approximate breakpoints, and it can be used to capture long-term dependencies between time series.  SegTime also shows that the label changing frequency can be controlled by adjusting the size of the sliding window.  "
5829,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,Graph Neural Networks ( GNNs ) USED-FOR graph data. faithfulness problems CONJUNCTION unnatural artifacts. unnatural artifacts CONJUNCTION faithfulness problems. methods USED-FOR approximation based and perturbation based approaches. faithful explanation USED-FOR GNN predictions. subgraph level interpretation algorithm USED-FOR complex interactions. GNN characteristics USED-FOR algorithm. synthetic and real - world datasets EVALUATE-FOR DEGREE. node classification and graph classification tasks EVALUATE-FOR DEGREE. Method is GNNs. Generic is models. OtherScientificTerm is graph nodes. ,"Graph Neural Networks (GNNs) have been widely used for graph data, but there are many issues with faithfulness problems and unnatural artifacts in GNNs. In this paper, the authors propose DEGREE, a faithful explanation for GNN predictions. The proposed methods are based on approximation based and perturbation based approaches. The authors propose a subgraph level interpretation algorithm that can handle complex interactions between graph nodes. The algorithm is based on GNN characteristics and the authors show that DEGree achieves state-of-the-art performance on both synthetic and real-world datasets. The paper is well-written and well-motivated. The experiments on node classification and graph classification tasks demonstrate the effectiveness of the proposed algorithm.","Graph Neural Networks (GNNs) have been widely used for graph data, but there are many issues with faithfulness problems and unnatural artifacts in GNNs. In this paper, the authors propose DEGREE, a faithful explanation for GNN predictions. The proposed methods are based on approximation based and perturbation based approaches. The authors propose a subgraph level interpretation algorithm that can handle complex interactions between graph nodes. The algorithm is based on GNN characteristics and the authors show that DEGree achieves state-of-the-art performance on both synthetic and real-world datasets. The paper is well-written and well-motivated. The experiments on node classification and graph classification tasks demonstrate the effectiveness of the proposed algorithm."
5845,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"strided convolutions CONJUNCTION pooling layers. pooling layers CONJUNCTION strided convolutions. downsampling operators USED-FOR resolution of intermediate representations. downsampling operators PART-OF Convolutional neural networks. strided convolutions HYPONYM-OF downsampling operators. pooling layers HYPONYM-OF downsampling operators. computational complexity EVALUATE-FOR architecture. hyperparameter FEATURE-OF layers. stride FEATURE-OF hyperparameter. crossvalidation CONJUNCTION discrete optimization. discrete optimization CONJUNCTION crossvalidation. architecture search HYPONYM-OF discrete optimization. gradient descent USED-FOR search space. DiffStride HYPONYM-OF downsampling layer. learnable strides FEATURE-OF downsampling layer. layer USED-FOR resizing. layer USED-FOR cropping mask. Fourier domain FEATURE-OF cropping mask. DiffStride USED-FOR downsampling layers. audio and image classification EVALUATE-FOR solution. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. layer PART-OF ResNet-18 architecture. computational complexity EVALUATE-FOR architecture. regularization term USED-FOR architecture. computational complexity EVALUATE-FOR regularization term. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. efficiency EVALUATE-FOR regularization. accuracy EVALUATE-FOR regularization. ImageNet EVALUATE-FOR regularization. OtherScientificTerm are shift - invariance, integer factor of downsampling, strides, random stride configurations, and learnable variables. Metric is computational cost. Generic is them. ","Convolutional neural networks have a number of downsampling operators, including strided convolutions and pooling layers, that are designed to reduce the resolution of intermediate representations and reduce the computational cost. However, the computational complexity of each architecture depends on the hyperparameter of these layers, the stride of each layer, and the shift-invariance of the weights. In order to reduce this computational cost, the authors propose to use crossvalidation and discrete optimization (e.g., architecture search) to find the optimal stride for each layer. The authors propose a new downsample layer called DiffStride, which is an integer factor of the stride, and they use gradient descent to search the search space. They show that the downsampled layer has learnable strides and that the strides are shift invariant. They also show that this layer can be used for resizing and cropping in a convolutional network.   The authors also propose a novel cropping mask in the Fourier domain, where they use the same layer for cropping and a different layer for each cropping. They demonstrate that this solution can be applied to audio and image classification tasks.  Finally, they demonstrate that the Diffstride can also be used to improve the performance of other downsample layers.  They also demonstrate that their new layer is compatible with the ResNet-18 architecture, and that their regularization term can reduce computational complexity and improve the accuracy of the architecture.  The paper also shows that the regularization improves the accuracy on CIFAR10, Cifar100, CIFR10, and ImageNet, and improves the efficiency on ImageNet. The main contribution of the paper is that the authors consider random stride configurations, which allows them to consider learnable variables.","Convolutional neural networks have a number of downsampling operators, including strided convolutions and pooling layers, that are designed to reduce the resolution of intermediate representations and reduce the computational cost. However, the computational complexity of each architecture depends on the hyperparameter of these layers, the stride of each layer, and the shift-invariance of the weights. In order to reduce this computational cost, the authors propose to use crossvalidation and discrete optimization (e.g., architecture search) to find the optimal stride for each layer. The authors propose a new downsample layer called DiffStride, which is an integer factor of the stride, and they use gradient descent to search the search space. They show that the downsampled layer has learnable strides and that the strides are shift invariant. They also show that this layer can be used for resizing and cropping in a convolutional network.   The authors also propose a novel cropping mask in the Fourier domain, where they use the same layer for cropping and a different layer for each cropping. They demonstrate that this solution can be applied to audio and image classification tasks.  Finally, they demonstrate that the Diffstride can also be used to improve the performance of other downsample layers.  They also demonstrate that their new layer is compatible with the ResNet-18 architecture, and that their regularization term can reduce computational complexity and improve the accuracy of the architecture.  The paper also shows that the regularization improves the accuracy on CIFAR10, Cifar100, CIFR10, and ImageNet, and improves the efficiency on ImageNet. The main contribution of the paper is that the authors consider random stride configurations, which allows them to consider learnable variables."
5861,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"image segmentation CONJUNCTION node classification. node classification CONJUNCTION image segmentation. Models USED-FOR image segmentation. Models USED-FOR node classification. node classification CONJUNCTION tasks. tasks CONJUNCTION node classification. Models USED-FOR tasks. method USED-FOR strictly local models. collective certificate USED-FOR softly local models. localized randomized smoothing approach USED-FOR certificate. collective guarantees FEATURE-OF locally smoothed model. image segmentation and node classification tasks EVALUATE-FOR locally smoothed model. prediction quality EVALUATE-FOR locally smoothed model. Metric is collective robustness certificate. OtherScientificTerm are small receptive field, and random perturbation strength. ","This paper proposes a novel method for learning a collective robustness certificate. Models for image segmentation, node classification, and node classification are commonly used for these tasks. The proposed method is for training strictly local models with a small receptive field. The key idea is to use a localized randomized smoothing approach to obtain a certificate that is robust to small perturbations in the input space. The authors show that the proposed method can be used to train softly local models under a collective certificate. They also show that a locally smoothed model with the proposed collective guarantees can achieve better prediction quality than a locally smooth model without the need for the random perturbation strength. Experiments are conducted on both the image segmentated and the node classification tasks to show the effectiveness of the locally smoothing model.","This paper proposes a novel method for learning a collective robustness certificate. Models for image segmentation, node classification, and node classification are commonly used for these tasks. The proposed method is for training strictly local models with a small receptive field. The key idea is to use a localized randomized smoothing approach to obtain a certificate that is robust to small perturbations in the input space. The authors show that the proposed method can be used to train softly local models under a collective certificate. They also show that a locally smoothed model with the proposed collective guarantees can achieve better prediction quality than a locally smooth model without the need for the random perturbation strength. Experiments are conducted on both the image segmentated and the node classification tasks to show the effectiveness of the locally smoothing model."
5877,SP:aacc31e83886c4c997412a1e51090202075eda86,"Normalizing flows USED-FOR general - purpose density estimators. domain - specific knowledge USED-FOR real world applications. general - purpose transformations CONJUNCTION structured layers. structured layers CONJUNCTION general - purpose transformations. general - purpose transformations PART-OF embedded - model flows ( EMF ). equivalent bijective transformations USED-FOR user - specified differentiable probabilistic models. EMFs USED-FOR desirable properties. multimodality CONJUNCTION hierarchical coupling. hierarchical coupling CONJUNCTION multimodality. hierarchical coupling CONJUNCTION continuity. continuity CONJUNCTION hierarchical coupling. EMFs USED-FOR multimodality. EMFs USED-FOR hierarchical coupling. continuity HYPONYM-OF desirable properties. multimodality HYPONYM-OF desirable properties. hierarchical coupling HYPONYM-OF desirable properties. EMFs USED-FOR variational inference. structure of the prior model PART-OF variational architecture. approach COMPARE alternative methods. alternative methods COMPARE approach. common structured inference problems EVALUATE-FOR alternative methods. common structured inference problems EVALUATE-FOR approach. Method are normalizing flows, gated structured layers, and prior model. OtherScientificTerm is domain - specific inductive biases. Generic are layers, and models. ","Normalizing flows have recently been proposed as general-purpose density estimators that can leverage domain-specific knowledge for real world applications. However, normalizing flows can be seen as a special case of gated structured layers, where the gated structure of the prior model is not available. This paper proposes embedded-model flows (EMF), which combines general-general-purpose transformations with structured layers. The authors show that EMFs can capture desirable properties such as multimodality, hierarchical coupling, and continuity. They also show that the equivalent bijective transformations can be used to construct user-specified differentiable probabilistic models. The paper also shows that the desirable properties of EMFs for variational inference can be obtained through the structure of a variational architecture. The proposed approach is evaluated on common structured inference problems and shows superior performance compared to alternative methods.    The paper is well-written and well-motivated, and the authors have done a good job of explaining their approach and explaining the domain -specific inductive biases.  However, there are a few issues that need to be addressed to make the paper more practical. For example, the paper does not clearly state that the layers are gated, and it does not explain how the gating is done, or how gated layers are used. It is also unclear how the models are trained. ","Normalizing flows have recently been proposed as general-purpose density estimators that can leverage domain-specific knowledge for real world applications. However, normalizing flows can be seen as a special case of gated structured layers, where the gated structure of the prior model is not available. This paper proposes embedded-model flows (EMF), which combines general-general-purpose transformations with structured layers. The authors show that EMFs can capture desirable properties such as multimodality, hierarchical coupling, and continuity. They also show that the equivalent bijective transformations can be used to construct user-specified differentiable probabilistic models. The paper also shows that the desirable properties of EMFs for variational inference can be obtained through the structure of a variational architecture. The proposed approach is evaluated on common structured inference problems and shows superior performance compared to alternative methods.    The paper is well-written and well-motivated, and the authors have done a good job of explaining their approach and explaining the domain -specific inductive biases.  However, there are a few issues that need to be addressed to make the paper more practical. For example, the paper does not clearly state that the layers are gated, and it does not explain how the gating is done, or how gated layers are used. It is also unclear how the models are trained. "
5893,SP:825a254c0725008143b260ead840ae35f9f096d1,"entities CONJUNCTION objects. objects CONJUNCTION entities. conceptual structure FEATURE-OF rich conceptual structure. LMs USED-FOR grounded world representation. LMs USED-FOR conceptual domain. it USED-FOR concepts. grid world FEATURE-OF concepts. GPT-2 CONJUNCTION GPT-3. GPT-3 CONJUNCTION GPT-2. GPT-2 HYPONYM-OF generative language models. GPT-3 HYPONYM-OF generative language models. text - only models USED-FOR rich conceptual structure. Method are text - only language models ( LMs ), and grounded language models. Generic are representation, and model. OtherScientificTerm is conceptual structure of language. ","This paper presents a study of text-only language models (LMs). The authors show that LMs are able to learn a grounded world representation that captures a rich conceptual structure in the conceptual structure of language. This representation can be seen as an extension of the representation learned by LMs to the conceptual domain, and that it can capture concepts in a grid world. The authors also show that grounded language models can be used to learn concepts that are rich in conceptual structure. The paper also shows that the generative language models, such as GPT-2 and GPT3, are capable of learning rich concepts.    The authors conclude that the richness of the representations learned by text- only models is due to the fact that the model is able to capture the concept of entities and objects, and it is also able to represent concepts in the grid world, which is an important property of the concept space. ","This paper presents a study of text-only language models (LMs). The authors show that LMs are able to learn a grounded world representation that captures a rich conceptual structure in the conceptual structure of language. This representation can be seen as an extension of the representation learned by LMs to the conceptual domain, and that it can capture concepts in a grid world. The authors also show that grounded language models can be used to learn concepts that are rich in conceptual structure. The paper also shows that the generative language models, such as GPT-2 and GPT3, are capable of learning rich concepts.    The authors conclude that the richness of the representations learned by text- only models is due to the fact that the model is able to capture the concept of entities and objects, and it is also able to represent concepts in the grid world, which is an important property of the concept space. "
5909,SP:702029739062693e3f96051cbb38f20c53f2a223,"Reinforcement learning USED-FOR phenomena. biases PART-OF learning process. biases USED-FOR shaped rewards. shaped rewards USED-FOR emergent phenomena. sender - receiver navigation game USED-FOR shaped rewards. Generic are they, and rewards. OtherScientificTerm are base reward, inductive bias, semantics, and environmental variables of interest. Task is emergent language experimentation. Material is emergent language. Metric is entropy. ","This paper studies the problem of emergent language experimentation, where agents are encouraged to communicate in a language where they are not given a base reward, but are instead given an inductive bias that encourages them to learn a language that is similar to the one they are trying to communicate with. The authors show that this phenomena is a result of Reinforcement learning, where there is a bias towards learning a language with which the agent is interested. They show that the emergent phenomena are related to the use of biases in the learning process, and that these biases lead to the emergence of shaped rewards that are more likely to be shaped rewards in a sender-receiver navigation game. They also show that these shaped rewards can be learned in a similar way to the ones that are learned in an environment where the agent has access to a set of environmental variables of interest, but where these variables are not present in the environment.    The paper is well-written, well-motivated, and well-structured, and the results are interesting. However, there are a few issues that prevent me from recommending acceptance of this paper as a must-read. First, it is not clear to me that this paper is really a new direction in the field.  Second, the paper is not well-explained.  Third, there is no discussion of how to define emergent languages, and it is unclear how to measure the entropy of the learned language.  Fourth, it does not seem to be clear how to distinguish between emergent and non-empirical languages.  The authors also do not show that there is an equivalency between the two sets of rewards, which is not very surprising.  Finally, it would be good to see a more detailed discussion of the role of the inductive biases in this paper.","This paper studies the problem of emergent language experimentation, where agents are encouraged to communicate in a language where they are not given a base reward, but are instead given an inductive bias that encourages them to learn a language that is similar to the one they are trying to communicate with. The authors show that this phenomena is a result of Reinforcement learning, where there is a bias towards learning a language with which the agent is interested. They show that the emergent phenomena are related to the use of biases in the learning process, and that these biases lead to the emergence of shaped rewards that are more likely to be shaped rewards in a sender-receiver navigation game. They also show that these shaped rewards can be learned in a similar way to the ones that are learned in an environment where the agent has access to a set of environmental variables of interest, but where these variables are not present in the environment.    The paper is well-written, well-motivated, and well-structured, and the results are interesting. However, there are a few issues that prevent me from recommending acceptance of this paper as a must-read. First, it is not clear to me that this paper is really a new direction in the field.  Second, the paper is not well-explained.  Third, there is no discussion of how to define emergent languages, and it is unclear how to measure the entropy of the learned language.  Fourth, it does not seem to be clear how to distinguish between emergent and non-empirical languages.  The authors also do not show that there is an equivalency between the two sets of rewards, which is not very surprising.  Finally, it would be good to see a more detailed discussion of the role of the inductive biases in this paper."
5925,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"deep multilingual language models USED-FOR cross - lingual knowledge. neural modeling USED-FOR deep multilingual language models. unparallel texts USED-FOR cross - lingual knowledge. invariance FEATURE-OF feature representations. invariance FEATURE-OF transfer. prior shift estimation CONJUNCTION correction. correction CONJUNCTION prior shift estimation. representation alignment CONJUNCTION prior shift estimation. prior shift estimation CONJUNCTION representation alignment. unsupervised cross - lingual learning method USED-FOR representation alignment. unsupervised cross - lingual learning method USED-FOR prior shift estimation. importance - weighted domain alignment ( IWDA ) HYPONYM-OF unsupervised cross - lingual learning method. method COMPARE semi - supervised learning techniques. semi - supervised learning techniques COMPARE method. Method are cross - lingually shared representations, and multilingual representations. OtherScientificTerm are distributional shift in class priors, and prior shifts. ","This paper studies the problem of cross-lingual knowledge transfer between multilingual language models trained with neural modeling. The authors propose a new unsupervised learning framework that learns a shared feature representation across multilingual languages. The proposed method is based on the idea of importance-weighted domain alignment (IWDA), which is used to learn a shared representation across multiple languages.  The authors show that the proposed method outperforms the state-of-the-art semi-supervised approaches on a number of tasks. ","This paper studies the problem of cross-lingual knowledge transfer between multilingual language models trained with neural modeling. The authors propose a new unsupervised learning framework that learns a shared feature representation across multilingual languages. The proposed method is based on the idea of importance-weighted domain alignment (IWDA), which is used to learn a shared representation across multiple languages.  The authors show that the proposed method outperforms the state-of-the-art semi-supervised approaches on a number of tasks. "
5941,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"Meta - learning USED-FOR artificial intelligence. algorithm USED-FOR problem. algorithm USED-FOR meta - learner. metric USED-FOR meta - optimisation. gradients USED-FOR meta - learning. bootstrapping mechanism USED-FOR meta - learning horizon. backpropagation USED-FOR bootstrapping mechanism. it USED-FOR multi - task meta - learning. Atari ALE benchmark EVALUATE-FOR model - free agents. it USED-FOR exploration. ε - greedy Q - learning agent USED-FOR exploration. Task is metaoptimisation problem. Method are metalearner, and bootstrapping. Metric is ( pseudo-)metric. OtherScientificTerm is update rule. ","Meta-learning is an important problem in artificial intelligence. This paper considers the metaoptimisation problem, where the goal is to learn a metalearner that optimally optimizes the (pseudo-)metric of the meta-optimization problem. The authors propose an algorithm to solve this problem, which is based on the idea that meta-learning can be viewed as learning a metric for the gradients of the metal-earner, and that the algorithm can be seen as a meta-learner. The bootstrapping mechanism proposed in this paper is based off of backpropagation, and the authors show that this bootstrapped mechanism can be used to extend the meta -learning horizon. They also show that it can be applied to multi-task meta-learners, and it can also be used for exploration by an ε-greedy Q-learning agent. They evaluate their model-free agents on the Atari ALE benchmark, and show that they outperform the previous state-of-the-art on the update rule.  ","Meta-learning is an important problem in artificial intelligence. This paper considers the metaoptimisation problem, where the goal is to learn a metalearner that optimally optimizes the (pseudo-)metric of the meta-optimization problem. The authors propose an algorithm to solve this problem, which is based on the idea that meta-learning can be viewed as learning a metric for the gradients of the metal-earner, and that the algorithm can be seen as a meta-learner. The bootstrapping mechanism proposed in this paper is based off of backpropagation, and the authors show that this bootstrapped mechanism can be used to extend the meta -learning horizon. They also show that it can be applied to multi-task meta-learners, and it can also be used for exploration by an ε-greedy Q-learning agent. They evaluate their model-free agents on the Atari ALE benchmark, and show that they outperform the previous state-of-the-art on the update rule.  "
5957,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"model - free agents USED-FOR generalization. generalization ability EVALUATE-FOR model - based agents. model - based agents COMPARE model - free counterparts. model - free counterparts COMPARE model - based agents. generalization ability EVALUATE-FOR model - based agents. generalization ability EVALUATE-FOR model - free counterparts. MuZero HYPONYM-OF model - based agent. procedural and task generalization EVALUATE-FOR its. self - supervised representation learning CONJUNCTION procedural data diversity. procedural data diversity CONJUNCTION self - supervised representation learning. planning CONJUNCTION self - supervised representation learning. self - supervised representation learning CONJUNCTION planning. generalization CONJUNCTION data efficiency. data efficiency CONJUNCTION generalization. data efficiency EVALUATE-FOR Procgen. planning HYPONYM-OF procedural generalization. Procgen EVALUATE-FOR techniques. generalization EVALUATE-FOR techniques. data efficiency EVALUATE-FOR techniques. factors USED-FOR task generalization. Meta - World USED-FOR task generalization. single - task, model - free paradigm CONJUNCTION self - supervised model - based agents. self - supervised model - based agents CONJUNCTION single - task, model - free paradigm. rich, procedural, multi - task environments FEATURE-OF self - supervised model - based agents. single - task, model - free paradigm USED-FOR generalizable agents. Method is model - based reinforcement learning. OtherScientificTerm is internal model of the world. Task is transfer. ","This paper studies the generalization ability of model-based RL agents in the context of multi-task generalization. The authors show that model-free RL agents are able to generalize to new tasks in a single task, but not to new environments. They also show that the performance of MuZero, a model-focussed RL agent, is similar to that of a self-supervised RL agent in the setting where the agent is trained on a large number of tasks.","This paper studies the generalization ability of model-based RL agents in the context of multi-task generalization. The authors show that model-free RL agents are able to generalize to new tasks in a single task, but not to new environments. They also show that the performance of MuZero, a model-focussed RL agent, is similar to that of a self-supervised RL agent in the setting where the agent is trained on a large number of tasks."
5973,SP:ba80e35d452d894181d51624183b60541c0f3704,"fixed graph USED-FOR relational inductive biases. graph neural networks HYPONYM-OF Machine learning frameworks. fixed graph USED-FOR Machine learning frameworks. network inverse ( deconvolution ) problem USED-FOR graph learning task. eigendecomposition - based spectral methods CONJUNCTION iterative optimization solutions. iterative optimization solutions CONJUNCTION eigendecomposition - based spectral methods. Graph Deconvolution Network ( GDN ) HYPONYM-OF parameterized neural network architecture. GDNs USED-FOR link prediction or edge - weight regression tasks. GDNs USED-FOR distribution of graphs. loss function USED-FOR GDNs. loss function USED-FOR link prediction or edge - weight regression tasks. layers USED-FOR graph objects. GDNs USED-FOR larger - sized graphs. graph objects COMPARE node features. node features COMPARE graph objects. GDN USED-FOR graph recovery. synthetic data USED-FOR GDN. synthetic data USED-FOR graph recovery. Human Connectome Project - Young Adult neuroimaging dataset EVALUATE-FOR model. inferring structural brain networks USED-FOR model. functional connectivity USED-FOR inferring structural brain networks. functional connectivity USED-FOR model. Material is network data. OtherScientificTerm are graphs, graph convolutional relationship, observed and latent graphs, and structural brain networks. Task is inferring graph structure. Method is proximal gradient iterations. ","Machine learning frameworks based on graph neural networks, typically based on a fixed graph, can suffer from relational inductive biases due to the inability of the fixed graph to be decoupled from the network data. This paper proposes a novel parameterized neural network architecture called Graph Deconvolution Network (GDN) that decouples the graph convolutional relationship between the observed and latent graphs. The authors formulate the graph learning task as a network inverse (deconvolutions) problem, and propose eigendecomposition-based spectral methods and iterative optimization solutions. GDNs can be used for link prediction or edge-weight regression tasks by learning a loss function that encourages the distribution of graphs to be similar across layers, and for larger-sized graphs, GDNs are able to learn a distribution that is invariant to proximal gradient iterations. GDN is applied to graph recovery on synthetic data, and is shown to be able to recover graph objects from graph objects rather than node features. The model is also tested on the Human Connectome Project-Young Adult neuroimaging dataset, where the model is trained using functional connectivity for inferring structural brain networks. ","Machine learning frameworks based on graph neural networks, typically based on a fixed graph, can suffer from relational inductive biases due to the inability of the fixed graph to be decoupled from the network data. This paper proposes a novel parameterized neural network architecture called Graph Deconvolution Network (GDN) that decouples the graph convolutional relationship between the observed and latent graphs. The authors formulate the graph learning task as a network inverse (deconvolutions) problem, and propose eigendecomposition-based spectral methods and iterative optimization solutions. GDNs can be used for link prediction or edge-weight regression tasks by learning a loss function that encourages the distribution of graphs to be similar across layers, and for larger-sized graphs, GDNs are able to learn a distribution that is invariant to proximal gradient iterations. GDN is applied to graph recovery on synthetic data, and is shown to be able to recover graph objects from graph objects rather than node features. The model is also tested on the Human Connectome Project-Young Adult neuroimaging dataset, where the model is trained using functional connectivity for inferring structural brain networks. "
5989,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"Reward shaping ( RS ) USED-FOR reinforcement learning ( RL ). manually engineered shaping - reward functions USED-FOR RS. domain knowledge USED-FOR It. Reinforcement Learning Optimising Shaping Algorithm ( ROSA ) HYPONYM-OF automated RS framework. Markov game USED-FOR shaping - reward function. agent ( Controller ) USED-FOR optimal policy. agent ( Controller ) USED-FOR task. optimal policy USED-FOR task. shaped rewards USED-FOR agent ( Controller ). switching controls USED-FOR reward - shaping agent ( Shaper ). shaped rewards USED-FOR optimal policy. ROSA USED-FOR shapingreward function. shapingreward function USED-FOR task. RL algorithms USED-FOR ROSA. RS algorithms USED-FOR sparse reward environments. OtherScientificTerm are sparse or uninformative rewards, shaping rewards, and congenial properties. Task is autonomous learning. ","Reward shaping (RS) is an important problem in reinforcement learning (RL) and there are many manually engineered shaping-reward functions for RS. It can be used to leverage domain knowledge to improve the generalization ability of RL algorithms. This paper proposes an automated RS framework called the Reinforcement Learning Optimising Shaping Algorithm (ROSA), which is an extension of the standard RL algorithms to the setting where there are sparse or uninformative rewards. The key idea of the paper is to train an agent (controller) that learns a shaping-value function in a Markov game, and then use this agent (Controllers) to learn an optimal policy for a new task using the shaped rewards. This agent (Shaper) is trained with switching controls, and can be trained to learn a reward-shaping agent (shaper). The paper shows that the optimal policy can be learned using the learned shaped rewards, and that the shaping rewards have congenial properties. The paper also shows that a variant of ROSA can learn a shapingreward function that is suitable for the new task. The authors also show that the proposed RS algorithms can be applied to sparse reward environments.   ","Reward shaping (RS) is an important problem in reinforcement learning (RL) and there are many manually engineered shaping-reward functions for RS. It can be used to leverage domain knowledge to improve the generalization ability of RL algorithms. This paper proposes an automated RS framework called the Reinforcement Learning Optimising Shaping Algorithm (ROSA), which is an extension of the standard RL algorithms to the setting where there are sparse or uninformative rewards. The key idea of the paper is to train an agent (controller) that learns a shaping-value function in a Markov game, and then use this agent (Controllers) to learn an optimal policy for a new task using the shaped rewards. This agent (Shaper) is trained with switching controls, and can be trained to learn a reward-shaping agent (shaper). The paper shows that the optimal policy can be learned using the learned shaped rewards, and that the shaping rewards have congenial properties. The paper also shows that a variant of ROSA can learn a shapingreward function that is suitable for the new task. The authors also show that the proposed RS algorithms can be applied to sparse reward environments.   "
6005,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,Vertical Federated Learning ( VFL ) HYPONYM-OF distributed learning paradigm. backdoor attacks FEATURE-OF VFL. robustness EVALUATE-FOR VFL. horizontal federated learning COMPARE VFL. VFL COMPARE horizontal federated learning. RVFR HYPONYM-OF VFL training and inference framework. RVFR USED-FOR uncorrupted features. RVFR USED-FOR model. RVFR USED-FOR inferencephase adversarial and missing feature attacks. RVFR COMPARE baselines. baselines COMPARE RVFR. robustness EVALUATE-FOR baselines. robustness EVALUATE-FOR RVFR. Method is global model. OtherScientificTerm is features. Task is inference - phase attacks. Material is NUS - WIDE and CIFAR-10 datasets. ," of vertical federated learning (Vertical Federated Learning (VFL) is a distributed learning paradigm that is vulnerable to backdoor attacks. VFL has been shown to be robust against backdoor attacks, but the robustness of VFL is not well studied. In this paper, the authors propose a new VFL training and inference framework called RVFR, which is based on the observation that the global model can be corrupted in a way that the features of the local model are not robust to inference-phase attacks. The authors propose RVFR to learn uncorrupted features that can be used to improve the model’s robustness. Experiments on NUS-WIDE and CIFAR-10 datasets show that RVFR is robust against inferencephase adversarial and missing feature attacks, and is more robust than other baselines."," of vertical federated learning (Vertical Federated Learning (VFL) is a distributed learning paradigm that is vulnerable to backdoor attacks. VFL has been shown to be robust against backdoor attacks, but the robustness of VFL is not well studied. In this paper, the authors propose a new VFL training and inference framework called RVFR, which is based on the observation that the global model can be corrupted in a way that the features of the local model are not robust to inference-phase attacks. The authors propose RVFR to learn uncorrupted features that can be used to improve the model’s robustness. Experiments on NUS-WIDE and CIFAR-10 datasets show that RVFR is robust against inferencephase adversarial and missing feature attacks, and is more robust than other baselines."
6021,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,question answering CONJUNCTION fact checking. fact checking CONJUNCTION question answering. Information retrieval USED-FOR knowledge intensive tasks. Information retrieval HYPONYM-OF natural language processing. natural language processing USED-FOR knowledge intensive tasks. fact checking HYPONYM-OF knowledge intensive tasks. question answering HYPONYM-OF knowledge intensive tasks. dense retrievers COMPARE sparse methods. sparse methods COMPARE dense retrievers. dense retrievers PART-OF information retrieval. neural networks USED-FOR dense retrievers. term - frequency USED-FOR sparse methods. datasets EVALUATE-FOR models. BM25 HYPONYM-OF term - frequency methods. contrastive learning USED-FOR unsupervised dense retrievers. it USED-FOR retrieval. model COMPARE BM25. BM25 COMPARE model. BEIR benchmark EVALUATE-FOR model. model COMPARE BM25. BM25 COMPARE model. MS MARCO dataset EVALUATE-FOR technique. pre - training USED-FOR technique. pre - training CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pre - training. fine - tuning EVALUATE-FOR technique. MS MARCO dataset EVALUATE-FOR pre - training. MS MARCO dataset EVALUATE-FOR fine - tuning. BEIR benchmark EVALUATE-FOR technique. Generic is they. Material is new domains. OtherScientificTerm is supervision. ,"Information retrieval is an important component of natural language processing for knowledge intensive tasks such as question answering and fact checking. In this paper, the authors show that dense retrievers in information retrieval can be trained with neural networks and outperform sparse methods based on term-frequency (e.g. BM25). They also show that models trained with contrastive learning can outperform models trained on standard datasets.   The paper also shows that unsupervised dense retrieval can also be trained using contrastive loss and that it can be used to improve the quality of the retrieved information.  The authors also demonstrate that their model outperforms BM25 on the BEIR benchmark, and that their technique can be applied to pre-training and fine-tuning on the MS MARCO dataset. The authors claim that they are able to transfer knowledge from existing domains to new domains without any additional supervision.","Information retrieval is an important component of natural language processing for knowledge intensive tasks such as question answering and fact checking. In this paper, the authors show that dense retrievers in information retrieval can be trained with neural networks and outperform sparse methods based on term-frequency (e.g. BM25). They also show that models trained with contrastive learning can outperform models trained on standard datasets.   The paper also shows that unsupervised dense retrieval can also be trained using contrastive loss and that it can be used to improve the quality of the retrieved information.  The authors also demonstrate that their model outperforms BM25 on the BEIR benchmark, and that their technique can be applied to pre-training and fine-tuning on the MS MARCO dataset. The authors claim that they are able to transfer knowledge from existing domains to new domains without any additional supervision."
6037,SP:ed4e2896dc882bd089f420f719da232d706097c5,fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. fine - tuning HYPONYM-OF methods. linear probing HYPONYM-OF methods. accuracy in - distribution ( ID ) EVALUATE-FOR fine - tuning. accuracy EVALUATE-FOR fine - tuning. fine - tuning COMPARE linear probing. linear probing COMPARE fine - tuning. DomainNet CONJUNCTION CIFAR. CIFAR CONJUNCTION DomainNet. Breeds - Entity30 CONJUNCTION DomainNet. DomainNet CONJUNCTION Breeds - Entity30. distribution shift datasets EVALUATE-FOR fine - tuning. Breeds - Living17 CONJUNCTION Breeds - Entity30. Breeds - Entity30 CONJUNCTION Breeds - Living17. CIFAR10.1 CONJUNCTION FMoW. FMoW CONJUNCTION CIFAR10.1. accuracy OOD EVALUATE-FOR linear probing. CIFAR CONJUNCTION CIFAR10.1. CIFAR10.1 CONJUNCTION CIFAR. accuracy ID EVALUATE-FOR linear probing. CIFAR HYPONYM-OF distribution shift datasets. FMoW HYPONYM-OF distribution shift datasets. Breeds - Living17 HYPONYM-OF distribution shift datasets. CIFAR10.1 HYPONYM-OF distribution shift datasets. DomainNet HYPONYM-OF distribution shift datasets. Breeds - Entity30 HYPONYM-OF distribution shift datasets. accuracy OOD EVALUATE-FOR fine - tuning. accuracy ID EVALUATE-FOR fine - tuning. fine - tuning USED-FOR pretrained features. fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. linear probing COMPARE fine - tuning. fine - tuning COMPARE linear probing. ID and OOD accuracy EVALUATE-FOR fine - tuning. linear probing CONJUNCTION fine - tuning. fine - tuning CONJUNCTION linear probing. fine - tuning COMPARE fine - tuning. fine - tuning COMPARE fine - tuning. ID CONJUNCTION OOD. OOD CONJUNCTION ID. datasets EVALUATE-FOR fine - tuning. Method is pretrained model,"This paper proposes two methods: fine-tuning and linear probing, which is a combination of two popular methods. The authors show that fine-tuning improves accuracy in-distribution (ID) and OOD accuracy on three different distribution shift datasets: Breeds-Entropy30, DomainNet, CIFAR10.1, and FMoW. They also show that linear probing improves accuracy ID and accuracy OOD on the same amount of training data. They further show that the pretrained model is more robust to distribution shift, and that the accuracy of fine- tuning is better than linear probing on all three datasets. Finally, they show that for the three datasets (CIFAR-10, CifAR-100, and DomainNet), the accuracy ID is higher for fine tuning than for linear probing.    The authors conclude that the main contribution of this paper is that fine tuning is a useful way to improve the ID of a pre-trained model. However, the authors do not provide any theoretical analysis to support their claims.  They also do not discuss the impact of fine tuning on the accuracy on other datasets.  Finally, the paper concludes with a series of experiments that show that in addition to improving ID, fine tuning also improves OOD performance on all datasets. In particular, it is shown that the performance on the datasets with distribution shift is higher than the ones with no distribution shift.  The paper also shows that the results are consistent across all datasets and across different methods.  In the end, it seems to be the case that fine -tuning is more effective at improving ID and Ood accuracy. ","This paper proposes two methods: fine-tuning and linear probing, which is a combination of two popular methods. The authors show that fine-tuning improves accuracy in-distribution (ID) and OOD accuracy on three different distribution shift datasets: Breeds-Entropy30, DomainNet, CIFAR10.1, and FMoW. They also show that linear probing improves accuracy ID and accuracy OOD on the same amount of training data. They further show that the pretrained model is more robust to distribution shift, and that the accuracy of fine- tuning is better than linear probing on all three datasets. Finally, they show that for the three datasets (CIFAR-10, CifAR-100, and DomainNet), the accuracy ID is higher for fine tuning than for linear probing.    The authors conclude that the main contribution of this paper is that fine tuning is a useful way to improve the ID of a pre-trained model. However, the authors do not provide any theoretical analysis to support their claims.  They also do not discuss the impact of fine tuning on the accuracy on other datasets.  Finally, the paper concludes with a series of experiments that show that in addition to improving ID, fine tuning also improves OOD performance on all datasets. In particular, it is shown that the performance on the datasets with distribution shift is higher than the ones with no distribution shift.  The paper also shows that the results are consistent across all datasets and across different methods.  In the end, it seems to be the case that fine -tuning is more effective at improving ID and Ood accuracy. "
6053,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"meta - learning COMPARE L2DNC. L2DNC COMPARE meta - learning. meta - learning USED-FOR L2DNC. meta - learning algorithms USED-FOR L2DNC problem. meta - learning - based methodology USED-FOR unlabeled data. meta - learning - based methodology USED-FOR it. L2DNC COMPARE labeling in causality. labeling in causality COMPARE L2DNC. unseen - class data USED-FOR seen - class data. Material are labeled data, and limited data. Method is clustering models. OtherScientificTerm is high - level semantic features. ","This paper proposes a meta-learning-based algorithm to solve the problem of unlabeled data clustering in the presence of limited labeled data. The paper shows that meta-learners can be used to solve a variant of the L2DNC problem, where the labeled data is not available for training clustering models. The authors show that the proposed method is able to achieve better performance than existing meta learning algorithms.    The paper also shows that it is possible to use a meta learning-based methodology for unlabeling data, and that it can be applied to the case where high-level semantic features are not available.  The authors also show that meta learning outperforms the state-of-the-art meta learning methods in the case of limited data. In particular, they show that for unseen-class data, meta learning performs better than labeling in causality, which is the case when there is a large amount of unseen data.","This paper proposes a meta-learning-based algorithm to solve the problem of unlabeled data clustering in the presence of limited labeled data. The paper shows that meta-learners can be used to solve a variant of the L2DNC problem, where the labeled data is not available for training clustering models. The authors show that the proposed method is able to achieve better performance than existing meta learning algorithms.    The paper also shows that it is possible to use a meta learning-based methodology for unlabeling data, and that it can be applied to the case where high-level semantic features are not available.  The authors also show that meta learning outperforms the state-of-the-art meta learning methods in the case of limited data. In particular, they show that for unseen-class data, meta learning performs better than labeling in causality, which is the case when there is a large amount of unseen data."
6069,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"POMDPs USED-FOR model - based RL agents. online and offline experiences USED-FOR causal transition model. offline experiences USED-FOR agent. reinforcement learning CONJUNCTION causality. causality CONJUNCTION reinforcement learning. causal inference problem USED-FOR model - based reinforcement learning. offline data USED-FOR learning. methodology USED-FOR offline data. latent - based causal transition model USED-FOR interventional and observational regimes. latent - based causal transition model USED-FOR method. latent variable USED-FOR deconfounding. deconfounding USED-FOR POMDP transition model. latent variable USED-FOR POMDP transition model. generalization guarantees EVALUATE-FOR it. synthetic toy problems EVALUATE-FOR it. generalization guarantees EVALUATE-FOR method. synthetic toy problems EVALUATE-FOR method. OtherScientificTerm are online experiences, privileged information, and learning agent. Material are interventional data, and observational data. Method is causal framework of do - calculus. ","This paper considers the problem of training model-based RL agents in POMDPs, where the agent is trained using both online and offline experiences. The authors propose a causal transition model that can be used to model the relationship between the online experiences and the offline experiences in order to train an agent that can generalize well to unseen environments. This is a natural extension of the causal inference problem that is commonly used in the context of both reinforcement learning and causality.   The paper proposes a methodology for learning from offline data. The proposed method is based on the use of a latent-based causal model for both the interventional and observational regimes. In the case of interventional data, the latent variable is used to perform deconfounding, which is used as a proxy for the causal framework of do-calculus. In contrast, in the observational regime, the learning agent is learned from the offline data, and the learned latent variable serves as a surrogate for the decoder.  The authors show that the proposed method achieves good generalization guarantees for their method on synthetic toy problems, and show that it is able to generalize to unseen domains. ","This paper considers the problem of training model-based RL agents in POMDPs, where the agent is trained using both online and offline experiences. The authors propose a causal transition model that can be used to model the relationship between the online experiences and the offline experiences in order to train an agent that can generalize well to unseen environments. This is a natural extension of the causal inference problem that is commonly used in the context of both reinforcement learning and causality.   The paper proposes a methodology for learning from offline data. The proposed method is based on the use of a latent-based causal model for both the interventional and observational regimes. In the case of interventional data, the latent variable is used to perform deconfounding, which is used as a proxy for the causal framework of do-calculus. In contrast, in the observational regime, the learning agent is learned from the offline data, and the learned latent variable serves as a surrogate for the decoder.  The authors show that the proposed method achieves good generalization guarantees for their method on synthetic toy problems, and show that it is able to generalize to unseen domains. "
6085,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"textual knowledge corpus USED-FOR retriever. Wikipedia HYPONYM-OF textual knowledge corpus. retriever USED-FOR text generation systems. methods USED-FOR generator. retriever CONJUNCTION generator. generator CONJUNCTION retriever. methods USED-FOR retriever. generating informative utterances in conversations HYPONYM-OF open - ended generation tasks. retriever CONJUNCTION generator. generator CONJUNCTION retriever. informative conversations EVALUATE-FOR retriever. generator USED-FOR it. Wizard of Wikipedia dataset FEATURE-OF informative conversations. retriever USED-FOR it. posterior - guided training USED-FOR informative conversations. evidence lower bound ( ELBo ) USED-FOR it. posterior distribution Q USED-FOR guide retriever. OtherScientificTerm are top-10, and generator ’s responses. Method is end - to - end system. ","This paper proposes a retriever for text generation systems that is trained on a textual knowledge corpus (e.g., Wikipedia). The retriever and generator are trained in an end-to-end manner, with methods to guide the generator and the retriever. The paper focuses on open-ended generation tasks such as generating informative utterances in conversations, where the goal is to generate sentences that are more likely to be in the top-10 of the Wizard of Wikipedia dataset.  The paper shows that a guide retriever trained with posterior distribution Q is able to generate informative conversations from informative conversations generated by a retrieever and a generator, and that it can be trained with evidence lower bound (ELBo) and it can also be trained using posterior-guided training. The authors also show that the generator’s responses can be used as a guide to guide a retrained retriever to generate more informative sentences.    The authors claim that this is the first work that uses an end to end system to generate a sequence of utterances from an informative conversation. ","This paper proposes a retriever for text generation systems that is trained on a textual knowledge corpus (e.g., Wikipedia). The retriever and generator are trained in an end-to-end manner, with methods to guide the generator and the retriever. The paper focuses on open-ended generation tasks such as generating informative utterances in conversations, where the goal is to generate sentences that are more likely to be in the top-10 of the Wizard of Wikipedia dataset.  The paper shows that a guide retriever trained with posterior distribution Q is able to generate informative conversations from informative conversations generated by a retrieever and a generator, and that it can be trained with evidence lower bound (ELBo) and it can also be trained using posterior-guided training. The authors also show that the generator’s responses can be used as a guide to guide a retrained retriever to generate more informative sentences.    The authors claim that this is the first work that uses an end to end system to generate a sequence of utterances from an informative conversation. "
6101,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"machine learning USED-FOR combinatorial optimization. real - world problems USED-FOR large graphs. influence maximization HYPONYM-OF NP - hard problem. influence estimation HYPONYM-OF # P - hard counting problem. influence estimation CONJUNCTION influence maximization. influence maximization CONJUNCTION influence estimation. influence estimation HYPONYM-OF problems. influence maximization HYPONYM-OF problems. Graph Neural Network ( GNN ) USED-FOR upper bound of influence estimation. GLIE HYPONYM-OF Graph Neural Network ( GNN ). small simulated graphs USED-FOR it. GLIE USED-FOR influence estimation. real graphs USED-FOR influence estimation. it USED-FOR influence maximization. GLIE CONJUNCTION simulated influence estimation. simulated influence estimation CONJUNCTION GLIE. simulated influence estimation USED-FOR Lazy Forward optimization. GLIE USED-FOR Lazy Forward optimization. time complexity CONJUNCTION quality of influence. quality of influence CONJUNCTION time complexity. first HYPONYM-OF Q - network. GLIE ’s predictions USED-FOR Q - network. second USED-FOR provably submodular function. GLIE ’s representations USED-FOR provably submodular function. time efficiency CONJUNCTION influence spread. influence spread CONJUNCTION time efficiency. latter COMPARE SOTA benchmarks. SOTA benchmarks COMPARE latter. influence spread EVALUATE-FOR latter. time efficiency EVALUATE-FOR latter. Generic are perspective, and approaches. Task is small graph problems. OtherScientificTerm are predictions ranking, and computational overhead. Metric is accuracy. ","This paper presents a new perspective on the use of machine learning for combinatorial optimization. The authors consider two problems: influence estimation and influence maximization, which is a #P-hard counting problem (influence maximization is an NP-hard problem). The authors propose a Graph Neural Network (GNN) for the upper bound of influence estimation, called GLIE, and apply it to two real-world problems for large graphs and two small graph problems.   The authors show that GLIE can be used for influence estimation on real graphs, and it can also be used to perform Lazy Forward optimization on small simulated graphs.  The main contribution of the paper is that the authors propose two approaches to improve the performance of GLIE. The first is a Q-network based on GLIE’s predictions, and the second is a provably submodular function based on the representations of two Q-networks. The latter achieves better time efficiency and better influence spread compared to the SOTA benchmarks. The paper also shows that the predictions ranking can be improved by using GLIE in order to reduce computational overhead. ","This paper presents a new perspective on the use of machine learning for combinatorial optimization. The authors consider two problems: influence estimation and influence maximization, which is a #P-hard counting problem (influence maximization is an NP-hard problem). The authors propose a Graph Neural Network (GNN) for the upper bound of influence estimation, called GLIE, and apply it to two real-world problems for large graphs and two small graph problems.   The authors show that GLIE can be used for influence estimation on real graphs, and it can also be used to perform Lazy Forward optimization on small simulated graphs.  The main contribution of the paper is that the authors propose two approaches to improve the performance of GLIE. The first is a Q-network based on GLIE’s predictions, and the second is a provably submodular function based on the representations of two Q-networks. The latter achieves better time efficiency and better influence spread compared to the SOTA benchmarks. The paper also shows that the predictions ranking can be improved by using GLIE in order to reduce computational overhead. "
6117,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"active learning strategies USED-FOR domain adaptation. localized class of functions USED-FOR labeling. Rademacher average CONJUNCTION localized discrepancy. localized discrepancy CONJUNCTION Rademacher average. localized discrepancy FEATURE-OF loss functions. generalization error bounds USED-FOR active learning strategies. regularity condition FEATURE-OF loss functions. Kmedoids algorithm USED-FOR large data set. theoretical bounds USED-FOR Kmedoids algorithm. algorithm COMPARE active learning techniques. active learning techniques COMPARE algorithm. active learning techniques USED-FOR domain adaptation. algorithm USED-FOR domain adaptation. large data sets EVALUATE-FOR algorithm. OtherScientificTerm are assumption of Lipschitz functions, and discrepancy distance. ","This paper studies the generalization error bounds of active learning strategies for domain adaptation. The authors consider the assumption of Lipschitz functions, where the labeling is based on a localized class of functions, and the goal is to minimize the discrepancy distance between the Rademacher average and the localized discrepancy between the two sets of loss functions under the regularity condition. They provide theoretical bounds for the Kmedoids algorithm for a large data set, and show that under the assumption that the loss functions satisfy a certain form of regularity, the proposed algorithm achieves better generalization errors than existing active learning techniques for the problem of domain adaptation on large data sets. ","This paper studies the generalization error bounds of active learning strategies for domain adaptation. The authors consider the assumption of Lipschitz functions, where the labeling is based on a localized class of functions, and the goal is to minimize the discrepancy distance between the Rademacher average and the localized discrepancy between the two sets of loss functions under the regularity condition. They provide theoretical bounds for the Kmedoids algorithm for a large data set, and show that under the assumption that the loss functions satisfy a certain form of regularity, the proposed algorithm achieves better generalization errors than existing active learning techniques for the problem of domain adaptation on large data sets. "
6133,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,variational approximation USED-FOR Bayesian neural networks. singular statistical models USED-FOR neural networks. posterior distribution FEATURE-OF singular model. desingularization map HYPONYM-OF algebraic - geometrical transformation. generalized gamma mean - field variational family USED-FOR leading order term. leading order term FEATURE-OF model evidence. generalized gamma mean - field variational family USED-FOR model evidence. desingularization USED-FOR generalized gamma mean - field variational family. Affine coupling layers USED-FOR unknown desingularization map. source distribution USED-FOR normalizing flow. normalizing flow USED-FOR methodology. generalized gamma USED-FOR normalizing flow. Generic is approximation. Method is singular learning theory. OtherScientificTerm is mixture of standard forms. ,"This paper proposes a variational approximation for Bayesian neural networks based on singular statistical models. The authors show that the posterior distribution of a singular model can be approximated by a desingularization map, which is an algebraic-geometrical transformation. The approximation is based on a generalized gamma mean-field variational family of the leading order term of the model evidence. Affine coupling layers are used to map the unknown desingularity map to a mixture of standard forms. The methodology relies on a normalizing flow based on the generalized gamma of the source distribution. ","This paper proposes a variational approximation for Bayesian neural networks based on singular statistical models. The authors show that the posterior distribution of a singular model can be approximated by a desingularization map, which is an algebraic-geometrical transformation. The approximation is based on a generalized gamma mean-field variational family of the leading order term of the model evidence. Affine coupling layers are used to map the unknown desingularity map to a mixture of standard forms. The methodology relies on a normalizing flow based on the generalized gamma of the source distribution. "
6149,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"model USED-FOR domain generalization ( DG ) problem setting. known data distributions USED-FOR model. methods COMPARE empirical risk minimisation baseline. empirical risk minimisation baseline COMPARE methods. learning - theoretic generalisation bound USED-FOR DG. learning - theoretic generalisation bound USED-FOR domain generalisation. Rademacher complexity FEATURE-OF model. empirical risk - predictor complexity trade - off EVALUATE-FOR methods. regularised ERM USED-FOR domain generalisation. Task are general purpose DG, and DG problem. Material is DomainBed benchmark. ","This paper considers the domain generalization (DG) problem setting, where a model is trained on known data distributions and the goal is to generalize the model to unseen domains. The authors propose two methods that achieve a Rademacher complexity of $O(\sqrt{T})$ for general purpose DG and $O(1/T)$ for domain generalisation. The proposed methods are shown to achieve a better empirical risk-predictor complexity trade-off compared to the empirical risk minimisation baseline. They also provide a learning-theoretic generalisation bound for DG, which shows that the model has a RadEmacher complexity that matches that of the original DG problem. Finally, the authors propose a regularised ERM to improve the performance of the proposed methods. Experiments are conducted on the DomainBed benchmark. ","This paper considers the domain generalization (DG) problem setting, where a model is trained on known data distributions and the goal is to generalize the model to unseen domains. The authors propose two methods that achieve a Rademacher complexity of $O(\sqrt{T})$ for general purpose DG and $O(1/T)$ for domain generalisation. The proposed methods are shown to achieve a better empirical risk-predictor complexity trade-off compared to the empirical risk minimisation baseline. They also provide a learning-theoretic generalisation bound for DG, which shows that the model has a RadEmacher complexity that matches that of the original DG problem. Finally, the authors propose a regularised ERM to improve the performance of the proposed methods. Experiments are conducted on the DomainBed benchmark. "
6165,SP:b1f622cbc827e880f98de9e99eca498584efe011,overlays USED-FOR maximum n - times coverage problem. multi - set multi - cover problem USED-FOR Maximum n - times coverage. integer linear programming CONJUNCTION sequential greedy optimization. sequential greedy optimization CONJUNCTION integer linear programming. solutions USED-FOR n - times coverage. integer linear programming USED-FOR solutions. sequential greedy optimization USED-FOR solutions. it USED-FOR pan - strain COVID-19 vaccine design. pan - strain COVID-19 vaccine design COMPARE designs. designs COMPARE pan - strain COVID-19 vaccine design. maximum n - times coverage USED-FOR peptide vaccine design. predicted population coverage EVALUATE-FOR pan - strain COVID-19 vaccine design. predicted population coverage EVALUATE-FOR designs. Task is min - cost n - times coverage problem. OtherScientificTerm is HLA molecules. ,"This paper studies the min-cost n-times coverage problem, which is a generalization of the max-covering problem in the setting of multi-set multi-coverage problem. The authors consider the setting where the number of coverings is large enough to cover the entire population, but the coverage of a subset of the coverings can be much smaller than the total coverage of the whole population.    The authors propose a novel solution to the maximum n - times coverage problem which is based on the observation that there are many overlapping overlays in the max n-coverings problem, and the authors show that there exists a solution to this problem that is optimal for all coverings. Maximum n- times coverage is then formulated as a multi-sets multi-coverage problem, where each coverage is a set of HLA molecules.  The proposed solutions are based on integer linear programming and sequential greedy optimization, and it is applied to the pan-strain COVID-19 vaccine design, where it is shown to achieve better predicted population coverage compared to previous designs that do not consider maximum n-time coverage. The proposed peptide vaccine design is also shown to have maximum n times coverage. ","This paper studies the min-cost n-times coverage problem, which is a generalization of the max-covering problem in the setting of multi-set multi-coverage problem. The authors consider the setting where the number of coverings is large enough to cover the entire population, but the coverage of a subset of the coverings can be much smaller than the total coverage of the whole population.    The authors propose a novel solution to the maximum n - times coverage problem which is based on the observation that there are many overlapping overlays in the max n-coverings problem, and the authors show that there exists a solution to this problem that is optimal for all coverings. Maximum n- times coverage is then formulated as a multi-sets multi-coverage problem, where each coverage is a set of HLA molecules.  The proposed solutions are based on integer linear programming and sequential greedy optimization, and it is applied to the pan-strain COVID-19 vaccine design, where it is shown to achieve better predicted population coverage compared to previous designs that do not consider maximum n-time coverage. The proposed peptide vaccine design is also shown to have maximum n times coverage. "
6181,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"supervised learning algorithms USED-FOR SNNs. power consumption CONJUNCTION computational capability. computational capability CONJUNCTION power consumption. information encoding CONJUNCTION power consumption. power consumption CONJUNCTION information encoding. information encoding EVALUATE-FOR spiking neural networks ( SNNs ). weight initialization USED-FOR SNN training. It USED-FOR gradient generation. asymptotic formula USED-FOR response curve. asymptotic formula USED-FOR spiking neurons. initialization method USED-FOR gradient vanishing. slant asymptote USED-FOR initialization method. coding schemes USED-FOR classification tasks. method COMPARE deep learning initialization methods. deep learning initialization methods COMPARE method. method COMPARE SNN initialization methods. SNN initialization methods COMPARE method. deep learning initialization methods CONJUNCTION SNN initialization methods. SNN initialization methods CONJUNCTION deep learning initialization methods. model accuracy EVALUATE-FOR deep learning initialization methods. model accuracy EVALUATE-FOR SNN initialization methods. coding schemes EVALUATE-FOR method. training speed CONJUNCTION model accuracy. model accuracy CONJUNCTION training speed. MNIST and CIFAR10 dataset EVALUATE-FOR coding schemes. MNIST and CIFAR10 dataset USED-FOR classification tasks. training speed EVALUATE-FOR method. model accuracy EVALUATE-FOR method. Method is backpropagation. OtherScientificTerm are neuron response distribution, and training hyperparameters. Generic is methods. ","This paper studies the problem of initialization of spiking neural networks (SNNs) in terms of information encoding, power consumption, and computational capability. The authors consider supervised learning algorithms for SNNs, and show that weight initialization during SNN training can be regarded as a special case of weight initialization in the context of backpropagation, where the neuron response distribution is a function of the training hyperparameters. It is shown that the gradient generation of a SNN with weight initialization converges to zero as the number of neurons increases, and that spiking neurons have an asymptotic formula for the response curve. The paper then proposes an initialization method that avoids gradient vanishing by using a slant asymPTote. Experiments on MNIST and CIFAR10 dataset show that the proposed method outperforms other deep learning initialization methods and SNN initialization methods on training speed, model accuracy, and coding schemes for classification tasks. ","This paper studies the problem of initialization of spiking neural networks (SNNs) in terms of information encoding, power consumption, and computational capability. The authors consider supervised learning algorithms for SNNs, and show that weight initialization during SNN training can be regarded as a special case of weight initialization in the context of backpropagation, where the neuron response distribution is a function of the training hyperparameters. It is shown that the gradient generation of a SNN with weight initialization converges to zero as the number of neurons increases, and that spiking neurons have an asymptotic formula for the response curve. The paper then proposes an initialization method that avoids gradient vanishing by using a slant asymPTote. Experiments on MNIST and CIFAR10 dataset show that the proposed method outperforms other deep learning initialization methods and SNN initialization methods on training speed, model accuracy, and coding schemes for classification tasks. "
6197,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"Federated learning USED-FOR machine learning models. mobile devices FEATURE-OF decentralized client data. decentralized client data USED-FOR machine learning models. model USED-FOR federated learning systems. mixture of distributions USED-FOR it. block - cyclic pattern USED-FOR distribution shift. light - weight branches USED-FOR network. image classification CONJUNCTION next word prediction. next word prediction CONJUNCTION image classification. algorithm USED-FOR distribution shift. image classification EVALUATE-FOR algorithm. model EVALUATE-FOR algorithm. Stack Overflow dataset USED-FOR next word prediction. EMNIST and CIFAR datasets USED-FOR image classification. OtherScientificTerm are periodically shifting distributions, and daytime and nighttime modes. Method are Federated Expectation - Maximization algorithm, and mixture model. ","This paper studies the problem of federated learning for machine learning models with decentralized client data coming from mobile devices. The authors propose a new model for the problem, which they call Federated Expectation-Maximization algorithm. The proposed model is a generalization of existing federated models for learning with periodically shifting distributions. In particular, it is based on a mixture of distributions, and the authors propose to use a block-cyclic pattern to mitigate the distribution shift. The network is split into two light-weight branches, and each branch is trained to maximize the mutual information between the weights of the two light branches. The algorithm is evaluated on image classification on EMNIST and CIFAR datasets, and on Stack Overflow dataset for next word prediction. The results show that the proposed algorithm is able to mitigate distribution shift in both the daytime and nighttime modes, and that the model is more robust to changes in the number of clients in the model.   ","This paper studies the problem of federated learning for machine learning models with decentralized client data coming from mobile devices. The authors propose a new model for the problem, which they call Federated Expectation-Maximization algorithm. The proposed model is a generalization of existing federated models for learning with periodically shifting distributions. In particular, it is based on a mixture of distributions, and the authors propose to use a block-cyclic pattern to mitigate the distribution shift. The network is split into two light-weight branches, and each branch is trained to maximize the mutual information between the weights of the two light branches. The algorithm is evaluated on image classification on EMNIST and CIFAR datasets, and on Stack Overflow dataset for next word prediction. The results show that the proposed algorithm is able to mitigate distribution shift in both the daytime and nighttime modes, and that the model is more robust to changes in the number of clients in the model.   "
6213,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"accuracy EVALUATE-FOR winning ticket ” subnetwork. pruning USED-FOR DN ’s decision boundary. pruning DN nodes USED-FOR decision boundary. spline interpretation of DNs USED-FOR theory and visualization tools. DN spline theory CONJUNCTION lottery ticket hypothesis of DNs. lottery ticket hypothesis of DNs CONJUNCTION DN spline theory. early - bird ( EB ) phenomenon FEATURE-OF DN ’s spline mappings. pruning strategy USED-FOR DN nodes. spline partition regions FEATURE-OF DN nodes. spline - based DN pruning approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE spline - based DN pruning approach. accuracy EVALUATE-FOR state - of - the - art methods. networks EVALUATE-FOR spline - based DN pruning approach. spline - based DN pruning approach USED-FOR training FLOPs. accuracy EVALUATE-FOR spline - based DN pruning approach. Method are deep network ( DN ) training, and pruning technique. Generic is model. OtherScientificTerm is spline ’s partition. ","This paper studies the problem of deep network (DN) training and pruning. The authors propose a new pruning technique based on the spline interpretation of DNs. They show that pruning a “winning ticket” subnetwork can improve the accuracy of the whole model. They also show that the pruning of DN’s decision boundary can be achieved by pruning DN nodes in the decision boundary.    The authors draw inspiration from the DN spline theory and the lottery ticket hypothesis ofDNs.  They also propose a pruning strategy that prunes DN nodes that are located in the so-called spline partition regions of the model.  In particular, the authors show that DN nodes are pruned in the early-bird (EB) phenomenon, which is a phenomenon that occurs when the model is pruned early in the training process.  The paper also proposes a theory and visualization tools that can be used to analyze the impact of pruning in terms of the number of nodes in a spline's partition.  Finally, they show that their spline-based DN pruning approach is able to reduce training FLOPs and achieve higher accuracy than state-of-the-art methods on several networks. ","This paper studies the problem of deep network (DN) training and pruning. The authors propose a new pruning technique based on the spline interpretation of DNs. They show that pruning a “winning ticket” subnetwork can improve the accuracy of the whole model. They also show that the pruning of DN’s decision boundary can be achieved by pruning DN nodes in the decision boundary.    The authors draw inspiration from the DN spline theory and the lottery ticket hypothesis ofDNs.  They also propose a pruning strategy that prunes DN nodes that are located in the so-called spline partition regions of the model.  In particular, the authors show that DN nodes are pruned in the early-bird (EB) phenomenon, which is a phenomenon that occurs when the model is pruned early in the training process.  The paper also proposes a theory and visualization tools that can be used to analyze the impact of pruning in terms of the number of nodes in a spline's partition.  Finally, they show that their spline-based DN pruning approach is able to reduce training FLOPs and achieve higher accuracy than state-of-the-art methods on several networks. "
6229,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"data representation USED-FOR optimal predictors. inner - level USED-FOR optimal group predictors. bi - level optimization USED-FOR problem. inner optimization CONJUNCTION implicit differentiation. implicit differentiation CONJUNCTION inner optimization. implicit differentiation CONJUNCTION optimization path. optimization path CONJUNCTION implicit differentiation. implicit differentiation USED-FOR implicit path alignment algorithm. inner optimization USED-FOR implicit path alignment algorithm. sufficiency rule FEATURE-OF bi - level objective. error gap EVALUATE-FOR implicit approach. classification and regression settings EVALUATE-FOR method. Task are fair representation learning perspective, and fairness measurement. Generic is representation. Method are inner - level optimization, and fair representation learning. ","This paper considers the problem of fair representation learning from a fair representation representation learning perspective. The authors propose to learn optimal predictors from the data representation. The problem is formulated as a bi-level optimization problem, where the inner-level tries to find optimal group predictors that maximize the likelihood of the representation, while the outer-level optimizes the fairness measurement. The paper proposes an implicit path alignment algorithm based on inner optimization, implicit differentiation, and an optimization path. The implicit approach is shown to have a lower error gap than existing methods, and the paper also shows that the bi-levels of inner optimization and implicit differentiation converge to the optimal solution. In addition, the paper shows that there exists a sufficiency rule for the bi -level objective. The method is tested in both classification and regression settings, and it is shown that the proposed method achieves better performance than existing baselines. The main contribution of the paper is that the authors propose an implicit approach that does not rely on the need to explicitly optimize the representation at the inner level, but instead uses the results of the inner optimization at the outer level to guide the optimization of the implicit path.    The paper is well-written, well-motivated, and easy to follow. The idea of the authors is interesting. However, there are a few issues that prevent the paper from being a clear contribution to the field of fairness representation learning. ","This paper considers the problem of fair representation learning from a fair representation representation learning perspective. The authors propose to learn optimal predictors from the data representation. The problem is formulated as a bi-level optimization problem, where the inner-level tries to find optimal group predictors that maximize the likelihood of the representation, while the outer-level optimizes the fairness measurement. The paper proposes an implicit path alignment algorithm based on inner optimization, implicit differentiation, and an optimization path. The implicit approach is shown to have a lower error gap than existing methods, and the paper also shows that the bi-levels of inner optimization and implicit differentiation converge to the optimal solution. In addition, the paper shows that there exists a sufficiency rule for the bi -level objective. The method is tested in both classification and regression settings, and it is shown that the proposed method achieves better performance than existing baselines. The main contribution of the paper is that the authors propose an implicit approach that does not rely on the need to explicitly optimize the representation at the inner level, but instead uses the results of the inner optimization at the outer level to guide the optimization of the implicit path.    The paper is well-written, well-motivated, and easy to follow. The idea of the authors is interesting. However, there are a few issues that prevent the paper from being a clear contribution to the field of fairness representation learning. "
6245,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"value - based methods USED-FOR Offline reinforcement learning ( RL ). temporal difference ( TD ) learning USED-FOR value - based methods. supervised learning methods USED-FOR optimal policies. methods COMPARE value - based approximate dynamic programming algorithms. value - based approximate dynamic programming algorithms COMPARE methods. design decisions PART-OF methods. policy architectures HYPONYM-OF design decisions. large sequence models CONJUNCTION value - based weighting schemes. value - based weighting schemes CONJUNCTION large sequence models. RvS methods COMPARE prior methods. prior methods COMPARE RvS methods. optimal data FEATURE-OF datasets. datasets HYPONYM-OF offline RL benchmarks. offline RL benchmarks EVALUATE-FOR prior methods. offline RL benchmarks EVALUATE-FOR RvS methods. Task is offline RL problem. Generic is task. Material is suboptimal data. Method is reinforcement learning via supervised learning ( RvS ). OtherScientificTerm are conditioning variable, and model capacity. "," Offline reinforcement learning (RL) is an important problem in many domains, and value-based methods based on temporal difference (TD) learning have been shown to be effective for this task. However, the offline RL problem is challenging because of the lack of access to suboptimal data, and the need for reinforcement learning via supervised learning (RvS). In this paper, the authors propose two supervised learning methods to learn optimal policies in this setting. The authors show that their methods outperform state-of-the-art value- based approximate dynamic programming algorithms. They also show that the proposed methods are able to incorporate design decisions such as policy architectures and design decisions about the conditioning variable, and that the methods are more robust to design decisions regarding the model capacity. The paper also shows that RvS methods can outperform prior methods on several offline RL benchmarks with optimal data (i.e., datasets with optimal performance on optimal data). The authors also show the benefits of large sequence models and value -based weighting schemes."," Offline reinforcement learning (RL) is an important problem in many domains, and value-based methods based on temporal difference (TD) learning have been shown to be effective for this task. However, the offline RL problem is challenging because of the lack of access to suboptimal data, and the need for reinforcement learning via supervised learning (RvS). In this paper, the authors propose two supervised learning methods to learn optimal policies in this setting. The authors show that their methods outperform state-of-the-art value- based approximate dynamic programming algorithms. They also show that the proposed methods are able to incorporate design decisions such as policy architectures and design decisions about the conditioning variable, and that the methods are more robust to design decisions regarding the model capacity. The paper also shows that RvS methods can outperform prior methods on several offline RL benchmarks with optimal data (i.e., datasets with optimal performance on optimal data). The authors also show the benefits of large sequence models and value -based weighting schemes."
6261,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,computational cognitive mechanisms USED-FOR exploration algorithms. spatial information USED-FOR structure of unobserved spaces. Hierarchical Bayesian framework USED-FOR program induction. program induction USED-FOR compositional formation of proposed maps of complex spaces. Map Induction USED-FOR cognitive process. distribution of strong spatial priors USED-FOR model. computational framework USED-FOR human exploration behavior. computational framework COMPARE non - inductive models. non - inductive models COMPARE computational framework. Map Induction USED-FOR approximate planning algorithms. Task is behavioral Map Induction Task. Method is Map Induction framework. ,"This paper proposes a new behavioral Map Induction Task. The authors propose to use computational cognitive mechanisms to learn exploration algorithms based on the notion of program induction in the Hierarchical Bayesian framework. The idea is to use spatial information to model the structure of unobserved spaces and use program induction to learn the compositional formation of proposed maps of complex spaces. The model is trained using a distribution of strong spatial priors.  The authors show that the proposed computational framework is able to capture human exploration behavior better than existing non-inductive models. They also show that MapInduction can be used to learn approximate planning algorithms.   The paper is well-written, well-motivated, and well-structured. However, there are a few issues that need to be addressed in order for the paper to be accepted as a contribution to the field.  I have read the authors' response and other reviewers' comments. I am leaning towards accepting the paper, but am not convinced by the paper. ","This paper proposes a new behavioral Map Induction Task. The authors propose to use computational cognitive mechanisms to learn exploration algorithms based on the notion of program induction in the Hierarchical Bayesian framework. The idea is to use spatial information to model the structure of unobserved spaces and use program induction to learn the compositional formation of proposed maps of complex spaces. The model is trained using a distribution of strong spatial priors.  The authors show that the proposed computational framework is able to capture human exploration behavior better than existing non-inductive models. They also show that MapInduction can be used to learn approximate planning algorithms.   The paper is well-written, well-motivated, and well-structured. However, there are a few issues that need to be addressed in order for the paper to be accepted as a contribution to the field.  I have read the authors' response and other reviewers' comments. I am leaning towards accepting the paper, but am not convinced by the paper. "
6277,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"differentiable approach USED-FOR probabilistic factors. probabilistic factors USED-FOR inference. nonparametric belief propagation algorithm USED-FOR differentiable approach. nonparametric belief propagation algorithm USED-FOR inference. probabilistic factors PART-OF graphical model. domain - specific features FEATURE-OF probabilistic factors. domain - specific features USED-FOR nonparametric belief propagation methods. differentiable neural network USED-FOR factors. labeled data USED-FOR optimization routine. differentiable neural network USED-FOR crafted factor. optimization routine USED-FOR factors. differentiable neural networks CONJUNCTION belief propagation algorithm. belief propagation algorithm CONJUNCTION differentiable neural networks. method USED-FOR marginal posterior samples. differentiable neural networks USED-FOR method. end - to - end training USED-FOR marginal posterior samples. end - to - end training USED-FOR method. differentiable nonparametric belief propagation ( DNBP ) method COMPARE learned baselines. learned baselines COMPARE differentiable nonparametric belief propagation ( DNBP ) method. articulated pose tracking tasks EVALUATE-FOR differentiable nonparametric belief propagation ( DNBP ) method. learned factors USED-FOR tracking. Method are hand - crafted approaches, and Gradient Descent. OtherScientificTerm is Feature Extractor. ","This paper proposes a differentiable approach to learn probabilistic factors in a graphical model using a nonparametric belief propagation algorithm for inference. This is a natural extension of existing hand-crafted approaches that rely on domain-specific features in the training data to improve the performance of the probabilistically related factors. The authors argue that the existing state-of-the-art hand-crafted approaches are limited by the fact that they rely on the use of a single feature extractor, which can be suboptimal in practice. To address this issue, the authors propose to use differentiable belief propagation methods to learn a set of differentiable factors that can be incorporated into the training process.    The authors propose a method that uses differentiable neural networks to learn the factors and a belief propagation method to generate marginal posterior samples from the learned factors. In particular, they use Gradient Descent to train the Feature Extractor and then use the differentiable network to learn these factors using an optimization routine based on the labeled data. The method uses end-to-end training to generate the marginal posteriores.  Experiments on articulated pose tracking tasks show that the proposed proposed differentiable nonparametry belief propagation (DNBP) method outperforms learned baselines. The paper also shows that learned factors can be used for tracking using learned factors, and that the method can be applied to improve tracking performance. ","This paper proposes a differentiable approach to learn probabilistic factors in a graphical model using a nonparametric belief propagation algorithm for inference. This is a natural extension of existing hand-crafted approaches that rely on domain-specific features in the training data to improve the performance of the probabilistically related factors. The authors argue that the existing state-of-the-art hand-crafted approaches are limited by the fact that they rely on the use of a single feature extractor, which can be suboptimal in practice. To address this issue, the authors propose to use differentiable belief propagation methods to learn a set of differentiable factors that can be incorporated into the training process.    The authors propose a method that uses differentiable neural networks to learn the factors and a belief propagation method to generate marginal posterior samples from the learned factors. In particular, they use Gradient Descent to train the Feature Extractor and then use the differentiable network to learn these factors using an optimization routine based on the labeled data. The method uses end-to-end training to generate the marginal posteriores.  Experiments on articulated pose tracking tasks show that the proposed proposed differentiable nonparametry belief propagation (DNBP) method outperforms learned baselines. The paper also shows that learned factors can be used for tracking using learned factors, and that the method can be applied to improve tracking performance. "
6293,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"deep learning - based modeling of molecules USED-FOR in silico drug discovery. scaffold USED-FOR drug discovery projects. MoLeR HYPONYM-OF graph - based model. MoLeR COMPARE approaches. approaches COMPARE MoLeR. MoLeR COMPARE methods. methods COMPARE MoLeR. MoLeR COMPARE them. them COMPARE MoLeR. them COMPARE approaches. approaches COMPARE them. scaffoldbased tasks EVALUATE-FOR MoLeR. scaffoldbased tasks EVALUATE-FOR them. unconstrained molecular optimization tasks EVALUATE-FOR methods. unconstrained molecular optimization tasks EVALUATE-FOR MoLeR. Method are generative models, and generative procedure. OtherScientificTerm are fragmentby - fragment, scaffolds, and generation history. Generic is constraint. ","This paper proposes a deep learning-based modeling of molecules for in silico drug discovery. The authors propose a graph-based model, called MoLeR, that learns a scaffold for drug discovery projects. The idea is to learn a set of generative models that are able to generate molecules fragmentby-fragment, where each fragment is a constraint on the next molecule to be generated. The scaffolds are learned in an end-to-end manner, with each generation procedure being conditioned on the previous generation history. The paper shows that MoLER outperforms existing approaches on a number of unconstrained molecular optimization tasks, and outperforms them on several scaffoldbased tasks. ","This paper proposes a deep learning-based modeling of molecules for in silico drug discovery. The authors propose a graph-based model, called MoLeR, that learns a scaffold for drug discovery projects. The idea is to learn a set of generative models that are able to generate molecules fragmentby-fragment, where each fragment is a constraint on the next molecule to be generated. The scaffolds are learned in an end-to-end manner, with each generation procedure being conditioned on the previous generation history. The paper shows that MoLER outperforms existing approaches on a number of unconstrained molecular optimization tasks, and outperforms them on several scaffoldbased tasks. "
6309,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"Imitation learning algorithms USED-FOR policy. deterministic experts USED-FOR imitation learning. stationary reward FEATURE-OF reinforcement learning. reduction USED-FOR continuous control tasks. Task is recovery of expert reward. OtherScientificTerm is total variation distance. Method are imitation learner, and adversarial imitation learning. ","This paper studies the problem of imitation learning with deterministic experts in the setting where the expert policy is deterministic and the imitation learning algorithms are not able to learn a policy that maximizes the recovery of expert reward. The authors consider the setting of reinforcement learning with a stationary reward, where the total variation distance between the expert and an imitation learner is known. They show that in this setting, there is a reduction in total variation between the policy and the expert, and they show that this reduction can be applied to continuous control tasks. They also show that adversarial imitation learning can be used to improve the performance of the algorithm. ","This paper studies the problem of imitation learning with deterministic experts in the setting where the expert policy is deterministic and the imitation learning algorithms are not able to learn a policy that maximizes the recovery of expert reward. The authors consider the setting of reinforcement learning with a stationary reward, where the total variation distance between the expert and an imitation learner is known. They show that in this setting, there is a reduction in total variation between the policy and the expert, and they show that this reduction can be applied to continuous control tasks. They also show that adversarial imitation learning can be used to improve the performance of the algorithm. "
6325,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,reweighting algorithms USED-FOR machine learning models. reweighting algorithms USED-FOR fairness. machine learning models USED-FOR fairness. overparameterized setting FEATURE-OF algorithms. reweighting USED-FOR overparameterized model. worst - group test performance COMPARE ERM. ERM COMPARE worst - group test performance. overparameterized model USED-FOR ERM interpolator. reweighting algorithms USED-FOR interpolator. reweighting algorithms COMPARE ERM. ERM COMPARE reweighting algorithms. interpolator COMPARE ERM. ERM COMPARE interpolator. worst - group performance EVALUATE-FOR ERM. regularization CONJUNCTION data augmentation. data augmentation CONJUNCTION regularization. regularization USED-FOR reweighting algorithms. data augmentation USED-FOR reweighting algorithms. worst - group test performance EVALUATE-FOR reweighting algorithms. OtherScientificTerm is model parameters. Method is theoretical backing. Generic is model. ,"This paper studies the problem of reweighting algorithms for machine learning models for fairness in the overparameterized setting. The authors show that existing algorithms in this setting can be biased in the worst-group test performance when the model parameters are overparametrized. They also provide theoretical backing on why this is the case. They show that the reweighted model is biased when reweightings are applied to the model. They then show that under this setting, the reweights of an ERM interpolator trained on a re-weighted version of the original ERM are biased in favor of the worst group. Finally, they show that if the model is over-parametrizated, the interpolator is biased in favour of the best group.   The authors also show that, under a certain setting where the model parameter is overparametrised, reweighters that use regularization and/or data augmentation are able to outperform ERM in worst-groups test performance.","This paper studies the problem of reweighting algorithms for machine learning models for fairness in the overparameterized setting. The authors show that existing algorithms in this setting can be biased in the worst-group test performance when the model parameters are overparametrized. They also provide theoretical backing on why this is the case. They show that the reweighted model is biased when reweightings are applied to the model. They then show that under this setting, the reweights of an ERM interpolator trained on a re-weighted version of the original ERM are biased in favor of the worst group. Finally, they show that if the model is over-parametrizated, the interpolator is biased in favour of the best group.   The authors also show that, under a certain setting where the model parameter is overparametrised, reweighters that use regularization and/or data augmentation are able to outperform ERM in worst-groups test performance."
6341,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"Multi - agent reinforcement learning ( MARL ) USED-FOR emergent behavior. emergent behavior PART-OF complex agent - based simulations. model USED-FOR human - irrationality. Rational Inattention ( RI ) model HYPONYM-OF model. model PART-OF human - like RL agents. RI USED-FOR cost of cognitive information processing. mutual information USED-FOR RI. mutual information USED-FOR cost of cognitive information processing. multi - timestep dynamics CONJUNCTION information channels. information channels CONJUNCTION multi - timestep dynamics. heterogeneous processing costs FEATURE-OF information channels. RI USED-FOR information asymmetry. RIRL USED-FOR equilibrium behaviors. RIRL USED-FOR AI agents. AI agents USED-FOR real human behavior. Method are RL agents, and RIRL framework. OtherScientificTerm are human behavior, rational assumptions, Principal ’s inattention, inattention, and rationality assumptions. Metric is Agent welfare. ","This paper studies emergent behavior in multi-agent reinforcement learning (MARL) that can be observed in complex agent-based simulations. The authors propose a model called the Rational Inattention (RI) model, which is a model for human-irrationality in RL agents. The model is a generalization of human-like RL agents, where rational assumptions are made on the agent’s inattention.   The authors show that the cost of cognitive information processing in RI is related to the mutual information between the agents’ inattentive and rational assumptions. They also show that in the RIRL framework, the cost is proportional to the multi-timestep dynamics and the number of information channels with heterogeneous processing costs. Agent welfare is measured as the sum of mutual information across all information channels. They show that RI is able to capture information asymmetry between agents, and that inatt attention is correlated with rationality assumptions.  The paper also shows that RirL can be used to learn equilibrium behaviors for AI agents that are able to imitate real human behavior. ","This paper studies emergent behavior in multi-agent reinforcement learning (MARL) that can be observed in complex agent-based simulations. The authors propose a model called the Rational Inattention (RI) model, which is a model for human-irrationality in RL agents. The model is a generalization of human-like RL agents, where rational assumptions are made on the agent’s inattention.   The authors show that the cost of cognitive information processing in RI is related to the mutual information between the agents’ inattentive and rational assumptions. They also show that in the RIRL framework, the cost is proportional to the multi-timestep dynamics and the number of information channels with heterogeneous processing costs. Agent welfare is measured as the sum of mutual information across all information channels. They show that RI is able to capture information asymmetry between agents, and that inatt attention is correlated with rationality assumptions.  The paper also shows that RirL can be used to learn equilibrium behaviors for AI agents that are able to imitate real human behavior. "
6357,SP:100c91da177504d89f1819f4fdce72ebcf848902,perturbations USED-FOR Audio adversarial attacks. lp - norm CONJUNCTION auditory masking. auditory masking CONJUNCTION lp - norm. auditory masking USED-FOR magnitude spectrogram. lp - norm CONJUNCTION waveform. waveform CONJUNCTION lp - norm. perturbations USED-FOR audio adversarial attacks. phaseoriented algorithm USED-FOR imperceptible audio adversarial examples. energy dissipation FEATURE-OF imperceptible audio adversarial examples. PhaseFool HYPONYM-OF phaseoriented algorithm. energy patterns PART-OF spectrogram. spectrogram consistency USED-FOR short - time Fourier transform ( STFT ). weighted loss function USED-FOR PhaseFool. imperceptibility EVALUATE-FOR PhaseFool. weighted loss function USED-FOR imperceptibility. PhaseFool COMPARE imperceptible counterparts. imperceptible counterparts COMPARE PhaseFool. PhaseFool USED-FOR full - sentence imperceptible audio adversarial examples. realistic simulated environmental distortions USED-FOR adversarial examples. PhaseFool USED-FOR adversarial examples. phase - oriented energy dissipation FEATURE-OF audio adversarial example. PhaseFool COMPARE perturbations. perturbations COMPARE PhaseFool. perturbations FEATURE-OF audio waveform. phase - oriented energy dissipation COMPARE perturbations. perturbations COMPARE phase - oriented energy dissipation. phase - oriented energy dissipation USED-FOR PhaseFool. Method is automatic speech recognition ( ASR ) model. OtherScientificTerm is phase spectrogram. ,"Audio adversarial attacks are typically caused by perturbations to the magnitude spectrogram of an automatic speech recognition (ASR) model. Audio adversarial examples can be generated by perturbing the lp-norm, waveform, or auditory masking. This paper proposes a phase-oriented algorithm, called PhaseFool, which is a phaseoriented algorithm that generates imperceptible audio adversarial instances with high energy dissipation.    The authors propose a phase spectrogram, which consists of a sequence of energy patterns in the spectrogram. The authors use a short-time Fourier transform (STFT) based on spectrogram consistency to learn the phase of the waveform. They also propose a weighted loss function to improve the imperceptibility of the proposed phaseFool. Experiments show that the proposed Phasefool is able to generate full-sentence imperceptibly adversarial adversarial audio examples with realistic simulated environmental distortions, and that the adversarial example generated by the proposed StageFool is more likely to be detected than the perturbation of the audio waveform in the original audio. The paper also shows that the phase-observational energy of an audio adversary example with phase-orientated energy Dissipation is higher than that of the original perturbed audio.  Experiments also show that PhaseFools can generate adversarial samples that are more likely not to be classified as adversarial by the adversary, and can be used to train a full-spectrum ASR model.","Audio adversarial attacks are typically caused by perturbations to the magnitude spectrogram of an automatic speech recognition (ASR) model. Audio adversarial examples can be generated by perturbing the lp-norm, waveform, or auditory masking. This paper proposes a phase-oriented algorithm, called PhaseFool, which is a phaseoriented algorithm that generates imperceptible audio adversarial instances with high energy dissipation.    The authors propose a phase spectrogram, which consists of a sequence of energy patterns in the spectrogram. The authors use a short-time Fourier transform (STFT) based on spectrogram consistency to learn the phase of the waveform. They also propose a weighted loss function to improve the imperceptibility of the proposed phaseFool. Experiments show that the proposed Phasefool is able to generate full-sentence imperceptibly adversarial adversarial audio examples with realistic simulated environmental distortions, and that the adversarial example generated by the proposed StageFool is more likely to be detected than the perturbation of the audio waveform in the original audio. The paper also shows that the phase-observational energy of an audio adversary example with phase-orientated energy Dissipation is higher than that of the original perturbed audio.  Experiments also show that PhaseFools can generate adversarial samples that are more likely not to be classified as adversarial by the adversary, and can be used to train a full-spectrum ASR model."
6373,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,Non - contrastive methods USED-FOR representations. Non - contrastive methods USED-FOR self - supervised learning. BYOL HYPONYM-OF Non - contrastive methods. BYOL HYPONYM-OF self - supervised learning. augmentation process USED-FOR representation. DirectPred USED-FOR predictor. DirectSet(α ) USED-FOR projection matrix. sample complexity EVALUATE-FOR downstream tasks. DirectSet(α ) USED-FOR downstream tasks. DirectSet(α ) USED-FOR linear network. sample complexity EVALUATE-FOR DirectSet(α ). weight decay USED-FOR implicit threshold. eigen - decomposition step USED-FOR DirectPred. CIFAR-100 CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR-100. STL-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL-10. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. DirectCopy COMPARE DirectPred. DirectPred COMPARE DirectCopy. CIFAR-10 EVALUATE-FOR DirectCopy. ImageNet EVALUATE-FOR DirectCopy. CIFAR-10 EVALUATE-FOR DirectPred. CIFAR-10 CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR-10. Generic is approaches. Task is augmentation. ,"Non-contrastive methods (e.g. BYOL) have been shown to learn representations that are robust to augmentation in self-supervised learning. However, these approaches are computationally expensive. In this paper, the authors propose DirectSet(α), a new augmentation process that learns a representation that is robust to the augmentation. The authors propose to use DirectPred as a predictor, which is a linear network with a linear projection matrix. They show that the sample complexity for downstream tasks is O(1/\sqrt{T}^T) if the projection matrix of the linear network is directly learned, and O(T/T^2) otherwise. They also show that DirectPred can be trained with an eigen-decomposition step, where weight decay is used as an implicit threshold. They evaluate DirectCopy on CIFAR-10, CifAR-100, STL-10 and ImageNet, showing that DirectCopy outperforms DirectPred in terms of sample complexity. ","Non-contrastive methods (e.g. BYOL) have been shown to learn representations that are robust to augmentation in self-supervised learning. However, these approaches are computationally expensive. In this paper, the authors propose DirectSet(α), a new augmentation process that learns a representation that is robust to the augmentation. The authors propose to use DirectPred as a predictor, which is a linear network with a linear projection matrix. They show that the sample complexity for downstream tasks is O(1/\sqrt{T}^T) if the projection matrix of the linear network is directly learned, and O(T/T^2) otherwise. They also show that DirectPred can be trained with an eigen-decomposition step, where weight decay is used as an implicit threshold. They evaluate DirectCopy on CIFAR-10, CifAR-100, STL-10 and ImageNet, showing that DirectCopy outperforms DirectPred in terms of sample complexity. "
6389,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,method USED-FOR learning long - term sequential dependencies. Long Expressive Memory ( LEM ) USED-FOR learning long - term sequential dependencies. it USED-FOR input - output maps. it USED-FOR sequential tasks. long - term dependencies FEATURE-OF sequential tasks. time - discretization USED-FOR system. system of multiscale ordinary differential equations CONJUNCTION time - discretization. time - discretization CONJUNCTION system of multiscale ordinary differential equations. system of multiscale ordinary differential equations USED-FOR LEM. rigorous bounds USED-FOR exploding and vanishing gradients problem. rigorous bounds USED-FOR LEM. LEM USED-FOR dynamical systems. accuracy EVALUATE-FOR LEM. gated recurrent units CONJUNCTION long short - term memory models. long short - term memory models CONJUNCTION gated recurrent units. recurrent neural networks CONJUNCTION gated recurrent units. gated recurrent units CONJUNCTION recurrent neural networks. LEM COMPARE recurrent neural networks. recurrent neural networks COMPARE LEM. image and time - series classification CONJUNCTION dynamical systems prediction. dynamical systems prediction CONJUNCTION image and time - series classification. LEM COMPARE long short - term memory models. long short - term memory models COMPARE LEM. dynamical systems prediction CONJUNCTION keyword spotting and language modeling. keyword spotting and language modeling CONJUNCTION dynamical systems prediction. LEM COMPARE gated recurrent units. gated recurrent units COMPARE LEM. image and time - series classification CONJUNCTION keyword spotting and language modeling. keyword spotting and language modeling CONJUNCTION image and time - series classification. Method is gradient - based recurrent sequential learning methods. ,"This paper proposes Long Expressive Memory (LEM) as a method for learning long-term sequential dependencies. LEM is based on a system of multiscale ordinary differential equations and time-discretization, and it learns input-output maps for sequential tasks with long term dependencies. The authors provide rigorous bounds on the exploding and vanishing gradients problem of LEM, which is a common problem in gradient-based recurrent sequential learning methods. They show that LEM can be used to learn dynamical systems, and that it can achieve better accuracy than recurrent neural networks, gated recurrent units, and long short-term memory models. They also show that the LEM achieves better performance than the previous state-of-the-art long short term memory models, and LEM outperforms the previous work on image and time series classification, dynamical system prediction, and keyword spotting and language modeling.","This paper proposes Long Expressive Memory (LEM) as a method for learning long-term sequential dependencies. LEM is based on a system of multiscale ordinary differential equations and time-discretization, and it learns input-output maps for sequential tasks with long term dependencies. The authors provide rigorous bounds on the exploding and vanishing gradients problem of LEM, which is a common problem in gradient-based recurrent sequential learning methods. They show that LEM can be used to learn dynamical systems, and that it can achieve better accuracy than recurrent neural networks, gated recurrent units, and long short-term memory models. They also show that the LEM achieves better performance than the previous state-of-the-art long short term memory models, and LEM outperforms the previous work on image and time series classification, dynamical system prediction, and keyword spotting and language modeling."
6405,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"symmetry USED-FOR architectures. architectures USED-FOR deep learning. rotation CONJUNCTION permutation equivariance. permutation equivariance CONJUNCTION rotation. translation CONJUNCTION rotation. rotation CONJUNCTION translation. rotationand permutation - equivariant architectures USED-FOR deep learning. rotationand permutation - equivariant architectures USED-FOR small point clouds. products USED-FOR reductions. reductions PART-OF rotationand permutation - equivariant architectures. attention mechanism USED-FOR reductions. rotation invariance CONJUNCTION covariance. covariance CONJUNCTION rotation invariance. attention USED-FOR permutation equivariance. chemistry CONJUNCTION biology. biology CONJUNCTION chemistry. physics CONJUNCTION chemistry. chemistry CONJUNCTION physics. models USED-FOR architectures. Method are geometric deep learning, and geometric algebra. Task is physical sciences. OtherScientificTerm are twoor three - dimensional space, and vector. ","This paper studies the problem of geometric deep learning, which is an important problem in the physical sciences. The authors propose two new architectures that are based on symmetry and permutation equivariance for deep learning. Specifically, the authors propose rotationand permutation-equivariant architectures for small point clouds that are invariant to translation, rotation, and permuted equivariant to rotation. These reductions are composed of two products: (1) the reduction of a vector to a point in a twoor three-dimensional space, (2) the permutation of the vector, and (3) the rotation invariance and the covariance. These two reductions are combined in the proposed rotational-permutation-Equivariant architecture through an attention mechanism. Experiments on physics, chemistry, biology, and chemistry show that the proposed models are able to learn these architectures. ","This paper studies the problem of geometric deep learning, which is an important problem in the physical sciences. The authors propose two new architectures that are based on symmetry and permutation equivariance for deep learning. Specifically, the authors propose rotationand permutation-equivariant architectures for small point clouds that are invariant to translation, rotation, and permuted equivariant to rotation. These reductions are composed of two products: (1) the reduction of a vector to a point in a twoor three-dimensional space, (2) the permutation of the vector, and (3) the rotation invariance and the covariance. These two reductions are combined in the proposed rotational-permutation-Equivariant architecture through an attention mechanism. Experiments on physics, chemistry, biology, and chemistry show that the proposed models are able to learn these architectures. "
6421,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,order fulfillment problem HYPONYM-OF combinatorial optimization problems. it USED-FOR online retailing. combinatorial optimization problems PART-OF supply chain management. order fulfillment problem HYPONYM-OF supply chain management. exact mathematical programming methods USED-FOR problem. machine learning method USED-FOR it. tripartite graph USED-FOR machine learning method. edge - featureembedded graph attention mechanism USED-FOR assignment policy. edge - feature - embedded graph attention USED-FOR optimization problem. edge - feature - embedded graph attention USED-FOR heterogeneous information. edge - feature - embedded graph attention USED-FOR high - dimensional edge features. model COMPARE baseline heuristic method. baseline heuristic method COMPARE model. optimality EVALUATE-FOR baseline heuristic method. optimality EVALUATE-FOR model. online inference time COMPARE mathematical programming methods. mathematical programming methods COMPARE online inference time. ,"This paper considers the problem of order fulfillment problem, which is one of the most important combinatorial optimization problems in supply chain management, and it is particularly relevant for online retailing. The authors propose a machine learning method to solve it using a tripartite graph, and they propose a novel edge-featureembedded graph attention mechanism to learn an assignment policy. The optimization problem is formulated as a graph optimization problem, and exact mathematical programming methods are used to solve the problem. The proposed model is able to learn high-dimensional edge features by using edge-Feature-embeddings, which can capture heterogeneous information. Experiments show that the proposed model outperforms a baseline heuristic method in terms of optimality, and the online inference time is much faster than the existing state-of-the-art mathematical programming algorithms.   ","This paper considers the problem of order fulfillment problem, which is one of the most important combinatorial optimization problems in supply chain management, and it is particularly relevant for online retailing. The authors propose a machine learning method to solve it using a tripartite graph, and they propose a novel edge-featureembedded graph attention mechanism to learn an assignment policy. The optimization problem is formulated as a graph optimization problem, and exact mathematical programming methods are used to solve the problem. The proposed model is able to learn high-dimensional edge features by using edge-Feature-embeddings, which can capture heterogeneous information. Experiments show that the proposed model outperforms a baseline heuristic method in terms of optimality, and the online inference time is much faster than the existing state-of-the-art mathematical programming algorithms.   "
6437,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"natural language processing models USED-FOR inference process. inferring Concepts PART-OF dialogue summarization task. CODC inference module CONJUNCTION knowledge attention module. knowledge attention module CONJUNCTION CODC inference module. knowledge attention module PART-OF neural summarization model. CODC inference module PART-OF framework. knowledge attention module PART-OF framework. CODC USED-FOR evaluation metric. evaluation metric EVALUATE-FOR methods. automatic evaluation metrics EVALUATE-FOR out - of - context inference. automatic evaluation metrics EVALUATE-FOR natural language generation. CODC inference CONJUNCTION automatic evaluation metrics. automatic evaluation metrics CONJUNCTION CODC inference. natural language generation EVALUATE-FOR out - of - context inference. CODC inference EVALUATE-FOR summarization model. automatic evaluation metrics EVALUATE-FOR summarization model. CIDEr HYPONYM-OF automatic evaluation metrics. model EVALUATE-FOR model. Material are human dialogues, and WordNet. OtherScientificTerm are pragmatics studies, and Dialogue Context ( CODC ). ","This paper proposes a dialogue summarization model that uses natural language processing models to guide the inference process. The framework consists of a CODC inference module, a knowledge attention module, and a neural summarization module. The authors propose to use Dialogue Context (CODC) as an evaluation metric for inferring Concepts in the dialogue summary task. The evaluation metric is based on the observation that human dialogues can be interpreted as pragmatics studies, and that the evaluation of natural language generation using automatic evaluation metrics (e.g. CIDEr) can be seen as an extension of the evaluation metric used in the literature.   The authors show that the proposed methods outperform existing methods in terms of out-of-context inference. They also show that their model outperforms the previous state of the art on WordNet. Finally, the authors demonstrate that their summarization models outperform the state-of the-art on both the task of out of context inference and on the more general task of summarization, and demonstrate that the model is able to learn a good representation of the context of the dialogue.","This paper proposes a dialogue summarization model that uses natural language processing models to guide the inference process. The framework consists of a CODC inference module, a knowledge attention module, and a neural summarization module. The authors propose to use Dialogue Context (CODC) as an evaluation metric for inferring Concepts in the dialogue summary task. The evaluation metric is based on the observation that human dialogues can be interpreted as pragmatics studies, and that the evaluation of natural language generation using automatic evaluation metrics (e.g. CIDEr) can be seen as an extension of the evaluation metric used in the literature.   The authors show that the proposed methods outperform existing methods in terms of out-of-context inference. They also show that their model outperforms the previous state of the art on WordNet. Finally, the authors demonstrate that their summarization models outperform the state-of the-art on both the task of out of context inference and on the more general task of summarization, and demonstrate that the model is able to learn a good representation of the context of the dialogue."
6446,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"entity embeddings USED-FOR Artificial Intelligence. embedding models USED-FOR Horn rules. embedding strategies USED-FOR monotonic and non - monotonic attribute dependencies. OtherScientificTerm are embeddings, vectors, semantic dependencies, and attribute dependencies. Method is attribute embeddings. ","This paper studies the problem of learning entity embeddings for Artificial Intelligence. The authors propose a new way of learning embedding models based on the so-called ""Horn rules"", which is a generalization of existing embedding approaches. The key idea is to learn a set of vectors that are monotonic and non-monotonic (i.e. have no semantic dependencies) for each attribute in the dataset, and then to use these vectors to learn the embedding of a given attribute. The paper shows that the learned embedding strategies can be used to learn both monotonically and nonconstraintly attribute dependencies. Experiments are conducted on several datasets to demonstrate the effectiveness of the proposed approach.  ","This paper studies the problem of learning entity embeddings for Artificial Intelligence. The authors propose a new way of learning embedding models based on the so-called ""Horn rules"", which is a generalization of existing embedding approaches. The key idea is to learn a set of vectors that are monotonic and non-monotonic (i.e. have no semantic dependencies) for each attribute in the dataset, and then to use these vectors to learn the embedding of a given attribute. The paper shows that the learned embedding strategies can be used to learn both monotonically and nonconstraintly attribute dependencies. Experiments are conducted on several datasets to demonstrate the effectiveness of the proposed approach.  "
6455,SP:794cca5205d667900ceb9a1332b6272320752ef4,transformer - based models USED-FOR natural language processing tasks. transformers USED-FOR natural language. commonsense reasoning CONJUNCTION logical reasoning. logical reasoning CONJUNCTION commonsense reasoning. mathematical reasoning CONJUNCTION commonsense reasoning. commonsense reasoning CONJUNCTION mathematical reasoning. transformers USED-FOR reasoning tasks. mathematical reasoning HYPONYM-OF reasoning tasks. logical reasoning HYPONYM-OF reasoning tasks. commonsense reasoning HYPONYM-OF reasoning tasks. ,"This paper investigates the performance of transformer-based models for natural language processing tasks. The authors show that transformers are able to learn natural language and perform well on a variety of reasoning tasks, including mathematical reasoning, commonsense reasoning, logical reasoning, and logical reasoning. The paper also shows that the ability of transformers to generalize to reasoning tasks is not limited to natural language. ","This paper investigates the performance of transformer-based models for natural language processing tasks. The authors show that transformers are able to learn natural language and perform well on a variety of reasoning tasks, including mathematical reasoning, commonsense reasoning, logical reasoning, and logical reasoning. The paper also shows that the ability of transformers to generalize to reasoning tasks is not limited to natural language. "
6464,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"paradigms PART-OF machine learning. complexity FEATURE-OF tasks. algorithmic procedures USED-FOR representation transformations. domaingeneral framework USED-FOR algorithmic procedures. compositional recursive learner HYPONYM-OF domaingeneral framework. compositional recursive learner USED-FOR compositional generalization. compositional approach COMPARE baselines. baselines COMPARE compositional approach. compositional approach COMPARE learner. learner COMPARE compositional approach. Generic are it, and they. Task are generalization, and compositional generalization problem. OtherScientificTerm are compositional structure of the task distribution, compositional problem graph, and shared subproblems. ","This paper considers the problem of compositional generalization in machine learning, i.e. how to generalize to new tasks that share the same compositional structure of the task distribution. The authors consider the compositional problem graph, where each task is represented as a set of shared subproblems, and the complexity of the tasks is proportional to the number of tasks in the graph. They propose a compositional recursive learner, which is a variant of the domaingeneral framework (i.e., compositional recursively learning algorithmic procedures for representation transformations) that is based on the classical notion of a ""compositional recurrence"". They show that this compositional approach outperforms several baselines in terms of generalization performance, and that it is more computationally efficient than using a single learner. ","This paper considers the problem of compositional generalization in machine learning, i.e. how to generalize to new tasks that share the same compositional structure of the task distribution. The authors consider the compositional problem graph, where each task is represented as a set of shared subproblems, and the complexity of the tasks is proportional to the number of tasks in the graph. They propose a compositional recursive learner, which is a variant of the domaingeneral framework (i.e., compositional recursively learning algorithmic procedures for representation transformations) that is based on the classical notion of a ""compositional recurrence"". They show that this compositional approach outperforms several baselines in terms of generalization performance, and that it is more computationally efficient than using a single learner. "
6468,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"weight pruning HYPONYM-OF network model compression. activation pruning CONJUNCTION weight pruning. weight pruning CONJUNCTION activation pruning. weight pruning PART-OF Integral Pruning ( IP ) technique. activation pruning PART-OF Integral Pruning ( IP ) technique. IPnet HYPONYM-OF network. execution efficiency EVALUATE-FOR network. activation functions FEATURE-OF network models. datasets EVALUATE-FOR network models. network models EVALUATE-FOR IPnet. datasets EVALUATE-FOR IPnet. testing accuracy EVALUATE-FOR IPnet. IPnet COMPARE dense models. dense models COMPARE IPnet. computation cost EVALUATE-FOR IPnet. Method are deep neural networks ( DNNs ), and DNNs. Task is compression. OtherScientificTerm are weights, connections, sparsity, and activation and weight numbers. ","This paper proposes Integral Pruning (IP), a method to prune the weights of deep neural networks (DNNs) in order to reduce the computational cost of compression. The authors argue that activation pruning and weight pruning, a common technique in network model compression (weight pruning), are not well-suited for this problem. Integral pruning is an extension of Integral Pruning (IP) that combines the Integral Truning (IT) and Integral Weight Pruning, which prunes the weights and connections of a DNN to achieve sparsity.    The authors propose a network called IPnet, which is a network that prunes a network to improve execution efficiency while maintaining the performance of the network. IPnet is evaluated on three datasets for various network models with different activation functions, and is shown to outperform other network models in terms of testing accuracy. In addition, IPnet shows that IPnet can reduce the computation cost by up to 40% compared to dense models, and that the activation and weight numbers of a network can be pruned efficiently.","This paper proposes Integral Pruning (IP), a method to prune the weights of deep neural networks (DNNs) in order to reduce the computational cost of compression. The authors argue that activation pruning and weight pruning, a common technique in network model compression (weight pruning), are not well-suited for this problem. Integral pruning is an extension of Integral Pruning (IP) that combines the Integral Truning (IT) and Integral Weight Pruning, which prunes the weights and connections of a DNN to achieve sparsity.    The authors propose a network called IPnet, which is a network that prunes a network to improve execution efficiency while maintaining the performance of the network. IPnet is evaluated on three datasets for various network models with different activation functions, and is shown to outperform other network models in terms of testing accuracy. In addition, IPnet shows that IPnet can reduce the computation cost by up to 40% compared to dense models, and that the activation and weight numbers of a network can be pruned efficiently."
6472,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"discovery of relevant features USED-FOR task. Machine learning driven feature selection USED-FOR discovery. methodology USED-FOR False Discovery Rate. Generative Adversarial Networks framework USED-FOR knockoffs. stability network CONJUNCTION power network. power network CONJUNCTION stability network. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. discriminator CONJUNCTION stability network. stability network CONJUNCTION discriminator. networks CONJUNCTION generator. generator CONJUNCTION networks. generator CONJUNCTION stability network. stability network CONJUNCTION generator. discriminator CONJUNCTION power network. power network CONJUNCTION discriminator. networks PART-OF model. discriminator PART-OF model. stability network PART-OF model. generator PART-OF model. power network PART-OF model. model USED-FOR feature selection. it COMPARE model. model COMPARE it. it COMPARE knockoff generation model. knockoff generation model COMPARE it. model USED-FOR non - Gaussian settings. knockoff generation model USED-FOR Gaussian setting. non - Gaussian settings EVALUATE-FOR it. real - world dataset EVALUATE-FOR it. real - world dataset EVALUATE-FOR model. Task are Feature selection, and overfitting in prediction. OtherScientificTerm are relevant genetic factors, features, and feature distribution. Metric is expert time. Method is Knockoff framework. ","This paper studies the problem of feature selection. Feature selection is an important problem in machine learning driven feature selection, where the goal is to ensure the discovery of relevant features for a given task. The authors propose a methodology to reduce the False Discovery Rate (FDR) which is a measure of how well a feature is selected for a particular task.  Feature selection can be seen as a way to identify relevant genetic factors that are not present in the feature distribution.    This paper proposes a Generative Adversarial Networks framework to generate knockoffs. The proposed model consists of three networks: a generator, a discriminator, and a power network. The discriminator is used to discriminate whether a feature belongs to a particular group, and the stability network and power network are used to determine whether the features belong to the same group.  The authors show that the proposed model can be used to improve feature selection in the presence of overfitting in prediction. They also show that it outperforms the state-of-the-art knockoff generation model in the Gaussian setting, and outperforms it in the non-Gaussian settings. The paper also shows that the model is able to improve the performance of a real-world dataset, and that it can also be used for feature selection without the need for expert time.  In addition, the authors provide a theoretical analysis of the Knockoff framework. ","This paper studies the problem of feature selection. Feature selection is an important problem in machine learning driven feature selection, where the goal is to ensure the discovery of relevant features for a given task. The authors propose a methodology to reduce the False Discovery Rate (FDR) which is a measure of how well a feature is selected for a particular task.  Feature selection can be seen as a way to identify relevant genetic factors that are not present in the feature distribution.    This paper proposes a Generative Adversarial Networks framework to generate knockoffs. The proposed model consists of three networks: a generator, a discriminator, and a power network. The discriminator is used to discriminate whether a feature belongs to a particular group, and the stability network and power network are used to determine whether the features belong to the same group.  The authors show that the proposed model can be used to improve feature selection in the presence of overfitting in prediction. They also show that it outperforms the state-of-the-art knockoff generation model in the Gaussian setting, and outperforms it in the non-Gaussian settings. The paper also shows that the model is able to improve the performance of a real-world dataset, and that it can also be used for feature selection without the need for expert time.  In addition, the authors provide a theoretical analysis of the Knockoff framework. "
6476,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"Pruning techniques CONJUNCTION Winograd convolution. Winograd convolution CONJUNCTION Pruning techniques. Winograd convolution USED-FOR CNN computation. Pruning techniques USED-FOR CNN computation. Winograd transformation USED-FOR sparsity. learning rates USED-FOR Winograd - domain retraining. ReLU function PART-OF Winograd domain. pruning method USED-FOR Winograd - domain weight sparsity. spatial - Winograd pruning HYPONYM-OF pruning method. importance factor matrix USED-FOR weight importance. importance factor matrix USED-FOR weight gradients. weight importance CONJUNCTION weight gradients. weight gradients CONJUNCTION weight importance. pruning PART-OF Winograd domain. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR10. models EVALUATE-FOR method. datasets EVALUATE-FOR method. datasets EVALUATE-FOR models. CIFAR10 HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. Winograddomain sparsities EVALUATE-FOR method. Method are Deep convolutional neural networks ( CNNs ), sparse Winograd convolution, and Winograd - domain network. Generic are they, and technique. OtherScientificTerm are weight sparsity, network structure, network structures, spatial - domain weights, and spatial - domain sparsity. ","Deep convolutional neural networks (CNNs) have been shown to suffer from weight sparsity. Pruning techniques and Winograd convolution have been widely used to reduce the cost of CNN computation. However, they are computationally expensive because of the network structure. This paper proposes a new pruning method, called spatial-Winograd pruning, to achieve the goal of achieving a more efficient and sparsity-friendly solution.    The idea is that the sparsity is achieved by a sparse version of the original sparse convolution. The authors propose a novel Winovrad transformation to achieve this sparsity, and show that the resulting sparsity can be achieved through a simple but effective pruning of the ReLU function in the original and sparse version.  The authors also show that this technique can be applied to any network structure, and that it can be used to prune the network structures to achieve a better performance.  To achieve this, the authors propose to use the spatial-domain weights as the basis for the pruning. They also propose a new learning rates for the Winovadrad-domain retraining.  Experiments on three datasets (CIFAR10, CIFAR-100, and ImageNet) demonstrate that the proposed method outperforms existing models on all three datasets in terms of Winograddomain sparsities. The paper also shows that this pruning can also be applied in the case where the network is trained on the original dataset.  Finally, the paper shows that the authors have also proposed a new method, which is called ""spatial-winograd"", which is a pruning algorithm that prunes the weights of the sparse Winogrand convolution in a different part of the dataset. This pruning is based on the observation that the importance factor matrix of the weight importance and the weight gradients are highly correlated with the weight of the corresponding layer in the initial layer. ","Deep convolutional neural networks (CNNs) have been shown to suffer from weight sparsity. Pruning techniques and Winograd convolution have been widely used to reduce the cost of CNN computation. However, they are computationally expensive because of the network structure. This paper proposes a new pruning method, called spatial-Winograd pruning, to achieve the goal of achieving a more efficient and sparsity-friendly solution.    The idea is that the sparsity is achieved by a sparse version of the original sparse convolution. The authors propose a novel Winovrad transformation to achieve this sparsity, and show that the resulting sparsity can be achieved through a simple but effective pruning of the ReLU function in the original and sparse version.  The authors also show that this technique can be applied to any network structure, and that it can be used to prune the network structures to achieve a better performance.  To achieve this, the authors propose to use the spatial-domain weights as the basis for the pruning. They also propose a new learning rates for the Winovadrad-domain retraining.  Experiments on three datasets (CIFAR10, CIFAR-100, and ImageNet) demonstrate that the proposed method outperforms existing models on all three datasets in terms of Winograddomain sparsities. The paper also shows that this pruning can also be applied in the case where the network is trained on the original dataset.  Finally, the paper shows that the authors have also proposed a new method, which is called ""spatial-winograd"", which is a pruning algorithm that prunes the weights of the sparse Winogrand convolution in a different part of the dataset. This pruning is based on the observation that the importance factor matrix of the weight importance and the weight gradients are highly correlated with the weight of the corresponding layer in the initial layer. "
6480,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,Adversarially Learned Mixture Model ( AMM ) HYPONYM-OF generative model. generative model USED-FOR unsupervised or semi - supervised data clustering. adversarially optimized method USED-FOR conditional dependence. AMM USED-FOR conditional dependence. AMM HYPONYM-OF adversarially optimized method. AMM USED-FOR semantic separation of complex data. MNIST and SVHN datasets EVALUATE-FOR AMM. unsupervised clustering error rates EVALUATE-FOR AMM. MNIST and SVHN datasets EVALUATE-FOR AMM. AMM USED-FOR semi - supervised extension. SVHN dataset EVALUATE-FOR semi - supervised extension. classification error rate EVALUATE-FOR semi - supervised extension. Material is labeled data. ,"This paper proposes Adversarially Learned Mixture Model (AMM), a generative model for unsupervised or semi-supervised data clustering. AMM is an adversarially optimized method to learn the conditional dependence between the labeled data and unlabeled data. The authors show that AMM achieves competitive unsupervisory clustering error rates on the MNIST and SVHN datasets. They also show that the semi -supervised extension of AMM can achieve a better classification error rate on the SVhN dataset. Finally, AMM shows the ability to achieve semantic separation of complex data.","This paper proposes Adversarially Learned Mixture Model (AMM), a generative model for unsupervised or semi-supervised data clustering. AMM is an adversarially optimized method to learn the conditional dependence between the labeled data and unlabeled data. The authors show that AMM achieves competitive unsupervisory clustering error rates on the MNIST and SVHN datasets. They also show that the semi -supervised extension of AMM can achieve a better classification error rate on the SVhN dataset. Finally, AMM shows the ability to achieve semantic separation of complex data."
6484,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"stochastic binary layers USED-FOR gradients. reparameterization USED-FOR ARM estimator. REINFORCE CONJUNCTION reparameterization. reparameterization CONJUNCTION REINFORCE. variable augmentation CONJUNCTION REINFORCE. REINFORCE CONJUNCTION variable augmentation. variable augmentation USED-FOR ARM estimator. REINFORCE USED-FOR ARM estimator. adaptive variance reduction USED-FOR Monte Carlo integration. ARM estimator USED-FOR adaptive variance reduction. REINFORCE estimator USED-FOR augmented space. antithetic sampling USED-FOR augmented space. variance - reduction mechanism USED-FOR ARM estimator. antithetic sampling USED-FOR variance - reduction mechanism. auto - encoding variational inference CONJUNCTION maximum likelihood estimation. maximum likelihood estimation CONJUNCTION auto - encoding variational inference. ARM estimator USED-FOR discrete latent variable models. ARM estimator USED-FOR maximum likelihood estimation. ARM estimator USED-FOR auto - encoding variational inference. stochastic binary layers FEATURE-OF discrete latent variable models. Metric is computational complexity. OtherScientificTerm are common random numbers, and Python code. ","This paper proposes an adaptive variance reduction (ARM) estimator for discrete latent variable models with stochastic binary layers. The ARM estimator is based on variable augmentation, REINFORCE, reparameterization, and a variance-reduction mechanism. The authors show that the adaptive variance reduces the computational complexity of Monte Carlo integration, and that the variance of the augmented space can be reduced by using the variance reduction mechanism based on antithetic sampling. The paper also shows that the proposed ARM estimer can be used for both auto-encoding variational inference and maximum likelihood estimation.   ","This paper proposes an adaptive variance reduction (ARM) estimator for discrete latent variable models with stochastic binary layers. The ARM estimator is based on variable augmentation, REINFORCE, reparameterization, and a variance-reduction mechanism. The authors show that the adaptive variance reduces the computational complexity of Monte Carlo integration, and that the variance of the augmented space can be reduced by using the variance reduction mechanism based on antithetic sampling. The paper also shows that the proposed ARM estimer can be used for both auto-encoding variational inference and maximum likelihood estimation.   "
6488,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,Modularity PART-OF deep learning libraries. probabilistic distributions CONJUNCTION probabilistic model. probabilistic model CONJUNCTION probabilistic distributions. modularity FEATURE-OF probabilistic programming language. Bayesian nonparametric models HYPONYM-OF probabilistic model. probabilistic modules HYPONYM-OF re - usable building blocks. re - usable building blocks PART-OF modular probabilistic programming language. probabilistic distributions FEATURE-OF random variables. random variables PART-OF probabilistic module. probabilistic distributions PART-OF probabilistic module. inference methods PART-OF probabilistic module. pre - specified inference methods USED-FOR probabilistic modules. pre - specified inference methods USED-FOR variational inference. probabilistic modules PART-OF MXFusion. Gaussian process models EVALUATE-FOR probabilistic modules. real data EVALUATE-FOR probabilistic modules. Method is probabilistic programming. ,"This paper proposes a modular probabilistic programming language, called MXFusion, which extends the modularity of deep learning libraries. The core idea of the proposed modular approach is to introduce re-usable building blocks, called probabilism modules, which are re-useful building blocks that can be used to build a modular program. The probabilistics module consists of a set of random variables, each of which represents a different class of probabilistically-modelled random variables. These random variables are modelled as a combination of a probabilist distributions of the random variables and of the probabilists of the underlying probabilities. The authors show that this modularity is useful for a variety of applications, including Bayesian nonparametric models, which is an important class of model-free models.  The authors also show that the proposed approach can be applied to a number of different inference methods for a given model, and can be combined with other pre-specified inference methods in order to produce a more efficient probabilistical module.  Experiments are conducted on Gaussian process models, and the authors evaluate the performance of their probabilisms on real data.    The main contribution of the paper is the introduction of a modular approach to the problem of variational inference, which can be implemented in a way that allows for the use of pre- specified inference methods. ","This paper proposes a modular probabilistic programming language, called MXFusion, which extends the modularity of deep learning libraries. The core idea of the proposed modular approach is to introduce re-usable building blocks, called probabilism modules, which are re-useful building blocks that can be used to build a modular program. The probabilistics module consists of a set of random variables, each of which represents a different class of probabilistically-modelled random variables. These random variables are modelled as a combination of a probabilist distributions of the random variables and of the probabilists of the underlying probabilities. The authors show that this modularity is useful for a variety of applications, including Bayesian nonparametric models, which is an important class of model-free models.  The authors also show that the proposed approach can be applied to a number of different inference methods for a given model, and can be combined with other pre-specified inference methods in order to produce a more efficient probabilistical module.  Experiments are conducted on Gaussian process models, and the authors evaluate the performance of their probabilisms on real data.    The main contribution of the paper is the introduction of a modular approach to the problem of variational inference, which can be implemented in a way that allows for the use of pre- specified inference methods. "
6492,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"heuristically designed pruning schedules CONJUNCTION hyperparameters. hyperparameters CONJUNCTION heuristically designed pruning schedules. heuristically designed pruning schedules USED-FOR iterative optimization procedure. pruning PART-OF methods. iterative optimization procedure USED-FOR pruning. approach USED-FOR network. structurally important connections PART-OF network. connection sensitivity USED-FOR structurally important connections. network USED-FOR task. structurally important connections USED-FOR task. connection sensitivity USED-FOR saliency criterion. pretraining CONJUNCTION complex pruning schedule. complex pruning schedule CONJUNCTION pretraining. it USED-FOR architecture variations. pruning USED-FOR sparse network. method COMPARE reference network. reference network COMPARE method. accuracy EVALUATE-FOR reference network. method USED-FOR architectures. method USED-FOR sparse networks. sparse networks COMPARE reference network. reference network COMPARE sparse networks. Tiny - ImageNet classification tasks EVALUATE-FOR method. convolutional, residual and recurrent networks HYPONYM-OF architectures. MNIST EVALUATE-FOR method. Tiny - ImageNet classification tasks EVALUATE-FOR reference network. MNIST EVALUATE-FOR reference network. accuracy EVALUATE-FOR method. methods COMPARE approach. approach COMPARE methods. Task are Pruning large neural networks, and training. Metric is reduced space and time complexity. ","Pruning large neural networks is one of the most commonly used techniques to reduce the size of the network. Pruning large networks can be expensive and time-consuming. This paper proposes an iterative optimization procedure for pruning based on heuristically designed pruning schedules and hyperparameters. The proposed approach prunes a network to remove structurally important connections in the network for a given task. The saliency criterion is based on the connection sensitivity, which is a measure of how well a network can be pruned. The paper shows that the proposed method can be applied to different architectures, including convolutional, residual and recurrent networks, and that it can be used to prune different architecture variations. The method is shown to be able to achieve better performance on sparse networks compared to a reference network trained on MNIST and Tiny-ImageNet classification tasks. The authors also show that pruning with pruning is more efficient than pruning without pretraining or a complex pruning schedule.  The paper also shows that compared to existing methods, the proposed approach achieves a reduced space and time complexity. ","Pruning large neural networks is one of the most commonly used techniques to reduce the size of the network. Pruning large networks can be expensive and time-consuming. This paper proposes an iterative optimization procedure for pruning based on heuristically designed pruning schedules and hyperparameters. The proposed approach prunes a network to remove structurally important connections in the network for a given task. The saliency criterion is based on the connection sensitivity, which is a measure of how well a network can be pruned. The paper shows that the proposed method can be applied to different architectures, including convolutional, residual and recurrent networks, and that it can be used to prune different architecture variations. The method is shown to be able to achieve better performance on sparse networks compared to a reference network trained on MNIST and Tiny-ImageNet classification tasks. The authors also show that pruning with pruning is more efficient than pruning without pretraining or a complex pruning schedule.  The paper also shows that compared to existing methods, the proposed approach achieves a reduced space and time complexity. "
6496,SP:986b9781534ffec84619872cd269ad48d235f869,"inference algorithm USED-FOR decoding neural sequence models. Beam search HYPONYM-OF inference algorithm. Beam search USED-FOR decoding neural sequence models. greedy search COMPARE beam search. beam search COMPARE greedy search. beam search USED-FOR non - greedy local decisions. beam widths USED-FOR beam search. beam search algorithm USED-FOR sequence synthesis tasks. evaluation score FEATURE-OF sequences. highly non - greedy decisions USED-FOR beam search. constrained beam search USED-FOR beam search degradation. methods USED-FOR search. OtherScientificTerm are beam width, early and highly non - greedy decisions, and ( conditional ) probability. Metric is evaluation scores. Material is copies. ","This paper proposes a new inference algorithm, called Beam search, for decoding neural sequence models. Unlike greedy search, beam search is able to make non-greedy local decisions about beam widths. The authors show that beam search can be used to search for a sequence of sequence synthesis tasks, and that the beam search algorithm can be applied to a wide range of sequences with different evaluation scores. They also show that the evaluation score of such sequences is highly non-negligible when the beam width is large enough.    The authors also propose a constrained beam search to avoid beam search degradation due to early and highly non -greedy decisions. They show that their methods are able to speed up the search when the (conditional) probability of finding the optimal solution is small. They demonstrate that their method can also be used for finding sequences that are more likely to have a high evaluation score, and to find sequences that have a low evaluation score. ","This paper proposes a new inference algorithm, called Beam search, for decoding neural sequence models. Unlike greedy search, beam search is able to make non-greedy local decisions about beam widths. The authors show that beam search can be used to search for a sequence of sequence synthesis tasks, and that the beam search algorithm can be applied to a wide range of sequences with different evaluation scores. They also show that the evaluation score of such sequences is highly non-negligible when the beam width is large enough.    The authors also propose a constrained beam search to avoid beam search degradation due to early and highly non -greedy decisions. They show that their methods are able to speed up the search when the (conditional) probability of finding the optimal solution is small. They demonstrate that their method can also be used for finding sequences that are more likely to have a high evaluation score, and to find sequences that have a low evaluation score. "
6500,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"method USED-FOR sample efficiency. demonstrations USED-FOR method. curriculum USED-FOR task. Backplay COMPARE competitive methods. competitive methods COMPARE Backplay. Backplay USED-FOR large grid worlds. sample efficiency EVALUATE-FOR competitive methods. training speed EVALUATE-FOR Backplay. sample efficiency EVALUATE-FOR Backplay. reward shaping CONJUNCTION behavioral cloning. behavioral cloning CONJUNCTION reward shaping. behavioral cloning CONJUNCTION reverse curriculum generation. reverse curriculum generation CONJUNCTION behavioral cloning. Task is Model - free reinforcement learning ( RL ). OtherScientificTerm are policy, and sparse rewards. Generic is approach. ","Model-free reinforcement learning (RL) is an important problem in which the goal is to learn a policy that learns to solve a new task given sparse rewards. This paper proposes a method to improve sample efficiency by learning from demonstrations. The proposed method, called Backplay, uses a curriculum to guide the learning of the new task. Backplay is shown to outperform competitive methods in terms of sample efficiency in large grid worlds and training speed. The approach is based on a combination of reward shaping, behavioral cloning, and reverse curriculum generation. ","Model-free reinforcement learning (RL) is an important problem in which the goal is to learn a policy that learns to solve a new task given sparse rewards. This paper proposes a method to improve sample efficiency by learning from demonstrations. The proposed method, called Backplay, uses a curriculum to guide the learning of the new task. Backplay is shown to outperform competitive methods in terms of sample efficiency in large grid worlds and training speed. The approach is based on a combination of reward shaping, behavioral cloning, and reverse curriculum generation. "
6504,SP:426c98718b2dbad640380ec4ccb2b656958389bc,"Model compression USED-FOR large neural networks. model size CONJUNCTION accuracy. accuracy CONJUNCTION model size. hand - crafted heuristics USED-FOR compression techniques. AlexNet CONJUNCTION VGG16. VGG16 CONJUNCTION AlexNet. hyper - parameters USED-FOR method. expert knowledge USED-FOR method. OtherScientificTerm are compression ratio, compression ratios, Hessian, and pruning criterion. Method are Multi - Layer Pruning method ( MLPrune ), and Kroneckerfactored Approximate Curvature method. Generic is state - of - theart. Material is ImageNet. ","Model compression is an important problem for large neural networks, but there is a trade-off between model size and accuracy. This paper proposes a Multi-Layer Pruning method (MLPrune) that prunes layers based on the compression ratio. Previous compression techniques have been based on hand-crafted heuristics, but this paper proposes to use the Kroneckerfactored Approximate Curvature (KAC) method, where the compression ratios are approximated by the Hessian of the hyper-parameters of each layer. The proposed method is based on expert knowledge, and the authors show that the proposed method outperforms the state-of-theart on ImageNet, AlexNet, and VGG16. The authors also propose a new pruning criterion, and show that their method can be combined with existing pruning methods.","Model compression is an important problem for large neural networks, but there is a trade-off between model size and accuracy. This paper proposes a Multi-Layer Pruning method (MLPrune) that prunes layers based on the compression ratio. Previous compression techniques have been based on hand-crafted heuristics, but this paper proposes to use the Kroneckerfactored Approximate Curvature (KAC) method, where the compression ratios are approximated by the Hessian of the hyper-parameters of each layer. The proposed method is based on expert knowledge, and the authors show that the proposed method outperforms the state-of-theart on ImageNet, AlexNet, and VGG16. The authors also propose a new pruning criterion, and show that their method can be combined with existing pruning methods."
6508,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"sample quality CONJUNCTION training stability. training stability CONJUNCTION sample quality. Generative Adversarial Networks ( GANs ) USED-FOR real - world applications. training stability EVALUATE-FOR GAN variants. sample quality EVALUATE-FOR GAN variants. architectural choices USED-FOR GAN learning. analytic framework USED-FOR GANs. unit-, object-, and scene - level FEATURE-OF GANs. object concepts FEATURE-OF interpretable units. segmentation - based network dissection method USED-FOR interpretable units. object concepts PART-OF images. models CONJUNCTION datasets. datasets CONJUNCTION models. internal representations CONJUNCTION models. models CONJUNCTION internal representations. artifact - causing units USED-FOR GANs. framework USED-FOR applications. open source interpretation tools USED-FOR GAN models. Generic are they, and units. Method is GAN. OtherScientificTerm is interventions. ","This paper studies the problem of interpretability of Generative Adversarial Networks (GANs) in real-world applications. The authors propose a new analytic framework for analyzing GANs at the unit-, object-, and scene-level, and show that the sample quality and training stability of different GAN variants are related to architectural choices for GAN learning. They also propose a segmentation-based network dissection method to identify interpretable units with object concepts in the images, which they call ""artifact-causing units"". They show that a GAN can be interpretable by disentangling its internal representations, models, datasets, and datasets. Finally, the authors show that existing open source interpretation tools can be used to train GAN models with their proposed framework.    The paper is well-written, well-motivated, and well-structured. It is clear that the authors are interested in the application of their framework to a wide range of applications, and that they have done a good job of providing a theoretical analysis of their findings. The paper also shows that the interpretability is not necessarily due to the internal representations of the GAN, but rather to the interventions that are applied to the units.","This paper studies the problem of interpretability of Generative Adversarial Networks (GANs) in real-world applications. The authors propose a new analytic framework for analyzing GANs at the unit-, object-, and scene-level, and show that the sample quality and training stability of different GAN variants are related to architectural choices for GAN learning. They also propose a segmentation-based network dissection method to identify interpretable units with object concepts in the images, which they call ""artifact-causing units"". They show that a GAN can be interpretable by disentangling its internal representations, models, datasets, and datasets. Finally, the authors show that existing open source interpretation tools can be used to train GAN models with their proposed framework.    The paper is well-written, well-motivated, and well-structured. It is clear that the authors are interested in the application of their framework to a wide range of applications, and that they have done a good job of providing a theoretical analysis of their findings. The paper also shows that the interpretability is not necessarily due to the internal representations of the GAN, but rather to the interventions that are applied to the units."
6512,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"parameter ` distances COMPARE function L distances. function L distances COMPARE parameter ` distances. space FEATURE-OF networks. L distance USED-FOR optimization. L - space FEATURE-OF network. loss curvature HYPONYM-OF local approximations. Method is neural network. OtherScientificTerm are L Hilbert space, optimization trajectory, catastrophic forgetting, learning rule, and function distances. Generic are distances, and applications. Metric is L/ ` ratio. Task is multitask learning. ","This paper considers the problem of learning a neural network in an L Hilbert space, where the optimization trajectory of the network is in the L-space. In this space, there are two types of networks: parameter `divergences' in the space and function L distances. The paper shows that the `parameter` distances can be much smaller than the `function L distances, and that the L distance can be used to guide the optimization of a network in the `L-space'. The paper also shows that there exists local approximations (e.g., loss curvature) that can be applied to these distances.   The paper is well motivated and well motivated, and the results are interesting. However, the paper suffers from a lack of clarity, and there is no discussion of the applications of the distances. For example, it is not clear how to define `L/` ratio, which is a measure of the distance between a network and a function, and it is unclear how to quantify the distance in the case of multitask learning, and what is the case for catastrophic forgetting. It is also not clear whether the learning rule can be interpreted as a learning rule, or if the function distances are a function.","This paper considers the problem of learning a neural network in an L Hilbert space, where the optimization trajectory of the network is in the L-space. In this space, there are two types of networks: parameter `divergences' in the space and function L distances. The paper shows that the `parameter` distances can be much smaller than the `function L distances, and that the L distance can be used to guide the optimization of a network in the `L-space'. The paper also shows that there exists local approximations (e.g., loss curvature) that can be applied to these distances.   The paper is well motivated and well motivated, and the results are interesting. However, the paper suffers from a lack of clarity, and there is no discussion of the applications of the distances. For example, it is not clear how to define `L/` ratio, which is a measure of the distance between a network and a function, and it is unclear how to quantify the distance in the case of multitask learning, and what is the case for catastrophic forgetting. It is also not clear whether the learning rule can be interpreted as a learning rule, or if the function distances are a function."
6516,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,measurements of dynamic entities PART-OF Biological data. DyMoN HYPONYM-OF neural network framework. probability distribution FEATURE-OF deep generative Markov model. deep generative Markov model USED-FOR neural network framework. deep generative Markov model USED-FOR DyMoN. Dynamics Modeling Network HYPONYM-OF deep neural network framework. DyMoN USED-FOR idiosyncrasies of biological data. noise CONJUNCTION sparsity. sparsity CONJUNCTION noise. data USED-FOR probability distributions. trajectories HYPONYM-OF probability distributions. dimensionality reduction methods USED-FOR trajectories. probability distributions USED-FOR DyMoN. training efficiency CONJUNCTION accuracy. accuracy CONJUNCTION training efficiency. Kalman filters CONJUNCTION hidden Markov models. hidden Markov models CONJUNCTION Kalman filters. deep models COMPARE shallow models. shallow models COMPARE deep models. hidden Markov models HYPONYM-OF shallow models. Kalman filters HYPONYM-OF shallow models. DyMoN USED-FOR biological systems. DyMoN USED-FOR features of the dynamics. OtherScientificTerm is longitudinal measurements. Material is biological data. Generic is model. ,"This paper proposes DyMoN, a neural network framework based on a deep generative Markov model with a probability distribution.  Biological data contains measurements of dynamic entities in the form of longitudinal measurements.  The authors propose a deep neural network model called Dynamics Modeling Network (DyMoN), which is a variant of a well-studied neural network called Dynamics Network (DMN).  DyMoNs are trained to capture the idiosyncrasies of biological data, i.e., noise, sparsity, etc.   In biological data there is a large amount of noise and sparsity.  In this paper, the authors propose to use this data to learn probability distributions (e.g., trajectories) based on dimensionality reduction methods.  They show that the proposed model is able to capture features of the dynamics of biological systems, and that the training efficiency and accuracy are improved.  Experiments show that deep models outperform shallow models such as Kalman filters and hidden Markov models. ","This paper proposes DyMoN, a neural network framework based on a deep generative Markov model with a probability distribution.  Biological data contains measurements of dynamic entities in the form of longitudinal measurements.  The authors propose a deep neural network model called Dynamics Modeling Network (DyMoN), which is a variant of a well-studied neural network called Dynamics Network (DMN).  DyMoNs are trained to capture the idiosyncrasies of biological data, i.e., noise, sparsity, etc.   In biological data there is a large amount of noise and sparsity.  In this paper, the authors propose to use this data to learn probability distributions (e.g., trajectories) based on dimensionality reduction methods.  They show that the proposed model is able to capture features of the dynamics of biological systems, and that the training efficiency and accuracy are improved.  Experiments show that deep models outperform shallow models such as Kalman filters and hidden Markov models. "
6520,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"graph Laplacian USED-FOR unsupervised classification method. classification method COMPARE architecture. architecture COMPARE classification method. approximate linear map CONJUNCTION spectral clustering theory. spectral clustering theory CONJUNCTION approximate linear map. spectral clustering theory USED-FOR generative adversarial networks. approximate linear map USED-FOR generative adversarial networks. spectral clustering theory USED-FOR dimension reduced spaces. framework USED-FOR images. approximate linear connector network C USED-FOR cerebral cortex. spectral clustering HYPONYM-OF unsupervised learning. unsupervised classification method USED-FOR method. Method are human visual recognition system, and connector network. OtherScientificTerm is human brains. ","This paper proposes an unsupervised classification method based on graph Laplacian. The classification method is similar to the architecture of the human visual recognition system, but the proposed method is based on an existing method of spectral clustering, i.e. the graph-augmented version of the connector network. The authors combine the idea of the approximate linear map and the recent advances in the spectrum clustering theory for dimension reduced spaces with generative adversarial networks based on the recent work of [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21]. The framework is applied to images from the cerebral cortex of an approximate linear connector network C.    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11]  [12] ","This paper proposes an unsupervised classification method based on graph Laplacian. The classification method is similar to the architecture of the human visual recognition system, but the proposed method is based on an existing method of spectral clustering, i.e. the graph-augmented version of the connector network. The authors combine the idea of the approximate linear map and the recent advances in the spectrum clustering theory for dimension reduced spaces with generative adversarial networks based on the recent work of [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21]. The framework is applied to images from the cerebral cortex of an approximate linear connector network C.    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11]  [12] "
6524,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,permuted set USED-FOR permutation - invariant representation. bottleneck PART-OF set models. model USED-FOR permutations and set representations. number sorting CONJUNCTION image mosaics. image mosaics CONJUNCTION number sorting. classification CONJUNCTION visual question answering. visual question answering CONJUNCTION classification. image mosaics CONJUNCTION classification. classification CONJUNCTION image mosaics. image mosaics CONJUNCTION visual question answering. visual question answering CONJUNCTION image mosaics. image mosaics USED-FOR classification. explicit or implicit supervision USED-FOR model. explicit or implicit supervision USED-FOR permutations and set representations. OtherScientificTerm is Representations of sets. Method is Permutation - Optimisation module. ,"This paper proposes a new model for learning permutation-invariant representations of sets. Representations of sets can be permutation invariant if the permuted set is permuted. The authors propose a Permutation-Optimisation module that optimizes the permutation of the set. The bottleneck in existing set models is the number of permutations in the set, so the authors propose to use the permutations of the permuting set as a bottleneck to learn a permutation - invariant representation. The proposed model is able to learn permutations and set representations with explicit or implicit supervision. Experiments are conducted on number sorting, image mosaics, classification, visual question answering, and classification with permutations. ","This paper proposes a new model for learning permutation-invariant representations of sets. Representations of sets can be permutation invariant if the permuted set is permuted. The authors propose a Permutation-Optimisation module that optimizes the permutation of the set. The bottleneck in existing set models is the number of permutations in the set, so the authors propose to use the permutations of the permuting set as a bottleneck to learn a permutation - invariant representation. The proposed model is able to learn permutations and set representations with explicit or implicit supervision. Experiments are conducted on number sorting, image mosaics, classification, visual question answering, and classification with permutations. "
6528,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"Projective Subspace Networks ( PSN ) HYPONYM-OF deep learning paradigm. deep learning paradigm USED-FOR non - linear embeddings. limited supervision USED-FOR non - linear embeddings. PSN approach USED-FOR end - to - end learning. Method are learning techniques, and PSN. OtherScientificTerm are dynamical environments, affine subspace, projective subspace, and higher - order information datapoints. Task is lifelong learning. Generic is modeling. ","This paper proposes a new deep learning paradigm called Projective Subspace Networks (PSN) that learns non-linear embeddings with limited supervision. The authors argue that existing learning techniques do not generalize well to dynamical environments where the affine subspace is not available. To address this problem, the authors propose to learn a projective subspace, which is a subspace of the original subspace. The PSN approach can be used for end-to-end learning and lifelong learning. In particular, the proposed PSN is able to generalize to higher-order information datapoints, which are more suitable for modeling dynamical systems. ","This paper proposes a new deep learning paradigm called Projective Subspace Networks (PSN) that learns non-linear embeddings with limited supervision. The authors argue that existing learning techniques do not generalize well to dynamical environments where the affine subspace is not available. To address this problem, the authors propose to learn a projective subspace, which is a subspace of the original subspace. The PSN approach can be used for end-to-end learning and lifelong learning. In particular, the proposed PSN is able to generalize to higher-order information datapoints, which are more suitable for modeling dynamical systems. "
6532,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"meta - learning method USED-FOR fast adaptation. CAML HYPONYM-OF meta - learning method. context parameters CONJUNCTION shared parameters. shared parameters CONJUNCTION context parameters. gradient steps USED-FOR task - specific loss. gradient steps USED-FOR context parameters. approaches COMPARE method. method COMPARE approaches. method USED-FOR networks. memory writes CONJUNCTION network communication. network communication CONJUNCTION memory writes. memory writes USED-FOR training. approach COMPARE MAML. MAML COMPARE approach. approach USED-FOR task embeddings. task - specific learning rate EVALUATE-FOR approach. context parameters USED-FOR task embeddings. OtherScientificTerm are model parameters, overfitting, and partitionings of the parameter vectors. Generic are model, network, and task. Method is distributed machine learning systems. ","This paper proposes CAML, a meta-learning method for fast adaptation of model parameters to new tasks in distributed machine learning systems. Unlike previous approaches, the proposed method does not require memory writes or network communication during training. Instead, the authors propose to use gradient steps to update the context parameters and shared parameters for each task-specific loss. The proposed method is able to adapt the networks to a new task without memory writes, without overfitting, and without partitionings of the parameter vectors. Experiments show that the proposed approach outperforms MAML in terms of task embeddings with different context parameters, and achieves a higher task specific learning rate.   ","This paper proposes CAML, a meta-learning method for fast adaptation of model parameters to new tasks in distributed machine learning systems. Unlike previous approaches, the proposed method does not require memory writes or network communication during training. Instead, the authors propose to use gradient steps to update the context parameters and shared parameters for each task-specific loss. The proposed method is able to adapt the networks to a new task without memory writes, without overfitting, and without partitionings of the parameter vectors. Experiments show that the proposed approach outperforms MAML in terms of task embeddings with different context parameters, and achieves a higher task specific learning rate.   "
6536,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"Utility providers USED-FOR data privacy. natural informationtheoretic bounds FEATURE-OF utility - privacy trade - off. explicit learning architectures USED-FOR privacypreserving representations. utility algorithms USED-FOR privacy requirements. gender CONJUNCTION emotion detection. emotion detection CONJUNCTION gender. use cases EVALUATE-FOR framework. face recognition USED-FOR mobile devices. mobile devices HYPONYM-OF application. subject - withinsubject HYPONYM-OF use cases. Task are privacy protecting challenges, and facial verification. Generic are paradigm, bound, and algorithm. Method are machine learning algorithms, privacy - preserving representations, sanitization process, space - preserving transformations, and face identity detector. OtherScientificTerm are gender - and - subject, gender attribute, emotion - and - gender, and independent variables. ","This paper studies the problem of privacy protecting challenges in the context of utility providers for data privacy. The paper proposes a new paradigm where the utility-privacy trade-off is given as a natural informationtheoretic bounds on the privacy-preserving representations learned by explicit learning architectures.  The paper shows that existing machine learning algorithms can be seen as utility algorithms that satisfy certain privacy requirements.  In this paper, the paper proposes two new use cases: (1) face verification for facial verification, and (2) subject-withinsubject, which is an application of face recognition on mobile devices.   The main contribution of the paper is the introduction of a new notion of privacy preserving representations, which the paper calls privacy-and-subject.  This is defined as gender- and-subject, where each gender attribute is associated with a gender attribute and the subject attribute is assigned to a different gender. The authors show that under this definition, privacy- preserving representations can be learned in a sanitization process, where the privacy is preserved in the sense that the utility of the utility is preserved.  To achieve this bound, the authors propose a new algorithm that is based on the idea of space preserving transformations, which can be applied to any pair of independent variables (e.g. gender and emotion detection).  The authors also propose an algorithm that can be used to train a face identity detector.  Experiments on two use cases are conducted to validate the effectiveness of the proposed framework.","This paper studies the problem of privacy protecting challenges in the context of utility providers for data privacy. The paper proposes a new paradigm where the utility-privacy trade-off is given as a natural informationtheoretic bounds on the privacy-preserving representations learned by explicit learning architectures.  The paper shows that existing machine learning algorithms can be seen as utility algorithms that satisfy certain privacy requirements.  In this paper, the paper proposes two new use cases: (1) face verification for facial verification, and (2) subject-withinsubject, which is an application of face recognition on mobile devices.   The main contribution of the paper is the introduction of a new notion of privacy preserving representations, which the paper calls privacy-and-subject.  This is defined as gender- and-subject, where each gender attribute is associated with a gender attribute and the subject attribute is assigned to a different gender. The authors show that under this definition, privacy- preserving representations can be learned in a sanitization process, where the privacy is preserved in the sense that the utility of the utility is preserved.  To achieve this bound, the authors propose a new algorithm that is based on the idea of space preserving transformations, which can be applied to any pair of independent variables (e.g. gender and emotion detection).  The authors also propose an algorithm that can be used to train a face identity detector.  Experiments on two use cases are conducted to validate the effectiveness of the proposed framework."
6540,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"training instability FEATURE-OF Generative Adversarial Networks ( GANs ). backpropagation signal USED-FOR generator. task difficulty FEATURE-OF discriminator. progressive augmentation USED-FOR generator. progressive augmentation USED-FOR GAN objective. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR10 CONJUNCTION CELEBA. CELEBA CONJUNCTION CIFAR10. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. image generation task EVALUATE-FOR approach. Method are hyper - parameter tuning, progressive augmentation of GANs ( PAGAN ), and GAN training. OtherScientificTerm are fragile training behaviour, and input space. ","This paper studies the problem of training instability in Generative Adversarial Networks (GANs) due to hyper-parameter tuning. The authors propose a progressive augmentation of GANs (PAGAN) to address the issue of fragile training behaviour. Specifically, the generator is augmented with a backpropagation signal that is sensitive to the task difficulty of the discriminator, and the progressive augmentation is applied to the GAN objective. Experiments are conducted on MNIST, Fashion-MNIST, CIFAR10, and CELEBA. The approach is evaluated on the image generation task, and it is shown that the progressive augmented GAN training results in a better performance. ","This paper studies the problem of training instability in Generative Adversarial Networks (GANs) due to hyper-parameter tuning. The authors propose a progressive augmentation of GANs (PAGAN) to address the issue of fragile training behaviour. Specifically, the generator is augmented with a backpropagation signal that is sensitive to the task difficulty of the discriminator, and the progressive augmentation is applied to the GAN objective. Experiments are conducted on MNIST, Fashion-MNIST, CIFAR10, and CELEBA. The approach is evaluated on the image generation task, and it is shown that the progressive augmented GAN training results in a better performance. "
6544,SP:c210982ccdd134d4b293dbe144990398eefe1a86,"models USED-FOR neural responses. models USED-FOR primary visual cortex ( V1 ). convolutional neural networks ( CNNs ) USED-FOR V1 activity. orientation selectivity CONJUNCTION phase invariance. phase invariance CONJUNCTION orientation selectivity. orientation selectivity FEATURE-OF V1 neurons. phase invariance FEATURE-OF V1 neurons. V1 neurons USED-FOR features. framework USED-FOR common features. rotation - equivariant convolutional neural network USED-FOR framework. mouse primary visual cortex FEATURE-OF natural images. two - photon imaging USED-FOR rotation - equivariant CNN. rotation - equivariant network COMPARE regular CNN. regular CNN COMPARE rotation - equivariant network. rotation - equivariant network USED-FOR common features. feature maps USED-FOR regular CNN. Method is energy models. Task are V1 computations, and nonlinear functional organization of visual cortex. OtherScientificTerm is neural activity. ","This paper presents a study of how models for primary visual cortex (V1) are related to neural responses. It shows that convolutional neural networks (CNNs) can capture V1 activity, and that models for neural responses in V1 are equivariant to energy models. The paper also shows that V1 neurons have a high orientation selectivity and phase invariance, which explains why V1 computations tend to be nonlinear.  The paper further shows that the features learned by V1 networks are invariant to the rotation of the input image, which is a result of the nonlinear functional organization of visual cortex. The authors then propose a framework to learn common features from a rotation-equivariant convolution neural network, and show that the rotation-Equivariant CNN can be trained using two-photon imaging of natural images from the mouse primary visual cortices. They also show that rotation equivariance of the rotation in neural activity is correlated with the rotation selectivity of V1.  Finally, the paper shows that a rotation -equivariance network is able to learn more common features than a regular CNN trained on the same feature maps. ","This paper presents a study of how models for primary visual cortex (V1) are related to neural responses. It shows that convolutional neural networks (CNNs) can capture V1 activity, and that models for neural responses in V1 are equivariant to energy models. The paper also shows that V1 neurons have a high orientation selectivity and phase invariance, which explains why V1 computations tend to be nonlinear.  The paper further shows that the features learned by V1 networks are invariant to the rotation of the input image, which is a result of the nonlinear functional organization of visual cortex. The authors then propose a framework to learn common features from a rotation-equivariant convolution neural network, and show that the rotation-Equivariant CNN can be trained using two-photon imaging of natural images from the mouse primary visual cortices. They also show that rotation equivariance of the rotation in neural activity is correlated with the rotation selectivity of V1.  Finally, the paper shows that a rotation -equivariance network is able to learn more common features than a regular CNN trained on the same feature maps. "
6548,SP:f17090812ace9c83d418b17bf165649232c223e3,"large datasets USED-FOR neural networks. algorithm USED-FOR robust, communication - efficient learning. algorithm USED-FOR SIGNSGD. algorithm COMPARE full - precision, distributed SGD. full - precision, distributed SGD COMPARE algorithm. communication COMPARE full - precision, distributed SGD. full - precision, distributed SGD COMPARE communication. communication USED-FOR algorithm. convergence USED-FOR parameter regime. parameter regime FEATURE-OF ADAM. large and mini - batch settings FEATURE-OF SIGNSGD. majority vote USED-FOR sign gradients. SGD COMPARE majority vote. majority vote COMPARE SGD. Pytorch USED-FOR distributed training system. time EVALUATE-FOR resnet50. collective communications library ( NCCL ) COMPARE framework. framework COMPARE collective communications library ( NCCL ). Imagenet USED-FOR resnet50. parameter server PART-OF framework. time EVALUATE-FOR framework. AWS p3.2xlarge machines USED-FOR resnet50. Generic is networks. OtherScientificTerm are communicating gradients, machine counts, network faults, gradient vector, server, class of adversaries, and gradient estimate. ","This paper proposes a new algorithm for robust, communication-efficient learning for neural networks trained on large datasets. The authors consider the problem of communicating gradients in the presence of machine counts, network faults, and adversarial perturbations. They propose an algorithm called SIGNSGD, which is an extension of ADAM in the parameter regime. The algorithm is shown to outperform full-precision, distributed SGD in terms of communication compared to the standard, full-principality,distributed SGD. In particular, the authors show that the convergence of the proposed algorithm is faster than ADAM under the same parameter regime, and that the algorithm is more robust to class of adversaries.   The authors also show that in large and mini-batch settings, the algorithm outperforms SGD with majority vote on the sign gradients. The main contribution of the paper is that the authors propose a distributed training system based on Pytorch, where the gradient vector is shared across all machines and the server is the only responsible for updating the gradients of each machine. The paper also shows that in the case of resnet50 trained on Imagenet on 2xlarge machines on Amazon p3, the proposed framework is able to reduce the time of training by a factor of $O(\sqrt{O(log n))$ compared to SGD, where SGD uses majority vote for the sign of the gradient and the majority vote is used to update the gradient of the server. The proposed framework also includes a parameter server, which can be used to share the gradient estimate between all machines. Experiments are conducted on a distributed version of Resnet50 on Amazon's p3 and on ResNet50 on resnet-50 on Resnet-100. The results show that this framework outperforms the standard collective communications library (NCCL) and the original framework (Resnet50) in time. ","This paper proposes a new algorithm for robust, communication-efficient learning for neural networks trained on large datasets. The authors consider the problem of communicating gradients in the presence of machine counts, network faults, and adversarial perturbations. They propose an algorithm called SIGNSGD, which is an extension of ADAM in the parameter regime. The algorithm is shown to outperform full-precision, distributed SGD in terms of communication compared to the standard, full-principality,distributed SGD. In particular, the authors show that the convergence of the proposed algorithm is faster than ADAM under the same parameter regime, and that the algorithm is more robust to class of adversaries.   The authors also show that in large and mini-batch settings, the algorithm outperforms SGD with majority vote on the sign gradients. The main contribution of the paper is that the authors propose a distributed training system based on Pytorch, where the gradient vector is shared across all machines and the server is the only responsible for updating the gradients of each machine. The paper also shows that in the case of resnet50 trained on Imagenet on 2xlarge machines on Amazon p3, the proposed framework is able to reduce the time of training by a factor of $O(\sqrt{O(log n))$ compared to SGD, where SGD uses majority vote for the sign of the gradient and the majority vote is used to update the gradient of the server. The proposed framework also includes a parameter server, which can be used to share the gradient estimate between all machines. Experiments are conducted on a distributed version of Resnet50 on Amazon's p3 and on ResNet50 on resnet-50 on Resnet-100. The results show that this framework outperforms the standard collective communications library (NCCL) and the original framework (Resnet50) in time. "
6552,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,approach USED-FOR realistic and high - fidelity stock market data. generative adversarial networks USED-FOR approach. conditional Wasserstein GAN USED-FOR history dependence of orders. stock market FEATURE-OF history dependence of orders. finite history dependence FEATURE-OF stochastic process. stochastic process USED-FOR order stream. actual market and synthetic data EVALUATE-FOR approach. Material is real data. ,This paper proposes a new approach to generate realistic and high-fidelity stock market data using generative adversarial networks. The idea is to use conditional Wasserstein GAN to model the history dependence of orders in the stock market. The order stream is modeled as a stochastic process with finite history dependence. The proposed approach is evaluated on both actual market and synthetic data. ,This paper proposes a new approach to generate realistic and high-fidelity stock market data using generative adversarial networks. The idea is to use conditional Wasserstein GAN to model the history dependence of orders in the stock market. The order stream is modeled as a stochastic process with finite history dependence. The proposed approach is evaluated on both actual market and synthetic data. 
6556,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"deep reinforcement learning ( DRL ) algorithm USED-FOR arbitrary errors. deep reinforcement learning ( DRL ) algorithm USED-FOR applications. robotics HYPONYM-OF applications. RL agents USED-FOR observed rewards. reward confusion matrix USED-FOR observed rewards. RL agents USED-FOR noisy environments. approaches USED-FOR supervised learning. supervised learning USED-FOR framework. noisy data USED-FOR approaches. noisy data USED-FOR supervised learning. approaches USED-FOR framework. sample complexity EVALUATE-FOR approach. convergence CONJUNCTION sample complexity. sample complexity CONJUNCTION convergence. convergence EVALUATE-FOR approach. policies COMPARE baselines. baselines COMPARE policies. expected rewards EVALUATE-FOR policies. estimated surrogate reward USED-FOR policies. Atari games EVALUATE-FOR PPO algorithm. Method are reinforcement learning ( RL ) models, and unbiased reward estimator aided robust RL framework. OtherScientificTerm are noises, observed reward channel, perturbed rewards, and unbiased surrogate rewards. Task is noisy RL problems. Generic is solution. Material is DRL platforms. Metric is error rates. ","This paper proposes a deep reinforcement learning (RL) algorithm that can be applied to arbitrary errors in noisy environments, which is an important problem in many applications such as robotics and reinforcement learning where noises are present in the environment. The authors propose an unbiased reward estimator aided robust RL framework, where the observed reward channel is perturbed and the observed rewards are perturbed. The proposed solution is based on the observation that RL agents learn to predict observed rewards from the reward confusion matrix, which can be used to train RL agents for noisy environments. The framework builds on previous approaches to supervised learning with noisy data, which uses existing approaches for supervised learning. The paper shows that the proposed approach can achieve better convergence and sample complexity in noisy RL problems. In particular, the authors show that policies trained with the estimated surrogate reward are more robust to perturbed rewards than baselines that do not use the unbiased surrogate rewards. The PPO algorithm is tested on a number of Atari games and is shown to outperform existing DRL platforms in terms of error rates.  ","This paper proposes a deep reinforcement learning (RL) algorithm that can be applied to arbitrary errors in noisy environments, which is an important problem in many applications such as robotics and reinforcement learning where noises are present in the environment. The authors propose an unbiased reward estimator aided robust RL framework, where the observed reward channel is perturbed and the observed rewards are perturbed. The proposed solution is based on the observation that RL agents learn to predict observed rewards from the reward confusion matrix, which can be used to train RL agents for noisy environments. The framework builds on previous approaches to supervised learning with noisy data, which uses existing approaches for supervised learning. The paper shows that the proposed approach can achieve better convergence and sample complexity in noisy RL problems. In particular, the authors show that policies trained with the estimated surrogate reward are more robust to perturbed rewards than baselines that do not use the unbiased surrogate rewards. The PPO algorithm is tested on a number of Atari games and is shown to outperform existing DRL platforms in terms of error rates.  "
6560,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"network structure USED-FOR halting time. overparametrization USED-FOR weight space traversal. power - law - like relationship USED-FOR average step size. Method is gradient descent. OtherScientificTerm are computational requirements, model ’s width, gradient vectors, and traversal. Generic are larger models, and applications. ","This paper studies the problem of gradient descent in the setting where the computational requirements are large and the network structure has a halting time. The authors show that the overparametrization of the weight space traversal is the main cause of the slow convergence of the gradient descent. They also show that a power-law-like relationship between the model’s width and the number of gradient vectors is necessary for the average step size of the traversal to converge to a stationary point. They show that larger models are more likely to reach stationary points, and that this is the case for larger models. The paper also shows that this phenomenon is also observed in other applications. ","This paper studies the problem of gradient descent in the setting where the computational requirements are large and the network structure has a halting time. The authors show that the overparametrization of the weight space traversal is the main cause of the slow convergence of the gradient descent. They also show that a power-law-like relationship between the model’s width and the number of gradient vectors is necessary for the average step size of the traversal to converge to a stationary point. They show that larger models are more likely to reach stationary points, and that this is the case for larger models. The paper also shows that this phenomenon is also observed in other applications. "
6564,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"learning framework USED-FOR setting. reinforcement learning USED-FOR online optimization problems. reinforcement learning USED-FOR theoretically optimal algorithms. theoretically optimal algorithms USED-FOR online optimization problems. algorithms CONJUNCTION complexity theory. complexity theory CONJUNCTION algorithms. universal and high - entropy training sets HYPONYM-OF adversarial distributions. online knapsack problem CONJUNCTION secretary problem. secretary problem CONJUNCTION online knapsack problem. AdWords problem CONJUNCTION online knapsack problem. online knapsack problem CONJUNCTION AdWords problem. online knapsack problem EVALUATE-FOR ideas. secretary problem EVALUATE-FOR ideas. AdWords problem EVALUATE-FOR ideas. optimal algorithms USED-FOR problems. online primal - dual framework USED-FOR problems. OtherScientificTerm are distributions, and worst case. Method is learner. Generic is models. ","This paper proposes a learning framework for the online optimization setting, where the goal is to learn theoretically optimal algorithms for online optimization problems using reinforcement learning. The authors draw inspiration from existing algorithms and complexity theory to develop algorithms that are robust to adversarial distributions (i.e., universal and high-entropy training sets). The authors evaluate their ideas on the AdWords problem, the online knapsack problem, and the secretary problem. They show that optimal algorithms can be learned for all three problems using the online primal-dual framework, and that the optimal algorithms are robust against these distributions. They also show that the learner is able to generalize to new distributions in the worst case. The paper also shows that the learned models can be used to train models that generalize well to unseen distributions.","This paper proposes a learning framework for the online optimization setting, where the goal is to learn theoretically optimal algorithms for online optimization problems using reinforcement learning. The authors draw inspiration from existing algorithms and complexity theory to develop algorithms that are robust to adversarial distributions (i.e., universal and high-entropy training sets). The authors evaluate their ideas on the AdWords problem, the online knapsack problem, and the secretary problem. They show that optimal algorithms can be learned for all three problems using the online primal-dual framework, and that the optimal algorithms are robust against these distributions. They also show that the learner is able to generalize to new distributions in the worst case. The paper also shows that the learned models can be used to train models that generalize well to unseen distributions."
6568,SP:b99732087f5a929ab248acdcd7a943bce8671510,"inductive biases PART-OF deep reinforcement learning algorithms. domain knowledge CONJUNCTION pretuned hyperparameters. pretuned hyperparameters CONJUNCTION domain knowledge. domain knowledge HYPONYM-OF inductive biases. pretuned hyperparameters HYPONYM-OF inductive biases. fixed components COMPARE adaptive solutions. adaptive solutions COMPARE fixed components. components COMPARE adaptive components. adaptive components COMPARE components. learning EVALUATE-FOR systems. continuous control problems EVALUATE-FOR systems. adaptive components USED-FOR system. tasks EVALUATE-FOR system. deep RL algorithms USED-FOR tasks. deep reinforcement learning ( RL ) community USED-FOR tasks. Go CONJUNCTION Chess. Chess CONJUNCTION Go. board - games CONJUNCTION video - games. video - games CONJUNCTION board - games. video - games CONJUNCTION 3D navigation tasks. 3D navigation tasks CONJUNCTION video - games. Atari HYPONYM-OF video - games. Go HYPONYM-OF board - games. Chess HYPONYM-OF board - games. tuning USED-FOR these. these USED-FOR new domains. tuning USED-FOR new domains. inductive biases FEATURE-OF agents. AlphaZero COMPARE AlphaGo algorithm. AlphaGo algorithm COMPARE AlphaZero. Go - specific inductive biases CONJUNCTION human data. human data CONJUNCTION Go - specific inductive biases. Chess CONJUNCTION Shogi. Shogi CONJUNCTION Chess. AlphaZero USED-FOR Chess. AlphaZero USED-FOR Shogi. biases USED-FOR AlphaZero. Go USED-FOR AlphaZero. generality CONJUNCTION performance. performance CONJUNCTION generality. inductive biases PART-OF algorithms. domain knowledge CONJUNCTION pretuned learning parameters. pretuned learning parameters CONJUNCTION domain knowledge. domain knowledge PART-OF Inductive biases. pretuned learning parameters PART-OF Inductive biases. domain knowledge CONJUNCTION pretune parameters. pretune parameters CONJUNCTION domain knowledge. Generic are problems, and approach. OtherScientificTerm is hyper - parameters. Method are domain - specific components, RL algorithms, AlphaZero algorithm, and learning algorithm. ","This paper investigates the role of inductive biases in deep reinforcement learning algorithms (i.e., domain knowledge and pretuned hyperparameters) in order to improve the performance of systems in learning on a variety of continuous control problems. The authors consider a set of tasks that have been popularly used in the deep RL community, where the goal is to train a system on these tasks using deep RL algorithms. These problems are often difficult to train as the hyper-parameters are not always the same across all domains, but rather, there are domain-specific components that are shared across different domains.    The authors show that, in general, the inductive bias of RL algorithms can be seen as a trade-off between the generality of the learned parameters and the generalization ability of the learning algorithm. In particular, they show that fixed components are more powerful than adaptive solutions, and that these are particularly useful for new domains that require tuning.  They also show that this approach can be applied to a wide range of domains, including board-games, video-games (e.g., Atari), and 3D navigation tasks.  Finally, the authors compare AlphaZero on Go, Chess, and Shogi using these biases, and show that AlphaZero outperforms the AlphaGo algorithm on all of these tasks. Inductive biases in RL algorithms include domain knowledge, pretuned learning parameters, and pretune parameters. They also find that agents with inductive biased biases are more likely to generalize well to new domains, and generalize better to new environments. ","This paper investigates the role of inductive biases in deep reinforcement learning algorithms (i.e., domain knowledge and pretuned hyperparameters) in order to improve the performance of systems in learning on a variety of continuous control problems. The authors consider a set of tasks that have been popularly used in the deep RL community, where the goal is to train a system on these tasks using deep RL algorithms. These problems are often difficult to train as the hyper-parameters are not always the same across all domains, but rather, there are domain-specific components that are shared across different domains.    The authors show that, in general, the inductive bias of RL algorithms can be seen as a trade-off between the generality of the learned parameters and the generalization ability of the learning algorithm. In particular, they show that fixed components are more powerful than adaptive solutions, and that these are particularly useful for new domains that require tuning.  They also show that this approach can be applied to a wide range of domains, including board-games, video-games (e.g., Atari), and 3D navigation tasks.  Finally, the authors compare AlphaZero on Go, Chess, and Shogi using these biases, and show that AlphaZero outperforms the AlphaGo algorithm on all of these tasks. Inductive biases in RL algorithms include domain knowledge, pretuned learning parameters, and pretune parameters. They also find that agents with inductive biased biases are more likely to generalize well to new domains, and generalize better to new environments. "
6572,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,photo - realistic unknown environments FEATURE-OF navigational instruction. visual - textual co - grounding module CONJUNCTION progress monitor. progress monitor CONJUNCTION visual - textual co - grounding module. complementary components PART-OF self - monitoring agent. visual - textual co - grounding module HYPONYM-OF complementary components. progress monitor HYPONYM-OF complementary components. ablation studies USED-FOR primary components. ablation studies USED-FOR approach. benchmark EVALUATE-FOR selfmonitoring agent. method USED-FOR state. method USED-FOR art. Generic is task. OtherScientificTerm is navigation progress. Metric is success rate. ,"This paper proposes a self-monitoring agent that combines two complementary components: a visual-textual co-grounding module and a progress monitor. The authors present a new benchmark for the task of learning to follow navigational instruction in photo-realistic unknown environments. The proposed approach is based on ablation studies on the primary components of the proposed approach, and the authors show that their approach achieves a high success rate. They also show that the proposed benchmark can be used to evaluate the performance of a selfmonitoring policy. The paper also shows that the method can be applied to any state-of-the-art self-supervised learning method, and that it can be combined with any art to improve the performance.","This paper proposes a self-monitoring agent that combines two complementary components: a visual-textual co-grounding module and a progress monitor. The authors present a new benchmark for the task of learning to follow navigational instruction in photo-realistic unknown environments. The proposed approach is based on ablation studies on the primary components of the proposed approach, and the authors show that their approach achieves a high success rate. They also show that the proposed benchmark can be used to evaluate the performance of a selfmonitoring policy. The paper also shows that the method can be applied to any state-of-the-art self-supervised learning method, and that it can be combined with any art to improve the performance."
6576,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,input - output examples USED-FOR Neural program synthesis. encoder USED-FOR embedding. decoder USED-FOR program. syntax FEATURE-OF embedding. encoder USED-FOR encoder - decoder architecture. embedding USED-FOR decoder. encoder - decoder architecture USED-FOR neural program synthesis approaches. approaches USED-FOR tasks. approaches USED-FOR tasks. tasks EVALUATE-FOR state - of - the - art approach. tasks HYPONYM-OF tasks. accuracy EVALUATE-FOR state - of - the - art approach. Karel HYPONYM-OF tasks. Karel HYPONYM-OF tasks. FlashFill HYPONYM-OF tasks. execution - guided synthesis CONJUNCTION synthesizer ensemble. synthesizer ensemble CONJUNCTION execution - guided synthesis. techniques USED-FOR semantic information. synthesizer ensemble HYPONYM-OF techniques. execution - guided synthesis HYPONYM-OF techniques. synthesizer ensemble HYPONYM-OF semantic information. execution - guided synthesis HYPONYM-OF semantic information. techniques CONJUNCTION encoder - decoder - style neural program synthesizer. encoder - decoder - style neural program synthesizer CONJUNCTION techniques. accuracy EVALUATE-FOR techniques. Karel dataset EVALUATE-FOR techniques. ,"Neural program synthesis from input-output examples is an important problem in computer vision. This paper proposes a novel encoder-decoder architecture for neural program synthesis approaches that uses an encoder to learn an embedding of the syntax and a decoder to synthesize the program. The authors show that the encoder learns a good embedding for the program and the decoder is able to learn the embedding from the learned embedding. The proposed approaches are applied to a variety of tasks (e.g., execution-guided synthesis, synthesizer ensemble, and FlashFill) and achieve state-of-the-art accuracy on a number of tasks, including Karel, which is a popular benchmark for program synthesis. The paper also shows that the proposed techniques can capture semantic information (executed execution, synthesized execution, and so on) and improve the accuracy on the Karel dataset. Finally, the authors demonstrate that their techniques can be combined with a traditional encoder - decoder-style neural program synthesizer.","Neural program synthesis from input-output examples is an important problem in computer vision. This paper proposes a novel encoder-decoder architecture for neural program synthesis approaches that uses an encoder to learn an embedding of the syntax and a decoder to synthesize the program. The authors show that the encoder learns a good embedding for the program and the decoder is able to learn the embedding from the learned embedding. The proposed approaches are applied to a variety of tasks (e.g., execution-guided synthesis, synthesizer ensemble, and FlashFill) and achieve state-of-the-art accuracy on a number of tasks, including Karel, which is a popular benchmark for program synthesis. The paper also shows that the proposed techniques can capture semantic information (executed execution, synthesized execution, and so on) and improve the accuracy on the Karel dataset. Finally, the authors demonstrate that their techniques can be combined with a traditional encoder - decoder-style neural program synthesizer."
6580,SP:dc7dfc1eec473800580dba309446871122be6040,"stability FEATURE-OF batch normalization ( BN ). BN USED-FOR simplified model. ordinary least squares ( OLS ) HYPONYM-OF simplified model. modeling approach USED-FOR problem. scaling law CONJUNCTION convergence. convergence CONJUNCTION scaling law. gradient descent USED-FOR OLS. arbitrary learning rates FEATURE-OF weights. convergence FEATURE-OF arbitrary learning rates. convergence CONJUNCTION acceleration effects. acceleration effects CONJUNCTION convergence. BN USED-FOR gradient descent. mathematical principles USED-FOR batch normalization. OtherScientificTerm is learning rates. Task are OLS problem, and supervised learning problems. ","This paper studies the stability of batch normalization (BN) and its application to the simplified model called ordinary least squares (OLS). The authors propose a new modeling approach to solve the problem. They prove a scaling law and convergence result for OLS under arbitrary learning rates for the weights. They also show that gradient descent with BN converges to OLS in the case of OLS with arbitrarily large learning rates. Finally, they show that the OLS problem can be seen as a special case of supervised learning problems.    The authors also provide a theoretical analysis of the convergence and acceleration effects of BN. They show that under certain mathematical principles, BN with gradient descent converges as long as the learning rate is large enough. ","This paper studies the stability of batch normalization (BN) and its application to the simplified model called ordinary least squares (OLS). The authors propose a new modeling approach to solve the problem. They prove a scaling law and convergence result for OLS under arbitrary learning rates for the weights. They also show that gradient descent with BN converges to OLS in the case of OLS with arbitrarily large learning rates. Finally, they show that the OLS problem can be seen as a special case of supervised learning problems.    The authors also provide a theoretical analysis of the convergence and acceleration effects of BN. They show that under certain mathematical principles, BN with gradient descent converges as long as the learning rate is large enough. "
6584,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,data noising USED-FOR recurrent neural network language models. theoretical perspective USED-FOR data noising. data noising HYPONYM-OF Bayesian recurrent neural networks. variational distribution FEATURE-OF Bayesian recurrent neural networks. variational framework USED-FOR data noising. variational smoothing CONJUNCTION element - wise variational smoothing method. element - wise variational smoothing method CONJUNCTION variational smoothing. tied input and output embedding matrices CONJUNCTION element - wise variational smoothing method. element - wise variational smoothing method CONJUNCTION tied input and output embedding matrices. tied input and output embedding matrices USED-FOR variational smoothing. benchmark language modeling datasets EVALUATE-FOR data noising methods. Method is mixture of Gaussians. OtherScientificTerm is unigram distribution. Generic is method. ,"This paper studies the problem of data noising in recurrent neural network language models from a theoretical perspective. In particular, the authors consider Bayesian recurrent neural networks with a variational distribution over a mixture of Gaussians. The authors propose a new variational framework for dealing with data no noise in such a way that the unigram distribution of the input and the output of a Bayesian neural network can be approximated by a mixture. The proposed variational smoothing is based on tied input and output embedding matrices, an element-wise variational smoothhing method, and a combination of the two. Experiments on several benchmark language modeling datasets are conducted to demonstrate the effectiveness of the proposed data noizing methods. The results show that the proposed method outperforms existing methods.","This paper studies the problem of data noising in recurrent neural network language models from a theoretical perspective. In particular, the authors consider Bayesian recurrent neural networks with a variational distribution over a mixture of Gaussians. The authors propose a new variational framework for dealing with data no noise in such a way that the unigram distribution of the input and the output of a Bayesian neural network can be approximated by a mixture. The proposed variational smoothing is based on tied input and output embedding matrices, an element-wise variational smoothhing method, and a combination of the two. Experiments on several benchmark language modeling datasets are conducted to demonstrate the effectiveness of the proposed data noizing methods. The results show that the proposed method outperforms existing methods."
6588,SP:f4a914d3df1a5a21a7365ba78279420f39210884,"saliency maps USED-FOR classification. classifier - dependent methods USED-FOR Extracting saliency maps. approach USED-FOR saliency maps. approach COMPARE weakly - supervised localization techniques. weakly - supervised localization techniques COMPARE approach. Method are classifier - agnostic saliency map extraction, and classifier. Material is ImageNet dataset. ","This paper proposes a classifier-agnostic saliency map extraction method. The idea is that saliency maps are useful for classification, but they are also useful for weakly-supervised localization. The authors propose a new approach for learning saliencymaps that are agnostic to the classifier. They show that the proposed method outperforms existing methods on the ImageNet dataset.","This paper proposes a classifier-agnostic saliency map extraction method. The idea is that saliency maps are useful for classification, but they are also useful for weakly-supervised localization. The authors propose a new approach for learning saliencymaps that are agnostic to the classifier. They show that the proposed method outperforms existing methods on the ImageNet dataset."
6592,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,deep nets USED-FOR task. net USED-FOR task. net USED-FOR model. deep nets USED-FOR task. knowledge flow USED-FOR deep net model. teachers HYPONYM-OF deep nets. tasks EVALUATE-FOR they. output spaces FEATURE-OF tasks. fine - tuning COMPARE knowledge exchange ’ methods. knowledge exchange ’ methods COMPARE fine - tuning. supervised and reinforcement learning tasks EVALUATE-FOR fine - tuning. approach COMPARE fine - tuning. fine - tuning COMPARE approach. supervised and reinforcement learning tasks EVALUATE-FOR approach. ,"This paper proposes to use deep nets (i.e. teachers) to fine-tune deep nets for a given task. The idea is that a deep net model is trained using a knowledge flow, where each net is trained on a different task, and the goal is to learn a model that generalizes well to new tasks. The authors show that deep nets can be fine-tuned on a variety of tasks with different output spaces, and that they are able to generalize well to unseen tasks. They show that this approach outperforms other ‘fine-tuning’ methods on both supervised and reinforcement learning tasks.","This paper proposes to use deep nets (i.e. teachers) to fine-tune deep nets for a given task. The idea is that a deep net model is trained using a knowledge flow, where each net is trained on a different task, and the goal is to learn a model that generalizes well to new tasks. The authors show that deep nets can be fine-tuned on a variety of tasks with different output spaces, and that they are able to generalize well to unseen tasks. They show that this approach outperforms other ‘fine-tuning’ methods on both supervised and reinforcement learning tasks."
6596,SP:a72072879f7c61270d952f06d9ce995e8150632c,"spatial and temporal dimensions FEATURE-OF higher - order inter - dependencies. higher - order inter - dependencies FEATURE-OF data streams. soft - clustering USED-FOR compact dynamical model. dynamics USED-FOR compact dynamical model. predictive accuracy EVALUATE-FOR mathematical representation. stochastic calculus PART-OF information theory inspired approach. predictive ability CONJUNCTION causal interdependence ( relatedness ) constraints. causal interdependence ( relatedness ) constraints CONJUNCTION predictive ability. data streams CONJUNCTION compact model. compact model CONJUNCTION data streams. maximization of the compression of the state variables USED-FOR model construction. convergence FEATURE-OF learning algorithm. iterative scheme USED-FOR model parameters. high - dimensional Gaussian case study EVALUATE-FOR framework. compression and prediction accuracy EVALUATE-FOR dynamical systems. algorithm USED-FOR prediction. reduced dimensions FEATURE-OF prediction. real - world dataset of multimodal sentiment intensity EVALUATE-FOR algorithm. Task are Extracting relevant information, modeling complex systems, and causal inference. Metric is accuracy. Generic is tasks. Material is high - dimensional heterogeneous data streams. "," Extracting relevant information from data streams with higher-order inter-dependencies in spatial and temporal dimensions is an important problem in modeling complex systems. This paper proposes a compact dynamical model based on soft-clustering. The dynamics of the dynamics is decomposed into two parts: (1) the maximization of the compression of the state variables during model construction, and (2) the predictive accuracy of the mathematical representation. The authors introduce an information theory inspired approach that incorporates stochastic calculus. The framework is tested on a high-dimensional Gaussian case study, and is shown to improve the compression and prediction accuracy of dynamical systems.   The authors also show that the convergence of the learning algorithm can be improved in the presence of data streams and a compact model. They also propose an iterative scheme to compress the model parameters, and show that their algorithm improves the prediction performance on a real-world dataset of multimodal sentiment intensity. Finally, the authors demonstrate that the proposed algorithm can improve the accuracy on a number of tasks, and that it can be applied to high-dimension heterogeneous data streams. The main contribution of the paper is the theoretical analysis of the relationship between the predictive ability and the causal interdependence (relatedness) constraints, and the experimental results on causal inference. The paper also shows that the prediction on reduced dimensions is more accurate."," Extracting relevant information from data streams with higher-order inter-dependencies in spatial and temporal dimensions is an important problem in modeling complex systems. This paper proposes a compact dynamical model based on soft-clustering. The dynamics of the dynamics is decomposed into two parts: (1) the maximization of the compression of the state variables during model construction, and (2) the predictive accuracy of the mathematical representation. The authors introduce an information theory inspired approach that incorporates stochastic calculus. The framework is tested on a high-dimensional Gaussian case study, and is shown to improve the compression and prediction accuracy of dynamical systems.   The authors also show that the convergence of the learning algorithm can be improved in the presence of data streams and a compact model. They also propose an iterative scheme to compress the model parameters, and show that their algorithm improves the prediction performance on a real-world dataset of multimodal sentiment intensity. Finally, the authors demonstrate that the proposed algorithm can improve the accuracy on a number of tasks, and that it can be applied to high-dimension heterogeneous data streams. The main contribution of the paper is the theoretical analysis of the relationship between the predictive ability and the causal interdependence (relatedness) constraints, and the experimental results on causal inference. The paper also shows that the prediction on reduced dimensions is more accurate."
6600,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"variational autoencoder USED-FOR single neural probabilistic model. stochastic variational Bayes USED-FOR model. feature imputation CONJUNCTION image inpainting problems. image inpainting problems CONJUNCTION feature imputation. synthetic data CONJUNCTION feature imputation. feature imputation CONJUNCTION synthetic data. synthetic data EVALUATE-FOR approach. OtherScientificTerm are observed features, and features. ","This paper proposes a variational autoencoder for a single neural probabilistic model. The model is based on stochastic variational Bayes, where the observed features are used to train a single model. This approach is evaluated on synthetic data, feature imputation and image inpainting problems. ","This paper proposes a variational autoencoder for a single neural probabilistic model. The model is based on stochastic variational Bayes, where the observed features are used to train a single model. This approach is evaluated on synthetic data, feature imputation and image inpainting problems. "
6604,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"architecture design USED-FOR neural network models. tasks EVALUATE-FOR neural network models. intermediate layer activations USED-FOR back - propagation. memory footprint FEATURE-OF models. spatial resolutions FEATURE-OF layers. approximation strategy USED-FOR network. memory footprint FEATURE-OF network. training EVALUATE-FOR approximation strategy. lower - precision approximations USED-FOR activations. approximate activations USED-FOR backward pass. approximation USED-FOR gradients. CIFAR CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR. ImageNet EVALUATE-FOR approach. CIFAR EVALUATE-FOR approach. 32 - bit floating - point activations USED-FOR approach. training and validation performance EVALUATE-FOR approach. OtherScientificTerm are Limited GPU memory, memory, and forward and backward pass. Generic are architecture, and forward pass. Metric are computational expense, and memory usage. ","This paper proposes a new architecture design for neural network models on a variety of tasks. Limited GPU memory is of great importance in many applications, and the memory footprint of modern models is critical in many of these tasks. The authors propose to use intermediate layer activations for back-propagation in order to reduce the computational expense.    The architecture is based on the idea that the memory of the forward and backward pass can be reduced by using lower-precision approximations to the activations of the intermediate layers.  The authors show that this approximation strategy can reduce the network's memory footprint during training and during validation. The backward pass uses approximate activations, while the forward pass uses the approximation to compute the gradients.  Experiments on CIFAR and ImageNet show that the proposed approach achieves competitive performance on both training and validation performance with 32-bit floating-point activations. The paper also shows that the lower memory usage can be further reduced by reducing the spatial resolutions of the layers.","This paper proposes a new architecture design for neural network models on a variety of tasks. Limited GPU memory is of great importance in many applications, and the memory footprint of modern models is critical in many of these tasks. The authors propose to use intermediate layer activations for back-propagation in order to reduce the computational expense.    The architecture is based on the idea that the memory of the forward and backward pass can be reduced by using lower-precision approximations to the activations of the intermediate layers.  The authors show that this approximation strategy can reduce the network's memory footprint during training and during validation. The backward pass uses approximate activations, while the forward pass uses the approximation to compute the gradients.  Experiments on CIFAR and ImageNet show that the proposed approach achieves competitive performance on both training and validation performance with 32-bit floating-point activations. The paper also shows that the lower memory usage can be further reduced by reducing the spatial resolutions of the layers."
6608,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"Face completion HYPONYM-OF task. complexity CONJUNCTION controllable attributes of filled - in fragments. controllable attributes of filled - in fragments CONJUNCTION complexity. end - to - end framework USED-FOR generative adversarial networks ( GANs ). conditional vectors USED-FOR controllable attributes. end - to - end framework USED-FOR system. mean inference time FEATURE-OF images. system USED-FOR structural and appearance variations. mean inference time FEATURE-OF feed - forward pass of computation. 1024× 1024 resolution FEATURE-OF images. feed - forward pass of computation USED-FOR system. approach COMPARE face completion methods. face completion methods COMPARE approach. OtherScientificTerm are high resolution, and frequency components. Generic are model, and network. ","Face completion is an important task that is challenging due to the high resolution, complexity, and controllable attributes of filled-in fragments. This paper proposes an end-to-end framework for generative adversarial networks (GANs) that learns a system that is able to capture structural and appearance variations. The system is trained with a feed-forward pass of computation on images of 1024×1024 resolution with mean inference time of $O(1/\sqrt{n})$ and $O(\log n)$, where $n$ is the size of the input image and $N$ the number of generated conditional vectors that encode the controlled attributes. The proposed approach is evaluated on a variety of face completion methods and shows that the proposed approach outperforms the state-of-the-art.    The model is based on a GAN-based approach, where the model is trained to generate a sequence of conditional vectors $f$ that capture the frequency components of the face, and the network is trained on the generated sequences. ","Face completion is an important task that is challenging due to the high resolution, complexity, and controllable attributes of filled-in fragments. This paper proposes an end-to-end framework for generative adversarial networks (GANs) that learns a system that is able to capture structural and appearance variations. The system is trained with a feed-forward pass of computation on images of 1024×1024 resolution with mean inference time of $O(1/\sqrt{n})$ and $O(\log n)$, where $n$ is the size of the input image and $N$ the number of generated conditional vectors that encode the controlled attributes. The proposed approach is evaluated on a variety of face completion methods and shows that the proposed approach outperforms the state-of-the-art.    The model is based on a GAN-based approach, where the model is trained to generate a sequence of conditional vectors $f$ that capture the frequency components of the face, and the network is trained on the generated sequences. "
6612,SP:a300122021e93d695af85e158f2b402d21525bc8,"high - precision accumulators USED-FOR systems. precision requirements FEATURE-OF partial sum accumulations. reduced accumulation precision USED-FOR deep learning training. statistical approach USED-FOR reduced accumulation precision. analysis USED-FOR benchmark networks. ImageNet ResNet 18 CONJUNCTION ImageNet AlexNet. ImageNet AlexNet CONJUNCTION ImageNet ResNet 18. single precision floating - point baseline FEATURE-OF networks. OtherScientificTerm are numerical precision, multiply - accumulate units, accumulation precision, ensemble of partial sums, accumulation, and computation hardware. Metric is quality of convergence. Generic are variance, and equations. Task is areaand power - optimal systems. ","This paper studies the precision requirements of partial sum accumulations. The authors propose a statistical approach to estimate the reduced accumulation precision for deep learning training. The analysis is applied to benchmark networks such as ImageNet ResNet 18 and ImageNet AlexNet and shows that the quality of convergence is improved when the numerical precision is reduced. The paper also shows that systems with high-precision accumulators tend to converge faster than systems with multiply-accommodate units.    The main contribution of the paper is the analysis of the variance of the convergence of a system with and without accumulation precision. The variance is defined as the difference between the number of units in an ensemble of partial sums and the total number of total units in the accumulation.  The paper shows that for areaand power-optimal systems, the variance can be reduced to 0.5-1.5.5 for a single precision floating-point baseline for networks with a fixed number of layers. This is the case even if the computation hardware is limited. The equations are also shown to hold for systems with multiple layers.","This paper studies the precision requirements of partial sum accumulations. The authors propose a statistical approach to estimate the reduced accumulation precision for deep learning training. The analysis is applied to benchmark networks such as ImageNet ResNet 18 and ImageNet AlexNet and shows that the quality of convergence is improved when the numerical precision is reduced. The paper also shows that systems with high-precision accumulators tend to converge faster than systems with multiply-accommodate units.    The main contribution of the paper is the analysis of the variance of the convergence of a system with and without accumulation precision. The variance is defined as the difference between the number of units in an ensemble of partial sums and the total number of total units in the accumulation.  The paper shows that for areaand power-optimal systems, the variance can be reduced to 0.5-1.5.5 for a single precision floating-point baseline for networks with a fixed number of layers. This is the case even if the computation hardware is limited. The equations are also shown to hold for systems with multiple layers."
6616,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"gradient flow CONJUNCTION gradient descent. gradient descent CONJUNCTION gradient flow. risk convergence CONJUNCTION asymptotic weight matrix alignment. asymptotic weight matrix alignment CONJUNCTION risk convergence. gradient descent USED-FOR deep linear networks. asymptotic weight matrix alignment HYPONYM-OF implicit regularization. gradient flow USED-FOR deep linear networks. asymptotic weight matrix alignment USED-FOR gradient flow. linearly separable data USED-FOR deep linear networks. gradient flow USED-FOR strictly decreasing loss functions. normalized ith weight matrix COMPARE rank-1 approximation. rank-1 approximation COMPARE normalized ith weight matrix. decreasing step sizes FEATURE-OF gradient descent. linear function COMPARE maximum margin solution. maximum margin solution COMPARE linear function. binary cross entropy FEATURE-OF logistic loss. weight matrices USED-FOR linear function. network USED-FOR linear function. Metric is risk. OtherScientificTerm are rank-1 matrices, and alignment phenomenon. ","This paper studies the implicit regularization of gradient flow and gradient descent in deep linear networks with linearly separable data. The authors prove the convergence of risk convergence and the asymptotic weight matrix alignment, which is a type of implicit regularisation. They show that the gradient flow converges to strictly decreasing loss functions with decreasing step sizes. They also show that gradient flow is a special case of gradient descent that converges strictly to a linear function.    The main contribution of this paper is that the authors prove that the linear function of the network converges in a similar way to the maximum margin solution when the weight matrices of the logistic loss have a binary cross entropy. In particular, the authors show that if the normalized ith weight matrix is close to the rank-1 approximation, then the gradient descent converges linearly to the optimal solution. The paper also shows that the risk is similar to that of the case where the weight matrix has rank 1 entries.  The authors also provide a theoretical analysis of the alignment phenomenon. ","This paper studies the implicit regularization of gradient flow and gradient descent in deep linear networks with linearly separable data. The authors prove the convergence of risk convergence and the asymptotic weight matrix alignment, which is a type of implicit regularisation. They show that the gradient flow converges to strictly decreasing loss functions with decreasing step sizes. They also show that gradient flow is a special case of gradient descent that converges strictly to a linear function.    The main contribution of this paper is that the authors prove that the linear function of the network converges in a similar way to the maximum margin solution when the weight matrices of the logistic loss have a binary cross entropy. In particular, the authors show that if the normalized ith weight matrix is close to the rank-1 approximation, then the gradient descent converges linearly to the optimal solution. The paper also shows that the risk is similar to that of the case where the weight matrix has rank 1 entries.  The authors also provide a theoretical analysis of the alignment phenomenon. "
6620,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,dialogue topic CONJUNCTION speaker sentiments. speaker sentiments CONJUNCTION dialogue topic. speaker identity CONJUNCTION dialogue topic. dialogue topic CONJUNCTION speaker identity. hredGAN architecture USED-FOR utterance attributes. speaker sentiments HYPONYM-OF utterance attributes. speaker identity HYPONYM-OF utterance attributes. dialogue topic HYPONYM-OF utterance attributes. persona - based HRED generator ( PHRED ) CONJUNCTION conditional discriminator. conditional discriminator CONJUNCTION persona - based HRED generator ( PHRED ). persona - based HRED generator ( PHRED ) PART-OF phredGAN. conditional discriminator PART-OF phredGAN. persona - based HRED generator ( PHRED ) PART-OF system. conditional discriminator PART-OF system. approaches USED-FOR conditional discriminator. phredGANd HYPONYM-OF dual discriminator system. dual discriminator system CONJUNCTION adversarial discriminator. adversarial discriminator CONJUNCTION dual discriminator system. phredGANa HYPONYM-OF approaches. phredGANd HYPONYM-OF approaches. Ubuntu Dialogue Corpus ( UDC ) CONJUNCTION TV series transcripts. TV series transcripts CONJUNCTION Ubuntu Dialogue Corpus ( UDC ). phredGAN COMPARE persona SeqSeq model. persona SeqSeq model COMPARE phredGAN. Big Bang Theory and Friends FEATURE-OF TV series transcripts. conversational datasets EVALUATE-FOR phredGAN. Ubuntu Dialogue Corpus ( UDC ) HYPONYM-OF conversational datasets. TV series transcripts HYPONYM-OF conversational datasets. quantitative measures CONJUNCTION crowd - sourced human evaluation. crowd - sourced human evaluation CONJUNCTION quantitative measures. datasets CONJUNCTION ones. ones CONJUNCTION datasets. attribute modalities FEATURE-OF customer - agent interactions. Ubuntu dataset FEATURE-OF customer - agent interactions. ones EVALUATE-FOR phredGAN. attribute modalities FEATURE-OF datasets. attribute modalities FEATURE-OF ones. datasets EVALUATE-FOR phredGAN. weak attribute modalities FEATURE-OF datasets. Big Bang Theory HYPONYM-OF weak attribute modalities. Task is multi - turn dialogue scenario. Method is attribute representation. ,"This paper proposes a multi-turn dialogue scenario where the utterance attributes are encoded in a hredGAN architecture. The system is composed of a persona-based HRED generator (PHRED) and a conditional discriminator (PhredGANd), which is a dual discriminator system that is trained to discriminate between utterance attribute representation of the speaker identity, dialogue topic, speaker sentiments, and speaker sentiments. Two approaches are proposed: phredGANa, which is an adversarial discriminator that discriminates whether an utterance is from the speaker or not, and phredGANd, which uses a dual adversinator system where the discriminator is trained on the attribute representation. Experiments are conducted on two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from Big Bang Theory and Friends, and on two customer-agent interactions on the Ubuntu dataset. The authors show that phedGAN outperforms the previous persona SeqSeq model on both datasets with different attribute modalities (e.g. Big Bang theory and Friends). The authors also provide quantitative measures and crowd-sourced human evaluation. ","This paper proposes a multi-turn dialogue scenario where the utterance attributes are encoded in a hredGAN architecture. The system is composed of a persona-based HRED generator (PHRED) and a conditional discriminator (PhredGANd), which is a dual discriminator system that is trained to discriminate between utterance attribute representation of the speaker identity, dialogue topic, speaker sentiments, and speaker sentiments. Two approaches are proposed: phredGANa, which is an adversarial discriminator that discriminates whether an utterance is from the speaker or not, and phredGANd, which uses a dual adversinator system where the discriminator is trained on the attribute representation. Experiments are conducted on two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from Big Bang Theory and Friends, and on two customer-agent interactions on the Ubuntu dataset. The authors show that phedGAN outperforms the previous persona SeqSeq model on both datasets with different attribute modalities (e.g. Big Bang theory and Friends). The authors also provide quantitative measures and crowd-sourced human evaluation. "
6624,SP:017b66d6262427cca551ef50006784498ffc741d,"language CONJUNCTION vision. vision CONJUNCTION language. vision CONJUNCTION action. action CONJUNCTION vision. virtual environment FEATURE-OF action. language PART-OF goal - driven collaborative task. vision PART-OF goal - driven collaborative task. action PART-OF goal - driven collaborative task. movable clip art objects PART-OF virtual world. virtual world USED-FOR game. natural language USED-FOR two - way communication. protocols CONJUNCTION metrics. metrics CONJUNCTION protocols. imitation learning CONJUNCTION goal - driven training. goal - driven training CONJUNCTION imitation learning. nearest - neighbor techniques CONJUNCTION neural network approaches. neural network approaches CONJUNCTION nearest - neighbor techniques. models USED-FOR task. nearest - neighbor techniques PART-OF models. neural network approaches PART-OF models. imitation learning USED-FOR neural network approaches. goal - driven training USED-FOR neural network approaches. game EVALUATE-FOR models. live human agents USED-FOR game. fully automated evaluation EVALUATE-FOR models. Task is Collaborative image - Drawing game. Method is CoDraw. OtherScientificTerm are clip art pieces, and crosstalk condition. Material is CoDraw dataset. Generic is testbed. ","This paper proposes a Collaborative image-Drawing game, called CoDraw. The game is played in a virtual world with movable clip art objects, and the goal-driven collaborative task is to communicate a goal to a partner using language, vision, and action in the virtual environment. The goal of the game is to draw a set of clip art pieces from the environment, and to use two-way communication in natural language. The paper introduces protocols, metrics, and protocols to evaluate the quality of the proposed protocols and protocols. It also introduces a crosstalk condition to ensure that the models trained for the task can be trained with nearest-neighbor techniques and neural network approaches based on imitation learning and goal-driven training. The models are evaluated on the game with live human agents, and a fully automated evaluation is also performed. The CoDraw dataset is also used as a testbed. ","This paper proposes a Collaborative image-Drawing game, called CoDraw. The game is played in a virtual world with movable clip art objects, and the goal-driven collaborative task is to communicate a goal to a partner using language, vision, and action in the virtual environment. The goal of the game is to draw a set of clip art pieces from the environment, and to use two-way communication in natural language. The paper introduces protocols, metrics, and protocols to evaluate the quality of the proposed protocols and protocols. It also introduces a crosstalk condition to ensure that the models trained for the task can be trained with nearest-neighbor techniques and neural network approaches based on imitation learning and goal-driven training. The models are evaluated on the game with live human agents, and a fully automated evaluation is also performed. The CoDraw dataset is also used as a testbed. "
6628,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,model spaces USED-FOR machine learning. neural networks USED-FOR potential functions. potential functions USED-FOR undirected models. neural networks USED-FOR Neural random fields ( NRFs ). approach USED-FOR NRFs. gradient information USED-FOR model sampling. inclusive - NRF approach USED-FOR continuous data. inclusive - divergence minimized auxiliary generator USED-FOR approach. inclusive - divergence minimized auxiliary generator USED-FOR NRFs. images HYPONYM-OF continuous data. unsupervised / supervised image generation CONJUNCTION semi - supervised classification. semi - supervised classification CONJUNCTION unsupervised / supervised image generation. inclusive - NRFs USED-FOR unsupervised / supervised image generation. inclusive - NRFs USED-FOR semi - supervised classification. random fields USED-FOR tasks. inclusive - NRFs USED-FOR random fields. CIFAR-10 EVALUATE-FOR sample generation quality. unsupervised and supervised settings EVALUATE-FOR inclusiveNRFs. sample generation quality EVALUATE-FOR inclusiveNRFs. CIFAR-10 EVALUATE-FOR inclusiveNRFs. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. Semi - supervised inclusive - NRFs COMPARE generative model based semi - supervised learning methods. generative model based semi - supervised learning methods COMPARE Semi - supervised inclusive - NRFs. classification EVALUATE-FOR generative model based semi - supervised learning methods. generation EVALUATE-FOR Semi - supervised inclusive - NRFs. classification EVALUATE-FOR Semi - supervised inclusive - NRFs. ,"Neural random fields (NRFs) are a family of neural networks that learn potential functions for training undirected models in different model spaces. The authors propose a novel approach to learning NRFs by using neural networks to learn the potential functions, and then use gradient information to guide model sampling. The proposed approach is based on an inclusive-divergence minimized auxiliary generator for NRFs, which allows for continuous data (e.g. images) to be generated from the proposed inclusive-NRF approach.  The authors show that the proposed approach improves sample generation quality for both unsupervised/supervised image generation and semi-supervised classification with the proposed inclusion of the inclusive-Divergence Minimization (IDM) auxiliary generator. Semi-supervision of the random fields for these two tasks is achieved by using the inclusion of a number of different random fields learned by the proposed additive-NRFs. The paper also shows that inclusiveNRFs improve sample generation in terms of quality on MNIST, SVHN, CIFAR-10, and Cifar-10 with the help of the IDM auxiliary generator, and that inclusive- NRFs improve the quality of the generated random fields in both the case of unsupervision and supervised settings, and can be used for both generation and classification as well.  Finally, the paper shows that Semi-subsupervised inclusive-NFs outperform the state-of-the-art generative model based semi-substituted learning methods.   ","Neural random fields (NRFs) are a family of neural networks that learn potential functions for training undirected models in different model spaces. The authors propose a novel approach to learning NRFs by using neural networks to learn the potential functions, and then use gradient information to guide model sampling. The proposed approach is based on an inclusive-divergence minimized auxiliary generator for NRFs, which allows for continuous data (e.g. images) to be generated from the proposed inclusive-NRF approach.  The authors show that the proposed approach improves sample generation quality for both unsupervised/supervised image generation and semi-supervised classification with the proposed inclusion of the inclusive-Divergence Minimization (IDM) auxiliary generator. Semi-supervision of the random fields for these two tasks is achieved by using the inclusion of a number of different random fields learned by the proposed additive-NRFs. The paper also shows that inclusiveNRFs improve sample generation in terms of quality on MNIST, SVHN, CIFAR-10, and Cifar-10 with the help of the IDM auxiliary generator, and that inclusive- NRFs improve the quality of the generated random fields in both the case of unsupervision and supervised settings, and can be used for both generation and classification as well.  Finally, the paper shows that Semi-subsupervised inclusive-NFs outperform the state-of-the-art generative model based semi-substituted learning methods.   "
6632,SP:0841febf2e95da495b41e12ded491ba5e9633538,"Deep learning models USED-FOR graphs. Deep learning models USED-FOR tasks. graph neural networks USED-FOR node classification. training time attacks FEATURE-OF graph neural networks. meta - gradients USED-FOR bilevel problem. meta - gradients USED-FOR training - time attacks. small graph perturbations USED-FOR graph convolutional networks. they COMPARE baseline. baseline COMPARE they. relational information USED-FOR baseline. algorithm USED-FOR perturbations. Metric is robustness. OtherScientificTerm are discrete graph structure, and graph. Task is unsupervised embeddings. Generic is attacks. Method is classifiers. ","Deep learning models for graphs have been shown to be robust to attacks on the discrete graph structure. Deep learning models can be applied to a variety of tasks, including node classification, graph classification, etc. This paper studies the problem of robustness to training time attacks on graph neural networks for node classification. The authors show that training-time attacks can be mitigated using meta-gradients to solve the bilevel problem. They show that small graph perturbations to graph convolutional networks can be used to attack the training time of a classifier. They also show that meta-gradient attacks can also be applied on unsupervised embeddings.   The authors also propose an algorithm to mitigate the effect of perturbation on the classifiers, and show that they outperform a baseline based on relational information. ","Deep learning models for graphs have been shown to be robust to attacks on the discrete graph structure. Deep learning models can be applied to a variety of tasks, including node classification, graph classification, etc. This paper studies the problem of robustness to training time attacks on graph neural networks for node classification. The authors show that training-time attacks can be mitigated using meta-gradients to solve the bilevel problem. They show that small graph perturbations to graph convolutional networks can be used to attack the training time of a classifier. They also show that meta-gradient attacks can also be applied on unsupervised embeddings.   The authors also propose an algorithm to mitigate the effect of perturbation on the classifiers, and show that they outperform a baseline based on relational information. "
6636,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"Wasserstein Autoencoder ( WAE ) HYPONYM-OF generative models. Sliced - Wasserstein Autoencoders ( SWAE ) CONJUNCTION WAE - MMD ( WAE. WAE - MMD ( WAE CONJUNCTION Sliced - Wasserstein Autoencoders ( SWAE ). Cramer - Wold AutoEncoder ( CWAE ) HYPONYM-OF generative model. maximum mean discrepancy based distance function USED-FOR WAE - MMD ( WAE. CramerWold kernel HYPONYM-OF characteristic kernel. characteristic kernel USED-FOR CWAE cost function. CWAE COMPARE WAE - MMD. WAE - MMD COMPARE CWAE. OtherScientificTerm are normal prior, and distance function. Method are optimization procedure, and SWAE. ","This paper proposes two generative models, Sliced-Wasserstein Autoencoders (SWAE) and Wasserstein autoencoder (WAE). The authors propose a new generative model, the Cramer-Wold AutoEncoder (CWAE), which is a generalization of the WAE-MMD. The main difference between SWAE and WAE is that SWAE uses a normal prior, while WAE uses the maximum mean discrepancy based distance function. The CWAE cost function is based on the characteristic kernel, the so-called CramerWold kernel, which is defined as the distance function between the output of the optimization procedure and the target distribution. Experiments show that the CWAE outperforms the state-of-the-art WAE - MMD.  ","This paper proposes two generative models, Sliced-Wasserstein Autoencoders (SWAE) and Wasserstein autoencoder (WAE). The authors propose a new generative model, the Cramer-Wold AutoEncoder (CWAE), which is a generalization of the WAE-MMD. The main difference between SWAE and WAE is that SWAE uses a normal prior, while WAE uses the maximum mean discrepancy based distance function. The CWAE cost function is based on the characteristic kernel, the so-called CramerWold kernel, which is defined as the distance function between the output of the optimization procedure and the target distribution. Experiments show that the CWAE outperforms the state-of-the-art WAE - MMD.  "
6640,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"It USED-FOR many - class ” problem. It USED-FOR class hierarchy. coarse - class label HYPONYM-OF class hierarchy. convolutional neural network ( CNN ) USED-FOR features. memory - augmented attention module CONJUNCTION multi - layer perceptron ( MLP ). multi - layer perceptron ( MLP ) CONJUNCTION memory - augmented attention module. multi - layer perceptron ( MLP ) USED-FOR MahiNet. memory - augmented attention module PART-OF MahiNet. convolutional neural network ( CNN ) USED-FOR MahiNet. attention module CONJUNCTION KNN classifier. KNN classifier CONJUNCTION attention module. linear classifier USED-FOR MLP. training strategies USED-FOR supervised learning. MahiNet USED-FOR supervised learning. training strategies USED-FOR MahiNet. mcfsOmniglot ” ( re - splitted Omniglot ) USED-FOR MCFS problem. benchmark datasets USED-FOR MCFS problem. mcfsOmniglot ” ( re - splitted Omniglot ) HYPONYM-OF benchmark datasets. MahiNet COMPARE models. models COMPARE MahiNet. MCFS classification tasks EVALUATE-FOR models. supervised learning and meta - learning scenarios EVALUATE-FOR models. supervised learning and meta - learning scenarios EVALUATE-FOR MahiNet. MCFS classification tasks EVALUATE-FOR MahiNet. Task are MCFS, MCFS learning, and few - shot ” problem. OtherScientificTerm is fine class. Material is ImageNet. ","This paper tackles the problem of few-shot learning in the multi-class setting (MCFS). The authors propose MahiNet, a new architecture that combines a memory-augmented attention module and a multi-layer perceptron (MLP) to tackle the “many-class” problem. It aims to learn a class hierarchy (e.g., a coarse-class label) that can be used to guide the learning of a fine class to a fine-class.    The key idea is to use a convolutional neural network (CNN) to encode features from the fine class and a KNN classifier to predict the features of the coarse class. The attention module is trained in a self-supervised manner, and the MLP is trained with a linear classifier.  The authors also propose two new training strategies for supervised learning that are shown to improve the quality of the learned features. The authors conduct experiments on three benchmark datasets for the MCFS problem, including “re-splitted Omniglot” (re-spun OmnIGlot) and “MCFSOmniglot,” which is a “few-shot” version of MCFS learning. They also show that the attention module can be combined with a standard CNN and KNN to achieve better performance.  On ImageNet, the authors show that Mahi net outperforms other models on a number of standard MCFS classification tasks. On supervised learning and meta-learning scenarios, Mahinet outperforms the other models.","This paper tackles the problem of few-shot learning in the multi-class setting (MCFS). The authors propose MahiNet, a new architecture that combines a memory-augmented attention module and a multi-layer perceptron (MLP) to tackle the “many-class” problem. It aims to learn a class hierarchy (e.g., a coarse-class label) that can be used to guide the learning of a fine class to a fine-class.    The key idea is to use a convolutional neural network (CNN) to encode features from the fine class and a KNN classifier to predict the features of the coarse class. The attention module is trained in a self-supervised manner, and the MLP is trained with a linear classifier.  The authors also propose two new training strategies for supervised learning that are shown to improve the quality of the learned features. The authors conduct experiments on three benchmark datasets for the MCFS problem, including “re-splitted Omniglot” (re-spun OmnIGlot) and “MCFSOmniglot,” which is a “few-shot” version of MCFS learning. They also show that the attention module can be combined with a standard CNN and KNN to achieve better performance.  On ImageNet, the authors show that Mahi net outperforms other models on a number of standard MCFS classification tasks. On supervised learning and meta-learning scenarios, Mahinet outperforms the other models."
6644,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,Recurrent neural networks ( RNNs ) USED-FOR natural language. neural speed reading HYPONYM-OF inference. Structural - Jump - LSTM HYPONYM-OF neural speed reading model. LSTM CONJUNCTION agents. agents CONJUNCTION LSTM. one CONJUNCTION one. one CONJUNCTION one. one HYPONYM-OF agents. one HYPONYM-OF agents. one PART-OF model. one PART-OF model. LSTM PART-OF model. agents PART-OF model. model COMPARE neural reading models. neural reading models COMPARE model. accuracy EVALUATE-FOR vanilla LSTM. Structural - Jump - LSTM COMPARE vanilla LSTM. vanilla LSTM COMPARE Structural - Jump - LSTM. model COMPARE Structural - Jump - LSTM. Structural - Jump - LSTM COMPARE model. neural reading models COMPARE Structural - Jump - LSTM. Structural - Jump - LSTM COMPARE neural reading models. accuracy EVALUATE-FOR Structural - Jump - LSTM. floating point operations ( FLOP ) reduction EVALUATE-FOR Structural - Jump - LSTM. Method is RNNs. Metric is inference time. OtherScientificTerm is end of text markers. ,"This paper proposes a neural speed reading model called Structural-Jump-LSTM, which is an extension of previous work on the use of recurrent neural networks (RNNs) for natural language. The authors argue that RNNs have the potential to speed up inference time by reducing the number of floating-point operations (FLOPs) required for inference (e.g., neural speedreading). The proposed model consists of an LSTM and two agents: one that is trained to jump to the end of text markers, and one that takes the entire sentence as input. Experiments show that the proposed model outperforms other neural reading models in terms of accuracy and floating point operations (flOP) reduction.","This paper proposes a neural speed reading model called Structural-Jump-LSTM, which is an extension of previous work on the use of recurrent neural networks (RNNs) for natural language. The authors argue that RNNs have the potential to speed up inference time by reducing the number of floating-point operations (FLOPs) required for inference (e.g., neural speedreading). The proposed model consists of an LSTM and two agents: one that is trained to jump to the end of text markers, and one that takes the entire sentence as input. Experiments show that the proposed model outperforms other neural reading models in terms of accuracy and floating point operations (flOP) reduction."
6648,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"Mahalanobis distance function USED-FOR convolutional features. Mahalanobis distance function USED-FOR tight clusters. Mahalanobis distance function USED-FOR nonlinear radial basis convolutional feature transformation. MNIST CONJUNCTION ISBI ISIC skin lesion. ISBI ISIC skin lesion CONJUNCTION MNIST. ISBI ISIC skin lesion CONJUNCTION NIH ChestX - ray14. NIH ChestX - ray14 CONJUNCTION ISBI ISIC skin lesion. NIH ChestX - ray14 HYPONYM-OF image classification and segmentation data - sets. MNIST HYPONYM-OF image classification and segmentation data - sets. ISBI ISIC skin lesion HYPONYM-OF image classification and segmentation data - sets. image classification and segmentation data - sets EVALUATE-FOR method. robustness EVALUATE-FOR method. it COMPARE non - gradient masking defense strategies. non - gradient masking defense strategies COMPARE it. method USED-FOR deep convolutional neural networks. deep convolutional neural networks USED-FOR adversarial perturbations. Method is deep convolutional models. Generic is them. OtherScientificTerm are carefully crafted adversarial perturbations, clusters, small adversarial perturbations, and decision boundary. Material is clean data. ","This paper proposes a new adversarial defense method for deep convolutional neural networks. The proposed method is based on the Mahalanobis distance function (MDSF) between the input and the output of the network. The authors show that the MDSF can be applied to any non-linear basis convolutions, and that it can be used as a defense against adversarial perturbations. The method is tested on MNIST, ISBI skin lesion, and NIH ChestX-ray14.","This paper proposes a new adversarial defense method for deep convolutional neural networks. The proposed method is based on the Mahalanobis distance function (MDSF) between the input and the output of the network. The authors show that the MDSF can be applied to any non-linear basis convolutions, and that it can be used as a defense against adversarial perturbations. The method is tested on MNIST, ISBI skin lesion, and NIH ChestX-ray14."
6652,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"exploratory behaviors USED-FOR local optima. exploratory behaviors USED-FOR Reinforcement learning agents. immediate dithering perturbation CONJUNCTION temporally consistent exploration. temporally consistent exploration CONJUNCTION immediate dithering perturbation. immediate dithering perturbation PART-OF behaviors. temporally consistent exploration PART-OF behaviors. sparse rewards CONJUNCTION long term information. long term information CONJUNCTION sparse rewards. stochastic policy model USED-FOR tasks. sparse rewards FEATURE-OF tasks. long term information FEATURE-OF tasks. global random variable USED-FOR conditional distribution. dropout USED-FOR reinforcement learning policies. inherent temporal consistency FEATURE-OF them. factors USED-FOR NADPEx policy. gradients ’ alignment HYPONYM-OF factors. naive exploration CONJUNCTION parameter noise. parameter noise CONJUNCTION naive exploration. NADPEx USED-FOR tasks. sparse reward USED-FOR NADPEx. naive exploration USED-FOR NADPEx. parameter noise USED-FOR NADPEx. sparse reward USED-FOR tasks. mujoco benchmark USED-FOR continuous control. mujoco benchmark EVALUATE-FOR It. Method are on - policy temporally consistent exploration strategy, and deep reinforcement learning agents. OtherScientificTerm are reward signals, and policy space. ","This paper proposes a novel on-policy temporally consistent exploration strategy for deep reinforcement learning agents. Reinforcement learning agents have been shown to learn exploratory behaviors that lead to local optima. These behaviors include an immediate dithering perturbation, and a more long-term temporally inconsistent exploration. The authors propose a stochastic policy model that can be applied to a wide range of tasks with sparse rewards and long term information. In particular, the authors propose to learn a conditional distribution over a global random variable, where the reward signals are sampled from the policy space. This conditional distribution is modeled as a dropout, and reinforcement learning policies are trained with dropout. They show that the proposed NADPEx policy can be trained with two factors: (1) naive exploration, and (2) the inherent temporal consistency of the reward. They also show that this policy space is invariant to two factors, namely the gradients’ alignment and the number of samples.   The authors show that by incorporating these two factors into the policy, NADPE can be used to learn tasks with a sparse reward and long-range information. It is tested on the mujoco benchmark for continuous control. ","This paper proposes a novel on-policy temporally consistent exploration strategy for deep reinforcement learning agents. Reinforcement learning agents have been shown to learn exploratory behaviors that lead to local optima. These behaviors include an immediate dithering perturbation, and a more long-term temporally inconsistent exploration. The authors propose a stochastic policy model that can be applied to a wide range of tasks with sparse rewards and long term information. In particular, the authors propose to learn a conditional distribution over a global random variable, where the reward signals are sampled from the policy space. This conditional distribution is modeled as a dropout, and reinforcement learning policies are trained with dropout. They show that the proposed NADPEx policy can be trained with two factors: (1) naive exploration, and (2) the inherent temporal consistency of the reward. They also show that this policy space is invariant to two factors, namely the gradients’ alignment and the number of samples.   The authors show that by incorporating these two factors into the policy, NADPE can be used to learn tasks with a sparse reward and long-range information. It is tested on the mujoco benchmark for continuous control. "
6656,SP:304930c105cf036ab48e9653926a5f61879dfea6,"gradient - based metric USED-FOR network. nonlinearity coefficient ( NLC ) HYPONYM-OF metric. NLC USED-FOR test error. NLC USED-FOR architecture search and design. Method are neural architectures, expert hand - tuning, and fully - connected feedforward networks. Task is architecture design. OtherScientificTerm is exploding or vanishing gradients. Generic are guideline, and it. ","This paper proposes a new gradient-based metric for evaluating the performance of a network, called the nonlinearity coefficient (NLC), which is a new metric for neural architectures. The authors show that the NLC can be used to measure the test error of a neural architecture, which is then used to guide the architecture design. The NLC is a guideline that can be applied to any architecture search and design, and is shown to be more robust to exploding or vanishing gradients than expert hand-tuning. In particular, it is shown that for fully-connected feedforward networks, NLC does not suffer from exploding gradients, and that it is more robust than expert tuning. ","This paper proposes a new gradient-based metric for evaluating the performance of a network, called the nonlinearity coefficient (NLC), which is a new metric for neural architectures. The authors show that the NLC can be used to measure the test error of a neural architecture, which is then used to guide the architecture design. The NLC is a guideline that can be applied to any architecture search and design, and is shown to be more robust to exploding or vanishing gradients than expert hand-tuning. In particular, it is shown that for fully-connected feedforward networks, NLC does not suffer from exploding gradients, and that it is more robust than expert tuning. "
6660,SP:17d8dc884e15131636a8c2490085ce42c05433c1,"bias amplification FEATURE-OF classifiers. inductive bias in gradient descent methods USED-FOR bias amplification. feature - wise bias amplification HYPONYM-OF bias. features FEATURE-OF model. targeted feature selection USED-FOR feature - wise bias amplification. feature selection algorithms USED-FOR bias amplification. they USED-FOR convolutional neural networks. feature selection algorithms USED-FOR linear models. bias amplification PART-OF linear models. algorithms USED-FOR reduced bias. synthetic and real data EVALUATE-FOR algorithms. Method is machine learning model. OtherScientificTerm are moderately - predictive “ weak ” features, and predictive bias. Material is insufficient training data. Metric is accuracy. ","This paper studies the problem of bias amplification in classifiers. The authors show that the inductive bias in gradient descent methods is responsible for bias amplification. The bias is called feature-wise bias amplification, which is the bias that arises when a machine learning model is trained on moderately-predictive “weak” features. The paper shows that the bias amplification happens when the features of the model are biased (i.e., when the model is biased) due to insufficient training data. To mitigate this bias, the paper proposes targeted feature selection, which aims to select features that are moderately predictive “strongly” predictive. The proposed feature selection algorithms are applied to linear models, and they are shown to be effective for training convolutional neural networks.  The paper also shows that linear models with bias amplification can be reduced by the feature selection methods. The algorithms are tested on both synthetic and real data, and the results show that algorithms with reduced bias are more effective at reducing the predictive bias. ","This paper studies the problem of bias amplification in classifiers. The authors show that the inductive bias in gradient descent methods is responsible for bias amplification. The bias is called feature-wise bias amplification, which is the bias that arises when a machine learning model is trained on moderately-predictive “weak” features. The paper shows that the bias amplification happens when the features of the model are biased (i.e., when the model is biased) due to insufficient training data. To mitigate this bias, the paper proposes targeted feature selection, which aims to select features that are moderately predictive “strongly” predictive. The proposed feature selection algorithms are applied to linear models, and they are shown to be effective for training convolutional neural networks.  The paper also shows that linear models with bias amplification can be reduced by the feature selection methods. The algorithms are tested on both synthetic and real data, and the results show that algorithms with reduced bias are more effective at reducing the predictive bias. "
6664,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"over - parametrization USED-FOR generalization. over - parametrization USED-FOR neural networks. neural networks USED-FOR generalization. normalized margin CONJUNCTION generalization error bounds. generalization error bounds CONJUNCTION normalized margin. global minimizer FEATURE-OF weakly - regularized cross - entropy loss. generalization error bounds FEATURE-OF deep networks. maximum normalized margin FEATURE-OF global minimizer. two - layer networks FEATURE-OF infinite - width neural network. generalization guarantees EVALUATE-FOR infinite - width neural network. neural net margin COMPARE kernel methods. kernel methods COMPARE neural net margin. kernel methods HYPONYM-OF infinite feature methods. generalization guarantees EVALUATE-FOR kernel methods. infinite - neuron viewpoint USED-FOR analyzing optimization. perturbed gradient flow USED-FOR global optimizer. perturbed gradient flow USED-FOR infinite - size networks. polynomial time FEATURE-OF global optimizer. Method are margin - based perspective, and multi - layer feedforward relu networks. Material is natural instances. ","This paper studies the generalization performance of neural networks with over-parametrization in terms of normalized margin and generalization error bounds. In particular, the authors consider a margin-based perspective, where the global minimizer of the weakly-regularized cross-entropy loss is the maximum normalized margin of the network. The authors show that for infinite-width neural network with two-layer networks, the resulting generalization guarantees for an infinite-wide neural network are asymptotically tight as for multi-layer feedforward relu networks. They also show that the neural net margin is tighter than standard kernel methods, which are infinite feature methods.    The authors also provide generalisation error bounds for deep networks.  The main contribution of this paper is to extend the infinite-neuron viewpoint for analyzing optimization to infinite-size networks from the perspective of perturbed gradient flow, which allows for a global optimizer to be found in polynomial time. The paper also shows that for natural instances, the neural network margin is tight. ","This paper studies the generalization performance of neural networks with over-parametrization in terms of normalized margin and generalization error bounds. In particular, the authors consider a margin-based perspective, where the global minimizer of the weakly-regularized cross-entropy loss is the maximum normalized margin of the network. The authors show that for infinite-width neural network with two-layer networks, the resulting generalization guarantees for an infinite-wide neural network are asymptotically tight as for multi-layer feedforward relu networks. They also show that the neural net margin is tighter than standard kernel methods, which are infinite feature methods.    The authors also provide generalisation error bounds for deep networks.  The main contribution of this paper is to extend the infinite-neuron viewpoint for analyzing optimization to infinite-size networks from the perspective of perturbed gradient flow, which allows for a global optimizer to be found in polynomial time. The paper also shows that for natural instances, the neural network margin is tight. "
6668,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"image captions HYPONYM-OF natural language descriptions. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. object pathway USED-FOR generator. object pathway USED-FOR discriminator. semantic layout USED-FOR approach. image background CONJUNCTION image layout. image layout CONJUNCTION image background. global pathway USED-FOR image background. global pathway USED-FOR image layout. Multi - MNIST CONJUNCTION CLEVR. CLEVR CONJUNCTION Multi - MNIST. CLEVR CONJUNCTION MSCOCO data set. MSCOCO data set CONJUNCTION CLEVR. Multi - MNIST CONJUNCTION MSCOCO data set. MSCOCO data set CONJUNCTION Multi - MNIST. global image characteristics CONJUNCTION image background. image background CONJUNCTION global image characteristics. object pathway USED-FOR features. global pathway USED-FOR image background. global pathway USED-FOR global image characteristics. object pathway COMPARE global pathway. global pathway COMPARE object pathway. Method is Generative Adversarial Networks ( GANs ). OtherScientificTerm are bounding boxes, and complex scenes. ","This paper proposes a new approach to generate adversarial examples using Generative Adversarial Networks (GANs) based on natural language descriptions (e.g. image captions). The approach is based on the semantic layout of a scene, where bounding boxes are used to represent objects in the scene. The generator and discriminator are trained on the object pathway. The object pathway is used to train the generator and the discriminator. The global pathway is then used to learn the image background, image layout, and image characteristics. Experiments are conducted on Multi-MNIST, CLEVR, and MSCOCO data set. The results show that the global pathway learns global image characteristics, image background and image layout. In addition, the object path is used for learning features that are more complex than the global path. The authors also show that object pathway can be used to generate more complex scenes. ","This paper proposes a new approach to generate adversarial examples using Generative Adversarial Networks (GANs) based on natural language descriptions (e.g. image captions). The approach is based on the semantic layout of a scene, where bounding boxes are used to represent objects in the scene. The generator and discriminator are trained on the object pathway. The object pathway is used to train the generator and the discriminator. The global pathway is then used to learn the image background, image layout, and image characteristics. Experiments are conducted on Multi-MNIST, CLEVR, and MSCOCO data set. The results show that the global pathway learns global image characteristics, image background and image layout. In addition, the object path is used for learning features that are more complex than the global path. The authors also show that object pathway can be used to generate more complex scenes. "
6672,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,models USED-FOR policy improvement. learning models USED-FOR global model methods. local model methods USED-FOR system dynamics. representations USED-FOR simple dynamics. local models USED-FOR policy learning in complex systems. real Sawyer robotic arm USED-FOR manipulation task. manipulation task HYPONYM-OF robotics tasks. robotics tasks EVALUATE-FOR approach. camera images USED-FOR manipulation task. OtherScientificTerm is local improvements. ,"This paper proposes a method for learning a local model of the dynamics of a system. The idea is to learn a local representation of the system dynamics, and then use this representation to improve the performance of the global model. The method is evaluated on a number of robotic manipulation tasks. The results show that the proposed method is able to learn the local dynamics of complex systems.","This paper proposes a method for learning a local model of the dynamics of a system. The idea is to learn a local representation of the system dynamics, and then use this representation to improve the performance of the global model. The method is evaluated on a number of robotic manipulation tasks. The results show that the proposed method is able to learn the local dynamics of complex systems."
6676,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"reinforcement learning algorithms USED-FOR real experience. models USED-FOR Learning policies. POMDPs FEATURE-OF learning policies. off - policy experience USED-FOR learning policies. structural causal models USED-FOR counterfactual evaluation of arbitrary policies. It USED-FOR counterfactual evaluation of arbitrary policies. structural causal models USED-FOR It. CF - GPS COMPARE vanilla model - based RL algorithms. vanilla model - based RL algorithms COMPARE CF - GPS. logged data USED-FOR CF - GPS. off - policy algorithms COMPARE CF - GPS. CF - GPS COMPARE off - policy algorithms. model USED-FOR CF - GPS. experience data USED-FOR algorithm. Importance Sampling USED-FOR off - policy algorithms. CF - GPS USED-FOR Guided Policy Search. counterfactual methods USED-FOR reparameterization - based algorithms. Stochastic Value Gradient HYPONYM-OF reparameterization - based algorithms. Task are simulating plausible experience de novo, modelbased policy evaluation and search, and de novo synthesis of data. OtherScientificTerm are logged, real experience, counterfactual actions, and off - policy episodes. ","This paper studies the problem of simulating plausible experience de novo. Learning policies from models trained on logged, real experience are used to train reinforcement learning algorithms for real experience. It uses structural causal models to perform counterfactual evaluation of arbitrary policies from off-policy experience in order to learn learning policies in POMDPs. The paper shows that CF-GPS outperforms vanilla model-based RL algorithms on logged data, and outperforms existing off-policies on real-world, logged, and modelbased policy evaluation and search. The algorithm is trained on experience data collected from a model trained on the offline dataset. The authors also show that the algorithm is able to learn from experience data that is different from the offline data.   The paper also proposes a new algorithm called Guided Policy Search (GPS) based on the idea of Importance Sampling, where the algorithm learns a model for each episode of the offline setting, and then uses the model to learn a policy that maximizes the expected return of the algorithm. In order to do this, the authors propose to use counterfactually actions that are not observed during the offline training phase, and to use the model for the counterfactuality of the off-execution of the current policy. They also propose a reparameterization-based algorithms based on counterfactulary methods (e.g., Stochastic Value Gradient). The paper concludes that the proposed algorithm is more robust to the de novelling of the model, and that it can be used in combination with existing methods for model-free and model-agnostic RL algorithms.  The main contribution of the paper is that the paper proposes a method for de-novoicing the model based on experience from offline data, which can be applied to a diverse set of offline and online settings. The idea is to use a model-independent algorithm to learn the model from offline experience, and use the learned model for model based RL algorithms for online RL. ","This paper studies the problem of simulating plausible experience de novo. Learning policies from models trained on logged, real experience are used to train reinforcement learning algorithms for real experience. It uses structural causal models to perform counterfactual evaluation of arbitrary policies from off-policy experience in order to learn learning policies in POMDPs. The paper shows that CF-GPS outperforms vanilla model-based RL algorithms on logged data, and outperforms existing off-policies on real-world, logged, and modelbased policy evaluation and search. The algorithm is trained on experience data collected from a model trained on the offline dataset. The authors also show that the algorithm is able to learn from experience data that is different from the offline data.   The paper also proposes a new algorithm called Guided Policy Search (GPS) based on the idea of Importance Sampling, where the algorithm learns a model for each episode of the offline setting, and then uses the model to learn a policy that maximizes the expected return of the algorithm. In order to do this, the authors propose to use counterfactually actions that are not observed during the offline training phase, and to use the model for the counterfactuality of the off-execution of the current policy. They also propose a reparameterization-based algorithms based on counterfactulary methods (e.g., Stochastic Value Gradient). The paper concludes that the proposed algorithm is more robust to the de novelling of the model, and that it can be used in combination with existing methods for model-free and model-agnostic RL algorithms.  The main contribution of the paper is that the paper proposes a method for de-novoicing the model based on experience from offline data, which can be applied to a diverse set of offline and online settings. The idea is to use a model-independent algorithm to learn the model from offline experience, and use the learned model for model based RL algorithms for online RL. "
6680,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"flat local minima of loss surface USED-FOR generalization. parameter space FEATURE-OF flat local minima of loss surface. parameter space FEATURE-OF loss surface. generalization EVALUATE-FOR loss surface. parameter space CONJUNCTION input space. input space CONJUNCTION parameter space. input space FEATURE-OF decision surfaces. parameter space FEATURE-OF decision surfaces. input space FEATURE-OF decision surface. adversarial robustness indicator USED-FOR neural network ’s intrinsic robustness property. method USED-FOR network ’s intrinsic adversarial robustness. Method are neural network generalization, robust training method, and adversarial training. OtherScientificTerm are adversarial settings, and adversarial attacks. Metric is adversarial robustness. ","This paper studies the problem of neural network generalization in adversarial settings. The authors show that flat local minima of loss surface in the parameter space and in the input space are necessary for good generalization. They also show that the generalization performance of a robust training method can be improved if the loss surface of the network is flat.   The authors also propose a method to measure the network’s intrinsic adversarial robustness to adversarial attacks, which is a measure of how well the decision surfaces of decision surfaces in both parameter space as well as input space lie flat. They show that adversarial training with this method can improve the robustness of a neural network against adversarial examples. They further show that a robustness indicator can be used to quantify the neural network's intrinsic robustness property. ","This paper studies the problem of neural network generalization in adversarial settings. The authors show that flat local minima of loss surface in the parameter space and in the input space are necessary for good generalization. They also show that the generalization performance of a robust training method can be improved if the loss surface of the network is flat.   The authors also propose a method to measure the network’s intrinsic adversarial robustness to adversarial attacks, which is a measure of how well the decision surfaces of decision surfaces in both parameter space as well as input space lie flat. They show that adversarial training with this method can improve the robustness of a neural network against adversarial examples. They further show that a robustness indicator can be used to quantify the neural network's intrinsic robustness property. "
6684,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"autonomous drones CONJUNCTION wearable devices. wearable devices CONJUNCTION autonomous drones. energy consumption PART-OF DNN training. weighted sparse projection CONJUNCTION input masking. input masking CONJUNCTION weighted sparse projection. end - to - end DNN training framework USED-FOR quantitative energy consumption guarantees. weighted sparse projection USED-FOR end - to - end DNN training framework. input masking USED-FOR end - to - end DNN training framework. weighted sparse projection USED-FOR quantitative energy consumption guarantees. input masking USED-FOR quantitative energy consumption guarantees. optimization problem USED-FOR DNN training. DNN training process USED-FOR constrained optimization. quantitative DNN energy estimation PART-OF DNN training process. approximate algorithm USED-FOR optimization problem. framework USED-FOR DNNs. prior energy - saving methods COMPARE framework. framework COMPARE prior energy - saving methods. Method is Deep Neural Networks ( DNNs ). OtherScientificTerm are energy budget, optimization constraint, and energy budgets. ","This paper proposes an end-to-end DNN training framework to reduce the energy consumption of Deep Neural Networks (DNNs) in the context of autonomous drones and wearable devices. The authors propose a weighted sparse projection and input masking to achieve quantitative energy consumption guarantees based on weighted sparse projected and input masks. The paper also proposes an optimization problem that can be used to optimize the energy budget of a DNN during training. The optimization problem is formulated as a constrained optimization problem, and the authors propose an approximate algorithm to solve this optimization problem. They show that the optimization constraint can be applied to any optimization constraint, and that the proposed framework can save energy budgets for DNNs. They also show that their framework is more efficient than prior energy-saving methods.   The authors also propose to incorporate the quantitative DNN energy estimation into the training process, which can be incorporated into the end-of-the-art of the constrained optimization.","This paper proposes an end-to-end DNN training framework to reduce the energy consumption of Deep Neural Networks (DNNs) in the context of autonomous drones and wearable devices. The authors propose a weighted sparse projection and input masking to achieve quantitative energy consumption guarantees based on weighted sparse projected and input masks. The paper also proposes an optimization problem that can be used to optimize the energy budget of a DNN during training. The optimization problem is formulated as a constrained optimization problem, and the authors propose an approximate algorithm to solve this optimization problem. They show that the optimization constraint can be applied to any optimization constraint, and that the proposed framework can save energy budgets for DNNs. They also show that their framework is more efficient than prior energy-saving methods.   The authors also propose to incorporate the quantitative DNN energy estimation into the training process, which can be incorporated into the end-of-the-art of the constrained optimization."
6688,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"Addressing uncertainty USED-FOR autonomous systems. latent model parameters FEATURE-OF posterior distribution. belief distribution USED-FOR expected long - term reward. universal policy USED-FOR Bayesian value function. policy optimization algorithms USED-FOR universal policy. universal policy USED-FOR exploration - exploitation trade - off. policy optimization algorithms USED-FOR Bayesian Policy Optimization. policy optimization algorithms USED-FOR algorithm. policy network architecture USED-FOR belief distribution. observable state FEATURE-OF belief distribution. method COMPARE Partially Observable Markov Decision Process solvers. Partially Observable Markov Decision Process solvers COMPARE method. method COMPARE algorithms. algorithms COMPARE method. algorithms USED-FOR model uncertainty. algorithms COMPARE Partially Observable Markov Decision Process solvers. Partially Observable Markov Decision Process solvers COMPARE algorithms. method USED-FOR model uncertainty. OtherScientificTerm are continuous latent parameter space, and belief distributions. ","This paper considers the problem of addressing uncertainty in autonomous systems. In particular, the authors consider the setting where there is a continuous latent latent parameter space, and the posterior distribution over the latent model parameters is unknown. The authors propose an algorithm called Bayesian Policy Optimization, which uses policy optimization algorithms to learn a universal policy that maximizes the expected long-term reward from a belief distribution over a set of observations. This belief distribution is learned using a policy network architecture, and is then used to optimize a Bayesian value function that maximises the expected return of the universal policy.  The authors show that the exploration-exploitation trade-off between the belief distribution of the agent over an observable state and the one over the belief distributions over the entire continuous latent space can be solved using the proposed algorithm. The proposed method is shown to outperform existing algorithms in terms of model uncertainty, and outperforms existing Partially Observable Markov Decision Process solvers. ","This paper considers the problem of addressing uncertainty in autonomous systems. In particular, the authors consider the setting where there is a continuous latent latent parameter space, and the posterior distribution over the latent model parameters is unknown. The authors propose an algorithm called Bayesian Policy Optimization, which uses policy optimization algorithms to learn a universal policy that maximizes the expected long-term reward from a belief distribution over a set of observations. This belief distribution is learned using a policy network architecture, and is then used to optimize a Bayesian value function that maximises the expected return of the universal policy.  The authors show that the exploration-exploitation trade-off between the belief distribution of the agent over an observable state and the one over the belief distributions over the entire continuous latent space can be solved using the proposed algorithm. The proposed method is shown to outperform existing algorithms in terms of model uncertainty, and outperforms existing Partially Observable Markov Decision Process solvers. "
6692,SP:3823faee83bc07a989934af5495dafd003c27921,"unified framework USED-FOR unsupervised representations of entities. optimal transport USED-FOR representations. method USED-FOR uncertainty. sentence similarity CONJUNCTION word entailment detection. word entailment detection CONJUNCTION sentence similarity. unsupervised representations USED-FOR text. tasks EVALUATE-FOR it. word entailment detection HYPONYM-OF tasks. sentence similarity HYPONYM-OF tasks. approach USED-FOR unsupervised or supervised problem. co - occurrence structure FEATURE-OF unsupervised or supervised problem. sequence data HYPONYM-OF co - occurrence structure. Wasserstein distances CONJUNCTION Wasserstein barycenters. Wasserstein barycenters CONJUNCTION Wasserstein distances. Wasserstein distances USED-FOR framework. OtherScientificTerm are histogram ( or distribution ), entities, distributions, and optimal transport map. Method is rich and powerful feature representations. ","This paper proposes a unified framework for learning unsupervised representations of entities from a histogram (or distribution) of entities. The authors propose to use optimal transport to learn such representations, and show that it can be applied to two tasks: sentence similarity and word entailment detection. They also show that the proposed method is able to capture uncertainty in the representation space, and that it is more robust to the co-occurrence of entities in the dataset.    The authors show that their approach can be used to solve any standard unsuper supervised or supervised problem with a co-incidence structure (e.g., sequence data). They also demonstrate that their framework can be combined with Wasserstein distances and WASSERSTEIN barycenters. They show that this framework can learn rich and powerful feature representations that are robust to changes in the distribution of the entities in a dataset. They further show that they can learn unsupervisory representations for text that are more robust than those learned from the original text.  Finally, they demonstrate that they are able to learn distributions that are invariant to perturbations in the data, and they show that these distributions can be mapped to the optimal transport map. ","This paper proposes a unified framework for learning unsupervised representations of entities from a histogram (or distribution) of entities. The authors propose to use optimal transport to learn such representations, and show that it can be applied to two tasks: sentence similarity and word entailment detection. They also show that the proposed method is able to capture uncertainty in the representation space, and that it is more robust to the co-occurrence of entities in the dataset.    The authors show that their approach can be used to solve any standard unsuper supervised or supervised problem with a co-incidence structure (e.g., sequence data). They also demonstrate that their framework can be combined with Wasserstein distances and WASSERSTEIN barycenters. They show that this framework can learn rich and powerful feature representations that are robust to changes in the distribution of the entities in a dataset. They further show that they can learn unsupervisory representations for text that are more robust than those learned from the original text.  Finally, they demonstrate that they are able to learn distributions that are invariant to perturbations in the data, and they show that these distributions can be mapped to the optimal transport map. "
6696,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,model - based reinforcement learning methods COMPARE model - free approaches. model - free approaches COMPARE model - based reinforcement learning methods. model - based reinforcement learning methods USED-FOR complex simulated environments. dynamics model CONJUNCTION planner. planner CONJUNCTION dynamics model. recursions USED-FOR long - range planning. recursions USED-FOR state estimates. model accuracy CONJUNCTION performance. performance CONJUNCTION model accuracy. task reward EVALUATE-FOR model - free approaches. Material is MuJoCo environments. OtherScientificTerm is long planning horizons. ," in MuJoCo environments, the authors show that model-based reinforcement learning methods outperform model-free approaches in complex simulated environments with long planning horizons. This is achieved by learning a dynamics model and a planner, and then using recursions for long-range planning. The authors also show that recursions can be used to improve the state estimates, and that model accuracy and performance can be improved compared to the state of the art in terms of task reward."," in MuJoCo environments, the authors show that model-based reinforcement learning methods outperform model-free approaches in complex simulated environments with long planning horizons. This is achieved by learning a dynamics model and a planner, and then using recursions for long-range planning. The authors also show that recursions can be used to improve the state estimates, and that model accuracy and performance can be improved compared to the state of the art in terms of task reward."
6700,SP:da14205470819495a3aad69d64de4033749d4d3e,2or 3 - bit precision HYPONYM-OF ultra - low precision. precision highway USED-FOR ultralow - precision computation. precision highway USED-FOR convolutional and recurrent neural networks. precision highway USED-FOR accumulated quantization error. accumulated quantization error FEATURE-OF convolutional and recurrent neural networks. hardware accelerator EVALUATE-FOR overhead. method COMPARE quantization methods. quantization methods COMPARE method. 3 - bit weight / activation quantization CONJUNCTION 2 - bit quantization. 2 - bit quantization CONJUNCTION 3 - bit weight / activation quantization. top-1 accuracy loss EVALUATE-FOR ResNet-50. 3 - bit weight / activation quantization EVALUATE-FOR method. ResNet-50 EVALUATE-FOR 2 - bit quantization. top-1 accuracy loss EVALUATE-FOR 2 - bit quantization. accuracy loss EVALUATE-FOR method. top-1 accuracy loss EVALUATE-FOR method. method COMPARE method. method COMPARE method. LSTM USED-FOR language modeling. 2 - bit quantization FEATURE-OF LSTM. 2 - bit quantization USED-FOR method. 2 - bit quantization FEATURE-OF method. Method is Neural network quantization. ,"This paper proposes a new ultra-low precision, i.e., 2or 3-bit precision. The authors propose a precision highway for ultralow-precision computation to reduce the accumulated quantization error of convolutional and recurrent neural networks. Neural network quantization has been a hot topic in recent years, and this paper proposes to use a hardware accelerator to reduce this overhead. Experiments show that the proposed method outperforms previous quantization methods on ResNet-50 with a top-1 accuracy loss of 2-bit weight/activation quantization and 2- bit quantization. The proposed method is also shown to outperform the previous method with 2-bits quantization for language modeling with an LSTM in language modeling. ","This paper proposes a new ultra-low precision, i.e., 2or 3-bit precision. The authors propose a precision highway for ultralow-precision computation to reduce the accumulated quantization error of convolutional and recurrent neural networks. Neural network quantization has been a hot topic in recent years, and this paper proposes to use a hardware accelerator to reduce this overhead. Experiments show that the proposed method outperforms previous quantization methods on ResNet-50 with a top-1 accuracy loss of 2-bit weight/activation quantization and 2- bit quantization. The proposed method is also shown to outperform the previous method with 2-bits quantization for language modeling with an LSTM in language modeling. "
6704,SP:0355b54430b39b52df94014d78289dd6e1e81795,"method USED-FOR image restoration problems. deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION deblurring. denoising CONJUNCTION deblurring. deblurring CONJUNCTION denoising. denoising HYPONYM-OF image restoration problems. super - resolution HYPONYM-OF image restoration problems. deblurring HYPONYM-OF image restoration problems. constrained optimization problem USED-FOR problem. Generative Adversarial Network ( GAN ) USED-FOR density estimation model. OtherScientificTerm are posteriori probability of latent variables, latent variables, and degraded image. Material is MNIST dataset. ","This paper proposes a new method for image restoration problems such as denoising, deblurring, and super-resolution. The problem is formulated as a constrained optimization problem, where the posteriori probability of latent variables is constrained to be a function of the input image. The authors propose a density estimation model based on a Generative Adversarial Network (GAN) to estimate the density of the latent variables, which is then used to train a density estimator for the degraded image. Experiments are conducted on the MNIST dataset. ","This paper proposes a new method for image restoration problems such as denoising, deblurring, and super-resolution. The problem is formulated as a constrained optimization problem, where the posteriori probability of latent variables is constrained to be a function of the input image. The authors propose a density estimation model based on a Generative Adversarial Network (GAN) to estimate the density of the latent variables, which is then used to train a density estimator for the degraded image. Experiments are conducted on the MNIST dataset. "
6708,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"softmax outputs CONJUNCTION feature responses. feature responses CONJUNCTION softmax outputs. large "" teacher "" network CONJUNCTION compact "" student "" network. compact "" student "" network CONJUNCTION large "" teacher "" network. full training data USED-FOR knowledge distillation methods. method USED-FOR knowledge distillation. conv - layer PART-OF student - net. layer PART-OF conv - layer. layer PART-OF conv - layer. computation cost FEATURE-OF conv - layer. teacher - net CONJUNCTION student - net constructing. student - net constructing CONJUNCTION teacher - net. OtherScientificTerm are human cognition, feature map sizes, and block - level outputs. ","This paper proposes a novel method for knowledge distillation based on the observation that human cognition is highly dependent on softmax outputs and feature responses. The authors propose to use a large ""teacher"" network and a compact ""student"" network with different feature map sizes to distill the knowledge from a large teacher-net to a smaller student-net. They show that the knowledge of a student network can be distilled from a teacher network by learning a conv-layer that consists of a layer with a fixed number of blocks and block-level outputs. They also show that this conv - layer has a lower computation cost compared to the full training data used in existing knowledge distillations methods.","This paper proposes a novel method for knowledge distillation based on the observation that human cognition is highly dependent on softmax outputs and feature responses. The authors propose to use a large ""teacher"" network and a compact ""student"" network with different feature map sizes to distill the knowledge from a large teacher-net to a smaller student-net. They show that the knowledge of a student network can be distilled from a teacher network by learning a conv-layer that consists of a layer with a fixed number of blocks and block-level outputs. They also show that this conv - layer has a lower computation cost compared to the full training data used in existing knowledge distillations methods."
6712,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,task relatedness USED-FOR transferability. transferred representations USED-FOR classification problems. H - score HYPONYM-OF evaluation function. evaluation function EVALUATE-FOR transferred representations. asymptotic error probability FEATURE-OF decision function. information theoretic approach USED-FOR H - score. asymptotic error probability FEATURE-OF H - score. transferred feature USED-FOR asymptotic error probability. transferred feature USED-FOR H - score. source tasks USED-FOR task transfer learning problems. transferability USED-FOR source tasks. it USED-FOR inference problems. recognition tasks USED-FOR 3D indoor - scene understanding. classification HYPONYM-OF inference problems. recognition tasks HYPONYM-OF inference problems. Task is task transfer learning. Metric is task transferability. OtherScientificTerm is representations. Generic is metric. Method is transfer learning policies. Material is synthetic and real image data. ,"This paper studies the problem of task transfer learning, where the goal is to learn representations that are transferable across different tasks. The authors propose to measure the transferability based on the task relatedness between the source task and the target task. They propose a new evaluation function called H-score, which is based on an information theoretic approach to measure transferability between source and target tasks. They show that transferring representations from source tasks to target tasks improves the asymptotic error probability of the decision function of the transferred representations for classification problems. They also show that the H-scores obtained from the transferred feature can be used as a measure of transferability for other source tasks, and that it can be applied to other inference problems such as classification and recognition tasks for 3D indoor-scene understanding. The paper also shows that the proposed metric is robust to the choice of transfer learning policies, and can be easily applied to both synthetic and real image data. ","This paper studies the problem of task transfer learning, where the goal is to learn representations that are transferable across different tasks. The authors propose to measure the transferability based on the task relatedness between the source task and the target task. They propose a new evaluation function called H-score, which is based on an information theoretic approach to measure transferability between source and target tasks. They show that transferring representations from source tasks to target tasks improves the asymptotic error probability of the decision function of the transferred representations for classification problems. They also show that the H-scores obtained from the transferred feature can be used as a measure of transferability for other source tasks, and that it can be applied to other inference problems such as classification and recognition tasks for 3D indoor-scene understanding. The paper also shows that the proposed metric is robust to the choice of transfer learning policies, and can be easily applied to both synthetic and real image data. "
6716,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"Planning USED-FOR complex languages. planning phase USED-FOR global sentence structure. planning phase PART-OF neural machine translation. discrete structural representations USED-FOR syntactic information. approach USED-FOR discrete structural representations. beam search USED-FOR structural codes. discrete codes USED-FOR word generation. codes USED-FOR pure structural variations. codes USED-FOR translation. structural planning USED-FOR global sentence structure. Method is language generation models. Metric are structural diversity metric, and diversity scores. Material is sampled paraphrase translations. ","This paper proposes a novel approach to learning discrete structural representations for syntactic information in the planning phase of neural machine translation, which is an important aspect of planning in complex languages. The authors propose to use beam search to search for structural codes for each word in the input sentence, and then use discrete codes for word generation. The codes are then used for pure structural variations, and the codes are used to train language generation models.  The authors also propose a structural diversity metric to measure the diversity of the learned codes, and show that the diversity scores are highly correlated with the number of structural variations. They also show that these codes can be used to improve the quality of the translation.   The paper also shows that structural planning is able to capture the global sentence structure, and can be applied to sampled paraphrase translations as well. ","This paper proposes a novel approach to learning discrete structural representations for syntactic information in the planning phase of neural machine translation, which is an important aspect of planning in complex languages. The authors propose to use beam search to search for structural codes for each word in the input sentence, and then use discrete codes for word generation. The codes are then used for pure structural variations, and the codes are used to train language generation models.  The authors also propose a structural diversity metric to measure the diversity of the learned codes, and show that the diversity scores are highly correlated with the number of structural variations. They also show that these codes can be used to improve the quality of the translation.   The paper also shows that structural planning is able to capture the global sentence structure, and can be applied to sampled paraphrase translations as well. "
6720,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"discriminator PART-OF generative adversarial network ( SGAN ). SGAN COMPARE integral probability metric ( IPM ) GANs. integral probability metric ( IPM ) GANs COMPARE SGAN. real data COMPARE randomly sampled fake data. randomly sampled fake data COMPARE real data. real data COMPARE fake data. fake data COMPARE real data. approaches USED-FOR GAN loss functions. Relativistic GANs ( RGANs ) CONJUNCTION Relativistic average GANs ( RaGANs ). Relativistic average GANs ( RaGANs ) CONJUNCTION Relativistic GANs ( RGANs ). Relativistic GANs ( RGANs ) HYPONYM-OF them. Relativistic average GANs ( RaGANs ) HYPONYM-OF them. IPM - based GANs HYPONYM-OF RGANs. identity function USED-FOR RGANs. identity function USED-FOR IPM - based GANs. RaGANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE RaGANs. RGANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE RGANs. images COMPARE ones. ones COMPARE images. WGAN - GP CONJUNCTION SGAN. SGAN CONJUNCTION WGAN - GP. RGANs CONJUNCTION RaGANs. RaGANs CONJUNCTION RGANs. GAN CONJUNCTION LSGAN. LSGAN CONJUNCTION GAN. RaGAN COMPARE WGAN - GP. WGAN - GP COMPARE RaGAN. WGAN - GP CONJUNCTION spectral normalization. spectral normalization CONJUNCTION WGAN - GP. discriminator update CONJUNCTION generator update. generator update CONJUNCTION discriminator update. spectral normalization USED-FOR SGAN. SGAN USED-FOR images. WGAN - GP USED-FOR images. gradient penalty USED-FOR RaGAN. SGAN USED-FOR ones. WGAN - GP USED-FOR ones. spectral normalization USED-FOR ones. Method are generator G, divergence minimization, and relativistic discriminator. OtherScientificTerm is priori knowledge. Generic is code.","This paper proposes a new generative adversarial network (SGAN) that includes a discriminator that can be regarded as a priori knowledge of the prior of the generator G. The authors show that SGAN outperforms integral probability metric (IPM) GANs in terms of performance on real data as well as on randomly sampled fake data. They also propose two approaches to train GAN loss functions that are relativistic (i.e., the divergence minimization) and non-relativistic, i.e. the divergence between the prior and the discriminator. They compare them to two existing GAN methods, Relativistically GAN (RGAN) and RaGAN, and show that RGANs outperform SGAN in most cases, while RaGAN outperforms WGAN-GP and SGAN on images generated by SGAN with spectral normalization.   The authors also show that the identity function of RGAN is non-discriminative, which means that the discreteness of the discrediting function is not a function of prior knowledge, but of the code. They further show that this is the case for RGAN with and without a prior. Finally, they show that RaGAN can be seen as a generalization of GAN with a gradient penalty, and that it outperforms RGAN, RaGAN with a prior, and WGAN with SGAN.  In addition, the authors also compare the performance of SGAN and RGAN against LSGAN, GAN, and LSGAN on a variety of datasets. They show that images generated from SGAN using SGAN, SGAN-GP, WGAN and WAN are more discriminative than those generated by LSGAN. ","This paper proposes a new generative adversarial network (SGAN) that includes a discriminator that can be regarded as a priori knowledge of the prior of the generator G. The authors show that SGAN outperforms integral probability metric (IPM) GANs in terms of performance on real data as well as on randomly sampled fake data. They also propose two approaches to train GAN loss functions that are relativistic (i.e., the divergence minimization) and non-relativistic, i.e. the divergence between the prior and the discriminator. They compare them to two existing GAN methods, Relativistically GAN (RGAN) and RaGAN, and show that RGANs outperform SGAN in most cases, while RaGAN outperforms WGAN-GP and SGAN on images generated by SGAN with spectral normalization.   The authors also show that the identity function of RGAN is non-discriminative, which means that the discreteness of the discrediting function is not a function of prior knowledge, but of the code. They further show that this is the case for RGAN with and without a prior. Finally, they show that RaGAN can be seen as a generalization of GAN with a gradient penalty, and that it outperforms RGAN, RaGAN with a prior, and WGAN with SGAN.  In addition, the authors also compare the performance of SGAN and RGAN against LSGAN, GAN, and LSGAN on a variety of datasets. They show that images generated from SGAN using SGAN, SGAN-GP, WGAN and WAN are more discriminative than those generated by LSGAN. "
6724,SP:8df1599919dcb3329553e75ffb19059f192542ea,"catastrophic forgetting FEATURE-OF neural network architectures. approach USED-FOR problem. approach USED-FOR model. solver HYPONYM-OF model. Task is AI and lifelong learning systems. Method are continual learning methods, and Parameter Generation. ","This paper studies the problem of catastrophic forgetting in AI and lifelong learning systems. The authors propose a new approach to tackle this problem, which is based on continual learning methods. Specifically, the authors propose Parameter Generation (PG) as a way to generate new neural network architectures that are robust to catastrophic forgetting. The proposed approach is shown to be able to improve the performance of a model (e.g., a solver) on a number of datasets. ","This paper studies the problem of catastrophic forgetting in AI and lifelong learning systems. The authors propose a new approach to tackle this problem, which is based on continual learning methods. Specifically, the authors propose Parameter Generation (PG) as a way to generate new neural network architectures that are robust to catastrophic forgetting. The proposed approach is shown to be able to improve the performance of a model (e.g., a solver) on a number of datasets. "
6728,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"artificial agents USED-FOR them. rich and orderly structure USED-FOR systems. behavioral dynamics FEATURE-OF multi - agent systems. rich and orderly structure FEATURE-OF behavioral dynamics. rich and orderly structure FEATURE-OF multi - agent systems. Relational Forward Models ( RFM ) USED-FOR multi - agent learning. they USED-FOR interpretable intermediate representations. discrete entities USED-FOR models. RFM modules USED-FOR learning systems. learning systems COMPARE non - augmented baselines. non - augmented baselines COMPARE learning systems. RFM modules PART-OF agents. RFM modules COMPARE non - augmented baselines. non - augmented baselines COMPARE RFM modules. Generic is networks. OtherScientificTerm are multi - agent environments, and social interactions. Method is autonomous systems. ","This paper proposes Relational Forward Models (RFM) for multi-agent learning, which is based on the idea that the rich and orderly structure in systems with behavioral dynamics that have rich social interaction between agents is a key factor that enables them to be learned by artificial agents. RFM modules are used to augment existing learning systems by augmenting them with discrete entities, and they are shown to be able to learn interpretable intermediate representations that can be used to guide the training of the networks. The authors show that the resulting learning systems outperform existing non-augmented baselines and are able to generalize to multi- agent environments with rich social interactions. They also show that RFM can be applied to autonomous systems. ","This paper proposes Relational Forward Models (RFM) for multi-agent learning, which is based on the idea that the rich and orderly structure in systems with behavioral dynamics that have rich social interaction between agents is a key factor that enables them to be learned by artificial agents. RFM modules are used to augment existing learning systems by augmenting them with discrete entities, and they are shown to be able to learn interpretable intermediate representations that can be used to guide the training of the networks. The authors show that the resulting learning systems outperform existing non-augmented baselines and are able to generalize to multi- agent environments with rich social interactions. They also show that RFM can be applied to autonomous systems. "
6732,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"reinforcement learning USED-FOR real world problems. expert behavior USED-FOR reward function. method USED-FOR rewards. prior USED-FOR approach. images USED-FOR rewards. OtherScientificTerm are oracle reward function, reward functions, and expressive reward functions. Method is Inverse reinforcement learning ( IRL ). Material is datasets of demonstrations. Task is IRL. Generic is tasks. ","This paper proposes a new method for learning the oracle reward function. Inverse reinforcement learning (IRL) is an important problem in reinforcement learning for real world problems, where the reward function is learned from expert behavior. In this paper, the authors propose a method to learn the rewards from images that are more expressive than those learned from datasets of demonstrations. The proposed approach is based on a prior that encourages the learned reward functions to be more expressive. The authors show that IRL is able to learn more expressive reward functions on a variety of tasks. ","This paper proposes a new method for learning the oracle reward function. Inverse reinforcement learning (IRL) is an important problem in reinforcement learning for real world problems, where the reward function is learned from expert behavior. In this paper, the authors propose a method to learn the rewards from images that are more expressive than those learned from datasets of demonstrations. The proposed approach is based on a prior that encourages the learned reward functions to be more expressive. The authors show that IRL is able to learn more expressive reward functions on a variety of tasks. "
6736,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"framework USED-FOR data efficient and versatile learning. framework USED-FOR Meta - Learning approximate Probabilistic Inference. Meta - Learning approximate Probabilistic Inference USED-FOR Prediction. ML - PIP HYPONYM-OF framework. ML - PIP USED-FOR methods. probabilistic interpretations of meta - learning USED-FOR methods. probabilistic interpretations of meta - learning USED-FOR ML - PIP. VERSA HYPONYM-OF framework. flexible and versatile amortization network USED-FOR framework. flexible and versatile amortization network USED-FOR VERSA. benchmark datasets EVALUATE-FOR method. benchmark datasets EVALUATE-FOR VERSA. few - shot ShapeNet view reconstruction task EVALUATE-FOR approach. Material is few - shot learning datasets. OtherScientificTerm are task - specific parameters, and second derivatives. Task are optimization, inference, and classification. Method is inference networks. ","This paper proposes a new framework for data efficient and versatile learning, called Meta-Learning approximate Probabilistic Inference (ML-PIP) for few-shot learning datasets. Prediction is based on Meta-Lingual Inference for Prediction (MIPIP), which is a framework for learning a set of task-specific parameters for each task. The authors propose two methods based on the probabilistic interpretations of meta-learning, namely, ML-PUIP and VERSA. The proposed framework, called VERSA, uses a flexible and versatile amortization network that can be used for both optimization and inference. The method is evaluated on several benchmark datasets, and verifies the effectiveness of the proposed method. The approach is also evaluated on a few -shot ShapeNet view reconstruction task, where it is shown that the proposed approach is able to achieve state-of-the-art performance.    The main contribution of the paper is the introduction of two inference networks. The first one, called VerSA, is a variant of the previous work, where the second derivatives of the second layer of the neural network is amortized in a way that allows for efficient inference and classification. The second, called PIP, is an extension of the work of [1]. ","This paper proposes a new framework for data efficient and versatile learning, called Meta-Learning approximate Probabilistic Inference (ML-PIP) for few-shot learning datasets. Prediction is based on Meta-Lingual Inference for Prediction (MIPIP), which is a framework for learning a set of task-specific parameters for each task. The authors propose two methods based on the probabilistic interpretations of meta-learning, namely, ML-PUIP and VERSA. The proposed framework, called VERSA, uses a flexible and versatile amortization network that can be used for both optimization and inference. The method is evaluated on several benchmark datasets, and verifies the effectiveness of the proposed method. The approach is also evaluated on a few -shot ShapeNet view reconstruction task, where it is shown that the proposed approach is able to achieve state-of-the-art performance.    The main contribution of the paper is the introduction of two inference networks. The first one, called VerSA, is a variant of the previous work, where the second derivatives of the second layer of the neural network is amortized in a way that allows for efficient inference and classification. The second, called PIP, is an extension of the work of [1]. "
6740,SP:44e0f63ffee15796ba6135463134084bb370627b,"deep learning architecture USED-FOR classifying structured objects. ultrafine - grained datasets USED-FOR deep learning architecture. localvisual features CONJUNCTION neighboring class information. neighboring class information CONJUNCTION localvisual features. linear - chain CRFs USED-FOR images. visual features COMPARE class - structure information. class - structure information COMPARE visual features. convolutional layers USED-FOR visual features. CRF pairwise potential matrix USED-FOR class - structure information. parametrization USED-FOR nonlinear objective function. surrogate likelihood USED-FOR local likelihood approximation. local likelihood approximation USED-FOR CRF. surrogate likelihood USED-FOR CRF. integrated batch - normalization USED-FOR CRF. integrated batch - normalization USED-FOR local likelihood approximation. CRF methods USED-FOR contextual relationships. model COMPARE CRF methods. CRF methods COMPARE model. method COMPARE linear CRF parametrization. linear CRF parametrization COMPARE method. unnormalized likelihood optimization CONJUNCTION RNN modeling. RNN modeling CONJUNCTION unnormalized likelihood optimization. linear CRF parametrization CONJUNCTION unnormalized likelihood optimization. unnormalized likelihood optimization CONJUNCTION linear CRF parametrization. linear CRF parametrization COMPARE RNN modeling. RNN modeling COMPARE linear CRF parametrization. method COMPARE RNN modeling. RNN modeling COMPARE method. OtherScientificTerm are context - based semantic similarity space, visual similarities, and contextual information. Material is images of retail - store product displays. ","This paper proposes a novel deep learning architecture for classifying structured objects on ultrafine-grained datasets. The authors consider the context-based semantic similarity space, which is defined as visual similarities between two images generated by linear-chain CRFs. The visual features of convolutional layers are decomposed into localvisual features and neighboring class information. The class-structure information is extracted from the CRF pairwise potential matrix. The proposed parametrization of the nonlinear objective function is based on a local likelihood approximation based on the integrated batch-normalization of a CRF with a surrogate likelihood. Experiments on images of retail-store product displays show that the proposed model outperforms existing CRF methods for learning contextual relationships. In addition, the proposed method outperforms linear CRF parametrized with unnormalized likelihood optimization and RNN modeling. ","This paper proposes a novel deep learning architecture for classifying structured objects on ultrafine-grained datasets. The authors consider the context-based semantic similarity space, which is defined as visual similarities between two images generated by linear-chain CRFs. The visual features of convolutional layers are decomposed into localvisual features and neighboring class information. The class-structure information is extracted from the CRF pairwise potential matrix. The proposed parametrization of the nonlinear objective function is based on a local likelihood approximation based on the integrated batch-normalization of a CRF with a surrogate likelihood. Experiments on images of retail-store product displays show that the proposed model outperforms existing CRF methods for learning contextual relationships. In addition, the proposed method outperforms linear CRF parametrized with unnormalized likelihood optimization and RNN modeling. "
6744,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"global structure CONJUNCTION fine - scale waveform coherence. fine - scale waveform coherence CONJUNCTION global structure. audio synthesis HYPONYM-OF machine learning task. global structure FEATURE-OF human perception. fine - scale waveform coherence FEATURE-OF human perception. local structure FEATURE-OF Autoregressive models. WaveNet HYPONYM-OF Autoregressive models. global latent conditioning CONJUNCTION parallel sampling. parallel sampling CONJUNCTION global latent conditioning. global latent conditioning FEATURE-OF Generative Adversarial Networks ( GANs ). parallel sampling USED-FOR Generative Adversarial Networks ( GANs ). log magnitudes CONJUNCTION instantaneous frequencies. instantaneous frequencies CONJUNCTION log magnitudes. GANs USED-FOR high - fidelity and locally - coherent audio. log magnitudes USED-FOR GANs. spectral domain FEATURE-OF sufficient frequency resolution. sufficient frequency resolution FEATURE-OF log magnitudes. sufficient frequency resolution FEATURE-OF instantaneous frequencies. spectral domain FEATURE-OF instantaneous frequencies. sufficient frequency resolution USED-FOR GANs. GANs COMPARE WaveNet baselines. WaveNet baselines COMPARE GANs. NSynth dataset EVALUATE-FOR GANs. automated and human evaluation metrics EVALUATE-FOR WaveNet baselines. automated and human evaluation metrics EVALUATE-FOR GANs. Method is iterative sampling. OtherScientificTerm are global latent structure, and locally - coherent audio waveforms. ","This paper studies the problem of audio synthesis, an important machine learning task in which the goal is to capture both global structure and fine-scale waveform coherence in human perception. Autoregressive models such as WaveNet have been shown to have a local structure, and iterative sampling is used to capture the global latent structure. In this paper, the authors show that Generative Adversarial Networks (GANs) with global latent conditioning and parallel sampling share the same local structure. They also show that GANs with sufficient frequency resolution in the spectral domain and log magnitudes and instantaneous frequencies (in spectral domain) are able to capture high-fidelity and locally-coherent audio. The authors also show empirically that the performance of the GAN on the NSynth dataset is comparable to that of WaveNet baselines on both automated and human evaluation metrics.   ","This paper studies the problem of audio synthesis, an important machine learning task in which the goal is to capture both global structure and fine-scale waveform coherence in human perception. Autoregressive models such as WaveNet have been shown to have a local structure, and iterative sampling is used to capture the global latent structure. In this paper, the authors show that Generative Adversarial Networks (GANs) with global latent conditioning and parallel sampling share the same local structure. They also show that GANs with sufficient frequency resolution in the spectral domain and log magnitudes and instantaneous frequencies (in spectral domain) are able to capture high-fidelity and locally-coherent audio. The authors also show empirically that the performance of the GAN on the NSynth dataset is comparable to that of WaveNet baselines on both automated and human evaluation metrics.   "
6748,SP:0c0f078c208600f541a76ecaae49cf9a98588736,"training accuracy EVALUATE-FOR Neural networks. mixed integer program USED-FOR piecewise - linear neural networks. verifier COMPARE state - of - the - art. state - of - the - art COMPARE verifier. finding minimum adversarial distortions EVALUATE-FOR verifier. tight formulations USED-FOR non - linearities. tight formulations CONJUNCTION presolve algorithm. presolve algorithm CONJUNCTION tight formulations. tight formulations USED-FOR computational speedup. verifier USED-FOR networks. adversarial accuracy EVALUATE-FOR MNIST classifier. adversarial accuracy EVALUATE-FOR perturbations. perturbations FEATURE-OF MNIST classifier. adversarial example USED-FOR classifier. bounded l∞ norm FEATURE-OF perturbations. robust training procedures CONJUNCTION network architectures. network architectures CONJUNCTION robust training procedures. adversarial examples COMPARE first - order attack. first - order attack COMPARE adversarial examples. Task are Verification of networks, and verification of piecewise - linear neural networks. OtherScientificTerm are minimum adversarial distortions, ReLUs, and norm - bounded perturbations. Method is convolutional and residual networks. Material is MNIST and CIFAR-10 datasets. ","This paper studies the problem of verifying the robustness of piecewise-linear neural networks to adversarial perturbations.    The authors consider the setting of verification of networks that are piecewise linear in the form of a mixed integer program, where the goal is to ensure that the training accuracy of Neural networks is not affected by the perturbation. Verification of networks is a challenging problem because of the large number of parameters and the need to find minimum adversarial distortions.  This paper proposes to use ReLUs to verify the verification of such networks. The authors show that the verifier is more robust than the state-of-the-art in terms of finding minimum perturbational distortions. They also show that for convolutional and residual networks, a verifier that is robust to norm-bounded adversarial attacks is more powerful than the one that verifies robustness to first-order attacks.  Finally, the authors propose to use tight formulations for non-linearities and a presolve algorithm to achieve a computational speedup.  They show that a classifier trained on an adversarial example with a bounded l∞ norm can be trained on a perturbated version of the MNIST classifier to achieve the same level of adversarial accuracy as the one trained on the original adversarial examples.  Experiments are conducted on MNIST and CIFAR-10 datasets, and they show that robust training procedures and network architectures are more robust to perturbed examples than to first order attacks, and that the robust training is more effective than robust training with first order attack. ","This paper studies the problem of verifying the robustness of piecewise-linear neural networks to adversarial perturbations.    The authors consider the setting of verification of networks that are piecewise linear in the form of a mixed integer program, where the goal is to ensure that the training accuracy of Neural networks is not affected by the perturbation. Verification of networks is a challenging problem because of the large number of parameters and the need to find minimum adversarial distortions.  This paper proposes to use ReLUs to verify the verification of such networks. The authors show that the verifier is more robust than the state-of-the-art in terms of finding minimum perturbational distortions. They also show that for convolutional and residual networks, a verifier that is robust to norm-bounded adversarial attacks is more powerful than the one that verifies robustness to first-order attacks.  Finally, the authors propose to use tight formulations for non-linearities and a presolve algorithm to achieve a computational speedup.  They show that a classifier trained on an adversarial example with a bounded l∞ norm can be trained on a perturbated version of the MNIST classifier to achieve the same level of adversarial accuracy as the one trained on the original adversarial examples.  Experiments are conducted on MNIST and CIFAR-10 datasets, and they show that robust training procedures and network architectures are more robust to perturbed examples than to first order attacks, and that the robust training is more effective than robust training with first order attack. "
6752,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,rich structure FEATURE-OF real world tasks. repeated structure USED-FOR learning. default policy HYPONYM-OF component. component PART-OF KL regularized expected reward objective. it USED-FOR reusable behaviours. reusable behaviours USED-FOR policy. information bottleneck approaches CONJUNCTION variational EM algorithm. variational EM algorithm CONJUNCTION information bottleneck approaches. default policy CONJUNCTION policy. policy CONJUNCTION default policy. default policy USED-FOR tasks. Method is fixed default policy. Generic is strategy. Material is discrete and continuous action domains. ,"This paper studies the problem of learning from rich structure in real world tasks with repeated structure. The authors propose a KL regularized expected reward objective that includes a component called the default policy, which is a fixed default policy. They show that it can be used to learn reusable behaviours that can be applied to any policy, and it is shown that it performs well in both discrete and continuous action domains. They also show that a combination of information bottleneck approaches and a variational EM algorithm is sufficient to learn a good default policy and a good policy that performs well on a variety of tasks.","This paper studies the problem of learning from rich structure in real world tasks with repeated structure. The authors propose a KL regularized expected reward objective that includes a component called the default policy, which is a fixed default policy. They show that it can be used to learn reusable behaviours that can be applied to any policy, and it is shown that it performs well in both discrete and continuous action domains. They also show that a combination of information bottleneck approaches and a variational EM algorithm is sufficient to learn a good default policy and a good policy that performs well on a variety of tasks."
6756,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,conversation history USED-FOR Conversational machine comprehension. single - turn models USED-FOR history. FLOW USED-FOR single - turn models. intermediate representations PART-OF FLOW. intermediate representations PART-OF mechanism. alternating parallel processing structure USED-FOR FLOW. alternating parallel processing structure USED-FOR intermediate representations. approaches COMPARE FLOW. FLOW COMPARE approaches. latent semantics PART-OF FLOW. CoQA CONJUNCTION QuAC. QuAC CONJUNCTION CoQA. FLOWQA HYPONYM-OF model. conversational challenges EVALUATE-FOR FLOWQA. conversational challenges EVALUATE-FOR model. tasks EVALUATE-FOR FLOW. FLOWQA COMPARE models. models COMPARE FLOWQA. sequential instruction understanding USED-FOR conversational machine comprehension. SCONE EVALUATE-FOR models. accuracy EVALUATE-FOR FLOWQA. SCONE EVALUATE-FOR FLOWQA. Metric is F1. ," Conversational machine comprehension based on conversational history is an important problem in the field of machine learning. The paper proposes FLOW, a new mechanism that combines two existing approaches, FLOWQA and CoQA, to learn the history of a conversation using single-turn models. The mechanism consists of two steps: 1) intermediate representations are learned using an alternating parallel processing structure, and 2) the latent semantics of the intermediate representations is learned. The proposed model is evaluated on two conversational challenges, CoQa and QuAC, and is shown to outperform the existing approaches on both tasks. On SCONE, the model is also shown to be able to improve the accuracy on SCONE in terms of F1 and F2. On the other hand, FLowQA outperforms the existing models on the SCONE task of sequential instruction understanding.   "," Conversational machine comprehension based on conversational history is an important problem in the field of machine learning. The paper proposes FLOW, a new mechanism that combines two existing approaches, FLOWQA and CoQA, to learn the history of a conversation using single-turn models. The mechanism consists of two steps: 1) intermediate representations are learned using an alternating parallel processing structure, and 2) the latent semantics of the intermediate representations is learned. The proposed model is evaluated on two conversational challenges, CoQa and QuAC, and is shown to outperform the existing approaches on both tasks. On SCONE, the model is also shown to be able to improve the accuracy on SCONE in terms of F1 and F2. On the other hand, FLowQA outperforms the existing models on the SCONE task of sequential instruction understanding.   "
6760,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,"Programming languages HYPONYM-OF machine learning. generative models USED-FOR generating static snapshots of code. source code HYPONYM-OF dynamic object. synthetic data USED-FOR edit patterns. synthetic data USED-FOR neural networks. generalization USED-FOR edit patterns. large - scale dataset USED-FOR models. fine - grained edits PART-OF large - scale dataset. Method are generative models of source code, and attentional and pointer network components. OtherScientificTerm are static snapshots of code, and source code files. Generic are it, and tools. Metric is scalability. ","Programming languages are a hot topic in machine learning. This paper proposes to use generative models of source code for generating static snapshots of code, which are then used to train neural networks on synthetic data to learn edit patterns that can be used to improve generalization to new edit patterns. The authors propose to use a large-scale dataset with fine-grained edits to train the models. They also propose to add attentional and pointer network components to improve the scalability of the model. The paper is well-written and well-motivated, and it is easy to follow. However, there are a few issues with the paper:   1. The source code is a dynamic object, which makes it difficult to understand how the model works.  2. There are no source code files.  3. It is not clear to me that the model is able to generalize to new edits. ","Programming languages are a hot topic in machine learning. This paper proposes to use generative models of source code for generating static snapshots of code, which are then used to train neural networks on synthetic data to learn edit patterns that can be used to improve generalization to new edit patterns. The authors propose to use a large-scale dataset with fine-grained edits to train the models. They also propose to add attentional and pointer network components to improve the scalability of the model. The paper is well-written and well-motivated, and it is easy to follow. However, there are a few issues with the paper:   1. The source code is a dynamic object, which makes it difficult to understand how the model works.  2. There are no source code files.  3. It is not clear to me that the model is able to generalize to new edits. "
6764,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"strategy USED-FOR multi - class classification. pairwise similarity HYPONYM-OF annotation. pairwise similarity USED-FOR strategy. binary classifier USED-FOR pairwise similarity prediction. method USED-FOR binary classifier. submodule USED-FOR multi - class classifier. loss function USED-FOR neural network - based models. probabilistic graphical model USED-FOR it. method COMPARE state of the art. state of the art COMPARE method. learning paradigms EVALUATE-FOR state of the art. learning paradigms EVALUATE-FOR method. multi - class labels FEATURE-OF learning multi - class classification. accuracy EVALUATE-FOR state of the art. accuracy EVALUATE-FOR method. OtherScientificTerm is class - specific labels. Method is meta classification learning. Generic are approach, and framework. ","This paper proposes a new strategy for multi-class classification based on pairwise similarity, which is an important annotation in the context of class-specific labels. The proposed strategy is based on the idea that a binary classifier is trained on a set of pairs of labels, and a sub-classifier is learned to predict the similarity between the pair of labels. This submodule is then used to train a multi -class classifier. The authors propose a new loss function for neural network-based models, and show that it is a probabilistic graphical model. Experiments show that the proposed method outperforms the state of the art on several learning paradigms, and achieves better accuracy than the state-of-the-art on the task of meta classification learning. The paper is well-written and well-motivated, and the proposed approach is well motivated. However, there are a few issues in the paper that prevent me from recommending acceptance of this paper as a must-read for the community.    I have read the authors' response and other reviews, and I am happy to increase my score if the authors address my concerns. I think the proposed framework is novel and interesting, but there are some issues that need to be addressed to improve the quality of the paper. For example, it is not clear to me that the paper is clearly stated that the method is a generalization of the previous work, and it is unclear to me whether the method can be applied to the task with multi-classes. Also, there is a lack of discussion about the problem of learning multi-labeled labels, which may be an important problem in the field.  I would also like to see a more detailed discussion of the issues with the proposed methods. ","This paper proposes a new strategy for multi-class classification based on pairwise similarity, which is an important annotation in the context of class-specific labels. The proposed strategy is based on the idea that a binary classifier is trained on a set of pairs of labels, and a sub-classifier is learned to predict the similarity between the pair of labels. This submodule is then used to train a multi -class classifier. The authors propose a new loss function for neural network-based models, and show that it is a probabilistic graphical model. Experiments show that the proposed method outperforms the state of the art on several learning paradigms, and achieves better accuracy than the state-of-the-art on the task of meta classification learning. The paper is well-written and well-motivated, and the proposed approach is well motivated. However, there are a few issues in the paper that prevent me from recommending acceptance of this paper as a must-read for the community.    I have read the authors' response and other reviews, and I am happy to increase my score if the authors address my concerns. I think the proposed framework is novel and interesting, but there are some issues that need to be addressed to improve the quality of the paper. For example, it is not clear to me that the paper is clearly stated that the method is a generalization of the previous work, and it is unclear to me whether the method can be applied to the task with multi-classes. Also, there is a lack of discussion about the problem of learning multi-labeled labels, which may be an important problem in the field.  I would also like to see a more detailed discussion of the issues with the proposed methods. "
6768,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"visual scene USED-FOR interpretation of quantifier statements. inference mechanisms USED-FOR interpretation of quantifier statements. cognitive concepts USED-FOR strategies. deep learning models USED-FOR visual question answering. spatial arrangement of the scene HYPONYM-OF confounding factors. Task is psycholinguistics. Method are FiLM visual question answering model, and approximate number system. OtherScientificTerm is Weber ’s law. Generic is system. ","This paper studies the question answering problem in the context of psycholinguistics. The authors propose a FiLM visual question answering model, where the interpretation of quantifier statements in a visual scene is based on the inference mechanisms of two different inference mechanisms. The two strategies are based on cognitive concepts: (1) the approximate number system, and (2) the use of Weber’s law.  The authors show that deep learning models can be trained to solve visual questions answering in this setting. However, there are some confounding factors (e.g. spatial arrangement of the scene) that hinder the performance of the system.  ","This paper studies the question answering problem in the context of psycholinguistics. The authors propose a FiLM visual question answering model, where the interpretation of quantifier statements in a visual scene is based on the inference mechanisms of two different inference mechanisms. The two strategies are based on cognitive concepts: (1) the approximate number system, and (2) the use of Weber’s law.  The authors show that deep learning models can be trained to solve visual questions answering in this setting. However, there are some confounding factors (e.g. spatial arrangement of the scene) that hinder the performance of the system.  "
6772,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"embedding models USED-FOR link prediction in relational knowledge graphs. relational facts PART-OF Knowledge graphs. Bayesian framework USED-FOR knowledge graphs. it USED-FOR gradient based optimization. divergences FEATURE-OF non - Bayesian treatment. gradient based optimization USED-FOR divergences. hyperparameters USED-FOR gradient based optimization. Models COMPARE state - of - the - art. state - of - the - art COMPARE Models. hyperparameters COMPARE state - of - the - art. state - of - the - art COMPARE hyperparameters. hyperparameters USED-FOR Models. Material is relational knowledge graphs. Task is small data problem. OtherScientificTerm is parameter uncertainty. Method are variational inference, and Bayesian approach. ","This paper studies the problem of embedding models for link prediction in relational knowledge graphs. Knowledge graphs consist of a set of relational facts, each of which represents a small data problem. The authors propose a Bayesian framework for embedding knowledge graphs, and apply it to gradient based optimization to find divergences in the non-Bayesian treatment. The main contribution of the paper is to use hyperparameters from the Bayesian approach to select the best hyperparameter to optimize, and to use variational inference to estimate the parameter uncertainty. Models are compared with the state-of-the-art, and show that the proposed models are more robust to the choice of hyperparamters compared to the state of the art.  ","This paper studies the problem of embedding models for link prediction in relational knowledge graphs. Knowledge graphs consist of a set of relational facts, each of which represents a small data problem. The authors propose a Bayesian framework for embedding knowledge graphs, and apply it to gradient based optimization to find divergences in the non-Bayesian treatment. The main contribution of the paper is to use hyperparameters from the Bayesian approach to select the best hyperparameter to optimize, and to use variational inference to estimate the parameter uncertainty. Models are compared with the state-of-the-art, and show that the proposed models are more robust to the choice of hyperparamters compared to the state of the art.  "
6776,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"dimension reduction USED-FOR visualization or prediction enhancement. online learning approaches USED-FOR supervised dimension reduction. algorithm USED-FOR supervised dimension reduction. sliced inverse regression ( SIR ) HYPONYM-OF algorithm. sliced inverse regression ( SIR ) USED-FOR algorithm. algorithm USED-FOR subspace of significant factors. intrinsic lower dimensionality FEATURE-OF subspace of significant factors. overlapping technique USED-FOR algorithm. real data applications EVALUATE-FOR algorithms. Method are Online learning, and incremental sliced inverse regression ( ISIR ). Task is online dimension reduction. Generic is it. ","This paper studies the problem of dimension reduction in the context of visualization or prediction enhancement. Online learning is an important problem in this context, and online learning approaches to supervised dimension reduction have recently gained a lot of attention. This paper proposes a new algorithm called incremental sliced inverse regression (ISIR) which is an algorithm that uses an existing algorithm (sliced inverse regression) to perform supervised dimension Reduction (SIR) in the setting of online dimension reduction. The algorithm uses an overlapping technique to learn a subspace of significant factors with intrinsic lower dimensionality, and then uses it to reduce the dimension of the subspace. Experiments on real data applications demonstrate the effectiveness of the proposed algorithms. ","This paper studies the problem of dimension reduction in the context of visualization or prediction enhancement. Online learning is an important problem in this context, and online learning approaches to supervised dimension reduction have recently gained a lot of attention. This paper proposes a new algorithm called incremental sliced inverse regression (ISIR) which is an algorithm that uses an existing algorithm (sliced inverse regression) to perform supervised dimension Reduction (SIR) in the setting of online dimension reduction. The algorithm uses an overlapping technique to learn a subspace of significant factors with intrinsic lower dimensionality, and then uses it to reduce the dimension of the subspace. Experiments on real data applications demonstrate the effectiveness of the proposed algorithms. "
6780,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"models USED-FOR intra - modal and cross - modal interactions. models USED-FOR unexpected missing or noisy modalities. intra - modal and cross - modal interactions USED-FOR prediction. models USED-FOR prediction. multimodal data USED-FOR joint generative - discriminative objective. model USED-FOR representations. model USED-FOR independent factors. multimodal discriminative and modality - specific generative factors HYPONYM-OF independent factors. joint multimodal features USED-FOR discriminative tasks. Multimodal discriminative factors USED-FOR discriminative tasks. joint multimodal features PART-OF Multimodal discriminative factors. sentiment prediction HYPONYM-OF discriminative tasks. model USED-FOR multimodal representations. multimodal datasets EVALUATE-FOR multimodal representations. model USED-FOR missing modalities. factorized representations USED-FOR multimodal learning. Task is Learning multimodal representations. OtherScientificTerm are multiple modalities, and Modality - specific generative factors. ","This paper studies the problem of learning multimodal representations from multiple modalities. The authors propose a joint generative-discriminative objective for learning from multi-modal data, where models are trained to learn intra-moda and cross-modality interactions for prediction from multiple models to avoid unexpected missing or noisy modalities, and a model is used to learn representations that are robust to multiple factors (i.e., multimodality discriminative and modality-specific generative factors). Multimodal discriminatives are then used to train a model to learn multimodial representations on a set of multi-dimensional datasets. The paper shows that the joint multimodale features of the joint discriminator and discriminator can be used to improve the performance of a model trained on a single model on the missing modalities and on the modalities that are not present in the training data. Multimmodal features are also used for other types of discrimination tasks (e.g., sentiment prediction) that rely on the use of multiple multimodalities. Modality specific generative factorization is also used. Experiments are conducted on several multimodar datasets and show that the multimodals learned by the model are more robust to missing and missing-conditioned modalities than those learned by a model that does not use factorized representations.    The authors also show that multimodally learning with factorized representation improves the performance on a variety of datasets.","This paper studies the problem of learning multimodal representations from multiple modalities. The authors propose a joint generative-discriminative objective for learning from multi-modal data, where models are trained to learn intra-moda and cross-modality interactions for prediction from multiple models to avoid unexpected missing or noisy modalities, and a model is used to learn representations that are robust to multiple factors (i.e., multimodality discriminative and modality-specific generative factors). Multimodal discriminatives are then used to train a model to learn multimodial representations on a set of multi-dimensional datasets. The paper shows that the joint multimodale features of the joint discriminator and discriminator can be used to improve the performance of a model trained on a single model on the missing modalities and on the modalities that are not present in the training data. Multimmodal features are also used for other types of discrimination tasks (e.g., sentiment prediction) that rely on the use of multiple multimodalities. Modality specific generative factorization is also used. Experiments are conducted on several multimodar datasets and show that the multimodals learned by the model are more robust to missing and missing-conditioned modalities than those learned by a model that does not use factorized representations.    The authors also show that multimodally learning with factorized representation improves the performance on a variety of datasets."
6784,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"shared conditional WaveNet core CONJUNCTION independent learned embeddings. independent learned embeddings CONJUNCTION shared conditional WaveNet core. independent learned embeddings USED-FOR multi - speaker model. shared conditional WaveNet core USED-FOR multi - speaker model. fixed weights USED-FOR neural network. stochastic gradient descent USED-FOR architecture. trained neural network encoder USED-FOR speaker embedding. sample naturalness CONJUNCTION voice similarity. voice similarity CONJUNCTION sample naturalness. approaches USED-FOR multi - speaker neural network. Method are meta - learning approach, and TTS system. Generic are network, and strategies. OtherScientificTerm is WaveNet core. Material is audio data. ","This paper proposes a meta-learning approach to learn a multi-speaker model with a shared conditional WaveNet core and independent learned embeddings. The proposed architecture is based on stochastic gradient descent, where the neural network is trained with fixed weights and the speaker embedding is learned by a trained neural network encoder. The authors show that the network is able to generalize to a large number of speakers in a TTS system. They also show that this network can generalize well to a smaller number of speaker pairs. The paper also shows that this approach can be combined with existing approaches to train a multi -speaker neural network. The main contribution of this paper is that the authors propose two strategies to train the network. First, the authors use the WaveNet cores of all speakers in the system to share information across all the speakers. Second, they use a trained speaker encoder to learn the speaker encoding from audio data.    The authors also provide a theoretical analysis of the effect of sample naturalness and voice similarity. ","This paper proposes a meta-learning approach to learn a multi-speaker model with a shared conditional WaveNet core and independent learned embeddings. The proposed architecture is based on stochastic gradient descent, where the neural network is trained with fixed weights and the speaker embedding is learned by a trained neural network encoder. The authors show that the network is able to generalize to a large number of speakers in a TTS system. They also show that this network can generalize well to a smaller number of speaker pairs. The paper also shows that this approach can be combined with existing approaches to train a multi -speaker neural network. The main contribution of this paper is that the authors propose two strategies to train the network. First, the authors use the WaveNet cores of all speakers in the system to share information across all the speakers. Second, they use a trained speaker encoder to learn the speaker encoding from audio data.    The authors also provide a theoretical analysis of the effect of sample naturalness and voice similarity. "
6788,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"Robust estimation HYPONYM-OF statistics. Huber ’s -contamination model USED-FOR Robust estimation. Tukey ’s median CONJUNCTION estimators. estimators CONJUNCTION Tukey ’s median. estimators HYPONYM-OF Statistically optimal procedures. depth functions USED-FOR estimators. Tukey ’s median HYPONYM-OF Statistically optimal procedures. f -GANs CONJUNCTION depth functions. depth functions CONJUNCTION f -GANs. f -Learning USED-FOR depth functions. f -Learning USED-FOR f -GANs. depth functions USED-FOR statistically optimal robust estimators. total variation distance FEATURE-OF variational lower bounds. f -Learning USED-FOR variational lower bounds. variational lower bounds USED-FOR depth functions. GANs USED-FOR computing robust estimators. Gaussian distribution CONJUNCTION elliptical distributions. elliptical distributions CONJUNCTION Gaussian distribution. statistically optimal robust location estimators USED-FOR Gaussian distribution. statistically optimal robust location estimators USED-FOR elliptical distributions. hidden layers PART-OF GANs. discriminator networks PART-OF GANs. hidden layers PART-OF discriminator networks. OtherScientificTerm are computational intractability, and first moment. Method is f GANs. ","Robust estimation is one of the most commonly used statistics in statistics. Robust estimation in Huber’s-contamination model can be computationally intractable due to computational intractability. Statistically optimal procedures, such as Tukey's median and estimators based on depth functions (e.g., f-GANs and depth functions based on f-Learning) have been shown to be statistically optimal robust estimators. This paper studies the problem of computing robust estimator in GANs.    The authors show that f -GANs, f-learning, and their depth functions can be approximated by variational lower bounds on the total variation distance between the first moment of the f GAN and the true first moment.  They also show that for Gaussian distribution and elliptical distributions, there exists a set of ""statistically optimal robust location estimators"" based on the statistically optimal locations of the hidden layers of the discriminator networks in GGANs. The authors further show that the number of hidden layers in GANNs and the depth functions used for computing statistically optimal estimates of these depth functions is a function of the dimensionality of the input data and the dimension of the function.  The paper also shows that f-Gradient and f-Lingual can be used to approximate the depth of f-GANs based on a variational upper bound on the dimension and dimension of depth functions. ","Robust estimation is one of the most commonly used statistics in statistics. Robust estimation in Huber’s-contamination model can be computationally intractable due to computational intractability. Statistically optimal procedures, such as Tukey's median and estimators based on depth functions (e.g., f-GANs and depth functions based on f-Learning) have been shown to be statistically optimal robust estimators. This paper studies the problem of computing robust estimator in GANs.    The authors show that f -GANs, f-learning, and their depth functions can be approximated by variational lower bounds on the total variation distance between the first moment of the f GAN and the true first moment.  They also show that for Gaussian distribution and elliptical distributions, there exists a set of ""statistically optimal robust location estimators"" based on the statistically optimal locations of the hidden layers of the discriminator networks in GGANs. The authors further show that the number of hidden layers in GANNs and the depth functions used for computing statistically optimal estimates of these depth functions is a function of the dimensionality of the input data and the dimension of the function.  The paper also shows that f-Gradient and f-Lingual can be used to approximate the depth of f-GANs based on a variational upper bound on the dimension and dimension of depth functions. "
6792,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"symmetry CONJUNCTION repetition. repetition CONJUNCTION symmetry. higher - level, abstract regularities PART-OF scene. repetition HYPONYM-OF higher - level, abstract regularities. symmetry HYPONYM-OF higher - level, abstract regularities. vision recognition modules CONJUNCTION scene representations. scene representations CONJUNCTION vision recognition modules. symbolic program USED-FOR scene programs. model USED-FOR scene programs. hierarchical, object - based scene representation USED-FOR model. hierarchical, object - based scene representation USED-FOR scene programs. compositional structure FEATURE-OF real images. synthetic data EVALUATE-FOR model. complex visual analogy - making CONJUNCTION scene extrapolation. scene extrapolation CONJUNCTION complex visual analogy - making. scene programs USED-FOR applications. scene extrapolation HYPONYM-OF applications. complex visual analogy - making HYPONYM-OF applications. Task is Human scene perception. ","This paper proposes a new model for scene representation learning based on a symbolic program. The proposed model is based on the idea that higher-level, abstract regularities in a scene (e.g. symmetry, repetition, etc.) can be represented as a set of scene programs that can be learned by a model. The model uses a hierarchical, object-based scene representation to learn the scene programs, which are then combined with vision recognition modules and scene representations. Human scene perception is evaluated on synthetic data and real images with compositional structure. The paper shows that the learned scene programs can be used for two applications: complex visual analogy-making and scene extrapolation. ","This paper proposes a new model for scene representation learning based on a symbolic program. The proposed model is based on the idea that higher-level, abstract regularities in a scene (e.g. symmetry, repetition, etc.) can be represented as a set of scene programs that can be learned by a model. The model uses a hierarchical, object-based scene representation to learn the scene programs, which are then combined with vision recognition modules and scene representations. Human scene perception is evaluated on synthetic data and real images with compositional structure. The paper shows that the learned scene programs can be used for two applications: complex visual analogy-making and scene extrapolation. "
6796,SP:a8df2aa6870a05f8580117f433e07e70a5342930,"long sequence data USED-FOR Recurrent neural networks. methods USED-FOR RNN state updates. memory FEATURE-OF network. methods PART-OF architectures. timing - gated LSTM RNN model USED-FOR reducing state updates. longer memory persistence CONJUNCTION error - gradient flow. error - gradient flow CONJUNCTION longer memory persistence. time gate USED-FOR longer memory persistence. model USED-FOR long temporal dependencies. model COMPARE LSTM. LSTM COMPARE model. non - optimal initialization USED-FOR time gate parameters. temporal curriculum learning schedule USED-FOR g - LSTM. computational budget term USED-FOR network. convergence time EVALUATE-FOR LSTM. computational budget term USED-FOR training loss. long sequences EVALUATE-FOR LSTM. Task are vanishing gradient problem, and network update. OtherScientificTerm are neuron, and neuron state. ","Recurrent neural networks are well known to suffer from the vanishing gradient problem on long sequence data. This paper proposes two methods to reduce RNN state updates in order to reduce the memory of the network. These two methods are implemented in different architectures and architectures. The authors propose a timing-gated LSTM RNN model for reducing state updates. The key idea is to add a time gate to each neuron to encourage longer memory persistence and error-gradient flow. The proposed g-LSTM is trained with a temporal curriculum learning schedule, where the time gate parameters are updated according to a non-optimal initialization. The paper shows that the proposed model is more robust to long temporal dependencies than the standard LSTMs, and that the convergence time of the g-lSTM can be faster than that of the standard lSTM with the same computational budget term for training the network on long sequences.  ","Recurrent neural networks are well known to suffer from the vanishing gradient problem on long sequence data. This paper proposes two methods to reduce RNN state updates in order to reduce the memory of the network. These two methods are implemented in different architectures and architectures. The authors propose a timing-gated LSTM RNN model for reducing state updates. The key idea is to add a time gate to each neuron to encourage longer memory persistence and error-gradient flow. The proposed g-LSTM is trained with a temporal curriculum learning schedule, where the time gate parameters are updated according to a non-optimal initialization. The paper shows that the proposed model is more robust to long temporal dependencies than the standard LSTMs, and that the convergence time of the g-lSTM can be faster than that of the standard lSTM with the same computational budget term for training the network on long sequences.  "
6800,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"ensembling USED-FOR first and second order deep feature statistics. re - training CONJUNCTION pre - processing. pre - processing CONJUNCTION re - training. pre - processing CONJUNCTION model. model CONJUNCTION pre - processing. method COMPARE state - of - the - art. state - of - the - art COMPARE method. benchmarking tasks EVALUATE-FOR state - of - the - art. benchmarking tasks EVALUATE-FOR method. TinyImageNet resize FEATURE-OF out - of - distribution dataset. true negative rate EVALUATE-FOR method. DenseNet USED-FOR method. DenseNet USED-FOR in - distribution ( CIFAR-100 ). Method are deep neural networks, and plug - and - play detection procedure. OtherScientificTerm is feature maps. ","This paper proposes a plug-and-play detection method to detect out-of-distribution (OOD) samples from deep neural networks. The proposed method is based on the idea of ensembling first and second order deep feature statistics, where the feature maps are updated during training. The authors show that the proposed method achieves a true negative rate of 0.1% for the out-odds detection and 0.2% negative rate for the in-distributions detection. The method is tested on several benchmarking tasks and shows that the method outperforms the state- of-the-art in terms of the number of samples detected and the performance of the model. In addition, the authors also show that DenseNet can be used to detect the out of distribution (CIFAR-100) samples for the method, and that the true negatives of the method can be detected in the presence of re-training, pre-processing, and pre-training. Finally, the method is also tested on the TinyImageNet resize of an existing dataset, and it is shown to be able to detect an out-out of distribution dataset with a small amount of data.    The authors also provide a thorough ablation study on the effectiveness of the proposed Plug-andPlay detection procedure, and show that their method is able to identify the true negative rates of out-OOD samples.","This paper proposes a plug-and-play detection method to detect out-of-distribution (OOD) samples from deep neural networks. The proposed method is based on the idea of ensembling first and second order deep feature statistics, where the feature maps are updated during training. The authors show that the proposed method achieves a true negative rate of 0.1% for the out-odds detection and 0.2% negative rate for the in-distributions detection. The method is tested on several benchmarking tasks and shows that the method outperforms the state- of-the-art in terms of the number of samples detected and the performance of the model. In addition, the authors also show that DenseNet can be used to detect the out of distribution (CIFAR-100) samples for the method, and that the true negatives of the method can be detected in the presence of re-training, pre-processing, and pre-training. Finally, the method is also tested on the TinyImageNet resize of an existing dataset, and it is shown to be able to detect an out-out of distribution dataset with a small amount of data.    The authors also provide a thorough ablation study on the effectiveness of the proposed Plug-andPlay detection procedure, and show that their method is able to identify the true negative rates of out-OOD samples."
6804,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,model recovery USED-FOR data classification. neural network USED-FOR weight vectors. Gaussian inputs USED-FOR empirical risk function. cross entropy USED-FOR empirical risk function. gradient descent USED-FOR one - hidden - layer neural networks. near - optimal sample CONJUNCTION computational complexity. computational complexity CONJUNCTION near - optimal sample. global convergence guarantee USED-FOR empirical risk minimization. computational complexity EVALUATE-FOR network input dimension. gradient descent USED-FOR cross entropy. cross entropy USED-FOR global convergence guarantee. cross entropy USED-FOR empirical risk minimization. gradient descent USED-FOR global convergence guarantee. gradient descent USED-FOR empirical risk minimization. OtherScientificTerm is sigmoid activations. Metric is sample complexity. Method is tensor method. ,"This paper studies the problem of model recovery for data classification, where the neural network is trained to recover the weight vectors of the weights of a given neural network with sigmoid activations. The authors show that the empirical risk function on Gaussian inputs can be approximated by the cross entropy between the weights and weights of one-hidden-layer neural networks trained with gradient descent. They also provide a global convergence guarantee for empirical risk minimization based on cross entropy, which shows that gradient descent converges to a near-optimal sample and a computational complexity that scales linearly with the network input dimension.    The authors also provide an analysis of the sample complexity of the tensor method, showing that the gradient descent can be used to approximate cross entropy in order to obtain a global converging guarantee for the case where the number of weights is small.","This paper studies the problem of model recovery for data classification, where the neural network is trained to recover the weight vectors of the weights of a given neural network with sigmoid activations. The authors show that the empirical risk function on Gaussian inputs can be approximated by the cross entropy between the weights and weights of one-hidden-layer neural networks trained with gradient descent. They also provide a global convergence guarantee for empirical risk minimization based on cross entropy, which shows that gradient descent converges to a near-optimal sample and a computational complexity that scales linearly with the network input dimension.    The authors also provide an analysis of the sample complexity of the tensor method, showing that the gradient descent can be used to approximate cross entropy in order to obtain a global converging guarantee for the case where the number of weights is small."
6808,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"method USED-FOR salient convolutional channels. convolutional layers USED-FOR features. small auxiliary connections PART-OF convolutional layers. small auxiliary connections USED-FOR FBS. it USED-FOR convolution. channel pruning methods COMPARE it. it COMPARE channel pruning methods. it USED-FOR full network structures. it USED-FOR CNNs. stochastic gradient descent USED-FOR FBS - augmented networks. channel pruning CONJUNCTION dynamic execution schemes. dynamic execution schemes CONJUNCTION channel pruning. FBS COMPARE channel pruning. channel pruning COMPARE FBS. FBS COMPARE dynamic execution schemes. dynamic execution schemes COMPARE FBS. ImageNet classification EVALUATE-FOR FBS. VGG-16 CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION VGG-16. VGG-16 EVALUATE-FOR FBS. ResNet-18 EVALUATE-FOR FBS. Method are deep convolutional neural networks, and feature boosting and suppression ( FBS ). Metric are computational and memory resources, and top-5 accuracy loss. OtherScientificTerm is channels. ","This paper proposes feature boosting and suppression (FBS), a method to remove salient convolutional channels from deep convolution neural networks to save computational and memory resources. FBS removes small auxiliary connections in the convolutionsal layers to remove features that are salient to the current layer. Unlike previous channel pruning methods, it does not require the top-5 accuracy loss, and it can be applied to any convolution. The authors show that FBS-augmented networks can be trained with stochastic gradient descent. ImageNet classification results on VGG-16 and ResNet-18 show that the proposed FBS outperforms previous work in terms of performance, and that it can also be used to augment the full network structures of CNNs. Experiments are conducted to compare the performance of FBS with that of previous work, and show that it outperforms both channel and dynamic execution schemes. ","This paper proposes feature boosting and suppression (FBS), a method to remove salient convolutional channels from deep convolution neural networks to save computational and memory resources. FBS removes small auxiliary connections in the convolutionsal layers to remove features that are salient to the current layer. Unlike previous channel pruning methods, it does not require the top-5 accuracy loss, and it can be applied to any convolution. The authors show that FBS-augmented networks can be trained with stochastic gradient descent. ImageNet classification results on VGG-16 and ResNet-18 show that the proposed FBS outperforms previous work in terms of performance, and that it can also be used to augment the full network structures of CNNs. Experiments are conducted to compare the performance of FBS with that of previous work, and show that it outperforms both channel and dynamic execution schemes. "
6812,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,mixed Nash Equilibria ( NE ) perspective FEATURE-OF Generative Adversarial Networks ( GANs ). algorithmic framework USED-FOR GANs. prox methods USED-FOR algorithmic framework. infinite - dimensional two - player game USED-FOR algorithmic framework. infinite - dimensional two - player game USED-FOR GANs. procedure USED-FOR prox methods. sampling routines USED-FOR prox methods. approach COMPARE methods. methods COMPARE approach. Adam CONJUNCTION RMSProp. RMSProp CONJUNCTION Adam. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. methods USED-FOR pure strategy equilibria. SGD HYPONYM-OF pure strategy equilibria. quality EVALUATE-FOR approach. OtherScientificTerm is mixed NE. ,"This paper studies the mixed Nash Equilibria (NE) perspective of Generative Adversarial Networks (GANs) from a mixed NE perspective. The authors propose a new algorithmic framework based on prox methods for GANs in an infinite-dimensional two-player game. The main contribution of the paper is a procedure that allows prox methods to be applied to any sampling routines. The approach is shown to outperform existing methods for finding pure strategy equilibria such as SGD, Adam, and RMSProp. The quality of the proposed approach is also demonstrated. ","This paper studies the mixed Nash Equilibria (NE) perspective of Generative Adversarial Networks (GANs) from a mixed NE perspective. The authors propose a new algorithmic framework based on prox methods for GANs in an infinite-dimensional two-player game. The main contribution of the paper is a procedure that allows prox methods to be applied to any sampling routines. The approach is shown to outperform existing methods for finding pure strategy equilibria such as SGD, Adam, and RMSProp. The quality of the proposed approach is also demonstrated. "
6816,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"deep neural networks USED-FOR method. approach USED-FOR model. pretrained network USED-FOR problems. depth - wise convolutions HYPONYM-OF low - parameter layers. approach USED-FOR sequential transfer learning. multi - task learning problems EVALUATE-FOR logits - only fine - tuning. Generic are task, and network. Method are Single Shot MultiBox Detection ( SSD ) model, 1000 - class image classification model, and SSD feature extractor. Metric is transfer - learning accuracy. OtherScientificTerm is singletask. ","This paper proposes a method for transfer learning from a single task to multiple tasks. The proposed method is based on deep neural networks. The authors propose a Single Shot MultiBox Detection (SSD) model, where a 1000-class image classification model is pretrained on a single dataset and then a new task is added to the dataset, and the network is fine-tuned to solve the new task. This approach is shown to improve the performance of the model on a number of different problems. The key idea is to use low-parameter layers (e.g. depth-wise convolutions) to improve transfer-learning accuracy, and then fine-tune the network on the new problem. The approach is also applied to sequential transfer learning, where the new problems are solved by a pretrained network. The results show that the proposed approach improves the performance on multi-task learning problems, especially on logits-only fine- tuning. The main contribution of the paper is the SSD feature extractor, which is trained on the original dataset.  ","This paper proposes a method for transfer learning from a single task to multiple tasks. The proposed method is based on deep neural networks. The authors propose a Single Shot MultiBox Detection (SSD) model, where a 1000-class image classification model is pretrained on a single dataset and then a new task is added to the dataset, and the network is fine-tuned to solve the new task. This approach is shown to improve the performance of the model on a number of different problems. The key idea is to use low-parameter layers (e.g. depth-wise convolutions) to improve transfer-learning accuracy, and then fine-tune the network on the new problem. The approach is also applied to sequential transfer learning, where the new problems are solved by a pretrained network. The results show that the proposed approach improves the performance on multi-task learning problems, especially on logits-only fine- tuning. The main contribution of the paper is the SSD feature extractor, which is trained on the original dataset.  "
6820,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"Normalization methods PART-OF deep learning toolbox. batch normalization ( BN ) HYPONYM-OF normalization method. method COMPARE BN. BN COMPARE method. method COMPARE normalization techniques. normalization techniques COMPARE method. BN CONJUNCTION normalization techniques. normalization techniques CONJUNCTION BN. single and multi - task datasets EVALUATE-FOR method. Generic are They, and approach. OtherScientificTerm are manually tuned learning rate schedules, and multi - modal distributions. Method is normalization. ",Normalization methods are an important part of the deep learning toolbox. They can be seen as an extension of batch normalization (BN) which is a popular normalization method. The paper proposes a new approach to normalize the training data using manually tuned learning rate schedules. The authors show that the proposed method outperforms BN and other normalization techniques on both single and multi-task datasets. They also show that normalization can be applied to multi-modal distributions as well.,Normalization methods are an important part of the deep learning toolbox. They can be seen as an extension of batch normalization (BN) which is a popular normalization method. The paper proposes a new approach to normalize the training data using manually tuned learning rate schedules. The authors show that the proposed method outperforms BN and other normalization techniques on both single and multi-task datasets. They also show that normalization can be applied to multi-modal distributions as well.
6824,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"pruning technique USED-FOR subnetworks. test accuracy EVALUATE-FOR network. OtherScientificTerm are lottery ticket hypothesis, winning tickets, and initialization lottery. Task is training. ","This paper investigates the lottery ticket hypothesis, which claims that winning tickets are not available in the initialization lottery. The authors propose a pruning technique that prunes subnetworks that are not winning tickets. They show that the test accuracy of the network trained with this pruning method can be significantly improved. They also show that training with pruning is more efficient than without. ","This paper investigates the lottery ticket hypothesis, which claims that winning tickets are not available in the initialization lottery. The authors propose a pruning technique that prunes subnetworks that are not winning tickets. They show that the test accuracy of the network trained with this pruning method can be significantly improved. They also show that training with pruning is more efficient than without. "
6828,SP:08c662296c7cf346f027e462d29184275fd6a102,"exploration FEATURE-OF reinforcement learning. attentive dynamics model ( ADM ) USED-FOR controllable elements of the observations. state representation USED-FOR exploration purposes. contingency information USED-FOR state representation. contingency information USED-FOR exploration purposes. high - level information CONJUNCTION supervisory data. supervisory data CONJUNCTION high - level information. actor - critic algorithm CONJUNCTION count - based exploration. count - based exploration CONJUNCTION actor - critic algorithm. expert demonstrations CONJUNCTION high - level information. high - level information CONJUNCTION expert demonstrations. representation USED-FOR count - based exploration. RAM states HYPONYM-OF high - level information. representation USED-FOR actor - critic algorithm. contingency - awareness USED-FOR exploration problems. exploration problems PART-OF reinforcement learning. contingency - awareness USED-FOR reinforcement learning. Method are Arcade Learning Element ( ALE ), and ADM. Material are Atari games, and MONTEZUMA ’S REVENGE. ","This paper proposes a new method for exploration in reinforcement learning based on the Arcade Learning Element (ALE). The authors propose an attentive dynamics model (ADM) to capture the controllable elements of the observations and use this state representation for exploration purposes using the contingency information. The authors show that the representation can be used for both actor-critic algorithm and count-based exploration. The representation is trained using expert demonstrations, high-level information (e.g., RAM states) and supervisory data. The paper also shows that the ADM is able to learn a representation that can be applied to different exploration problems in the context of reinforcement learning with the use of the contingency-awareness. Experiments are conducted on two Atari games and MONTEZUMA’s REVENGE. ","This paper proposes a new method for exploration in reinforcement learning based on the Arcade Learning Element (ALE). The authors propose an attentive dynamics model (ADM) to capture the controllable elements of the observations and use this state representation for exploration purposes using the contingency information. The authors show that the representation can be used for both actor-critic algorithm and count-based exploration. The representation is trained using expert demonstrations, high-level information (e.g., RAM states) and supervisory data. The paper also shows that the ADM is able to learn a representation that can be applied to different exploration problems in the context of reinforcement learning with the use of the contingency-awareness. Experiments are conducted on two Atari games and MONTEZUMA’s REVENGE. "
6832,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"HyperGAN HYPONYM-OF generative network. generative network USED-FOR weight parameters. weight parameters PART-OF deep neural networks. generative network USED-FOR deep neural networks. HyperGAN USED-FOR latent space. HyperGAN USED-FOR low dimensional noise. architecture COMPARE generative adversarial networks. generative adversarial networks COMPARE architecture. generated network parameter distribution CONJUNCTION unknown true parameter distribution. unknown true parameter distribution CONJUNCTION generated network parameter distribution. KL - divergence FEATURE-OF generated network parameter distribution. KL - divergence FEATURE-OF unknown true parameter distribution. HyperGAN USED-FOR classification. HyperGAN COMPARE fully supervised learning. fully supervised learning COMPARE HyperGAN. HyperGAN USED-FOR MNIST and CIFAR-10 datasets. HyperGAN COMPARE ensembles. ensembles COMPARE HyperGAN. HyperGAN USED-FOR uncertainty. uncertainty EVALUATE-FOR ensembles. HyperGAN - generated ensembles USED-FOR out of distribution data. out of distribution data CONJUNCTION adversarial examples. adversarial examples CONJUNCTION out of distribution data. HyperGAN - generated ensembles USED-FOR adversarial examples. HyperGAN USED-FOR uncertainty estimates. OtherScientificTerm are classification loss, and rich distribution of effective parameters. Material is inlier data. ","This paper proposes HyperGAN, a generative network that generates weight parameters for deep neural networks. HyperGAN is able to generate low dimensional noise in the latent space. The architecture is similar to existing generative adversarial networks, but the classification loss is much smaller. The authors show that HyperGAN can be used for classification and adversarial examples, and can be trained with a rich distribution of effective parameters. The proposed architecture is tested on MNIST and CIFAR-10 datasets, where HyperGAN outperforms fully supervised learning, and outperforms ensembles in terms of uncertainty on out of distribution data. The main contribution of this paper is to use HyperGAN to generate uncertainty estimates for uncertainty estimates, and to use the generated network parameter distribution with respect to the KL-divergence to the unknown true parameter distribution.  The authors also show that the proposed HyperGAN-generated ensembled can also be used to generate adversarial data, which is useful in cases where there is no inlier data. ","This paper proposes HyperGAN, a generative network that generates weight parameters for deep neural networks. HyperGAN is able to generate low dimensional noise in the latent space. The architecture is similar to existing generative adversarial networks, but the classification loss is much smaller. The authors show that HyperGAN can be used for classification and adversarial examples, and can be trained with a rich distribution of effective parameters. The proposed architecture is tested on MNIST and CIFAR-10 datasets, where HyperGAN outperforms fully supervised learning, and outperforms ensembles in terms of uncertainty on out of distribution data. The main contribution of this paper is to use HyperGAN to generate uncertainty estimates for uncertainty estimates, and to use the generated network parameter distribution with respect to the KL-divergence to the unknown true parameter distribution.  The authors also show that the proposed HyperGAN-generated ensembled can also be used to generate adversarial data, which is useful in cases where there is no inlier data. "
6836,SP:230b3e008e687e03a8b914084b93fc81609051c0,Variational Auto Encoder ( VAE ) HYPONYM-OF generative latent variable model. generative latent variable model USED-FOR representation learning. Variational Auto Encoder ( VAE ) USED-FOR representation learning. continuous valued latent variables USED-FOR VAEs. differentiable estimate USED-FOR ELBO. reparametrized sampling USED-FOR differentiable estimate. Stochastic Gradient Descend ( SGD ) USED-FOR it. discrete valued latent variables USED-FOR VAEs. binary or categorically valued latent representations USED-FOR VAEs. differentiable estimator USED-FOR ELBO. importance sampling USED-FOR differentiable estimator. benchmark datasets EVALUATE-FOR VAEs architectures. Bernoulli and Categorically distributed latent representations USED-FOR VAEs architectures. variational auto encoder ( VAE ) HYPONYM-OF generative model. it HYPONYM-OF generative model. It USED-FOR model. VAE USED-FOR tasks. data generation CONJUNCTION data interpolation. data interpolation CONJUNCTION data generation. density estimation CONJUNCTION data generation. data generation CONJUNCTION density estimation. data interpolation CONJUNCTION outlier and anomaly detection. outlier and anomaly detection CONJUNCTION data interpolation. VAE USED-FOR density estimation. outlier and anomaly detection CONJUNCTION clustering. clustering CONJUNCTION outlier and anomaly detection. VAE USED-FOR data generation. VAE USED-FOR outlier and anomaly detection. density estimation HYPONYM-OF tasks. data interpolation HYPONYM-OF tasks. clustering HYPONYM-OF tasks. outlier and anomaly detection HYPONYM-OF tasks. data generation HYPONYM-OF tasks. VAE HYPONYM-OF latent variable model. Generic is approach. Method is VARIATIONAL AUTO ENCODER. OtherScientificTerm is nonlinear dependent elements. ,"This paper proposes Variational Auto Encoder (VAE), a generative latent variable model for representation learning, which is a variational auto encoder (variational auto-encoder). VAEs are continuous valued latent variables, which can be represented as binary or categorically valued latent representations. The paper proposes a differentiable estimate of the ELBO based on reparametrized sampling, and uses it with Stochastic Gradient Descend (SGD) to train the model.   The paper shows that VAEs can be trained with discrete valued latent variable, and that the VAE can be learned with binary or categorical values of the discrete valued variables. They also show that a VAE trained with VAEs architectures based on Bernoulli and Categorically distributed latent representations outperforms the state-of-the-art on several benchmark datasets.  They also propose an approach called VARIATIONAL AUTO ENCODER, which uses an important differentiable estimator based on importance sampling to learn a VAEs with nonlinear dependent elements. It can be used to train a model for a variety of tasks, including density estimation, data generation, data interpolation, outlier and anomaly detection, and clustering. ","This paper proposes Variational Auto Encoder (VAE), a generative latent variable model for representation learning, which is a variational auto encoder (variational auto-encoder). VAEs are continuous valued latent variables, which can be represented as binary or categorically valued latent representations. The paper proposes a differentiable estimate of the ELBO based on reparametrized sampling, and uses it with Stochastic Gradient Descend (SGD) to train the model.   The paper shows that VAEs can be trained with discrete valued latent variable, and that the VAE can be learned with binary or categorical values of the discrete valued variables. They also show that a VAE trained with VAEs architectures based on Bernoulli and Categorically distributed latent representations outperforms the state-of-the-art on several benchmark datasets.  They also propose an approach called VARIATIONAL AUTO ENCODER, which uses an important differentiable estimator based on importance sampling to learn a VAEs with nonlinear dependent elements. It can be used to train a model for a variety of tasks, including density estimation, data generation, data interpolation, outlier and anomaly detection, and clustering. "
6840,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,feed forward neural network COMPARE training approaches. training approaches COMPARE feed forward neural network. robustness EVALUATE-FOR training approaches. robustness EVALUATE-FOR feed forward neural network. mean field description of a Boltzmann machine USED-FOR pre - trained building block. MNIST dataset EVALUATE-FOR method. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. adversarial resistance EVALUATE-FOR method. OtherScientificTerm is adversarial attacks. Method is Boltzmann machine. ,"This paper proposes a new method to improve the robustness of a feed forward neural network compared to other training approaches. The key idea is to use the mean field description of a Boltzmann machine as a pre-trained building block, which is more robust to adversarial attacks. The proposed method is evaluated on the MNIST dataset, and the results show that the proposed method achieves better adversarial resistance compared to existing methods. The paper also shows that data augmentation and adversarial training can be used to improve robustness. ","This paper proposes a new method to improve the robustness of a feed forward neural network compared to other training approaches. The key idea is to use the mean field description of a Boltzmann machine as a pre-trained building block, which is more robust to adversarial attacks. The proposed method is evaluated on the MNIST dataset, and the results show that the proposed method achieves better adversarial resistance compared to existing methods. The paper also shows that data augmentation and adversarial training can be used to improve robustness. "
6844,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,"visible region PART-OF minimal image. deep neural networks ( DNNs ) COMPARE DNNs. DNNs COMPARE deep neural networks ( DNNs ). object location FEATURE-OF DNNs. DNN recognition ability FEATURE-OF natural images. robustness EVALUATE-FOR DNNs. natural images FEATURE-OF DNNs. Material is Minimal images. Metric are human recognition accuracy, and accuracy. OtherScientificTerm are invariance, and adversarial patterns. ","Minimal images are an important tool for improving human recognition accuracy. This paper shows that deep neural networks (DNNs) are more robust to adversarial attacks than DNNs. Minimal images contain a visible region in the minimal image that is invariant to changes in the object location. The paper also shows that the invariance is not a result of adversarial patterns, but rather of the DNN recognition ability on natural images. The authors also show that the robustness of the natural images produced by DNN is correlated with the accuracy.","Minimal images are an important tool for improving human recognition accuracy. This paper shows that deep neural networks (DNNs) are more robust to adversarial attacks than DNNs. Minimal images contain a visible region in the minimal image that is invariant to changes in the object location. The paper also shows that the invariance is not a result of adversarial patterns, but rather of the DNN recognition ability on natural images. The authors also show that the robustness of the natural images produced by DNN is correlated with the accuracy."
6848,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"multi - agent reinforcement learning ( MARL ) USED-FOR optimal collaboration. worker agents HYPONYM-OF self - interested agents. super agent USED-FOR them. super agent USED-FOR optimal coordination. manager HYPONYM-OF super agent. agent modeling CONJUNCTION policy learning. policy learning CONJUNCTION agent modeling. approach USED-FOR multi - agent management problems. environments EVALUATE-FOR approach. Resource Collection and Crafting EVALUATE-FOR approach. Resource Collection and Crafting HYPONYM-OF environments. approach USED-FOR optimal ad - hoc teaming. approach USED-FOR worker agents ’ minds. generalization FEATURE-OF optimal ad - hoc teaming. OtherScientificTerm are policy, and contracts. Generic is agents. Task is ad - hoc worker teaming. ","This paper proposes a multi-agent reinforcement learning (MARL) approach for optimal collaboration between agents. The authors propose to train self-interested agents (i.e., worker agents) and then train a super agent to coordinate with them. The super agent, called a manager, is trained to learn optimal coordination between agents and to learn a policy. The proposed approach is tested on two environments, called Resource Collection and Crafting, and is shown to improve the generalization performance of ad-hoc worker teaming. It is shown that the proposed approach can learn worker agents’ minds, and that the agent modeling and policy learning can be further improved. The paper also shows that the approach can be applied to multi-manual management problems, where agents are given a set of contracts, and the super agent can learn to optimize the optimal coordination.  ","This paper proposes a multi-agent reinforcement learning (MARL) approach for optimal collaboration between agents. The authors propose to train self-interested agents (i.e., worker agents) and then train a super agent to coordinate with them. The super agent, called a manager, is trained to learn optimal coordination between agents and to learn a policy. The proposed approach is tested on two environments, called Resource Collection and Crafting, and is shown to improve the generalization performance of ad-hoc worker teaming. It is shown that the proposed approach can learn worker agents’ minds, and that the agent modeling and policy learning can be further improved. The paper also shows that the approach can be applied to multi-manual management problems, where agents are given a set of contracts, and the super agent can learn to optimize the optimal coordination.  "
6852,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"time series USED-FOR recurrent neural network ( RNN ) solutions. asynchronous time series HYPONYM-OF series. unified RNN USED-FOR feature types. RNN framework USED-FOR sequential features. time features USED-FOR cell ’s memory state. sequential level FEATURE-OF time features. Method are RNN cells, and modeling framework. OtherScientificTerm are sparse and dense features, static ( whole sequence level ) features, and encoder output. Task is cell updates. ","This paper proposes a new way to model recurrent neural network (RNN) solutions for time series (asynchronous time series, i.e. asynchronous time series). In particular, the authors consider RNN cells that have sparse and dense features. The authors propose a unified RNN that can handle both feature types, which allows them to model both static (whole sequence level) features and dynamic (continuous) features.   The authors also propose a new modeling framework, which is based on the RNN framework for learning sequential features on the sequential level. The cell’s memory state is modeled as a function of time features, and cell updates are modeled as the sum of these time features and the encoder output. ","This paper proposes a new way to model recurrent neural network (RNN) solutions for time series (asynchronous time series, i.e. asynchronous time series). In particular, the authors consider RNN cells that have sparse and dense features. The authors propose a unified RNN that can handle both feature types, which allows them to model both static (whole sequence level) features and dynamic (continuous) features.   The authors also propose a new modeling framework, which is based on the RNN framework for learning sequential features on the sequential level. The cell’s memory state is modeled as a function of time features, and cell updates are modeled as the sum of these time features and the encoder output. "
6856,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"Method are neural model, feedforward neural network, and recurrent neural networks. OtherScientificTerm are propositional formula, and propositional atoms. Generic are network, and model. ","This paper studies the problem of learning a neural model that is able to learn a propositional formula, i.e., a set of propositional atoms. The authors propose a feedforward neural network, which is a generalization of a previous work (Zhang et al., 2017). The authors show that this network can learn propositional formulas that can be represented as a sequence of atoms, and that the model can be trained to learn such a sequence. They also show that recurrent neural networks can learn such sequences of atoms. ","This paper studies the problem of learning a neural model that is able to learn a propositional formula, i.e., a set of propositional atoms. The authors propose a feedforward neural network, which is a generalization of a previous work (Zhang et al., 2017). The authors show that this network can learn propositional formulas that can be represented as a sequence of atoms, and that the model can be trained to learn such a sequence. They also show that recurrent neural networks can learn such sequences of atoms. "
6860,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,random mini - batches USED-FOR Training neural networks. accuracy EVALUATE-FOR network. speed of learning EVALUATE-FOR network. difficulty FEATURE-OF mini - batches. curriculum learning USED-FOR problem. CNNs USED-FOR image recognition. CIFAR-10 CONJUNCTION CIFAR-100 datasets. CIFAR-100 datasets CONJUNCTION CIFAR-10. performance EVALUATE-FOR small and competitive networks. learning speed EVALUATE-FOR small and competitive networks. competitive ” teacher ” network USED-FOR transfer learning. Imagenet database USED-FOR competitive ” teacher ” network. CIFAR-100 datasets EVALUATE-FOR small and competitive networks. CIFAR-10 EVALUATE-FOR small and competitive networks. transfer learning USED-FOR difficulty. approach COMPARE Self - Paced Learning. Self - Paced Learning COMPARE approach. Metric is difficulty measure. OtherScientificTerm is ” teacher ” network. Generic is method. ,"Training neural networks with random mini-batches is a popular technique to improve the accuracy and speed of learning. However, the difficulty measure is expensive to compute. To address this problem, the authors propose to use curriculum learning, where a “teacher” network is trained to predict the difficulty of a mini-batch based on the current state of the problem. The authors show that CNNs trained with curriculum learning can achieve state-of-the-art performance on image recognition tasks. The performance of small and competitive networks trained with this approach outperforms Self-Paced Learning in terms of accuracy and learning speed on CIFAR-10 and the CifAR-100 datasets. They also show that transfer learning with a competitive ”teacher ” network trained on the Imagenet database can also improve the performance. ","Training neural networks with random mini-batches is a popular technique to improve the accuracy and speed of learning. However, the difficulty measure is expensive to compute. To address this problem, the authors propose to use curriculum learning, where a “teacher” network is trained to predict the difficulty of a mini-batch based on the current state of the problem. The authors show that CNNs trained with curriculum learning can achieve state-of-the-art performance on image recognition tasks. The performance of small and competitive networks trained with this approach outperforms Self-Paced Learning in terms of accuracy and learning speed on CIFAR-10 and the CifAR-100 datasets. They also show that transfer learning with a competitive ”teacher ” network trained on the Imagenet database can also improve the performance. "
6864,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"generalization guarantees FEATURE-OF neural networks. pre - activation values FEATURE-OF network. Method are overparameterized deep networks, stochastic gradient descent ( SGD ), and PAC - Bayesian framework. OtherScientificTerm are small random noise, weight matrices, wide training loss minimum, and wide test loss minimum. Generic are approach, framework, matrices, and prior approaches. Metric is generalization guarantee. ","This paper studies the generalization guarantees of neural networks with overparameterized deep networks. In particular, the authors focus on stochastic gradient descent (SGD) under the PAC-Bayesian framework, where small random noise is added to the weight matrices of the weights of the network. The authors show that under this approach, a wide training loss minimum can be obtained, and that the network with large pre-activation values can be trained with wide test loss minimum. They also show that this approach can be extended to the case where the matrices are overparametrized. Finally, they show that their generalization guarantee is robust to the size of the training set. ","This paper studies the generalization guarantees of neural networks with overparameterized deep networks. In particular, the authors focus on stochastic gradient descent (SGD) under the PAC-Bayesian framework, where small random noise is added to the weight matrices of the weights of the network. The authors show that under this approach, a wide training loss minimum can be obtained, and that the network with large pre-activation values can be trained with wide test loss minimum. They also show that this approach can be extended to the case where the matrices are overparametrized. Finally, they show that their generalization guarantee is robust to the size of the training set. "
6868,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"Deep neural networks USED-FOR symbolic reasoning. Deep neural networks USED-FOR learning abstractions. symbolic reasoning CONJUNCTION learning abstractions. learning abstractions CONJUNCTION symbolic reasoning. discrete latent variables FEATURE-OF Deep neural networks. perplexity EVALUATE-FOR VAE. CIFAR-10 EVALUATE-FOR VAE. CIFAR-10 HYPONYM-OF datasets. datasets EVALUATE-FOR VAE. training technique USED-FOR VQ - VAE. it CONJUNCTION sequence level knowledge distillation. sequence level knowledge distillation CONJUNCTION it. nonautoregressive machine translation model COMPARE greedy autoregressive baseline Transformer. greedy autoregressive baseline Transformer COMPARE nonautoregressive machine translation model. accuracy EVALUATE-FOR greedy autoregressive baseline Transformer. it USED-FOR nonautoregressive machine translation model. sequence level knowledge distillation USED-FOR nonautoregressive machine translation model. accuracy EVALUATE-FOR nonautoregressive machine translation model. EM USED-FOR discrete autoencoder. Method are discrete latent variable models, vector quantized autoencoders ( VQ - VAE ), and Expectation Maximization ( EM ) algorithm. Task is inference. ","This paper studies the perplexity of discrete latent variable models. Deep neural networks with discrete latent variables are well-known to be useful for symbolic reasoning and learning abstractions. This paper proposes a new training technique called vector quantized autoencoders (VQ-VAE), which is based on the Expectation Maximization (EM) algorithm. Experiments on CIFAR-10 and ImageNet datasets show that VAE achieves the best perplexity on both datasets. The paper also proposes a training technique to improve the performance of VAE. In particular, it combines it with sequence level knowledge distillation to train a nonautoregressive machine translation model that achieves better accuracy than the greedy autoregressive baseline Transformer. The authors also show that EM can be applied to any discrete autoencoder. ","This paper studies the perplexity of discrete latent variable models. Deep neural networks with discrete latent variables are well-known to be useful for symbolic reasoning and learning abstractions. This paper proposes a new training technique called vector quantized autoencoders (VQ-VAE), which is based on the Expectation Maximization (EM) algorithm. Experiments on CIFAR-10 and ImageNet datasets show that VAE achieves the best perplexity on both datasets. The paper also proposes a training technique to improve the performance of VAE. In particular, it combines it with sequence level knowledge distillation to train a nonautoregressive machine translation model that achieves better accuracy than the greedy autoregressive baseline Transformer. The authors also show that EM can be applied to any discrete autoencoder. "
6872,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"Multi - view learning USED-FOR self - supervision. Distributional hypothesis USED-FOR self - supervision. multi - view frameworks USED-FOR sentence representations. learning architectures USED-FOR sentence meaning. generative objective CONJUNCTION discriminative one. discriminative one CONJUNCTION generative objective. generative objective USED-FOR framework. multi - view frameworks COMPARE single - view learnt counterparts. single - view learnt counterparts COMPARE multi - view frameworks. multi - view frameworks USED-FOR representations. single - view learnt counterparts USED-FOR representations. Material is large unlabelled corpora. Generic are frameworks, and representation. Method are Recurrent Neural Network ( RNN ), and linear model. Task is downstream tasks. ","Multi-view learning is a popular technique for self-supervision based on the Distributional hypothesis. This paper proposes to use multi-view frameworks to learn sentence representations from large unlabelled corpora. The framework is based on a generative objective and a discriminative one. The authors propose two learning architectures to learn the sentence meaning from multiple views. One is a Recurrent Neural Network (RNN) that takes as input two views of the same sentence, while the other is a linear model that takes the same view as the previous one.  The authors show that the proposed framework is able to learn representations that are more interpretable than the single-view learnt counterparts. They also show that these representations are more expressive than the representations learned from single-views. The paper also shows that the representation learned from multi-views can be used for downstream tasks.  ","Multi-view learning is a popular technique for self-supervision based on the Distributional hypothesis. This paper proposes to use multi-view frameworks to learn sentence representations from large unlabelled corpora. The framework is based on a generative objective and a discriminative one. The authors propose two learning architectures to learn the sentence meaning from multiple views. One is a Recurrent Neural Network (RNN) that takes as input two views of the same sentence, while the other is a linear model that takes the same view as the previous one.  The authors show that the proposed framework is able to learn representations that are more interpretable than the single-view learnt counterparts. They also show that these representations are more expressive than the representations learned from single-views. The paper also shows that the representation learned from multi-views can be used for downstream tasks.  "
6876,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"Distributed optimization USED-FOR large - scale machine learning problems. stragglers HYPONYM-OF slow nodes. Anytime Minibatch HYPONYM-OF online distributed optimization method. fixed communication time FEATURE-OF minibatch gradients. consensus USED-FOR minibatch gradients. dual averaging USED-FOR primal variables. approach COMPARE it. it COMPARE approach. Amazon EC2 EVALUATE-FOR approach. Method are distributed optimization techniques, and convergence analysis. OtherScientificTerm are gradients, and compute node performance. Metric is wall time. ","Distributed optimization is an important problem in large-scale machine learning problems due to slow nodes (i.e. stragglers) and high communication costs. In this paper, the authors propose a new online distributed optimization method called Anytime Minibatch, which is an extension of existing distributed optimization techniques. The authors provide a convergence analysis and show that the minibatch gradients with fixed communication time can be computed with a fixed number of nodes, and that the gradients can be shared across all the nodes in the network. They also provide a consensus to compute minib batch gradients across all nodes, which can be used to improve the compute node performance. The proposed approach is based on dual averaging for primal variables, and it is evaluated on Amazon EC2 and shows significant improvements in wall time compared to previous work.","Distributed optimization is an important problem in large-scale machine learning problems due to slow nodes (i.e. stragglers) and high communication costs. In this paper, the authors propose a new online distributed optimization method called Anytime Minibatch, which is an extension of existing distributed optimization techniques. The authors provide a convergence analysis and show that the minibatch gradients with fixed communication time can be computed with a fixed number of nodes, and that the gradients can be shared across all the nodes in the network. They also provide a consensus to compute minib batch gradients across all nodes, which can be used to improve the compute node performance. The proposed approach is based on dual averaging for primal variables, and it is evaluated on Amazon EC2 and shows significant improvements in wall time compared to previous work."
6880,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,"approach USED-FOR reinforcement learning. task - independent intrinsic reward function USED-FOR approach. peripheral pulse measurements USED-FOR task - independent intrinsic reward function. reward functions USED-FOR sparse and skewed rewards. reward functions USED-FOR reinforcement learning settings. reward functions USED-FOR sample efficiency. sparse and skewed rewards FEATURE-OF reinforcement learning settings. it USED-FOR learning. simulated driving environment EVALUATE-FOR this. OtherScientificTerm are intrinsic feedback, Physiological changes, biological preparations, and human autonomic nervous system responses. Task is learning stage. ","This paper proposes a novel approach to reinforcement learning based on a task-independent intrinsic reward function based on peripheral pulse measurements. The authors show that reward functions with different reward functions can improve sample efficiency in reinforcement learning settings with sparse and skewed rewards. They also show that this can be applied to a simulated driving environment where intrinsic feedback is not available, and that it can be used to speed up learning.  Physiological changes are observed during the learning stage, which is consistent with biological preparations of the human autonomic nervous system responses. ","This paper proposes a novel approach to reinforcement learning based on a task-independent intrinsic reward function based on peripheral pulse measurements. The authors show that reward functions with different reward functions can improve sample efficiency in reinforcement learning settings with sparse and skewed rewards. They also show that this can be applied to a simulated driving environment where intrinsic feedback is not available, and that it can be used to speed up learning.  Physiological changes are observed during the learning stage, which is consistent with biological preparations of the human autonomic nervous system responses. "
6884,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"they USED-FOR predictive distributions. NPs USED-FOR conditional distributions. NPs USED-FOR observed data. attention PART-OF NPs. Method is Neural Processes ( NPs ). OtherScientificTerm are regression functions, and functions. Metric is linear complexity. Task is underfitting. Generic is this. ","This paper studies the generalization of Neural Processes (NPs) to regression functions. In particular, the authors show that NPs can be used to learn conditional distributions over the data, and that they can learn predictive distributions that are robust to underfitting. The authors also show that the linear complexity of these functions is linear in the number of samples, which is a result of the attention in NPs. Finally, they show that underfitting can be alleviated by training NPs on observed data.","This paper studies the generalization of Neural Processes (NPs) to regression functions. In particular, the authors show that NPs can be used to learn conditional distributions over the data, and that they can learn predictive distributions that are robust to underfitting. The authors also show that the linear complexity of these functions is linear in the number of samples, which is a result of the attention in NPs. Finally, they show that underfitting can be alleviated by training NPs on observed data."
6888,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,credit assignment USED-FOR pre - adaptation behavior. sample - efficiency EVALUATE-FOR metatraining. credit assignment USED-FOR gradient - based Meta - RL. meta - learning algorithm USED-FOR estimating meta - policy gradients. meta - learning algorithm USED-FOR poor credit assignment. algorithm USED-FOR meta - learning. statistical distance FEATURE-OF pre - adaptation and adapted policies. pre - adaptation and adapted policies USED-FOR meta - policy search. approach COMPARE Meta - RL algorithms. Meta - RL algorithms COMPARE approach. approach USED-FOR pre - adaptation policy behavior. wall - clock time CONJUNCTION asymptotic performance. asymptotic performance CONJUNCTION wall - clock time. sample - efficiency CONJUNCTION wall - clock time. wall - clock time CONJUNCTION sample - efficiency. asymptotic performance EVALUATE-FOR Meta - RL algorithms. wall - clock time EVALUATE-FOR Meta - RL algorithms. asymptotic performance EVALUATE-FOR approach. wall - clock time EVALUATE-FOR approach. sample - efficiency EVALUATE-FOR Meta - RL algorithms. sample - efficiency EVALUATE-FOR approach. Task is Credit assignment. Generic is it. Method is task identification strategies. OtherScientificTerm is meta - policy gradients. ,"This paper studies the problem of credit assignment in gradient-based Meta-RL, which aims to improve sample-efficiency in metatraining. Credit assignment is an important problem in meta-RL as it can be used to improve the sample efficiency of the meta-learning algorithm. The authors propose a new meta learning algorithm for estimating meta-policy gradients, which is motivated by the observation that poor credit assignment can lead to a poor pre-adaptation behavior. The proposed algorithm is based on the idea that the algorithm can be seen as an extension of the work of [Zhang et al. 2017].    The paper proposes two task identification strategies: (1) the authors propose to use the statistical distance between the pre -adaptation and adapted policies for meta-approximation, and (2) to use a combination of meta-adaptive and adaptive policies for the task.  The authors show that the proposed approach outperforms the state-of-the-art Meta-Reinforcement-Learning algorithms in terms of sample-efficient, wall-clock time, and asymptotic performance. They also show that their approach is able to learn a good preadaptation policy behavior that is consistent with the task-specific credit assignment. ","This paper studies the problem of credit assignment in gradient-based Meta-RL, which aims to improve sample-efficiency in metatraining. Credit assignment is an important problem in meta-RL as it can be used to improve the sample efficiency of the meta-learning algorithm. The authors propose a new meta learning algorithm for estimating meta-policy gradients, which is motivated by the observation that poor credit assignment can lead to a poor pre-adaptation behavior. The proposed algorithm is based on the idea that the algorithm can be seen as an extension of the work of [Zhang et al. 2017].    The paper proposes two task identification strategies: (1) the authors propose to use the statistical distance between the pre -adaptation and adapted policies for meta-approximation, and (2) to use a combination of meta-adaptive and adaptive policies for the task.  The authors show that the proposed approach outperforms the state-of-the-art Meta-Reinforcement-Learning algorithms in terms of sample-efficient, wall-clock time, and asymptotic performance. They also show that their approach is able to learn a good preadaptation policy behavior that is consistent with the task-specific credit assignment. "
6892,SP:be5f2c827605914206f5645087b94a50f59f9214,"classifier USED-FOR satisfiability. NeuroSAT HYPONYM-OF message passing neural network. message passing neural network USED-FOR SAT problems. it COMPARE SAT solvers. SAT solvers COMPARE it. NeuroSAT USED-FOR problems. SAT solvers COMPARE NeuroSAT. NeuroSAT COMPARE SAT solvers. it COMPARE NeuroSAT. NeuroSAT COMPARE it. clique detection CONJUNCTION dominating set. dominating set CONJUNCTION clique detection. graph coloring CONJUNCTION clique detection. clique detection CONJUNCTION graph coloring. it USED-FOR SAT problems. dominating set CONJUNCTION vertex cover problems. vertex cover problems CONJUNCTION dominating set. NeuroSAT USED-FOR distributions. graph coloring HYPONYM-OF SAT problems. clique detection HYPONYM-OF SAT problems. OtherScientificTerm are random SAT problems, and small random graphs. ","This paper proposes NeuroSAT, a message passing neural network for solving SAT problems. The idea is to learn a classifier that predicts the satisfiability of a set of random SAT problems, and then use it as a regularizer to improve the performance of existing SAT solvers. The authors show that NeuroSat is able to solve a wide range of problems, including graph coloring, clique detection, dominating set, and vertex cover problems. They also show that it outperforms the state-of-the-art SAT solver on small random graphs.    The authors also show how to train the distributions of the learned classifier and how it can be applied to other SAT problems as well. ","This paper proposes NeuroSAT, a message passing neural network for solving SAT problems. The idea is to learn a classifier that predicts the satisfiability of a set of random SAT problems, and then use it as a regularizer to improve the performance of existing SAT solvers. The authors show that NeuroSat is able to solve a wide range of problems, including graph coloring, clique detection, dominating set, and vertex cover problems. They also show that it outperforms the state-of-the-art SAT solver on small random graphs.    The authors also show how to train the distributions of the learned classifier and how it can be applied to other SAT problems as well. "
6896,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,policy USED-FOR autonomous driving. imitation learning USED-FOR policy. behavior cloning USED-FOR complex driving scenarios. perception system CONJUNCTION controller. controller CONJUNCTION perception system. perturbations FEATURE-OF expert ’s driving. synthesized data USED-FOR learner. perturbations FEATURE-OF synthesized data. robustness EVALUATE-FOR model. losses USED-FOR imitation loss. causal factors FEATURE-OF model. OtherScientificTerm is collisions. ,This paper proposes a new method to learn a policy for autonomous driving based on imitation learning. The key idea is to use behavior cloning to tackle complex driving scenarios where there is a perception system and a controller. The learner is trained on synthesized data with perturbations to the expert’s driving. The paper shows that the robustness of the model is improved by incorporating additional losses to the imitation loss. The model is also shown to be robust to collisions and other causal factors. ,This paper proposes a new method to learn a policy for autonomous driving based on imitation learning. The key idea is to use behavior cloning to tackle complex driving scenarios where there is a perception system and a controller. The learner is trained on synthesized data with perturbations to the expert’s driving. The paper shows that the robustness of the model is improved by incorporating additional losses to the imitation loss. The model is also shown to be robust to collisions and other causal factors. 
6900,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"language modeling CONJUNCTION content recommendation. content recommendation CONJUNCTION language modeling. machine learning models USED-FOR tasks. content recommendation CONJUNCTION advertising. advertising CONJUNCTION content recommendation. image classification CONJUNCTION language modeling. language modeling CONJUNCTION image classification. data USED-FOR machine learning models. data USED-FOR tasks. advertising HYPONYM-OF tasks. image classification HYPONYM-OF tasks. language modeling HYPONYM-OF tasks. content recommendation HYPONYM-OF tasks. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. dataset USED-FOR model. SVHN EVALUATE-FOR approach. CIFAR10 EVALUATE-FOR approach. Material are text, and images. Method are small proxy model, and large target model. ","This paper proposes a new dataset for training machine learning models on data collected from different tasks (e.g. image classification, language modeling, content recommendation, advertising, etc.). The dataset consists of text, images, and a small proxy model. The model is trained on the dataset, and the dataset is then used to train a large target model.  The approach is evaluated on CIFAR10 and SVHN.  ","This paper proposes a new dataset for training machine learning models on data collected from different tasks (e.g. image classification, language modeling, content recommendation, advertising, etc.). The dataset consists of text, images, and a small proxy model. The model is trained on the dataset, and the dataset is then used to train a large target model.  The approach is evaluated on CIFAR10 and SVHN.  "
6904,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"formulation of planning USED-FOR it. probabilistic inference problem USED-FOR formulation of planning. probabilistic inference problem USED-FOR it. classical methods USED-FOR control. classical methods USED-FOR inference. Sequential Monte Carlo CONJUNCTION Bayesian smoothing. Bayesian smoothing CONJUNCTION Sequential Monte Carlo. Bayesian smoothing USED-FOR control. classical methods USED-FOR Sequential Monte Carlo. classical methods USED-FOR Bayesian smoothing. inference USED-FOR control. classical methods USED-FOR Sequential Monte Carlo Planning. classical methods USED-FOR algorithm. Sequential Monte Carlo Planning USED-FOR continuous control tasks. Sequential Monte Carlo Planning USED-FOR multimodal policies. Method is sampling methods. OtherScientificTerm are continuous domains, and fixed computational budget. ","This paper proposes a new formulation of planning that uses it as a probabilistic inference problem. The authors show that classical methods for control based on Sequential Monte Carlo and Bayesian smoothing can be used to perform inference for control. They also show that the proposed algorithm can be implemented using classical methods in a way that is computationally tractable.   The authors also provide a theoretical analysis of sampling methods and show that they can be applied to continuous domains with a fixed computational budget. Finally, the authors apply their algorithm to a number of continuous control tasks and demonstrate that their algorithm is able to learn multimodal policies that are robust to changes in the environment. ","This paper proposes a new formulation of planning that uses it as a probabilistic inference problem. The authors show that classical methods for control based on Sequential Monte Carlo and Bayesian smoothing can be used to perform inference for control. They also show that the proposed algorithm can be implemented using classical methods in a way that is computationally tractable.   The authors also provide a theoretical analysis of sampling methods and show that they can be applied to continuous domains with a fixed computational budget. Finally, the authors apply their algorithm to a number of continuous control tasks and demonstrate that their algorithm is able to learn multimodal policies that are robust to changes in the environment. "
6908,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"Neural networks USED-FOR small adversarial perturbations. Adversarial robustness COMPARE clean accuracy. clean accuracy COMPARE Adversarial robustness. adversarial training HYPONYM-OF robust training method. robustness EVALUATE-FOR adversarial trained model. semantics - preserving transformations FEATURE-OF data distribution. distribution USED-FOR adversarial trained model. clean accuracy CONJUNCTION robust accuracy. robust accuracy CONJUNCTION clean accuracy. clean accuracy EVALUATE-FOR Bayes classifier. robust accuracy EVALUATE-FOR Bayes classifier. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. semantically - identical variants USED-FOR CIFAR10. semantically - identical variants USED-FOR MNIST. robustness accuracies EVALUATE-FOR adversarially trained models. adversarial robustness EVALUATE-FOR neural networks. Generic are models, and them. Metric is clean accuracies. OtherScientificTerm is input data distribution. ","This paper studies the robustness of neural networks to small adversarial perturbations. Adversarial robustness is defined as the clean accuracy of a trained model that is robust to a small perturbation in the input space. The paper proposes a new robust training method, called adversarial training, to improve the clean accuracies of adversarial trained models. The key idea is to train the models to be more robust to small changes to the input data distribution (i.e. semantics-preserving transformations) and then fine-tune them to be robust to larger changes in the data distribution. The authors show that adversarial robust training improves robustness in the presence of large changes of the input distribution. They also show that the adversarial learned model is more robust when the distribution of the data is similar to that of the training distribution.   The paper also shows that adversarially trained models can achieve better robustness accuracies than clean neural networks. The clean accuracy and robust accuracy of the Bayes classifier are also improved when the training data distribution is similar. Experiments are conducted on MNIST and CIFAR10 using semantically-identical variants of the original MNIST. ","This paper studies the robustness of neural networks to small adversarial perturbations. Adversarial robustness is defined as the clean accuracy of a trained model that is robust to a small perturbation in the input space. The paper proposes a new robust training method, called adversarial training, to improve the clean accuracies of adversarial trained models. The key idea is to train the models to be more robust to small changes to the input data distribution (i.e. semantics-preserving transformations) and then fine-tune them to be robust to larger changes in the data distribution. The authors show that adversarial robust training improves robustness in the presence of large changes of the input distribution. They also show that the adversarial learned model is more robust when the distribution of the data is similar to that of the training distribution.   The paper also shows that adversarially trained models can achieve better robustness accuracies than clean neural networks. The clean accuracy and robust accuracy of the Bayes classifier are also improved when the training data distribution is similar. Experiments are conducted on MNIST and CIFAR10 using semantically-identical variants of the original MNIST. "
6912,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,transformation of layer weights COMPARE layer outputs. layer outputs COMPARE transformation of layer weights. normalization technique USED-FOR batch normalization. transformation of layer weights USED-FOR normalization technique. positive and negative weights USED-FOR layer output. SVHN CONJUNCTION ILSVRC 2012 ImageNet. ILSVRC 2012 ImageNet CONJUNCTION SVHN. CIFAR-10/100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-10/100. CIFAR-10/100 HYPONYM-OF benchmarks. ILSVRC 2012 ImageNet HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR method. SVHN HYPONYM-OF benchmarks. Generic is technique. ,"This paper proposes a new normalization technique for batch normalization based on the transformation of layer weights instead of layer outputs. Specifically, the authors propose to normalize the layer output based on both positive and negative weights. The proposed method is evaluated on standard benchmarks such as CIFAR-10/100, SVHN, and ILSVRC 2012 ImageNet. The experimental results show the effectiveness of the proposed technique.","This paper proposes a new normalization technique for batch normalization based on the transformation of layer weights instead of layer outputs. Specifically, the authors propose to normalize the layer output based on both positive and negative weights. The proposed method is evaluated on standard benchmarks such as CIFAR-10/100, SVHN, and ILSVRC 2012 ImageNet. The experimental results show the effectiveness of the proposed technique."
6916,SP:8188f15c8521099305aa8664e05f102ee6cea402,Memorization USED-FOR over - parameterized neural networks. Memorization USED-FOR generalization. implicit regularization effect USED-FOR stochastic gradient descent. learning rates FEATURE-OF implicit regularization effect. learning rates FEATURE-OF stochastic gradient descent. loss statistics USED-FOR mislabeled examples. algorithm USED-FOR mislabeled examples. artificial and real - world mislabeled examples FEATURE-OF datasets. datasets EVALUATE-FOR ODD. Method is DATA DENOISING ( ODD ). OtherScientificTerm is computational overhead. ,"Memorization has been shown to improve generalization in over-parameterized neural networks. Memorization is used as a regularizer to prevent generalization to mislabeled examples. This paper studies the implicit regularization effect of stochastic gradient descent with increasing learning rates. The authors propose a new algorithm, called Data Denoising (DDD), which uses the loss statistics of the original training set to identify mislabeling examples. ODD is evaluated on two datasets with artificial and real-world mislabelED examples, and the authors show that ODD outperforms the existing algorithms in terms of computational overhead.  ","Memorization has been shown to improve generalization in over-parameterized neural networks. Memorization is used as a regularizer to prevent generalization to mislabeled examples. This paper studies the implicit regularization effect of stochastic gradient descent with increasing learning rates. The authors propose a new algorithm, called Data Denoising (DDD), which uses the loss statistics of the original training set to identify mislabeling examples. ODD is evaluated on two datasets with artificial and real-world mislabelED examples, and the authors show that ODD outperforms the existing algorithms in terms of computational overhead.  "
6920,SP:fbf023a772013e6eca62f92982aecf857c16a428,Pretrained language models USED-FOR downstream NLP task. analysis framework USED-FOR pretraining and downstream tasks. latent variables FEATURE-OF posterior distribution. latent variable generative model of text USED-FOR pretraining and downstream tasks. head tuning CONJUNCTION prompt tuning. prompt tuning CONJUNCTION head tuning. frozen pretrained model USED-FOR classifier. Hidden Markov Model ( HMM ) CONJUNCTION HMM. HMM CONJUNCTION Hidden Markov Model ( HMM ). latent memory component USED-FOR long - term dependencies. natural language FEATURE-OF long - term dependencies. Hidden Markov Model ( HMM ) HYPONYM-OF generative model. HMM HYPONYM-OF generative model. latent memory component USED-FOR HMM. classification heads USED-FOR downstream task. non - degeneracy conditions FEATURE-OF HMM. memory - augmented HMM COMPARE vanilla HMM. vanilla HMM COMPARE memory - augmented HMM. prompt tuning USED-FOR downstream guarantees. recovery guarantees FEATURE-OF memory - augmented HMM. non - degeneracy conditions FEATURE-OF downstream guarantees. long - term memory USED-FOR task - relevant information. HMMs USED-FOR synthetically generated data. Generic is models. Task is downstream classifier. ,"This paper proposes a new analysis framework for the pretraining and downstream tasks based on a latent variable generative model of text. The authors show that pretrained language models can be used to train a downstream NLP task. The models are trained in a way that the posterior distribution over the latent variables of the latent variable of the text is frozen. The frozen pretrained model is then used to fine-tune a classifier on a downstream task using either head tuning or prompt tuning.   The authors propose two generative models, the Hidden Markov Model (HMM) and HMM, where the HMM has a latent memory component to capture long-term dependencies in natural language. The HMM is trained with non-degeneracy conditions, and the classification heads for the downstream task are also frozen.  The paper shows that the memory-augmented HMM achieves better recovery guarantees than the vanilla HMM under the same downstream guarantees with respect to non-degeneracy conditions. The paper also shows that HMMs can also be used for synthetically generated data, and that the long term memory of HMMs is able to capture task-relevant information.","This paper proposes a new analysis framework for the pretraining and downstream tasks based on a latent variable generative model of text. The authors show that pretrained language models can be used to train a downstream NLP task. The models are trained in a way that the posterior distribution over the latent variables of the latent variable of the text is frozen. The frozen pretrained model is then used to fine-tune a classifier on a downstream task using either head tuning or prompt tuning.   The authors propose two generative models, the Hidden Markov Model (HMM) and HMM, where the HMM has a latent memory component to capture long-term dependencies in natural language. The HMM is trained with non-degeneracy conditions, and the classification heads for the downstream task are also frozen.  The paper shows that the memory-augmented HMM achieves better recovery guarantees than the vanilla HMM under the same downstream guarantees with respect to non-degeneracy conditions. The paper also shows that HMMs can also be used for synthetically generated data, and that the long term memory of HMMs is able to capture task-relevant information."
6936,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,invariant features USED-FOR classifier. transferability USED-FOR domain generalization. total variation CONJUNCTION Wasserstein distance. Wasserstein distance CONJUNCTION total variation. algorithms USED-FOR domain generalization. feature embeddings USED-FOR domain generalization. transferability FEATURE-OF feature embeddings. algorithms USED-FOR feature embeddings. algorithms USED-FOR transferability. RotatedMNIST CONJUNCTION PACS. PACS CONJUNCTION RotatedMNIST. PACS CONJUNCTION Office - Home. Office - Home CONJUNCTION PACS. algorithm USED-FOR transferable features. benchmark datasets EVALUATE-FOR it. RotatedMNIST HYPONYM-OF benchmark datasets. Office - Home HYPONYM-OF benchmark datasets. PACS HYPONYM-OF benchmark datasets. algorithm COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE algorithm. Task is Out - of - distribution generalization. Generic is model. OtherScientificTerm is transferable ” features. ,"This paper studies the problem of Out-of-distribution generalization, i.e., the problem that a classifier trained on invariant features is unable to generalize well to a new domain. The authors propose two algorithms to improve the transferability of feature embeddings for domain generalization by maximizing the total variation and Wasserstein distance between the target domain and the source domain. In particular, the authors propose an algorithm to learn transferable features that are invariant to the source and target domains. They evaluate it on three benchmark datasets (RotatedMNIST, PACS, Office-Home) and show that it outperforms the state-of the-art algorithms. They also show that the model is able to learn more “transferable” features. ","This paper studies the problem of Out-of-distribution generalization, i.e., the problem that a classifier trained on invariant features is unable to generalize well to a new domain. The authors propose two algorithms to improve the transferability of feature embeddings for domain generalization by maximizing the total variation and Wasserstein distance between the target domain and the source domain. In particular, the authors propose an algorithm to learn transferable features that are invariant to the source and target domains. They evaluate it on three benchmark datasets (RotatedMNIST, PACS, Office-Home) and show that it outperforms the state-of the-art algorithms. They also show that the model is able to learn more “transferable” features. "
6952,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"Reward USED-FOR reinforcement - learning agents. partial ordering CONJUNCTION partial ordering. partial ordering CONJUNCTION partial ordering. reward USED-FOR tasks. polynomial - time algorithms USED-FOR Markov reward function. OtherScientificTerm are acceptable behaviors, and reward function. ","This paper studies the problem of reward learning for reinforcement learning. The authors propose a new reward function for RL agents, which is based on the notion of acceptable behaviors. They show that the proposed reward function can be used to train RL agents on a variety of tasks. They also provide a polynomial-time algorithm for learning the reward function. ","This paper studies the problem of reward learning for reinforcement learning. The authors propose a new reward function for RL agents, which is based on the notion of acceptable behaviors. They show that the proposed reward function can be used to train RL agents on a variety of tasks. They also provide a polynomial-time algorithm for learning the reward function. "
6968,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"approaches USED-FOR generalization. sequential structure FEATURE-OF RL problem. approaches USED-FOR RL problem. approaches USED-FOR sequential structure. implicit partial observability USED-FOR generalization. generalization USED-FOR RL. epistemic POMDP HYPONYM-OF induced partially observed Markov decision process. induced partially observed Markov decision process USED-FOR generalization. ensemble - based technique USED-FOR partially observed problem. algorithm COMPARE methods. methods COMPARE algorithm. generalization EVALUATE-FOR methods. epistemic POMDP USED-FOR algorithm. generalization EVALUATE-FOR algorithm. Procgen benchmark suite EVALUATE-FOR algorithm. Procgen benchmark suite EVALUATE-FOR methods. Task is Generalization. Method are reinforcement learning ( RL ) systems, supervised learning, supervised learning methods, and POMDPs. OtherScientificTerm are epistemic uncertainty, fullyobserved MDPs, failure modes of algorithms, and partial observability. ","This paper considers the problem of generalization in reinforcement learning (RL) systems. Generalization is a fundamental problem in RL, and there are several approaches to improve generalization under the sequential structure of the RL problem. The authors consider the implicit partial observability of the generalization of RL, i.e., the assumption that the epistemic uncertainty of the learned policy is a function of the partial observation of the environment.    The authors propose an algorithm that uses an induced partially observed Markov decision process (i.e. an epistemic POMDP, which is an extension of supervised learning) to generalize to fullyobserved MDPs. The generalization is based on an ensemble-based technique for solving the partially observed problem.  The proposed algorithm is evaluated on the Procgen benchmark suite, and compared to other methods that do not consider the partial observable MDP. The results show that the proposed algorithm outperforms other methods in generalization, especially in the case of fullyobservable and partially observed MDP, where the failure modes of algorithms do not depend on the degree of observation.  In addition, the authors show that supervised learning methods do not perform as well in the presence of partial observablity, and that the generalisation performance of the algorithm is improved when the partial observable MDP is more generalizable. The paper also shows that the performance of algorithms that do rely on partial observabilities improves when the epistemically observed MDEs are more complex (e.g. when the state is more complex).","This paper considers the problem of generalization in reinforcement learning (RL) systems. Generalization is a fundamental problem in RL, and there are several approaches to improve generalization under the sequential structure of the RL problem. The authors consider the implicit partial observability of the generalization of RL, i.e., the assumption that the epistemic uncertainty of the learned policy is a function of the partial observation of the environment.    The authors propose an algorithm that uses an induced partially observed Markov decision process (i.e. an epistemic POMDP, which is an extension of supervised learning) to generalize to fullyobserved MDPs. The generalization is based on an ensemble-based technique for solving the partially observed problem.  The proposed algorithm is evaluated on the Procgen benchmark suite, and compared to other methods that do not consider the partial observable MDP. The results show that the proposed algorithm outperforms other methods in generalization, especially in the case of fullyobservable and partially observed MDP, where the failure modes of algorithms do not depend on the degree of observation.  In addition, the authors show that supervised learning methods do not perform as well in the presence of partial observablity, and that the generalisation performance of the algorithm is improved when the partial observable MDP is more generalizable. The paper also shows that the performance of algorithms that do rely on partial observabilities improves when the epistemically observed MDEs are more complex (e.g. when the state is more complex)."
6984,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,Hessian matrix of value functions USED-FOR Model - agnostic meta - reinforcement learning. framework USED-FOR higherorder derivatives of value functions. off - policy evaluation USED-FOR framework. framework USED-FOR prior approaches. framework USED-FOR estimates. auto - differentiation libraries USED-FOR estimates. OtherScientificTerm is biased Hessian estimates. ,"Model-agnostic meta-reinforcement learning uses the Hessian matrix of value functions as a surrogate for off-policy evaluation. The paper proposes a framework for learning higherorder derivatives of value function, which can be used to overcome the issue of biased Hessian estimates. The authors show that the proposed framework outperforms prior approaches in terms of accuracy and efficiency. They also show that these estimates can be obtained from auto-differentiation libraries. ","Model-agnostic meta-reinforcement learning uses the Hessian matrix of value functions as a surrogate for off-policy evaluation. The paper proposes a framework for learning higherorder derivatives of value function, which can be used to overcome the issue of biased Hessian estimates. The authors show that the proposed framework outperforms prior approaches in terms of accuracy and efficiency. They also show that these estimates can be obtained from auto-differentiation libraries. "
7000,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"approach USED-FOR communication constraints. approach USED-FOR distributed learning problem. communication constraints FEATURE-OF distributed learning problem. central server USED-FOR distributed learning problem. algorithm COMPARE algorithms. algorithms COMPARE algorithm. algorithm USED-FOR bidirectional compression. convergence rate EVALUATE-FOR algorithms. convergence rate EVALUATE-FOR algorithm. MCM HYPONYM-OF algorithm. gradients FEATURE-OF local servers. perturbed models USED-FOR gradients. model compression CONJUNCTION memory mechanism. memory mechanism CONJUNCTION model compression. memory mechanism PART-OF MCM. model compression PART-OF MCM. worker dependent randomized - models CONJUNCTION partial participation. partial participation CONJUNCTION worker dependent randomized - models. Method is downlink compression. OtherScientificTerm are local models, global model, and perturbation. Task is convergence proofs. ","This paper proposes a new approach to address the communication constraints in the distributed learning problem with a central server. The authors propose a new algorithm called bidirectional compression (MCM) which is based on the idea of downlink compression, where the local models are perturbed and the global model is perturbed. The algorithm is shown to have a better convergence rate than existing algorithms for the case where the gradients of the local servers have been perturbed by the perturbed models.  The authors also provide convergence proofs for their algorithm.  MCM is a simple algorithm that combines model compression with a memory mechanism, and is a generalization of previous work on worker dependent randomized-models and partial participation.   ","This paper proposes a new approach to address the communication constraints in the distributed learning problem with a central server. The authors propose a new algorithm called bidirectional compression (MCM) which is based on the idea of downlink compression, where the local models are perturbed and the global model is perturbed. The algorithm is shown to have a better convergence rate than existing algorithms for the case where the gradients of the local servers have been perturbed by the perturbed models.  The authors also provide convergence proofs for their algorithm.  MCM is a simple algorithm that combines model compression with a memory mechanism, and is a generalization of previous work on worker dependent randomized-models and partial participation.   "
7016,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"causal inference USED-FOR stress testing. counterfactual invariance USED-FOR outof - domain model. causal structure USED-FOR counterfactual invariance. regularization schemes USED-FOR counterfactual invariance. regularization schemes USED-FOR causal structures. causal structure FEATURE-OF domain shift guarantees. domain shift guarantees FEATURE-OF counterfactual invariance. OtherScientificTerm are spurious correlation, spurious correlations, and counterfactual examples. Method are model, and machine learning. Generic are models, and schemes. Task is text classification. ","This paper studies the problem of counterfactual invariance in out-of-domain (OOD) learning, which is a problem that arises when there is a spurious correlation between the source and target domains. The authors consider the problem in the context of causal inference for stress testing, where a model is trained on the source domain and tested on the target domain. In this setting, the authors show that the spurious correlations are invariant to the source/target domain, but not to the out of domain. They then propose two regularization schemes for counterfactually invariance to spurious correlations in machine learning, and show that these schemes are robust to domain shift. Finally, they also show that there exists a causal structure in the causal structure that can be used to enforce counterfactuality in the domain shift guarantees of an outof-distribution model.  ","This paper studies the problem of counterfactual invariance in out-of-domain (OOD) learning, which is a problem that arises when there is a spurious correlation between the source and target domains. The authors consider the problem in the context of causal inference for stress testing, where a model is trained on the source domain and tested on the target domain. In this setting, the authors show that the spurious correlations are invariant to the source/target domain, but not to the out of domain. They then propose two regularization schemes for counterfactually invariance to spurious correlations in machine learning, and show that these schemes are robust to domain shift. Finally, they also show that there exists a causal structure in the causal structure that can be used to enforce counterfactuality in the domain shift guarantees of an outof-distribution model.  "
7032,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"limited data USED-FOR GANs. data augmentations CONJUNCTION model regularization. model regularization CONJUNCTION data augmentations. generator USED-FOR real data distribution. APA USED-FOR overfitting. method COMPARE approaches. approaches COMPARE method. generator USED-FOR APA. model regularization USED-FOR approaches. data augmentations USED-FOR approaches. APA USED-FOR low - data regime. low - data regime FEATURE-OF synthesis quality. synthesis quality EVALUATE-FOR APA. It USED-FOR GANs. StyleGAN2 HYPONYM-OF GANs. Method are Generative adversarial networks ( GANs ), and Adaptive Pseudo Augmentation ( APA ). Material are high - fidelity images, and generated images. OtherScientificTerm are discriminator overfitting, and discriminator. Generic are strategy, and training strategy. ","Generative adversarial networks (GANs) have been shown to overfit to high-fidelity images, but this paper proposes Adaptive Pseudo Augmentation (APA) to tackle the issue of discriminator overfitting in GANs with limited data. The proposed method, called APA, learns a generator to augment the real data distribution with pseudo-augmentations to avoid overfitting. The authors show that the proposed method outperforms existing approaches based on data augmentations and model regularization. APA is also shown to improve the synthesis quality in the low-data regime. It is shown that APA can be applied to a variety of GAN models (e.g., StyleGAN2) and is shown to outperform existing approaches. The paper also shows that the generated images generated by APA are more likely to be high-quality than the original generated images.   The authors also show that their strategy is more robust to the amount of data used in the training strategy and that the discriminator is more sensitive to the number of augmentations used. ","Generative adversarial networks (GANs) have been shown to overfit to high-fidelity images, but this paper proposes Adaptive Pseudo Augmentation (APA) to tackle the issue of discriminator overfitting in GANs with limited data. The proposed method, called APA, learns a generator to augment the real data distribution with pseudo-augmentations to avoid overfitting. The authors show that the proposed method outperforms existing approaches based on data augmentations and model regularization. APA is also shown to improve the synthesis quality in the low-data regime. It is shown that APA can be applied to a variety of GAN models (e.g., StyleGAN2) and is shown to outperform existing approaches. The paper also shows that the generated images generated by APA are more likely to be high-quality than the original generated images.   The authors also show that their strategy is more robust to the amount of data used in the training strategy and that the discriminator is more sensitive to the number of augmentations used. "
7048,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,Causal inference CONJUNCTION discovery. discovery CONJUNCTION Causal inference. observational data USED-FOR discovery. observational data USED-FOR Causal inference. formalization USED-FOR causal inference. Rubin ’s framework USED-FOR multivariate point processes. average treatment effect ( ATE ) CONJUNCTION propensity scores. propensity scores CONJUNCTION average treatment effect ( ATE ). Rubin ’s framework USED-FOR average treatment effect ( ATE ). Rubin ’s framework USED-FOR propensity scores. multivariate recurrent event streams USED-FOR causal inference. multivariate point process USED-FOR data. joint probability distribution USED-FOR i.i.d. data. joint probability distribution COMPARE multivariate point process. multivariate point process COMPARE joint probability distribution. causal inference framework COMPARE baseline pairwise causal association scores. baseline pairwise causal association scores COMPARE causal inference framework. synthetic and real - world event datasets EVALUATE-FOR causal inference framework. Method is point process causal framework. Generic is measure. ,"Causal inference from observational data is an important problem in both causal inference and discovery. This paper proposes a new formalization for causal inference based on multivariate recurrent event streams. The authors extend the point process causal framework to multivariate point processes and apply Rubin’s framework for multivariate Point Processes to learn the average treatment effect (ATE) and propensity scores. They show that the data can be represented as a multimensional point process, and that the joint probability distribution for i.i.d. data is similar to that of the joint distribution of the multi-dimensional point process. They also show that their causal inference framework outperforms baseline pairwise causal association scores on both synthetic and real-world event datasets. ","Causal inference from observational data is an important problem in both causal inference and discovery. This paper proposes a new formalization for causal inference based on multivariate recurrent event streams. The authors extend the point process causal framework to multivariate point processes and apply Rubin’s framework for multivariate Point Processes to learn the average treatment effect (ATE) and propensity scores. They show that the data can be represented as a multimensional point process, and that the joint probability distribution for i.i.d. data is similar to that of the joint distribution of the multi-dimensional point process. They also show that their causal inference framework outperforms baseline pairwise causal association scores on both synthetic and real-world event datasets. "
7064,SP:5db39fbba518e24a22b99c8256491295048ec417,Graph neural networks ( GNNs ) USED-FOR graph representation learning. residual connections PART-OF message passing. they USED-FOR GNNs. message passing USED-FOR GNNs. abnormal node features FEATURE-OF GNNs. node features PART-OF graphs. GNNs USED-FOR abnormal features. resilience FEATURE-OF GNNs. AirGNN1 HYPONYM-OF GNN. Adaptive residual USED-FOR GNN. Task is real - world applications. OtherScientificTerm is abnormal feature scenarios. Generic is algorithm. ,"Graph neural networks (GNNs) have been a popular choice for graph representation learning, but they can suffer from the issue that residual connections in message passing can be unstable. In real-world applications, there are many abnormal feature scenarios, and GNNs have been shown to suffer from this issue. This paper proposes a new algorithm, called AirGNN1, to address this issue, and shows that they can be used to improve the resilience of GNN's to abnormal node features in graphs. Adaptive residual is used to train the GNN, and the proposed algorithm is shown to be effective.","Graph neural networks (GNNs) have been a popular choice for graph representation learning, but they can suffer from the issue that residual connections in message passing can be unstable. In real-world applications, there are many abnormal feature scenarios, and GNNs have been shown to suffer from this issue. This paper proposes a new algorithm, called AirGNN1, to address this issue, and shows that they can be used to improve the resilience of GNN's to abnormal node features in graphs. Adaptive residual is used to train the GNN, and the proposed algorithm is shown to be effective."
7080,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"Thompson sampling policy PART-OF Bayesian ‘ optimistic ’ policies. algorithm USED-FOR policies. optimistic set FEATURE-OF policies. zero - sum matrix games CONJUNCTION constrained bandits. constrained bandits CONJUNCTION zero - sum matrix games. regret analysis USED-FOR bilinear saddle - point problems. regret analysis USED-FOR optimistic policies. zero - sum matrix games HYPONYM-OF bilinear saddle - point problems. Thompson sampling USED-FOR policies. policy USED-FOR convex optimization problem. optimistic set FEATURE-OF policy. log - concavity CONJUNCTION unimodality. unimodality CONJUNCTION log - concavity. unimodality CONJUNCTION smoothness. smoothness CONJUNCTION unimodality. procedure USED-FOR posteriors. regularization CONJUNCTION constraints. constraints CONJUNCTION regularization. Task are online sequential decision problems, and stochastic multi - armed bandit case. Metric is Bayesian regret. Generic are problem, and it. OtherScientificTerm are linear regret, posterior, and exploration - exploitation tradeoff. Method is variational Bayesian optimistic sampling ’ ( VBOS ). ","This paper considers online sequential decision problems where the goal is to maximize the Bayesian regret of the policy. The authors consider the stochastic multi-armed bandit case, where the objective is to minimize the linear regret of a Thompson sampling policy in a family of Bayesian ‘optimistic’ policies.   The authors propose an algorithm to learn policies that maximize the regret of these policies in the optimistic set.  The regret analysis of these optimistic policies is based on the regret analysis for bilinear saddle-point problems (e.g., zero-sum matrix games and constrained bandits).  The paper shows that the policies learned by Thompson sampling are optimal when the posterior of the posterior is smooth and log-concave, and that the policy is optimal in the pessimistic set of the convex optimization problem. The paper also shows that for any policy that is optimistic in this optimistic set, the optimal policy is a policy that maximizes the regret.  Finally, the paper proposes a new problem, called ‘variational Bayesian optimistic sampling’ (VBOS), and shows that it is a special case of the exploration-exploitation tradeoff.  In particular, the authors show that under certain conditions (log-convexity, unimodality, and/or smoothness), the policy can be learned to maximize its regret, and they show that this procedure can be used to learn the posteriors of policies that are optimistically selected.  They also show that the proposed procedure is robust to regularization and constraints. ","This paper considers online sequential decision problems where the goal is to maximize the Bayesian regret of the policy. The authors consider the stochastic multi-armed bandit case, where the objective is to minimize the linear regret of a Thompson sampling policy in a family of Bayesian ‘optimistic’ policies.   The authors propose an algorithm to learn policies that maximize the regret of these policies in the optimistic set.  The regret analysis of these optimistic policies is based on the regret analysis for bilinear saddle-point problems (e.g., zero-sum matrix games and constrained bandits).  The paper shows that the policies learned by Thompson sampling are optimal when the posterior of the posterior is smooth and log-concave, and that the policy is optimal in the pessimistic set of the convex optimization problem. The paper also shows that for any policy that is optimistic in this optimistic set, the optimal policy is a policy that maximizes the regret.  Finally, the paper proposes a new problem, called ‘variational Bayesian optimistic sampling’ (VBOS), and shows that it is a special case of the exploration-exploitation tradeoff.  In particular, the authors show that under certain conditions (log-convexity, unimodality, and/or smoothness), the policy can be learned to maximize its regret, and they show that this procedure can be used to learn the posteriors of policies that are optimistically selected.  They also show that the proposed procedure is robust to regularization and constraints. "
7096,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"without - replacement sampling orders COMPARE uniform - iid - sampling. uniform - iid - sampling COMPARE without - replacement sampling orders. Without - replacement sampling USED-FOR SGD without variance reduction. convergence analysis CONJUNCTION rates of variance reduction. rates of variance reduction CONJUNCTION convergence analysis. without - replacement sampling orders USED-FOR composite finite - sum minimization. rates of variance reduction EVALUATE-FOR without - replacement sampling orders. random reshuffling CONJUNCTION cyclic sampling. cyclic sampling CONJUNCTION random reshuffling. Prox - DFinito HYPONYM-OF Finito. random reshuffling USED-FOR convergence rates. rates EVALUATE-FOR full - batch gradient descent. variance - reduction USED-FOR without - replacement sampling. cyclic order USED-FOR cyclic sampling. variance reduction USED-FOR uniform - iid - sampling. Prox - DFinito USED-FOR data - heterogeneous scenario. optimal cyclic sampling USED-FOR Prox - DFinito. sample - size - independent convergence rate EVALUATE-FOR Prox - DFinito. method USED-FOR optimal cyclic ordering. Method is stochastic algorithm. OtherScientificTerm are convex and strongly convex scenarios, and optimal fixed ordering. ","This paper studies the convergence analysis and rates of variance reduction for without-replacement sampling orders for composite finite-sum minimization, which is a popular stochastic algorithm in both convex and strongly convex scenarios.    The authors show that for SGD without variance reduction, the optimal fixed ordering is a function of the number of samples and the order of the samples. Without-replaceable sampling orders are shown to converge faster than uniform-iid-sampling, and the convergence rates are also faster than the rates for full-batch gradient descent. The authors also show convergence rates for random reshuffling and cyclic sampling, which are based on Prox-DFinito, a variant of Finito.  The main contribution of this paper is to show that the optimal cyclic ordering of the order is the only one that is optimal in the convex case, and that the variance-reduction is a necessary condition for  without-replaceability sampling to converge to the optimal order.  Finally, the authors propose a new method for finding the optimal Cyclic ordering, which they show is a sample-size-independent convergence rate, and they also show that in the data-heterogeneous scenario, Prox -Dfinito can be used to find a data-h heterogeneous scenario with a sample size smaller than that of the optimal. ","This paper studies the convergence analysis and rates of variance reduction for without-replacement sampling orders for composite finite-sum minimization, which is a popular stochastic algorithm in both convex and strongly convex scenarios.    The authors show that for SGD without variance reduction, the optimal fixed ordering is a function of the number of samples and the order of the samples. Without-replaceable sampling orders are shown to converge faster than uniform-iid-sampling, and the convergence rates are also faster than the rates for full-batch gradient descent. The authors also show convergence rates for random reshuffling and cyclic sampling, which are based on Prox-DFinito, a variant of Finito.  The main contribution of this paper is to show that the optimal cyclic ordering of the order is the only one that is optimal in the convex case, and that the variance-reduction is a necessary condition for  without-replaceability sampling to converge to the optimal order.  Finally, the authors propose a new method for finding the optimal Cyclic ordering, which they show is a sample-size-independent convergence rate, and they also show that in the data-heterogeneous scenario, Prox -Dfinito can be used to find a data-h heterogeneous scenario with a sample size smaller than that of the optimal. "
7112,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"algorithmic components USED-FOR reinforcement learning ( RL ) algorithms. relative entropy policy search ( REPS ) USED-FOR policy learning. simulated and real - world robotic domains EVALUATE-FOR policy learning. stochastic, gradient - based solvers USED-FOR REPS. first - order optimization methods USED-FOR REPS objective. sub - optimality FEATURE-OF policy. convergence rates FEATURE-OF sub - optimality. convergence rates FEATURE-OF policy. first - order optimization methods USED-FOR policy. technique USED-FOR parameter updates. generative access USED-FOR parameter updates. generative access USED-FOR Markov decision process. favorable convergence FEATURE-OF parameter updates. Markov decision process USED-FOR technique. generative access USED-FOR technique. OtherScientificTerm are exact gradients, near - optimality, stochastic gradients, and optimal regularized policy. ","This paper proposes a new algorithm for policy search in reinforcement learning (RL) algorithms. The authors propose relative entropy policy search (REPS) for policy learning in both simulated and real-world robotic domains. REPS is based on stochastic, gradient-based solvers, where exact gradients are not available, but near-optimality can be obtained through the use of first-order optimization methods. The paper shows that the REPS objective can be approximated by first order optimization methods, and that the convergence rates of the policy with respect to the optimal regularized policy are near-optimal. The proposed technique uses a Markov decision process with generative access to perform parameter updates with favorable convergence. ","This paper proposes a new algorithm for policy search in reinforcement learning (RL) algorithms. The authors propose relative entropy policy search (REPS) for policy learning in both simulated and real-world robotic domains. REPS is based on stochastic, gradient-based solvers, where exact gradients are not available, but near-optimality can be obtained through the use of first-order optimization methods. The paper shows that the REPS objective can be approximated by first order optimization methods, and that the convergence rates of the policy with respect to the optimal regularized policy are near-optimal. The proposed technique uses a Markov decision process with generative access to perform parameter updates with favorable convergence. "
7128,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,deep neural networks ( DNNs ) USED-FOR 3D point cloud processing. knowledge representations USED-FOR 3D point cloud processing. deep neural networks ( DNNs ) USED-FOR knowledge representations. translation CONJUNCTION scale. scale CONJUNCTION translation. scale CONJUNCTION local 3D structures. local 3D structures CONJUNCTION scale. rotation CONJUNCTION translation. translation CONJUNCTION rotation. representation complexity EVALUATE-FOR DNN. metrics CONJUNCTION representation complexity. representation complexity CONJUNCTION metrics. metrics USED-FOR spatial smoothness. metrics EVALUATE-FOR DNN. spatial smoothness FEATURE-OF encoding 3D structures. DNNs USED-FOR representation problems. Generic is method. Method is adversarial training. ,"This paper studies the problem of learning knowledge representations from deep neural networks (DNNs) for 3D point cloud processing. The authors consider rotation, translation, scale, and local 3D structures, and show that DNNs can be trained to solve these representation problems. They also show that the representation complexity of a DNN can be improved by using different metrics for spatial smoothness, and that adversarial training can be used to improve the performance of the method. ","This paper studies the problem of learning knowledge representations from deep neural networks (DNNs) for 3D point cloud processing. The authors consider rotation, translation, scale, and local 3D structures, and show that DNNs can be trained to solve these representation problems. They also show that the representation complexity of a DNN can be improved by using different metrics for spatial smoothness, and that adversarial training can be used to improve the performance of the method. "
7144,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"economics CONJUNCTION game theory. game theory CONJUNCTION economics. game theory CONJUNCTION computer science. computer science CONJUNCTION game theory. economics USED-FOR design of optimal auctions. methods USED-FOR approximating optimal auctions. deep learning USED-FOR approximating optimal auctions. deep learning USED-FOR methods. allocation fairness CONJUNCTION diversity. diversity CONJUNCTION allocation fairness. auction mechanisms USED-FOR socially desirable constraints. diversity HYPONYM-OF socially desirable constraints. allocation fairness HYPONYM-OF socially desirable constraints. neural - network - based auction mechanisms USED-FOR constraints. neural - network - based auction mechanisms USED-FOR PreferenceNet. method COMPARE neural - network based auction designs. neural - network based auction designs COMPARE method. metric EVALUATE-FOR method. metric USED-FOR auction allocations. socially desirable constraints FEATURE-OF auction allocations. human subject research USED-FOR approach. Method is strategyproof, revenuemaximizing auction designs. OtherScientificTerm are restricted settings, optimal auctions, and real human preferences. Generic are baselines, and they. Metric is maximizing revenue. ","This paper studies the design of optimal auctions in the context of economics, game theory, and computer science, and proposes two methods for approximating optimal auctions using deep learning. The authors propose two strategies for strategyproof, revenuemaximizing auction designs. They show that existing auction mechanisms satisfy two socially desirable constraints: allocation fairness and diversity. They then propose two neural-network-based auction mechanisms that satisfy these two constraints for PreferenceNet, and show that their method outperforms baselines in terms of maximizing revenue. They also provide a new metric for measuring the quality of auction allocations that satisfy the socially desirable constraint. The approach is based on human subject research, and the authors show that the proposed method is able to match or outperform existing neural-net based auction designs, and that the optimal auctions are more likely to satisfy real human preferences. ","This paper studies the design of optimal auctions in the context of economics, game theory, and computer science, and proposes two methods for approximating optimal auctions using deep learning. The authors propose two strategies for strategyproof, revenuemaximizing auction designs. They show that existing auction mechanisms satisfy two socially desirable constraints: allocation fairness and diversity. They then propose two neural-network-based auction mechanisms that satisfy these two constraints for PreferenceNet, and show that their method outperforms baselines in terms of maximizing revenue. They also provide a new metric for measuring the quality of auction allocations that satisfy the socially desirable constraint. The approach is based on human subject research, and the authors show that the proposed method is able to match or outperform existing neural-net based auction designs, and that the optimal auctions are more likely to satisfy real human preferences. "
7160,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"user - level differential privacy USED-FOR personalization of supervised learning. user - level privacy guarantees EVALUATE-FOR approach. non - private approaches USED-FOR algorithms. nearly optimal estimation error guarantees FEATURE-OF algorithms. exponential mechanism - based algorithm USED-FOR information - theoretic upper bound. OtherScientificTerm are shared structure, and joint, user - level differential privacy. Task is linear regression problems. ","This paper studies the problem of user-level differential privacy for personalization of supervised learning, where the goal is to ensure that the shared structure between the user and the server is not violated. The authors propose a new approach that provides user-levels differential privacy guarantees for linear regression problems. They provide an information-theoretic upper bound for joint, user-layer differential privacy, which is based on an exponential mechanism-based algorithm. They also provide nearly optimal estimation error guarantees for algorithms that are based on non-private approaches.","This paper studies the problem of user-level differential privacy for personalization of supervised learning, where the goal is to ensure that the shared structure between the user and the server is not violated. The authors propose a new approach that provides user-levels differential privacy guarantees for linear regression problems. They provide an information-theoretic upper bound for joint, user-layer differential privacy, which is based on an exponential mechanism-based algorithm. They also provide nearly optimal estimation error guarantees for algorithms that are based on non-private approaches."
7176,SP:3925fc528de17b8b2e93808f5440ea0503895b75,"human - adversarial examples USED-FOR them. examples EVALUATE-FOR state - of - the - art models. Material are Visual Question Answering dataset ( VQA v2 ), adversarial examples, and Adversarial VQA ( AdVQA ) benchmark. Metric is human accuracy. Method are VQA models, and VQA model. Generic is model. ","This paper introduces the Adversarial VQA (AdVQA) benchmark, which is a new benchmark for evaluating the robustness of visual question answering models to adversarial attacks. The authors show that the state-of-the-art models are robust against adversarial examples, and they also show that human-adversarial examples are more likely to be generated by them. The paper also introduces a new Visual Question Answering dataset, called AdVQa v2, that is designed to evaluate the ability of a model to correctly answer a question in an adversarial setting. ","This paper introduces the Adversarial VQA (AdVQA) benchmark, which is a new benchmark for evaluating the robustness of visual question answering models to adversarial attacks. The authors show that the state-of-the-art models are robust against adversarial examples, and they also show that human-adversarial examples are more likely to be generated by them. The paper also introduces a new Visual Question Answering dataset, called AdVQa v2, that is designed to evaluate the ability of a model to correctly answer a question in an adversarial setting. "
7192,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"Medial entorhinal cortex ( MEC ) USED-FOR navigational and memory related behaviors. they USED-FOR MEC functionality. they USED-FOR behavior. response profiles FEATURE-OF heterogeneous ” cells. models USED-FOR response profiles. response profiles FEATURE-OF stereotypical and heterogeneous MEC cells. task - optimized neural network models COMPARE grid cell - centric models. grid cell - centric models COMPARE task - optimized neural network models. grid cell - centric models USED-FOR MEC neuronal response profiles. task - optimized neural network models USED-FOR MEC neuronal response profiles. gated nonlinearities CONJUNCTION intermediate place cell representation. intermediate place cell representation CONJUNCTION gated nonlinearities. intermediate place cell representation HYPONYM-OF network architecture. gated nonlinearities HYPONYM-OF network architecture. heterogeneous cells COMPARE grid and border cells. grid and border cells COMPARE heterogeneous cells. heterogeneous cells USED-FOR downstream functional outcomes. path integration HYPONYM-OF downstream functional outcomes. spatial response selectivity FEATURE-OF MEC cells. reward - modulated path integration USED-FOR MEC model. non - spatial rewards FEATURE-OF MEC cells. variable - reward conditions FEATURE-OF neural recordings. OtherScientificTerm are MEC, grid, stereotypical response profiles, MEC neurons, stereotypical firing patterns, heterogeneous MEC cells, and response patterns. Method are computational approach, statistical analysis, and goal - driven modeling approach. Generic is model. Task is Neural Information Processing Systems. ","This paper studies the behavior of the medial entorhinal cortex (MEC) in the context of navigational and memory related behaviors. The authors propose a new computational approach to study the behavior and response patterns of MEC neurons. They show that “heterogeneous” cells have different response profiles (i.e. different response patterns) and that they can be used to model the MEC functionality. They also show that the response profiles of “stitihic” and “heterogeneous’ cells have similar behavior patterns.    The authors show that task-optimized neural network models are able to learn MEC neuronal response profiles that are similar to stereotypical and heterogeneous MEC cells with similar response profiles. They further show that heterogeneous cells are more likely to have downstream functional outcomes such as path integration than grid and border cells, and they can learn such behavior in an unsupervised way.  They also provide a statistical analysis that shows that there is a correlation between the spatial response selectivity of heterogeneous and stereotypical response profiles and the number of neurons in a MEC cell.  Finally, they propose a novel network architecture, gated nonlinearities and an intermediate place cell representation, and show that these two models can learn response profiles similar to those of the stereotypical MEC, but that they are more robust to changes in the grid.  The MEC model is trained using reward-modulated path integration, which is a goal-driven modeling approach, and is able to capture non-spatial rewards in the Mec cells. The neural recordings under variable-reward reward conditions are also shown to be heterogeneous, and the authors also find that the heterogeneous response patterns are more sensitive to the spatial region of the grid, and that stereotypical response patterns tend to be more selective to the region where the cell is located.  In addition, they show that, in the case of neural Information Processing Systems (NIS) models, heterogeneous responses are more highly correlated with the region to which the cell belongs, which suggests that there are more heterogeneous regions in the model. ","This paper studies the behavior of the medial entorhinal cortex (MEC) in the context of navigational and memory related behaviors. The authors propose a new computational approach to study the behavior and response patterns of MEC neurons. They show that “heterogeneous” cells have different response profiles (i.e. different response patterns) and that they can be used to model the MEC functionality. They also show that the response profiles of “stitihic” and “heterogeneous’ cells have similar behavior patterns.    The authors show that task-optimized neural network models are able to learn MEC neuronal response profiles that are similar to stereotypical and heterogeneous MEC cells with similar response profiles. They further show that heterogeneous cells are more likely to have downstream functional outcomes such as path integration than grid and border cells, and they can learn such behavior in an unsupervised way.  They also provide a statistical analysis that shows that there is a correlation between the spatial response selectivity of heterogeneous and stereotypical response profiles and the number of neurons in a MEC cell.  Finally, they propose a novel network architecture, gated nonlinearities and an intermediate place cell representation, and show that these two models can learn response profiles similar to those of the stereotypical MEC, but that they are more robust to changes in the grid.  The MEC model is trained using reward-modulated path integration, which is a goal-driven modeling approach, and is able to capture non-spatial rewards in the Mec cells. The neural recordings under variable-reward reward conditions are also shown to be heterogeneous, and the authors also find that the heterogeneous response patterns are more sensitive to the spatial region of the grid, and that stereotypical response patterns tend to be more selective to the region where the cell is located.  In addition, they show that, in the case of neural Information Processing Systems (NIS) models, heterogeneous responses are more highly correlated with the region to which the cell belongs, which suggests that there are more heterogeneous regions in the model. "
7208,SP:57f9812fa5e7d0c66d412beb035301684d760746,"them USED-FOR physical real - world tasks. KL - regularized reinforcement learning USED-FOR deep reinforcement learning algorithms. sample efficiency EVALUATE-FOR deep reinforcement learning algorithms. KL - regularized reinforcement learning USED-FOR them. expert demonstrations USED-FOR KL - regularized reinforcement learning. sample efficiency EVALUATE-FOR KL - regularized reinforcement learning. behavioral reference policies USED-FOR KL - regularized reinforcement learning. expert demonstrations USED-FOR behavioral reference policies. expert demonstrations USED-FOR KL - regularized reinforcement learning. sample efficiency CONJUNCTION online policy. online policy CONJUNCTION sample efficiency. KL - regularized reinforcement learning COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE KL - regularized reinforcement learning. locomotion and dexterous hand manipulation tasks EVALUATE-FOR KL - regularized reinforcement learning. locomotion and dexterous hand manipulation tasks EVALUATE-FOR state - of - the - art approaches. non - parametric behavioral reference policies USED-FOR pathology. OtherScientificTerm are pathological training dynamics, and behavioral policy classes. Method is online learning. ","This paper studies the use of KL-regularized reinforcement learning to improve the sample efficiency of deep reinforcement learning algorithms, and applies them to physical real-world tasks. The authors propose to use behavioral reference policies learned from expert demonstrations to improve sample efficiency and online policy learning. They show that the KL regularized RL approach outperforms state-of-the-art approaches on locomotion and dexterous hand manipulation tasks. They also show that non-parametric behavior reference policies can be used to identify pathology in pathological training dynamics, and that online learning can be performed in a similar way.","This paper studies the use of KL-regularized reinforcement learning to improve the sample efficiency of deep reinforcement learning algorithms, and applies them to physical real-world tasks. The authors propose to use behavioral reference policies learned from expert demonstrations to improve sample efficiency and online policy learning. They show that the KL regularized RL approach outperforms state-of-the-art approaches on locomotion and dexterous hand manipulation tasks. They also show that non-parametric behavior reference policies can be used to identify pathology in pathological training dynamics, and that online learning can be performed in a similar way."
7224,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"teacher - student framework USED-FOR kernel regression. neural tangent kernel PART-OF convolutional architectures. neural tangent kernel USED-FOR convolutional ’ kernels. filter size FEATURE-OF convolutional architectures. teacher - student framework USED-FOR problem. convolutional ’ kernels USED-FOR teacher - student framework. convolutional ’ kernels USED-FOR problem. locality USED-FOR learning curve exponent β. physics USED-FOR heuristic methods. ridge USED-FOR kernel regression. Method is Convolutional neural networks. OtherScientificTerm are ridgeless case, translational invariance, natural universality assumption, and learning curve exponents. ","This paper proposes a teacher-student framework for kernel regression using convolutional neural networks. The problem is formulated as the problem of learning ‘convolutional’ kernels based on the neural tangent kernel of the filter size of the convolutions of a given layer. The authors show that in the ridgeless case, this problem can be solved using a teacher - student framework based on ‘convolutions’ of the filters.    The authors also show that this problem is a special case of the ‘translational invariance’ problem, i.e. that the translational invariant assumption holds for the natural universality assumption of the learning curve exponents.  The paper also shows that locality is necessary for learning curve exponent β, and that heuristic methods based on physics can be used to solve this problem.  Finally, the authors show empirically that the ridge in kernel regression is invariant to the number of filters and the size of filters, which is an important property of the problem, and they show that ‘Convolutions of the ridge’ are invariant in the case of ‘learned’ filters.","This paper proposes a teacher-student framework for kernel regression using convolutional neural networks. The problem is formulated as the problem of learning ‘convolutional’ kernels based on the neural tangent kernel of the filter size of the convolutions of a given layer. The authors show that in the ridgeless case, this problem can be solved using a teacher - student framework based on ‘convolutions’ of the filters.    The authors also show that this problem is a special case of the ‘translational invariance’ problem, i.e. that the translational invariant assumption holds for the natural universality assumption of the learning curve exponents.  The paper also shows that locality is necessary for learning curve exponent β, and that heuristic methods based on physics can be used to solve this problem.  Finally, the authors show empirically that the ridge in kernel regression is invariant to the number of filters and the size of filters, which is an important property of the problem, and they show that ‘Convolutions of the ridge’ are invariant in the case of ‘learned’ filters."
7240,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"Variational Autoencoders ( VAEs ) USED-FOR representations of complex data distributions. Variational Autoencoders ( VAEs ) HYPONYM-OF probabilistic models. probabilistic models USED-FOR representations of complex data distributions. model USED-FOR latent representations. uni - modal Gaussian distribution USED-FOR latent representations. regularized autoencoders USED-FOR deterministic autoencoding framework. ex - post density estimation step USED-FOR they. latent space FEATURE-OF model. deterministic autoencoding framework USED-FOR latent space. expressive multi - modal latent distributions USED-FOR deterministic autoencoding framework. latent distribution USED-FOR encoded data. sample quality EVALUATE-FOR model. continuous and discrete domains EVALUATE-FOR model. Method are VAEs, and variational training procedure. OtherScientificTerm is VAE objective. Generic are models, and training procedure. ","This paper proposes Variational Autoencoders (VAEs), a class of probabilistic models that can learn representations of complex data distributions. The model learns latent representations from an uni-modal Gaussian distribution. The authors propose a deterministic autoencoding framework that extends the work of previous work on regularized autoencopers. The main idea is that the VAE objective can be decomposed into two parts: (1) a variational training procedure where the model is trained to learn the latent space of the model, and (2) an ex-post density estimation step to ensure that the latent representations learned by the model are invariant to changes in the variational distribution of the data.  The authors show that the proposed deterministic auto-encoder can learn a latent space that is invariant in the sense that they can be learned from the latent representation of the original data. They also show that this latent space can be efficiently learned using the proposed generative model.   The main contribution of the paper is the introduction of expressive multi-modual latent distributions that can be used to learn a model from the learned latent space. This allows the authors to train the models in an unsupervised way, without the need for any additional training procedure. The proposed model is evaluated on both continuous and discrete domains, and the authors demonstrate that the model can improve the sample quality of the encoded data when the latent distribution is expressive. ","This paper proposes Variational Autoencoders (VAEs), a class of probabilistic models that can learn representations of complex data distributions. The model learns latent representations from an uni-modal Gaussian distribution. The authors propose a deterministic autoencoding framework that extends the work of previous work on regularized autoencopers. The main idea is that the VAE objective can be decomposed into two parts: (1) a variational training procedure where the model is trained to learn the latent space of the model, and (2) an ex-post density estimation step to ensure that the latent representations learned by the model are invariant to changes in the variational distribution of the data.  The authors show that the proposed deterministic auto-encoder can learn a latent space that is invariant in the sense that they can be learned from the latent representation of the original data. They also show that this latent space can be efficiently learned using the proposed generative model.   The main contribution of the paper is the introduction of expressive multi-modual latent distributions that can be used to learn a model from the learned latent space. This allows the authors to train the models in an unsupervised way, without the need for any additional training procedure. The proposed model is evaluated on both continuous and discrete domains, and the authors demonstrate that the model can improve the sample quality of the encoded data when the latent distribution is expressive. "
7256,SP:6232d8738592c9728feddec4462e61903a17d131,adversarial examples USED-FOR Deep learning models. Autoencoder USED-FOR self - supervised ) adversarial detection. benign examples USED-FOR Autoencoder. autoencoder structure USED-FOR disentangled represen8 tations of images. disentangled represen8 tations of images USED-FOR adversarial examples. class features CONJUNCTION semantic features. semantic features CONJUNCTION class features. paired class / semantic features CONJUNCTION paired class / semantic features. paired class / semantic features CONJUNCTION paired class / semantic features. paired class / semantic features USED-FOR autoencoder. paired class / semantic features USED-FOR autoencoder. discriminator network USED-FOR autoencoder. generalization ability FEATURE-OF autoencoder. AUC CONJUNCTION FPR. FPR CONJUNCTION AUC. FPR CONJUNCTION TPR. TPR CONJUNCTION FPR. method COMPARE self - supervised detection methods. self - supervised detection methods COMPARE method. adversarial attacks CONJUNCTION victim models. victim models CONJUNCTION adversarial attacks. method COMPARE it. it COMPARE method. AUC CONJUNCTION TPR. TPR CONJUNCTION AUC. victim models USED-FOR self - supervised detection methods. adversarial attacks USED-FOR method. adversarial attacks FEATURE-OF self - supervised detection methods. AUC HYPONYM-OF measurements. measurements EVALUATE-FOR it. TPR HYPONYM-OF measurements. AUC EVALUATE-FOR it. FPR HYPONYM-OF measurements. AUC EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. Autoencoder - based detectors COMPARE method. method COMPARE Autoencoder - based detectors. method USED-FOR adaptive adversary. Metric is reconstruction error. ,"This paper proposes an autoencoder for (self-supervised) adversarial detection. Deep learning models are trained on adversarial examples generated by an adversary, and the authors propose to use the autoencoders to generate benign examples. The authors propose disentangling the disentangled represen8 tations of images based on the original image and autoen coder structure, and then using a discriminator network to distinguish between the generated images and the ones that are adversarial or benign.  The authors show that the adversarial example generated by the discriminator can be seen as a function of the class features and the semantic features of the image, and that the reconstruction error is proportional to the difference between the reconstructed image and the original one.   The paper also shows that the autencoder can be trained with paired class/semantic features (i.e., paired class / semantic features from the original and adversarial images) and paired class features from adversarial and benign examples, and shows that this improves the generalization ability of the autencoder.  Experiments on CIFAR-10 show that this method outperforms existing self-supervision detection methods under adversarial attacks and victim models on standard measurements such as AUC, FPR, and TPR. Autoencoded-based detectors are also shown to be more robust to adaptive adversaries, and it is shown that the proposed method can also be used to detect an adaptive adversary.","This paper proposes an autoencoder for (self-supervised) adversarial detection. Deep learning models are trained on adversarial examples generated by an adversary, and the authors propose to use the autoencoders to generate benign examples. The authors propose disentangling the disentangled represen8 tations of images based on the original image and autoen coder structure, and then using a discriminator network to distinguish between the generated images and the ones that are adversarial or benign.  The authors show that the adversarial example generated by the discriminator can be seen as a function of the class features and the semantic features of the image, and that the reconstruction error is proportional to the difference between the reconstructed image and the original one.   The paper also shows that the autencoder can be trained with paired class/semantic features (i.e., paired class / semantic features from the original and adversarial images) and paired class features from adversarial and benign examples, and shows that this improves the generalization ability of the autencoder.  Experiments on CIFAR-10 show that this method outperforms existing self-supervision detection methods under adversarial attacks and victim models on standard measurements such as AUC, FPR, and TPR. Autoencoded-based detectors are also shown to be more robust to adaptive adversaries, and it is shown that the proposed method can also be used to detect an adaptive adversary."
7272,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"it USED-FOR brain activity. embedding space FEATURE-OF syntactic structure. low signal - to - noise ratio FEATURE-OF neuroimaging tools. functional Magnetic Resonance Imaging ( fMRI ) HYPONYM-OF neuroimaging tools. multi - dimensional features USED-FOR syntactic structure. features CONJUNCTION fMRI recordings. fMRI recordings CONJUNCTION features. fMRI recordings USED-FOR brain representation of syntax. fMRI recordings USED-FOR natural text. features USED-FOR brain representation of syntax. syntactic structure - based features USED-FOR brain activity. complexity metrics USED-FOR processing load. language system FEATURE-OF brain activity. OtherScientificTerm are semantics, semantic processing load, semantic representation of the stimulus words, syntactic processing load, and syntactic features. Generic is approaches. Task is syntax. ","This paper proposes a new way to measure the semantic processing load of a language system by measuring the syntactic structure in the embedding space. The authors show that it can be used as a measure of brain activity, and that it correlates well with neuroimaging tools such as functional Magnetic Resonance Imaging (fMRI) with a low signal-to-noise ratio. They also show that multi-dimensional features of the syntactical structure can be extracted from fMRI recordings and used to infer the brain representation of syntax in natural text.    The paper also shows that syntactic processing load is correlated with the semantic representation of the stimulus words in the input language system. The paper further shows that these syntactic features are correlated with brain activity in the language system and that the brain activity can be measured using syntactic-based features. Finally, the paper shows that the complexity metrics for measuring the processing load are similar to existing approaches. ","This paper proposes a new way to measure the semantic processing load of a language system by measuring the syntactic structure in the embedding space. The authors show that it can be used as a measure of brain activity, and that it correlates well with neuroimaging tools such as functional Magnetic Resonance Imaging (fMRI) with a low signal-to-noise ratio. They also show that multi-dimensional features of the syntactical structure can be extracted from fMRI recordings and used to infer the brain representation of syntax in natural text.    The paper also shows that syntactic processing load is correlated with the semantic representation of the stimulus words in the input language system. The paper further shows that these syntactic features are correlated with brain activity in the language system and that the brain activity can be measured using syntactic-based features. Finally, the paper shows that the complexity metrics for measuring the processing load are similar to existing approaches. "
7288,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,deep generative models USED-FOR real - world applications. deep generative models USED-FOR Controllable generation. compositional ability USED-FOR concept combinations. compositional ability FEATURE-OF models. energybased models ( EBMs ) USED-FOR compositional generation. attributes USED-FOR compositional generation. them USED-FOR high - resolution image generation. EBM USED-FOR pre - trained generative model. latent space FEATURE-OF pre - trained generative model. latent space FEATURE-OF EBM. EBM USED-FOR them. EBM USED-FOR high - resolution image generation. StyleGAN HYPONYM-OF pre - trained generative model. EBM formulation USED-FOR joint distribution. it USED-FOR sampling. ordinary differential equation ( ODE ) USED-FOR sampling. pre - trained generator USED-FOR controllable generation. controllable generation USED-FOR attribute classifier. latent space USED-FOR Sampling. ODEs USED-FOR Sampling. conditional sampling CONJUNCTION sequential editing. sequential editing CONJUNCTION conditional sampling. method COMPARE state - of - the - art. state - of - the - art COMPARE method. sequential editing EVALUATE-FOR state - of - the - art. conditional sampling EVALUATE-FOR state - of - the - art. conditional sampling EVALUATE-FOR method. sequential editing EVALUATE-FOR method. method USED-FOR attribute combinations. method USED-FOR compositional generation. energy functions CONJUNCTION logical operators. logical operators CONJUNCTION energy functions. compositionality FEATURE-OF generating photo - realistic images. OtherScientificTerm is hyperparameters. Material is photo - realistic images. ,"Controllable generation from deep generative models is an important problem in real-world applications, and the compositional ability of these models is of great importance. This paper proposes to use energybased models (EBMs) for compositional generation from attributes, and uses them for high-resolution image generation using an EBM in the latent space of a pre-trained generative model (e.g., StyleGAN). Sampling from a latent space is performed using ODEs. The joint distribution over attributes is learned by the EBM formulation, and it is then used for sampling from an ordinary differential equation (ODE) to control the hyperparameters of the joint distribution. Sampling is performed in a similar way as in conditional sampling and sequential editing. The controllable generation is achieved by a pre -trained generator that is able to generate a sequence of attribute classifier. The proposed method is evaluated on conditional sampling, sequential editing, and generating photo-realistic images, and is shown to outperform the state-of-the-art. The authors also show that the proposed method can be used to learn attribute combinations, and that the method can also be applied to learn more complex attribute combinations such as energy functions and logical operators. ","Controllable generation from deep generative models is an important problem in real-world applications, and the compositional ability of these models is of great importance. This paper proposes to use energybased models (EBMs) for compositional generation from attributes, and uses them for high-resolution image generation using an EBM in the latent space of a pre-trained generative model (e.g., StyleGAN). Sampling from a latent space is performed using ODEs. The joint distribution over attributes is learned by the EBM formulation, and it is then used for sampling from an ordinary differential equation (ODE) to control the hyperparameters of the joint distribution. Sampling is performed in a similar way as in conditional sampling and sequential editing. The controllable generation is achieved by a pre -trained generator that is able to generate a sequence of attribute classifier. The proposed method is evaluated on conditional sampling, sequential editing, and generating photo-realistic images, and is shown to outperform the state-of-the-art. The authors also show that the proposed method can be used to learn attribute combinations, and that the method can also be applied to learn more complex attribute combinations such as energy functions and logical operators. "
7304,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,common global parameters USED-FOR K - armed stochastic bandits. geometric structure USED-FOR collaborative algorithm. local feature vectors CONJUNCTION raw data. raw data CONJUNCTION local feature vectors. collaborative algorithm USED-FOR heterogeneity. geometric structure FEATURE-OF linear rewards. Fed - PE USED-FOR heterogeneity. Fed - PE HYPONYM-OF collaborative algorithm. near - optimal regrets FEATURE-OF disjoint and shared parameter cases. logarithmic communication costs FEATURE-OF near - optimal regrets. multi - client G - optimal design USED-FOR Fed - PE. tight minimax regret lower bound USED-FOR disjoint parameter case. collinearly - dependent policies HYPONYM-OF concept. synthetic and real - world datasets EVALUATE-FOR algorithms. Method is federated linear contextual bandits model. ,"This paper studies the problem of K-armed stochastic bandits with common global parameters in a federated linear contextual bandits model. The authors propose a new collaborative algorithm called Fed-PE, which is motivated by the geometric structure of linear rewards and the heterogeneity between local feature vectors and raw data. They prove a tight minimax regret lower bound for the disjoint parameter case, and show that Fed- PE is a collaborative algorithm that alleviates the heterogeneity by leveraging the multi-client G-optimal design. They also show that the proposed algorithms achieve near-optimistic regrets with logarithmic communication costs for both disj joint and shared parameter cases. The paper also introduces a new concept called collinearly-dependent policies. Experiments are conducted on both synthetic and real-world datasets to validate the effectiveness of the algorithms.","This paper studies the problem of K-armed stochastic bandits with common global parameters in a federated linear contextual bandits model. The authors propose a new collaborative algorithm called Fed-PE, which is motivated by the geometric structure of linear rewards and the heterogeneity between local feature vectors and raw data. They prove a tight minimax regret lower bound for the disjoint parameter case, and show that Fed- PE is a collaborative algorithm that alleviates the heterogeneity by leveraging the multi-client G-optimal design. They also show that the proposed algorithms achieve near-optimistic regrets with logarithmic communication costs for both disj joint and shared parameter cases. The paper also introduces a new concept called collinearly-dependent policies. Experiments are conducted on both synthetic and real-world datasets to validate the effectiveness of the algorithms."
7320,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"model USED-FOR conservative planning. explicit uncertainty quantification USED-FOR incorporating conservatism. explicit uncertainty quantification USED-FOR model - based algorithms. deep neural networks USED-FOR Uncertainty estimation. complex models USED-FOR Uncertainty estimation. deep neural networks HYPONYM-OF complex models. uncertainty estimation USED-FOR offline model - based RL. offline dataset CONJUNCTION data. data CONJUNCTION offline dataset. COMBO USED-FOR value function. model - based offline RL algorithm USED-FOR value function. rollouts USED-FOR data. COMBO HYPONYM-OF model - based offline RL algorithm. data USED-FOR value function. offline dataset USED-FOR value function. policy improvement guarantee FEATURE-OF COMBO. COMBO COMPARE offline RL. offline RL COMPARE COMBO. offline RL USED-FOR problems. COMBO USED-FOR problems. COMBO COMPARE offline RL methods. offline RL methods COMPARE COMBO. generalization FEATURE-OF problems. offline RL benchmarks EVALUATE-FOR offline RL methods. image - based tasks HYPONYM-OF offline RL benchmarks. Method is dynamics model. Material is logged experience. Task are offline reinforcement learning ( offline RL ), and explicit uncertainty estimation. OtherScientificTerm is model rollouts. ","This paper studies the problem of offline reinforcement learning (offline RL) in the setting where the dynamics model is not available for offline RL. The authors propose a model-based algorithms based on explicit uncertainty quantification for incorporating conservatism through the use of a model to guide conservative planning. Uncertainty estimation is typically performed using complex models such as deep neural networks. The paper proposes to use uncertainty estimation in the context of offline model - based RL, where offline RL can be seen as a special case of offline RL with explicit uncertainty estimation.   The main contribution of the paper is to propose COMBO, a model -based offline RL algorithm that learns a value function from an offline dataset and data using rollouts. The key idea of COMBO is that the value function can be learned from the offline dataset with the help of the model rollouts, and the policy improvement guarantee of the proposed COMBO.  The paper shows that COMBO outperforms other offline RL methods on a number of standard offline RL benchmarks (e.g., image-based tasks) in terms of generalization to new problems, and outperforms offline RL when the dataset is limited to logged experience.","This paper studies the problem of offline reinforcement learning (offline RL) in the setting where the dynamics model is not available for offline RL. The authors propose a model-based algorithms based on explicit uncertainty quantification for incorporating conservatism through the use of a model to guide conservative planning. Uncertainty estimation is typically performed using complex models such as deep neural networks. The paper proposes to use uncertainty estimation in the context of offline model - based RL, where offline RL can be seen as a special case of offline RL with explicit uncertainty estimation.   The main contribution of the paper is to propose COMBO, a model -based offline RL algorithm that learns a value function from an offline dataset and data using rollouts. The key idea of COMBO is that the value function can be learned from the offline dataset with the help of the model rollouts, and the policy improvement guarantee of the proposed COMBO.  The paper shows that COMBO outperforms other offline RL methods on a number of standard offline RL benchmarks (e.g., image-based tasks) in terms of generalization to new problems, and outperforms offline RL when the dataset is limited to logged experience."
7336,SP:ca6f11ed297290e487890660d9a9a088aa106801,"stochastic gradient descent USED-FOR neural networks. deep learning training USED-FOR evolution of features. stochastic differential equations ( SDEs ) USED-FOR evolution of features. backpropagation USED-FOR features. drift term PART-OF SDE. sharp phase transition phenomenon FEATURE-OF intra - class impact. neural collapse of the features HYPONYM-OF geometric structure. local elasticity USED-FOR neural networks. synthesized dataset of geometric shapes CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION synthesized dataset of geometric shapes. Method are deep learning models, modeling strategy, and SDEs. Generic is models. OtherScientificTerm are feature spaces, and vanishing training loss. ","This paper studies the evolution of features during deep learning training using stochastic differential equations (SDEs). The authors show that neural networks trained with stochedastic gradient descent (SGD) exhibit a sharp phase transition phenomenon, which is a result of the drift term in the SDE. They also show that backpropagation can cause the features to collapse during training, and that the intra-class impact is due to a particular kind of geometric structure (neural collapse of the features). The paper also shows that the local elasticity of neural networks is the same as that of deep learning models.    The paper shows that models trained with SDEs are invariant to changes in the feature spaces, and can be seen as a special case of the vanishing training loss. The paper further shows that a simple modeling strategy can be used to show that the vanishing loss can be explained by the vanishing drift term.  The authors also provide a synthetic experiment on a synthesized dataset of geometric shapes and CIFAR-10, and show that a certain geometric structure such as neural collapse is present in the neural networks. ","This paper studies the evolution of features during deep learning training using stochastic differential equations (SDEs). The authors show that neural networks trained with stochedastic gradient descent (SGD) exhibit a sharp phase transition phenomenon, which is a result of the drift term in the SDE. They also show that backpropagation can cause the features to collapse during training, and that the intra-class impact is due to a particular kind of geometric structure (neural collapse of the features). The paper also shows that the local elasticity of neural networks is the same as that of deep learning models.    The paper shows that models trained with SDEs are invariant to changes in the feature spaces, and can be seen as a special case of the vanishing training loss. The paper further shows that a simple modeling strategy can be used to show that the vanishing loss can be explained by the vanishing drift term.  The authors also provide a synthetic experiment on a synthesized dataset of geometric shapes and CIFAR-10, and show that a certain geometric structure such as neural collapse is present in the neural networks. "
7352,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"DRL methods USED-FOR neural network policies. learning programmatic policies USED-FOR generalization. input / output state pairs CONJUNCTION expert demonstrations. expert demonstrations CONJUNCTION input / output state pairs. decision trees CONJUNCTION state machines. state machines CONJUNCTION decision trees. state machines CONJUNCTION predefined program templates. predefined program templates CONJUNCTION state machines. expert demonstrations HYPONYM-OF supervision. predefined program templates HYPONYM-OF limited policy representations. input / output state pairs HYPONYM-OF supervision. state machines HYPONYM-OF limited policy representations. limited policy representations USED-FOR works. decision trees HYPONYM-OF limited policy representations. supervision USED-FOR works. framework USED-FOR program. program embedding space USED-FOR program. framework USED-FOR task - solving programs. framework COMPARE DRL and program synthesis baselines. DRL and program synthesis baselines COMPARE framework. methods USED-FOR program embedding. Method are deep reinforcement learning ( DRL ) methods, and two - stage learning scheme. Generic is task. OtherScientificTerm are reward signals, and programs. ","This paper proposes a two-stage learning framework for learning programmatic policies that generalize well to new tasks. The main idea is to learn a program embedding space for a new task, and then learn a set of neural network policies to solve the program. This is an extension of existing deep reinforcement learning (DRL) methods, where the goal is to generalize the learned policy to unseen tasks. Previous works have used limited policy representations (decision trees, state machines, predefined program templates) with limited supervision (input/output state pairs, expert demonstrations, etc.). The authors propose a framework that learns a program that generalizes well to unseen task-solving programs. The framework is based on the idea that the reward signals of the learned program embeddings can be used to guide the learning of new tasks, and that the learned programs can be learned in an unsupervised way. The authors show that the proposed framework outperforms existing DRL and program synthesis baselines on a number of tasks. They also show that their methods can learn a good program embeding that generalises well to novel tasks.    The authors also propose a two - stage learning scheme, where they first learn the embedding of the program and then train a neural network on the learned embedding. They show that this framework generalizes better than existing methods. ","This paper proposes a two-stage learning framework for learning programmatic policies that generalize well to new tasks. The main idea is to learn a program embedding space for a new task, and then learn a set of neural network policies to solve the program. This is an extension of existing deep reinforcement learning (DRL) methods, where the goal is to generalize the learned policy to unseen tasks. Previous works have used limited policy representations (decision trees, state machines, predefined program templates) with limited supervision (input/output state pairs, expert demonstrations, etc.). The authors propose a framework that learns a program that generalizes well to unseen task-solving programs. The framework is based on the idea that the reward signals of the learned program embeddings can be used to guide the learning of new tasks, and that the learned programs can be learned in an unsupervised way. The authors show that the proposed framework outperforms existing DRL and program synthesis baselines on a number of tasks. They also show that their methods can learn a good program embeding that generalises well to novel tasks.    The authors also propose a two - stage learning scheme, where they first learn the embedding of the program and then train a neural network on the learned embedding. They show that this framework generalizes better than existing methods. "
7368,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"scientific machine learning USED-FOR physicsinformed neural network ( PINN ) models. machine learning methodologies USED-FOR model. soft constraints FEATURE-OF empirical loss function. soft constraints FEATURE-OF physical domain knowledge. physical domain knowledge USED-FOR approach. PINN methodologies USED-FOR models. they USED-FOR physical phenomena. soft regularization USED-FOR subtle problems. soft regularization USED-FOR PINNs. PDE - based differential operators PART-OF soft regularization. PINN ’s setup USED-FOR loss landscape. solutions USED-FOR failure modes. curriculum regularization USED-FOR PINN ’s loss term. curriculum regularization USED-FOR approach. PDE regularization USED-FOR PINN ’s loss term. approach USED-FOR problem. sequence - to - sequence learning task USED-FOR problem. methods COMPARE regular PINN training. regular PINN training COMPARE methods. Task are physical interest, and learning differential equations. OtherScientificTerm is differential equations. Method are NN architecture, and NN. ","This paper studies the problem of physicsinformed neural network (PINN) models for scientific machine learning. PINN methodologies are well-known for learning models that are robust to perturbations in physical interest, and for learning differential equations. This paper proposes a new approach that leverages physical domain knowledge with soft constraints on the empirical loss function.    The key idea is to use machine learning methodologies to learn a model that is robust to changes in the physical interest. The authors show that PINNs are sensitive to soft regularization, and that they are able to learn physical phenomena that are not present in the original NN architecture. They also show that soft regularisation can be used to tackle subtle problems where the NN is not robust enough to learn PDE-based differential operators.  The paper also shows that the loss landscape of a PINN’s setup can be approximated by a sequence-to-sequence learning task, where the goal is to find solutions to avoid failure modes where the solution to a particular problem is not available. The proposed approach is based on curriculum regularization that is used to regularize the loss term of the PINN with PDE regularization.  Experiments show that the proposed methods are more robust than regular PINN training, and can be applied to a variety of problems that are more challenging to solve (e.g. the problem is difficult to solve due to the lack of a sequence of solutions). ","This paper studies the problem of physicsinformed neural network (PINN) models for scientific machine learning. PINN methodologies are well-known for learning models that are robust to perturbations in physical interest, and for learning differential equations. This paper proposes a new approach that leverages physical domain knowledge with soft constraints on the empirical loss function.    The key idea is to use machine learning methodologies to learn a model that is robust to changes in the physical interest. The authors show that PINNs are sensitive to soft regularization, and that they are able to learn physical phenomena that are not present in the original NN architecture. They also show that soft regularisation can be used to tackle subtle problems where the NN is not robust enough to learn PDE-based differential operators.  The paper also shows that the loss landscape of a PINN’s setup can be approximated by a sequence-to-sequence learning task, where the goal is to find solutions to avoid failure modes where the solution to a particular problem is not available. The proposed approach is based on curriculum regularization that is used to regularize the loss term of the PINN with PDE regularization.  Experiments show that the proposed methods are more robust than regular PINN training, and can be applied to a variety of problems that are more challenging to solve (e.g. the problem is difficult to solve due to the lack of a sequence of solutions). "
7384,SP:cfd501bca783590a78305f0592f537e8f20bce27,"domaininvariant representations USED-FOR domain shift. domaininvariant representations USED-FOR unsupervised domain adaptation ( UDA ). self - training USED-FOR UDA. self - training USED-FOR unlabeled target data. Cycle Self - Training ( CST ) HYPONYM-OF self - training algorithm. CST USED-FOR target pseudo - labels. source - trained classifier USED-FOR CST. source - trained classifier USED-FOR target pseudo - labels. shared representations USED-FOR classifier. CST USED-FOR classifier. target pseudo - labels USED-FOR CST. target pseudo - labels USED-FOR classifier. Tsallis entropy USED-FOR confidence - friendly regularization. invariant feature learning CONJUNCTION vanilla self - training. vanilla self - training CONJUNCTION invariant feature learning. CST COMPARE state - of - the - arts. state - of - the - arts COMPARE CST. visual recognition and sentiment analysis benchmarks EVALUATE-FOR state - of - the - arts. visual recognition and sentiment analysis benchmarks EVALUATE-FOR CST. OtherScientificTerm are hardness or impossibility theorems, distributional shift, and pseudo - labels. Generic is forward step. ","This paper proposes Cycle Self-Training (CST), a self-training algorithm that learns domaininvariant representations for unsupervised domain adaptation (UDA) from unlabeled target data. The authors show that self-learning for UDA can be improved by learning from the source domain to the target domain without hardness or impossibility theorems. To this end, the authors propose a new forward step, called Cycle-Self-Training, which learns a classifier from the shared representations of the source and target domains.   The authors also propose a confidence-friendly regularization based on Tsallis entropy to encourage the classifier to learn from the target pseudo-labels. They show that the proposed CST learns to learn to predict target pseudo labels from target data using the source-trained classifier, and then applies the learned classifier on the target data to learn a new classifier based on the new class labels.  Experiments are conducted on visual recognition and sentiment analysis benchmarks and show that CST outperforms the state-of-the-arts in terms of invariant feature learning and vanilla self-trainings. The paper also shows that the distributional shift can be alleviated by the use of pseudo-labeling.","This paper proposes Cycle Self-Training (CST), a self-training algorithm that learns domaininvariant representations for unsupervised domain adaptation (UDA) from unlabeled target data. The authors show that self-learning for UDA can be improved by learning from the source domain to the target domain without hardness or impossibility theorems. To this end, the authors propose a new forward step, called Cycle-Self-Training, which learns a classifier from the shared representations of the source and target domains.   The authors also propose a confidence-friendly regularization based on Tsallis entropy to encourage the classifier to learn from the target pseudo-labels. They show that the proposed CST learns to learn to predict target pseudo labels from target data using the source-trained classifier, and then applies the learned classifier on the target data to learn a new classifier based on the new class labels.  Experiments are conducted on visual recognition and sentiment analysis benchmarks and show that CST outperforms the state-of-the-arts in terms of invariant feature learning and vanilla self-trainings. The paper also shows that the distributional shift can be alleviated by the use of pseudo-labeling."
7400,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"unsupervised representation learning CONJUNCTION structured network pruning. structured network pruning CONJUNCTION unsupervised representation learning. deep learning USED-FOR compact representations of features. neural network USED-FOR compact representations of features. DiscriminAtive Masking ( DAM ) HYPONYM-OF single - stage structured pruning method. graph representation learning CONJUNCTION structured pruning. structured pruning CONJUNCTION graph representation learning. recommendation system CONJUNCTION graph representation learning. graph representation learning CONJUNCTION recommendation system. dimensionality reduction CONJUNCTION recommendation system. recommendation system CONJUNCTION dimensionality reduction. structured pruning USED-FOR image classification. representation learning CONJUNCTION structured pruning. structured pruning CONJUNCTION representation learning. applications USED-FOR structured pruning. applications USED-FOR representation learning. applications EVALUATE-FOR DAM approach. graph representation learning HYPONYM-OF applications. dimensionality reduction HYPONYM-OF applications. recommendation system HYPONYM-OF applications. learning objective FEATURE-OF DAM. Generic are state - of - the - art methods, and systematic approach. Method are fine - tuning, and dam - pytorch. OtherScientificTerm is masking layer. ","This paper proposes a single-stage structured pruning method, called DiscriminAtive Masking (DAM), which uses deep learning to learn compact representations of features extracted by a neural network. The authors show that the state-of-the-art methods do not generalize well when the masking layer is removed during fine-tuning, and propose a systematic approach, called dam-pytorch, to address this issue. The DAM approach is evaluated on three applications: dimensionality reduction, recommendation system, graph representation learning, and structured network pruning for image classification. The paper also shows that the learning objective of DAM can be applied to any learning objective. ","This paper proposes a single-stage structured pruning method, called DiscriminAtive Masking (DAM), which uses deep learning to learn compact representations of features extracted by a neural network. The authors show that the state-of-the-art methods do not generalize well when the masking layer is removed during fine-tuning, and propose a systematic approach, called dam-pytorch, to address this issue. The DAM approach is evaluated on three applications: dimensionality reduction, recommendation system, graph representation learning, and structured network pruning for image classification. The paper also shows that the learning objective of DAM can be applied to any learning objective. "
7416,SP:f831d25830efa88434b43e900241a5ad81119360,"compositional reasoning CONJUNCTION reuse of knowledge. reuse of knowledge CONJUNCTION compositional reasoning. they USED-FOR systematic generalization. architecture USED-FOR inference. self - attention network USED-FOR inference. functions HYPONYM-OF system of modules. architecture USED-FOR capacity extension. architecture USED-FOR computation. image classification CONJUNCTION visual abstract reasoning. visual abstract reasoning CONJUNCTION image classification. settings EVALUATE-FOR Neural Interpreters. settings EVALUATE-FOR it. image classification EVALUATE-FOR it. Raven Progressive Matrices USED-FOR visual abstract reasoning. visual abstract reasoning HYPONYM-OF settings. image classification HYPONYM-OF settings. Neural Interpreters COMPARE vision transformer. vision transformer COMPARE Neural Interpreters. former EVALUATE-FOR Neural Interpreters. Neural Interpreters COMPARE state - of - the - art. state - of - the - art COMPARE Neural Interpreters. latter EVALUATE-FOR Neural Interpreters. systematic generalization EVALUATE-FOR state - of - the - art. systematic generalization EVALUATE-FOR Neural Interpreters. Method is neural network architectures. Generic are model, and task. ","This paper proposes a new architecture for neural network architectures that is able to perform compositional reasoning and reuse of knowledge. The authors argue that they are able to achieve systematic generalization because they can learn a system of modules (i.e., functions) that can be used to extend the capacity of the whole model to a new task. The proposed architecture allows for inference using a self-attention network, and for capacity extension using a different architecture for different types of computation. They evaluate it in two settings: image classification and visual abstract reasoning using Raven Progressive Matrices, and show that Neural Interpreters outperform the state-of-the-art in the former, and the latter outperforms the state of the art in the latter. ","This paper proposes a new architecture for neural network architectures that is able to perform compositional reasoning and reuse of knowledge. The authors argue that they are able to achieve systematic generalization because they can learn a system of modules (i.e., functions) that can be used to extend the capacity of the whole model to a new task. The proposed architecture allows for inference using a self-attention network, and for capacity extension using a different architecture for different types of computation. They evaluate it in two settings: image classification and visual abstract reasoning using Raven Progressive Matrices, and show that Neural Interpreters outperform the state-of-the-art in the former, and the latter outperforms the state of the art in the latter. "
7432,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"fine - tuning strategies USED-FOR transfer. transfer USED-FOR challenging domains. fine - tuning strategies USED-FOR challenging domains. pre - trained policies USED-FOR exploration. Behavior Transfer ( BT ) HYPONYM-OF technique. pre - trained policies USED-FOR technique. intrinsic motivation 10 objectives USED-FOR complex behaviors. large - scale pre - training CONJUNCTION intrinsic motivation 10 objectives. intrinsic motivation 10 objectives CONJUNCTION large - scale pre - training. BT CONJUNCTION fine - tuning strategies. fine - tuning strategies CONJUNCTION BT. BT USED-FOR pre - trained 11 policies. Generic is it. Task are reinforcement learning, unsupervised pre - training phase, reinforcement learning problem, and structured exploration. Method is fine3 tuning neural network weights. OtherScientificTerm are rewards, neural network weights, pre - training, and pre - trained 15 policies. Material is supervised domains. ","This paper proposes a new technique called Behavior Transfer (BT) which is a technique that uses pre-trained policies and fine-tuning strategies to improve transfer to challenging domains. The authors argue that it is useful in reinforcement learning because the unsupervised pre-training phase can be extended to a more challenging setting where the rewards are more complex and the reinforcement learning problem is more challenging. They also argue that fine3 tuning neural network weights in this setting can be more effective than fine-tuneing the weights in the supervised setting.  The authors propose a technique called behavior transfer (BT), which is an extension of an existing technique called “Behavioral Transfer”. The key idea is to use pre-trained policies to guide exploration during the exploration phase, and then use the learned policies during the fine3tuning phase to transfer the learned rewards to a new task. The motivation for this is that the reward of a task is more complex than that of a simple reinforcement learning task, and it is more difficult to transfer from one task to another. To this end, the authors propose to pre-train a policy in a supervised setting and then fine-train it on a different task. They show that the resulting policy is able to transfer to the new task more effectively than the one it was trained on. They further show that BT can be combined with other fine - tuning strategies to increase transfer to more challenging domains, and that it can be used to improve the transfer from a single task to multiple tasks.  They also show that this transfer can be done in a way that is beneficial to the agent. They demonstrate that large-scale pre-pre-training, intrinsic motivation 10 objectives for complex behaviors, as well-suited for structured exploration, and BT can transfer from pre-learned 11 policies to a task with more complex behaviors.  Finally, they show that in supervised domains, BT and other fine- tuning strategies are effective at transferring the learned policy to new tasks. In particular, BT is shown to transfer well from a task where the agent is trained on one task (e.g. a reinforcement learning setting) to another task (i.e. a more complex task). The authors also demonstrate that BT is effective at fine-tracking the transfer between two tasks, and show that it also works well in the case of structured exploration.   ","This paper proposes a new technique called Behavior Transfer (BT) which is a technique that uses pre-trained policies and fine-tuning strategies to improve transfer to challenging domains. The authors argue that it is useful in reinforcement learning because the unsupervised pre-training phase can be extended to a more challenging setting where the rewards are more complex and the reinforcement learning problem is more challenging. They also argue that fine3 tuning neural network weights in this setting can be more effective than fine-tuneing the weights in the supervised setting.  The authors propose a technique called behavior transfer (BT), which is an extension of an existing technique called “Behavioral Transfer”. The key idea is to use pre-trained policies to guide exploration during the exploration phase, and then use the learned policies during the fine3tuning phase to transfer the learned rewards to a new task. The motivation for this is that the reward of a task is more complex than that of a simple reinforcement learning task, and it is more difficult to transfer from one task to another. To this end, the authors propose to pre-train a policy in a supervised setting and then fine-train it on a different task. They show that the resulting policy is able to transfer to the new task more effectively than the one it was trained on. They further show that BT can be combined with other fine - tuning strategies to increase transfer to more challenging domains, and that it can be used to improve the transfer from a single task to multiple tasks.  They also show that this transfer can be done in a way that is beneficial to the agent. They demonstrate that large-scale pre-pre-training, intrinsic motivation 10 objectives for complex behaviors, as well-suited for structured exploration, and BT can transfer from pre-learned 11 policies to a task with more complex behaviors.  Finally, they show that in supervised domains, BT and other fine- tuning strategies are effective at transferring the learned policy to new tasks. In particular, BT is shown to transfer well from a task where the agent is trained on one task (e.g. a reinforcement learning setting) to another task (i.e. a more complex task). The authors also demonstrate that BT is effective at fine-tracking the transfer between two tasks, and show that it also works well in the case of structured exploration.   "
7448,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"machine learning approaches USED-FOR ranking. performance metrics of interest CONJUNCTION surrogate loss functions. surrogate loss functions CONJUNCTION performance metrics of interest. gradient - based methods USED-FOR surrogate loss functions. sorting operation PART-OF ranking metrics. ranking metrics EVALUATE-FOR surrogates. differentiable surrogates USED-FOR ranking. PiRank HYPONYM-OF differentiable surrogates. continuous, temperature - controlled relaxation USED-FOR sorting operator. NeuralSort USED-FOR continuous, temperature - controlled relaxation. continuous, temperature - controlled relaxation USED-FOR PiRank. continuous, temperature - controlled relaxation USED-FOR differentiable surrogates. NeuralSort USED-FOR differentiable surrogates. PiRank USED-FOR metrics. PiRank COMPARE approaches. approaches COMPARE PiRank. OtherScientificTerm are model parameters, and zero temperature. Task are real - world applications, and training. Method is divideand - conquer extension. ","This paper proposes a new method for differentiable surrogates of differentiable ranking algorithms. The authors propose to use a divide-and-conquer (DAC) approach to solve the problem. The DAC approach is based on the idea of divide and conquer, which is an extension of divideand-contain. The paper shows that DAC can be used to compute differentiable surrogate loss functions, which are then used to train a differentiable sorting operator. The proposed method is evaluated on a number of different ranking metrics, and compared with several baselines.","This paper proposes a new method for differentiable surrogates of differentiable ranking algorithms. The authors propose to use a divide-and-conquer (DAC) approach to solve the problem. The DAC approach is based on the idea of divide and conquer, which is an extension of divideand-contain. The paper shows that DAC can be used to compute differentiable surrogate loss functions, which are then used to train a differentiable sorting operator. The proposed method is evaluated on a number of different ranking metrics, and compared with several baselines."
7464,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"they USED-FOR near - term quantum devices. machine learning USED-FOR problem. methods USED-FOR VQE structure optimization. reinforcement learning algorithm USED-FOR ansatzes. reinforcement learning algorithm USED-FOR economic circuits. feedback - driven curriculum learning method USED-FOR learning problem. complexity EVALUATE-FOR learning problem. it USED-FOR circuit depth. feedback - driven curriculum learning method USED-FOR algorithm. chemical accuracy EVALUATE-FOR benchmark problem. Task are Variational Quantum Eigensolvers ( VQEs ), and optimization of the VQE ansatz. OtherScientificTerm are variational ansatz, near - term restrictions, VQE ansatz, low depth, and ground energy estimates. Method is learning algorithm. ","This paper considers the problem of learning Variational Quantum Eigensolvers (VQEs) where they are used to design near-term quantum devices. The problem is formulated as learning a variational ansatz, which is a generalization of the classical optimization of the VQE ansatz. The authors propose to use machine learning to solve this problem. They show that existing methods for VQEs structure optimization are computationally intractable, and propose a reinforcement learning algorithm to learn such ansatzes. The algorithm is based on a feedback-driven curriculum learning method, where the learning problem is solved by reducing the complexity of the problem to a learning problem that is computationally tractable. The learning algorithm is shown to outperform existing methods on a benchmark problem with chemical accuracy.    The main contribution of the paper is that the authors propose an algorithm that learns a learning algorithm that can be used to learn an economic circuits, and that it is able to learn a circuit depth that matches the circuit depth of the circuit. This is achieved by learning a low-depth version of the learning algorithm. The paper also shows that this learning algorithm can be applied to a number of existing methods, and it is shown that it can learn circuits that satisfy a set of requirements (e.g., low depth, low depth with respect to the ground energy estimates).","This paper considers the problem of learning Variational Quantum Eigensolvers (VQEs) where they are used to design near-term quantum devices. The problem is formulated as learning a variational ansatz, which is a generalization of the classical optimization of the VQE ansatz. The authors propose to use machine learning to solve this problem. They show that existing methods for VQEs structure optimization are computationally intractable, and propose a reinforcement learning algorithm to learn such ansatzes. The algorithm is based on a feedback-driven curriculum learning method, where the learning problem is solved by reducing the complexity of the problem to a learning problem that is computationally tractable. The learning algorithm is shown to outperform existing methods on a benchmark problem with chemical accuracy.    The main contribution of the paper is that the authors propose an algorithm that learns a learning algorithm that can be used to learn an economic circuits, and that it is able to learn a circuit depth that matches the circuit depth of the circuit. This is achieved by learning a low-depth version of the learning algorithm. The paper also shows that this learning algorithm can be applied to a number of existing methods, and it is shown that it can learn circuits that satisfy a set of requirements (e.g., low depth, low depth with respect to the ground energy estimates)."
7480,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"Transductive inference USED-FOR few - shot learning. it COMPARE inductive counterpart. inductive counterpart COMPARE it. statistics of the unlabeled query set USED-FOR few - shot task. it USED-FOR few - shot task. statistics of the unlabeled query set USED-FOR it. class - balanced tasks USED-FOR inference. few - shot benchmarks EVALUATE-FOR inference. class - balanced tasks FEATURE-OF few - shot benchmarks. arbitrary and unknown label marginals FEATURE-OF unlabeled query sets. few - shot tasks USED-FOR inference. arbitrary class distributions FEATURE-OF few - shot tasks. arbitrary class distributions USED-FOR inference. Dirichlet - distributed random variables USED-FOR marginal probabilities. arbitrary class distributions FEATURE-OF testing tasks. α - divergences USED-FOR mutual - information loss. transductive α - divergence optimization COMPARE state - of - the - art methods. state - of - the - art methods COMPARE transductive α - divergence optimization. OtherScientificTerm are artificial regularity, marginal label probability, uniform distribution, class - balance artefact, simplex, class - distribution variations, and few - shot settings. Method are transductive methods, and inductive methods. ","This paper proposes Transductive inference for few-shot learning, where the goal is to avoid artificial regularity in the marginal label probability. The authors show that it outperforms its inductive counterpart when it relies on the statistics of the unlabeled query set for a few -shot task. They also show that inference on class-balanced tasks improves the performance of inference on a number of standard few-shots benchmarks.    The main contribution of the paper is that the marginal probabilities are approximated by Dirichlet-distributed random variables, and that it can be applied to any few-set tasks with arbitrary class distributions. The paper also shows that transductive methods do not suffer from the class-balance artefact, which is a result of class-distribution variations in testing tasks.  In particular, the paper shows that for a uniform distribution, the marginal probability of a test set is equal to that of the true test set, while for a simplex, it's equal to the uniform distribution of the test set.  The paper then shows that in the case of testing with arbitrary and unknown label marginals for unlabeling query sets, the authors propose to use the mutual-information loss based on the α-divergences between the true and marginal probabilities of test sets.  Finally, they show that in a few set of testing tasks, they can achieve better performance than existing methods under arbitrary class distribution variations. They show that the proposed transductively optimizing the mutual information loss outperforms state-of-based methods, and they also demonstrate that the performance improvement is due to the transductivity of the proposed method is more pronounced than that of existing inductive methods. ","This paper proposes Transductive inference for few-shot learning, where the goal is to avoid artificial regularity in the marginal label probability. The authors show that it outperforms its inductive counterpart when it relies on the statistics of the unlabeled query set for a few -shot task. They also show that inference on class-balanced tasks improves the performance of inference on a number of standard few-shots benchmarks.    The main contribution of the paper is that the marginal probabilities are approximated by Dirichlet-distributed random variables, and that it can be applied to any few-set tasks with arbitrary class distributions. The paper also shows that transductive methods do not suffer from the class-balance artefact, which is a result of class-distribution variations in testing tasks.  In particular, the paper shows that for a uniform distribution, the marginal probability of a test set is equal to that of the true test set, while for a simplex, it's equal to the uniform distribution of the test set.  The paper then shows that in the case of testing with arbitrary and unknown label marginals for unlabeling query sets, the authors propose to use the mutual-information loss based on the α-divergences between the true and marginal probabilities of test sets.  Finally, they show that in a few set of testing tasks, they can achieve better performance than existing methods under arbitrary class distribution variations. They show that the proposed transductively optimizing the mutual information loss outperforms state-of-based methods, and they also demonstrate that the performance improvement is due to the transductivity of the proposed method is more pronounced than that of existing inductive methods. "
7496,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"imbalanced memory distribution FEATURE-OF convolutional neural network ( CNN ) designs. overlapping patches CONJUNCTION computation overhead. computation overhead CONJUNCTION overlapping patches. receptive field redistribution USED-FOR receptive field. receptive field redistribution USED-FOR FLOPs. receptive field redistribution USED-FOR computation overhead. neural architecture CONJUNCTION inference scheduling. inference scheduling CONJUNCTION neural architecture. peak memory usage EVALUATE-FOR networks. neural networks USED-FOR MCUNetV2. Patch - based inference USED-FOR networks. visual wake words dataset EVALUATE-FOR accuracy. MCU EVALUATE-FOR MCUNetV2. ImageNet accuracy EVALUATE-FOR MCUNetV2. peak memory usage EVALUATE-FOR Patch - based inference. accuracy EVALUATE-FOR MCUNetV2. visual wake words dataset EVALUATE-FOR MCUNetV2. MCUNetV2 USED-FOR object detection. tiny devices USED-FOR object detection. mAP EVALUATE-FOR MCUNetV2. memory bottleneck PART-OF tinyML. image classification HYPONYM-OF vision applications. Task are Tiny deep learning, and Manually redistributing the receptive field. OtherScientificTerm are limited memory size, feature map, and peak memory. Metric is memory usage. Generic is network. Method are generic patch - by - patch inference scheduling, naive implementation, and neural architecture search. Material is Pascal VOC. ","This paper studies the problem of patch-by-patch inference in deep learning. Tiny deep learning has been a hot topic in the last few years due to limited memory size and limited computational resources. In particular, many convolutional neural network (CNN) designs suffer from an imbalanced memory distribution due to overlapping patches and computation overhead due to the limited capacity of the feature map. This paper proposes to address this issue by manually redistributing the receptive field to reduce FLOPs. The paper shows that receptive field redistribution can be used to reduce the number of overlapping patches, and thus reduce the computation overhead. Patch-based inference is shown to significantly reduce the peak memory usage of the networks trained with different neural networks.    The paper also shows that MCUNetV2, a variant of neural networks trained on a single MCU, achieves the same ImageNet accuracy as a fully connected MCU and achieves similar accuracy on the visual wake words dataset. The main contribution of the paper is that the network can be trained with a single neural architecture and inference scheduling, and the paper also provides a naive implementation that can be applied to any network.  This paper also proposes a generic patch-based-inference scheduling that is applicable to any neural architecture search, and shows that it can be implemented in a way that is scalable and efficient.  The authors also show that the performance on Pascal VOC is comparable to the state-of-the-art on ImageNet, and MCUNETV2 achieves the best performance on the mAP on the ImageNet dataset.  Finally, the paper shows the performance of MCUNAT on object detection on tiny devices on the tiny devices, and on the Pascal mAP. This work shows that the memory bottleneck in tinyML is a common problem in many vision applications (e.g. image classification, object detection). ","This paper studies the problem of patch-by-patch inference in deep learning. Tiny deep learning has been a hot topic in the last few years due to limited memory size and limited computational resources. In particular, many convolutional neural network (CNN) designs suffer from an imbalanced memory distribution due to overlapping patches and computation overhead due to the limited capacity of the feature map. This paper proposes to address this issue by manually redistributing the receptive field to reduce FLOPs. The paper shows that receptive field redistribution can be used to reduce the number of overlapping patches, and thus reduce the computation overhead. Patch-based inference is shown to significantly reduce the peak memory usage of the networks trained with different neural networks.    The paper also shows that MCUNetV2, a variant of neural networks trained on a single MCU, achieves the same ImageNet accuracy as a fully connected MCU and achieves similar accuracy on the visual wake words dataset. The main contribution of the paper is that the network can be trained with a single neural architecture and inference scheduling, and the paper also provides a naive implementation that can be applied to any network.  This paper also proposes a generic patch-based-inference scheduling that is applicable to any neural architecture search, and shows that it can be implemented in a way that is scalable and efficient.  The authors also show that the performance on Pascal VOC is comparable to the state-of-the-art on ImageNet, and MCUNETV2 achieves the best performance on the mAP on the ImageNet dataset.  Finally, the paper shows the performance of MCUNAT on object detection on tiny devices on the tiny devices, and on the Pascal mAP. This work shows that the memory bottleneck in tinyML is a common problem in many vision applications (e.g. image classification, object detection). "
7512,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"unstructured dynamic environments FEATURE-OF Bayesian automated mechanism design. optimal mechanism USED-FOR principal ’s utility. algorithm USED-FOR optimal mechanisms. linear program formulation USED-FOR algorithm. constant factor FEATURE-OF principal ’s optimal utility. unstructured environments FEATURE-OF automated dynamic mechanism design. time complexity EVALUATE-FOR algorithm. optimality CONJUNCTION computational tractability. computational tractability CONJUNCTION optimality. solution USED-FOR problem. Markov decision processes USED-FOR memoryless mechanisms. algorithms USED-FOR synthetic dynamic environments. algorithms USED-FOR algorithms. OtherScientificTerm are self - interested strategic agent, payments, individual - rationality constraints, time horizon, and strategic behavior. Task is dynamic mechanism design. ","This paper considers the problem of Bayesian automated mechanism design in unstructured dynamic environments, where a self-interested strategic agent is given a sequence of tasks to complete, and the goal is to design an optimal mechanism that maximizes the principal’s utility by a constant factor. The authors propose an algorithm for designing optimal mechanisms that is based on a linear program formulation. The algorithm has a time complexity of $O(\sqrt{T}^T})$, where $T$ is the number of tasks, $T$, the amount of time it takes to complete each task, and $T^T$ the amount to complete the task.   The authors consider the problem in the setting of automated dynamic mechanism design, which is a special case of the problem considered in the paper, in which the agent has access to a set of trajectories that are not available in the environment, but are available to the agent via a series of individual-rationality constraints. They show that the optimal mechanism design can be formulated as a Bayesian optimization problem, and that the algorithm can be solved efficiently with respect to both optimality and computational tractability. They also provide a solution to the problem for the case of Markov decision processes, which allows for memoryless mechanisms.  They also show that their algorithms can be used to design algorithms for synthetic dynamic environments that can be applied to existing algorithms for automated dynamic environments.  Finally, the authors provide a theoretical analysis of their algorithm, showing that their algorithm is optimal when the time horizon is long enough and the strategic behavior is well-behaved. ","This paper considers the problem of Bayesian automated mechanism design in unstructured dynamic environments, where a self-interested strategic agent is given a sequence of tasks to complete, and the goal is to design an optimal mechanism that maximizes the principal’s utility by a constant factor. The authors propose an algorithm for designing optimal mechanisms that is based on a linear program formulation. The algorithm has a time complexity of $O(\sqrt{T}^T})$, where $T$ is the number of tasks, $T$, the amount of time it takes to complete each task, and $T^T$ the amount to complete the task.   The authors consider the problem in the setting of automated dynamic mechanism design, which is a special case of the problem considered in the paper, in which the agent has access to a set of trajectories that are not available in the environment, but are available to the agent via a series of individual-rationality constraints. They show that the optimal mechanism design can be formulated as a Bayesian optimization problem, and that the algorithm can be solved efficiently with respect to both optimality and computational tractability. They also provide a solution to the problem for the case of Markov decision processes, which allows for memoryless mechanisms.  They also show that their algorithms can be used to design algorithms for synthetic dynamic environments that can be applied to existing algorithms for automated dynamic environments.  Finally, the authors provide a theoretical analysis of their algorithm, showing that their algorithm is optimal when the time horizon is long enough and the strategic behavior is well-behaved. "
7528,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,Graph Neural Networks ( GNNs ) architectures USED-FOR tasks. Neural Architecture Search ( NAS ) USED-FOR GNN architectures. GNN architectures USED-FOR tasks. Neural Architecture Search ( NAS ) COMPARE manually designed architectures. manually designed architectures COMPARE Neural Architecture Search ( NAS ). Neural Architecture Search ( NAS ) USED-FOR tasks. NAS USED-FOR GNN architectures. NAS USED-FOR GNN structures. gradient based NAS methods USED-FOR architectures. gradient based NAS USED-FOR searching suboptimal GNN architectures. graph structure learning USED-FOR search procedure. graph structure learning USED-FOR denoising process. denoising process USED-FOR search procedure. Structure Optimization ( GASSO ) USED-FOR Graph differentiable Architecture Search model. graph structure learning USED-FOR graph neural architectures. real - world graph datasets EVALUATE-FOR GASSO model. GASSO model COMPARE baselines. baselines COMPARE GASSO model. real - world graph datasets EVALUATE-FOR baselines. OtherScientificTerm is graph. Method is gradient descent. ,"Graph Neural Networks (GNNs) architectures can be used for a variety of tasks, and Neural Architecture Search (NAS) has been shown to outperform manually designed architectures on many tasks. This paper proposes to use NAS to search for GNN architectures that are more suitable for these tasks. The main contribution of this paper is a Graph differentiable Architecture Search model based on Structure Optimization (GASSO) that uses graph structure learning to guide the search procedure during the denoising process. The authors show that NAS can find GNN structures that are better suited for a number of different tasks. They also show that gradient based NAS methods can be applied to find such architectures. The paper also shows that searching suboptimal GNNs is a more efficient way of searching than gradient descent. The GASSO model is evaluated on several real-world graph datasets and shows that the Gasso model outperforms several baselines.","Graph Neural Networks (GNNs) architectures can be used for a variety of tasks, and Neural Architecture Search (NAS) has been shown to outperform manually designed architectures on many tasks. This paper proposes to use NAS to search for GNN architectures that are more suitable for these tasks. The main contribution of this paper is a Graph differentiable Architecture Search model based on Structure Optimization (GASSO) that uses graph structure learning to guide the search procedure during the denoising process. The authors show that NAS can find GNN structures that are better suited for a number of different tasks. They also show that gradient based NAS methods can be applied to find such architectures. The paper also shows that searching suboptimal GNNs is a more efficient way of searching than gradient descent. The GASSO model is evaluated on several real-world graph datasets and shows that the Gasso model outperforms several baselines."
7544,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"Clustering HYPONYM-OF unsupervised learning problem. metric space FEATURE-OF clusters. fairness FEATURE-OF algorithm. cost FEATURE-OF clustering objective. upper bound USED-FOR clustering problem. upper bound USED-FOR constraint. constraint USED-FOR clustering problem. upper bound USED-FOR clustering objective. it USED-FOR equality of representation. group utilitarian objective CONJUNCTION group egalitarian objective. group egalitarian objective CONJUNCTION group utilitarian objective. group egalitarian objective CONJUNCTION group leximin objective. group leximin objective CONJUNCTION group egalitarian objective. group leximin objective CONJUNCTION group egalitarian objective. group egalitarian objective CONJUNCTION group leximin objective. group leximin objective HYPONYM-OF fairness objectives. group egalitarian objective HYPONYM-OF fairness objectives. group utilitarian objective HYPONYM-OF fairness objectives. algorithms USED-FOR them. lower bounds USED-FOR approximation of the utilitarian and egalitarian objectives. heuristic algorithm USED-FOR leximin objective. impossibility results USED-FOR natural fairness objectives. real - world datasets EVALUATE-FOR algorithms. Method is fair clustering. OtherScientificTerm are group membership, group fairness, and utilitarian and egalitarian objectives. Generic is model. ","This paper considers the problem of fair clustering, i.e., the problem that a group membership of a group is fair to all members of the same group. Clustering is an unsupervised learning problem, where the goal is to find clusters in the metric space that are fair to the entire group. The authors propose an algorithm for this problem, which is based on the observation that the fairness of the algorithm depends on the cost of the clustering objective. They show that under certain conditions, the upper bound of the cost for this clustering problem can be used as a constraint to enforce the group fairness. They also show that it can enforce the equality of representation across groups.   The authors then propose three fairness objectives: the group utilitarian objective, the group egalitarian objective, and the group leximin objective. The fairness objectives are approximations of existing algorithms for them. They provide lower bounds for the approximation of the utilitarian and egalitarian objectives, and show that their algorithms are robust to the number of groups and the amount of training data. They further propose a heuristic algorithm for the leximine objective. Finally, they provide impossibility results for natural fairness objectives. They evaluate their algorithms on three real-world datasets and show the performance of their algorithms. ","This paper considers the problem of fair clustering, i.e., the problem that a group membership of a group is fair to all members of the same group. Clustering is an unsupervised learning problem, where the goal is to find clusters in the metric space that are fair to the entire group. The authors propose an algorithm for this problem, which is based on the observation that the fairness of the algorithm depends on the cost of the clustering objective. They show that under certain conditions, the upper bound of the cost for this clustering problem can be used as a constraint to enforce the group fairness. They also show that it can enforce the equality of representation across groups.   The authors then propose three fairness objectives: the group utilitarian objective, the group egalitarian objective, and the group leximin objective. The fairness objectives are approximations of existing algorithms for them. They provide lower bounds for the approximation of the utilitarian and egalitarian objectives, and show that their algorithms are robust to the number of groups and the amount of training data. They further propose a heuristic algorithm for the leximine objective. Finally, they provide impossibility results for natural fairness objectives. They evaluate their algorithms on three real-world datasets and show the performance of their algorithms. "
7560,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"neural - network - based graph generative models USED-FOR real - world network characteristics. high triangle density HYPONYM-OF real - world network characteristics. variational graph autoencoders CONJUNCTION CELL. CELL CONJUNCTION variational graph autoencoders. NetGAN CONJUNCTION variational graph autoencoders. variational graph autoencoders CONJUNCTION NetGAN. Erdös - Rényi and stochastic block models CONJUNCTION generative models. generative models CONJUNCTION Erdös - Rényi and stochastic block models. CELL HYPONYM-OF generative models. NetGAN HYPONYM-OF generative models. variational graph autoencoders HYPONYM-OF generative models. generative models PART-OF models. Erdös - Rényi and stochastic block models PART-OF models. edge independent models USED-FOR graphs. real - world social networks CONJUNCTION graphs. graphs CONJUNCTION real - world social networks. generative model USED-FOR graph statistics. overlap CONJUNCTION accuracy. accuracy CONJUNCTION overlap. overlap EVALUATE-FOR generative model. accuracy EVALUATE-FOR generative model. Method is edge independent random graph models. OtherScientificTerm are graph, and bounded overlap condition. Generic is model. ","This paper studies edge-independent random graph models. The authors propose to use neural-network-based graph generative models to capture real-world network characteristics such as high triangle density. The proposed models are a combination of three existing generative approaches: NetGAN, variational graph autoencoders, and CELL. The models are based on Erdös-Rényi and stochastic block models, and other recent advances in edge independent models for graphs.  The authors show that the generative model can capture graph statistics of real-life social networks and graphs, and that the model is able to capture the overlap and accuracy of the graph under a bounded overlap condition.","This paper studies edge-independent random graph models. The authors propose to use neural-network-based graph generative models to capture real-world network characteristics such as high triangle density. The proposed models are a combination of three existing generative approaches: NetGAN, variational graph autoencoders, and CELL. The models are based on Erdös-Rényi and stochastic block models, and other recent advances in edge independent models for graphs.  The authors show that the generative model can capture graph statistics of real-life social networks and graphs, and that the model is able to capture the overlap and accuracy of the graph under a bounded overlap condition."
7576,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"backpropagation CONJUNCTION training. training CONJUNCTION backpropagation. ReLU′(0 ) USED-FOR neural network. ReLU′(0 ) USED-FOR backpropagation. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. SVHN CONJUNCTION ImageNet. ImageNet CONJUNCTION SVHN. networks CONJUNCTION datasets. datasets CONJUNCTION networks. ReLU′(0 ) USED-FOR precision levels. ImageNet HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. CIFAR10 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. ResNet HYPONYM-OF networks. VGG HYPONYM-OF networks. test accuracy EVALUATE-FOR ReLU′(0 ) = 1. batch - norm CONJUNCTION ADAM. ADAM CONJUNCTION batch - norm. batch - norm HYPONYM-OF reconditioning approaches. ADAM HYPONYM-OF reconditioning approaches. OtherScientificTerm are default precision, deep learning problems, backpropagation outputs, and double precision. Method are training methods, and vanilla SGD training. Task is algorithmic differentiation of nonsmooth problems. ","This paper studies the effect of ReLU′(0) on the precision of a neural network during backpropagation and training. The authors show that the default precision is not optimal for most deep learning problems. They show that for some networks (VGG, ResNet, etc.) and datasets (MNIST, CIFAR10, SVHN, and ImageNet), ReLU|0 is optimal for certain precision levels, but not for others. They also show that this is not the case for vanilla SGD training. Finally, they show that under certain training methods, ReLU~0 can be used to improve the test accuracy of the network.  ","This paper studies the effect of ReLU′(0) on the precision of a neural network during backpropagation and training. The authors show that the default precision is not optimal for most deep learning problems. They show that for some networks (VGG, ResNet, etc.) and datasets (MNIST, CIFAR10, SVHN, and ImageNet), ReLU|0 is optimal for certain precision levels, but not for others. They also show that this is not the case for vanilla SGD training. Finally, they show that under certain training methods, ReLU~0 can be used to improve the test accuracy of the network.  "
7592,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"generalization CONJUNCTION transfer. transfer CONJUNCTION generalization. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. transfer CONJUNCTION computational efficiency. computational efficiency CONJUNCTION transfer. minimizing information USED-FOR supervised learning setting. method USED-FOR policies. RPC ) USED-FOR policies. method USED-FOR RPC ). information bottlenecks CONJUNCTION model - based RL. model - based RL CONJUNCTION information bottlenecks. model - based RL CONJUNCTION bits - back coding. bits - back coding CONJUNCTION model - based RL. latent - space model CONJUNCTION policy. policy CONJUNCTION latent - space model. method USED-FOR policy. method USED-FOR latent - space model. method COMPARE prior methods. prior methods COMPARE method. reward EVALUATE-FOR information bottleneck. compression EVALUATE-FOR prior methods. method COMPARE information bottleneck. information bottleneck COMPARE method. compression EVALUATE-FOR method. reward EVALUATE-FOR method. compression USED-FOR policies. method USED-FOR policies. Method are reinforcement learning ( RL ) algorithms, and RL algorithms. Task is RL setting. OtherScientificTerm are past information, and decision making. ","This paper proposes a method to learn policies that minimize information in reinforcement learning (RL) algorithms. The authors consider the RL setting where the goal is to learn a policy that maximizes the reward while minimizing information in the supervised learning setting. This is an important problem for RL algorithms, as it is important for robustness, generalization, transfer, and computational efficiency.  The authors propose a method (called RPC) to learn such policies by minimizing information. The method is motivated by information bottlenecks in model-based RL, bits-back coding, etc. The proposed method learns a latent-space model, a policy, and then computes the reward by minimizing the information bottleneck, which is the difference between the true reward and the reward obtained from past information.  Experiments show that the proposed method outperforms prior methods in terms of reward and compression for learning such policies. It is also shown that the method can be applied to any RL algorithm, and that it can be used in combination with any RL algorithms. ","This paper proposes a method to learn policies that minimize information in reinforcement learning (RL) algorithms. The authors consider the RL setting where the goal is to learn a policy that maximizes the reward while minimizing information in the supervised learning setting. This is an important problem for RL algorithms, as it is important for robustness, generalization, transfer, and computational efficiency.  The authors propose a method (called RPC) to learn such policies by minimizing information. The method is motivated by information bottlenecks in model-based RL, bits-back coding, etc. The proposed method learns a latent-space model, a policy, and then computes the reward by minimizing the information bottleneck, which is the difference between the true reward and the reward obtained from past information.  Experiments show that the proposed method outperforms prior methods in terms of reward and compression for learning such policies. It is also shown that the method can be applied to any RL algorithm, and that it can be used in combination with any RL algorithms. "
7608,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"Transformer architecture USED-FOR sequence processing. graphs HYPONYM-OF data structures. node PART-OF graph. full Laplacian spectrum USED-FOR learned positional encoding ( LPE ). learned positional encoding ( LPE ) USED-FOR Spectral Attention Network ( SAN ). LPE PART-OF node features. LPE USED-FOR fully - connected Transformer. node features PART-OF graph. model USED-FOR similar sub - structures. model USED-FOR distinguishing graphs. heat transfer CONJUNCTION electric interaction. electric interaction CONJUNCTION heat transfer. Transformer USED-FOR modeling of physical phenomenons. over - squashing HYPONYM-OF information bottleneck. information bottleneck PART-OF GNNs. electric interaction HYPONYM-OF modeling of physical phenomenons. heat transfer HYPONYM-OF modeling of physical phenomenons. datasets EVALUATE-FOR model. model COMPARE state - of - theart GNNs. state - of - theart GNNs COMPARE model. model COMPARE attention - based model. attention - based model COMPARE model. graph benchmarks EVALUATE-FOR fully - connected architecture. graph benchmarks EVALUATE-FOR attention - based model. OtherScientificTerm are Laplacian, resonance, and physical phenomenons. ","This paper proposes a new Transformer architecture for sequence processing on data structures such as graphs. The proposed Spectral Attention Network (SAN) is based on the learned positional encoding (LPE) using the full Laplacian spectrum of each node in a graph. The node features of a graph are encoded using the LPE of all nodes in the graph. A fully-connected Transformer is trained with LPE on the node features. The model is able to distinguish between similar sub-structures of the same node and different sub-structures of different nodes. The authors also show that the model can be used for distinguishing graphs that have resonance, which is an information bottleneck in GNNs (e.g. over-squashing). The proposed model is tested on two datasets and compared with state-of-theart GNN models. The Transformer performs well on the modeling of physical phenomenons such as heat transfer and electric interaction, and the authors show that their model outperforms the attention-based model on the graph benchmarks. ","This paper proposes a new Transformer architecture for sequence processing on data structures such as graphs. The proposed Spectral Attention Network (SAN) is based on the learned positional encoding (LPE) using the full Laplacian spectrum of each node in a graph. The node features of a graph are encoded using the LPE of all nodes in the graph. A fully-connected Transformer is trained with LPE on the node features. The model is able to distinguish between similar sub-structures of the same node and different sub-structures of different nodes. The authors also show that the model can be used for distinguishing graphs that have resonance, which is an information bottleneck in GNNs (e.g. over-squashing). The proposed model is tested on two datasets and compared with state-of-theart GNN models. The Transformer performs well on the modeling of physical phenomenons such as heat transfer and electric interaction, and the authors show that their model outperforms the attention-based model on the graph benchmarks. "
7624,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"Task is two - alternative elections. OtherScientificTerm are state variable, private information, and private signals. Method is Bayes Nash equilibrium. ","This paper studies the problem of two-alternate elections, where the state variable is private and the candidate is allowed to choose a candidate with a certain level of privacy. The paper shows that the Bayes Nash equilibrium of a two-player game between two agents is a Nash equilibrium when the private information is available. The authors also show that if the private signals are not available, then there is no Nash equilibrium.  ","This paper studies the problem of two-alternate elections, where the state variable is private and the candidate is allowed to choose a candidate with a certain level of privacy. The paper shows that the Bayes Nash equilibrium of a two-player game between two agents is a Nash equilibrium when the private information is available. The authors also show that if the private signals are not available, then there is no Nash equilibrium.  "
7640,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"Hessian USED-FOR parameter interactions. Hessian FEATURE-OF neural network. secondorder derivatives of the loss USED-FOR Hessian. secondorder derivatives of the loss USED-FOR parameter interactions. model design CONJUNCTION optimization. optimization CONJUNCTION model design. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. model design HYPONYM-OF deep learning. optimization HYPONYM-OF deep learning. low - rank approximations CONJUNCTION heuristics. heuristics CONJUNCTION low - rank approximations. theoretical tools USED-FOR Hessian map. Hessian rank FEATURE-OF deep linear networks. rectified and hyperbolic tangent networks HYPONYM-OF models. model architecture USED-FOR rank deficiency. Generic are It, and bounds. OtherScientificTerm are network structure, and numerical Hessian rank. Method is overparameterized neural networks. ","This paper studies the Hessian of a neural network based on the secondorder derivatives of the loss of the parameters of the neural network. The Hessian is a measure of the parameter interactions between the model design and optimization in deep learning. It is a well-studied quantity that can be used to measure the importance of parameter interactions in model design, optimization, and generalization. The paper shows that low-rank approximations and heuristics are sufficient to obtain a Hessian map that matches the numerical Hessian rank of deep linear networks. It also shows that overparameterized neural networks suffer from a rank deficiency, which can be explained by the network structure and the model architecture. The authors also provide theoretical tools to explain the rank deficiency and provide bounds on the rank of such models (rectified and hyperbolic tangent networks). ","This paper studies the Hessian of a neural network based on the secondorder derivatives of the loss of the parameters of the neural network. The Hessian is a measure of the parameter interactions between the model design and optimization in deep learning. It is a well-studied quantity that can be used to measure the importance of parameter interactions in model design, optimization, and generalization. The paper shows that low-rank approximations and heuristics are sufficient to obtain a Hessian map that matches the numerical Hessian rank of deep linear networks. It also shows that overparameterized neural networks suffer from a rank deficiency, which can be explained by the network structure and the model architecture. The authors also provide theoretical tools to explain the rank deficiency and provide bounds on the rank of such models (rectified and hyperbolic tangent networks). "
7656,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,metric USED-FOR LSBD. metric EVALUATE-FOR LSBD methods. metric USED-FOR LSBD. semi - supervised 6 method USED-FOR LSBD representations. LSBD - VAE HYPONYM-OF semi - supervised 6 method. metric USED-FOR LSBD - VAE. LSBD - VAE USED-FOR LSBD representations. methods USED-FOR LSBD representations. LSBD - VAE CONJUNCTION methods. methods CONJUNCTION LSBD - VAE. common VAE - based disentanglement methods USED-FOR LSBD representations. disentanglement metrics EVALUATE-FOR LSBD representations. limited supervision USED-FOR methods. limited supervision USED-FOR LSBD representations. OtherScientificTerm is Linear Symmetry - Based Disentanglement ( LSBD ). Method is linearly disentangled representations. Task is disentanglement. Metric is DLSBD. ,"This paper proposes a new disentanglement metric for linearly disentangled representations, called Linear Symmetry-Based Disentanglements (LSBD). LSBD is based on the idea that the disentangling process can be decomposed into two steps: (1) learning a disentangleable representation, and (2) learning the corresponding disentangles. The authors propose a new metric for LSBD, called DLSBD, and show that the proposed metric improves the performance of existing LSBD methods. They also propose a semi-supervised 6 method, called LSBD-VAE, to learn LSBD representations. The proposed metric is used to evaluate the quality of the LSBD representation learned by LSBD - VAE and other methods with limited supervision. The paper shows that the learned representations of LSBD and other common VAE-based disentangler methods are comparable to the state-of-the-art LSBD metrics, and that LSBD VAE is able to learn better LSBD features than other methods. ","This paper proposes a new disentanglement metric for linearly disentangled representations, called Linear Symmetry-Based Disentanglements (LSBD). LSBD is based on the idea that the disentangling process can be decomposed into two steps: (1) learning a disentangleable representation, and (2) learning the corresponding disentangles. The authors propose a new metric for LSBD, called DLSBD, and show that the proposed metric improves the performance of existing LSBD methods. They also propose a semi-supervised 6 method, called LSBD-VAE, to learn LSBD representations. The proposed metric is used to evaluate the quality of the LSBD representation learned by LSBD - VAE and other methods with limited supervision. The paper shows that the learned representations of LSBD and other common VAE-based disentangler methods are comparable to the state-of-the-art LSBD metrics, and that LSBD VAE is able to learn better LSBD features than other methods. "
7672,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,Deep state - space models ( DSSMs ) USED-FOR temporal predictions. dynamics of observed sequence data USED-FOR Deep state - space models ( DSSMs ). dynamics of observed sequence data USED-FOR temporal predictions. evidence lower bound USED-FOR They. model USED-FOR dynamics. approach USED-FOR DSSMs. constrained optimisation framework USED-FOR DSSMs. constrained optimisation framework USED-FOR approach. amortised variational inference CONJUNCTION Bayesian filtering / smoothing. Bayesian filtering / smoothing CONJUNCTION amortised variational inference. extended Kalman VAE ( EKVAE ) COMPARE RNN - based DSSMs. RNN - based DSSMs COMPARE extended Kalman VAE ( EKVAE ). amortised variational inference PART-OF extended Kalman VAE ( EKVAE ). Bayesian filtering / smoothing PART-OF extended Kalman VAE ( EKVAE ). constrained optimisation framework USED-FOR DSSMs. system identification and prediction accuracy EVALUATE-FOR DSSMs. system identification and prediction accuracy EVALUATE-FOR constrained optimisation framework. EKVAE COMPARE models. models COMPARE EKVAE. EKVAE USED-FOR dynamical systems. EKVAE USED-FOR state - space representations. static and dynamic features PART-OF state - space representations. prediction accuracy EVALUATE-FOR EKVAE. prediction accuracy EVALUATE-FOR models. Task is maximising the evidence lower bound. ,"Deep state-space models (DSSMs) are used to make temporal predictions based on the dynamics of observed sequence data. They are trained by maximising the evidence lower bound, which is defined as the number of times that the model is able to predict the dynamics. This paper proposes an approach to train DSSMs using a constrained optimisation framework. The extended Kalman VAE (EKVAE) combines amortised variational inference and Bayesian filtering/smoothing, and is shown to outperform RNN-based DSSM in terms of system identification and prediction accuracy. The paper also shows that EKVAEs can be used to learn dynamical systems, and that the learned state-spaces of dynamical system can be combined with existing models to improve the prediction accuracy of a system. The authors also show that the learnt state -space representations can incorporate both static and dynamic features.","Deep state-space models (DSSMs) are used to make temporal predictions based on the dynamics of observed sequence data. They are trained by maximising the evidence lower bound, which is defined as the number of times that the model is able to predict the dynamics. This paper proposes an approach to train DSSMs using a constrained optimisation framework. The extended Kalman VAE (EKVAE) combines amortised variational inference and Bayesian filtering/smoothing, and is shown to outperform RNN-based DSSM in terms of system identification and prediction accuracy. The paper also shows that EKVAEs can be used to learn dynamical systems, and that the learned state-spaces of dynamical system can be combined with existing models to improve the prediction accuracy of a system. The authors also show that the learnt state -space representations can incorporate both static and dynamic features."
7688,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"Explanation techniques USED-FOR introspecting black - box models. deep inversion approach USED-FOR counterfactual explanations. deep models USED-FOR images. training distribution USED-FOR deep models. deep inversion methods USED-FOR counterfactuals. deep inversion methods USED-FOR conditional image synthesis. DISC USED-FOR Synthesizing Counterfactuals. deep inversion USED-FOR DISC. image priors USED-FOR DISC. counterfactuals USED-FOR classifier decision boundaries. DISC USED-FOR counterfactuals. counterfactuals USED-FOR visually meaningful explanations. Task is model prediction. OtherScientificTerm are discernible changes, data manifold, manifold consistency objective, and unknown test - time corruptions. Method are deep classifier, and progressive optimization strategy. ","This paper proposes a deep inversion approach to generate counterfactual explanations for introspecting black-box models. Explanation techniques have been widely used in the literature for explaining the performance of a model prediction. However, deep models trained on images from the training distribution may not be robust to discernible changes in the data manifold. This paper proposes to learn counterfactually explanations for a deep classifier that are robust to the manifold consistency objective. The authors propose a progressive optimization strategy to learn a set of ""counterfactuals"" that are invariant to manifold consistency and robust to unknown test-time corruptions.    The authors use DISC (Disentangled Discrete Inversion) for conditional image synthesis, which is an extension of previous work that uses the idea of using deep inversions methods for generating counterfactuality. Synthesizing Counterfactuallys using DISC is based on image priors, and the authors show that DISC can be used to generate a set (or set of) of counterfactUALs that match the classifier decision boundaries. The paper also shows that the counterfactionals generated by DISC are visually meaningful explanations, and that the proposed method is more robust to manifold changes.","This paper proposes a deep inversion approach to generate counterfactual explanations for introspecting black-box models. Explanation techniques have been widely used in the literature for explaining the performance of a model prediction. However, deep models trained on images from the training distribution may not be robust to discernible changes in the data manifold. This paper proposes to learn counterfactually explanations for a deep classifier that are robust to the manifold consistency objective. The authors propose a progressive optimization strategy to learn a set of ""counterfactuals"" that are invariant to manifold consistency and robust to unknown test-time corruptions.    The authors use DISC (Disentangled Discrete Inversion) for conditional image synthesis, which is an extension of previous work that uses the idea of using deep inversions methods for generating counterfactuality. Synthesizing Counterfactuallys using DISC is based on image priors, and the authors show that DISC can be used to generate a set (or set of) of counterfactUALs that match the classifier decision boundaries. The paper also shows that the counterfactionals generated by DISC are visually meaningful explanations, and that the proposed method is more robust to manifold changes."
7704,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"causal inference problem USED-FOR this. empirical objective USED-FOR algorithm. algorithm USED-FOR region of heterogeneity. algorithm COMPARE baselines. baselines COMPARE algorithm. real - world healthcare datasets EVALUATE-FOR algorithm. OtherScientificTerm are drug - related offenses, inter - decision - maker disagreement, generalization bound, and clinical knowledge. ","This paper considers the problem of drug-related offenses in which there is inter-decision-maker disagreement. The authors consider this as a causal inference problem, and propose an algorithm that uses an empirical objective to learn an algorithm to identify the region of heterogeneity. The algorithm is evaluated on two real-world healthcare datasets, and is shown to outperform baselines in terms of generalization bound. It is also shown that the algorithm does not rely on clinical knowledge, which is an interesting finding.","This paper considers the problem of drug-related offenses in which there is inter-decision-maker disagreement. The authors consider this as a causal inference problem, and propose an algorithm that uses an empirical objective to learn an algorithm to identify the region of heterogeneity. The algorithm is evaluated on two real-world healthcare datasets, and is shown to outperform baselines in terms of generalization bound. It is also shown that the algorithm does not rely on clinical knowledge, which is an interesting finding."
7720,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"perspective USED-FOR image synthesis. visual token generation problem USED-FOR task. paradigms COMPARE formulation. formulation COMPARE paradigms. flexible local manipulation USED-FOR image regions. formulation USED-FOR flexible local manipulation. latent tokens USED-FOR visual tokens. latent tokens USED-FOR it. constant content tokens CONJUNCTION style tokens. style tokens CONJUNCTION constant content tokens. style tokens PART-OF latent space. visual tokens USED-FOR TokenGAN. constant content tokens HYPONYM-OF visual tokens. style tokens HYPONYM-OF visual tokens. TokenGAN USED-FOR image synthesis. style tokens USED-FOR TokenGAN. Transformer USED-FOR attention mechanism. attention mechanism USED-FOR styles. attention mechanism USED-FOR TokenGAN. FFHQ CONJUNCTION LSUN CHURCH. LSUN CHURCH CONJUNCTION FFHQ. image synthesis benchmarks EVALUATE-FOR TokenGAN. LSUN CHURCH HYPONYM-OF image synthesis benchmarks. FFHQ HYPONYM-OF image synthesis benchmarks. generator USED-FOR high - fidelity images. 1024× 1024 size FEATURE-OF high - fidelity images. OtherScientificTerm are latent code, content tokens, and convolutions. Method is token - based generator. ","This paper proposes a new perspective for image synthesis. The authors consider the visual token generation problem as a task where the latent code can be decomposed into content tokens and style tokens. Unlike previous paradigms, the new formulation allows flexible local manipulation of image regions, and it uses latent tokens to generate visual tokens (constant content tokens, style tokens). TokenGAN uses visual tokens, i.e., constant content tokens or style tokens, as input to a token-based generator. The attention mechanism in TokenGAN is based on the attention mechanism of a Transformer.  The authors evaluate TokenGAN on two image synthesis benchmarks: FFHQ and LSUN CHURCH, and show that the generator can generate high-fidelity images with a 1024× 1024 size.  ","This paper proposes a new perspective for image synthesis. The authors consider the visual token generation problem as a task where the latent code can be decomposed into content tokens and style tokens. Unlike previous paradigms, the new formulation allows flexible local manipulation of image regions, and it uses latent tokens to generate visual tokens (constant content tokens, style tokens). TokenGAN uses visual tokens, i.e., constant content tokens or style tokens, as input to a token-based generator. The attention mechanism in TokenGAN is based on the attention mechanism of a Transformer.  The authors evaluate TokenGAN on two image synthesis benchmarks: FFHQ and LSUN CHURCH, and show that the generator can generate high-fidelity images with a 1024× 1024 size.  "
7736,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"min - norm interpolators CONJUNCTION max - margin classifiers. max - margin classifiers CONJUNCTION min - norm interpolators. overparameterization USED-FOR min - norm interpolators. overparameterization USED-FOR variance. variance FEATURE-OF min - norm interpolators. overparameterization USED-FOR max - margin classifiers. ridge regularization USED-FOR high dimensions. generalization EVALUATE-FOR avoiding interpolation. ridge regularization USED-FOR avoiding interpolation. linear regression CONJUNCTION classification. classification CONJUNCTION linear regression. linear regression FEATURE-OF robust risk. classification FEATURE-OF robust risk. OtherScientificTerm are noise, and interpolation. Task is robust overfitting. ",This paper studies the variance of min-norm interpolators and max-margin classifiers with overparameterization. The authors show that avoiding interpolation with ridge regularization is beneficial for avoiding robust overfitting. They also show that the robust risk of robust risk in linear regression and classification can be reduced by avoiding the noise in the interpolation. ,This paper studies the variance of min-norm interpolators and max-margin classifiers with overparameterization. The authors show that avoiding interpolation with ridge regularization is beneficial for avoiding robust overfitting. They also show that the robust risk of robust risk in linear regression and classification can be reduced by avoiding the noise in the interpolation. 
7752,SP:09f080f47db81b513af26add851822c5c32bb94e,"canonical primitive USED-FOR arbitrarily ordered point cloud. sphere HYPONYM-OF canonical primitive. primitive USED-FOR unordered point clouds. unordered point clouds PART-OF canonical surface. annotation CONJUNCTION selfsupervised part segmentation network. selfsupervised part segmentation network CONJUNCTION annotation. method USED-FOR unaligned input point clouds. selfsupervised part segmentation network USED-FOR method. annotation USED-FOR method. rotation range FEATURE-OF unaligned input point clouds. 3D semantic keypoint transfer CONJUNCTION part segmentation transfer. part segmentation transfer CONJUNCTION 3D semantic keypoint transfer. model COMPARE correspondence learning methods. correspondence learning methods COMPARE model. 3D semantic keypoint transfer EVALUATE-FOR model. part segmentation transfer EVALUATE-FOR model. Method are canonical point autoencoder ( CPAE ), and autoencoder. Material is 3D shapes. OtherScientificTerm is primitive surface. Generic is models. ",This paper proposes a canonical point autoencoder (CPAE) for 3D shapes. The canonical primitive is an arbitrarily ordered point cloud on a sphere. The primitive is used to generate unordered point clouds on the canonical surface. The proposed method uses annotation and a selfsupervised part segmentation network to learn unaligned input point clouds within a rotation range. The model is evaluated on 3D semantic keypoint transfer and 3D segmentation transfer and shows superior performance compared to correspondence learning methods. ,This paper proposes a canonical point autoencoder (CPAE) for 3D shapes. The canonical primitive is an arbitrarily ordered point cloud on a sphere. The primitive is used to generate unordered point clouds on the canonical surface. The proposed method uses annotation and a selfsupervised part segmentation network to learn unaligned input point clouds within a rotation range. The model is evaluated on 3D semantic keypoint transfer and 3D segmentation transfer and shows superior performance compared to correspondence learning methods. 
7768,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"Domain generalization ( DG ) methods USED-FOR generalizability. empirical risk minimization ( ERM ) approach COMPARE methods. methods COMPARE empirical risk minimization ( ERM ) approach. complex, non - convex loss function USED-FOR ERM. sharp minima USED-FOR sub - optimal generalizability. domain generalization gap EVALUATE-FOR flat minima. method USED-FOR flat minima. SWAD COMPARE vanilla SWA. vanilla SWA COMPARE SWAD. SWAD USED-FOR flatter minima. VLCS CONJUNCTION OfficeHome. OfficeHome CONJUNCTION VLCS. OfficeHome CONJUNCTION TerraIncognita. TerraIncognita CONJUNCTION OfficeHome. TerraIncognita CONJUNCTION DomainNet. DomainNet CONJUNCTION TerraIncognita. PACS CONJUNCTION VLCS. VLCS CONJUNCTION PACS. DG benchmarks EVALUATE-FOR SWAD. outof - domain accuracy EVALUATE-FOR SWAD. PACS HYPONYM-OF DG benchmarks. OfficeHome HYPONYM-OF DG benchmarks. DomainNet HYPONYM-OF DG benchmarks. VLCS HYPONYM-OF DG benchmarks. TerraIncognita HYPONYM-OF DG benchmarks. SWAD COMPARE generalization methods. generalization methods COMPARE SWAD. data augmentation and consistency regularization methods HYPONYM-OF generalization methods. SWAD CONJUNCTION DG method. DG method CONJUNCTION SWAD. SWAD USED-FOR DG. DG method USED-FOR DG. SWAD USED-FOR DG methods. Method are DomainBed, and Stochastic Weight Averaging Densely ( SWAD ). OtherScientificTerm is overfitting. Metric is in - domain generalizability. ","Domain generalization (DG) methods have been shown to be effective at improving generalizability across different domains. However, the empirical risk minimization (ERM) approach is more general than existing methods due to the use of a complex, non-convex loss function. This paper proposes Stochastic Weight Averaging Densely (SWAD), a variant of ERM that uses sharp minima to achieve sub-optimal generalization. The authors show that the domain generalization gap between flat minima (i.e., the gap between in-domain and out-of-domain generalization) can be reduced to zero by using SWAD. They also show that SWAD can achieve flatter minima than vanilla SWA, and that the method can also achieve flat minma in the presence of overfitting.  SWAD is evaluated on four DG benchmarks (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). SWAD achieves better out-domain accuracy than other DG methods, and also achieves better performance in the domain with respect to out of-distribution accuracy. SWAD also outperforms other generalization methods (e.g., data augmentation and consistency regularization methods).   The authors also show how SWAD and the DG method can be combined to improve the generalization performance of DG. ","Domain generalization (DG) methods have been shown to be effective at improving generalizability across different domains. However, the empirical risk minimization (ERM) approach is more general than existing methods due to the use of a complex, non-convex loss function. This paper proposes Stochastic Weight Averaging Densely (SWAD), a variant of ERM that uses sharp minima to achieve sub-optimal generalization. The authors show that the domain generalization gap between flat minima (i.e., the gap between in-domain and out-of-domain generalization) can be reduced to zero by using SWAD. They also show that SWAD can achieve flatter minima than vanilla SWA, and that the method can also achieve flat minma in the presence of overfitting.  SWAD is evaluated on four DG benchmarks (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet). SWAD achieves better out-domain accuracy than other DG methods, and also achieves better performance in the domain with respect to out of-distribution accuracy. SWAD also outperforms other generalization methods (e.g., data augmentation and consistency regularization methods).   The authors also show how SWAD and the DG method can be combined to improve the generalization performance of DG. "
7784,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"initialization time CONJUNCTION query time. query time CONJUNCTION initialization time. evaluation metric CONJUNCTION optimization. optimization CONJUNCTION evaluation metric. weight - sharing CONJUNCTION supervised learning. supervised learning CONJUNCTION weight - sharing. supervised learning CONJUNCTION zero - cost proxies. zero - cost proxies CONJUNCTION supervised learning. learning curve extrapolation CONJUNCTION weight - sharing. weight - sharing CONJUNCTION learning curve extrapolation. zero - cost proxies HYPONYM-OF techniques. learning curve extrapolation HYPONYM-OF techniques. supervised learning HYPONYM-OF techniques. weight - sharing HYPONYM-OF techniques. technique USED-FOR predictor - based NAS frameworks. Task are neural architecture search ( NAS ), and performance predictors. Method are neural networks, neural architectures, and performance prediction methods. Metric are correlationand rank - based performance measures, and predictive power. OtherScientificTerm is predictors. Generic is code. ","This paper studies the problem of neural architecture search (NAS) from the perspective of performance predictors. The authors consider the setting where neural networks are trained on a large number of datasets and the goal is to find the optimal architecture for each dataset. The paper proposes a new evaluation metric, optimization, and query time for neural architectures. The evaluation metric is based on correlationand rank-based performance measures, and the query time is a function of the initialization time, query time, and initialization time.  The paper also proposes a number of techniques to improve the performance prediction methods, including learning curve extrapolation, weight-sharing, supervised learning, and zero-cost proxies. The proposed technique is applied to several predictor-based NAS frameworks, and it is shown that the proposed technique improves the predictive power of the predictors and the performance of the code.   ","This paper studies the problem of neural architecture search (NAS) from the perspective of performance predictors. The authors consider the setting where neural networks are trained on a large number of datasets and the goal is to find the optimal architecture for each dataset. The paper proposes a new evaluation metric, optimization, and query time for neural architectures. The evaluation metric is based on correlationand rank-based performance measures, and the query time is a function of the initialization time, query time, and initialization time.  The paper also proposes a number of techniques to improve the performance prediction methods, including learning curve extrapolation, weight-sharing, supervised learning, and zero-cost proxies. The proposed technique is applied to several predictor-based NAS frameworks, and it is shown that the proposed technique improves the predictive power of the predictors and the performance of the code.   "
7800,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,Dirichlet posterior sampling CONJUNCTION privacy 4 guarantees. privacy 4 guarantees CONJUNCTION Dirichlet posterior sampling. exponential families FEATURE-OF differential privacy of posterior sampling. truncated concentrated differential privacy ( tCDP ) USED-FOR privacy guarantee. privacy guarantee FEATURE-OF Dirichlet posterior sampling. Dirichlet posterior sampling USED-FOR Multinomial8 Dirichlet sampling. Dirichlet posterior sampling USED-FOR private normalized histogram publishing. accuracy guarantees EVALUATE-FOR Dirichlet posterior sampling. Metric is inherent privacy. OtherScientificTerm is Dirichlet posterior 1 distribution. Method is posterior sampling. ,"This paper studies the differential privacy of posterior sampling in exponential families. The authors show that Dirichlet posterior sampling and privacy 4 guarantees can be obtained under certain assumptions. The privacy guarantee is based on truncated concentrated differential privacy (tCDP). The authors also show that the inherent privacy is guaranteed for the case where the posterior sampling is sampled from the Dirichlett posterior 1 distribution.    The paper also shows that the privacy guarantee of Dirichle posterior sampling for Multinomial8 Dirichlets is guaranteed under certain conditions. Finally, the paper shows that for private normalized histogram publishing, Dirichtle posterior sampling can achieve accuracy guarantees with high accuracy.","This paper studies the differential privacy of posterior sampling in exponential families. The authors show that Dirichlet posterior sampling and privacy 4 guarantees can be obtained under certain assumptions. The privacy guarantee is based on truncated concentrated differential privacy (tCDP). The authors also show that the inherent privacy is guaranteed for the case where the posterior sampling is sampled from the Dirichlett posterior 1 distribution.    The paper also shows that the privacy guarantee of Dirichle posterior sampling for Multinomial8 Dirichlets is guaranteed under certain conditions. Finally, the paper shows that for private normalized histogram publishing, Dirichtle posterior sampling can achieve accuracy guarantees with high accuracy."
7816,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,clustering CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION clustering. Random walks USED-FOR machine learning algorithms. parallel algorithm USED-FOR random walks. random walks USED-FOR it. random walk USED-FOR algorithm. technique USED-FOR parallel local clustering algorithm. algorithm COMPARE approaches. approaches COMPARE algorithm. Generic is method. OtherScientificTerm is graph. ,"Random walks are a popular technique in machine learning algorithms for clustering and semi-supervised learning. This paper proposes a new parallel algorithm for performing random walks on a graph. The proposed algorithm is based on random walks, and it uses the idea of random walks as a parallel algorithm. The technique is applied to a parallel local clustering algorithm, and the proposed algorithm outperforms existing approaches.  ","Random walks are a popular technique in machine learning algorithms for clustering and semi-supervised learning. This paper proposes a new parallel algorithm for performing random walks on a graph. The proposed algorithm is based on random walks, and it uses the idea of random walks as a parallel algorithm. The technique is applied to a parallel local clustering algorithm, and the proposed algorithm outperforms existing approaches.  "
7832,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,statistical mechanics USED-FOR replica method. typical sample complexity EVALUATE-FOR ` 1 - LinR. paramagnetic phase FEATURE-OF random regular graphs. ` 1 - LinR USED-FOR model selection. order of sample complexity EVALUATE-FOR model selection. order of sample complexity EVALUATE-FOR ` 1 - LinR. method USED-FOR nonasymptotic behavior. precision CONJUNCTION recall. recall CONJUNCTION precision. method USED-FOR ` 1 - LinR. nonasymptotic behavior FEATURE-OF ` 1 - LinR. precision HYPONYM-OF nonasymptotic behavior. recall HYPONYM-OF nonasymptotic behavior. ` 1 - LogR CONJUNCTION interaction screening. interaction screening CONJUNCTION ` 1 - LogR. method USED-FOR ` 1 - regularized M -estimators. interaction screening HYPONYM-OF ` 1 - regularized M -estimators. ` 1 - LogR HYPONYM-OF ` 1 - regularized M -estimators. Task is Ising model selection. OtherScientificTerm is model misspecification. Metric is M. Method is Ising model. ,"This paper studies the problem of Ising model selection, and proposes a replica method based on statistical mechanics. The authors show that the typical sample complexity of `1-LinR is $O(\sqrt{n\log n})$, which is the order of sample complexity for model selection. They also show that there is a paramagnetic phase in the random regular graphs, and that the order $n$ of the sample complexity depends on the model misspecification.   The authors then propose a method that can exploit the nonasymptotic behavior of `LinR' (i.e., the precision and recall of the model) to improve the performance of the original model. The proposed method is applied to two types of ` 1-regularized M-estimators, i.e. `1 - LogR and interaction screening, and is shown to outperform the original method in terms of precision and memory. The method is also shown to be more robust to the size of the M. ","This paper studies the problem of Ising model selection, and proposes a replica method based on statistical mechanics. The authors show that the typical sample complexity of `1-LinR is $O(\sqrt{n\log n})$, which is the order of sample complexity for model selection. They also show that there is a paramagnetic phase in the random regular graphs, and that the order $n$ of the sample complexity depends on the model misspecification.   The authors then propose a method that can exploit the nonasymptotic behavior of `LinR' (i.e., the precision and recall of the model) to improve the performance of the original model. The proposed method is applied to two types of ` 1-regularized M-estimators, i.e. `1 - LogR and interaction screening, and is shown to outperform the original method in terms of precision and memory. The method is also shown to be more robust to the size of the M. "
7848,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"k - means USED-FOR datasets. fuzzy or soft k - means objective HYPONYM-OF kmeans problem. framework USED-FOR clustering. similarity queries USED-FOR polynomial - time approximation algorithm. algorithms USED-FOR fuzzy clustering. k - means USED-FOR fuzzy k - means objective. non - negative matrix factorization HYPONYM-OF nonconvex problem. Lloyd - type algorithms CONJUNCTION alternating - minimization algorithms. alternating - minimization algorithms CONJUNCTION Lloyd - type algorithms. similarity queries USED-FOR problem. real - world applications EVALUATE-FOR algorithms. real - world datasets EVALUATE-FOR algorithms. Method is semisupervised active clustering framework. OtherScientificTerm are similarity, O(poly(k ) log n ) similarity queries, and local minima. Metric is polynomialtime - complexity. ","This paper proposes a semisupervised active clustering framework for the k-means problem, i.e., the fuzzy or soft k - means objective, which is a kmeans objective for datasets where the goal is to minimize the similarity between two samples. The authors propose a new framework for clustering based on the assumption that the similarity is non-negative and non-convex. They also propose a polynomial-time approximation algorithm based on similarity queries.    The main contribution of this paper is to extend existing algorithms for fuzzy clustering using k-mean to the case where the problem is solved using similarity queries, and to show that the problem can be solved efficiently with O(poly(k) log n) similarity queries and O(log n) local minima.  The authors also show that for a non-negligible number of queries, the problem of solving this problem with similarity queries is equivalent to solving the non-positive matrix factorization of the problem, and they show that this problem is a special case of the nonconveX problem.  Finally, the authors show that both Lloyd-type algorithms and alternating-minimization algorithms can be used to solve this problem efficiently, and the algorithms are tested on several real-world applications and show that their algorithms are competitive with existing algorithms.","This paper proposes a semisupervised active clustering framework for the k-means problem, i.e., the fuzzy or soft k - means objective, which is a kmeans objective for datasets where the goal is to minimize the similarity between two samples. The authors propose a new framework for clustering based on the assumption that the similarity is non-negative and non-convex. They also propose a polynomial-time approximation algorithm based on similarity queries.    The main contribution of this paper is to extend existing algorithms for fuzzy clustering using k-mean to the case where the problem is solved using similarity queries, and to show that the problem can be solved efficiently with O(poly(k) log n) similarity queries and O(log n) local minima.  The authors also show that for a non-negligible number of queries, the problem of solving this problem with similarity queries is equivalent to solving the non-positive matrix factorization of the problem, and they show that this problem is a special case of the nonconveX problem.  Finally, the authors show that both Lloyd-type algorithms and alternating-minimization algorithms can be used to solve this problem efficiently, and the algorithms are tested on several real-world applications and show that their algorithms are competitive with existing algorithms."
7864,SP:a8057c4708dceb4f934e449080043037a70fabf7,"models USED-FOR planning. value functions CONJUNCTION policies. policies CONJUNCTION value functions. computation USED-FOR policies. computation USED-FOR value functions. model CONJUNCTION value function. value function CONJUNCTION model. approach COMPARE planning methods. planning methods COMPARE approach. Dyna HYPONYM-OF planning methods. self - consistency USED-FOR policy evaluation. self - consistency USED-FOR control. policy evaluation CONJUNCTION control. control CONJUNCTION policy evaluation. tabular and function approximation settings EVALUATE-FOR these. Method are reinforcement learning ( RL ) agents, model - based RL, and self - consistency updates. OtherScientificTerm is environment interactions. ","This paper proposes a method for learning a model-based RL agent that can be used for planning and control tasks. The proposed method is based on self-consistency, which is an extension of the self-constraint method in the RL literature. The key idea is to learn a model of the environment and a value function, which can then be used to train a policy. The authors show that the proposed method outperforms existing baselines on a number of tasks.","This paper proposes a method for learning a model-based RL agent that can be used for planning and control tasks. The proposed method is based on self-consistency, which is an extension of the self-constraint method in the RL literature. The key idea is to learn a model of the environment and a value function, which can then be used to train a policy. The authors show that the proposed method outperforms existing baselines on a number of tasks."
7880,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"Episodic training USED-FOR models. few - shot learning USED-FOR models. Episodic training PART-OF few - shot learning. method USED-FOR episode sampling distributions. difficulty USED-FOR method. curriculum HYPONYM-OF sampling schemes. few - shot learning accuracies EVALUATE-FOR episodic training algorithms. algorithms CONJUNCTION network architectures. network architectures CONJUNCTION algorithms. network architectures CONJUNCTION protocols. protocols CONJUNCTION network architectures. few - shot learning datasets CONJUNCTION algorithms. algorithms CONJUNCTION few - shot learning datasets. network architectures EVALUATE-FOR method. algorithms EVALUATE-FOR method. few - shot learning datasets EVALUATE-FOR method. protocols EVALUATE-FOR method. Material is limited labelled data. Method are episodic training, and sampling method. OtherScientificTerm is episode difficulty. ","Episodic training in few-shot learning is a popular technique to train models with limited labelled data. However, episodic training can be problematic when the number of episodes is limited. This paper proposes a new sampling method, called ""curriculum"", to improve the performance of episodic learning. The proposed method is based on the observation that the episode sampling distributions are highly dependent on the difficulty of the episode. The authors propose two sampling schemes, one based on curriculum and the other based on episode difficulty. Experiments are conducted to show that the proposed method improves the performance on a number of few-set learning datasets, algorithms, network architectures, and protocols.   ","Episodic training in few-shot learning is a popular technique to train models with limited labelled data. However, episodic training can be problematic when the number of episodes is limited. This paper proposes a new sampling method, called ""curriculum"", to improve the performance of episodic learning. The proposed method is based on the observation that the episode sampling distributions are highly dependent on the difficulty of the episode. The authors propose two sampling schemes, one based on curriculum and the other based on episode difficulty. Experiments are conducted to show that the proposed method improves the performance on a number of few-set learning datasets, algorithms, network architectures, and protocols.   "
7896,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"logistic bandits USED-FOR binary rewards. logistic bandits HYPONYM-OF ones. ones PART-OF generalized linear bandits. logistic bandits HYPONYM-OF generalized linear bandits. algorithms USED-FOR logistic bandits. unknown parameter FEATURE-OF MNL probabilistic model. MNL - UCB HYPONYM-OF upper confidence bound ( UCB)-based algorithm. MNL - UCB USED-FOR problem. Generic is extension. Method is multinomial logit ( MNL ). OtherScientificTerm are revenue parameter, problemdependent constants, and loose regret bounds. ","This paper studies the problem of generalized linear bandits, which is a special case of the generalized linear bandit problem. In this problem, there are two types of linear bandits: logistic bandits and multinomial logit bandits. In the logistic bandit case, the goal is to maximize the expected return of the bandit with respect to the revenue. The authors propose a new algorithm, called MNL-UCB, to solve this problem. The main idea of the algorithm is to use the upper confidence bound (UCB) of the upper-confidence bound of the logit bandit, and then use the lower-confidence bounds of the UCB to solve the problem. ","This paper studies the problem of generalized linear bandits, which is a special case of the generalized linear bandit problem. In this problem, there are two types of linear bandits: logistic bandits and multinomial logit bandits. In the logistic bandit case, the goal is to maximize the expected return of the bandit with respect to the revenue. The authors propose a new algorithm, called MNL-UCB, to solve this problem. The main idea of the algorithm is to use the upper confidence bound (UCB) of the upper-confidence bound of the logit bandit, and then use the lower-confidence bounds of the UCB to solve the problem. "
7912,SP:0eaf058ed224464f6682cbbd80f716c89759f467,max - min entropy framework USED-FOR reinforcement learning ( RL ). maximum entropy RL USED-FOR model - free sample - based learning. soft actor - critic ( SAC ) algorithm USED-FOR maximum entropy RL. maximum entropy RL USED-FOR policies. entropy USED-FOR exploration. entropy FEATURE-OF low - entropy states. max - min entropy framework USED-FOR algorithm. algorithm USED-FOR Markov decision processes ( MDPs ). algorithm COMPARE RL algorithms. RL algorithms COMPARE algorithm. ,"This paper proposes a new algorithm based on the max-min entropy framework for reinforcement learning (RL) in the context of model-free sample-based learning. The authors propose a soft actor-critic (SAC) algorithm for maximum entropy RL for model-based RL, where policies are learned by maximizing the entropy of low-entropy states and minimizing the entropy for exploration. The algorithm is applied to Markov decision processes (MDPs) and is shown to outperform existing RL algorithms. ","This paper proposes a new algorithm based on the max-min entropy framework for reinforcement learning (RL) in the context of model-free sample-based learning. The authors propose a soft actor-critic (SAC) algorithm for maximum entropy RL for model-based RL, where policies are learned by maximizing the entropy of low-entropy states and minimizing the entropy for exploration. The algorithm is applied to Markov decision processes (MDPs) and is shown to outperform existing RL algorithms. "
7928,SP:19107a648d3d23403a8693b065ee842833a0b893,"cross - sectional data USED-FOR learning task. continuous - time Markov chains USED-FOR problem. approximate likelihood maximization method USED-FOR continuous - time Markov chains. synthetic and real cancer data EVALUATE-FOR approach. OtherScientificTerm are genetic mutations, time order, and underspecification. Task is biomedical applications. Generic is methods. ","This paper considers the problem of learning from cross-sectional data for a learning task with genetic mutations. The problem is formulated as learning from continuous-time Markov chains, and the authors propose an approximate likelihood maximization method to solve this problem. The proposed approach is evaluated on synthetic and real cancer data, and is shown to be effective in terms of time order, underspecification, and generalization to biomedical applications. The authors also provide a theoretical analysis of the proposed methods. ","This paper considers the problem of learning from cross-sectional data for a learning task with genetic mutations. The problem is formulated as learning from continuous-time Markov chains, and the authors propose an approximate likelihood maximization method to solve this problem. The proposed approach is evaluated on synthetic and real cancer data, and is shown to be effective in terms of time order, underspecification, and generalization to biomedical applications. The authors also provide a theoretical analysis of the proposed methods. "
7944,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,Document intelligence USED-FOR business applications. self - supervised learning methods USED-FOR annotation efforts. large - scale unlabeled document datasets USED-FOR self - supervised learning methods. self - supervised objectives FEATURE-OF models. models USED-FOR annotation efforts. unified pretraining framework USED-FOR document understanding. UDoc HYPONYM-OF unified pretraining framework. UDoc USED-FOR document understanding tasks. Transformer USED-FOR UDoc. multimodal embeddings USED-FOR Transformer. semantic region USED-FOR words and visual features. it USED-FOR generic representation. representation USED-FOR similarities. self - supervised losses USED-FOR it. self - supervised losses USED-FOR generic representation. pretraining procedure USED-FOR joint representations. pretraining procedure USED-FOR downstream tasks. Material is documents. Method is document pretraining methods. ,"Document intelligence is an important problem in many business applications, and many self-supervised learning methods for annotation efforts on large-scale unlabeled document datasets have been proposed in the recent years. However, most of these models are based on the assumption that all documents are labeled, which is not always the case. This paper proposes a unified pretraining framework called UDoc for document understanding, where models are trained to achieve different self -supervised objectives. UDoc is based on Transformer with multimodal embeddings, where words and visual features are mapped to a semantic region, and it learns a generic representation based on a set of standard self-surrogate losses. This representation is used to capture similarities between documents, and the pretraining procedure is applied to learn joint representations for downstream tasks. Experiments show that UDoc achieves state-of-the-art performance on a variety of document understanding tasks. ","Document intelligence is an important problem in many business applications, and many self-supervised learning methods for annotation efforts on large-scale unlabeled document datasets have been proposed in the recent years. However, most of these models are based on the assumption that all documents are labeled, which is not always the case. This paper proposes a unified pretraining framework called UDoc for document understanding, where models are trained to achieve different self -supervised objectives. UDoc is based on Transformer with multimodal embeddings, where words and visual features are mapped to a semantic region, and it learns a generic representation based on a set of standard self-surrogate losses. This representation is used to capture similarities between documents, and the pretraining procedure is applied to learn joint representations for downstream tasks. Experiments show that UDoc achieves state-of-the-art performance on a variety of document understanding tasks. "
7960,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"individual fairness FEATURE-OF data clustering problems. lp - norm objectives FEATURE-OF data clustering problems. k - MEDIAN HYPONYM-OF data clustering problems. k - MEDIAN HYPONYM-OF lp - norm objectives. objective guarantees FEATURE-OF l∞ or k - CENTER objective. clustering algorithm USED-FOR l∞ or k - CENTER objective. objective guarantees FEATURE-OF clustering algorithm. local - search algorithm USED-FOR lp - norms. kMEDIAN CONJUNCTION k - MEANS. k - MEANS CONJUNCTION kMEDIAN. algorithms USED-FOR problem. Linear Programming ( LP ) techniques USED-FOR algorithms. theoretical fairness guarantees COMPARE MV20. MV20 COMPARE theoretical fairness guarantees. sparsification technique USED-FOR algorithm. run - time EVALUATE-FOR algorithm. run - time EVALUATE-FOR sparsification technique. Generic are dataset, concept, and objective. OtherScientificTerm is individual fairness constraint. Metric are fairness, and worst - case guarantee. Method is LP rounding techniques. ","This paper studies individual fairness in data clustering problems with lp-norm objectives (e.g., k-MEDIAN, k-MEANS) where the dataset is partitioned into groups of equal size and each group is assigned a different individual fairness constraint.  The paper provides theoretical guarantees for the l∞ or k-CENTER objective of the standard clustering algorithm. The main contribution of the paper is to provide a local-search algorithm to compute the lp norms for each group in the dataset. The paper also provides two algorithms for this problem based on Linear Programming (LP) techniques. The theoretical fairness guarantees are better than MV20 and the worst-case guarantee is tighter than that of MV20. The authors also propose a sparsification technique to improve the run-time of the proposed algorithm.    The main idea is to use the concept of ""sparsification"" in this paper. In particular, the authors introduce a new concept called ""sparseness"" and show that sparsifying the dataset will improve the performance of the algorithm. They also show that the objective guarantees of the clustering method are tighter than those of the previous work.  Finally, the paper provides a theoretical analysis of the objective of kMEDIAN and k- MEANS and shows that under certain assumptions on the dataset and the objective, the algorithm will converge to the optimal solution. This paper also shows that LP rounding techniques can be used to speed up the computation. ","This paper studies individual fairness in data clustering problems with lp-norm objectives (e.g., k-MEDIAN, k-MEANS) where the dataset is partitioned into groups of equal size and each group is assigned a different individual fairness constraint.  The paper provides theoretical guarantees for the l∞ or k-CENTER objective of the standard clustering algorithm. The main contribution of the paper is to provide a local-search algorithm to compute the lp norms for each group in the dataset. The paper also provides two algorithms for this problem based on Linear Programming (LP) techniques. The theoretical fairness guarantees are better than MV20 and the worst-case guarantee is tighter than that of MV20. The authors also propose a sparsification technique to improve the run-time of the proposed algorithm.    The main idea is to use the concept of ""sparsification"" in this paper. In particular, the authors introduce a new concept called ""sparseness"" and show that sparsifying the dataset will improve the performance of the algorithm. They also show that the objective guarantees of the clustering method are tighter than those of the previous work.  Finally, the paper provides a theoretical analysis of the objective of kMEDIAN and k- MEANS and shows that under certain assumptions on the dataset and the objective, the algorithm will converge to the optimal solution. This paper also shows that LP rounding techniques can be used to speed up the computation. "
7976,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"MAX - K - CUT CONJUNCTION correlation clustering. correlation clustering CONJUNCTION MAX - K - CUT. MAX - K - CUT HYPONYM-OF graph partitioning problems. correlation clustering HYPONYM-OF graph partitioning problems. MAX - K - CUT CONJUNCTION MAX - AGREE variant of correlation clustering. MAX - AGREE variant of correlation clustering CONJUNCTION MAX - K - CUT. methods USED-FOR MAX - K - CUT. methods USED-FOR approximation guarantees. methods USED-FOR SDPs. approximation guarantees CONJUNCTION MAX - K - CUT. MAX - K - CUT CONJUNCTION approximation guarantees. O(n ) constraints USED-FOR SDPs. polynomial - time Gaussian sampling - based algorithms USED-FOR problems. O(n + |E| ) memory USED-FOR polynomial - time Gaussian sampling - based algorithms. approach CONJUNCTION sparsification. sparsification CONJUNCTION approach. OtherScientificTerm are memory bottleneck, and dense graphs. Metric are storage complexity, and approximation ratio. ",This paper studies graph partitioning problems such as MAX-K-CUT and the MAX-AGREE variant of correlation clustering. The authors propose two methods to improve the approximation guarantees of MAX-k-cut and MAX-cUT and propose methods for both SDPs with O(n) constraints. They also propose polynomial-time Gaussian sampling-based algorithms for both problems that can run in O(N + |E|) memory. The main contribution of this paper is that the memory bottleneck is alleviated by the proposed approach and sparsification. The storage complexity is reduced to O(1/n) and the approximation ratio is improved to O(\sqrt{n}) for dense graphs. ,This paper studies graph partitioning problems such as MAX-K-CUT and the MAX-AGREE variant of correlation clustering. The authors propose two methods to improve the approximation guarantees of MAX-k-cut and MAX-cUT and propose methods for both SDPs with O(n) constraints. They also propose polynomial-time Gaussian sampling-based algorithms for both problems that can run in O(N + |E|) memory. The main contribution of this paper is that the memory bottleneck is alleviated by the proposed approach and sparsification. The storage complexity is reduced to O(1/n) and the approximation ratio is improved to O(\sqrt{n}) for dense graphs. 
7992,SP:cfd6cf88a823729c281059e179788248238a6ed7,predicting inter - frame motion information USED-FOR video prediction tasks. Motion - Aware Unit ( MAU ) USED-FOR inter - frame motion information. temporal receptive field FEATURE-OF predictive units. attention module CONJUNCTION fusion module. fusion module CONJUNCTION attention module. modules PART-OF MAU. fusion module HYPONYM-OF modules. attention module HYPONYM-OF modules. attention module PART-OF MAU. fusion module PART-OF MAU. attention module USED-FOR attention map. historical temporal states PART-OF augmented motion information ( AMI ). attention map USED-FOR historical temporal states. predictive unit USED-FOR temporal dynamics. receptive field USED-FOR predictive unit. receptive field USED-FOR temporal dynamics. fusion module USED-FOR augmented motion information ( AMI ). unit USED-FOR predictive models. encoders CONJUNCTION decoders. decoders CONJUNCTION encoders. information recalling scheme USED-FOR encoders. information recalling scheme USED-FOR decoders. video prediction CONJUNCTION early action recognition tasks. early action recognition tasks CONJUNCTION video prediction. early action recognition tasks EVALUATE-FOR MAU. video prediction EVALUATE-FOR MAU. MAU COMPARE methods. methods COMPARE MAU. tasks EVALUATE-FOR methods. tasks EVALUATE-FOR MAU. OtherScientificTerm is historical spatial states. ,"This paper proposes a new method for predicting inter-frame motion information for video prediction tasks. The proposed method, called Motion-Aware Unit (MAU), is based on the idea that the temporal receptive field of the predictive units in a video is highly correlated with the historical spatial states. The MAU consists of two modules: an attention module that maps the attention map of the input frame to the attention of a predictive unit in the receptive field, and a fusion module that combines augmented motion information (AMI) from historical temporal states in the input frames with augmented spatial states from the previous frames. This unit is then used to train two different predictive models. The encoders and decoders are trained with an information recalling scheme, and the final predictive unit is trained to capture temporal dynamics in a receptive field. Experiments on video prediction and early action recognition tasks show that MAU outperforms existing methods on both tasks.","This paper proposes a new method for predicting inter-frame motion information for video prediction tasks. The proposed method, called Motion-Aware Unit (MAU), is based on the idea that the temporal receptive field of the predictive units in a video is highly correlated with the historical spatial states. The MAU consists of two modules: an attention module that maps the attention map of the input frame to the attention of a predictive unit in the receptive field, and a fusion module that combines augmented motion information (AMI) from historical temporal states in the input frames with augmented spatial states from the previous frames. This unit is then used to train two different predictive models. The encoders and decoders are trained with an information recalling scheme, and the final predictive unit is trained to capture temporal dynamics in a receptive field. Experiments on video prediction and early action recognition tasks show that MAU outperforms existing methods on both tasks."
8008,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"neural net approximation of the Q function USED-FOR Deep Reinforcement Learning ( RL ). neural net approximations USED-FOR nonlinear RL. ReLU and polynomial activation functions PART-OF two - layer neural networks. two - layer neural networks USED-FOR function approximation. algorithm USED-FOR generative model setting. algebraic dimension FEATURE-OF sample complexity. Task is RL. OtherScientificTerm are neural net function class, and deterministic dynamics. Method is linear ( or eluder dimension ) methods. ","This paper studies the problem of neural net approximation of the Q function in Deep Reinforcement Learning (RL) using neural net approximations for nonlinear RL. The function approximation is based on two-layer neural networks with ReLU and polynomial activation functions. The authors provide an algorithm for the generative model setting where the neural net function class is not deterministic, but the deterministic dynamics is. The sample complexity is of the algebraic dimension, which is much lower than existing linear (or eluder dimension) methods. ","This paper studies the problem of neural net approximation of the Q function in Deep Reinforcement Learning (RL) using neural net approximations for nonlinear RL. The function approximation is based on two-layer neural networks with ReLU and polynomial activation functions. The authors provide an algorithm for the generative model setting where the neural net function class is not deterministic, but the deterministic dynamics is. The sample complexity is of the algebraic dimension, which is much lower than existing linear (or eluder dimension) methods. "
8024,SP:cac881243abde92a28c110f5bd84d115ed189bda,"representations USED-FOR zero - shot transfer. Deep Metric Learning ( DML ) USED-FOR representations. priori unknown test distributions USED-FOR zero - shot transfer. ooDML benchmark USED-FOR generalization. ooDML USED-FOR generalization. ooDML USED-FOR train - to - test distribution shifts. benchmark EVALUATE-FOR DML methods. few - shot DML USED-FOR generalization. unknown test shifts FEATURE-OF generalization. OtherScientificTerm are distribution shifts, train - test splits, out - of - distribution shifts, and distribution shift. Method are DML, and ooDML1. Generic is methods. ","This paper studies the problem of zero-shot transfer from a priori unknown test distributions using Deep Metric Learning (DML) to learn representations that are transferable across different distribution shifts. The authors propose the ooDML benchmark to evaluate the generalization performance of DML on train-to-test splits. They show that under the assumption that out-of-distribution shifts are not present in the training data, ooOdML1 outperforms existing methods. They also show that ooWML1 can be used to evaluate generalization on a set of test splits that have a distribution shift between the training and test sets.    The authors also provide a new benchmark to compare the performance of different DML methods on a few-shot DML, and show that the performance on the new benchmark is comparable to that of existing methods, and that on the old benchmark, the performance is much worse. ","This paper studies the problem of zero-shot transfer from a priori unknown test distributions using Deep Metric Learning (DML) to learn representations that are transferable across different distribution shifts. The authors propose the ooDML benchmark to evaluate the generalization performance of DML on train-to-test splits. They show that under the assumption that out-of-distribution shifts are not present in the training data, ooOdML1 outperforms existing methods. They also show that ooWML1 can be used to evaluate generalization on a set of test splits that have a distribution shift between the training and test sets.    The authors also provide a new benchmark to compare the performance of different DML methods on a few-shot DML, and show that the performance on the new benchmark is comparable to that of existing methods, and that on the old benchmark, the performance is much worse. "
8040,SP:bacff3685476855a32549d03095375649fd89df2,"outlier detection algorithm CONJUNCTION hyperparameter(s ). hyperparameter(s ) CONJUNCTION outlier detection algorithm. model HYPONYM-OF hyperparameter(s ). data - driven approach USED-FOR UOMS. METAOD HYPONYM-OF data - driven approach. meta - learning USED-FOR data - driven approach. meta - learning USED-FOR METAOD. model selection USED-FOR clustering. UOMS problem COMPARE model selection. model selection COMPARE UOMS problem. model evaluations CONJUNCTION model comparisons. model comparisons CONJUNCTION model evaluations. model USED-FOR dataset. METAOD USED-FOR model. detection models USED-FOR METAOD. historical outlier detection benchmark datasets EVALUATE-FOR detection models. meta - learning framework USED-FOR task similarity. metafeatures USED-FOR task similarity. meta - learning techniques USED-FOR UOMS. model COMPARE model selection. model selection COMPARE model. METAOD COMPARE model selection. model selection COMPARE METAOD. model selection COMPARE meta - learning techniques. meta - learning techniques COMPARE model selection. METAOD USED-FOR model. meta-)training EVALUATE-FOR METAOD. METAOD CONJUNCTION meta - learning database. meta - learning database CONJUNCTION METAOD. METAOD USED-FOR UOMS problem. meta - learning database USED-FOR UOMS problem. Task are unsupervised outlier detection task, unsupervised outlier model selection ( UOMS ) problem, model evaluation, and model comparison. OtherScientificTerm is universal objective function. Generic is task. ","This paper tackles the unsupervised outlier detection task. The authors propose a data-driven approach called METAOD, which is a meta-learning-based approach to tackle the outlier model selection (UOMS) problem. In this paper, the authors propose to use a universal objective function to select the best model selection for each outlier in the dataset, which can be a combination of any outlier detector algorithm and any hyperparameter(s) in the model (e.g., the number of samples, the number and the hyperparameters of the model). The authors show that the UOMS problem is similar to the standard model selection problem for clustering, but the model selection is based on a meta learning framework, where the task similarity is measured using the metafeatures of all the models in the task.   The authors evaluate the performance of the proposed method on two historical outlier discovery benchmark datasets, and show that MetaOD outperforms the state-of-the-art detection models on both historical and modern detection models. They also show that a model trained on the same dataset trained on a different dataset is more likely to outperform the model on the new dataset.  The paper also shows that the model evaluation and model comparison can be improved by using model evaluations as well as model comparisons.  Finally, the paper shows that a trained model on a new (meta-)training dataset (METAOD) outperforms a model that is trained on all the datasets on the original dataset, and that is only trained on one of the datasets.  In addition to the paper, they also show how the proposed METAod can be used to compare the model performance of a model on different datasets. They show that their model is more robust to model selection than a single model selection, and can outperform meta-learned models on a single dataset. Finally, they show that meta learning techniques can be applied to the proposed model selection to improve the performance on the proposed (meta-learning) meta-training dataset. The paper concludes that the proposed approach is able to improve upon the state of the art on both the original (unsupervised) and meta-)training datasets.","This paper tackles the unsupervised outlier detection task. The authors propose a data-driven approach called METAOD, which is a meta-learning-based approach to tackle the outlier model selection (UOMS) problem. In this paper, the authors propose to use a universal objective function to select the best model selection for each outlier in the dataset, which can be a combination of any outlier detector algorithm and any hyperparameter(s) in the model (e.g., the number of samples, the number and the hyperparameters of the model). The authors show that the UOMS problem is similar to the standard model selection problem for clustering, but the model selection is based on a meta learning framework, where the task similarity is measured using the metafeatures of all the models in the task.   The authors evaluate the performance of the proposed method on two historical outlier discovery benchmark datasets, and show that MetaOD outperforms the state-of-the-art detection models on both historical and modern detection models. They also show that a model trained on the same dataset trained on a different dataset is more likely to outperform the model on the new dataset.  The paper also shows that the model evaluation and model comparison can be improved by using model evaluations as well as model comparisons.  Finally, the paper shows that a trained model on a new (meta-)training dataset (METAOD) outperforms a model that is trained on all the datasets on the original dataset, and that is only trained on one of the datasets.  In addition to the paper, they also show how the proposed METAod can be used to compare the model performance of a model on different datasets. They show that their model is more robust to model selection than a single model selection, and can outperform meta-learned models on a single dataset. Finally, they show that meta learning techniques can be applied to the proposed model selection to improve the performance on the proposed (meta-learning) meta-training dataset. The paper concludes that the proposed approach is able to improve upon the state of the art on both the original (unsupervised) and meta-)training datasets."
8056,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"Prediction+optimization HYPONYM-OF real - world paradigm. SPO+ CONJUNCTION direct optimization. direct optimization CONJUNCTION SPO+. direct optimization HYPONYM-OF decision - focused prediction approaches. SPO+ HYPONYM-OF decision - focused prediction approaches. max operator USED-FOR real - world objectives. max operator USED-FOR soft constraints. framework USED-FOR closed - form solution. predictive parameters CONJUNCTION gradients. gradients CONJUNCTION predictive parameters. predictive parameters USED-FOR closed - form solution. gradients USED-FOR closed - form solution. synthetic linear programming CONJUNCTION portfolio optimization. portfolio optimization CONJUNCTION synthetic linear programming. portfolio optimization CONJUNCTION resource provisioning. resource provisioning CONJUNCTION portfolio optimization. method COMPARE decision - focused approaches. decision - focused approaches COMPARE method. two - staged methods CONJUNCTION decision - focused approaches. decision - focused approaches CONJUNCTION two - staged methods. method COMPARE two - staged methods. two - staged methods COMPARE method. applications EVALUATE-FOR method. method COMPARE method. method COMPARE method. soft constraints USED-FOR method. soft constraints FEATURE-OF applications. applications EVALUATE-FOR method. resource provisioning HYPONYM-OF applications. synthetic linear programming HYPONYM-OF applications. portfolio optimization HYPONYM-OF applications. Task are optimization problem, and downstream optimization problem. Method are prediction model, and analytically differentiable surrogate objective framework. Generic is they. OtherScientificTerm are soft linear and non - negative hard constraints, and theoretical bounds. ","This paper proposes a new real-world paradigm called Prediction+optimization, where the optimization problem is formulated as a decision-focused prediction problem, and the goal is to find a solution that maximizes the expected return of the prediction model. The authors propose an analytically differentiable surrogate objective framework, where soft linear and non-negative hard constraints are introduced, and they show that the max operator can be used to impose soft constraints on the soft constraints, which can be interpreted as a max operator that is applicable to a variety of real- world objectives. This framework is then used to derive a closed-form solution based on the predictive parameters and gradients of the downstream optimization problem. Experiments are conducted on three applications: synthetic linear programming, portfolio optimization, and resource provisioning. The proposed method outperforms two-staged methods, decision-focussed approaches, and two-stage methods without soft constraints.  ","This paper proposes a new real-world paradigm called Prediction+optimization, where the optimization problem is formulated as a decision-focused prediction problem, and the goal is to find a solution that maximizes the expected return of the prediction model. The authors propose an analytically differentiable surrogate objective framework, where soft linear and non-negative hard constraints are introduced, and they show that the max operator can be used to impose soft constraints on the soft constraints, which can be interpreted as a max operator that is applicable to a variety of real- world objectives. This framework is then used to derive a closed-form solution based on the predictive parameters and gradients of the downstream optimization problem. Experiments are conducted on three applications: synthetic linear programming, portfolio optimization, and resource provisioning. The proposed method outperforms two-staged methods, decision-focussed approaches, and two-stage methods without soft constraints.  "
8072,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"graph USED-FOR GNN. GNN USED-FOR DropGNNs. DropGNNs USED-FOR graph neighborhoods. GNN benchmarks EVALUATE-FOR DropGNNs. Method are Dropout Graph Neural Networks ( DropGNNs ), GNN frameworks, and message passing GNNs. Generic is approach. OtherScientificTerm is theoretical bounds. Metric is expressiveness. ","This paper proposes Dropout Graph Neural Networks (DropGNNs), a generalization of existing GNN frameworks. The key idea of DropGNN is to use a GNN on a graph, and then drop out the message passing GNNs. This approach is well-motivated, and the theoretical bounds on the expressiveness are well-supported by experiments. The paper also provides a theoretical analysis of the properties of the dropout graph neighborhoods, and shows that the drop out graph neighborhoods of the GNN are not the result of a single GNN, but rather of a dropout of the entire GNN. The authors also provide empirical results on standard GNN benchmarks, and show that DropGraphNets are able to learn graph neighborhoods with high expressiveness.   ","This paper proposes Dropout Graph Neural Networks (DropGNNs), a generalization of existing GNN frameworks. The key idea of DropGNN is to use a GNN on a graph, and then drop out the message passing GNNs. This approach is well-motivated, and the theoretical bounds on the expressiveness are well-supported by experiments. The paper also provides a theoretical analysis of the properties of the dropout graph neighborhoods, and shows that the drop out graph neighborhoods of the GNN are not the result of a single GNN, but rather of a dropout of the entire GNN. The authors also provide empirical results on standard GNN benchmarks, and show that DropGraphNets are able to learn graph neighborhoods with high expressiveness.   "
8088,SP:090dc0471d54e237f423034b1e1c46a510202807,"Transformers USED-FOR visual tasks. global representation capacities FEATURE-OF Transformers. local and global pattern features USED-FOR image classification. representation capacity FEATURE-OF local and global pattern features. DS - Net USED-FOR fine - grained and integrated features. DS - Net USED-FOR them. Inter - Scale Alignment module USED-FOR information interaction. Intra - scale Propagation module CONJUNCTION Inter - Scale Alignment module. Inter - Scale Alignment module CONJUNCTION Intra - scale Propagation module. Intra - scale Propagation module USED-FOR resolutions. Intra - scale Propagation module USED-FOR information interaction. contextual information USED-FOR downstream dense predictions. Vision Transformers CONJUNCTION ResNets. ResNets CONJUNCTION Vision Transformers. DS - Net COMPARE DeiT - Small. DeiT - Small COMPARE DS - Net. DS - Net COMPARE Vision Transformers. Vision Transformers COMPARE DS - Net. DS - Net COMPARE ResNets. ResNets COMPARE DS - Net. ImageNet-1k EVALUATE-FOR DeiT - Small. top-1 accuracy EVALUATE-FOR DeiT - Small. ImageNet-1k EVALUATE-FOR DS - Net. top-1 accuracy EVALUATE-FOR DS - Net. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. DS - Net - Small COMPARE ResNet-50. ResNet-50 COMPARE DS - Net - Small. DS - Net - Small USED-FOR object detection. mAP EVALUATE-FOR DS - Net - Small. DS - Net - Small USED-FOR instance segmentation. OtherScientificTerm are high - level local pattern information, features, and dual scales. Material is MSCOCO 2017. Generic is state - of - the - art scheme. Task is vision tasks. ","This paper proposes a new architecture for learning fine-grained and integrated features for vision tasks. The proposed architecture, called DS-Net, aims to improve the global representation capacities of Transformers for visual tasks. In particular, the authors propose to use both local and global pattern features for image classification, which is an important task that requires high-level local pattern information. To achieve this, they propose a new state-of-the-art scheme, which extends the previous state of the art, DeiT-Small, by introducing a new representation capacity for local andglobal pattern features.    The main idea is to learn features at different resolutions and use DS- Net to combine them. The Intra-scale Propagation module is used to generate different resolutions, and an Inter-Scale Alignment module is introduced to encourage information interaction between the features of the two scales. The authors also introduce contextual information for downstream dense predictions. Experiments on ImageNet-1k show that DS-net achieves a top-1 accuracy of 99.9% on top of MSCOCO 2017, and is competitive with Vision Transformers and ResNets. On mAP, DS- net-small achieves a mAP of 98.5% on object detection and 99.7% on image segmentation.  The authors claim that the proposed DS - Net-Small outperforms ResNet-50 in most of the vision tasks, and outperforms Vision Transformers in some cases. ","This paper proposes a new architecture for learning fine-grained and integrated features for vision tasks. The proposed architecture, called DS-Net, aims to improve the global representation capacities of Transformers for visual tasks. In particular, the authors propose to use both local and global pattern features for image classification, which is an important task that requires high-level local pattern information. To achieve this, they propose a new state-of-the-art scheme, which extends the previous state of the art, DeiT-Small, by introducing a new representation capacity for local andglobal pattern features.    The main idea is to learn features at different resolutions and use DS- Net to combine them. The Intra-scale Propagation module is used to generate different resolutions, and an Inter-Scale Alignment module is introduced to encourage information interaction between the features of the two scales. The authors also introduce contextual information for downstream dense predictions. Experiments on ImageNet-1k show that DS-net achieves a top-1 accuracy of 99.9% on top of MSCOCO 2017, and is competitive with Vision Transformers and ResNets. On mAP, DS- net-small achieves a mAP of 98.5% on object detection and 99.7% on image segmentation.  The authors claim that the proposed DS - Net-Small outperforms ResNet-50 in most of the vision tasks, and outperforms Vision Transformers in some cases. "
8104,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,unified framework USED-FOR visual concepts. unified framework USED-FOR physics models of objects. visual concepts CONJUNCTION physics models of objects. physics models of objects CONJUNCTION visual concepts. videos USED-FOR physics models of objects. visual perception module CONJUNCTION concept learner. concept learner CONJUNCTION visual perception module. concept learner CONJUNCTION differentiable physics engine. differentiable physics engine CONJUNCTION concept learner. differentiable physics engine PART-OF components. visual perception module PART-OF components. concept learner HYPONYM-OF components. latent scene representations USED-FOR them. color CONJUNCTION shape. shape CONJUNCTION color. shape CONJUNCTION material. material CONJUNCTION shape. prior knowledge USED-FOR physics engine. concept learner USED-FOR visual concepts. language USED-FOR object - centric representations. object - centric representations USED-FOR visual concepts. material HYPONYM-OF visual concepts. color HYPONYM-OF visual concepts. shape HYPONYM-OF visual concepts. mass CONJUNCTION restitution. restitution CONJUNCTION mass. restitution CONJUNCTION velocity. velocity CONJUNCTION restitution. differentiable physical simulation USED-FOR physical properties. differentiable physical simulation USED-FOR differentiable physics model. video observations USED-FOR simulated trajectories. impulse - based differentiable rigid - body simulator USED-FOR differentiable physics model. grounded concepts USED-FOR differentiable physical simulation. velocity HYPONYM-OF physical properties. mass HYPONYM-OF physical properties. restitution HYPONYM-OF physical properties. concepts CONJUNCTION physical models. physical models CONJUNCTION concepts. differentiable physics PART-OF dynamic reasoning framework. VRDP COMPARE counterpart. counterpart COMPARE VRDP. accuracy EVALUATE-FOR predictive and counterfactual questions. physics models USED-FOR dynamics prediction. predictive and counterfactual questions EVALUATE-FOR VRDP. synthetic and real - world benchmarks EVALUATE-FOR dynamics prediction. accuracy EVALUATE-FOR VRDP. VRDP USED-FOR concepts. physical parameters USED-FOR VRDP. OtherScientificTerm is object - centric trajectories. Metric is interpretability. ,"This paper proposes a unified framework for learning visual concepts and physics models of objects from videos. The proposed framework consists of three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual concepts are encoded in object-centric representations using a language, and the physics engine is trained with prior knowledge of the object's physical properties.  The visual concept learners are trained to capture visual concepts such as color, shape, shape and material, and use them as latent scene representations. The differentiable physical simulation is used to learn physical properties such as mass, restitution, velocity, etc.    The key contribution of the paper is that the differentiable particle physics model is trained using an impulse-based differentiable rigid-body simulator, where simulated trajectories are generated from video observations. The paper also proposes a dynamic reasoning framework that combines differentiable Physics with visual concepts, which is called VRDP. VRDP is able to learn concepts and physical models in an end-to-end manner, and uses grounded concepts as inputs to a dynamic physics model, which can be used as inputs for differentiating physical properties (mass, mass, and restitution).  The paper shows that VRDP outperforms its counterpart in terms of accuracy on predictive and counterfactual questions and dynamics prediction on both synthetic and real-world benchmarks. The authors also show that the VRDP can learn concepts that are grounded in physical parameters, and that the learned physics models are able to be used for dynamics prediction.  In addition, the paper also shows that the proposed VRDPP can be combined with existing methods to improve interpretability.","This paper proposes a unified framework for learning visual concepts and physics models of objects from videos. The proposed framework consists of three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual concepts are encoded in object-centric representations using a language, and the physics engine is trained with prior knowledge of the object's physical properties.  The visual concept learners are trained to capture visual concepts such as color, shape, shape and material, and use them as latent scene representations. The differentiable physical simulation is used to learn physical properties such as mass, restitution, velocity, etc.    The key contribution of the paper is that the differentiable particle physics model is trained using an impulse-based differentiable rigid-body simulator, where simulated trajectories are generated from video observations. The paper also proposes a dynamic reasoning framework that combines differentiable Physics with visual concepts, which is called VRDP. VRDP is able to learn concepts and physical models in an end-to-end manner, and uses grounded concepts as inputs to a dynamic physics model, which can be used as inputs for differentiating physical properties (mass, mass, and restitution).  The paper shows that VRDP outperforms its counterpart in terms of accuracy on predictive and counterfactual questions and dynamics prediction on both synthetic and real-world benchmarks. The authors also show that the VRDP can learn concepts that are grounded in physical parameters, and that the learned physics models are able to be used for dynamics prediction.  In addition, the paper also shows that the proposed VRDPP can be combined with existing methods to improve interpretability."
8120,SP:c511066c38f9793bacb4986c564eafa36e032f39,"Active learning USED-FOR minimizing labeling costs. out - of - distribution data CONJUNCTION redundancy. redundancy CONJUNCTION out - of - distribution data. submodular information measures ( SIM ) USED-FOR acquisition functions. acquisition functions USED-FOR unified active learning framework. submodular information measures ( SIM ) USED-FOR unified active learning framework. one - stop solution USED-FOR active learning. SIMILAR USED-FOR active learning. SIMILAR COMPARE active learning algorithms. active learning algorithms COMPARE SIMILAR. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. ImageNet HYPONYM-OF image classification tasks. CIFAR-10 HYPONYM-OF image classification tasks. MNIST HYPONYM-OF image classification tasks. DISTIL toolkit USED-FOR SIMILAR. Method are active learning methods, and Submodular Information Measures based actIve LeARning. OtherScientificTerm are imbalance or rare classes, and rare classes. Material is large real - world datasets. ","This paper proposes a unified active learning framework based on submodular information measures (SIM) to tackle the problem of minimizing labeling costs in the presence of imbalance or rare classes. The authors propose a one-stopper solution for active learning based on Submodular Information Measures based actIve LeARning. They show that SIMILAR outperforms existing active learning methods on a variety of image classification tasks including CIFAR-10, MNIST, and ImageNet. They also show that the DISTIL toolkit can be used to improve the quality of the results obtained by SIMILR.   ","This paper proposes a unified active learning framework based on submodular information measures (SIM) to tackle the problem of minimizing labeling costs in the presence of imbalance or rare classes. The authors propose a one-stopper solution for active learning based on Submodular Information Measures based actIve LeARning. They show that SIMILAR outperforms existing active learning methods on a variety of image classification tasks including CIFAR-10, MNIST, and ImageNet. They also show that the DISTIL toolkit can be used to improve the quality of the results obtained by SIMILR.   "
8136,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"identity tests USED-FOR ranking data. Mallows model USED-FOR asymptotic and non - asymptotic settings. Mallows model USED-FOR ranking data. algorithms USED-FOR spread parameter. spread parameter FEATURE-OF Mallows model. Uniformly Most Powerful Unbiased ( UMPU ) test USED-FOR asymptotic setting. sample - optimal non - asymptotic identity test USED-FOR it. Uniformly Most Powerful Unbiased ( UMPU ) test USED-FOR one. distribution of the sufficient statistic USED-FOR it. optimal learning algorithm USED-FOR Mallows model. optimal learning algorithm USED-FOR nonasymptotic test. Mallows models USED-FOR unknown central ranking case. asymptotic setting USED-FOR case. OtherScientificTerm is central ranking. Generic are test, and tests. Material is medium sized data. ","This paper studies identity tests for ranking data from a Mallows model for both asymptotic and non-asymptotic settings. The authors propose two algorithms for estimating the spread parameter of the Mallow's model for the ranking data. The first one is based on the Uniformly Most Powerful Unbiased (UMPU) test for the asymPTotic setting, and it is a sample-optimal non-asymptotically identity test. The second one is an optimal learning algorithm for the Mallows models for the unknown central ranking case, where the central ranking is not known. In this case, the authors show that the test is nonasymmetric if it depends on the distribution of the sufficient statistic. They also show that in the case of medium sized data, the tests are nonasymetric. ","This paper studies identity tests for ranking data from a Mallows model for both asymptotic and non-asymptotic settings. The authors propose two algorithms for estimating the spread parameter of the Mallow's model for the ranking data. The first one is based on the Uniformly Most Powerful Unbiased (UMPU) test for the asymPTotic setting, and it is a sample-optimal non-asymptotically identity test. The second one is an optimal learning algorithm for the Mallows models for the unknown central ranking case, where the central ranking is not known. In this case, the authors show that the test is nonasymmetric if it depends on the distribution of the sufficient statistic. They also show that in the case of medium sized data, the tests are nonasymetric. "
8152,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"sparse multi - view cameras USED-FOR free - viewpoint video. pixel - aligned features USED-FOR radiance fields. heavy occlusions CONJUNCTION dynamic articulations of body parts. dynamic articulations of body parts CONJUNCTION heavy occlusions. parametric human body model USED-FOR robust performance capture. approach USED-FOR generalizable neural radiance fields. parametric human body model USED-FOR generalizable neural radiance fields. parametric human body model USED-FOR approach. temporal transformer USED-FOR tracked visual features. skeletal body motion USED-FOR temporal transformer. skeletal body motion USED-FOR tracked visual features. multi - view transformer USED-FOR cross - attention. temporally - fused features CONJUNCTION pixel - aligned features. pixel - aligned features CONJUNCTION temporally - fused features. ZJU - MoCap and AIST datasets EVALUATE-FOR method. method COMPARE generalizable NeRF methods. generalizable NeRF methods COMPARE method. ZJU - MoCap and AIST datasets EVALUATE-FOR generalizable NeRF methods. OtherScientificTerm is appearance. Method are generalization approaches, and Neural Human Performer. ","This paper proposes a new approach for learning generalizable neural radiance fields from sparse multi-view cameras for free-viewpoint video. The approach is based on a parametric human body model for robust performance capture in the presence of heavy occlusions and dynamic articulations of body parts. A temporal transformer is used to generate tracked visual features based on skeletal body motion, and pixel-aligned features are used to map radiance field to the radiances fields of different parts of the human body. The cross-attention is performed by a multi-views transformer, where the temporal transformer maps temporally-fused features to pixel-altered features. The proposed method is evaluated on the ZJU-MoCap and AIST datasets, and compared with other generalizable NeRF methods. The results show that the proposed method outperforms other generalization approaches, and that the Neural Human Performer is more robust to changes in appearance. ","This paper proposes a new approach for learning generalizable neural radiance fields from sparse multi-view cameras for free-viewpoint video. The approach is based on a parametric human body model for robust performance capture in the presence of heavy occlusions and dynamic articulations of body parts. A temporal transformer is used to generate tracked visual features based on skeletal body motion, and pixel-aligned features are used to map radiance field to the radiances fields of different parts of the human body. The cross-attention is performed by a multi-views transformer, where the temporal transformer maps temporally-fused features to pixel-altered features. The proposed method is evaluated on the ZJU-MoCap and AIST datasets, and compared with other generalizable NeRF methods. The results show that the proposed method outperforms other generalization approaches, and that the Neural Human Performer is more robust to changes in appearance. "
8168,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"recognition CONJUNCTION detection. detection CONJUNCTION recognition. Vision Transformer USED-FOR vision tasks. detection HYPONYM-OF vision tasks. recognition HYPONYM-OF vision tasks. neural architecture search USED-FOR process. architecture CONJUNCTION search space. search space CONJUNCTION architecture. E - T Error USED-FOR search dimensions. weight - sharing supernet USED-FOR E - T Error. weight - sharing supernet USED-FOR search dimensions. Swin CONJUNCTION DeiT. DeiT CONJUNCTION Swin. DeiT CONJUNCTION ViT. ViT CONJUNCTION DeiT. searched models COMPARE models. models COMPARE searched models. searched space COMPARE models. models COMPARE searched space. searched space USED-FOR searched models. S3 HYPONYM-OF searched models. ViT HYPONYM-OF models. Swin HYPONYM-OF models. DeiT HYPONYM-OF models. ImageNet EVALUATE-FOR models. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. semantic segmentation CONJUNCTION visual question answering. visual question answering CONJUNCTION semantic segmentation. S3 USED-FOR vision and vision - language tasks. semantic segmentation EVALUATE-FOR S3. visual question answering EVALUATE-FOR S3. object detection EVALUATE-FOR S3. Generic is architectures. Method is vision transformers. Task are space searching process, and vision transformer. OtherScientificTerm is Search Space. ","This paper proposes a new architecture search for Vision Transformer for vision tasks such as recognition, detection, and semantic segmentation. The proposed architectures are based on the idea that vision transformers can be seen as a space searching process. The process is based on neural architecture search, where the architecture and search space are shared across all layers of the vision transformer. Search space is searched by computing the E-T Error of the search dimensions via a weight-sharing supernet. The searched models are compared with other models such as Swin, DeiT, ViT, and S3 on ImageNet. S3 is shown to outperform the other models on object detection, object segmentation, and visual question answering.  ","This paper proposes a new architecture search for Vision Transformer for vision tasks such as recognition, detection, and semantic segmentation. The proposed architectures are based on the idea that vision transformers can be seen as a space searching process. The process is based on neural architecture search, where the architecture and search space are shared across all layers of the vision transformer. Search space is searched by computing the E-T Error of the search dimensions via a weight-sharing supernet. The searched models are compared with other models such as Swin, DeiT, ViT, and S3 on ImageNet. S3 is shown to outperform the other models on object detection, object segmentation, and visual question answering.  "
8184,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"learning from label proportions ( LLP ) framework USED-FOR linear threshold functions ( LTFs ). algorithm USED-FOR LTF. algorithm USED-FOR LTF. d - dimensional boolean vectors USED-FOR OR. accuracy EVALUATE-FOR LTF. LTFs USED-FOR monotone ORs. LTF HYPONYM-OF algorithm. unit - sized bags HYPONYM-OF supervised learning setup. accuracy EVALUATE-FOR algorithm. linear programming USED-FOR LTFs. techniques USED-FOR LLP setting. LTFs USED-FOR LLP learning of LTFs. LLP CONJUNCTION supervised learning. supervised learning CONJUNCTION LLP. inapproximability EVALUATE-FOR LLP learning LTFs. OtherScientificTerm are bags of feature - vectors, label proportions, bags, labeled feature - vectors, non - monochromatic bags, monotone OR, and non - monochromatic bags case. Metric are algorithmic bounds, and complexity. Generic is bound. ","This paper studies the learning from label proportions (LLP) framework for linear threshold functions (LTFs) for bags of feature-vectors. The authors propose an algorithm for learning an LTF that is monotone and monochromatic. The algorithm, called LTF, is based on the assumption that the label proportions of the bags are monotonically monotonic and that the labeled feature-versus of each bag are monotonal.    The algorithm is motivated by the observation that the accuracy of the LTF can be improved when the labels of the bag are non-monotonic. In particular, the authors show that under certain assumptions on the bags, the algorithm can achieve a monotonicity of the labels.  The authors also provide algorithmic bounds on the complexity of the algorithm.  They show that LTFs can be learned with linear programming, which is a well-studied topic in the literature, and they show that the algorithm that learns a monotonality of an OR with d-dimensional boolean vectors can achieve the same accuracy as the algorithm used to learn a LTF.  In addition, they provide a bound on the inapproximability of LTF to monotones.  Finally, they apply their algorithm to a supervised learning setup (i.e., unit-sized bags) and show that their algorithm achieves the same level of accuracy as LTF in this setting. They also show that this algorithm can be extended to non-Monotone bags, and to a more general case where the labels come from non-convex bags.  This paper also shows that the algorithms can be applied to the LLP setting, and shows that their algorithms can also be used in the non-non-monotonicity setting.  Lastly, they also show how to apply their techniques to the standard LLP setting. The main contribution of this paper is that they provide an algorithm that can be used for the LLP learning of L TFs, which can be combined with supervised learning. ","This paper studies the learning from label proportions (LLP) framework for linear threshold functions (LTFs) for bags of feature-vectors. The authors propose an algorithm for learning an LTF that is monotone and monochromatic. The algorithm, called LTF, is based on the assumption that the label proportions of the bags are monotonically monotonic and that the labeled feature-versus of each bag are monotonal.    The algorithm is motivated by the observation that the accuracy of the LTF can be improved when the labels of the bag are non-monotonic. In particular, the authors show that under certain assumptions on the bags, the algorithm can achieve a monotonicity of the labels.  The authors also provide algorithmic bounds on the complexity of the algorithm.  They show that LTFs can be learned with linear programming, which is a well-studied topic in the literature, and they show that the algorithm that learns a monotonality of an OR with d-dimensional boolean vectors can achieve the same accuracy as the algorithm used to learn a LTF.  In addition, they provide a bound on the inapproximability of LTF to monotones.  Finally, they apply their algorithm to a supervised learning setup (i.e., unit-sized bags) and show that their algorithm achieves the same level of accuracy as LTF in this setting. They also show that this algorithm can be extended to non-Monotone bags, and to a more general case where the labels come from non-convex bags.  This paper also shows that the algorithms can be applied to the LLP setting, and shows that their algorithms can also be used in the non-non-monotonicity setting.  Lastly, they also show how to apply their techniques to the standard LLP setting. The main contribution of this paper is that they provide an algorithm that can be used for the LLP learning of L TFs, which can be combined with supervised learning. "
8200,SP:2eb193c76355aac08003c9b377895202fd3bd297,extreme computational resources USED-FOR neural architecture search ( NAS ). benchmarks EVALUATE-FOR multi - fidelity techniques. learning curve extrapolation HYPONYM-OF multi - fidelity techniques. NAS - Bench-111 CONJUNCTION NAS - Bench-311. NAS - Bench-311 CONJUNCTION NAS - Bench-111. method USED-FOR surrogate benchmarks. NAS - Bench-311 CONJUNCTION NAS - Bench - NLP11. NAS - Bench - NLP11 CONJUNCTION NAS - Bench-311. singular value decomposition and noise modeling USED-FOR method. NAS - Bench-111 HYPONYM-OF surrogate benchmarks. NAS - Bench - NLP11 HYPONYM-OF surrogate benchmarks. NAS - Bench-311 HYPONYM-OF surrogate benchmarks. learning curve extrapolation framework USED-FOR single - fidelity algorithms. it COMPARE single - fidelity algorithms. single - fidelity algorithms COMPARE it. Material is tabular and surrogate benchmarks. Generic is architecture. OtherScientificTerm is architectures. Metric is validation accuracy. ,"This paper studies the problem of neural architecture search (NAS) with extreme computational resources in the context of tabular and surrogate benchmarks, where the goal is to search for the best architecture for a given architecture. These benchmarks are popular benchmarks for evaluating multi-fidelity techniques such as learning curve extrapolation. This paper proposes a method that uses singular value decomposition and noise modeling to train surrogate benchmarks such as NAS-Bench-111, NAS- Bench-311, and NAS-bench-NLP11. The authors show that the proposed method can be applied to three popular surrogate benchmarks (NAS-benchmark-11, NAS Bench-NLI, NASBench-11) and shows that it outperforms single-f fidelity algorithms based on the learning curves extrapolation framework. The paper also shows that the method is robust to the number of architectures and the validation accuracy.   ","This paper studies the problem of neural architecture search (NAS) with extreme computational resources in the context of tabular and surrogate benchmarks, where the goal is to search for the best architecture for a given architecture. These benchmarks are popular benchmarks for evaluating multi-fidelity techniques such as learning curve extrapolation. This paper proposes a method that uses singular value decomposition and noise modeling to train surrogate benchmarks such as NAS-Bench-111, NAS- Bench-311, and NAS-bench-NLP11. The authors show that the proposed method can be applied to three popular surrogate benchmarks (NAS-benchmark-11, NAS Bench-NLI, NASBench-11) and shows that it outperforms single-f fidelity algorithms based on the learning curves extrapolation framework. The paper also shows that the method is robust to the number of architectures and the validation accuracy.   "
8216,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"user - centred method USED-FOR example - based explanations. SimplEx HYPONYM-OF user - centred method. corpus USED-FOR SimplEx. Integrated Jacobian HYPONYM-OF approach. mortality prediction CONJUNCTION image classification. image classification CONJUNCTION mortality prediction. tasks EVALUATE-FOR decompositions. image classification HYPONYM-OF tasks. mortality prediction HYPONYM-OF tasks. Method are machine learning models, convoluted latent representations, mixture of corpus latent representations, and model representations. Generic are latent representations, model, and mixture. OtherScientificTerm are latent space, post - hoc explanations, and features. ","This paper proposes SimplEx, a user-centred method for example-based explanations for machine learning models. SimplEx is based on the idea that convoluted latent representations can be decomposed into a mixture of corpus latent representations. The idea is that the latent representations in the latent space can be decomposable, and that post-hoc explanations can be obtained by decomposing the model representations into the mixture. The authors propose an approach called Integrated Jacobian, which decomposes the model into a set of features that are shared across all the features. They evaluate their decompositions on two tasks: mortality prediction and image classification. ","This paper proposes SimplEx, a user-centred method for example-based explanations for machine learning models. SimplEx is based on the idea that convoluted latent representations can be decomposed into a mixture of corpus latent representations. The idea is that the latent representations in the latent space can be decomposable, and that post-hoc explanations can be obtained by decomposing the model representations into the mixture. The authors propose an approach called Integrated Jacobian, which decomposes the model into a set of features that are shared across all the features. They evaluate their decompositions on two tasks: mortality prediction and image classification. "
8232,SP:c8f82ec90f891d7394933483b7f926155ac363ef,image - text pairs USED-FOR multi - modal representations. Transformer USED-FOR images. CNN USED-FOR images. CNN - Transformer architecture USED-FOR VLP models. Visual relationship between visual contents USED-FOR image understanding. CNNs USED-FOR visual relation learning. visual relation CONJUNCTION inter - modal alignment. inter - modal alignment CONJUNCTION visual relation. objectives PART-OF Transformer network. learning visual relation PART-OF Transformer network. inter - modal alignment PART-OF Transformer network. learning visual relation HYPONYM-OF objectives. inter - modal alignment HYPONYM-OF objectives. design USED-FOR inter - modal alignment learning. Transformer USED-FOR inter - modal alignment learning. fully Transformer visual embedding USED-FOR inter - modal alignment. fully Transformer visual embedding USED-FOR VLP. fully Transformer visual embedding USED-FOR visual relation. Inter - Modality Flow ( IMF ) HYPONYM-OF metric. masking optimization mechanism USED-FOR inter - modality learning. Masked Feature Regression ( MFR ) USED-FOR inter - modality learning. Masked Feature Regression ( MFR ) USED-FOR Transformer. masking optimization mechanism PART-OF Transformer. Masked Feature Regression ( MFR ) HYPONYM-OF masking optimization mechanism. Transformer USED-FOR visual feature learning in VLP. Visual Entailment CONJUNCTION Visual Reasoning. Visual Reasoning CONJUNCTION Visual Entailment. Visual Question Answering ( VQA ) CONJUNCTION Visual Entailment. Visual Entailment CONJUNCTION Visual Question Answering ( VQA ). Image - Text Retrieval CONJUNCTION Visual Question Answering ( VQA ). Visual Question Answering ( VQA ) CONJUNCTION Image - Text Retrieval. vision - language tasks EVALUATE-FOR method. Visual Reasoning HYPONYM-OF vision - language tasks. Image - Text Retrieval HYPONYM-OF vision - language tasks. Visual Question Answering ( VQA ) HYPONYM-OF vision - language tasks. Visual Entailment HYPONYM-OF vision - language tasks. approach COMPARE V,"This paper proposes to learn multi-modal representations from image-text pairs using a Transformer. The authors propose a new CNN-Transformer architecture for VLP models, which they call Inter-Modality Flow (IMF). Visual relationship between visual contents and text is important for image understanding, and many CNNs are used for visual relation learning. The paper proposes two objectives in the Transformer network: 1) learning visual relation and 2) inter-modality alignment. The design of Inter- modality learning is based on the design of a fully Transformer visual embedding, which can be used to learn the visual relation between two modalities. The proposed masking optimization mechanism in Transformer, called Masked Feature Regression (MFR), is used to improve the performance of the inter-medality learning using Transformer for visual feature learning in VLP. Experiments on vision-language tasks such as Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment, and Visual Reasoning show that the proposed method achieves state-of-the-art performance on all three tasks. In addition, the paper also introduces a new metric called Inter- Modality Flow, which is a measure of the impact of each modality on each other.   ","This paper proposes to learn multi-modal representations from image-text pairs using a Transformer. The authors propose a new CNN-Transformer architecture for VLP models, which they call Inter-Modality Flow (IMF). Visual relationship between visual contents and text is important for image understanding, and many CNNs are used for visual relation learning. The paper proposes two objectives in the Transformer network: 1) learning visual relation and 2) inter-modality alignment. The design of Inter- modality learning is based on the design of a fully Transformer visual embedding, which can be used to learn the visual relation between two modalities. The proposed masking optimization mechanism in Transformer, called Masked Feature Regression (MFR), is used to improve the performance of the inter-medality learning using Transformer for visual feature learning in VLP. Experiments on vision-language tasks such as Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment, and Visual Reasoning show that the proposed method achieves state-of-the-art performance on all three tasks. In addition, the paper also introduces a new metric called Inter- Modality Flow, which is a measure of the impact of each modality on each other.   "
8248,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"information leakage FEATURE-OF iterative randomized learning algorithm. model USED-FOR information leakage. noisy gradient descent algorithms USED-FOR problem. Rényi divergence FEATURE-OF probability distributions. probability distributions FEATURE-OF models. smooth and strongly convex loss functions COMPARE composition theorems. composition theorems COMPARE smooth and strongly convex loss functions. noisy gradient descent algorithms USED-FOR optimal utility. gradient complexity FEATURE-OF optimal utility. Generic is algorithm. OtherScientificTerm are dynamics of Rényi differential privacy loss, privacy loss, and intermediate gradient computations. ","This paper studies the problem of information leakage in an iterative randomized learning algorithm. In particular, the authors consider a model where the goal is to learn a model that minimizes the Rényi differential privacy loss, and the algorithm is designed to minimize the information leakage. The problem is formulated as the problem with noisy gradient descent algorithms, where the objective is to maximize the optimal utility of the algorithm. The authors consider the dynamics of Rényian differential privacy and show that under certain assumptions on the model and on the distribution of the data, the algorithm achieves the optimal privacy loss. They also prove that under some conditions, the optimal value of the privacy loss is a function of the number of intermediate gradient computations.   The authors also show that for models with different probability distributions over Rényier divergence, their algorithm achieves a loss that is a lower bound of the optimal loss.  Finally, they prove that for smooth and strongly convex loss functions, their algorithms achieve the same optimal utility as existing composition theorems, and that the optimal risk minimization is a result of the noisy gradients of the algorithms. They further show that their algorithm has a lower gradient complexity than existing algorithms.","This paper studies the problem of information leakage in an iterative randomized learning algorithm. In particular, the authors consider a model where the goal is to learn a model that minimizes the Rényi differential privacy loss, and the algorithm is designed to minimize the information leakage. The problem is formulated as the problem with noisy gradient descent algorithms, where the objective is to maximize the optimal utility of the algorithm. The authors consider the dynamics of Rényian differential privacy and show that under certain assumptions on the model and on the distribution of the data, the algorithm achieves the optimal privacy loss. They also prove that under some conditions, the optimal value of the privacy loss is a function of the number of intermediate gradient computations.   The authors also show that for models with different probability distributions over Rényier divergence, their algorithm achieves a loss that is a lower bound of the optimal loss.  Finally, they prove that for smooth and strongly convex loss functions, their algorithms achieve the same optimal utility as existing composition theorems, and that the optimal risk minimization is a result of the noisy gradients of the algorithms. They further show that their algorithm has a lower gradient complexity than existing algorithms."
8264,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,large - scale machine learning CONJUNCTION embedded optimal control. embedded optimal control CONJUNCTION large - scale machine learning. First - order methods USED-FOR large - scale machine learning. First - order methods USED-FOR quadratic optimization. First - order methods USED-FOR embedded optimal control. OSQP HYPONYM-OF First - order methods. OSQP HYPONYM-OF quadratic optimization. manual hyperparameter tuning CONJUNCTION convergence time. convergence time CONJUNCTION manual hyperparameter tuning. Reinforcement Learning ( RL ) USED-FOR policy. RL policy COMPARE QP solvers. QP solvers COMPARE RL policy. RLQP COMPARE QP solvers. QP solvers COMPARE RLQP. RLQP HYPONYM-OF RL policy. RLQP USED-FOR problems. RLQP USED-FOR applications. Maros - Mészáros problems HYPONYM-OF applications. QPLIB HYPONYM-OF applications. Generic is methods. Material is QP benchmarks. ,"This paper studies the use of First-order methods for large-scale machine learning and embedded optimal control, such as OSQP, for quadratic optimization. The authors propose RLQP (RLQP), an RL policy that uses Reinforcement Learning (RL) to learn a policy to solve the problem. The RL policy is shown to outperform existing QP solvers on a number of standard QP benchmarks, and the authors show that the RL policy can also be used to solve more complex problems (e.g., the Maros-Mészáros problems). The paper also shows that the proposed methods can be applied to other applications such as QPLIB, where manual hyperparameter tuning and convergence time can be used. ","This paper studies the use of First-order methods for large-scale machine learning and embedded optimal control, such as OSQP, for quadratic optimization. The authors propose RLQP (RLQP), an RL policy that uses Reinforcement Learning (RL) to learn a policy to solve the problem. The RL policy is shown to outperform existing QP solvers on a number of standard QP benchmarks, and the authors show that the RL policy can also be used to solve more complex problems (e.g., the Maros-Mészáros problems). The paper also shows that the proposed methods can be applied to other applications such as QPLIB, where manual hyperparameter tuning and convergence time can be used. "
8280,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"convergence rate EVALUATE-FOR model. PC - bias USED-FOR linear and non8 linear networks. PC - bias USED-FOR early stopping. early stopping USED-FOR PCA. random labels USED-FOR deep networks. Method are convolutional neural networks, and over - parametrized deep linear network model. OtherScientificTerm are asymptotic analysis, hidden layers, principal components, singular 6 values, convergence pattern, Principal Components bias ( PC - bias ), and spectral bias. Task is learning. Generic is biases. ","This paper studies the over-parametrized deep linear network (DRLN) problem. The authors propose a new bias term called Principal Components Bias (PC-Bias) for linear and non-linear neural networks. The bias term is based on the observation that the convergence rate of the model converges to a stationary point when the number of hidden layers increases to infinity.   The authors show that PC-bias can be used for early stopping in PCA, and that it can also be used to improve the performance of deep networks trained with random labels. ","This paper studies the over-parametrized deep linear network (DRLN) problem. The authors propose a new bias term called Principal Components Bias (PC-Bias) for linear and non-linear neural networks. The bias term is based on the observation that the convergence rate of the model converges to a stationary point when the number of hidden layers increases to infinity.   The authors show that PC-bias can be used for early stopping in PCA, and that it can also be used to improve the performance of deep networks trained with random labels. "
8296,SP:1598bad835a657e56af3261501c671897b7e9ffd,"Backdoor attack HYPONYM-OF deep neural networks ( DNNs ). defense methods USED-FOR detecting or erasing backdoors. anti - backdoor learning USED-FOR clean models. backdoor - poisoned data USED-FOR clean models. dual - task USED-FOR learning process. models USED-FOR backdoored data. backdoored data USED-FOR model. Anti - Backdoor Learning ( ABL ) USED-FOR backdoor attacks. learning scheme USED-FOR backdoor attacks. Anti - Backdoor Learning ( ABL ) HYPONYM-OF learning scheme. two - stage gradient ascent mechanism USED-FOR ABL. ABL - trained models COMPARE they. they COMPARE ABL - trained models. backdoor - poisoned data USED-FOR ABL - trained models. clean data USED-FOR ABL - trained models. clean data USED-FOR they. OtherScientificTerm are backdoor triggers, backdoor task, and backdoor target class. ","This paper studies the problem of backdoor attack on deep neural networks (DNNs). The authors propose two defense methods for detecting or erasing backdoors: 1) anti-backdoor learning to train clean models on backdoor-poisoned data, and 2) dual-task to guide the learning process. The authors show that models trained on backdoored data can be more robust to backdoors than models that are trained on clean data. They also propose a new learning scheme called Anti-Backdoor Learning (ABL) to defend against backdoor attacks, which is based on a two-stage gradient ascent mechanism. They show that ABL-trained models are more robust than they would be if they were trained on only clean data, but they are also robust to backdoor attacks if they are trained with clean data poisoned with a backdoor task. The paper also shows that the backdoor target class is not the same as the target class. ","This paper studies the problem of backdoor attack on deep neural networks (DNNs). The authors propose two defense methods for detecting or erasing backdoors: 1) anti-backdoor learning to train clean models on backdoor-poisoned data, and 2) dual-task to guide the learning process. The authors show that models trained on backdoored data can be more robust to backdoors than models that are trained on clean data. They also propose a new learning scheme called Anti-Backdoor Learning (ABL) to defend against backdoor attacks, which is based on a two-stage gradient ascent mechanism. They show that ABL-trained models are more robust than they would be if they were trained on only clean data, but they are also robust to backdoor attacks if they are trained with clean data poisoned with a backdoor task. The paper also shows that the backdoor target class is not the same as the target class. "
8312,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"generative radiance fields USED-FOR 3Daware image synthesis. multi - view constraint USED-FOR 3D radiance fields. regularization USED-FOR 3D radiance fields. methods USED-FOR 3D radiance fields. multi - view constraint USED-FOR methods. 2D images USED-FOR 3D radiance fields. they USED-FOR 3D shapes. shading - guided generative implicit model USED-FOR shape representation. 3D shape USED-FOR realistic rendering. lighting conditions FEATURE-OF realistic rendering. lighting conditions FEATURE-OF shading. discriminator USED-FOR Gradients. surface tracking USED-FOR volume rendering strategy. approach USED-FOR photorealistic 3D - aware image synthesis. approach USED-FOR 3D shapes. approach COMPARE methods. methods COMPARE approach. approach USED-FOR image relighting. approach USED-FOR 3D shape reconstruction. OtherScientificTerm are shapecolor ambiguity, multi - lighting constraint, illumination, computational burden, and surface normals. Metric is training and inference time. ","This paper proposes a new method for 3Daware image synthesis based on 3D radiance fields. The proposed method is based on a multi-view constraint on the lighting of the scene, which is used to train a generative implicit model. The paper also proposes a volume rendering strategy based on surface tracking. Experiments show that the proposed method outperforms the state-of-the-art methods in terms of image quality. ","This paper proposes a new method for 3Daware image synthesis based on 3D radiance fields. The proposed method is based on a multi-view constraint on the lighting of the scene, which is used to train a generative implicit model. The paper also proposes a volume rendering strategy based on surface tracking. Experiments show that the proposed method outperforms the state-of-the-art methods in terms of image quality. "
8328,SP:4b3dad77d79507c512877867dfea6db87a78682d,"flexible machine learning models USED-FOR instrumental variable ( IV ) regression. quasi - Bayesian procedure USED-FOR IV regression. kernelized IV models USED-FOR quasi - Bayesian procedure. Bayesian modeling USED-FOR IV. Bayesian modeling COMPARE approach. approach COMPARE Bayesian modeling. approach USED-FOR approximate inference algorithm. approximate inference algorithm COMPARE point estimation methods. point estimation methods COMPARE approximate inference algorithm. time cost EVALUATE-FOR point estimation methods. time cost EVALUATE-FOR approximate inference algorithm. algorithm USED-FOR neural network models. Method is uncertainty quantification methodology. OtherScientificTerm are data generating process, and quasi - posterior. Generic is method. ","This paper proposes a new uncertainty quantification methodology for instrumental variable (IV) regression with flexible machine learning models. The authors propose a quasi-Bayesian procedure for IV regression with kernelized IV models, where the data generating process is flexible and the quasi-posterior is a function of the data distribution. The paper shows that Bayesian modeling for IV is more efficient than the traditional approach, and the proposed approach can be used as an approximate inference algorithm that has a lower time cost than existing point estimation methods. The proposed algorithm can be applied to neural network models and is shown to be applicable to a wide range of datasets. The method is evaluated on a variety of datasets and shows that the proposed method performs well. ","This paper proposes a new uncertainty quantification methodology for instrumental variable (IV) regression with flexible machine learning models. The authors propose a quasi-Bayesian procedure for IV regression with kernelized IV models, where the data generating process is flexible and the quasi-posterior is a function of the data distribution. The paper shows that Bayesian modeling for IV is more efficient than the traditional approach, and the proposed approach can be used as an approximate inference algorithm that has a lower time cost than existing point estimation methods. The proposed algorithm can be applied to neural network models and is shown to be applicable to a wide range of datasets. The method is evaluated on a variety of datasets and shows that the proposed method performs well. "
8344,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,language - specific annotated data CONJUNCTION knowledge sources. knowledge sources CONJUNCTION language - specific annotated data. translation CONJUNCTION in - language retrieval modules. in - language retrieval modules CONJUNCTION translation. multilingual autoregressive generation model CONJUNCTION CORA. CORA CONJUNCTION multilingual autoregressive generation model. annotated data USED-FOR iterative training method. high - resource languages FEATURE-OF annotated data. multilingual open QA benchmarks EVALUATE-FOR CORA. cross - lingual retrieval CONJUNCTION generation. generation CONJUNCTION cross - lingual retrieval. Method is dense passage retrieval algorithm. OtherScientificTerm is low - resource ones. Material is low - resource settings. Generic is model. ,"This paper proposes a dense passage retrieval algorithm for multi-lingual open QA, where language-specific annotated data and knowledge sources are available. The authors propose a multilingual autoregressive generation model, called CORA, which combines the benefits of translation and in-language retrieval modules. The proposed iterative training method is based on the use of annotated training data from high-resource languages and low-resource ones. The model is evaluated on the multilingual openQA benchmarks, and the results show that CORA outperforms the state-of-the-art on the high resource and low resource settings. Experiments are conducted on cross-lengual retrieval and generation.   ","This paper proposes a dense passage retrieval algorithm for multi-lingual open QA, where language-specific annotated data and knowledge sources are available. The authors propose a multilingual autoregressive generation model, called CORA, which combines the benefits of translation and in-language retrieval modules. The proposed iterative training method is based on the use of annotated training data from high-resource languages and low-resource ones. The model is evaluated on the multilingual openQA benchmarks, and the results show that CORA outperforms the state-of-the-art on the high resource and low resource settings. Experiments are conducted on cross-lengual retrieval and generation.   "
8360,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"specialized training algorithms USED-FOR domain generalization. deep neural networks COMPARE specialized training algorithms. specialized training algorithms COMPARE deep neural networks. distribution shift FEATURE-OF deep neural networks. Empirical Risk Minimization ( ERM ) USED-FOR deep neural networks. domain generalization datasets USED-FOR ERM models. Fisher information CONJUNCTION predictive entropy. predictive entropy CONJUNCTION Fisher information. predictive entropy CONJUNCTION maximum mean discrepancy. maximum mean discrepancy CONJUNCTION predictive entropy. measures USED-FOR out - of - distribution generalization. measures CONJUNCTION predictive entropy. predictive entropy CONJUNCTION measures. out - of - distribution generalization EVALUATE-FOR ERM models. Fisher information FEATURE-OF measures. maximum mean discrepancy FEATURE-OF measures. deep networks USED-FOR out - of - distribution. ERM USED-FOR deep networks. Method are domain adaptation theory, and ERMs. Generic is theory. Task are out - of - domain generalization, and generalization. ","This paper studies the problem of out-of-distribution generalization of deep neural networks trained with Empirical Risk Minimization (ERM) in the presence of distribution shift. The authors draw inspiration from domain adaptation theory and show empirically that ERM models trained on standard domain generalization datasets outperform specialized training algorithms for domain generalisation. They also show that the performance of ERM trained on ERM-trained deep networks is similar to that of standard deep networks trained without ERM. The paper also shows empirically how different measures such as Fisher information, predictive entropy, and maximum mean discrepancy affect the performance and generalization.   ","This paper studies the problem of out-of-distribution generalization of deep neural networks trained with Empirical Risk Minimization (ERM) in the presence of distribution shift. The authors draw inspiration from domain adaptation theory and show empirically that ERM models trained on standard domain generalization datasets outperform specialized training algorithms for domain generalisation. They also show that the performance of ERM trained on ERM-trained deep networks is similar to that of standard deep networks trained without ERM. The paper also shows empirically how different measures such as Fisher information, predictive entropy, and maximum mean discrepancy affect the performance and generalization.   "
8376,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,backdoor data poisoning attack HYPONYM-OF adversarial attack. watermarked examples USED-FOR model. backdoor data poisoning attacks USED-FOR classification problems. formal theoretical framework USED-FOR backdoor data poisoning attacks. this USED-FOR statistical and computational issues. statistical and computational issues FEATURE-OF attacks. intrinsic vulnerability FEATURE-OF learning problem. learning problem USED-FOR backdoor attack. memorization capacity HYPONYM-OF parameter. robustness FEATURE-OF natural learning problems. backdoor attacks FEATURE-OF natural learning problems. natural problem settings USED-FOR backdoor attacks. adversarial training USED-FOR backdoors. backdoor filtering CONJUNCTION robust generalization. robust generalization CONJUNCTION backdoor filtering. robust generalization HYPONYM-OF problems. backdoor filtering HYPONYM-OF problems. Generic is assumptions. Method is learning algorithm. ,"This paper studies the backdoor data poisoning attack, which is an adversarial attack where watermarked examples are used to fool a model. The authors propose a formal theoretical framework for backdoor data poisoned attacks for classification problems, and show that this can be used to study the statistical and computational issues of such attacks. They also show that the intrinsic vulnerability of the learning problem for a backdoor attack is related to a parameter called the memorization capacity, and that under certain assumptions, the learning algorithm can be modified to make the backdoor attack more difficult to detect. The paper also shows that the robustness of natural learning problems against backdoor attacks can be improved in natural problem settings (backdoor filtering and robust generalization). Finally, the paper shows that adversarial training can be applied to detect backdoors.","This paper studies the backdoor data poisoning attack, which is an adversarial attack where watermarked examples are used to fool a model. The authors propose a formal theoretical framework for backdoor data poisoned attacks for classification problems, and show that this can be used to study the statistical and computational issues of such attacks. They also show that the intrinsic vulnerability of the learning problem for a backdoor attack is related to a parameter called the memorization capacity, and that under certain assumptions, the learning algorithm can be modified to make the backdoor attack more difficult to detect. The paper also shows that the robustness of natural learning problems against backdoor attacks can be improved in natural problem settings (backdoor filtering and robust generalization). Finally, the paper shows that adversarial training can be applied to detect backdoors."
8392,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"networks COMPARE ones. ones COMPARE networks. Large width limits PART-OF deep learning research. representational power FEATURE-OF networks. capacity CONJUNCTION width. width CONJUNCTION capacity. neural networks USED-FOR Deep Gaussian Processes ( Deep GP ). Deep Gaussian Processes ( Deep GP ) HYPONYM-OF nonparametric hierarchical models. neural nets HYPONYM-OF nonparametric hierarchical models. they USED-FOR modeling task. width USED-FOR neural networks. nonparametric Deep GP USED-FOR Gaussian processes. mixture of data - adaptable basis functions FEATURE-OF posterior. width CONJUNCTION depth. depth CONJUNCTION width. depth USED-FOR model. non - Gaussianity FEATURE-OF model. hidden units USED-FOR neural networks. L2 regularization USED-FOR neural networks. OtherScientificTerm are computational practicalities, GP behavior, adaptability, and Gaussian prior on parameters. Method are Deep GP, and hierarchical models. ","Large width limits in deep learning research have been a topic of interest in recent years, but the computational practicalities have not been well studied. This paper studies the representational power of networks with large capacity and width. The authors propose Deep Gaussian Processes (Deep GP), a family of nonparametric hierarchical models, including neural nets and neural networks with neural networks.    The authors show that networks with larger width and depth are more powerful than ones with smaller ones, and that they are more adaptable to the modeling task.  They also show that Deep GP is a generalization of Deep GP.  The main contribution of the paper is to show that the width and the depth of neural networks trained with L2 regularization are the limiting factors of the GP behavior.  In particular, they show that for a Gaussian processes trained with a non-parametric Deep GP, the width of the neural networks is the limiting factor of the model performance, and for a hierarchical models trained with the same depth, the model's performance is the limit of the non-Gaussianity of the posterior over a mixture of data-adaptable basis functions.  Finally, the authors provide a theoretical analysis of the adaptability of the network to the Gaussian prior on parameters, and show that neural networks that have more hidden units are more robust to the change in Gaussianity. ","Large width limits in deep learning research have been a topic of interest in recent years, but the computational practicalities have not been well studied. This paper studies the representational power of networks with large capacity and width. The authors propose Deep Gaussian Processes (Deep GP), a family of nonparametric hierarchical models, including neural nets and neural networks with neural networks.    The authors show that networks with larger width and depth are more powerful than ones with smaller ones, and that they are more adaptable to the modeling task.  They also show that Deep GP is a generalization of Deep GP.  The main contribution of the paper is to show that the width and the depth of neural networks trained with L2 regularization are the limiting factors of the GP behavior.  In particular, they show that for a Gaussian processes trained with a non-parametric Deep GP, the width of the neural networks is the limiting factor of the model performance, and for a hierarchical models trained with the same depth, the model's performance is the limit of the non-Gaussianity of the posterior over a mixture of data-adaptable basis functions.  Finally, the authors provide a theoretical analysis of the adaptability of the network to the Gaussian prior on parameters, and show that neural networks that have more hidden units are more robust to the change in Gaussianity. "
8408,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"algorithmic framework USED-FOR challenges. systems heterogeneity CONJUNCTION infrequent and imprecise communication. infrequent and imprecise communication CONJUNCTION systems heterogeneity. objective heterogeneity CONJUNCTION systems heterogeneity. systems heterogeneity CONJUNCTION objective heterogeneity. challenges PART-OF FL. FedLin HYPONYM-OF algorithmic framework. infrequent and imprecise communication HYPONYM-OF challenges. objective heterogeneity HYPONYM-OF challenges. systems heterogeneity HYPONYM-OF challenges. speed - accuracy conflict FEATURE-OF FL algorithms. FedLin USED-FOR linear convergence. matching upper and lower bounds FEATURE-OF convergence rate. convergence rate FEATURE-OF FedLin. matching upper and lower bounds FEATURE-OF FedLin. compression level FEATURE-OF convergence rate. gradient sparsification USED-FOR FedLin. linear convergence rates FEATURE-OF FedLin. gradient sparsification USED-FOR FL. Task is federated learning ( FL ) setup. Method is statistical model. Generic are framework, and they. OtherScientificTerm are global minimum, sub - linear rate, fast convergence, clients ’ local loss functions, objective and systems heterogeneity, infrequent, periodic communication, and tight linear convergence rate guarantees. Metric is accuracy. ","This paper proposes FedLin, an algorithmic framework that addresses three challenges in federated learning (FL) setup: (1) systems heterogeneity, (2) infrequent and imprecise communication, and (3) the speed-accuracy conflict in FL algorithms. FedLin is an algorithm that addresses these challenges in FL by introducing a new statistical model and a new algorithm, which is called FedLin. The authors show that FedLin converges to a global minimum at a sub-linear rate, and that this fast convergence is guaranteed for all clients’ local loss functions. They also provide matching upper and lower bounds on the convergence rate of FedLin at any compression level. The main contribution of this paper is that Fed Lin is able to provide tight linear convergence rate guarantees for all three of these challenges (objective heterogeneity, systems heterogeneity and infrequent, periodic communication). In addition, FedLin shows that gradient sparsification in FL leads to linear convergence rates that are asymptotically tight as those obtained by FedLin when the objective and systems heterogeneity are well-calibrated.    The authors also provide a theoretical analysis of their framework and show that they provide a convergence rate guarantee for FedLin with respect to all compression levels.  The paper also shows that the accuracy of the algorithm is not affected by the clients' local loss function, which shows that there is a trade-off between accuracy and speed. ","This paper proposes FedLin, an algorithmic framework that addresses three challenges in federated learning (FL) setup: (1) systems heterogeneity, (2) infrequent and imprecise communication, and (3) the speed-accuracy conflict in FL algorithms. FedLin is an algorithm that addresses these challenges in FL by introducing a new statistical model and a new algorithm, which is called FedLin. The authors show that FedLin converges to a global minimum at a sub-linear rate, and that this fast convergence is guaranteed for all clients’ local loss functions. They also provide matching upper and lower bounds on the convergence rate of FedLin at any compression level. The main contribution of this paper is that Fed Lin is able to provide tight linear convergence rate guarantees for all three of these challenges (objective heterogeneity, systems heterogeneity and infrequent, periodic communication). In addition, FedLin shows that gradient sparsification in FL leads to linear convergence rates that are asymptotically tight as those obtained by FedLin when the objective and systems heterogeneity are well-calibrated.    The authors also provide a theoretical analysis of their framework and show that they provide a convergence rate guarantee for FedLin with respect to all compression levels.  The paper also shows that the accuracy of the algorithm is not affected by the clients' local loss function, which shows that there is a trade-off between accuracy and speed. "
8424,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,Sliced - Wasserstein distance ( SW ) USED-FOR machine learning applications. Sliced - Wasserstein distance ( SW ) COMPARE Wasserstein distance. Wasserstein distance COMPARE Sliced - Wasserstein distance ( SW ). Monte Carlo USED-FOR SW. perspective USED-FOR SW. one - dimensional projections FEATURE-OF highdimensional random vector. concentration of measure phenomenon USED-FOR perspective. concentration of measure phenomenon USED-FOR SW. deterministic approximation USED-FOR SW. method COMPARE Monte Carlo approximation. Monte Carlo approximation COMPARE method. weak dependence condition FEATURE-OF data distribution. nonasymptotical guarantees USED-FOR approach. generative modeling problem EVALUATE-FOR approximation. Generic is it. OtherScientificTerm is random projections. Metric is approximation error. Material is synthetic datasets. ,"This paper proposes Sliced-Wasserstein distance (SW) as an alternative to the Wasserstein Distance (WSD) for machine learning problems. The main idea is to use the concentration of measure phenomenon (CCM) phenomenon to derive a deterministic version of the WSD. The authors show that the proposed SW can be approximated by Monte Carlo, and that it is nonasymptotically tractable. Experiments are conducted on synthetic data and a generative modeling problem. ","This paper proposes Sliced-Wasserstein distance (SW) as an alternative to the Wasserstein Distance (WSD) for machine learning problems. The main idea is to use the concentration of measure phenomenon (CCM) phenomenon to derive a deterministic version of the WSD. The authors show that the proposed SW can be approximated by Monte Carlo, and that it is nonasymptotically tractable. Experiments are conducted on synthetic data and a generative modeling problem. "
8440,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,neural language models CONJUNCTION translation models. translation models CONJUNCTION neural language models. translation models CONJUNCTION language tagging tasks. language tagging tasks CONJUNCTION translation models. language tagging tasks USED-FOR representations. neural language models USED-FOR representations. translation models USED-FOR representations. networks USED-FOR language tasks. hidden representations USED-FOR networks. computer vision USED-FOR encoder - decoder transfer learning method. hidden representations USED-FOR feature spaces. language models CONJUNCTION translation models. translation models CONJUNCTION language models. word embeddings CONJUNCTION syntactic and semantic tasks. syntactic and semantic tasks CONJUNCTION word embeddings. syntactic and semantic tasks CONJUNCTION word embeddings. word embeddings CONJUNCTION syntactic and semantic tasks. method USED-FOR low - dimensional structure. it USED-FOR NLP ( natural language processing ) tasks. language representation embedding USED-FOR low - dimensional structure. feature space USED-FOR human brain responses. representation embedding USED-FOR feature space. natural language stimuli USED-FOR human brain responses. fMRI USED-FOR natural language stimuli. fMRI USED-FOR human brain responses. metric USED-FOR brain ’s natural language processing hierarchy. principal dimension USED-FOR metric. principal dimension FEATURE-OF structure. structure USED-FOR metric. embedding USED-FOR brain ’s natural language representation structure. ,"This paper proposes an encoder-decoder transfer learning method based on computer vision to learn feature spaces based on hidden representations from neural language models, translation models, and language tagging tasks. The authors show that networks trained on language tasks can learn representations that are similar to the representations learned by language models and translation models. The method learns a low-dimensional structure from the language representation embedding. The paper also shows that it can be applied to a variety of NLP (natural language processing) tasks, including word embeddings, syntactic and semantic tasks. Experiments show that the feature space learned by learning the representations is similar to human brain responses to natural language stimuli from fMRI, and that the representation learned from the representations of these stimuli can be used to learn a representation space that is more similar to that of a human brain.   The authors also show that this embedding can capture the brain’s natural language representation structure and that a metric for measuring the structure based on the principal dimension of the structure is learned. ","This paper proposes an encoder-decoder transfer learning method based on computer vision to learn feature spaces based on hidden representations from neural language models, translation models, and language tagging tasks. The authors show that networks trained on language tasks can learn representations that are similar to the representations learned by language models and translation models. The method learns a low-dimensional structure from the language representation embedding. The paper also shows that it can be applied to a variety of NLP (natural language processing) tasks, including word embeddings, syntactic and semantic tasks. Experiments show that the feature space learned by learning the representations is similar to human brain responses to natural language stimuli from fMRI, and that the representation learned from the representations of these stimuli can be used to learn a representation space that is more similar to that of a human brain.   The authors also show that this embedding can capture the brain’s natural language representation structure and that a metric for measuring the structure based on the principal dimension of the structure is learned. "
8456,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"paradigm USED-FOR unconditional variational autoencoders ( VAEs ). unconditional variational autoencoders ( VAEs ) USED-FOR few - shot conditional image generation. Diffusion - Decoding models CONJUNCTION Contrastive representations ( D2C ). Contrastive representations ( D2C ) CONJUNCTION Diffusion - Decoding models. diffusion - based prior USED-FOR generation. diffusion - based prior USED-FOR latent representations. D2C USED-FOR generation. contrastive selfsupervised learning USED-FOR representation quality. contrastive selfsupervised learning USED-FOR D2C. diffusion - based prior USED-FOR D2C. D2C USED-FOR generation tasks. D2C COMPARE diffusion models. diffusion models COMPARE D2C. D2C USED-FOR conditional generation. D2C generations COMPARE StyleGAN2 ones. StyleGAN2 ones COMPARE D2C generations. double - blind study EVALUATE-FOR human evaluators. D2C generations USED-FOR conditional image manipulation. double - blind study EVALUATE-FOR D2C generations. Method are Conditional generative models of high - dimensional images, and d2c. OtherScientificTerm are supervision signals, and manipulation constraints. ","Conditional generative models of high-dimensional images have been a focus of interest in recent years. Conditional variational autoencoders (VAEs) have been proposed as a paradigm for few-shot conditional image generation, where supervision signals are not available. This paper proposes a new paradigm for conditional generational VAEs, called Diffusion-Decoding models and Contrastive representations (D2C). D2C is based on diffusion-decoding models, where a diffusion-based prior on the latent representations is used to guide the generation, and contrastive selfsupervised learning is applied to improve the representation quality. The authors show that d2c is able to achieve state-of-the-art performance on a number of generation tasks. They also show that the conditional generation of d2C outperforms other diffusion models, and demonstrate that the generation of a conditional generation using D2c can be more robust to manipulation constraints. Finally, the authors conduct a double-blind study on human evaluators, and show that D2Cs outperform the StyleGAN2 generations on the task of conditional image manipulation. ","Conditional generative models of high-dimensional images have been a focus of interest in recent years. Conditional variational autoencoders (VAEs) have been proposed as a paradigm for few-shot conditional image generation, where supervision signals are not available. This paper proposes a new paradigm for conditional generational VAEs, called Diffusion-Decoding models and Contrastive representations (D2C). D2C is based on diffusion-decoding models, where a diffusion-based prior on the latent representations is used to guide the generation, and contrastive selfsupervised learning is applied to improve the representation quality. The authors show that d2c is able to achieve state-of-the-art performance on a number of generation tasks. They also show that the conditional generation of d2C outperforms other diffusion models, and demonstrate that the generation of a conditional generation using D2c can be more robust to manipulation constraints. Finally, the authors conduct a double-blind study on human evaluators, and show that D2Cs outperform the StyleGAN2 generations on the task of conditional image manipulation. "
8472,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"contrastive learning paradigm USED-FOR representations. Edges PART-OF graph. ground - truth classes PART-OF connected sub - graphs. contrastive learning objective USED-FOR neural net representations. population augmentation graph USED-FOR spectral decomposition. spectral decomposition USED-FOR loss. contrastive learning objective USED-FOR loss. objective USED-FOR features. linear probe evaluation FEATURE-OF features. generalization bounds USED-FOR accuracy guarantees. objective COMPARE baselines. baselines COMPARE objective. features COMPARE baselines. baselines COMPARE features. objective USED-FOR features. benchmark vision datasets EVALUATE-FOR objective. benchmark vision datasets EVALUATE-FOR baselines. Task is self - supervised learning. OtherScientificTerm are conditional independence of the positive pairs, correlated positive pairs, conditional independence of positive pairs, and augmentation graph. Method is contrastive learning. Metric is training contrastive loss. ",This paper proposes a new contrastive learning paradigm to learn representations that are independent of negative samples and correlated with positive samples in self-supervised learning. The key idea is to use the conditional independence of the positive pairs in the training set and the correlated positive pairs to learn the representations. Edges in the graph are replaced by connected sub-graphs that contain ground-truth classes and correlated negative pairs. The loss is based on spectral decomposition on the population augmentation graph. The authors show that the proposed contrastive loss is a generalization of the training contrastive objective for learning neural net representations. They also provide generalization bounds for accuracy guarantees and show that their objective can learn features that are robust to linear probe evaluation. Experiments on benchmark vision datasets show that this objective is able to learn features with better features than the baselines.,This paper proposes a new contrastive learning paradigm to learn representations that are independent of negative samples and correlated with positive samples in self-supervised learning. The key idea is to use the conditional independence of the positive pairs in the training set and the correlated positive pairs to learn the representations. Edges in the graph are replaced by connected sub-graphs that contain ground-truth classes and correlated negative pairs. The loss is based on spectral decomposition on the population augmentation graph. The authors show that the proposed contrastive loss is a generalization of the training contrastive objective for learning neural net representations. They also provide generalization bounds for accuracy guarantees and show that their objective can learn features that are robust to linear probe evaluation. Experiments on benchmark vision datasets show that this objective is able to learn features with better features than the baselines.
8488,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"parameterized complexity EVALUATE-FOR Bayesian Network Structure Learning ( BNSL ). complexity EVALUATE-FOR BNSL. parameterization USED-FOR fixed - parameter tractability. feedback edge set HYPONYM-OF parameterization. lower bounds USED-FOR complexity classification of BNSL. complexity classification EVALUATE-FOR BNSL. complexity EVALUATE-FOR BNSL. additive representation USED-FOR BNSL. OtherScientificTerm are superstructure, graph parameters, and treewidth. Task are fixed - parameter tractable, and Polytree Learning. Method is non - zero representation. ","This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL). The authors show that the complexity of BNSL depends on the superstructure of the structure of the network and the choice of the parameterization (e.g., feedback edge set) that is used for the fixed-parameter tractability. The authors also provide lower bounds for the complexity classification of the complexity classifier.    The main contribution of the paper is to show that under certain assumptions on the structure and the number of graph parameters, there exists a non-zero representation that is non-trivial to compute.  The paper also shows that the fixed parameter tractability can be improved by using a different parameterization, i.e., the treewidth.  Finally, the authors provide a lower bound on the complexity for Polytree Learning. They show that for any additive representation, the complexity is bounded by a constant factor that depends only on the size of the representation.","This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL). The authors show that the complexity of BNSL depends on the superstructure of the structure of the network and the choice of the parameterization (e.g., feedback edge set) that is used for the fixed-parameter tractability. The authors also provide lower bounds for the complexity classification of the complexity classifier.    The main contribution of the paper is to show that under certain assumptions on the structure and the number of graph parameters, there exists a non-zero representation that is non-trivial to compute.  The paper also shows that the fixed parameter tractability can be improved by using a different parameterization, i.e., the treewidth.  Finally, the authors provide a lower bound on the complexity for Polytree Learning. They show that for any additive representation, the complexity is bounded by a constant factor that depends only on the size of the representation."
8504,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,active learning algorithm USED-FOR binary classification tasks. active learning algorithm USED-FOR streaming setting. streaming setting USED-FOR binary classification tasks. model USED-FOR surrogate loss. algorithm USED-FOR model. labeled and weak - labeled points USED-FOR surrogate loss. weak labels USED-FOR algorithm. theoretical guarantees FEATURE-OF general agnostic setting. Uncertainty Sampling HYPONYM-OF active learning algorithm. algorithm COMPARE baselines. baselines COMPARE algorithm. Margin Algorithm CONJUNCTION Uncertainty Sampling. Uncertainty Sampling CONJUNCTION Margin Algorithm. generalization and label complexity bounds EVALUATE-FOR algorithm. Margin Algorithm HYPONYM-OF baselines. Uncertainty Sampling HYPONYM-OF baselines. Material is real - world datasets. ,"This paper proposes an active learning algorithm for binary classification tasks in the streaming setting. The proposed algorithm learns a surrogate loss over both labeled and weak-labeled points, and then uses this model to train a new surrogate loss using both the original model and the surrogate loss with weak labels. The authors provide theoretical guarantees for the general agnostic setting, and show that the proposed algorithm can learn a model with good generalization and label complexity bounds. Experiments are conducted on two real-world datasets, and the algorithm is compared with two baselines: the Margin Algorithm and Uncertainty Sampling, a popular active learning method that uses the same number of points as the original algorithm. ","This paper proposes an active learning algorithm for binary classification tasks in the streaming setting. The proposed algorithm learns a surrogate loss over both labeled and weak-labeled points, and then uses this model to train a new surrogate loss using both the original model and the surrogate loss with weak labels. The authors provide theoretical guarantees for the general agnostic setting, and show that the proposed algorithm can learn a model with good generalization and label complexity bounds. Experiments are conducted on two real-world datasets, and the algorithm is compared with two baselines: the Margin Algorithm and Uncertainty Sampling, a popular active learning method that uses the same number of points as the original algorithm. "
8520,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"classifiers USED-FOR invariant feature representations. classifier ’s function space USED-FOR generalization. complexity USED-FOR generalization. complexity FEATURE-OF classifier ’s function space. KC FEATURE-OF functions. measure USED-FOR generalization error bounds. complexity EVALUATE-FOR measure. complexity USED-FOR generalization error bounds. Kolmogorov Growth ( KG ) HYPONYM-OF measure. Occam ’s razor USED-FOR neural networks. generalization ability EVALUATE-FOR classifiers. approach USED-FOR classifiers. generalization ability EVALUATE-FOR approach. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. KG CONJUNCTION test accuracies. test accuracies CONJUNCTION KG. Kolmogorov Growth HYPONYM-OF function complexity prior. Method are classifier, complexity theory, network - to - network regularization, N2N regularization, and cross - entropy baselines. Metric is Kolmogorov complexity ( KC ). OtherScientificTerm are classification function, network trajectory, low KG zone, and training data sizes. Generic is bounds. Task is learning. ","This paper proposes a new measure called Kolmogorov complexity (KC) to measure the complexity of a classifier’s function space for better generalization. The authors show that this measure improves generalization error bounds for any classifier, and that classifiers with invariant feature representations are more likely to learn invariant representations. They also show that the generalization ability of classifiers trained with the proposed approach can be improved when the classifier is trained with a network-to-network regularization.    The authors build on the work of [1] and [2] that shows that under certain assumptions on the classification function, the complexity can be reduced to that of a function complexity prior (Kolmogorev complexity) and that this complexity is a function of the number of samples in the training data.  The main contribution of this paper is to extend the results from [1,2,3,4] to the case that the function is invariant (i.e., that the complexity is lower bounded in the low KG zone of the function).   In particular, they show that under the assumption that the functions are invariant, the proposed measure of the complexity in the lower KG is a measure of how well the classifiers are able to generalize.  [1], [2], and [3] show that, under the same assumptions, the bounds are tighter than the ones in [1].  [2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,24,25,26,27,28,29,30,32,34,35,36,37,38,39,40,41,44,45,47,48,49,50,51,56,57,59,60,59 etc.  In addition to the bounds, the authors also propose a new approach to regularize classifiers to improve their generalization performance. The idea is based on the idea that the network trajectory can be decomposed into two parts: (1) a low-KG zone, where the low-k-th layer of the network is the one with the lowest KG, and (2) a high-kG region, where low KGs are the ones with the highest KGs.  They show that if the low kG is low enough, then the network-","This paper proposes a new measure called Kolmogorov complexity (KC) to measure the complexity of a classifier’s function space for better generalization. The authors show that this measure improves generalization error bounds for any classifier, and that classifiers with invariant feature representations are more likely to learn invariant representations. They also show that the generalization ability of classifiers trained with the proposed approach can be improved when the classifier is trained with a network-to-network regularization.    The authors build on the work of [1] and [2] that shows that under certain assumptions on the classification function, the complexity can be reduced to that of a function complexity prior (Kolmogorev complexity) and that this complexity is a function of the number of samples in the training data.  The main contribution of this paper is to extend the results from [1,2,3,4] to the case that the function is invariant (i.e., that the complexity is lower bounded in the low KG zone of the function).   In particular, they show that under the assumption that the functions are invariant, the proposed measure of the complexity in the lower KG is a measure of how well the classifiers are able to generalize.  [1], [2], and [3] show that, under the same assumptions, the bounds are tighter than the ones in [1].  [2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,24,25,26,27,28,29,30,32,34,35,36,37,38,39,40,41,44,45,47,48,49,50,51,56,57,59,60,59 etc.  In addition to the bounds, the authors also propose a new approach to regularize classifiers to improve their generalization performance. The idea is based on the idea that the network trajectory can be decomposed into two parts: (1) a low-KG zone, where the low-k-th layer of the network is the one with the lowest KG, and (2) a high-kG region, where low KGs are the ones with the highest KGs.  They show that if the low kG is low enough, then the network-"
8536,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"self - supervised methods USED-FOR image representation learning. self - supervised methods USED-FOR embedding vectors. encoders USED-FOR constant or non - informative vectors. regularizations terms USED-FOR embeddings. term CONJUNCTION term. term CONJUNCTION term. variance FEATURE-OF term. threshold FEATURE-OF variance. term HYPONYM-OF regularizations terms. term HYPONYM-OF regularizations terms. batch normalization CONJUNCTION feature - wise normalization. feature - wise normalization CONJUNCTION batch normalization. feature - wise normalization CONJUNCTION output quantization. output quantization CONJUNCTION feature - wise normalization. output quantization CONJUNCTION stop gradient. stop gradient CONJUNCTION output quantization. stop gradient CONJUNCTION memory banks. memory banks CONJUNCTION stop gradient. weight sharing CONJUNCTION batch normalization. batch normalization CONJUNCTION weight sharing. approaches USED-FOR problem. approaches COMPARE VICReg. VICReg COMPARE approaches. output quantization CONJUNCTION memory banks. memory banks CONJUNCTION output quantization. techniques USED-FOR VICReg. downstream tasks EVALUATE-FOR VICReg. stop gradient HYPONYM-OF techniques. weight sharing HYPONYM-OF techniques. memory banks HYPONYM-OF techniques. feature - wise normalization HYPONYM-OF techniques. output quantization HYPONYM-OF techniques. batch normalization HYPONYM-OF techniques. variance regularization term USED-FOR methods. Generic is method. OtherScientificTerm are collapse problem, and branches. ","This paper studies the problem of self-supervised methods for image representation learning. The authors propose a method called VICReg, which aims to avoid the collapse problem of existing self supervised methods for learning embedding vectors, where encoders are trained to produce constant or non-informative vectors. They propose three regularizations terms to improve the performance of embeddings: a term that encourages the variance of the term to be within a threshold, a term to encourage the variance to be larger than the threshold of the previous term, and a term which encourages the number of branches to be smaller than the previous one. They show that these three approaches can be combined to solve the problem, and show that VicReg outperforms existing approaches on several downstream tasks using these techniques, including weight sharing, batch normalization, feature-wise normalization and output quantization, stop gradient, and memory banks. They also show that the variance regularization term is also beneficial for these methods. ","This paper studies the problem of self-supervised methods for image representation learning. The authors propose a method called VICReg, which aims to avoid the collapse problem of existing self supervised methods for learning embedding vectors, where encoders are trained to produce constant or non-informative vectors. They propose three regularizations terms to improve the performance of embeddings: a term that encourages the variance of the term to be within a threshold, a term to encourage the variance to be larger than the threshold of the previous term, and a term which encourages the number of branches to be smaller than the previous one. They show that these three approaches can be combined to solve the problem, and show that VicReg outperforms existing approaches on several downstream tasks using these techniques, including weight sharing, batch normalization, feature-wise normalization and output quantization, stop gradient, and memory banks. They also show that the variance regularization term is also beneficial for these methods. "
8552,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"model USED-FOR RL algorithms. model USED-FOR reward. expected returns EVALUATE-FOR RL algorithms. Bayesian model USED-FOR reward. Bayesian model USED-FOR Information Directed Reward Learning ( IDRL ). prior active reward learning methods COMPARE IDRL. IDRL COMPARE prior active reward learning methods. reward model USED-FOR policy. Task are reinforcement learning ( RL ) applications, and RL setting. OtherScientificTerm are binary preferences, expert queries, and reward approximation error. Metric is information gain. Generic is it. ","This paper proposes Information Directed Reward Learning (IDRL) for reinforcement learning (RL) applications, where the goal is to learn a model of the reward that can be used to guide RL algorithms in terms of expected returns. The authors propose a Bayesian model to learn the reward for a particular RL setting, where binary preferences are available and expert queries are available. They show that the reward approximation error is a function of the information gain, and that it depends on the number of expert queries and the amount of expert information. They then propose a new reward model that is used to train a policy in this setting, and show that it outperforms prior active reward learning methods.   ","This paper proposes Information Directed Reward Learning (IDRL) for reinforcement learning (RL) applications, where the goal is to learn a model of the reward that can be used to guide RL algorithms in terms of expected returns. The authors propose a Bayesian model to learn the reward for a particular RL setting, where binary preferences are available and expert queries are available. They show that the reward approximation error is a function of the information gain, and that it depends on the number of expert queries and the amount of expert information. They then propose a new reward model that is used to train a policy in this setting, and show that it outperforms prior active reward learning methods.   "
8568,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,Deep learning USED-FOR features. Deep learning USED-FOR machine learning pipelines. features PART-OF machine learning pipelines. algorithms USED-FOR neural network parameters. deep learning USED-FOR parameters. it USED-FOR parameter prediction. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. large - scale dataset USED-FOR parameter prediction. diverse computational graphs of neural architectures FEATURE-OF large - scale dataset. DEEPNETS-1 M HYPONYM-OF large - scale dataset. ImageNet USED-FOR parameter prediction. CIFAR-10 USED-FOR parameter prediction. DEEPNETS-1 M HYPONYM-OF diverse computational graphs of neural architectures. graph neural networks USED-FOR hypernetwork. accuracy EVALUATE-FOR it. CIFAR-10 EVALUATE-FOR it. ImageNet EVALUATE-FOR networks. top-5 accuracy EVALUATE-FOR networks. task CONJUNCTION model. model CONJUNCTION task. model USED-FOR neural architectures. OtherScientificTerm is CPU. Method is ResNet-50. Task is training networks. ,"This paper proposes a new large-scale dataset called DEEPNETS-1M, which is a collection of diverse computational graphs of neural architectures. Deep learning is applied to these features in machine learning pipelines, and the authors propose two algorithms for learning neural network parameters. The authors show that deep learning is able to learn the parameters of a hypernetwork using graph neural networks, and that it can be used for parameter prediction on CIFAR-10 and ImageNet. They also show that the hypernetwork can be trained on top of ResNet-50, and it can achieve top-5 accuracy on ImageNet with a fraction of the CPU.   The authors also demonstrate that networks trained with a similar number of parameters are able to achieve the same or higher accuracy than training networks that are trained with fewer parameters. This shows that neural architectures trained on the same task and the same model can achieve similar performance. ","This paper proposes a new large-scale dataset called DEEPNETS-1M, which is a collection of diverse computational graphs of neural architectures. Deep learning is applied to these features in machine learning pipelines, and the authors propose two algorithms for learning neural network parameters. The authors show that deep learning is able to learn the parameters of a hypernetwork using graph neural networks, and that it can be used for parameter prediction on CIFAR-10 and ImageNet. They also show that the hypernetwork can be trained on top of ResNet-50, and it can achieve top-5 accuracy on ImageNet with a fraction of the CPU.   The authors also demonstrate that networks trained with a similar number of parameters are able to achieve the same or higher accuracy than training networks that are trained with fewer parameters. This shows that neural architectures trained on the same task and the same model can achieve similar performance. "
8584,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"distortion EVALUATE-FOR estimator. perception constraint FEATURE-OF minimal distortion. closed form expression USED-FOR distortion - perception ( DP ) function. mean squared - error ( MSE ) distortion CONJUNCTION Wasserstein-2 perception index. Wasserstein-2 perception index CONJUNCTION mean squared - error ( MSE ) distortion. closed form expression USED-FOR Wasserstein-2 perception index. mean squared - error ( MSE ) distortion EVALUATE-FOR distortion - perception ( DP ) function. closed form expression USED-FOR estimators. closed form expression USED-FOR Gaussian setting. global MSE minimizer CONJUNCTION minimizer. minimizer CONJUNCTION global MSE minimizer. global MSE minimizer CONJUNCTION MSE. MSE CONJUNCTION global MSE minimizer. minimizer FEATURE-OF MSE. perfect perceptual quality constraint FEATURE-OF minimizer. perfect perceptual quality constraint FEATURE-OF MSE. minimizer HYPONYM-OF tradeoff. global MSE minimizer HYPONYM-OF tradeoff. estimators USED-FOR estimators. stochastic transformation of the former USED-FOR latter. Metric are perception - distortion tradeoff, fidelity, and perceptual quality. Task is image restoration. OtherScientificTerm are statistics of natural images, perception - distortion plane, DP function, DP curve, and geodesic in Wasserstein space. ","This paper studies the perception-distortion tradeoff between the statistics of natural images and the distortion-perception (DP) function. The authors propose an estimator for the distortion of the DP function, which is a closed form expression of the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. They show that under the perception constraint on minimal distortion, the estimator is optimal in terms of the amount of distortion. The paper also shows that under a Gaussian setting, the closed-form expression can be used to derive two estimators of the distortion - the global MSE minimizer and the minimizer of the MSE under the perfect perceptual quality constraint. They also show that the latter is a stochastic transformation of the former.    The paper is well-written and well-motivated. The main contribution of the paper is to study the tradeoff of the two types of estimators for the two kinds of DP function. In particular, the paper shows that for the case of image restoration, if the DP curve is a geodesic in the space of images, then the fidelity of the resulting image is a function of the distance between the distortion in the DP-plane and the one in the perception - distortion plane. In contrast, the authors show that in the case where the DP is a convex function, the perceptual quality of the image is not affected by the distortion.","This paper studies the perception-distortion tradeoff between the statistics of natural images and the distortion-perception (DP) function. The authors propose an estimator for the distortion of the DP function, which is a closed form expression of the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. They show that under the perception constraint on minimal distortion, the estimator is optimal in terms of the amount of distortion. The paper also shows that under a Gaussian setting, the closed-form expression can be used to derive two estimators of the distortion - the global MSE minimizer and the minimizer of the MSE under the perfect perceptual quality constraint. They also show that the latter is a stochastic transformation of the former.    The paper is well-written and well-motivated. The main contribution of the paper is to study the tradeoff of the two types of estimators for the two kinds of DP function. In particular, the paper shows that for the case of image restoration, if the DP curve is a geodesic in the space of images, then the fidelity of the resulting image is a function of the distance between the distortion in the DP-plane and the one in the perception - distortion plane. In contrast, the authors show that in the case where the DP is a convex function, the perceptual quality of the image is not affected by the distortion."
8600,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,textual features CONJUNCTION neighbourhood information. neighbourhood information CONJUNCTION textual features. low - dimensional embeddings USED-FOR nodes. textual features USED-FOR low - dimensional embeddings. neighbourhood information USED-FOR low - dimensional embeddings. pretrained language models CONJUNCTION graph neural networks. graph neural networks CONJUNCTION pretrained language models. graph neural networks USED-FOR techniques. textual features FEATURE-OF nodes. language models USED-FOR textual features. graph neural networks USED-FOR textual embeddings. layerwise GNN components CONJUNCTION transformer blocks of language models. transformer blocks of language models CONJUNCTION layerwise GNN components. layerwise GNN components PART-OF GraphFormers. text encoding CONJUNCTION graph aggregation. graph aggregation CONJUNCTION text encoding. graph aggregation PART-OF iterative workflow. text encoding PART-OF iterative workflow. manipulated data CONJUNCTION original data. original data CONJUNCTION manipulated data. model PART-OF progressive learning strategy. manipulated data USED-FOR model. original data USED-FOR model. GraphFormers COMPARE SOTA baselines. SOTA baselines COMPARE GraphFormers. large - scale benchmark datasets EVALUATE-FOR GraphFormers. running efficiency EVALUATE-FOR SOTA baselines. running efficiency EVALUATE-FOR GraphFormers. OtherScientificTerm is textual graph. Method is cascaded model architecture. Generic is architecture. Task is independent modeling of textual features. ,"This paper proposes a cascaded model architecture for learning low-dimensional embeddings of nodes in a textual graph. The authors propose two techniques based on existing pretrained language models and graph neural networks to leverage the textual features and neighbourhood information to learn low-dimensionality of nodes. The proposed GraphFormers consists of layerwise GNN components and transformer blocks of language models for learning textual features from language models.  The authors also propose an iterative workflow that combines text encoding and graph aggregation in a progressive learning strategy, where the model is trained on manipulated data as well as original data.  Experiments on several large-scale benchmark datasets show that Graphformers outperform SOTA baselines in terms of running efficiency, and the authors also show that the proposed architecture can be used for independent modeling of textual features. ","This paper proposes a cascaded model architecture for learning low-dimensional embeddings of nodes in a textual graph. The authors propose two techniques based on existing pretrained language models and graph neural networks to leverage the textual features and neighbourhood information to learn low-dimensionality of nodes. The proposed GraphFormers consists of layerwise GNN components and transformer blocks of language models for learning textual features from language models.  The authors also propose an iterative workflow that combines text encoding and graph aggregation in a progressive learning strategy, where the model is trained on manipulated data as well as original data.  Experiments on several large-scale benchmark datasets show that Graphformers outperform SOTA baselines in terms of running efficiency, and the authors also show that the proposed architecture can be used for independent modeling of textual features. "
8616,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,algorithms USED-FOR learning tasks. userlevel differential privacy constraints FEATURE-OF learning tasks. empirical risk minimization CONJUNCTION stochastic convex optimization. stochastic convex optimization CONJUNCTION empirical risk minimization. stochastic convex optimization CONJUNCTION learning hypothesis classes. learning hypothesis classes CONJUNCTION stochastic convex optimization. high - dimensional mean estimation CONJUNCTION empirical risk minimization. empirical risk minimization CONJUNCTION high - dimensional mean estimation. smooth losses FEATURE-OF empirical risk minimization. finite metric entropy FEATURE-OF learning hypothesis classes. O(1 / n ) rate FEATURE-OF privacy cost. mean estimation CONJUNCTION stochastic convex optimization. stochastic convex optimization CONJUNCTION mean estimation. algorithms USED-FOR mean estimation. algorithms USED-FOR stochastic convex optimization. minimax optimality FEATURE-OF algorithms. lower bounds USED-FOR minimax optimality. lower bounds USED-FOR algorithms. techniques USED-FOR private mean estimation. techniques USED-FOR arbitrary dimension. error scaling FEATURE-OF techniques. private mean estimation USED-FOR algorithms. arbitrary dimension FEATURE-OF private mean estimation. techniques USED-FOR algorithms. Method is user - level DP. OtherScientificTerm is information leaks. ,"This paper studies algorithms for learning tasks with userlevel differential privacy constraints under the setting of user-level DP. In particular, the authors consider high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy. They provide lower bounds on the privacy cost with an O(1/n) rate, and show that algorithms for mean estimation with mean estimation in mean estimation and stochastically convex optimality can achieve minimax optimality. They also provide techniques for private mean estimation of arbitrary dimension and error scaling. ","This paper studies algorithms for learning tasks with userlevel differential privacy constraints under the setting of user-level DP. In particular, the authors consider high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy. They provide lower bounds on the privacy cost with an O(1/n) rate, and show that algorithms for mean estimation with mean estimation in mean estimation and stochastically convex optimality can achieve minimax optimality. They also provide techniques for private mean estimation of arbitrary dimension and error scaling. "
8632,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"they USED-FOR deep learning. infinite width / channel limit FEATURE-OF Deep neural networks ( DNNs ). deep learning USED-FOR finite DNNs. self - consistent Gaussian Process theory USED-FOR finite - DNN and feature learning effects. noisy gradient descent USED-FOR DNNs. this USED-FOR toy model. feature learning regime CONJUNCTION lazy learning regime. lazy learning regime CONJUNCTION feature learning regime. CIFAR-10 USED-FOR Myrtle5 CNN. self - consistent theory USED-FOR finite - DNN effects. self - consistent theory USED-FOR feature learning. feature learning HYPONYM-OF finite - DNN effects. Method is Gaussian Processes ( GPs ). Generic are model, and theory. ","Deep neural networks (DNNs) have been shown to have an infinite width/channel limit in the infinite-width limit. Deep neural networks have been considered to be a special case of Gaussian Processes (GPs) and they are widely used in deep learning. However, deep learning for finite DNNs has not been well studied. This paper studies the finite-DNN and feature learning effects of deep learning based on the self-consistent Gaussian process theory. The authors show that under certain assumptions on the model, the model can be seen as a function of the number of layers and the number (and depth) of data points. They also show that the feature learning regime and the lazy learning regime can be viewed as special cases of the theory. Finally, the authors propose a toy model based on this, which is trained using noisy gradient descent, and show that this leads to an improved performance on CIFAR-10 for a Myrtle5 CNN. The paper also shows that the self -consistent theory can be applied to the two types of finite-dNN effects (feature learning and lazy learning). ","Deep neural networks (DNNs) have been shown to have an infinite width/channel limit in the infinite-width limit. Deep neural networks have been considered to be a special case of Gaussian Processes (GPs) and they are widely used in deep learning. However, deep learning for finite DNNs has not been well studied. This paper studies the finite-DNN and feature learning effects of deep learning based on the self-consistent Gaussian process theory. The authors show that under certain assumptions on the model, the model can be seen as a function of the number of layers and the number (and depth) of data points. They also show that the feature learning regime and the lazy learning regime can be viewed as special cases of the theory. Finally, the authors propose a toy model based on this, which is trained using noisy gradient descent, and show that this leads to an improved performance on CIFAR-10 for a Myrtle5 CNN. The paper also shows that the self -consistent theory can be applied to the two types of finite-dNN effects (feature learning and lazy learning). "
8648,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"inductive biases USED-FOR compositional communication. training framework USED-FOR inductive biases. signaling games USED-FOR compositionality. noise levels USED-FOR compositionality. conflict count CONJUNCTION context independence. context independence CONJUNCTION conflict count. topographical similarity CONJUNCTION conflict count. conflict count CONJUNCTION topographical similarity. context independence HYPONYM-OF compositionality metrics. topographical similarity HYPONYM-OF compositionality metrics. conflict count HYPONYM-OF compositionality metrics. Task is Communication. OtherScientificTerm are complex signals, and noisy channel. Generic is model. ","This paper proposes a new training framework to learn inductive biases for compositional communication. Communication is a challenging problem in which complex signals are transmitted across multiple channels. The authors propose a training framework that learns to learn the compositionality of a noisy channel by learning a model that is robust to noise levels. They show that the learned compositionality can be used to improve the performance of signaling games. They also propose three new compositionality metrics: topographical similarity, conflict count, and context independence.","This paper proposes a new training framework to learn inductive biases for compositional communication. Communication is a challenging problem in which complex signals are transmitted across multiple channels. The authors propose a training framework that learns to learn the compositionality of a noisy channel by learning a model that is robust to noise levels. They show that the learned compositionality can be used to improve the performance of signaling games. They also propose three new compositionality metrics: topographical similarity, conflict count, and context independence."
8664,SP:9d326254d77a188baf5bde39229c09b3966b5418,"ResMLP HYPONYM-OF architecture. multi - layer perceptrons USED-FOR image classification. architecture USED-FOR image classification. multi - layer perceptrons USED-FOR ResMLP. multi - layer perceptrons USED-FOR architecture. It HYPONYM-OF residual network. heavy data - augmentation CONJUNCTION distillation. distillation CONJUNCTION heavy data - augmentation. training strategy USED-FOR it. distillation USED-FOR training strategy. heavy data - augmentation USED-FOR training strategy. ImageNet EVALUATE-FOR it. self - supervised setup USED-FOR ResMLP models. labelled dataset USED-FOR priors. model USED-FOR machine translation. Timm library CONJUNCTION pre - trained models. pre - trained models CONJUNCTION Timm library. OtherScientificTerm are image patches, and channels. Method is two - layer feed - forward network. ","This paper proposes a new architecture called ResMLP, which uses multi-layer perceptrons to perform image classification. It is a residual network, where each layer is a two-layer feed-forward network. The training strategy is based on heavy data-augmentation and distillation, and it is shown to perform well on ImageNet under a self-supervised setup. The model is also applied to machine translation, where the image patches are split into channels, and the priors are trained on a labelled dataset. Experiments are conducted on the Timm library and pre-trained models. ","This paper proposes a new architecture called ResMLP, which uses multi-layer perceptrons to perform image classification. It is a residual network, where each layer is a two-layer feed-forward network. The training strategy is based on heavy data-augmentation and distillation, and it is shown to perform well on ImageNet under a self-supervised setup. The model is also applied to machine translation, where the image patches are split into channels, and the priors are trained on a labelled dataset. Experiments are conducted on the Timm library and pre-trained models. "
8680,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"regret guarantees USED-FOR geometric problem of contextual search. multiclass classification CONJUNCTION binary classification. binary classification CONJUNCTION multiclass classification. Task is multi - class classification. Metric is misclassification rate. Method are nearest neighbor partition, and reduction technique. OtherScientificTerm is Euclidean distance. ","This paper considers the problem of multi-class classification, where the goal is to minimize the misclassification rate. The authors consider the geometric problem of contextual search with regret guarantees. They propose a reduction technique that is based on nearest neighbor partition, and show that the Euclidean distance between the nearest neighbors of the training data points is a special case of this reduction technique. They also provide theoretical guarantees for multiclass classification and binary classification. ","This paper considers the problem of multi-class classification, where the goal is to minimize the misclassification rate. The authors consider the geometric problem of contextual search with regret guarantees. They propose a reduction technique that is based on nearest neighbor partition, and show that the Euclidean distance between the nearest neighbors of the training data points is a special case of this reduction technique. They also provide theoretical guarantees for multiclass classification and binary classification. "
8696,SP:5c0114535065d5125349f00bafdbccc911461ede,"Methods USED-FOR Visual Question Anwering ( VQA ). dataset biases COMPARE reasoning. reasoning COMPARE dataset biases. attention layers PART-OF VQA model. attention layers FEATURE-OF reasoning patterns. perfect ( oracle ) visual inputs USED-FOR they. method USED-FOR knowledge transfer. regularization term PART-OF loss function. regularization term USED-FOR method. sample complexity EVALUATE-FOR program prediction. PAC - learning USED-FOR theoretical analysis. GQA dataset EVALUATE-FOR approach. Task is generalization. Method is deep neural networks. Generic are models, and transfer. OtherScientificTerm is reasoning operations. ","This paper studies the problem of generalization in the context of Visual Question Anwering (VQA) where dataset biases are more important than reasoning. In particular, the authors consider deep neural networks and argue that they are biased towards perfect (oracle) visual inputs, and that they do not generalize well to other datasets. To address this problem, they propose a method to learn reasoning patterns in the attention layers of a VQA model that are transferable across different datasets. The proposed method is based on a regularization term in the loss function that encourages the model to learn knowledge transfer between different models. The authors show that the proposed approach improves the sample complexity of program prediction in terms of the number of reasoning operations. The approach is evaluated on the GQA dataset, and the theoretical analysis is performed using PAC-learning.","This paper studies the problem of generalization in the context of Visual Question Anwering (VQA) where dataset biases are more important than reasoning. In particular, the authors consider deep neural networks and argue that they are biased towards perfect (oracle) visual inputs, and that they do not generalize well to other datasets. To address this problem, they propose a method to learn reasoning patterns in the attention layers of a VQA model that are transferable across different datasets. The proposed method is based on a regularization term in the loss function that encourages the model to learn knowledge transfer between different models. The authors show that the proposed approach improves the sample complexity of program prediction in terms of the number of reasoning operations. The approach is evaluated on the GQA dataset, and the theoretical analysis is performed using PAC-learning."
8712,SP:40fd96105e77063de4a07d4b36fe19385434c533,"neurons of fixed precision FEATURE-OF dynamically growing memory module. 54 - neuron bounded - precision RNN USED-FOR Universal Turing Machine. growing memory modules PART-OF 54 - neuron bounded - precision RNN. Turing completeness FEATURE-OF unbounded - precision and boundedprecision RNNs. Method are recurrent neural networks ( RNNs ), RNNs, memory module, and stack - augmented RNNs. OtherScientificTerm are unbounded precision, simulated machine ’s time, and memory size. Metric is time complexity. ","This paper considers the problem of training recurrent neural networks (RNNs) with unbounded precision and bounded precision. In particular, the authors consider RNNs where the memory module is unbounded and bounded. The authors show that a dynamically growing memory module with only a few neurons of fixed precision can be trained in a time-efficient way. They prove that a 54-neuron bounded-precision RNN with growing memory modules is equivalent to a Universal Turing Machine with a fixed number of neurons. They also prove Turing completeness for unbounded -precision and boundedprecision rNNs. Finally, they show that the simulated machine’s time is bounded by a factor that depends on the size of the memory.    The authors also show that stack-augmented RNN (i.e. stack of stacked layers) are equivalent to unbounded RNN and bounded RNN, and that the time complexity can be bounded by the number of layers and the memory size. ","This paper considers the problem of training recurrent neural networks (RNNs) with unbounded precision and bounded precision. In particular, the authors consider RNNs where the memory module is unbounded and bounded. The authors show that a dynamically growing memory module with only a few neurons of fixed precision can be trained in a time-efficient way. They prove that a 54-neuron bounded-precision RNN with growing memory modules is equivalent to a Universal Turing Machine with a fixed number of neurons. They also prove Turing completeness for unbounded -precision and boundedprecision rNNs. Finally, they show that the simulated machine’s time is bounded by a factor that depends on the size of the memory.    The authors also show that stack-augmented RNN (i.e. stack of stacked layers) are equivalent to unbounded RNN and bounded RNN, and that the time complexity can be bounded by the number of layers and the memory size. "
8728,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"Estimating the data uncertainty USED-FOR regression tasks. quantile function USED-FOR Estimating the data uncertainty. vanilla algorithm USED-FOR quantiles. uncertainty estimation algorithms USED-FOR quantiles. vanilla setting USED-FOR realizable linear quantile function. under - coverage bias FEATURE-OF quantile regression. α CONJUNCTION d / n. d / n CONJUNCTION α. quantile regression USED-FOR α - quantile. highdimensional parameter estimation error USED-FOR under - coverage bias. sample size CONJUNCTION model capacity. model capacity CONJUNCTION sample size. simulated and real data EVALUATE-FOR theory. model capacity FEATURE-OF under - coverage bias. OtherScientificTerm are asymptotic guarantees, coverage level, and noise distribution. ","Estimating the data uncertainty for regression tasks using a quantile function is an important problem. This paper provides asymptotic guarantees for a vanilla algorithm for estimating the quantiles. The authors show that under-covering bias in quantile regression is caused by the highdimensional parameter estimation error, which is a result of the coverage level and the noise distribution of the data. The paper also shows that the under-coverage bias is due to the high variance of the uncertainty estimation algorithms for quantiles in the vanilla setting. The main contribution of this paper is to provide a realizable linear quantile loss for the case that the coverage of the dataset is low and the data distribution is high dimensional.   The authors also provide a theoretical analysis of the α-quantile of the true data distribution under the setting where the coverage is high-dimensional. They show that the true quantile is a function of the number of samples, the sample size, and the model capacity. They also show that if the data is low-dimensional, then the true α-quadratic of the quantile can be approximated by the true Q-function.  The theory is verified on simulated and real data. ","Estimating the data uncertainty for regression tasks using a quantile function is an important problem. This paper provides asymptotic guarantees for a vanilla algorithm for estimating the quantiles. The authors show that under-covering bias in quantile regression is caused by the highdimensional parameter estimation error, which is a result of the coverage level and the noise distribution of the data. The paper also shows that the under-coverage bias is due to the high variance of the uncertainty estimation algorithms for quantiles in the vanilla setting. The main contribution of this paper is to provide a realizable linear quantile loss for the case that the coverage of the dataset is low and the data distribution is high dimensional.   The authors also provide a theoretical analysis of the α-quantile of the true data distribution under the setting where the coverage is high-dimensional. They show that the true quantile is a function of the number of samples, the sample size, and the model capacity. They also show that if the data is low-dimensional, then the true α-quadratic of the quantile can be approximated by the true Q-function.  The theory is verified on simulated and real data. "
8744,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"strict memory budget USED-FOR classifiers. learning USED-FOR incremental phase. static and ad hoc strategy USED-FOR memory allocation. dynamic memory management strategy USED-FOR incremental phases. incremental phases CONJUNCTION object classes. object classes CONJUNCTION incremental phases. dynamic memory management strategy USED-FOR object classes. reinforcement learning USED-FOR reinforced memory management ( RMM ). it USED-FOR tasks. RMM USED-FOR pseudo CIL tasks. policy function USED-FOR pseudo CIL tasks. policy function USED-FOR RMM. tasks HYPONYM-OF pseudo CIL tasks. it USED-FOR replaying - based CIL method. it USED-FOR memory management. LUCIR+AANets CONJUNCTION POD+AANets. POD+AANets CONJUNCTION LUCIR+AANets. RMM USED-FOR top - performing baselines. POD+AANets HYPONYM-OF top - performing baselines. LUCIR+AANets HYPONYM-OF top - performing baselines. ImageNet - Subset CONJUNCTION ImageNet - Full. ImageNet - Full CONJUNCTION ImageNet - Subset. CIFAR-100 CONJUNCTION ImageNet - Subset. ImageNet - Subset CONJUNCTION CIFAR-100. CIFAR-100 HYPONYM-OF benchmarks. ImageNet - Subset HYPONYM-OF benchmarks. ImageNet - Full HYPONYM-OF benchmarks. Method are Class - Incremental Learning ( CIL ), and RMM training. Task are replaying, and CIL. ","This paper proposes a reinforcement learning-based method for the problem of incremental learning in the presence of a strict memory budget. The authors propose a method called Class-incremental Learning (CIL) that uses reinforcement learning (RL) to learn a policy for each incremental phase of the learning, and then uses a dynamic memory management strategy to re-allocate the memory allocation during the replaying phase. They show that this approach is able to achieve state-of-the-art performance on CIFAR-10 and ImageNet-100 datasets.   ","This paper proposes a reinforcement learning-based method for the problem of incremental learning in the presence of a strict memory budget. The authors propose a method called Class-incremental Learning (CIL) that uses reinforcement learning (RL) to learn a policy for each incremental phase of the learning, and then uses a dynamic memory management strategy to re-allocate the memory allocation during the replaying phase. They show that this approach is able to achieve state-of-the-art performance on CIFAR-10 and ImageNet-100 datasets.   "
8760,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"it USED-FOR speeding up stochastic gradient descent ( SGD ). Ω ( √ T ) communications USED-FOR local gradient steps. Ω ( √ T ) communications USED-FOR it. linear speed - up EVALUATE-FOR √ N or N communications. optimal convergence rate EVALUATE-FOR one - shot averaging. Method are stochastic gradient descent ( SGD ), Local SGD method, Local SGD, and Local SGD scheme. OtherScientificTerm are SGD steps, stochastic gradients, communication, parallelism, Ω(N ) communications, and twice differentiability. Task is linear reduction in the variance. Metric is error. ","This paper studies the problem of speeding up the convergence of stochastic gradient descent (SGD) when the number of SGD steps is large. The authors propose a Local SGD method, where the local gradient steps are computed using Ω(√T) communications, and it is used to speed up the speeding up of the SGD (and thus the convergence rate). The authors show that it is possible to achieve a linear speed-up using only Ω (√ T) communications for a small fraction of the local gradients. They also show that the optimal convergence rate for one-shot averaging is O(1/\sqrt{T}) with linear reduction in the variance. The main contribution of the paper is that the communication between the local and global scales can be reduced to O(N/T) in the case of parallelism.   The authors also provide a theoretical analysis that shows that the local SGD scheme converges to the optimal solution when the communication is twice differentiable, which is the case for the case where the communication scales linearly in the dimension of the data.  The paper also shows that if the communication scale is small enough, the error can be minimized.  Finally, the authors provide some numerical experiments to verify the theoretical results. The experiments are conducted on two datasets, where they show that they can achieve linear speedup for either the Κ N or N communications, but not both.  In addition, they also provide an analysis of the effect of twice differentiability. ","This paper studies the problem of speeding up the convergence of stochastic gradient descent (SGD) when the number of SGD steps is large. The authors propose a Local SGD method, where the local gradient steps are computed using Ω(√T) communications, and it is used to speed up the speeding up of the SGD (and thus the convergence rate). The authors show that it is possible to achieve a linear speed-up using only Ω (√ T) communications for a small fraction of the local gradients. They also show that the optimal convergence rate for one-shot averaging is O(1/\sqrt{T}) with linear reduction in the variance. The main contribution of the paper is that the communication between the local and global scales can be reduced to O(N/T) in the case of parallelism.   The authors also provide a theoretical analysis that shows that the local SGD scheme converges to the optimal solution when the communication is twice differentiable, which is the case for the case where the communication scales linearly in the dimension of the data.  The paper also shows that if the communication scale is small enough, the error can be minimized.  Finally, the authors provide some numerical experiments to verify the theoretical results. The experiments are conducted on two datasets, where they show that they can achieve linear speedup for either the Κ N or N communications, but not both.  In addition, they also provide an analysis of the effect of twice differentiability. "
8776,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"Online Lazy Gradient Descent USED-FOR optimisation. strongly convex domain FEATURE-OF optimisation. O ( √ N ) regret EVALUATE-FOR algorithm. expected regret EVALUATE-FOR it. pseudo - regret CONJUNCTION expected regret. expected regret CONJUNCTION pseudo - regret. order bounds FEATURE-OF strongly convex domains. order bounds FEATURE-OF expected regret. order bounds FEATURE-OF pseudo - regret. OtherScientificTerm are adversarial opponents, i.i.d opponents, O(logN ) bounds, and simplex. Method is metaalgorithm. ","This paper studies Online Lazy Gradient Descent for optimisation in the strongly convex domain, where the adversarial opponents are i.i.d opponents. The authors show that the algorithm achieves O(N) regret in the simplex and O(log N) regret for the convex case. They also prove that the metaalgorithm obtains O(O(logN) bounds for the case where the opponent is non-convex. The paper also shows that it obtains a lower bound on the expected regret. Finally, the authors provide order bounds on the pseudo-regret and expected regret of the algorithm. ","This paper studies Online Lazy Gradient Descent for optimisation in the strongly convex domain, where the adversarial opponents are i.i.d opponents. The authors show that the algorithm achieves O(N) regret in the simplex and O(log N) regret for the convex case. They also prove that the metaalgorithm obtains O(O(logN) bounds for the case where the opponent is non-convex. The paper also shows that it obtains a lower bound on the expected regret. Finally, the authors provide order bounds on the pseudo-regret and expected regret of the algorithm. "
8792,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"Affine - Invariant ( AI ) geometry USED-FOR Riemannian optimization. Bures - Wasserstein ( BW ) geometry USED-FOR Riemannian optimization. Bures - Wasserstein ( BW ) geometry COMPARE Affine - Invariant ( AI ) geometry. Affine - Invariant ( AI ) geometry COMPARE Bures - Wasserstein ( BW ) geometry. symmetric positive definite ( SPD ) matrix manifold FEATURE-OF Riemannian optimization. linear dependence FEATURE-OF BW metric. BW metric USED-FOR Riemannian optimization problems. ill - conditioned SPD matrices USED-FOR Riemannian optimization problems. non - negative curvature FEATURE-OF BW geometry. OtherScientificTerm are SPD matrices, AI metric, and non - positively curved AI geometry. Metric is convergence rates. Method are cost functions, and AI geometry. Generic is applications. ","This paper studies the Affine-Invariant (AI) geometry of the Bures-Wasserstein (BW) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. The authors show that the affine-invariance of the BW geometry (i.e. the non-negative curvature of the SPD matrices) matches the affinities of Affine (AI)-invariant and Bures (Bures) manifolds, and the convergence rates of the two have linear dependence on the cost functions. They also show that in practice, the BW metric has a linear dependence in terms of the cost function.   The authors also provide a theoretical analysis that shows that the BW-AI geometry has non-negligible curvature, which is a result of the existence of a non-positively curved version of the AI metric.  Finally, the authors provide an application of the proposed BW metric to the problem of ill-conditioned optimization of Riemagnian optimization problems on the manifold with non-zero curvature.  In particular, they show that for the case where the AI geometry is positively curved, the corresponding non-positively curved AI geometry can be used to solve the problem.  The paper is well-written and well-motivated, and is well motivated. However, there are a few issues that need to be addressed in order for the paper to be accepted as a contribution to the literature. The paper suffers from a lack of clarity and clarity, and there are some issues with the presentation of the results. ","This paper studies the Affine-Invariant (AI) geometry of the Bures-Wasserstein (BW) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. The authors show that the affine-invariance of the BW geometry (i.e. the non-negative curvature of the SPD matrices) matches the affinities of Affine (AI)-invariant and Bures (Bures) manifolds, and the convergence rates of the two have linear dependence on the cost functions. They also show that in practice, the BW metric has a linear dependence in terms of the cost function.   The authors also provide a theoretical analysis that shows that the BW-AI geometry has non-negligible curvature, which is a result of the existence of a non-positively curved version of the AI metric.  Finally, the authors provide an application of the proposed BW metric to the problem of ill-conditioned optimization of Riemagnian optimization problems on the manifold with non-zero curvature.  In particular, they show that for the case where the AI geometry is positively curved, the corresponding non-positively curved AI geometry can be used to solve the problem.  The paper is well-written and well-motivated, and is well motivated. However, there are a few issues that need to be addressed in order for the paper to be accepted as a contribution to the literature. The paper suffers from a lack of clarity and clarity, and there are some issues with the presentation of the results. "
8808,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"Dynaboard HYPONYM-OF evaluation - as - a - service framework. evaluation - as - a - service framework USED-FOR holistic model comparison. Dynaboard CONJUNCTION Dynabench platform. Dynabench platform CONJUNCTION Dynaboard. platform USED-FOR NLP models. reproducibility CONJUNCTION accessibility. accessibility CONJUNCTION reproducibility. accessibility CONJUNCTION backwards compatibility. backwards compatibility CONJUNCTION accessibility. benchmarking USED-FOR NLP. memory use CONJUNCTION throughput. throughput CONJUNCTION memory use. throughput CONJUNCTION robustness. robustness CONJUNCTION throughput. robustness HYPONYM-OF metrics. memory use HYPONYM-OF metrics. throughput HYPONYM-OF metrics. Dynascore HYPONYM-OF utility - based aggregation. NLP models COMPARE benchmarks. benchmarks COMPARE NLP models. OtherScientificTerm is selfreported metrics. Generic are paradigm, models, and task. Material is leaderboards. ","This paper introduces Dynaboard, a new evaluation-as-a-service framework for holistic model comparison based on selfreported metrics. The authors propose a new paradigm for benchmarking in NLP, which is based on the Dynabench platform. The platform is designed to evaluate the performance of NLP models on a set of benchmarking tasks, including reproducibility, accessibility, backwards compatibility, robustness, and other metrics such as memory use, throughput, and robustness. The paper also proposes a utility-based aggregation, called Dynascore, to compare different models on the same task. Experiments are conducted on leaderboards, and the authors show that the proposed approach outperforms the state-of-the-art in terms of performance on a number of benchmarks.","This paper introduces Dynaboard, a new evaluation-as-a-service framework for holistic model comparison based on selfreported metrics. The authors propose a new paradigm for benchmarking in NLP, which is based on the Dynabench platform. The platform is designed to evaluate the performance of NLP models on a set of benchmarking tasks, including reproducibility, accessibility, backwards compatibility, robustness, and other metrics such as memory use, throughput, and robustness. The paper also proposes a utility-based aggregation, called Dynascore, to compare different models on the same task. Experiments are conducted on leaderboards, and the authors show that the proposed approach outperforms the state-of-the-art in terms of performance on a number of benchmarks."
8824,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"filmmaking CONJUNCTION video production. video production CONJUNCTION filmmaking. re - recording actors ’ dialogues USED-FOR filmmaking. re - recording actors ’ dialogues USED-FOR video production. re - recording actors ’ dialogues USED-FOR Dubbing. Neural Dubber HYPONYM-OF neural network model. Neural Dubber USED-FOR automatic video dubbing ( AVD ) task. neural network model USED-FOR automatic video dubbing ( AVD ) task. Neural Dubber USED-FOR synthesizing human speech. synthesizing human speech HYPONYM-OF automatic video dubbing ( AVD ) task. lip movement USED-FOR Neural Dubber. Neural Dubber USED-FOR speech. timbre FEATURE-OF Neural Dubber. timbre FEATURE-OF speech. chemistry lecture single - speaker dataset CONJUNCTION LRS2 multi - speaker dataset. LRS2 multi - speaker dataset CONJUNCTION chemistry lecture single - speaker dataset. Neural Dubber COMPARE TTS models. TTS models COMPARE Neural Dubber. chemistry lecture single - speaker dataset EVALUATE-FOR Neural Dubber. LRS2 multi - speaker dataset EVALUATE-FOR Neural Dubber. Neural Dubber USED-FOR speech audios. speech audios COMPARE TTS models. TTS models COMPARE speech audios. speech quality EVALUATE-FOR TTS models. speech quality EVALUATE-FOR Neural Dubber. Neural Dubber USED-FOR high - fidelity speech. qualitative and quantitative evaluations EVALUATE-FOR Neural Dubber. Generic is It. Material are pre - recorded videos, and multi - speaker setting. ","This paper proposes a neural network model called Neural Dubber, which is an automatic video dubbing (AVD) task, i.e., synthesizing human speech. Dubbing is done by re-recording actors’ dialogues from pre-recorded videos, which are then used for re-recorded audio for filmmaking and video production. The authors show that the neural Dubber is capable of synthesizing speech with varying timbre and timbre of different speakers, and that it can be used for synthesizing high-fidelity speech.    The authors also show that, in the multi-speaker setting, the neural dubber is able to synthesize speech with high-quality lip movement.  Experiments are conducted on the chemistry lecture single speaker dataset, the LRS2 multi-Speaker dataset, and on the speech audios, and show that Neural Dubbers outperforms TTS models in terms of speech quality, and can also produce high-idelity speech with different timbre. The paper also provides qualitative and quantitative evaluations to show the effectiveness of Neural DubBER. ","This paper proposes a neural network model called Neural Dubber, which is an automatic video dubbing (AVD) task, i.e., synthesizing human speech. Dubbing is done by re-recording actors’ dialogues from pre-recorded videos, which are then used for re-recorded audio for filmmaking and video production. The authors show that the neural Dubber is capable of synthesizing speech with varying timbre and timbre of different speakers, and that it can be used for synthesizing high-fidelity speech.    The authors also show that, in the multi-speaker setting, the neural dubber is able to synthesize speech with high-quality lip movement.  Experiments are conducted on the chemistry lecture single speaker dataset, the LRS2 multi-Speaker dataset, and on the speech audios, and show that Neural Dubbers outperforms TTS models in terms of speech quality, and can also produce high-idelity speech with different timbre. The paper also provides qualitative and quantitative evaluations to show the effectiveness of Neural DubBER. "
8840,SP:24ea12428bd675459f0509aa7cee821fa236382e,"Federated learning USED-FOR healthcare sector. neural network training USED-FOR COVID-19 diagnosis. chest X - ray ( CXR ) images USED-FOR neural network training. chest X - ray ( CXR ) images USED-FOR COVID-19 diagnosis. Vision Transformer USED-FOR split learning. Vision Transformer HYPONYM-OF deep learning architecture. decomposable configuration FEATURE-OF deep learning architecture. framework COMPARE data - centralized training. data - centralized training COMPARE framework. CXR datasets USED-FOR framework. CXR datasets USED-FOR non - independent and identically distributed data distribution. framework CONJUNCTION heterogeneous multi - task clients. heterogeneous multi - task clients CONJUNCTION framework. Transformer USED-FOR collaborative learning. collaborative learning USED-FOR medical imaging. Transformer USED-FOR medical imaging. Method are neural network, and network architecture. Generic are it, network, and methods. Material are decentralized data, and patient CXR data. OtherScientificTerm are data privacy, and network bandwidth. Task is diagnosis of COVID-19. ","This paper proposes a federated learning for the healthcare sector, where a neural network is trained in a decentralized fashion. The authors propose a new deep learning architecture called Vision Transformer, which is a decomposable configuration that can be applied to any neural network architecture. They show that it can be used to perform neural network training on COVID-19 diagnosis from chest X-ray (CXR) images. The proposed framework is shown to perform better than data-centralized training on CXR datasets, and is able to handle non-independent and identically distributed data distribution across multiple clients.    The authors also show that the proposed Vision Transformers can also be used for split learning, where each client only needs to share a small fraction of the data with the other clients. They also show how the proposed framework can be combined with heterogeneous multi-task clients to achieve better performance.  Finally, the authors demonstrate that the Transformer can perform collaborative learning for medical imaging on medical imaging using medical imaging datasets and demonstrate that their proposed framework performs better than the state-of-the-art methods.  The main contribution of the paper is that the authors propose to use decentralized data to train the network, and that they also provide data privacy guarantees to ensure that the network architecture is decoupled from the data. They demonstrate that this decoupling of data privacy helps to reduce the number of clients and network bandwidth, and show that their method performs well on the diagnosis of COVID19. The paper also shows that their Transformer performs well in the case of non-identical distributed data, and also on the case where there is no patient CX-ray data. ","This paper proposes a federated learning for the healthcare sector, where a neural network is trained in a decentralized fashion. The authors propose a new deep learning architecture called Vision Transformer, which is a decomposable configuration that can be applied to any neural network architecture. They show that it can be used to perform neural network training on COVID-19 diagnosis from chest X-ray (CXR) images. The proposed framework is shown to perform better than data-centralized training on CXR datasets, and is able to handle non-independent and identically distributed data distribution across multiple clients.    The authors also show that the proposed Vision Transformers can also be used for split learning, where each client only needs to share a small fraction of the data with the other clients. They also show how the proposed framework can be combined with heterogeneous multi-task clients to achieve better performance.  Finally, the authors demonstrate that the Transformer can perform collaborative learning for medical imaging on medical imaging using medical imaging datasets and demonstrate that their proposed framework performs better than the state-of-the-art methods.  The main contribution of the paper is that the authors propose to use decentralized data to train the network, and that they also provide data privacy guarantees to ensure that the network architecture is decoupled from the data. They demonstrate that this decoupling of data privacy helps to reduce the number of clients and network bandwidth, and show that their method performs well on the diagnosis of COVID19. The paper also shows that their Transformer performs well in the case of non-identical distributed data, and also on the case where there is no patient CX-ray data. "
8856,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"neural implicit representations USED-FOR 3D reconstruction. expressiveness CONJUNCTION flexibility. flexibility CONJUNCTION expressiveness. flexibility EVALUATE-FOR neural implicit representations. expressiveness EVALUATE-FOR neural implicit representations. oriented point cloud USED-FOR indicator function. differentiable PSR layer USED-FOR explicit 3D point representation. 3D mesh USED-FOR explicit 3D point representation. Chamfer distance HYPONYM-OF surface reconstruction metrics. oriented point clouds USED-FOR shapes. points CONJUNCTION patches. patches CONJUNCTION points. patches CONJUNCTION meshes. meshes CONJUNCTION patches. SAP USED-FOR topology - agnostic, watertight manifold surfaces. explicit representations COMPARE SAP. SAP COMPARE explicit representations. meshes HYPONYM-OF explicit representations. points HYPONYM-OF explicit representations. patches HYPONYM-OF explicit representations. SAP USED-FOR surface reconstruction. SAP USED-FOR learning - based reconstruction. unoriented point clouds CONJUNCTION learning - based reconstruction. learning - based reconstruction CONJUNCTION unoriented point clouds. learning - based reconstruction USED-FOR surface reconstruction. unoriented point clouds USED-FOR surface reconstruction. Metric are slow inference time, and inference time. Method are ubiquitous point cloud representation, differentiable point - to - mesh layer, and Poisson Surface Reconstruction ( PSR ). OtherScientificTerm is implicit indicator field. ","This paper studies the problem of learning neural implicit representations for 3D reconstruction, which aims to improve the expressiveness and flexibility of 3D reconstructions. The paper proposes a differentiable point-to-mesh (PSR) layer that can be applied to any ubiquitous point cloud representation of a surface, and is able to reduce the slow inference time. Specifically, the paper proposes Poisson Surface Reconstruction (PPSR), which is based on the observation that an oriented point cloud can be used as an indicator function for a surface reconstruction metric (e.g., Chamfer distance). A differentiable PSR layer is then used to learn an explicit 3D point representation from a 3D mesh.    The paper shows that the benefits of using oriented point clouds for shapes are that the implicit indicator field can be differentiable, and that the inference time can be significantly reduced. The authors also show that the proposed SAP can learn topology-agnostic, watertight manifold surfaces.  Experiments are conducted on a variety of explicit representations, including points, patches, meshes, and meshes that are not point clouds, and on surface reconstruction from unoriented point clouds and learning-based reconstruction from surface reconstruction based on unordered point clouds. The results are promising, and the paper is well-written. ","This paper studies the problem of learning neural implicit representations for 3D reconstruction, which aims to improve the expressiveness and flexibility of 3D reconstructions. The paper proposes a differentiable point-to-mesh (PSR) layer that can be applied to any ubiquitous point cloud representation of a surface, and is able to reduce the slow inference time. Specifically, the paper proposes Poisson Surface Reconstruction (PPSR), which is based on the observation that an oriented point cloud can be used as an indicator function for a surface reconstruction metric (e.g., Chamfer distance). A differentiable PSR layer is then used to learn an explicit 3D point representation from a 3D mesh.    The paper shows that the benefits of using oriented point clouds for shapes are that the implicit indicator field can be differentiable, and that the inference time can be significantly reduced. The authors also show that the proposed SAP can learn topology-agnostic, watertight manifold surfaces.  Experiments are conducted on a variety of explicit representations, including points, patches, meshes, and meshes that are not point clouds, and on surface reconstruction from unoriented point clouds and learning-based reconstruction from surface reconstruction based on unordered point clouds. The results are promising, and the paper is well-written. "
8872,SP:76b64e6b104818ed26e9331d134df0125d84291c,inverse problems USED-FOR recovering representations of corrupted data. pre - trained representation learning network R(x ) USED-FOR clean images. CLIP HYPONYM-OF clean images. representations USED-FOR corrupted images. supervised inversion method USED-FOR representations. forward operator USED-FOR corrupted version A(x ). contrastive objective USED-FOR supervised inversion method. blurring CONJUNCTION additive noise. additive noise CONJUNCTION blurring. linear probe USED-FOR robust representations. additive noise CONJUNCTION random pixel masking. random pixel masking CONJUNCTION additive noise. end - to - end supervised baselines USED-FOR classifying images. accuracy EVALUATE-FOR end - to - end supervised baselines. linear probe USED-FOR classifying images. linear probe COMPARE end - to - end supervised baselines. end - to - end supervised baselines COMPARE linear probe. distortions FEATURE-OF classifying images. random pixel masking HYPONYM-OF distortions. blurring HYPONYM-OF distortions. additive noise HYPONYM-OF distortions. ImageNet EVALUATE-FOR method. distortion FEATURE-OF method. method COMPARE end - to - end baselines. end - to - end baselines COMPARE method. forward operators FEATURE-OF labeled data. labeled data USED-FOR method. Material is images. ,"This paper considers the problem of recovering representations of corrupted data from inverse problems. The authors propose a supervised inversion method for recovering representations for corrupted images from a pre-trained representation learning network R(x) from clean images (e.g. CLIP). The proposed supervised Inversion method uses a contrastive objective to learn representations for the corrupted version A(x). The forward operator is used to learn the corrupted versions of the original and corrupted versions. The proposed method is evaluated on ImageNet and compared with end-to-end supervised baselines for classifying images with distortions such as blurring, additive noise, and random pixel masking. The results show that the proposed linear probe is able to learn robust representations that are more robust to distortions compared to the standard linear probe. The method is also shown to be able to recover the forward operators of the labeled data. The paper also shows that the method can recover the distortion of a corrupted version of a given image from a single labeled image.","This paper considers the problem of recovering representations of corrupted data from inverse problems. The authors propose a supervised inversion method for recovering representations for corrupted images from a pre-trained representation learning network R(x) from clean images (e.g. CLIP). The proposed supervised Inversion method uses a contrastive objective to learn representations for the corrupted version A(x). The forward operator is used to learn the corrupted versions of the original and corrupted versions. The proposed method is evaluated on ImageNet and compared with end-to-end supervised baselines for classifying images with distortions such as blurring, additive noise, and random pixel masking. The results show that the proposed linear probe is able to learn robust representations that are more robust to distortions compared to the standard linear probe. The method is also shown to be able to recover the forward operators of the labeled data. The paper also shows that the method can recover the distortion of a corrupted version of a given image from a single labeled image."
8888,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,backpropagation USED-FOR local training of nodes. neural networks USED-FOR Structural credit assignment. reinforcement learning method USED-FOR node. REINFORCE USED-FOR node. global reward signal USED-FOR reinforcement learning method. REINFORCE HYPONYM-OF reinforcement learning method. reinforcement learning approaches USED-FOR learning. finite - horizon reinforcement learning problem USED-FOR neural network. off - policy learning HYPONYM-OF reinforcement learning. on - policy REINFORCE approach USED-FOR suboptimal solutions. on - policy REINFORCE approach CONJUNCTION variance reduction approaches. variance reduction approaches CONJUNCTION on - policy REINFORCE approach. networks of agents USED-FOR correlated samples. Generic is approach. Method is off - policy approach. OtherScientificTerm is stochasticity. ,"This paper proposes a reinforcement learning method called REINFORCE, which learns a node-level reward function and a global reward function for each node in a graph. Structural credit assignment is learned by neural networks, where the local training of nodes is performed via backpropagation, and the global reward signal is learned via reinforcement learning.   The authors consider a finite-horizon reinforcement learning problem to learn a neural network, and apply existing reinforcement learning approaches to the problem of learning in this setting.  The main contribution of the paper is the proposed approach, which is based on an off-policy approach. In particular, the authors show that the on-policy REFORCE approach is able to learn suboptimal solutions, and that the variance reduction approaches can be used to reduce the variance of the learned solutions. The authors also show that correlated samples can be obtained from networks of agents that are trained with stochasticity, and show that this correlated samples are more likely to be generated by REINCE. ","This paper proposes a reinforcement learning method called REINFORCE, which learns a node-level reward function and a global reward function for each node in a graph. Structural credit assignment is learned by neural networks, where the local training of nodes is performed via backpropagation, and the global reward signal is learned via reinforcement learning.   The authors consider a finite-horizon reinforcement learning problem to learn a neural network, and apply existing reinforcement learning approaches to the problem of learning in this setting.  The main contribution of the paper is the proposed approach, which is based on an off-policy approach. In particular, the authors show that the on-policy REFORCE approach is able to learn suboptimal solutions, and that the variance reduction approaches can be used to reduce the variance of the learned solutions. The authors also show that correlated samples can be obtained from networks of agents that are trained with stochasticity, and show that this correlated samples are more likely to be generated by REINCE. "
8904,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"parallel, hierarchical specialized pathways PART-OF visual system. pathways USED-FOR behaviours. visual recognition and movement FEATURE-OF behaviours. deep neural networks USED-FOR ventral, recognition pathway. deep ANN USED-FOR pathways. model USED-FOR ventral and the dorsal pathways. loss function USED-FOR ventral and the dorsal pathways. loss function USED-FOR model. models USED-FOR mouse visual cortex. self - supervised predictive loss function USED-FOR parallel pathways. parallel pathways USED-FOR deep neural network architecture. self - supervised predictive loss function USED-FOR deep neural network architecture. self - supervised predictive learning approach USED-FOR functional specialization. self - supervised predictive learning approach USED-FOR parallel pathway architectures. functional specialization FEATURE-OF mammalian visual systems. Material is mice. Task is recognition and movement behaviours. OtherScientificTerm is dorsal and ventral pathways. ","This paper studies the functional specialization of parallel, hierarchical specialized pathways in the visual system of mice. These pathways are responsible for different behaviours in visual recognition and movement in mice. The authors show that deep neural networks are able to learn a ventral, recognition pathway and a dorsal, movement pathway, and that these pathways can be learned using deep ANN. They also show that the model can learn both the ventral and the dorsal pathways using a self-supervised predictive loss function, which encourages the model to learn parallel pathways. The paper also shows that the models learned in mouse visual cortex can be used to predict the recognition performance of a mouse, and can be trained in a way that is similar to that of a human.    The authors also demonstrate that a deep neural network architecture that uses parallel pathways is able to be learned with a simple but effective, but efficient, self - supervised predictive learning approach, which can be applied to a wide variety of parallel pathway architectures. They show that a model trained with this loss function can learn the dorsal and ventral pathways to predict functional specialization in mammalian visual systems. They further show that their model can also be used for predicting recognition performance in mice, and show that it can learn to predict a range of functional specialization, including functional specialization for recognition and for movement behaviours.","This paper studies the functional specialization of parallel, hierarchical specialized pathways in the visual system of mice. These pathways are responsible for different behaviours in visual recognition and movement in mice. The authors show that deep neural networks are able to learn a ventral, recognition pathway and a dorsal, movement pathway, and that these pathways can be learned using deep ANN. They also show that the model can learn both the ventral and the dorsal pathways using a self-supervised predictive loss function, which encourages the model to learn parallel pathways. The paper also shows that the models learned in mouse visual cortex can be used to predict the recognition performance of a mouse, and can be trained in a way that is similar to that of a human.    The authors also demonstrate that a deep neural network architecture that uses parallel pathways is able to be learned with a simple but effective, but efficient, self - supervised predictive learning approach, which can be applied to a wide variety of parallel pathway architectures. They show that a model trained with this loss function can learn the dorsal and ventral pathways to predict functional specialization in mammalian visual systems. They further show that their model can also be used for predicting recognition performance in mice, and show that it can learn to predict a range of functional specialization, including functional specialization for recognition and for movement behaviours."
8920,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"deep hierarchical topic models USED-FOR semantically meaningful topics. them PART-OF topic hierarchy. text corpus USED-FOR deep hierarchical topic models. text corpus USED-FOR semantically meaningful topics. prior belief USED-FOR learning of the topic hierarchy. knowledge graph HYPONYM-OF prior belief. TopicNet HYPONYM-OF deep hierarchical topic model. symmetric and asymmetric similarities FEATURE-OF Gaussian embedding vectors. TopicNet USED-FOR symmetric and asymmetric similarities. Gaussian - distributed embedding vector USED-FOR TopicNet. evidence lower bound CONJUNCTION regularization term. regularization term CONJUNCTION evidence lower bound. auto - encoding variational inference network USED-FOR model parameters. stochastic gradient descent USED-FOR regularization term. regularization term USED-FOR model parameters. evidence lower bound USED-FOR model parameters. stochastic gradient descent USED-FOR model parameters. deep topic models USED-FOR discovering deeper interpretable topics. TopicNet COMPARE deep topic models. deep topic models COMPARE TopicNet. TopicNet USED-FOR discovering deeper interpretable topics. TopicNet USED-FOR document representations. deep topic models USED-FOR document representations. OtherScientificTerm are prior structural knowledge, inductive bias, shared embedding space, and prior semantic hierarchies. Task is learning. ","This paper proposes TopicNet, a deep hierarchical topic model that is able to capture the prior structural knowledge of the topic hierarchy. The authors propose to learn semantically meaningful topics from a text corpus, and then use them as part of a topic hierarchy, and use a prior belief (i.e., a knowledge graph) to guide the learning of the discussed topics. TopicNet is trained with a Gaussian-distributed embedding vector, where the inductive bias is that the shared embedding space is shared across all topics, and prior semantic hierarchies are shared across different topics.  The authors show that TopicNet can capture symmetric and asymmetric similarities between the Gaussian embedding vectors. They also show that the model parameters are learned using an evidence lower bound and a regularization term based on stochastic gradient descent, and that the auto-encoding variational inference network can be used to regularize the learned model parameters.  Finally, the authors compare the performance of TopicNet with other deep topic models on discovering deeper interpretable topics and document representations.  ","This paper proposes TopicNet, a deep hierarchical topic model that is able to capture the prior structural knowledge of the topic hierarchy. The authors propose to learn semantically meaningful topics from a text corpus, and then use them as part of a topic hierarchy, and use a prior belief (i.e., a knowledge graph) to guide the learning of the discussed topics. TopicNet is trained with a Gaussian-distributed embedding vector, where the inductive bias is that the shared embedding space is shared across all topics, and prior semantic hierarchies are shared across different topics.  The authors show that TopicNet can capture symmetric and asymmetric similarities between the Gaussian embedding vectors. They also show that the model parameters are learned using an evidence lower bound and a regularization term based on stochastic gradient descent, and that the auto-encoding variational inference network can be used to regularize the learned model parameters.  Finally, the authors compare the performance of TopicNet with other deep topic models on discovering deeper interpretable topics and document representations.  "
8936,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,Image - level contrastive representation learning USED-FOR transfer learning. generality USED-FOR transfer learning. specificity FEATURE-OF downstream task. design principle USED-FOR alignment. self - supervised pretext task CONJUNCTION downstream task. downstream task CONJUNCTION self - supervised pretext task. alignment USED-FOR self - supervised pretext task. alignment USED-FOR downstream task. pretraining method USED-FOR object detection. object - level translation invariance CONJUNCTION scale invariance. scale invariance CONJUNCTION object - level translation invariance. dedicated modules USED-FOR detection pipeline. selective search bounding boxes USED-FOR object proposals. object detection properties FEATURE-OF pretraining. FPN HYPONYM-OF detection pipeline. dedicated modules PART-OF pretraining network architecture. FPN HYPONYM-OF dedicated modules. scale invariance HYPONYM-OF object detection properties. object - level translation invariance HYPONYM-OF object detection properties. selective search bounding boxes USED-FOR object - level representations. COCO detection EVALUATE-FOR transfer. Selective Object COntrastive learning ( SoCo ) HYPONYM-OF method. transfer EVALUATE-FOR method. Mask R - CNN framework USED-FOR COCO detection. ,"This paper proposes Image-level contrastive representation learning for transfer learning based on generality. The key design principle is to use alignment between self-supervised pretext task and downstream task to improve the specificity of the downstream task. The authors propose a novel pretraining method for object detection. The proposed method, called Selective Object COntrastive learning (SoCo), is based on the design principle of selective search bounding boxes for object proposals. Two dedicated modules are added to the pretraining network architecture, called FPN and FPN+, to further improve the performance of the detection pipeline. The object detection properties of pretraining, such as object-level translation invariance and scale invariance, are also considered. Experiments on COCO detection using the Mask R-CNN framework show that the proposed method improves the transfer performance on transfer in the case of selective selection of the best candidate from a set of objects. ","This paper proposes Image-level contrastive representation learning for transfer learning based on generality. The key design principle is to use alignment between self-supervised pretext task and downstream task to improve the specificity of the downstream task. The authors propose a novel pretraining method for object detection. The proposed method, called Selective Object COntrastive learning (SoCo), is based on the design principle of selective search bounding boxes for object proposals. Two dedicated modules are added to the pretraining network architecture, called FPN and FPN+, to further improve the performance of the detection pipeline. The object detection properties of pretraining, such as object-level translation invariance and scale invariance, are also considered. Experiments on COCO detection using the Mask R-CNN framework show that the proposed method improves the transfer performance on transfer in the case of selective selection of the best candidate from a set of objects. "
8952,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"Vehicle routing problems ( VRPs ) HYPONYM-OF combinatorial problems. heuristic or learning - based works USED-FOR decent solutions. small problem instances EVALUATE-FOR decent solutions. learning - augmented local search framework USED-FOR large - scale VRP. linear number of subproblems COMPARE exponential. exponential COMPARE linear number of subproblems. spatial locality USED-FOR linear number of subproblems. regression USED-FOR subproblem selection. method USED-FOR VRPs. solution qualities EVALUATE-FOR VRPs. method USED-FOR VRP solvers. solution qualities EVALUATE-FOR method. subproblem selection COMPARE heuristic or random selection. heuristic or random selection COMPARE subproblem selection. variants CONJUNCTION solvers. solvers CONJUNCTION variants. VRP distributions CONJUNCTION variants. variants CONJUNCTION VRP distributions. OtherScientificTerm are subproblems, and black box subsolver. Method is Transformer. ","This paper proposes a learning-augmented local search framework for solving vehicle routing problems (VRPs), a class of combinatorial problems where heuristic or learning-based works have been shown to provide decent solutions on small problem instances, but not decent solutions for large-scale VRPs. The authors propose a new method for solving VRPs based on a linear number of subproblems instead of an exponential, which is based on spatial locality. The subproblem selection is done via regression, and the authors propose to use a black box subsolver to select the subproblem to be solved. The proposed method is shown to improve the solution qualities of VRPs, and is able to train VRP solvers on a variety of VRP distributions, variants, and solvers. Experiments are performed on a Transformer, and show that subproblemselection is more effective than heuristic, random selection, and heuristic.","This paper proposes a learning-augmented local search framework for solving vehicle routing problems (VRPs), a class of combinatorial problems where heuristic or learning-based works have been shown to provide decent solutions on small problem instances, but not decent solutions for large-scale VRPs. The authors propose a new method for solving VRPs based on a linear number of subproblems instead of an exponential, which is based on spatial locality. The subproblem selection is done via regression, and the authors propose to use a black box subsolver to select the subproblem to be solved. The proposed method is shown to improve the solution qualities of VRPs, and is able to train VRP solvers on a variety of VRP distributions, variants, and solvers. Experiments are performed on a Transformer, and show that subproblemselection is more effective than heuristic, random selection, and heuristic."
8968,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"dynamic data distributions USED-FOR Continual learning. learning - triggered synaptic expansion CONJUNCTION synaptic convergence. synaptic convergence CONJUNCTION learning - triggered synaptic expansion. learning - triggered synaptic expansion USED-FOR biological neural networks. synaptic Expansion - Convergence ( AFEC ) FEATURE-OF Active Forgetting. visual classification tasks CONJUNCTION Atari reinforcement tasks. Atari reinforcement tasks CONJUNCTION visual classification tasks. CIFAR-10 regression tasks CONJUNCTION visual classification tasks. visual classification tasks CONJUNCTION CIFAR-10 regression tasks. AFEC USED-FOR learning of new tasks. continual learning benchmarks EVALUATE-FOR AFEC. continual learning benchmarks EVALUATE-FOR AFEC. Atari reinforcement tasks HYPONYM-OF continual learning benchmarks. visual classification tasks HYPONYM-OF continual learning benchmarks. CIFAR-10 regression tasks HYPONYM-OF continual learning benchmarks. OtherScientificTerm are knowledge transfer, and forward knowledge transfer. Task is continual learning. Method are biological active forgetting, and Bayesian continual learning. Generic are approach, method, and them. ","Continual learning with dynamic data distributions is a popular topic in the continual learning literature. This paper proposes a new approach, Active Forgetting with Synaptic Expansion-Convergence (AFEC), which is based on biological active forgetting. In biological neural networks, learning-triggered synaptic expansion and synaptic convergence have been shown to be important factors for knowledge transfer, and AFEC is an extension of this work to the case of active forgetting in continual learning. The authors show that AFEC can be used to guide the learning of new tasks in a Bayesian continual learning setting. The proposed method is tested on several continual learning benchmarks, including CIFAR-10 regression tasks, visual classification tasks, and Atari reinforcement tasks, where AFEC outperforms the state-of-the-art on all of them.","Continual learning with dynamic data distributions is a popular topic in the continual learning literature. This paper proposes a new approach, Active Forgetting with Synaptic Expansion-Convergence (AFEC), which is based on biological active forgetting. In biological neural networks, learning-triggered synaptic expansion and synaptic convergence have been shown to be important factors for knowledge transfer, and AFEC is an extension of this work to the case of active forgetting in continual learning. The authors show that AFEC can be used to guide the learning of new tasks in a Bayesian continual learning setting. The proposed method is tested on several continual learning benchmarks, including CIFAR-10 regression tasks, visual classification tasks, and Atari reinforcement tasks, where AFEC outperforms the state-of-the-art on all of them."
8984,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,Zeroth - order ( ZO ) optimization USED-FOR tasks. query - based black - box adversarial attacks CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION query - based black - box adversarial attacks. query - based black - box adversarial attacks HYPONYM-OF tasks. reinforcement learning HYPONYM-OF tasks. prior information PART-OF gradient estimation procedure. finite differences USED-FOR gradient estimation procedure. finite differences USED-FOR prior information. greedy descent framework CONJUNCTION gradient estimators. gradient estimators CONJUNCTION greedy descent framework. convergence FEATURE-OF prior - guided ZO algorithms. greedy descent framework USED-FOR prior - guided ZO algorithms. prior information USED-FOR accelerated random search ( ARS ) algorithm. convergence analysis USED-FOR accelerated random search ( ARS ) algorithm. numerical benchmarks CONJUNCTION adversarial attacks. adversarial attacks CONJUNCTION numerical benchmarks. OtherScientificTerm is convergence guarantee. Method is greedy descent methods. ,This paper studies the convergence of Zeroth-order (ZO) optimization for two tasks: query-based black-box adversarial attacks and reinforcement learning. The authors propose a greedy descent framework and gradient estimators that incorporate prior information from finite differences in the gradient estimation procedure. They provide a convergence guarantee for both greedy descent methods. They also provide an accelerated random search (ARS) algorithm based on this prior information and a convergence analysis. Experiments are conducted on a variety of numerical benchmarks as well as a few real-world applications such as numerical benchmarks for adversarial applications. ,This paper studies the convergence of Zeroth-order (ZO) optimization for two tasks: query-based black-box adversarial attacks and reinforcement learning. The authors propose a greedy descent framework and gradient estimators that incorporate prior information from finite differences in the gradient estimation procedure. They provide a convergence guarantee for both greedy descent methods. They also provide an accelerated random search (ARS) algorithm based on this prior information and a convergence analysis. Experiments are conducted on a variety of numerical benchmarks as well as a few real-world applications such as numerical benchmarks for adversarial applications. 
9000,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"test accuracy EVALUATE-FOR unpruned network. pruned network COMPARE unpruned network. unpruned network COMPARE pruned network. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep neural network ( DNN ) USED-FOR applications. LTH PART-OF deep neural network ( DNN ). natural language processing HYPONYM-OF applications. computer vision HYPONYM-OF applications. objective function CONJUNCTION sample complexity. sample complexity CONJUNCTION objective function. guaranteed generalization EVALUATE-FOR model. algorithm USED-FOR pruned neural network. non - pruned weights PART-OF hidden layer. accelerated ) stochastic gradient descent algorithm USED-FOR algorithm. pruned neural network USED-FOR model. pruned neural network COMPARE unpruned one. unpruned one COMPARE pruned neural network. Task are lottery ticket hypothesis ( LTH ), and pruning multi - layer neural networks. Metric are generalization, and generalization of the winning ticket. OtherScientificTerm are zero generalization error, and convex region. Method is neural network model. ","This paper studies the lottery ticket hypothesis (LTH), which claims that the generalization of a neural network trained with pruned weights is guaranteed to have zero generalization error, and that the test accuracy of an unpruned network is asymptotically the same as that of a pruned network. The authors propose to use the LTH in a deep neural network (DNN) for two applications: computer vision and natural language processing. They show that pruning multi-layer neural networks is equivalent to pruning the weights of a single hidden layer, and show that under certain assumptions on the objective function, sample complexity, and the number of weights, the pruned neural network has the same test accuracy as an unplunged network. They also provide an algorithm to prune the weights at each hidden layer to achieve the same guaranteed generalization. The algorithm is based on the (adversarial) stochastic gradient descent algorithm, where the algorithm first prunes the weights in the first hidden layer of the neural network model, and then uses the algorithm to train a pruning neural network that has a similar objective function and sample complexity to the one that was used to train the original neural network. Finally, the authors show that the model trained with the prune neural network is able to generalize as well as the original model with the same objective function.    The authors also provide a theoretical analysis that shows that the pruning of the winning ticket leads to a convex region where the non-pruned weights in a hidden layer have non-zero generalization performance. ","This paper studies the lottery ticket hypothesis (LTH), which claims that the generalization of a neural network trained with pruned weights is guaranteed to have zero generalization error, and that the test accuracy of an unpruned network is asymptotically the same as that of a pruned network. The authors propose to use the LTH in a deep neural network (DNN) for two applications: computer vision and natural language processing. They show that pruning multi-layer neural networks is equivalent to pruning the weights of a single hidden layer, and show that under certain assumptions on the objective function, sample complexity, and the number of weights, the pruned neural network has the same test accuracy as an unplunged network. They also provide an algorithm to prune the weights at each hidden layer to achieve the same guaranteed generalization. The algorithm is based on the (adversarial) stochastic gradient descent algorithm, where the algorithm first prunes the weights in the first hidden layer of the neural network model, and then uses the algorithm to train a pruning neural network that has a similar objective function and sample complexity to the one that was used to train the original neural network. Finally, the authors show that the model trained with the prune neural network is able to generalize as well as the original model with the same objective function.    The authors also provide a theoretical analysis that shows that the pruning of the winning ticket leads to a convex region where the non-pruned weights in a hidden layer have non-zero generalization performance. "
9016,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"private synthetic data generation USED-FOR query release. differential privacy FEATURE-OF sensitive dataset. algorithmic framework USED-FOR iterative algorithms. computational bottlenecks PART-OF algorithms. generative networks CONJUNCTION exponential mechanism ( GEM ). exponential mechanism ( GEM ) CONJUNCTION generative networks. MWEM HYPONYM-OF algorithms. neural networks USED-FOR generative models. MWEM USED-FOR private entropy projection ( PEP ). PEP COMPARE algorithms. algorithms COMPARE PEP. GEM COMPARE algorithms. algorithms COMPARE GEM. GEM CONJUNCTION PEP. PEP CONJUNCTION GEM. prior information USED-FOR GEM. public data USED-FOR prior information. public data USED-FOR state - of - the - art method. OtherScientificTerm is statistical queries. Generic are framework, methods, and method. Method are gradient - based optimization, and PMWPub. Metric is accuracy. ","This paper proposes a new algorithmic framework for private synthetic data generation for query release in the context of differential privacy for statistical queries. The proposed framework is based on gradient-based optimization. The authors propose a new algorithm, called PMWPub, which extends existing iterative algorithms with computational bottlenecks to the setting of query release under differential privacy on a sensitive dataset. Two algorithms, namely MWEM and GEM, are proposed, which are generative models based on neural networks and exponential mechanism (GEM). MWEM is used for private entropy projection (PEP), while GEM uses prior information from public data. The experimental results show that PEP outperforms existing algorithms, and that GEM outperforms the state-of-the-art algorithms in terms of accuracy. The paper also shows that the proposed method can be applied to any public dataset.   The main contribution of the paper is the introduction of the state of the art method based on the use of public data as prior information for GEM.","This paper proposes a new algorithmic framework for private synthetic data generation for query release in the context of differential privacy for statistical queries. The proposed framework is based on gradient-based optimization. The authors propose a new algorithm, called PMWPub, which extends existing iterative algorithms with computational bottlenecks to the setting of query release under differential privacy on a sensitive dataset. Two algorithms, namely MWEM and GEM, are proposed, which are generative models based on neural networks and exponential mechanism (GEM). MWEM is used for private entropy projection (PEP), while GEM uses prior information from public data. The experimental results show that PEP outperforms existing algorithms, and that GEM outperforms the state-of-the-art algorithms in terms of accuracy. The paper also shows that the proposed method can be applied to any public dataset.   The main contribution of the paper is the introduction of the state of the art method based on the use of public data as prior information for GEM."
9032,SP:d789e92c1e4f6a44de373210cd732198a6f809be,per - pixel classification task USED-FOR semantic segmentation. mask classification USED-FOR instance - level segmentation. mask classification USED-FOR semanticand instance - level segmentation tasks. model USED-FOR mask classification. training procedure USED-FOR mask classification. MaskFormer HYPONYM-OF mask classification model. mask classification model USED-FOR binary masks. single global class label prediction USED-FOR binary masks. approaches USED-FOR semantic and panoptic segmentation tasks. mask classification - based method USED-FOR approaches. mask classification - based method USED-FOR semantic and panoptic segmentation tasks. MaskFormer COMPARE per - pixel classification baselines. per - pixel classification baselines COMPARE MaskFormer. ,"This paper proposes a new per-pixel classification task for semantic segmentation. The authors propose a new model called MaskFormer, a mask classification model that can be applied to both semanticand instance-level segmentation tasks. The proposed model is able to perform mask classification on both semantic and instance level. The training procedure for mask classification is similar to previous work, but the authors propose to use binary masks instead of a single global class label prediction. Experiments show that the proposed mask classification-based method outperforms previous approaches on both the semantic and panoptic segmentation task. MaskFormer is also able to outperform other per- pixel classification baselines.","This paper proposes a new per-pixel classification task for semantic segmentation. The authors propose a new model called MaskFormer, a mask classification model that can be applied to both semanticand instance-level segmentation tasks. The proposed model is able to perform mask classification on both semantic and instance level. The training procedure for mask classification is similar to previous work, but the authors propose to use binary masks instead of a single global class label prediction. Experiments show that the proposed mask classification-based method outperforms previous approaches on both the semantic and panoptic segmentation task. MaskFormer is also able to outperform other per- pixel classification baselines."
9048,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,gradient descent USED-FOR random undercomplete two - layers ReLU neural networks. smooth activation function FEATURE-OF subexponential width random neural network. Material is overcomplete case. OtherScientificTerm is dimension. ,"This paper studies the problem of gradient descent for random undercomplete two-layer ReLU neural networks. In particular, the authors consider the overcomplete case, where the dimension of the network is larger than the input dimension. They show that under this setting, a subexponential width random neural network with a smooth activation function can be learned. ","This paper studies the problem of gradient descent for random undercomplete two-layer ReLU neural networks. In particular, the authors consider the overcomplete case, where the dimension of the network is larger than the input dimension. They show that under this setting, a subexponential width random neural network with a smooth activation function can be learned. "
9064,SP:220db9ed147bbe67de5d82778720a1549656e48d,"sample quality CONJUNCTION distribution coverage. distribution coverage CONJUNCTION sample quality. distribution coverage EVALUATE-FOR Score - based generative models ( SGMs ). sample quality EVALUATE-FOR Score - based generative models ( SGMs ). network evaluations USED-FOR sampling. data space FEATURE-OF they. approach USED-FOR SGMs. latent space FEATURE-OF SGMs. variational autoencoder framework USED-FOR approach. non - continuous data USED-FOR SGMs. score - matching objective USED-FOR LSGM setting. SGM USED-FOR mismatch of the target distribution. Normal one USED-FOR mismatch of the target distribution. LSGM COMPARE generative results. generative results COMPARE LSGM. dataset EVALUATE-FOR LSGM. dataset EVALUATE-FOR generative results. FID score EVALUATE-FOR LSGM. CIFAR-10 EVALUATE-FOR LSGM. LSGM COMPARE SGMs. SGMs COMPARE LSGM. CelebA - HQ-256 EVALUATE-FOR LSGM. LSGM COMPARE them. them COMPARE LSGM. CelebA - HQ-256 EVALUATE-FOR SGMs. sampling time EVALUATE-FOR them. sample quality EVALUATE-FOR SGMs. sample quality EVALUATE-FOR LSGM. LSGM USED-FOR binary images. binarized OMNIGLOT dataset EVALUATE-FOR LSGM. Method are generative models, and LSGMs. OtherScientificTerm is score function. Task is variance reduction of the training objective. Generic is implementation. ","Score-based generative models (SGMs) have been shown to improve both sample quality and distribution coverage. However, they are expensive to train due to the large number of network evaluations required for sampling. This paper proposes a new approach to train SGMs on non-continuous data using a variational autoencoder framework.    The authors propose LSGMs, which is a generalization of previous work that learns a score function that can be used to train a score-matching objective in the LSGM setting.  The key idea is to learn the score function in the latent space of the SGMs, and then use this score function as a regularizer in the training of SGMs in the data space. The authors show that the SGM can be trained to avoid the mismatch of the target distribution with the normal one, which can lead to a variance reduction of the training objective.  LSGM is evaluated on CIFAR-10, where it is shown that LSGM outperforms the generative results on the original dataset and achieves a FID score of 4.5. LSGM also outperforms SGMs trained on the binarized OMNIGLOT dataset, and outperforms them on CelebA-HQ-256, where they outperform them in terms of sampling time and sample quality.  In addition, LSGM can also be trained on binary images, and is shown to be able to improve the sample quality of binary images. ","Score-based generative models (SGMs) have been shown to improve both sample quality and distribution coverage. However, they are expensive to train due to the large number of network evaluations required for sampling. This paper proposes a new approach to train SGMs on non-continuous data using a variational autoencoder framework.    The authors propose LSGMs, which is a generalization of previous work that learns a score function that can be used to train a score-matching objective in the LSGM setting.  The key idea is to learn the score function in the latent space of the SGMs, and then use this score function as a regularizer in the training of SGMs in the data space. The authors show that the SGM can be trained to avoid the mismatch of the target distribution with the normal one, which can lead to a variance reduction of the training objective.  LSGM is evaluated on CIFAR-10, where it is shown that LSGM outperforms the generative results on the original dataset and achieves a FID score of 4.5. LSGM also outperforms SGMs trained on the binarized OMNIGLOT dataset, and outperforms them on CelebA-HQ-256, where they outperform them in terms of sampling time and sample quality.  In addition, LSGM can also be trained on binary images, and is shown to be able to improve the sample quality of binary images. "
9080,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"Neural networks COMPARE kernel methods. kernel methods COMPARE Neural networks. neural tangent kernels HYPONYM-OF kernel methods. hypothesis class USED-FOR realistic data. noise FEATURE-OF sparse signal. convolutional neural network USED-FOR noise. high - variance noise FEATURE-OF sparse signal. convolutional neural network USED-FOR data distribution. sparse signal FEATURE-OF data distribution. stochastic gradient descent USED-FOR convolutional neural network. predetermined features USED-FOR neural tangent kernel. CNN COMPARE neural tangent kernel. neural tangent kernel COMPARE CNN. CIFAR-10 and MNIST images EVALUATE-FOR CNN. neural networks COMPARE kernel methods. kernel methods COMPARE neural networks. OtherScientificTerm are complex hypothesis class, background noise, and local signal adaptivity ( LSA ) phenomenon. Task is image classification setting. ","Neural networks have been shown to outperform kernel methods such as neural tangent kernels. Neural networks can be seen as a special case of kernel methods where the data is a complex hypothesis class, and the hypothesis class is used to model realistic data. In this paper, the authors show that a convolutional neural network trained with stochastic gradient descent on a data distribution with sparse signal with high-variance noise is more robust to background noise than neural networks trained with kernel methods. The authors also show that the local signal adaptivity (LSA) phenomenon can be observed in the image classification setting, and that a CNN trained on CIFAR-10 and MNIST images can be more robust than a CNN with the same number of parameters and weights as a neural tangle kernel trained on predetermined features.","Neural networks have been shown to outperform kernel methods such as neural tangent kernels. Neural networks can be seen as a special case of kernel methods where the data is a complex hypothesis class, and the hypothesis class is used to model realistic data. In this paper, the authors show that a convolutional neural network trained with stochastic gradient descent on a data distribution with sparse signal with high-variance noise is more robust to background noise than neural networks trained with kernel methods. The authors also show that the local signal adaptivity (LSA) phenomenon can be observed in the image classification setting, and that a CNN trained on CIFAR-10 and MNIST images can be more robust than a CNN with the same number of parameters and weights as a neural tangle kernel trained on predetermined features."
9096,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"network USED-FOR decentralized machine learning. model USED-FOR local loss functions. known convergence rates EVALUATE-FOR GT algorithms. negative eigenvalues FEATURE-OF connectivity matrix. Method are stochastic model updates, gradient tracking ( GT ) algorithms, and GT method. OtherScientificTerm are workers ’ local data distributions, mixing parameter p, and O(p−3/2 ). Material are noiseless case, and stochastic case. ","This paper studies the problem of decentralized machine learning with a single network. The authors consider stochastic model updates where workers’ local data distributions are distributed across multiple machines. They show that gradient tracking (GT) algorithms converge to a stationary point when the mixing parameter p is O(p−3/2) and O(1/\sqrt{n}^2) in the noiseless case. They also show that in the case of a model with local loss functions that have negative eigenvalues of the connectivity matrix, the GT method converges to the stationary point. They provide known convergence rates for the standard GT algorithms that are O(n^{-1/2}^3) in both cases. ","This paper studies the problem of decentralized machine learning with a single network. The authors consider stochastic model updates where workers’ local data distributions are distributed across multiple machines. They show that gradient tracking (GT) algorithms converge to a stationary point when the mixing parameter p is O(p−3/2) and O(1/\sqrt{n}^2) in the noiseless case. They also show that in the case of a model with local loss functions that have negative eigenvalues of the connectivity matrix, the GT method converges to the stationary point. They provide known convergence rates for the standard GT algorithms that are O(n^{-1/2}^3) in both cases. "
9112,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"Upper Confidence Bound ( UCB ) policy HYPONYM-OF optimism - based MAB algorithms. O ( log n ) regret EVALUATE-FOR it. arm - sampling behavior FEATURE-OF UCB. UCB USED-FOR arm - sampling rates. O p n log n minimax regret EVALUATE-FOR UCB. process - level characterization FEATURE-OF MAB problem. diffusion scaling FEATURE-OF UCB. diffusion scaling FEATURE-OF MAB problem. UCB USED-FOR MAB problem. UCB CONJUNCTION Thompson Sampling. Thompson Sampling CONJUNCTION UCB. incomplete learning phenomenon FEATURE-OF latter. Metric are complexity, and problem complexity. OtherScientificTerm are mean rewards, instance gap, and small ” gap worst - case lens. ","This paper studies the Upper Confidence Bound (UCB) policy of optimism-based MAB algorithms, and shows that it has O(log n) regret with respect to the mean rewards. The complexity of UCB is defined as the number of times that the upper confidence bound is satisfied. The paper also shows that UCB has a similar arm-sampling behavior to Thompson Sampling, and that it can be seen as a special case of the “small” gap worst-case lens.   The paper shows that under certain assumptions, UCB can achieve O p n log n minimax regret with O(n log n) arms. The authors also provide a process-level characterization of the MAB problem under diffusion scaling, and show that the UCB converges to the optimal solution of the problem with high probability. Finally, the paper shows how UCB’s lower confidence bound can be used to control the arm-sourcing rates, and how the latter leads to a “complete learning phenomenon”, where UCB learns faster and with lower regret when the instance gap is small. ","This paper studies the Upper Confidence Bound (UCB) policy of optimism-based MAB algorithms, and shows that it has O(log n) regret with respect to the mean rewards. The complexity of UCB is defined as the number of times that the upper confidence bound is satisfied. The paper also shows that UCB has a similar arm-sampling behavior to Thompson Sampling, and that it can be seen as a special case of the “small” gap worst-case lens.   The paper shows that under certain assumptions, UCB can achieve O p n log n minimax regret with O(n log n) arms. The authors also provide a process-level characterization of the MAB problem under diffusion scaling, and show that the UCB converges to the optimal solution of the problem with high probability. Finally, the paper shows how UCB’s lower confidence bound can be used to control the arm-sourcing rates, and how the latter leads to a “complete learning phenomenon”, where UCB learns faster and with lower regret when the instance gap is small. "
9128,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"Cross - Domain Recommendation ( CDR ) USED-FOR cold - start problem. domain knowledge USED-FOR cold - start problem. cold - start problem PART-OF recommender systems. domain knowledge USED-FOR Cross - Domain Recommendation ( CDR ). cold - start CONJUNCTION CDR. CDR CONJUNCTION cold - start. approaches USED-FOR CDR. approaches USED-FOR cold - start. cross - domain recommendation framework USED-FOR CDCSR problem. DisAlign HYPONYM-OF cross - domain recommendation framework. rating and auxiliary representations USED-FOR recommendation. rating and auxiliary representations USED-FOR DisAlign. Stein path alignment USED-FOR latent embedding distributions. proxy Stein path HYPONYM-OF version. DisAlign COMPARE models. models COMPARE DisAlign. Douban and Amazon datasets EVALUATE-FOR DisAlign. CDCSR setting EVALUATE-FOR models. CDCSR setting EVALUATE-FOR DisAlign. OtherScientificTerm are latent embedding discrepancy, and model degradation. Metric is efficiency. ","This paper tackles the cold-start problem in recommender systems, which is a common problem in the domain knowledge for Cross-Domain Recommendation (CDR) that is motivated by the latent embedding discrepancy between the source and target domains. The authors propose two approaches to tackle the cold start and CDR problem. The first approach, DisAlign, is a cross-domain recommendation framework that tackles the CDCSR problem, where the recommendation is based on both the rating and auxiliary representations of the source domain. The second approach, Stein path alignment (SPALA), is a variant of the previous approach that aligns the latentembedding distributions of the target and source domains. This version, called proxy Stein path, is used to avoid model degradation. Experiments on the Douban and Amazon datasets show that DisAlIGN outperforms the previous models in the standard and the CIFAR-10/100 datasets, and is competitive with the state-of-the-art in the CDR setting.   ","This paper tackles the cold-start problem in recommender systems, which is a common problem in the domain knowledge for Cross-Domain Recommendation (CDR) that is motivated by the latent embedding discrepancy between the source and target domains. The authors propose two approaches to tackle the cold start and CDR problem. The first approach, DisAlign, is a cross-domain recommendation framework that tackles the CDCSR problem, where the recommendation is based on both the rating and auxiliary representations of the source domain. The second approach, Stein path alignment (SPALA), is a variant of the previous approach that aligns the latentembedding distributions of the target and source domains. This version, called proxy Stein path, is used to avoid model degradation. Experiments on the Douban and Amazon datasets show that DisAlIGN outperforms the previous models in the standard and the CIFAR-10/100 datasets, and is competitive with the state-of-the-art in the CDR setting.   "
9144,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,self - attention CONJUNCTION multi - layer perceptrons ( MLP ) models. multi - layer perceptrons ( MLP ) models CONJUNCTION self - attention. multi - layer perceptrons ( MLP ) models USED-FOR vision. self - attention USED-FOR vision. self - attention CONJUNCTION MLP. MLP CONJUNCTION self - attention. complexity EVALUATE-FOR MLP. complexity EVALUATE-FOR self - attention. Global Filter Network ( GFNet ) HYPONYM-OF architecture. Global Filter Network ( GFNet ) USED-FOR long - term spatial dependencies. architecture USED-FOR long - term spatial dependencies. frequency domain FEATURE-OF long - term spatial dependencies. 2D discrete Fourier transform CONJUNCTION element - wise multiplication. element - wise multiplication CONJUNCTION 2D discrete Fourier transform. global filters CONJUNCTION 2D inverse Fourier transform. 2D inverse Fourier transform CONJUNCTION global filters. self - attention layer PART-OF vision transformers. element - wise multiplication USED-FOR frequency - domain features. element - wise multiplication CONJUNCTION 2D inverse Fourier transform. 2D inverse Fourier transform CONJUNCTION element - wise multiplication. element - wise multiplication USED-FOR global filters. operations PART-OF architecture. self - attention layer PART-OF architecture. 2D discrete Fourier transform HYPONYM-OF operations. global filters HYPONYM-OF operations. element - wise multiplication HYPONYM-OF operations. 2D inverse Fourier transform HYPONYM-OF operations. accuracy / complexity trade - offs EVALUATE-FOR models. ImageNet and downstream tasks EVALUATE-FOR models. efficiency CONJUNCTION generalization ability. generalization ability CONJUNCTION efficiency. generalization ability CONJUNCTION robustness. robustness CONJUNCTION generalization ability. GFNet COMPARE transformer - style models. transformer - style models COMPARE GFNet. GFNet COMPARE CNNs. CNNs COMPARE GFNet. transformer - style models CONJUNCTION CNNs. CNNs CONJUNCTION transformer - style models. efficiency EVALUATE-FOR CNNs. generalization ability EVALUATE-FOR CNNs. robustness EVALUATE-FOR CNN,"This paper proposes a new architecture, called Global Filter Network (GFNet), that combines self-attention and multi-layer perceptrons (MLP) models for vision. The proposed architecture is designed to capture long-term spatial dependencies in the frequency domain. The architecture consists of three operations: global filters, 2D discrete Fourier transform, and element-wise multiplication for frequency-domain features. The authors show that the proposed GFNet achieves better accuracy/complexity trade-offs compared to other models on ImageNet and downstream tasks. They also show that GFNet is more robust than transformer-style models and CNNs in terms of efficiency, generalization ability, and robustness. ","This paper proposes a new architecture, called Global Filter Network (GFNet), that combines self-attention and multi-layer perceptrons (MLP) models for vision. The proposed architecture is designed to capture long-term spatial dependencies in the frequency domain. The architecture consists of three operations: global filters, 2D discrete Fourier transform, and element-wise multiplication for frequency-domain features. The authors show that the proposed GFNet achieves better accuracy/complexity trade-offs compared to other models on ImageNet and downstream tasks. They also show that GFNet is more robust than transformer-style models and CNNs in terms of efficiency, generalization ability, and robustness. "
9160,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"high - dimensional features CONJUNCTION visual concepts. visual concepts CONJUNCTION high - dimensional features. real - world large - scale datasets USED-FOR predicting trustworthiness. focal loss CONJUNCTION true class probability confidence loss. true class probability confidence loss CONJUNCTION focal loss. cross entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross entropy loss. cross entropy loss HYPONYM-OF trustworthiness predictors. focal loss HYPONYM-OF trustworthiness predictors. cross entropy loss HYPONYM-OF prior - art loss functions. true class probability confidence loss HYPONYM-OF prior - art loss functions. focal loss HYPONYM-OF prior - art loss functions. prior - art loss functions USED-FOR trustworthiness predictors. steep slope loss USED-FOR features. steep slope loss USED-FOR trustworthiness predictors. Vision Transformer CONJUNCTION ResNet. ResNet CONJUNCTION Vision Transformer. deep learning models USED-FOR trustworthiness predictors. trustworthiness predictors EVALUATE-FOR loss. deep learning models EVALUATE-FOR loss. Vision Transformer HYPONYM-OF deep learning models. ResNet HYPONYM-OF deep learning models. loss USED-FOR trustworthiness predictors. Method are classifier, and AI models. Material are small - scale datasets, and ImageNet. Generic is task. Metric are data complexity, and generalizability of trustworthiness predictors. OtherScientificTerm is slide - like curves. ","This paper studies the problem of predicting trustworthiness on real-world large-scale datasets with high-dimensional features and visual concepts. This task is challenging due to the data complexity and the difficulty of training a classifier. The authors propose three prior-art loss functions for trustworthiness predictors: cross entropy loss, focal loss, and true class probability confidence loss. They also propose a new loss called steep slope loss to improve the generalizability of the features learned by the trustiness predictors. The proposed loss is evaluated on two popular deep learning models, Vision Transformer and ResNet, and shows that the proposed loss improves the generalization performance of the trustworthiness predictions.    The authors also show that the generalisation performance of AI models is not only improved on small-scale dataset, but also on ImageNet, where the slide-like curves are more informative. ","This paper studies the problem of predicting trustworthiness on real-world large-scale datasets with high-dimensional features and visual concepts. This task is challenging due to the data complexity and the difficulty of training a classifier. The authors propose three prior-art loss functions for trustworthiness predictors: cross entropy loss, focal loss, and true class probability confidence loss. They also propose a new loss called steep slope loss to improve the generalizability of the features learned by the trustiness predictors. The proposed loss is evaluated on two popular deep learning models, Vision Transformer and ResNet, and shows that the proposed loss improves the generalization performance of the trustworthiness predictions.    The authors also show that the generalisation performance of AI models is not only improved on small-scale dataset, but also on ImageNet, where the slide-like curves are more informative. "
9176,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"adversarial examples USED-FOR Adversarial robustness. robustness FEATURE-OF adversarial attacks. robust models USED-FOR adversarial attacks. robustness EVALUATE-FOR robust models. linear components USED-FOR adversarial robustness. batch normalization CONJUNCTION maximum pooling. maximum pooling CONJUNCTION batch normalization. maximum pooling CONJUNCTION activation layers. activation layers CONJUNCTION maximum pooling. activation layers HYPONYM-OF non - linear components. maximum pooling HYPONYM-OF non - linear components. batch normalization HYPONYM-OF non - linear components. domain adaption CONJUNCTION robustness boosting. robustness boosting CONJUNCTION domain adaption. it USED-FOR tasks. it USED-FOR domain adaption. it USED-FOR robustness boosting. robustness boosting HYPONYM-OF tasks. domain adaption HYPONYM-OF tasks. OtherScientificTerm are statistical properties, and linearized sub - networks. Method is clustering strategy. ","Adversarial robustness against adversarial examples is one of the most studied topics in machine learning. This paper studies the statistical properties of the robustness of existing robust models to adversarial attacks. The authors show that robust models with linear components (i.e. linearized sub-networks) are more robust to adversarially perturbed samples than robust models that have non-linear components (e.g. batch normalization, maximum pooling, activation layers, etc.). The authors also propose a new clustering strategy to further improve robustness. They show that it improves robustness on two tasks: domain adaption and robustness boosting. ","Adversarial robustness against adversarial examples is one of the most studied topics in machine learning. This paper studies the statistical properties of the robustness of existing robust models to adversarial attacks. The authors show that robust models with linear components (i.e. linearized sub-networks) are more robust to adversarially perturbed samples than robust models that have non-linear components (e.g. batch normalization, maximum pooling, activation layers, etc.). The authors also propose a new clustering strategy to further improve robustness. They show that it improves robustness on two tasks: domain adaption and robustness boosting. "
9201,SP:590b67b1278267e966cf0b31456d981441e61bb1,approach USED-FOR end - to - end reconstruction operators. unpaired training data USED-FOR ill - posed inverse problems. unpaired training data USED-FOR approach. expected distortion CONJUNCTION Wasserstein-1 distance. Wasserstein-1 distance CONJUNCTION expected distortion. variational framework CONJUNCTION iterative unrolling. iterative unrolling CONJUNCTION variational framework. measurement space FEATURE-OF expected distortion. variational framework USED-FOR method. regularizer USED-FOR variational setting. deep neural network USED-FOR regularizer. unrolled reconstruction operator USED-FOR regularizer. reconstruction network USED-FOR variational problem. it COMPARE variational methods. variational methods COMPARE it. unrolled operator USED-FOR initialization. initialization USED-FOR it. well - posedness CONJUNCTION noise - stability guarantees. noise - stability guarantees CONJUNCTION well - posedness. noise - stability guarantees FEATURE-OF variational setting. well - posedness FEATURE-OF variational setting. end - to - end unrolled reconstruction USED-FOR approach. well - posedness FEATURE-OF approach. well - posedness FEATURE-OF end - to - end unrolled reconstruction. it COMPARE supervised data - driven reconstruction approaches. supervised data - driven reconstruction approaches COMPARE it. approach COMPARE unsupervised methods. unsupervised methods COMPARE approach. approach COMPARE it. it COMPARE approach. approach COMPARE supervised data - driven reconstruction approaches. supervised data - driven reconstruction approaches COMPARE approach. OtherScientificTerm is reconstruction. Material is X - ray computed tomography ( CT ). ,"This paper proposes an approach to learn end-to-end reconstruction operators from unpaired training data for ill-posed inverse problems. The proposed method is based on a variational framework that combines the benefits of variational formulation and iterative unrolling. In particular, the authors consider the expected distortion in the measurement space and the Wasserstein-1 distance between two points in the reconstruction space. The authors propose a regularizer for the variational setting based on an unrolled reconstruction operator that is learned by a deep neural network. This regularizer is then applied to the reconstruction network that is trained to solve a particular variational problem. The paper shows that the proposed approach achieves state-of-the-art well-posedness and noise-stability guarantees in the end of the line setting, and it outperforms other variational methods that do not use the unrolled operator as an initialization. The approach is also shown to be able to achieve state- of-the art results in terms of well-posteriority and noise stability in the case of X-ray computed tomography (CT). Finally, the approach is shown to outperform other unsupervised methods and it is shown that the approach can also achieve state of the art performance in the setting of unrolled and supervised data-driven reconstruction.  ","This paper proposes an approach to learn end-to-end reconstruction operators from unpaired training data for ill-posed inverse problems. The proposed method is based on a variational framework that combines the benefits of variational formulation and iterative unrolling. In particular, the authors consider the expected distortion in the measurement space and the Wasserstein-1 distance between two points in the reconstruction space. The authors propose a regularizer for the variational setting based on an unrolled reconstruction operator that is learned by a deep neural network. This regularizer is then applied to the reconstruction network that is trained to solve a particular variational problem. The paper shows that the proposed approach achieves state-of-the-art well-posedness and noise-stability guarantees in the end of the line setting, and it outperforms other variational methods that do not use the unrolled operator as an initialization. The approach is also shown to be able to achieve state- of-the art results in terms of well-posteriority and noise stability in the case of X-ray computed tomography (CT). Finally, the approach is shown to outperform other unsupervised methods and it is shown that the approach can also achieve state of the art performance in the setting of unrolled and supervised data-driven reconstruction.  "
9226,SP:115d679338ab35829dbc594472d13cc02be5ed4c,Large - scale vision and language representation learning USED-FOR vision - language tasks. transformer - based multimodal encoder USED-FOR word tokens. transformer - based multimodal encoder USED-FOR methods. multimodal encoder USED-FOR image - text interactions. visual tokens CONJUNCTION word tokens. word tokens CONJUNCTION visual tokens. contrastive loss USED-FOR image and text representations. cross - modal attention USED-FOR vision and language representation learning. methods COMPARE method. method COMPARE methods. bounding box annotations CONJUNCTION high - resolution images. high - resolution images CONJUNCTION bounding box annotations. high - resolution images USED-FOR method. momentum distillation HYPONYM-OF self - training method. pseudo - targets USED-FOR self - training method. momentum model USED-FOR pseudo - targets. momentum model USED-FOR self - training method. downstream visionlanguage tasks EVALUATE-FOR ALBEF. ALBEF COMPARE methods. methods COMPARE ALBEF. image - text retrieval EVALUATE-FOR ALBEF. image - text retrieval EVALUATE-FOR methods. NLVR2 EVALUATE-FOR ALBEF. VQA EVALUATE-FOR ALBEF. ALBEF COMPARE state - ofthe - art. state - ofthe - art COMPARE ALBEF. VQA CONJUNCTION NLVR2. NLVR2 CONJUNCTION VQA. Material is noisy web data. Task is training tasks. OtherScientificTerm is image - text pair. ,"Large-scale vision and language representation learning is an important problem in vision-language tasks, especially in the presence of noisy web data. Previous methods have used a transformer-based multimodal encoder to encode word tokens and visual tokens, but this paper proposes to use a different method, called ALBEF, to learn image-text interactions between visual tokens and word tokens. The authors propose to use cross-modal attention as a key component of vision and text representation learning, and use a contrastive loss to learn the image and text representations. They show that the multimodality of their multimodals improves the performance of the training tasks. They also show that their method can be trained with bounding box annotations and high-resolution images, and outperform existing methods.  The authors also propose a self-training method called momentum distillation, which uses pseudo-targets from a momentum model to learn pseudo-targeting for each image-language pair. They evaluate their method on downstream visionlanguage tasks and show that AL BEF outperforms existing methods on image-to-text retrieval, VQA, and NLVR2.  They also compare their method to other methods that do not use the momentum model.","Large-scale vision and language representation learning is an important problem in vision-language tasks, especially in the presence of noisy web data. Previous methods have used a transformer-based multimodal encoder to encode word tokens and visual tokens, but this paper proposes to use a different method, called ALBEF, to learn image-text interactions between visual tokens and word tokens. The authors propose to use cross-modal attention as a key component of vision and text representation learning, and use a contrastive loss to learn the image and text representations. They show that the multimodality of their multimodals improves the performance of the training tasks. They also show that their method can be trained with bounding box annotations and high-resolution images, and outperform existing methods.  The authors also propose a self-training method called momentum distillation, which uses pseudo-targets from a momentum model to learn pseudo-targeting for each image-language pair. They evaluate their method on downstream visionlanguage tasks and show that AL BEF outperforms existing methods on image-to-text retrieval, VQA, and NLVR2.  They also compare their method to other methods that do not use the momentum model."
9251,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,Markov decision processes ( MDPs ) USED-FOR offline policy evaluation ( OPE ). static datasets USED-FOR decisionmaking policies. OPE COMPARE realizable setting. realizable setting COMPARE OPE. unrealizability USED-FOR OPE. unrealizability USED-FOR OPE method. linear direct method ( DM ) HYPONYM-OF OPE method. doubly robust form FEATURE-OF OPE error. nonparametric consistency FEATURE-OF tile - coding estimators. OtherScientificTerm is approximate ) realizability assumptions. Method is hypothetical models. Task is real - world applications. ,"This paper studies offline policy evaluation (OPE) in Markov decision processes (MDPs) under (approximate) realizability assumptions. In particular, the authors consider decisionmaking policies on static datasets, where the goal is to evaluate the performance of a policy in the offline setting. The authors show that the OPE error in this setting has a doubly robust form, which implies that the unrealizability of an OPE method (i.e., a linear direct method (DM) based on the notion of ""unrealizability"") is lower than in the realizable setting. They also show that tile-coding estimators with nonparametric consistency are more robust to (potentially) incorrect (or incorrect) assumptions.   The authors also provide some theoretical analysis of their results, and show that their results hold for hypothetical models and for real-world applications. ","This paper studies offline policy evaluation (OPE) in Markov decision processes (MDPs) under (approximate) realizability assumptions. In particular, the authors consider decisionmaking policies on static datasets, where the goal is to evaluate the performance of a policy in the offline setting. The authors show that the OPE error in this setting has a doubly robust form, which implies that the unrealizability of an OPE method (i.e., a linear direct method (DM) based on the notion of ""unrealizability"") is lower than in the realizable setting. They also show that tile-coding estimators with nonparametric consistency are more robust to (potentially) incorrect (or incorrect) assumptions.   The authors also provide some theoretical analysis of their results, and show that their results hold for hypothetical models and for real-world applications. "
9276,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"stochastic 1 first - order methods USED-FOR large - scale machine learning models. theoretical guarantees FEATURE-OF expectation of the objective value. algorithms USED-FOR small objective residual. Existing methods USED-FOR non - smooth stochastic convex optimization. complexity 7 bounds FEATURE-OF Existing methods. logarithmic dependence FEATURE-OF high - probability convergence. stepsize rules USED-FOR stochastic methods. gradient 14 clipping USED-FOR stochastic methods. extension USED-FOR strongly convex problems. Hölder - continuous gradients FEATURE-OF generalized smooth objectives. extension USED-FOR methods. non - smooth setting FEATURE-OF one. iteration and oracle complexity EVALUATE-FOR accelerated ) 17 method. OtherScientificTerm are Random behavior, suboptimal objective value, confidence level, and negative - power. Generic is algorithm. Task are NLP tasks, and non - smooth convex stochastic 12 optimization problems. ","This paper studies stochastic 1 first-order methods for training large-scale machine learning models. Existing methods have been shown to have complexity 7 bounds for non-smooth convex convex optimization, and theoretical guarantees on the expectation of the objective value. Random behavior of the algorithm is studied, and it is shown that existing algorithms for small objective residual can converge to a suboptimal objective value with high-probability convergence with logarithmic dependence on the confidence level.    The main contribution of this paper is to extend the results of previous work on the stepsize rules for stochastastic methods based on gradient 14 clipping to the case of non-convex 1-order algorithms. This extension is applicable to strongly convex problems, and is shown to be equivalent to existing methods.  The authors also extend this extension to generalized smooth objectives with Hölder-continuous gradients, and show that these methods converge to the optimal one in a non-milder setting.  Finally, the authors show that the (accelerated) 17 method has the same iteration and oracle complexity as the previous work, and that the algorithm can be applied to several NLP tasks.  This paper also provides theoretical results on the (adversarial) convergence of the algorithms.  In particular, it shows that the algorithms converge to an optimal solution with high probability under the assumption that the negative-power of the loss is equal to the number of samples, and shows that this assumption holds for the non-asymptotic non-vanilla non-strongly-concavex non-monotonic non-divergence non-linear non-trivial non-uniform non-parametric non-stochastic (non-convergence).   In addition, this paper also shows theoretical results for the case where the nonlinearity of the losses is nonlinear, and provides a theoretical analysis of the (approximated) convergence rate. ","This paper studies stochastic 1 first-order methods for training large-scale machine learning models. Existing methods have been shown to have complexity 7 bounds for non-smooth convex convex optimization, and theoretical guarantees on the expectation of the objective value. Random behavior of the algorithm is studied, and it is shown that existing algorithms for small objective residual can converge to a suboptimal objective value with high-probability convergence with logarithmic dependence on the confidence level.    The main contribution of this paper is to extend the results of previous work on the stepsize rules for stochastastic methods based on gradient 14 clipping to the case of non-convex 1-order algorithms. This extension is applicable to strongly convex problems, and is shown to be equivalent to existing methods.  The authors also extend this extension to generalized smooth objectives with Hölder-continuous gradients, and show that these methods converge to the optimal one in a non-milder setting.  Finally, the authors show that the (accelerated) 17 method has the same iteration and oracle complexity as the previous work, and that the algorithm can be applied to several NLP tasks.  This paper also provides theoretical results on the (adversarial) convergence of the algorithms.  In particular, it shows that the algorithms converge to an optimal solution with high probability under the assumption that the negative-power of the loss is equal to the number of samples, and shows that this assumption holds for the non-asymptotic non-vanilla non-strongly-concavex non-monotonic non-divergence non-linear non-trivial non-uniform non-parametric non-stochastic (non-convergence).   In addition, this paper also shows theoretical results for the case where the nonlinearity of the losses is nonlinear, and provides a theoretical analysis of the (approximated) convergence rate. "
9301,SP:a22a893e25ce739dc757861741014764e78aa820,"extreme weather early warning CONJUNCTION long - term energy consumption planning. long - term energy consumption planning CONJUNCTION extreme weather early warning. self - attention mechanisms USED-FOR long - range dependencies. Transformerbased models USED-FOR long - range dependencies. self - attention mechanisms USED-FOR Transformerbased models. point - wise self - attentions USED-FOR long series efficiency. point - wise self - attentions USED-FOR Transformers. decomposition architecture USED-FOR Autoformer. Auto - Correlation mechanism USED-FOR decomposition architecture. Auto - Correlation mechanism USED-FOR Autoformer. pre - processing convention PART-OF series decomposition. design USED-FOR Autoformer. Autoformer USED-FOR complex time series. progressive decomposition capacities USED-FOR complex time series. progressive decomposition capacities FEATURE-OF Autoformer. dependencies discovery CONJUNCTION representation aggregation. representation aggregation CONJUNCTION dependencies discovery. stochastic process theory USED-FOR Auto - Correlation mechanism. series periodicity USED-FOR dependencies discovery. representation aggregation PART-OF sub - series level. series periodicity USED-FOR Auto - Correlation mechanism. Auto - Correlation COMPARE self - attention. self - attention COMPARE Auto - Correlation. efficiency EVALUATE-FOR Auto - Correlation. accuracy EVALUATE-FOR Auto - Correlation. traffic CONJUNCTION economics. economics CONJUNCTION traffic. energy CONJUNCTION traffic. traffic CONJUNCTION energy. energy CONJUNCTION economics. economics CONJUNCTION energy. long - term forecasting EVALUATE-FOR Autoformer. practical applications FEATURE-OF benchmarks. energy HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR Autoformer. accuracy EVALUATE-FOR Autoformer. traffic HYPONYM-OF benchmarks. economics HYPONYM-OF benchmarks. energy HYPONYM-OF practical applications. traffic HYPONYM-OF practical applications. economics HYPONYM-OF practical applications. OtherScientificTerm are forecasting time, and information utilization bottleneck. Task is long - term forecasting problem of time series. Generic are model, and it. Method is deep models. ","This paper proposes a new decomposition architecture for Transformer-based time series models. The authors propose Auto-Correlation, a decomposition-based model for long-range time series forecasting. Auto-correlation is based on stochastic process theory and is able to decompose time series into sub-series, which can be used to improve the long-term forecasting performance of Transformer based models.   The authors also propose a new pre-processing convention for the series decomposition, which is similar to the one used in self-attention mechanisms used in Transformerbased models to capture long term dependencies in time series. The main difference is that instead of using point-wise self attentions for long series efficiency, the authors propose to use Transformers, which use a series of self-distributions. The proposed design allows Autoformer to handle complex time series with progressive decomposition capacities, which are more suitable for the forecasting problem of time series, where the forecasting time can be extended. The Auto- Correlation mechanism uses series periodicity, dependencies discovery, and representation aggregation at the sub- series level.  The proposed Autoformer achieves state-of-the-art accuracy on several benchmarks in practical applications such as energy, traffic, economics, and long- term energy consumption planning (e.g. extreme weather early warning and long term energy usage planning). The authors show that Auto-relation achieves better efficiency than the standard Self-Attention (SOTA) model, and is more robust to the information utilization bottleneck. The paper also shows that the proposed by the authors outperforms deep models in terms of accuracy. ","This paper proposes a new decomposition architecture for Transformer-based time series models. The authors propose Auto-Correlation, a decomposition-based model for long-range time series forecasting. Auto-correlation is based on stochastic process theory and is able to decompose time series into sub-series, which can be used to improve the long-term forecasting performance of Transformer based models.   The authors also propose a new pre-processing convention for the series decomposition, which is similar to the one used in self-attention mechanisms used in Transformerbased models to capture long term dependencies in time series. The main difference is that instead of using point-wise self attentions for long series efficiency, the authors propose to use Transformers, which use a series of self-distributions. The proposed design allows Autoformer to handle complex time series with progressive decomposition capacities, which are more suitable for the forecasting problem of time series, where the forecasting time can be extended. The Auto- Correlation mechanism uses series periodicity, dependencies discovery, and representation aggregation at the sub- series level.  The proposed Autoformer achieves state-of-the-art accuracy on several benchmarks in practical applications such as energy, traffic, economics, and long- term energy consumption planning (e.g. extreme weather early warning and long term energy usage planning). The authors show that Auto-relation achieves better efficiency than the standard Self-Attention (SOTA) model, and is more robust to the information utilization bottleneck. The paper also shows that the proposed by the authors outperforms deep models in terms of accuracy. "
9326,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,NLP systems USED-FOR compositional language. Cryptic crosswords HYPONYM-OF dominant crossword variety. character - level manipulations USED-FOR wordplay cipher. creative intelligence USED-FOR cryptics. NLP systems USED-FOR compositional language. cryptic clues USED-FOR NLP systems. dataset USED-FOR NLP systems. dataset USED-FOR compositional language. cryptic clues FEATURE-OF dataset. model USED-FOR tasks. T5 HYPONYM-OF neural language model. non - neural approaches CONJUNCTION T5. T5 CONJUNCTION non - neural approaches. unscrambling words HYPONYM-OF tasks. meta - linguistic capabilities FEATURE-OF subword - tokenized models. T5 COMPARE human solving strategies. human solving strategies COMPARE T5. wordplay part of clues USED-FOR model systematicity. curricular approach COMPARE T5 baseline. T5 baseline COMPARE curricular approach. cryptic crosswords PART-OF NLP systems. OtherScientificTerm is Cryptic clues. Method is curriculum approach. ,"This paper presents a new dataset of cryptic clues for NLP systems to learn a compositional language. Cryptic crosswords are the dominant crossword variety, and cryptics are a form of creative intelligence that uses character-level manipulations to create a wordplay cipher. The authors present a dataset with cryptic clues that can be used to train a number of NLP networks to learn compositional languages.    The authors train a neural language model, called T5, on this dataset, and show that the model is able to solve a variety of tasks, including unscrambling words. They also show that non-neural approaches and T5 outperform human solving strategies, and that a curricular approach is more effective than the T5 baseline.  They also demonstrate that subword-tokenized models with meta-linguistic capabilities are able to learn from cryptic clues, and a curriculum approach is used to learn the wordplay part of clues to improve model systematicity. ","This paper presents a new dataset of cryptic clues for NLP systems to learn a compositional language. Cryptic crosswords are the dominant crossword variety, and cryptics are a form of creative intelligence that uses character-level manipulations to create a wordplay cipher. The authors present a dataset with cryptic clues that can be used to train a number of NLP networks to learn compositional languages.    The authors train a neural language model, called T5, on this dataset, and show that the model is able to solve a variety of tasks, including unscrambling words. They also show that non-neural approaches and T5 outperform human solving strategies, and that a curricular approach is more effective than the T5 baseline.  They also demonstrate that subword-tokenized models with meta-linguistic capabilities are able to learn from cryptic clues, and a curriculum approach is used to learn the wordplay part of clues to improve model systematicity. "
9351,SP:7693974b70806d9b67920b8ddd2335afc4883319,"Convolutional neural networks ( CNNs ) USED-FOR visual data. image classification tasks EVALUATE-FOR ( Vision ) Transformer models ( ViT ). Vision Transformers USED-FOR tasks. ViTs CONJUNCTION CNNs. CNNs CONJUNCTION ViTs. internal representation structure USED-FOR ViTs. internal representation structure USED-FOR CNNs. uniform representations USED-FOR ViT. image classification benchmarks EVALUATE-FOR internal representation structure. image classification benchmarks EVALUATE-FOR ViTs. ViT HYPONYM-OF architectures. image classification benchmarks EVALUATE-FOR CNNs. self - attention CONJUNCTION ViT residual connections. ViT residual connections CONJUNCTION self - attention. self - attention USED-FOR aggregation of global information. ViTs USED-FOR input spatial information. intermediate features CONJUNCTION transfer learning. transfer learning CONJUNCTION intermediate features. ( pretraining ) dataset scale USED-FOR transfer learning. ( pretraining ) dataset scale USED-FOR intermediate features. MLP - Mixer HYPONYM-OF architectures. Generic is they. Method are convolutional networks, and classification methods. OtherScientificTerm are visual representations, and features. Task are early aggregation of global information, and spatial localization. ","This paper investigates the internal representation structure of (Vision) Transformer models (ViT) on image classification tasks. The authors show that Vision Transformers (ViTs) outperform convolutional networks (CNNs) on a variety of tasks. They also show that ViTs and CNNs have similar internal representation structures, and that they can be seen as early aggregation of global information.    The paper also shows that ViT can learn uniform representations that are more robust to changes in the input spatial location, which is an important property of classification methods. The paper further shows that CNNs and ViTs share a common structure in their visual representations.  On image classification benchmarks, ViTs outperform CNNs on a number of image classification datasets.  The authors further show that, in contrast to CNNs, ViT is able to learn a uniform representation that is more robust than uniform representations learned by CNNs.  Finally, the paper shows that, for classification methods trained on a large number of datasets, the ViT learns a more complex representation than CNNs in terms of spatial localization.  In particular, they show that the self-attention and ViT residual connections are responsible for the aggregation of local information, while CNNs learn a more uniform representation.  They also find that, when ViTs are trained on large enough datasets, they can learn a better representation of input spatial information, which can be used to improve classification performance.  Lastly, they find that the (pretraining) dataset scale is important for intermediate features, transfer learning, and the (pre-training) dataset size is also important for the (transfer learning) performance of ViTs.  To further improve the performance, the authors propose two new architectures, namely MLP and MLP-Mixer.","This paper investigates the internal representation structure of (Vision) Transformer models (ViT) on image classification tasks. The authors show that Vision Transformers (ViTs) outperform convolutional networks (CNNs) on a variety of tasks. They also show that ViTs and CNNs have similar internal representation structures, and that they can be seen as early aggregation of global information.    The paper also shows that ViT can learn uniform representations that are more robust to changes in the input spatial location, which is an important property of classification methods. The paper further shows that CNNs and ViTs share a common structure in their visual representations.  On image classification benchmarks, ViTs outperform CNNs on a number of image classification datasets.  The authors further show that, in contrast to CNNs, ViT is able to learn a uniform representation that is more robust than uniform representations learned by CNNs.  Finally, the paper shows that, for classification methods trained on a large number of datasets, the ViT learns a more complex representation than CNNs in terms of spatial localization.  In particular, they show that the self-attention and ViT residual connections are responsible for the aggregation of local information, while CNNs learn a more uniform representation.  They also find that, when ViTs are trained on large enough datasets, they can learn a better representation of input spatial information, which can be used to improve classification performance.  Lastly, they find that the (pretraining) dataset scale is important for intermediate features, transfer learning, and the (pre-training) dataset size is also important for the (transfer learning) performance of ViTs.  To further improve the performance, the authors propose two new architectures, namely MLP and MLP-Mixer."
9376,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"Thompson sampling ( TS ) USED-FOR bandit area. approximation oracle USED-FOR TS. convergence analysis USED-FOR TS. exact oracle PART-OF CMAB. greedy oracle HYPONYM-OF common ( approximation ) oracle. theoretical guarantees FEATURE-OF common ( approximation ) oracle. TS USED-FOR CMAB problems. problemdependent regret lower bound USED-FOR TS. greedy oracle USED-FOR TS. TS USED-FOR CMAB. approximation oracles USED-FOR TS. Generic are It, and oracle. OtherScientificTerm are optimal solutions, reward gap, and almost matching regret upper bound. Task is combinatorial optimization problems. ","This paper studies Thompson sampling (TS) in the bandit area for combinatorial optimization problems. The authors provide a convergence analysis for TS based on the approximation oracle of a common (approximation) oracle (i.e., greedy oracle) with theoretical guarantees. It is shown that TS for CMAB problems with optimal solutions can be approximated by a common oracle, and that the exact oracle is a common one.  The authors also provide a problemdependent regret lower bound for TS for TS with a simple oracle. They also show that TS with TS with approximation oracles is almost matching the regret upper bound of the exact (oracle-free) bandit algorithm. ","This paper studies Thompson sampling (TS) in the bandit area for combinatorial optimization problems. The authors provide a convergence analysis for TS based on the approximation oracle of a common (approximation) oracle (i.e., greedy oracle) with theoretical guarantees. It is shown that TS for CMAB problems with optimal solutions can be approximated by a common oracle, and that the exact oracle is a common one.  The authors also provide a problemdependent regret lower bound for TS for TS with a simple oracle. They also show that TS with TS with approximation oracles is almost matching the regret upper bound of the exact (oracle-free) bandit algorithm. "
9401,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"Federated learning HYPONYM-OF distributed learning paradigm. accuracy rates EVALUATE-FOR federated learning. total error HYPONYM-OF social good properties. hedonic game USED-FOR federated learning. average error rates USED-FOR optimality. algorithm USED-FOR optimal ( error minimizing ) arrangement of players. stability CONJUNCTION optimality. optimality CONJUNCTION stability. stability EVALUATE-FOR arrangement. optimality EVALUATE-FOR arrangement. Method are global model, and game - theoretic approach. OtherScientificTerm are error - minimizing players, federating coalitions, stable coalition partitions, stable arrangements, Price of Anarchy, and constant - factor bound. Generic is stable solutions. ","This paper studies the problem of federated learning, a distributed learning paradigm in which a global model is shared across workers and the goal is to achieve high accuracy rates. The authors consider the social good properties, i.e., the total error, of the population of error-minimizing players, and consider the hedonic game in which the optimal (error minimizing) arrangement of players is the one that maximizes the average error rates of all players.  The authors propose a game-theoretic approach to this problem, which is based on the idea of federating coalitions. They propose an algorithm for finding the optimal (""stability"") arrangement, which they call the Price of Anarchy, and prove a constant-factor bound on the number of players in the federated coalitions, and show that the algorithm converges to a stable coalition of players. They show that this stability and optimality can be achieved by using average error lower bounds on the average errors of the players, as well as stable coalition partitions. They also show that for stable solutions, the algorithm can converge to a set of stable arrangements.   ","This paper studies the problem of federated learning, a distributed learning paradigm in which a global model is shared across workers and the goal is to achieve high accuracy rates. The authors consider the social good properties, i.e., the total error, of the population of error-minimizing players, and consider the hedonic game in which the optimal (error minimizing) arrangement of players is the one that maximizes the average error rates of all players.  The authors propose a game-theoretic approach to this problem, which is based on the idea of federating coalitions. They propose an algorithm for finding the optimal (""stability"") arrangement, which they call the Price of Anarchy, and prove a constant-factor bound on the number of players in the federated coalitions, and show that the algorithm converges to a stable coalition of players. They show that this stability and optimality can be achieved by using average error lower bounds on the average errors of the players, as well as stable coalition partitions. They also show that for stable solutions, the algorithm can converge to a set of stable arrangements.   "
9426,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"self - supervised capsule architecture USED-FOR 3D point clouds. capsule decompositions of objects USED-FOR capsule decompositions. permutation - equivariant attention USED-FOR capsule decompositions of objects. these USED-FOR decomposition. capsule invariance / equivariance properties FEATURE-OF decomposition. canonicalization operation USED-FOR object - centric reasoning. classification labels CONJUNCTION manually - aligned training datasets. manually - aligned training datasets CONJUNCTION classification labels. classification labels USED-FOR neural network. manually - aligned training datasets USED-FOR neural network. canonicalization CONJUNCTION unsupervised classification. unsupervised classification CONJUNCTION canonicalization. method COMPARE state - of - the - art. state - of - the - art COMPARE method. 3D point cloud reconstruction CONJUNCTION canonicalization. canonicalization CONJUNCTION 3D point cloud reconstruction. object - centric representation USED-FOR method. canonicalization EVALUATE-FOR method. unsupervised classification EVALUATE-FOR method. 3D point cloud reconstruction EVALUATE-FOR state - of - the - art. 3D point cloud reconstruction EVALUATE-FOR method. self - supervised manner USED-FOR object - centric representation. Generic is process. OtherScientificTerm are attention masks, and semantic keypoints. Method is semantically consistent decomposition. ","This paper proposes a self-supervised capsule architecture for 3D point clouds. The capsule decompositions of objects are learned via permutation-equivariant attention, and these are then used to learn a decomposition that satisfies capsule invariance/equivariance properties. The authors also propose a canonicalization operation for object-centric reasoning. The whole process can be seen as an extension of attention masks. The neural network is trained with classification labels and manually-aligned training datasets. Experiments show that the proposed method outperforms the state-of-the-art on 3d point cloud reconstruction, canonicalization, and unsupervised classification. In addition, the authors demonstrate that the method can also learn an object-centric representation in an efficient and efficient way, by learning a semantically consistent decomposition from semantic keypoints. The proposed method is also able to learn in a self supervised manner, which is a nice contribution to the field.","This paper proposes a self-supervised capsule architecture for 3D point clouds. The capsule decompositions of objects are learned via permutation-equivariant attention, and these are then used to learn a decomposition that satisfies capsule invariance/equivariance properties. The authors also propose a canonicalization operation for object-centric reasoning. The whole process can be seen as an extension of attention masks. The neural network is trained with classification labels and manually-aligned training datasets. Experiments show that the proposed method outperforms the state-of-the-art on 3d point cloud reconstruction, canonicalization, and unsupervised classification. In addition, the authors demonstrate that the method can also learn an object-centric representation in an efficient and efficient way, by learning a semantically consistent decomposition from semantic keypoints. The proposed method is also able to learn in a self supervised manner, which is a nice contribution to the field."
9451,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,conformal method USED-FOR prediction intervals. prediction intervals USED-FOR nonparametric regression. conformal method USED-FOR nonparametric regression. black - box machine learning algorithms USED-FOR conditional distribution. approximate conditional coverage FEATURE-OF prediction intervals. histograms USED-FOR black - box machine learning algorithms. conditional coverage CONJUNCTION optimal length. optimal length CONJUNCTION conditional coverage. finite samples FEATURE-OF marginal coverage. marginal coverage FEATURE-OF prediction intervals. conformalized quantile regression CONJUNCTION distributional conformal prediction approaches. distributional conformal prediction approaches CONJUNCTION conformalized quantile regression. simulated and real data EVALUATE-FOR state - of - the - art alternatives. distributional conformal prediction approaches HYPONYM-OF state - of - the - art alternatives. conformalized quantile regression HYPONYM-OF state - of - the - art alternatives. Material is skewed data. Method is black - box model. ,"This paper proposes a conformal method to learn prediction intervals for nonparametric regression based on the approximate conditional coverage of black-box machine learning algorithms trained on histograms. The authors show that the conditional coverage and optimal length of the prediction intervals can be learned from finite samples. They also show that for skewed data, the marginal coverage of the predicted intervals in terms of marginal coverage on finite samples is a function of the number of samples and the size of the conditional distribution. The paper also shows that conformalized quantile regression and distributional conformal prediction approaches outperform state-of-the-art alternatives on simulated and real data.   ","This paper proposes a conformal method to learn prediction intervals for nonparametric regression based on the approximate conditional coverage of black-box machine learning algorithms trained on histograms. The authors show that the conditional coverage and optimal length of the prediction intervals can be learned from finite samples. They also show that for skewed data, the marginal coverage of the predicted intervals in terms of marginal coverage on finite samples is a function of the number of samples and the size of the conditional distribution. The paper also shows that conformalized quantile regression and distributional conformal prediction approaches outperform state-of-the-art alternatives on simulated and real data.   "
9476,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"invariance USED-FOR generalisation. incorporating invariance PART-OF kernel ridge regression. effective dimension USED-FOR generalisation. feature averaging USED-FOR invariance. reproducing kernel Hilbert space CONJUNCTION kernel. kernel CONJUNCTION reproducing kernel Hilbert space. Generic is approach. OtherScientificTerm are function space perspective, and group. ","This paper studies the problem of incorporating invariance in kernel ridge regression. The authors show that invariance to generalisation via feature averaging can be obtained by incorporating the effective dimension of the group into the objective function. The approach is motivated from a function space perspective, where the reproducing kernel Hilbert space and the kernel are assumed to be the same group.   ","This paper studies the problem of incorporating invariance in kernel ridge regression. The authors show that invariance to generalisation via feature averaging can be obtained by incorporating the effective dimension of the group into the objective function. The approach is motivated from a function space perspective, where the reproducing kernel Hilbert space and the kernel are assumed to be the same group.   "
9501,SP:97fac361b69ed5871a60dc40e51900747a453df9,"assertion statements USED-FOR erroneous behavior. software programs USED-FOR they. applications EVALUATE-FOR deep learning programs. generative model USED-FOR neural network activations. DecNN HYPONYM-OF Decodable Neural Network. DecNN USED-FOR ensemble - like model. compositionality FEATURE-OF neural networks. uncertainty FEATURE-OF ensemble - like model. out - of - distribution detection CONJUNCTION adversarial example detection. adversarial example detection CONJUNCTION out - of - distribution detection. adversarial example detection CONJUNCTION calibration. calibration CONJUNCTION adversarial example detection. uncertainty USED-FOR out - of - distribution detection. uncertainty USED-FOR adversarial example detection. uncertainty USED-FOR calibration. accuracy EVALUATE-FOR neural networks. DecNN CONJUNCTION pretrained models. pretrained models CONJUNCTION DecNN. protected features USED-FOR neural networks. OtherScientificTerm is program logic. Generic are programs, and design. ","This paper proposes a Decodable Neural Network (DecNN), which is a generative model for neural network activations that can be interpreted as a program logic. The authors claim that they are able to learn software programs that are interpretable and interpretable for a variety of applications, including out-of-distribution detection, adversarial example detection, and calibration. In particular, they show that a DecNN can be used as an ensemble-like model with high uncertainty about the compositionality of the neural networks. They also show that assertion statements can capture the erroneous behavior of a program when the design of the program is not well-understood. They show that DecNN and other pretrained models can be seen as a way to improve the accuracy of neural networks trained with protected features. ","This paper proposes a Decodable Neural Network (DecNN), which is a generative model for neural network activations that can be interpreted as a program logic. The authors claim that they are able to learn software programs that are interpretable and interpretable for a variety of applications, including out-of-distribution detection, adversarial example detection, and calibration. In particular, they show that a DecNN can be used as an ensemble-like model with high uncertainty about the compositionality of the neural networks. They also show that assertion statements can capture the erroneous behavior of a program when the design of the program is not well-understood. They show that DecNN and other pretrained models can be seen as a way to improve the accuracy of neural networks trained with protected features. "
9526,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"Optimal transport maps USED-FOR machine learning and statistics. probability distributions FEATURE-OF Optimal transport maps. Plugin estimators USED-FOR transport maps. Plugin estimators USED-FOR computational optimal transport. rates of convergences EVALUATE-FOR plug - in estimators. barycentric projections USED-FOR plug - in estimators. stability estimate USED-FOR plug - in estimators. stability estimate USED-FOR barycentric projections. rates of convergence EVALUATE-FOR plug - in estimators. rates of convergence FEATURE-OF Wasserstein barycenter. asymptotic detection thresholds USED-FOR optimaltransport based tests of independence. probability distributions FEATURE-OF Wasserstein barycenter. Generic is maps. OtherScientificTerm are minimal smoothness assumptions, smoothness assumptions, curse of dimensionality, and Wasserstein distance. ","Optimal transport maps for machine learning and statistics are defined as the probability distributions of the optimal transport maps between two points in the space of probability distributions. Plug-in and plugin estimators for computing the transport maps are proposed.    The authors show that plug-in estimators based on barycentric projections based on a stability estimate converge to the Wasserstein barycenter with rates of convergences of order $O(1/\sqrt{n})$ under minimal smoothness assumptions. They also provide asymptotic detection thresholds for optimaltransport based tests of independence under the curse of dimensionality. The authors also show that the rates of convergence of plug-ins based on the barycenters of the two probability distributions can be improved if the smoothness assumption is removed.  The main contribution of the paper is to show that under certain conditions, plug- in estimators that rely on the stability estimate can converge to an optimal transport map that is independent of the number of points under which the maps are drawn. ","Optimal transport maps for machine learning and statistics are defined as the probability distributions of the optimal transport maps between two points in the space of probability distributions. Plug-in and plugin estimators for computing the transport maps are proposed.    The authors show that plug-in estimators based on barycentric projections based on a stability estimate converge to the Wasserstein barycenter with rates of convergences of order $O(1/\sqrt{n})$ under minimal smoothness assumptions. They also provide asymptotic detection thresholds for optimaltransport based tests of independence under the curse of dimensionality. The authors also show that the rates of convergence of plug-ins based on the barycenters of the two probability distributions can be improved if the smoothness assumption is removed.  The main contribution of the paper is to show that under certain conditions, plug- in estimators that rely on the stability estimate can converge to an optimal transport map that is independent of the number of points under which the maps are drawn. "
9551,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,training efficiency CONJUNCTION useful feature extraction. useful feature extraction CONJUNCTION training efficiency. useful feature extraction EVALUATE-FOR dataset distillation methods. training efficiency EVALUATE-FOR dataset distillation methods. distributed kernel - based meta - learning framework USED-FOR dataset distillation. infinitely wide convolutional neural networks USED-FOR distributed kernel - based meta - learning framework. infinitely wide convolutional neural networks USED-FOR dataset distillation. test accuracy EVALUATE-FOR CIFAR10 image classification task. Fashion - MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Fashion - MNIST. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. Fashion - MNIST CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION Fashion - MNIST. MNIST HYPONYM-OF settings. Fashion - MNIST HYPONYM-OF settings. CIFAR-100 HYPONYM-OF settings. CIFAR-10 HYPONYM-OF settings. they COMPARE naturally occurring data. naturally occurring data COMPARE they. distilled datasets COMPARE naturally occurring data. naturally occurring data COMPARE distilled datasets. Method is machine learning algorithms. ,"This paper proposes a distributed kernel-based meta-learning framework for dataset distillation based on infinitely wide convolutional neural networks to improve training efficiency and useful feature extraction. The authors demonstrate the effectiveness of the proposed distillation methods in terms of test accuracy on the CIFAR10 image classification task. They also show that the distilled datasets outperform naturally occurring data when they are used in conjunction with existing machine learning algorithms. They demonstrate this in three settings: MNIST, Fashion-MNIST, CIFar-10, and SVHN. ","This paper proposes a distributed kernel-based meta-learning framework for dataset distillation based on infinitely wide convolutional neural networks to improve training efficiency and useful feature extraction. The authors demonstrate the effectiveness of the proposed distillation methods in terms of test accuracy on the CIFAR10 image classification task. They also show that the distilled datasets outperform naturally occurring data when they are used in conjunction with existing machine learning algorithms. They demonstrate this in three settings: MNIST, Fashion-MNIST, CIFar-10, and SVHN. "
9576,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,Semi - supervised learning ( SSL ) USED-FOR model. unlabeled data USED-FOR model. unlabeled data USED-FOR Semi - supervised learning ( SSL ). label space FEATURE-OF labeled and unlabeled data. FixMatch HYPONYM-OF SSL methods. Learning representations of inliers USED-FOR OSSL. FixMatch CONJUNCTION novelty detection. novelty detection CONJUNCTION FixMatch. OpenMatch USED-FOR FixMatch. OpenMatch CONJUNCTION novelty detection. novelty detection CONJUNCTION OpenMatch. threshold USED-FOR outliers. OVA - classifier USED-FOR confidence score. open - set soft - consistency regularization loss USED-FOR outlier detection. smoothness FEATURE-OF OVA - classifier. open - set soft - consistency regularization loss USED-FOR smoothness. OpenMatch COMPARE supervised model. supervised model COMPARE OpenMatch. CIFAR10 EVALUATE-FOR supervised model. Method is SSL algorithms. ,"Semi-supervised learning (SSL) uses unlabeled data to train a model with unlabeling data. Semi-supervision is a popular technique for learning a model from unlabelled data. However, existing SSL methods, such as FixMatch and OpenMatch, rely on the assumption that the label space of the labeled and unlabelED data is the same. Learning representations of inliers in OSSL is a common technique in SSL algorithms. In this paper, the authors propose to use OpenMatch instead of FixMatch to learn representations of outliers. They also propose a threshold to detect outliers. The authors also propose an open-set soft-consistency regularization loss to improve the smoothness of the OVA-classifier used for outlier detection. Experiments on CIFAR10 show that OpenMatch outperforms a supervised model trained on the same amount of labeled data. The main contribution of the paper is the introduction of an OVA - classifier that is used to compute the confidence score of an outlier.   ","Semi-supervised learning (SSL) uses unlabeled data to train a model with unlabeling data. Semi-supervision is a popular technique for learning a model from unlabelled data. However, existing SSL methods, such as FixMatch and OpenMatch, rely on the assumption that the label space of the labeled and unlabelED data is the same. Learning representations of inliers in OSSL is a common technique in SSL algorithms. In this paper, the authors propose to use OpenMatch instead of FixMatch to learn representations of outliers. They also propose a threshold to detect outliers. The authors also propose an open-set soft-consistency regularization loss to improve the smoothness of the OVA-classifier used for outlier detection. Experiments on CIFAR10 show that OpenMatch outperforms a supervised model trained on the same amount of labeled data. The main contribution of the paper is the introduction of an OVA - classifier that is used to compute the confidence score of an outlier.   "
9601,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"it USED-FOR achiever policy. it USED-FOR explorer. explorer CONJUNCTION achiever policy. achiever policy CONJUNCTION explorer. Latent Explorer Achiever ( LEXA ) HYPONYM-OF agent. imagined rollouts USED-FOR it. image inputs USED-FOR world model. imagined rollouts USED-FOR achiever policy. prior methods COMPARE explorer. explorer COMPARE prior methods. LEXA USED-FOR tasks. goal images zero - shot FEATURE-OF tasks. approaches USED-FOR unsupervised goal reaching. LEXA COMPARE approaches. approaches COMPARE LEXA. LEXA USED-FOR unsupervised goal reaching. prior benchmarks CONJUNCTION benchmark. benchmark CONJUNCTION prior benchmarks. test tasks EVALUATE-FOR benchmark. robotic manipulation and locomotion domains FEATURE-OF test tasks. benchmark EVALUATE-FOR approaches. test tasks EVALUATE-FOR LEXA. benchmark EVALUATE-FOR LEXA. prior benchmarks EVALUATE-FOR approaches. prior benchmarks EVALUATE-FOR LEXA. Method is artificial agents. OtherScientificTerm are complex visual environments, supervision, and achiever. ","This paper proposes Latent Explorer Achiever (LEXA), an agent that is able to explore complex visual environments without the need for supervision. The agent is trained to be able to reach distant goals in an unsupervised way, and it learns an explorer and an achiever policy. The achiever is trained using imagined rollouts, and the world model is trained on image inputs. LEXA is shown to outperform prior methods on a number of tasks on goal images zero-shot, and outperforms existing approaches for unsupervision goal reaching on a variety of test tasks in robotic manipulation and locomotion domains. ","This paper proposes Latent Explorer Achiever (LEXA), an agent that is able to explore complex visual environments without the need for supervision. The agent is trained to be able to reach distant goals in an unsupervised way, and it learns an explorer and an achiever policy. The achiever is trained using imagined rollouts, and the world model is trained on image inputs. LEXA is shown to outperform prior methods on a number of tasks on goal images zero-shot, and outperforms existing approaches for unsupervision goal reaching on a variety of test tasks in robotic manipulation and locomotion domains. "
9626,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"trainable parameters USED-FOR Language models. networks USED-FOR tasks. task EVALUATE-FOR networks. parameter sharing CONJUNCTION factorized representations. factorized representations CONJUNCTION parameter sharing. model compression CONJUNCTION parameter sharing. parameter sharing CONJUNCTION model compression. factorized representations CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION factorized representations. reshaped and rearranged original matrix USED-FOR low - rank factorized representation. expressiveness EVALUATE-FOR low - rank layers. embedding CONJUNCTION attention. attention CONJUNCTION embedding. attention CONJUNCTION feed - forward layers. feed - forward layers CONJUNCTION attention. approach USED-FOR Transformer models. OtherScientificTerm are lottery ticket hypothesis, parameter space, self - attention layers, parameter matrix, and architecture of the network. Generic is models. Method is factorized representations of matrices. Task is deep networks. Metric is on - task performance. ","This paper studies the lottery ticket hypothesis: Language models with trainable parameters can be seen as lottery tickets. Language models have been shown to perform well on a number of tasks, but not all of these networks are trained on the same task. This paper proposes to use model compression, parameter sharing, factorized representations, and knowledge distillation to improve the performance of these models.    The authors propose to use the factorized representation of matrices in the parameter space as a way to reduce the number of self-attention layers in deep networks. The idea is to reshape and rearrange the original matrix of the parameter matrix, and then use the reshaped and rearranged original matrix to learn a low-rank factorization of the original parameter matrix. The authors show that this approach can be applied to Transformer models, and that the expressiveness of the low -rank layers can be improved. The paper also shows that the embedding, attention, and feed-forward layers of deep networks can be learned in a similar way, and on-task performance is improved. ","This paper studies the lottery ticket hypothesis: Language models with trainable parameters can be seen as lottery tickets. Language models have been shown to perform well on a number of tasks, but not all of these networks are trained on the same task. This paper proposes to use model compression, parameter sharing, factorized representations, and knowledge distillation to improve the performance of these models.    The authors propose to use the factorized representation of matrices in the parameter space as a way to reduce the number of self-attention layers in deep networks. The idea is to reshape and rearrange the original matrix of the parameter matrix, and then use the reshaped and rearranged original matrix to learn a low-rank factorization of the original parameter matrix. The authors show that this approach can be applied to Transformer models, and that the expressiveness of the low -rank layers can be improved. The paper also shows that the embedding, attention, and feed-forward layers of deep networks can be learned in a similar way, and on-task performance is improved. "
9651,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"structured source code representations USED-FOR models. syntax trees HYPONYM-OF structured source code representations. them PART-OF attention module of Transformer. path encoding methods PART-OF attention module of Transformer. path encoding PART-OF them. them PART-OF unified Transformer framework. TPTrans COMPARE baselines. baselines COMPARE TPTrans. code summarization EVALUATE-FOR approaches. Task is Learning distributed representation of source code. Method is positional encoding. OtherScientificTerm are pairwise path, tree root, and syntax tree. Generic is paths. ","This paper tackles the problem of Learning distributed representation of source code. The authors propose to use structured source code representations, such as syntax trees, to train models that are able to learn from a large number of paths in the syntax tree. The key idea is to learn a positional encoding of each pairwise path in a syntax tree, and then use them in the attention module of Transformer. The path encoding methods are based on the idea that a pair of paths can be represented as a tree root, and that the paths are connected to each other by a set of nodes. The paper proposes to combine these paths and incorporate them into a unified Transformer framework, and shows that TPTrans outperforms several baselines on code summarization tasks.","This paper tackles the problem of Learning distributed representation of source code. The authors propose to use structured source code representations, such as syntax trees, to train models that are able to learn from a large number of paths in the syntax tree. The key idea is to learn a positional encoding of each pairwise path in a syntax tree, and then use them in the attention module of Transformer. The path encoding methods are based on the idea that a pair of paths can be represented as a tree root, and that the paths are connected to each other by a set of nodes. The paper proposes to combine these paths and incorporate them into a unified Transformer framework, and shows that TPTrans outperforms several baselines on code summarization tasks."
9676,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"them USED-FOR high - resolution image generation. Attention - based models USED-FOR long range dependency. Transformer HYPONYM-OF Attention - based models. Generative Adversarial Networks ( GANs ) USED-FOR high - resolution image generation. multi - axis blocked self - attention USED-FOR mixing of local and global attention. global self - attention COMPARE multi - axis blocked self - attention. multi - axis blocked self - attention COMPARE global self - attention. implicit neural function FEATURE-OF multi - layer perceptrons. cross - attention USED-FOR self - modulation component. model USED-FOR synthesizing high definition images. linear computational complexity EVALUATE-FOR model. unconditional ImageNet CONJUNCTION FFHQ 256 × 256. FFHQ 256 × 256 CONJUNCTION unconditional ImageNet. FFHQ 256 × 256 EVALUATE-FOR HiT. FID scores EVALUATE-FOR HiT. unconditional ImageNet EVALUATE-FOR HiT. OtherScientificTerm are quadratic complexity of self - attention operation, low - resolution stages of the generative process, self - attention, image size, and convolutions. Method are generative process, and GANs. Task is high - resolution stages. ","This paper studies the quadratic complexity of self-attention operation in Generative Adversarial Networks (GANs) for high-resolution image generation. Attention-based models, such as the Transformer, are known to suffer from long range dependency on low-resolution stages of the generative process, which limits them to high-resolution image generation at the beginning of GANs. This paper proposes a new model, called HiT, for synthesizing high definition images with linear computational complexity. The key idea is to use global self attention instead of multi-axis blocked self attention, which allows mixing of local and global attention. The authors show that multi-layer perceptrons are an implicit neural function, and that the self-modulation component can be decomposed into cross-attentive and self-mutual attention. They also show that the proposed model is able to achieve FID scores on unconditional ImageNet and FFHQ 256 × 256, which is a significant improvement over the previous state-of-the-art HiT.    The authors also provide a theoretical analysis that shows that the high resolution stages of a GAN are more likely to be modulated by self-transitioning from low resolution to high resolution. The paper also shows that this is a result of the cross-transmission between low and high resolution, which can be explained by the quadratics complexity of the self attention operation.  Finally, the authors provide an empirical analysis of their model, showing that the model can be trained to generate high resolution images in a single pass, with the low-resolutions being modelled as low-dimensional convolutions of a single layer neural network. They show that their model is more robust to changes in the image size and the number of convolutions. ","This paper studies the quadratic complexity of self-attention operation in Generative Adversarial Networks (GANs) for high-resolution image generation. Attention-based models, such as the Transformer, are known to suffer from long range dependency on low-resolution stages of the generative process, which limits them to high-resolution image generation at the beginning of GANs. This paper proposes a new model, called HiT, for synthesizing high definition images with linear computational complexity. The key idea is to use global self attention instead of multi-axis blocked self attention, which allows mixing of local and global attention. The authors show that multi-layer perceptrons are an implicit neural function, and that the self-modulation component can be decomposed into cross-attentive and self-mutual attention. They also show that the proposed model is able to achieve FID scores on unconditional ImageNet and FFHQ 256 × 256, which is a significant improvement over the previous state-of-the-art HiT.    The authors also provide a theoretical analysis that shows that the high resolution stages of a GAN are more likely to be modulated by self-transitioning from low resolution to high resolution. The paper also shows that this is a result of the cross-transmission between low and high resolution, which can be explained by the quadratics complexity of the self attention operation.  Finally, the authors provide an empirical analysis of their model, showing that the model can be trained to generate high resolution images in a single pass, with the low-resolutions being modelled as low-dimensional convolutions of a single layer neural network. They show that their model is more robust to changes in the image size and the number of convolutions. "
9701,SP:41a6753bc56eb16040600666a859294ae36cfa9c,"query complexity EVALUATE-FOR learning geodesically convex halfspaces on graphs. Geodesic convexity HYPONYM-OF Euclidean convexity. treewidth CONJUNCTION minimum hull set size. minimum hull set size CONJUNCTION treewidth. query complexity CONJUNCTION VC dimension. VC dimension CONJUNCTION query complexity. query complexity EVALUATE-FOR Radon number. cut size FEATURE-OF labelling. approach COMPARE active learning algorithms. active learning algorithms COMPARE approach. ground - truth communities PART-OF real - world graphs. OtherScientificTerm are convex sets, diameter, and separation axioms. Material is unlabelled graph. ","This paper studies the query complexity of learning geodesically convex halfspaces on graphs. Geodesic convexity is a special case of Euclidean convexness, where the convex sets are defined in terms of their diameter, treewidth, and minimum hull set size. The authors show that for any unlabelled graph, there exists a query complexity equal to the Radon number, where Radon is the number of vertices in the halfspace, and the diameter is the diameter. They show that this query complexity is upper bounded by the VC dimension. They also show that if the cut size of the labelling is small enough, then there is a tradeoff between query complexity and VC dimension, and they show that the tradeoff depends on the radius of the cut and the distance to the boundary of the set. Finally, the authors show empirically that the proposed approach outperforms existing active learning algorithms.    The paper is well-written and well-motivated, and is well motivated. The paper provides a thorough analysis of the separation axioms, and shows that this approach can be applied to real-world graphs from the ground-truth communities. ","This paper studies the query complexity of learning geodesically convex halfspaces on graphs. Geodesic convexity is a special case of Euclidean convexness, where the convex sets are defined in terms of their diameter, treewidth, and minimum hull set size. The authors show that for any unlabelled graph, there exists a query complexity equal to the Radon number, where Radon is the number of vertices in the halfspace, and the diameter is the diameter. They show that this query complexity is upper bounded by the VC dimension. They also show that if the cut size of the labelling is small enough, then there is a tradeoff between query complexity and VC dimension, and they show that the tradeoff depends on the radius of the cut and the distance to the boundary of the set. Finally, the authors show empirically that the proposed approach outperforms existing active learning algorithms.    The paper is well-written and well-motivated, and is well motivated. The paper provides a thorough analysis of the separation axioms, and shows that this approach can be applied to real-world graphs from the ground-truth communities. "
9726,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"action localization dataset EVALUATE-FOR TAL head. large action classification dataset EVALUATE-FOR video encoder. transfer learning pipeline USED-FOR temporal action localization ( TAL ) methods. video encoder USED-FOR action classification. video encoder USED-FOR task discrepancy problem. video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. TAL head USED-FOR joint optimization. video encoder USED-FOR joint optimization. this USED-FOR TAL. video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. temporal, spatial or spatio - temporal resolution FEATURE-OF mini - batch composition. LoFi optimization approach USED-FOR TAL methods. ResNet18 based video encoder USED-FOR method. single RGB stream FEATURE-OF ResNet18 based video encoder. OtherScientificTerm are encoder, GPU memory constraints, mid - range hardware budget, gradients, and TAL supervision loss. Method are TAL learning, and feature representations. ","This paper proposes a transfer learning pipeline for temporal action localization (TAL) methods, where the video encoder for action classification is trained on a large action classification dataset, and the TAL head is trained to optimize the task discrepancy problem on a smaller action localization dataset. The key idea is to use the same encoder to learn the joint optimization of the video and TAL encoder, and then apply this to TAL for joint optimization with a single video and a single TAL decoder.    The main contribution of this paper is to extend the LoFi optimization approach used in previous TAL methods to the case where the mini-batch composition is of temporal, spatial or spatio-temporal resolution.  The authors show that the proposed method is able to learn a ResNet18 based video encoders on a single RGB stream, which can be used to overcome GPU memory constraints and mid-range hardware budget. The authors also show that TAL learning can be further improved by learning a mini-batch composition that is more suitable for the task at hand, and that this can be applied to a variety of mini-batches of different sizes. The paper also shows that the gradients of the joint optimizer and the joint decoder can be optimized in a single step, which is a nice contribution.  In addition, the authors also propose a TAL supervision loss that encourages the feature representations to be similar across mini batches, which improves the performance. ","This paper proposes a transfer learning pipeline for temporal action localization (TAL) methods, where the video encoder for action classification is trained on a large action classification dataset, and the TAL head is trained to optimize the task discrepancy problem on a smaller action localization dataset. The key idea is to use the same encoder to learn the joint optimization of the video and TAL encoder, and then apply this to TAL for joint optimization with a single video and a single TAL decoder.    The main contribution of this paper is to extend the LoFi optimization approach used in previous TAL methods to the case where the mini-batch composition is of temporal, spatial or spatio-temporal resolution.  The authors show that the proposed method is able to learn a ResNet18 based video encoders on a single RGB stream, which can be used to overcome GPU memory constraints and mid-range hardware budget. The authors also show that TAL learning can be further improved by learning a mini-batch composition that is more suitable for the task at hand, and that this can be applied to a variety of mini-batches of different sizes. The paper also shows that the gradients of the joint optimizer and the joint decoder can be optimized in a single step, which is a nice contribution.  In addition, the authors also propose a TAL supervision loss that encourages the feature representations to be similar across mini batches, which improves the performance. "
9751,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"Gaussian design matrix CONJUNCTION arbitrary 2 noise distribution. arbitrary 2 noise distribution CONJUNCTION Gaussian design matrix. Gaussian design matrix FEATURE-OF convex penalty in linear models. arbitrary 2 noise distribution FEATURE-OF convex penalty in linear models. gradient - Lipschitz loss function USED-FOR M - estimators. Huber loss CONJUNCTION Elastic - Net penalty. Elastic - Net penalty CONJUNCTION Huber loss. heavy - tails FEATURE-OF noise distribution. Elastic - Net penalty USED-FOR robust M - estimator. Huber loss USED-FOR robust M - estimator. differentiability structure FEATURE-OF convex regularized M - estimators. adaptive criterion USED-FOR regularized M - estimators. criterion USED-FOR out - of - sample error. noise distribution CONJUNCTION covariance of the design. covariance of the design CONJUNCTION noise distribution. criterion USED-FOR out - of - sample error. OtherScientificTerm are differentiation, intermediate high - dimensional 9 regime, dimension, distribution of the residuals, and out - of - sample 14 error. Generic is derivatives. Material is Simulated data. Method is M - estimator. ","This paper studies the convex penalty in linear models with a Gaussian design matrix and an arbitrary 2 noise distribution under the gradient-Lipschitz loss function. The authors show that M-estimators can be regularized with a gradient-LSW based on the gradient of the derivative of the design matrix. They also show that the differentiation is non-differentiable under the intermediate high-dimensional 9 regime.    The authors also provide an adaptive criterion for the regularization of convex regularized M - estimators under the differentiability structure. They show that under the Huber loss and the Elastic-Net penalty, a robust M-iterator can be learned with Huber losses. Simulated data is used to prove that the noise distribution with heavy-tailed tails is a function of the dimension of the data and the distribution of the residuals. This criterion can be used to compute the out-of-sample error for any M- estimator. The paper also shows that the criterion is robust to noise distribution, noise distribution and the covariance of the resulting design matrix, and shows that for the case of a large enough number of samples, the criterion converges to the optimal solution of the problem. In addition, the paper shows that if the noise is sufficiently large, then the M estimator will converge to an optimal solution.  Finally, the authors provide a theoretical analysis of the criterion and show that for a given dimension and noise, the M-increator will always converge to a solution that minimizes the out of sample 14 error. ","This paper studies the convex penalty in linear models with a Gaussian design matrix and an arbitrary 2 noise distribution under the gradient-Lipschitz loss function. The authors show that M-estimators can be regularized with a gradient-LSW based on the gradient of the derivative of the design matrix. They also show that the differentiation is non-differentiable under the intermediate high-dimensional 9 regime.    The authors also provide an adaptive criterion for the regularization of convex regularized M - estimators under the differentiability structure. They show that under the Huber loss and the Elastic-Net penalty, a robust M-iterator can be learned with Huber losses. Simulated data is used to prove that the noise distribution with heavy-tailed tails is a function of the dimension of the data and the distribution of the residuals. This criterion can be used to compute the out-of-sample error for any M- estimator. The paper also shows that the criterion is robust to noise distribution, noise distribution and the covariance of the resulting design matrix, and shows that for the case of a large enough number of samples, the criterion converges to the optimal solution of the problem. In addition, the paper shows that if the noise is sufficiently large, then the M estimator will converge to an optimal solution.  Finally, the authors provide a theoretical analysis of the criterion and show that for a given dimension and noise, the M-increator will always converge to a solution that minimizes the out of sample 14 error. "
9776,SP:be53bc4c064402489b644332ad9c17743502d73c,"calibrated beam - based algorithm USED-FOR neural abstractive summarization. calibrated beam - based algorithm USED-FOR local optimality problem. beam search USED-FOR local optimality problem. global attention distribution FEATURE-OF calibrated beam - based algorithm. attention distribution USED-FOR global protocol. global scoring mechanism USED-FOR beam search. global scoring mechanism USED-FOR beam search. global ( attention)-aware inference COMPARE summarization models. summarization models COMPARE global ( attention)-aware inference. empirical hyper - parameters USED-FOR summarization models. empirical hyper - parameters USED-FOR global ( attention)-aware inference. Generic are design, and algorithm. Task is inference. OtherScientificTerm is corrupted attention distributions. ","This paper proposes a calibrated beam-based algorithm for neural abstractive summarization based on the calibration of the global attention distribution of the local optimality problem solved by beam search. The key idea of the proposed design is that the attention distribution for a global protocol is calibrated to be the same as the one for the local protocol. This is achieved by using a global scoring mechanism for beam search, which is based on a previous work (Zhang et al., 2017). The authors show that global (attention)-aware inference outperforms previous summarization models in terms of empirical hyper-parameters. The authors also show that the inference is more robust to corrupted attention distributions, and that the algorithm is able to find the optimal solution to the local optimization problem.","This paper proposes a calibrated beam-based algorithm for neural abstractive summarization based on the calibration of the global attention distribution of the local optimality problem solved by beam search. The key idea of the proposed design is that the attention distribution for a global protocol is calibrated to be the same as the one for the local protocol. This is achieved by using a global scoring mechanism for beam search, which is based on a previous work (Zhang et al., 2017). The authors show that global (attention)-aware inference outperforms previous summarization models in terms of empirical hyper-parameters. The authors also show that the inference is more robust to corrupted attention distributions, and that the algorithm is able to find the optimal solution to the local optimization problem."
9801,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"Attention mechanism USED-FOR deep learning models. relative position encoding PART-OF deep learning models. canonical local coordinate system USED-FOR neighborhoods. attention USED-FOR manifolds. method USED-FOR feature vectors. regular field of cyclic groups USED-FOR feature fields. regular field of cyclic groups USED-FOR intermediate layers. feature fields USED-FOR intermediate layers. feature vectors USED-FOR fields. method USED-FOR expressive ability. regular field of cyclic groups USED-FOR expressive ability. position vector USED-FOR orientation of the coordinate system. ambient space FEATURE-OF orientation of the coordinate system. local coordinate system USED-FOR position vector. global coordinate system HYPONYM-OF orientation of the coordinate system. gauge equivariance USED-FOR self - attention. triangle meshes USED-FOR Gauge Equivariant Transformer ( GET ). common recognition tasks EVALUATE-FOR GET. Method are equivariant transformer, and multi - head selfattention. OtherScientificTerm are orientation of local coordinate systems, gauge equivariant, position - based and content - based information, and rotation invariance. ","This paper proposes a new attention mechanism for deep learning models that is equivariant to the orientation of local coordinate systems. The authors propose a method to learn a canonical local coordinate system for neighborhoods, which is then used to encode relative position encoding in deep neural networks. The proposed method learns feature vectors for these fields using feature vectors from a regular field of cyclic groups, which are then used as intermediate layers to encode feature fields for intermediate layers. The paper shows that the proposed method improves the expressive ability of self-attention through the use of gauge equivariance. The main contribution of the paper is the introduction of the Gauge Equivariant Transformer (GET) on triangle meshes, which can be seen as a generalization of the work of [1]. The authors show that the self attention of the proposed gauge-equivariant transformer (GEE) can be applied to multi-head selfattention, and that the attention can be used to learn manifolds that are more expressive.    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13]  [14] [15]  The paper is well-written and well-motivated. The idea is interesting. The motivation is clear. The work is well motivated. The method is intuitive and the results are interesting. However, the paper suffers from a lack of comparison with prior work, and there is no comparison with the state-of-the-art. ","This paper proposes a new attention mechanism for deep learning models that is equivariant to the orientation of local coordinate systems. The authors propose a method to learn a canonical local coordinate system for neighborhoods, which is then used to encode relative position encoding in deep neural networks. The proposed method learns feature vectors for these fields using feature vectors from a regular field of cyclic groups, which are then used as intermediate layers to encode feature fields for intermediate layers. The paper shows that the proposed method improves the expressive ability of self-attention through the use of gauge equivariance. The main contribution of the paper is the introduction of the Gauge Equivariant Transformer (GET) on triangle meshes, which can be seen as a generalization of the work of [1]. The authors show that the self attention of the proposed gauge-equivariant transformer (GEE) can be applied to multi-head selfattention, and that the attention can be used to learn manifolds that are more expressive.    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13]  [14] [15]  The paper is well-written and well-motivated. The idea is interesting. The motivation is clear. The work is well motivated. The method is intuitive and the results are interesting. However, the paper suffers from a lack of comparison with prior work, and there is no comparison with the state-of-the-art. "
9826,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"methods USED-FOR unsupervised learning of finite mixture models. expectation maximization CONJUNCTION Metropolis - Hastings algorithm. Metropolis - Hastings algorithm CONJUNCTION expectation maximization. Metropolis - Hastings algorithm USED-FOR approach. expectation maximization PART-OF approach. it USED-FOR shallow and deep mixture 8 models. mixtures of normalizing flows CONJUNCTION sum - product ( transform ) networks. sum - product ( transform ) networks CONJUNCTION mixtures of normalizing flows. synthetic and real - data 10 contexts EVALUATE-FOR deep models. sum - product ( transform ) networks HYPONYM-OF deep models. mixtures of normalizing flows HYPONYM-OF deep models. synthetic and real - data 10 contexts EVALUATE-FOR method. Method is finite mixture models. OtherScientificTerm are mixture, and complex, and possibly nonlinear, transformations. Metric is computational cost. ","This paper proposes two methods for unsupervised learning of finite mixture models. The proposed approach combines expectation maximization and the Metropolis-Hastings algorithm, and it is applicable to both shallow and deep mixture 8 models. Experiments on synthetic and real-data 10 contexts show that the proposed method outperforms existing deep models (mixtures of normalizing flows and sum-product (transform) networks) in terms of performance and computational cost. The paper also shows that the mixture 8 model is robust to complex, and possibly nonlinear, transformations. ","This paper proposes two methods for unsupervised learning of finite mixture models. The proposed approach combines expectation maximization and the Metropolis-Hastings algorithm, and it is applicable to both shallow and deep mixture 8 models. Experiments on synthetic and real-data 10 contexts show that the proposed method outperforms existing deep models (mixtures of normalizing flows and sum-product (transform) networks) in terms of performance and computational cost. The paper also shows that the mixture 8 model is robust to complex, and possibly nonlinear, transformations. "
9851,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"Sparse training USED-FOR deep neural networks. dense computation USED-FOR backward propagation step. sparse forward and backward passes USED-FOR sparse training method. global sparsity constraint FEATURE-OF continuous minimization problem. continuous minimization problem USED-FOR training process. weight update CONJUNCTION structure parameter update. structure parameter update CONJUNCTION weight update. structure parameter update HYPONYM-OF steps. weight update HYPONYM-OF steps. chain rule USED-FOR step. sparse structure USED-FOR chain rule. variance reduced policy gradient estimator USED-FOR sparse training. forward passes CONJUNCTION backward propagation. backward propagation CONJUNCTION forward passes. chain rule based gradient estimators USED-FOR variance reduced policy gradient estimator. variance reduced policy gradient estimator USED-FOR step. chain rule based gradient estimators USED-FOR step. forward passes USED-FOR variance reduced policy gradient estimator. algorithm USED-FOR training process. real - world datasets EVALUATE-FOR algorithm. OtherScientificTerm is memory usage. Method are neural networks, and gradient estimator. Generic is methods. Task is optimization process. ","This paper studies the problem of sparse training for deep neural networks. The authors propose a novel sparse training method based on sparse forward and backward passes, which avoids dense computation for the backward propagation step and reduces memory usage. The training process is formulated as a continuous minimization problem with a global sparsity constraint, and the training process consists of two steps: weight update and structure parameter update. The first step is based on a chain rule based on the sparse structure, while the second step uses a variance reduced policy gradient estimator based on forward passes and backward propagation. The proposed algorithm is evaluated on several real-world datasets, and is shown to be effective for sparse training.    The paper is well-written and well-motivated. The paper provides a detailed analysis of existing methods, and provides a theoretical analysis of the optimization process. It also provides an empirical analysis on the variance of the gradient estimators. ","This paper studies the problem of sparse training for deep neural networks. The authors propose a novel sparse training method based on sparse forward and backward passes, which avoids dense computation for the backward propagation step and reduces memory usage. The training process is formulated as a continuous minimization problem with a global sparsity constraint, and the training process consists of two steps: weight update and structure parameter update. The first step is based on a chain rule based on the sparse structure, while the second step uses a variance reduced policy gradient estimator based on forward passes and backward propagation. The proposed algorithm is evaluated on several real-world datasets, and is shown to be effective for sparse training.    The paper is well-written and well-motivated. The paper provides a detailed analysis of existing methods, and provides a theoretical analysis of the optimization process. It also provides an empirical analysis on the variance of the gradient estimators. "
9876,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"importance samplers ( IS ) CONJUNCTION Markov chain Monte Carlo ( MCMC ) samplers. Markov chain Monte Carlo ( MCMC ) samplers CONJUNCTION importance samplers ( IS ). iterated sampling - importance resampling mechanism USED-FOR π. NEO - IS CONJUNCTION iterated sampling - importance resampling mechanism. iterated sampling - importance resampling mechanism CONJUNCTION NEO - IS. NEO - IS COMPARE NEO - MCMC. NEO - MCMC COMPARE NEO - IS. NEO - MCMC USED-FOR π. NEO - IS USED-FOR NEO - MCMC. iterated sampling - importance resampling mechanism USED-FOR NEO - MCMC. NEO - MCMC USED-FOR multimodal targets. T USED-FOR conformal Hamiltonian system. NEO - IS COMPARE NEO - MCMC. NEO - MCMC COMPARE NEO - IS. discrete - time integrator USED-FOR conformal Hamiltonian system. T USED-FOR NEO - IS. T USED-FOR discrete - time integrator. NEO - MCMC USED-FOR explicit mixing time estimates. OtherScientificTerm are complex distribution π, intractable normalizing constant, invertible map T, forward and backward Orbits, proposal distribution ρ, map T, NEO, Non - Equilibrium Orbits, and normalizing constant. Generic are schemes, and methods. ","This paper considers the problem of sampling from a complex distribution π, where the intractable normalizing constant $T$ is a function of the invertible map T.  The authors propose two schemes: (1) importance samplers (IS) and (2) Markov chain Monte Carlo (MCMC) sampler.  In both cases, the forward and backward Orbits of the proposal distribution ρ are defined as the sum of a proposal distribution $P(T)$ and a map T, where $p(T|T) = P(T, T)$.    The main contribution of the paper is the development of two new methods, namely, the proposed NEO-IS and the novel iterated sampling-importance resampling mechanism for π.   In the first case, the authors show that the proposed algorithm NEO-IMO-IS is more efficient than the previous state-of-the-art algorithm, NEO-MCMC, for multimodal targets. In the second case, they show that a discrete-time integrator with T is able to learn a conformal Hamiltonian system, which can be used as a surrogate for the proposed algorithms.  They also show that, in the case of Non-Equilibrium Orbits, they are able to obtain explicit mixing time estimates using the proposed methods. ","This paper considers the problem of sampling from a complex distribution π, where the intractable normalizing constant $T$ is a function of the invertible map T.  The authors propose two schemes: (1) importance samplers (IS) and (2) Markov chain Monte Carlo (MCMC) sampler.  In both cases, the forward and backward Orbits of the proposal distribution ρ are defined as the sum of a proposal distribution $P(T)$ and a map T, where $p(T|T) = P(T, T)$.    The main contribution of the paper is the development of two new methods, namely, the proposed NEO-IS and the novel iterated sampling-importance resampling mechanism for π.   In the first case, the authors show that the proposed algorithm NEO-IMO-IS is more efficient than the previous state-of-the-art algorithm, NEO-MCMC, for multimodal targets. In the second case, they show that a discrete-time integrator with T is able to learn a conformal Hamiltonian system, which can be used as a surrogate for the proposed algorithms.  They also show that, in the case of Non-Equilibrium Orbits, they are able to obtain explicit mixing time estimates using the proposed methods. "
9901,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,training CONJUNCTION inference. inference CONJUNCTION training. permutation invariance CONJUNCTION equivariance. equivariance CONJUNCTION permutation invariance. permutation invariance FEATURE-OF set - function constraints. equivariance FEATURE-OF set - function constraints. property USED-FOR large scale mini - batch set encoding. Mini - Batch Consistency ( MBC ) USED-FOR large scale mini - batch set encoding. Mini - Batch Consistency ( MBC ) HYPONYM-OF property. attention - based set encoding mechanism USED-FOR set representations. mini - batch processing of sets USED-FOR attention - based set encoding mechanism. symmetries of invariance CONJUNCTION equivariance. equivariance CONJUNCTION symmetries of invariance. MBC USED-FOR method. symmetries of invariance FEATURE-OF method. method USED-FOR rich set encoding representations. rich set encoding representations USED-FOR set - structured data. Method is set encoding algorithms. OtherScientificTerm is computational and memory resources. Generic is assumptions. Task is large - scale set encoding. ,"This paper proposes a new property called Mini-Batch Consistency (MBC) for large scale mini-batch set encoding based on the property that permutation invariance and equivariance of set-function constraints (both in training and inference) are guaranteed. The authors propose an attention-based set encoding mechanism to learn set representations based on mini-batches of sets, which can be used to reduce the computational and memory resources. The proposed method is based on MBC and is able to learn rich set encoding representations for set-structured data. The paper is well-written and well-motivated. However, there are a few issues with the paper:  1. The assumptions are not well-grounded.  2. It is not clear to me that the proposed method satisfies symmetries of invariance or equivariantness.  3. There is a lack of experiments on large-scale set encoding.  ","This paper proposes a new property called Mini-Batch Consistency (MBC) for large scale mini-batch set encoding based on the property that permutation invariance and equivariance of set-function constraints (both in training and inference) are guaranteed. The authors propose an attention-based set encoding mechanism to learn set representations based on mini-batches of sets, which can be used to reduce the computational and memory resources. The proposed method is based on MBC and is able to learn rich set encoding representations for set-structured data. The paper is well-written and well-motivated. However, there are a few issues with the paper:  1. The assumptions are not well-grounded.  2. It is not clear to me that the proposed method satisfies symmetries of invariance or equivariantness.  3. There is a lack of experiments on large-scale set encoding.  "
9926,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"Diplomacy HYPONYM-OF game. human data USED-FOR policy. Diplomacy CONJUNCTION StarCraft. StarCraft CONJUNCTION Diplomacy. StarCraft CONJUNCTION Dota. Dota CONJUNCTION StarCraft. branching factors FEATURE-OF games. Diplomacy HYPONYM-OF branching factors. StarCraft HYPONYM-OF branching factors. Dota HYPONYM-OF games. Diplomacy HYPONYM-OF games. StarCraft HYPONYM-OF games. action exploration CONJUNCTION equilibrium approximation. equilibrium approximation CONJUNCTION action exploration. algorithm USED-FOR action exploration. algorithm USED-FOR equilibrium approximation. algorithm USED-FOR policy proposal network. value iteration USED-FOR algorithm. policy USED-FOR model training. equilibrium search procedure USED-FOR policy. equilibrium search procedure USED-FOR model training. algorithm USED-FOR agent. DORA USED-FOR two - player variant of Diplomacy. agent USED-FOR two - player variant of Diplomacy. DORA HYPONYM-OF agent. methods USED-FOR full - scale no - press Diplomacy. human data USED-FOR agent. agent COMPARE human - data bootstrapped agents. human - data bootstrapped agents COMPARE agent. self play USED-FOR superhuman performance. multiple equilibria FEATURE-OF Diplomacy. Diplomacy FEATURE-OF superhuman performance. Task is complex games. Method are handcrafted reward shaping, and double oracle step. OtherScientificTerm are combinatorial action spaces, and policy proposals. Generic is it. ","This paper considers the problem of learning a policy from human data for a game called Diplomacy. Diplomacy is a game where the agent is given a sequence of actions and a set of options, and the goal is to learn a policy that maximizes the sum of all possible actions in the sequence. In complex games with branching factors (e.g., Diplomacy, StarCraft, and Dota), there is no handcrafted reward shaping, but there are combinatorial action spaces.  The paper proposes an algorithm for learning the policy proposal network, which is based on a double oracle step, where the algorithm first learns an algorithm that optimizes action exploration and equilibrium approximation, and then uses value iteration to update the algorithm. The algorithm is then used to train a policy for model training based on the equilibrium search procedure. The agent, called DORA, is trained on a two-player variant of Diplomacy using human data, and is shown to outperform a human-data bootstrapped agent trained on self play. The paper also proposes two methods for learning a full-scale no-press Diplomacy and shows that the agent trained with human data is able to achieve superhuman performance in Diplomacy with multiple equilibria.  ","This paper considers the problem of learning a policy from human data for a game called Diplomacy. Diplomacy is a game where the agent is given a sequence of actions and a set of options, and the goal is to learn a policy that maximizes the sum of all possible actions in the sequence. In complex games with branching factors (e.g., Diplomacy, StarCraft, and Dota), there is no handcrafted reward shaping, but there are combinatorial action spaces.  The paper proposes an algorithm for learning the policy proposal network, which is based on a double oracle step, where the algorithm first learns an algorithm that optimizes action exploration and equilibrium approximation, and then uses value iteration to update the algorithm. The algorithm is then used to train a policy for model training based on the equilibrium search procedure. The agent, called DORA, is trained on a two-player variant of Diplomacy using human data, and is shown to outperform a human-data bootstrapped agent trained on self play. The paper also proposes two methods for learning a full-scale no-press Diplomacy and shows that the agent trained with human data is able to achieve superhuman performance in Diplomacy with multiple equilibria.  "
9951,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,it USED-FOR sequence modeling. attention heads PART-OF Multi - head attention. positive transfer CONJUNCTION negative interference. negative interference CONJUNCTION positive transfer. Multilingual and multi - domain learning USED-FOR sequence modeling. generalization EVALUATE-FOR non - selective attention sharing. attention sharing strategies USED-FOR multilingual and multi - domain sequence modeling. approach USED-FOR shared and specialized attention heads. attention sharing strategies USED-FOR sequence models. tasks EVALUATE-FOR attention sharing strategies. tasks EVALUATE-FOR sequence models. speech recognition HYPONYM-OF tasks. multi - head attention USED-FOR sequence models. BLEU EVALUATE-FOR multi - domain setting. BLEU EVALUATE-FOR speech - to - text translation. approach USED-FOR speech - to - text translation. multilingual setting EVALUATE-FOR approach. BLEU EVALUATE-FOR approach. BLEU EVALUATE-FOR approach. ,"Multilingual and multi-domain learning is an important problem in sequence modeling. Multi-head attention consists of multiple attention heads, and it has been shown that non-selective attention sharing improves the generalization. This paper proposes an approach to combine shared and specialized attention heads to improve the performance of sequence models on various tasks, including speech recognition. Experiments show that the proposed approach improves the BLEU in the multi-domains setting and improves the performance on speech-to-text translation.","Multilingual and multi-domain learning is an important problem in sequence modeling. Multi-head attention consists of multiple attention heads, and it has been shown that non-selective attention sharing improves the generalization. This paper proposes an approach to combine shared and specialized attention heads to improve the performance of sequence models on various tasks, including speech recognition. Experiments show that the proposed approach improves the BLEU in the multi-domains setting and improves the performance on speech-to-text translation."
9976,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,covariate shift HYPONYM-OF distribution shift. covariate shift FEATURE-OF real - world applications. high - dimensional asymptotics FEATURE-OF random feature regression. limiting test error CONJUNCTION bias. bias CONJUNCTION limiting test error. covariate shift USED-FOR random feature regression. robustness EVALUATE-FOR overparameterized models. Method is machine learning models. OtherScientificTerm is conditional label distributions. Task is machine learning. ,"This paper studies the problem of distribution shift in machine learning models, i.e., covariate shift. In real-world applications, there is a large amount of work on the topic, but this paper focuses on the case of conditional label distributions. The authors show that random feature regression with high-dimensional asymptotics can be affected by the distribution shift, which they call ""covariate shift"". The authors also show that the limiting test error and bias are related to the covariate of the data distribution, and that the robustness of overparameterized models is also affected by this distribution shift. Finally, the authors provide a theoretical analysis of the effect of covariate shifted data on machine learning.","This paper studies the problem of distribution shift in machine learning models, i.e., covariate shift. In real-world applications, there is a large amount of work on the topic, but this paper focuses on the case of conditional label distributions. The authors show that random feature regression with high-dimensional asymptotics can be affected by the distribution shift, which they call ""covariate shift"". The authors also show that the limiting test error and bias are related to the covariate of the data distribution, and that the robustness of overparameterized models is also affected by this distribution shift. Finally, the authors provide a theoretical analysis of the effect of covariate shifted data on machine learning."
10001,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"Thompson sampling CONJUNCTION Bayesian sequential decision - making algorithms. Bayesian sequential decision - making algorithms CONJUNCTION Thompson sampling. Bayesian sequential decision - making algorithms USED-FOR explore / exploit trade - offs. Thompson sampling USED-FOR explore / exploit trade - offs. explore / exploit trade - offs FEATURE-OF ( contextual ) bandits. prior USED-FOR algorithms. expected reward EVALUATE-FOR Thompson sampling ( TS ). misspecified prior USED-FOR Thompson sampling ( TS ). well - specified prior USED-FOR TS. parametric form FEATURE-OF prior. universal constants FEATURE-OF it. bounded support FEATURE-OF priors. algorithms USED-FOR Bayesian meta - learning setting. generic PAC guarantees USED-FOR algorithms. Bayesian POMDPs HYPONYM-OF Bayesian decision - making setting. knowledge gradient algorithm ( KG ) HYPONYM-OF Bayesian decision - making algorithms. multi - armed and contextual bandits USED-FOR meta - learning. structured and correlated priors FEATURE-OF multi - armed and contextual bandits. structured and correlated priors USED-FOR meta - learning. OtherScientificTerm are domain knowledge, misspecification, total - variation distance, learning horizon, cardinality or structure of the action space, sensitivity analysis, and prior misspecification. Generic is bound. Method are contextual bandits, and KG ). ","This paper studies the generalization of Thompson sampling and Bayesian sequential decision-making algorithms to explore/exploit trade-offs in (contextual) bandits with a misspecified prior. The authors show that Thompson sampling (TS) with a well-specified prior is equivalent to Bayesian POMDPs in terms of expected reward under the assumption that the domain knowledge is well-conditioned. They also provide a bound on the total-variance distance between the total reward and the expected reward of an algorithm with and without the misspecification of the prior.    The authors also show that when the prior has a parametric form, Thompson sampling with the missing prior (in the sense that it has universal constants) converges to the optimal solution of a Bayesian meta-learning algorithm (KG) with the same expected reward.  The paper also shows that the algorithms with the prior have generic PAC guarantees, which is a result of the fact that the learning horizon depends on the cardinality or structure of the action space.  Finally, the authors provide a sensitivity analysis, which shows that for any algorithm with the correct prior, the total variation of the reward is bounded by a constant that depends only on the number of arms and the total total variation distance to the prior, and that this bound holds for both multi-armed and contextual bandits in the meta-learn setting. They show that for contextual bandits, the bound is bounded for all arms and for all priors with bounded support. Finally, they show that the Bayesian decision making algorithms (e.g., the knowledge gradient algorithm (i.e., the KG) and the Thompson sampling algorithm (which is a variant of the well-defined prior) are equivalent to the algorithms in this setting.  They also show a connection between their results and those of previous work on multi-arm and contextual bands in the context of meta learning with structured and correlated priors, and show that these algorithms can be extended to the more general Bayesian Meta-learning setting.","This paper studies the generalization of Thompson sampling and Bayesian sequential decision-making algorithms to explore/exploit trade-offs in (contextual) bandits with a misspecified prior. The authors show that Thompson sampling (TS) with a well-specified prior is equivalent to Bayesian POMDPs in terms of expected reward under the assumption that the domain knowledge is well-conditioned. They also provide a bound on the total-variance distance between the total reward and the expected reward of an algorithm with and without the misspecification of the prior.    The authors also show that when the prior has a parametric form, Thompson sampling with the missing prior (in the sense that it has universal constants) converges to the optimal solution of a Bayesian meta-learning algorithm (KG) with the same expected reward.  The paper also shows that the algorithms with the prior have generic PAC guarantees, which is a result of the fact that the learning horizon depends on the cardinality or structure of the action space.  Finally, the authors provide a sensitivity analysis, which shows that for any algorithm with the correct prior, the total variation of the reward is bounded by a constant that depends only on the number of arms and the total total variation distance to the prior, and that this bound holds for both multi-armed and contextual bandits in the meta-learn setting. They show that for contextual bandits, the bound is bounded for all arms and for all priors with bounded support. Finally, they show that the Bayesian decision making algorithms (e.g., the knowledge gradient algorithm (i.e., the KG) and the Thompson sampling algorithm (which is a variant of the well-defined prior) are equivalent to the algorithms in this setting.  They also show a connection between their results and those of previous work on multi-arm and contextual bands in the context of meta learning with structured and correlated priors, and show that these algorithms can be extended to the more general Bayesian Meta-learning setting."
10026,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"PAC - learning model CONJUNCTION Equivalence - Query - learning model. Equivalence - Query - learning model CONJUNCTION PAC - learning model. sample / query complexity EVALUATE-FOR PAC - learning model. exponential separation FEATURE-OF sample / query complexity. adversarial training COMPARE training. training COMPARE adversarial training. adversarial training USED-FOR generalization. on - manifold adversarial examples USED-FOR adversarial training. Method are PAC model, Equivalence - Query model, adversarial model, and Equivalance - Query model. OtherScientificTerm are teacher, learner, PAC bound, adversarial examples, norm constraint, and adversary. Metric are adversarial robustness, and robustness. Generic is model. ","This paper studies the sample/query complexity of a PAC-learning model and an Equivalence-Query-Learning model. In particular, the authors show that for any PAC model, if the teacher and the learner have the same sample/queries complexity, the PAC bound is O(1/\sqrt{n}^2) and the Equivalency-Query model is O(\sqrt{\log n}^3). The authors also show that adversarial training with on-manifold adversarial examples is equivalent to training with adversarial robustness.    The main contribution of this paper is that the authors provide a theoretical analysis of the sample / query complexity of both the PAC-learning model and Equivalance-Quake-Learning (equivalently, the adversarial model). They show that the sample-and-queries of the PAC model are O(n^2/n) and O(log n) logarithmically separable, and that for Equivalent-Query, the samples and queries of the model are exponentially separable. In addition, they show that in the case that the number of adversarial instances is large enough, the robustness of a model to adversarial perturbations can be improved. Finally, they also show empirically that for a large enough number of samples, a PAC bound of O(\log n/n^3) is obtained. ","This paper studies the sample/query complexity of a PAC-learning model and an Equivalence-Query-Learning model. In particular, the authors show that for any PAC model, if the teacher and the learner have the same sample/queries complexity, the PAC bound is O(1/\sqrt{n}^2) and the Equivalency-Query model is O(\sqrt{\log n}^3). The authors also show that adversarial training with on-manifold adversarial examples is equivalent to training with adversarial robustness.    The main contribution of this paper is that the authors provide a theoretical analysis of the sample / query complexity of both the PAC-learning model and Equivalance-Quake-Learning (equivalently, the adversarial model). They show that the sample-and-queries of the PAC model are O(n^2/n) and O(log n) logarithmically separable, and that for Equivalent-Query, the samples and queries of the model are exponentially separable. In addition, they show that in the case that the number of adversarial instances is large enough, the robustness of a model to adversarial perturbations can be improved. Finally, they also show empirically that for a large enough number of samples, a PAC bound of O(\log n/n^3) is obtained. "
10051,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"models USED-FOR transfer learning. benchmarks EVALUATE-FOR task. techniques USED-FOR algorithms. methods USED-FOR PARC. PARC COMPARE methods. methods COMPARE PARC. methods USED-FOR diverse model selection. diverse model selection EVALUATE-FOR PARC. model selection USED-FOR transfer learning. Method is pretrained deep learning models. Material is large model banks. OtherScientificTerm is diversity of off - the - shelf models. Generic are model, and setting. Task is Scalable Diverse Model Selection. ","This paper addresses the problem of transfer learning from large model banks to smaller ones. The authors propose a new setting, called Scalable Diverse Model Selection (PARC), where the goal is to select diverse models for transfer learning across different models. They show that this task can be solved efficiently on standard benchmarks, and that the diversity of off-the-shelf models can be improved. They also propose two techniques to improve the performance of existing algorithms in this setting. Finally, they show that PARC outperforms existing methods for diverse model selection in comparison to other methods. ","This paper addresses the problem of transfer learning from large model banks to smaller ones. The authors propose a new setting, called Scalable Diverse Model Selection (PARC), where the goal is to select diverse models for transfer learning across different models. They show that this task can be solved efficiently on standard benchmarks, and that the diversity of off-the-shelf models can be improved. They also propose two techniques to improve the performance of existing algorithms in this setting. Finally, they show that PARC outperforms existing methods for diverse model selection in comparison to other methods. "
10076,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"low - dimensional binary codes USED-FOR compression of high - dimensional neural representations. large bit - codes USED-FOR compression of high - dimensional neural representations. method USED-FOR Low - dimensional binary Codes ( LLC ). method USED-FOR low - dimensional binary codes. annotated attributes CONJUNCTION label meta - data. label meta - data CONJUNCTION annotated attributes. label meta - data HYPONYM-OF side - information. annotated attributes HYPONYM-OF side - information. it USED-FOR image retrieval. it USED-FOR codes. binary codes COMPARE 10 dimensional real representations. 10 dimensional real representations COMPARE binary codes. binary codes COMPARE HashNet. HashNet COMPARE binary codes. ImageNet-100 retrieval problem EVALUATE-FOR binary codes. Material is ImageNet-1 K. Metric is classification accuracy. Method is ResNet50. Task is OOD detection. Generic are baseline, and Code. OtherScientificTerm is threshold. ","This paper proposes a method called Low-dimensional binary Codes (LLC) to learn low-dimensional Binary codes for compression of high-dimensional neural representations with large bit-codes. The proposed method is based on the observation that the classification accuracy of binary codes is highly correlated with the side-information (e.g., annotated attributes and label meta-data). The authors propose a simple baseline, ResNet50, and show that it can be used for image retrieval and OOD detection. The binary codes are compared to 10 dimensional real representations on the ImageNet-100 retrieval problem, and are shown to outperform HashNet. Code is also shown to be able to be used as a threshold to detect out of distribution images.","This paper proposes a method called Low-dimensional binary Codes (LLC) to learn low-dimensional Binary codes for compression of high-dimensional neural representations with large bit-codes. The proposed method is based on the observation that the classification accuracy of binary codes is highly correlated with the side-information (e.g., annotated attributes and label meta-data). The authors propose a simple baseline, ResNet50, and show that it can be used for image retrieval and OOD detection. The binary codes are compared to 10 dimensional real representations on the ImageNet-100 retrieval problem, and are shown to outperform HashNet. Code is also shown to be able to be used as a threshold to detect out of distribution images."
10101,SP:07def8c80d05f86402ce769313480b30cd99af43,"computational / storage costs EVALUATE-FOR convolutional neural networks ( CNNs ). model compression techniques CONJUNCTION adversarial training. adversarial training CONJUNCTION model compression techniques. adversarial perturbations FEATURE-OF robustness. throughput ( frames - per - second ) EVALUATE-FOR methods. GDWS USED-FOR pre - trained network. throughput EVALUATE-FOR pre - trained network. real - life hardware FEATURE-OF pre - trained network. robustness EVALUATE-FOR GDWS. throughput EVALUATE-FOR GDWS. pre - trained models USED-FOR it. algorithms USED-FOR GDWS convolutions. 2D convolution approximator USED-FOR GDWS. complexity and error constraints USED-FOR algorithms. ImageNet datasets EVALUATE-FOR GDWS. CIFAR-10 EVALUATE-FOR GDWS. Task is robust model compression. Method are Generalized Depthwise - Separable ( GDWS ) convolution, and 2D convolution. ","This paper studies the problem of robust model compression. The authors propose Generalized Depthwise-Separable (GDWS) convolution, which is a generalization of the 2D convolution approximator proposed in [1]. The authors argue that existing model compression techniques and adversarial training have been shown to be effective in reducing the computational/storage costs of convolutional neural networks (CNNs). However, the performance of existing methods is limited in terms of throughput (frames-per-second) and robustness to adversarial perturbations. GDWS is shown to improve the robustness of a pre-trained network on real-life hardware, and it is shown that GDWS can be applied to any pre-training models.  The authors also propose two algorithms for GDWS convolutions, which are based on the complexity and error constraints of existing algorithms.  Experiments on ImageNet datasets are conducted to demonstrate the effectiveness of GDWS on CIFAR-10.   ","This paper studies the problem of robust model compression. The authors propose Generalized Depthwise-Separable (GDWS) convolution, which is a generalization of the 2D convolution approximator proposed in [1]. The authors argue that existing model compression techniques and adversarial training have been shown to be effective in reducing the computational/storage costs of convolutional neural networks (CNNs). However, the performance of existing methods is limited in terms of throughput (frames-per-second) and robustness to adversarial perturbations. GDWS is shown to improve the robustness of a pre-trained network on real-life hardware, and it is shown that GDWS can be applied to any pre-training models.  The authors also propose two algorithms for GDWS convolutions, which are based on the complexity and error constraints of existing algorithms.  Experiments on ImageNet datasets are conducted to demonstrate the effectiveness of GDWS on CIFAR-10.   "
10126,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"Retrosynthesis prediction HYPONYM-OF organic synthesis. neural models USED-FOR task. model USED-FOR graph edits. model USED-FOR synthons. top-1 accuracy EVALUATE-FOR model. OtherScientificTerm are precursor molecules, graph topology, and chemical reaction. Method are model design, graph - based approach, and manual correction. Generic is architecture. "," in the field of organic synthesis, specifically in the context of ""Retrosynthesis prediction"". This paper proposes to use neural models to solve this task, where the goal is to predict the topology of the precursor molecules. The model design is based on a graph-based approach, where a graph topology is learned for each chemical reaction, and a model is trained to predict graph edits. The paper shows that the model is able to predict synthons with high top-1 accuracy, and that the architecture is robust to manual correction. "," in the field of organic synthesis, specifically in the context of ""Retrosynthesis prediction"". This paper proposes to use neural models to solve this task, where the goal is to predict the topology of the precursor molecules. The model design is based on a graph-based approach, where a graph topology is learned for each chemical reaction, and a model is trained to predict graph edits. The paper shows that the model is able to predict synthons with high top-1 accuracy, and that the architecture is robust to manual correction. "
10151,SP:772277d969c95924755113c86663fb0e009f24cc,"Bayesian formulation of deconditioning USED-FOR reproducing kernel Hilbert space formulation. deconditioning USED-FOR downscaling setup. conditional mean embedding estimator USED-FOR multiresolution data. solution USED-FOR deconditioning problem. posterior USED-FOR deconditioning problem. posterior USED-FOR latent field. posterior USED-FOR solution. minimax optimal convergence rate FEATURE-OF it. its EVALUATE-FOR methods. OtherScientificTerm are high - resolution ( HR ) information, LR samples, mediating variable, conditional expectation, conditional expectations, and inter - domain features. Task are statistical downscaling, and recovery of the underlying fine - grained field. Material is spatial datasets. ","This paper proposes a Bayesian formulation of deconditioning for the reproducing kernel Hilbert space formulation, which is motivated by the observation that high-resolution (HR) information can be lost during statistical downscaling. The authors show that deconditionsing in the downscaled setup can be seen as a variant of the classical decongestioning in which the mediating variable is the conditional expectation of the latent field of the decoder. They show that the recovery of the underlying fine-grained field can be achieved through the use of a conditional mean embedding estimator for multiresolution data. The proposed solution to the decongitioning problem is based on using the posterior of the posterior for the underlying latent field, and the authors prove that it has a minimax optimal convergence rate of $O(1/\sqrt{T})$. The authors also provide a theoretical analysis of their methods and evaluate its performance on several spatial datasets. ","This paper proposes a Bayesian formulation of deconditioning for the reproducing kernel Hilbert space formulation, which is motivated by the observation that high-resolution (HR) information can be lost during statistical downscaling. The authors show that deconditionsing in the downscaled setup can be seen as a variant of the classical decongestioning in which the mediating variable is the conditional expectation of the latent field of the decoder. They show that the recovery of the underlying fine-grained field can be achieved through the use of a conditional mean embedding estimator for multiresolution data. The proposed solution to the decongitioning problem is based on using the posterior of the posterior for the underlying latent field, and the authors prove that it has a minimax optimal convergence rate of $O(1/\sqrt{T})$. The authors also provide a theoretical analysis of their methods and evaluate its performance on several spatial datasets. "
10176,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"Deep sparse networks ( DSNs ) USED-FOR high - order feature interactions. highsparsity features FEATURE-OF prediction task. prediction task EVALUATE-FOR Deep sparse networks ( DSNs ). computation efficiency EVALUATE-FOR models. feature - interaction layer PART-OF DSNs. neural architecture search USED-FOR problem. distilled search space USED-FOR architectures. progressive search algorithm USED-FOR sparse prediction tasks. progressive search algorithm USED-FOR order - priority property. order - priority property FEATURE-OF sparse prediction tasks. Task is model inference. Material is real - world benchmark datasets. Metric are accuracy, and efficiency. Method is search algorithm. ","Deep sparse networks (DSNs) have been shown to be able to capture high-order feature interactions, which is important for model inference. Deep sparse networks have been also shown to improve computation efficiency on a prediction task with highsparsity features. However, models trained with DSNs typically have a feature-interaction layer, which can be expensive to compute. This paper proposes a neural architecture search to tackle this problem. The authors propose a distilled search space to search for architectures that have the order-prioritization property, and propose a progressive search algorithm for sparse prediction tasks. Experiments are conducted on several real-world benchmark datasets, and show that the proposed search algorithm is able to achieve better accuracy and efficiency.","Deep sparse networks (DSNs) have been shown to be able to capture high-order feature interactions, which is important for model inference. Deep sparse networks have been also shown to improve computation efficiency on a prediction task with highsparsity features. However, models trained with DSNs typically have a feature-interaction layer, which can be expensive to compute. This paper proposes a neural architecture search to tackle this problem. The authors propose a distilled search space to search for architectures that have the order-prioritization property, and propose a progressive search algorithm for sparse prediction tasks. Experiments are conducted on several real-world benchmark datasets, and show that the proposed search algorithm is able to achieve better accuracy and efficiency."
10201,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,algorithm USED-FOR transfer learning. pre - trained model USED-FOR task. fine - tuning HYPONYM-OF algorithm. labeled data USED-FOR pre - trained model. labeled data USED-FOR task. fine - tuning USED-FOR overfitting. noise FEATURE-OF robustness. generalization properties EVALUATE-FOR fine - tuning. noise stability FEATURE-OF fine - tuned model. self label - correction CONJUNCTION label - reweighting. label - reweighting CONJUNCTION self label - correction. layer - wise regularization CONJUNCTION self label - correction. self label - correction CONJUNCTION layer - wise regularization. interpolation between regularization and self - labeling methods PART-OF regularized self - labeling. layer - wise regularization PART-OF interpolation between regularization and self - labeling methods. self label - correction PART-OF regularized self - labeling. layer - wise regularization PART-OF regularized self - labeling. pre - trained model architectures USED-FOR image and text data sets. image and text data sets EVALUATE-FOR approach. pre - trained model architectures USED-FOR approach. image classification tasks CONJUNCTION few - shot classification task. few - shot classification task CONJUNCTION image classification tasks. approach COMPARE baseline methods. baseline methods COMPARE approach. few - shot classification task EVALUATE-FOR approach. image classification tasks EVALUATE-FOR baseline methods. image classification tasks EVALUATE-FOR approach. approach COMPARE baseline methods. baseline methods COMPARE approach. Metric is PAC - Bayes generalization bound. OtherScientificTerm is noisy labels. ,"This paper proposes a new algorithm for transfer learning, called fine-tuning, which trains a pre-trained model on labeled data for a new task using only the labeled data from the previous task. The authors provide a PAC-Bayes generalization bound for the case of noisy labels. They show that the generalization properties of fine-tuning are robust to noise in the training data, and that the robustness to noise is enhanced when the noise is low. They also provide a theoretical analysis of the noise stability of a fine-tailed model. They further propose an interpolation between regularization and self-labeling methods, which includes layer-wise regularization, self label-correction, and label-reweighting. The proposed approach is tested on both image and text data sets with different pre-training model architectures for both standard and few-shot classification task, and the proposed approach outperforms several baseline methods. ","This paper proposes a new algorithm for transfer learning, called fine-tuning, which trains a pre-trained model on labeled data for a new task using only the labeled data from the previous task. The authors provide a PAC-Bayes generalization bound for the case of noisy labels. They show that the generalization properties of fine-tuning are robust to noise in the training data, and that the robustness to noise is enhanced when the noise is low. They also provide a theoretical analysis of the noise stability of a fine-tailed model. They further propose an interpolation between regularization and self-labeling methods, which includes layer-wise regularization, self label-correction, and label-reweighting. The proposed approach is tested on both image and text data sets with different pre-training model architectures for both standard and few-shot classification task, and the proposed approach outperforms several baseline methods. "
10226,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"value - at - risk ( VaR ) HYPONYM-OF tail - risk measures. finance and insurance industries HYPONYM-OF tail - risk measures. weighted sum of CVaR CONJUNCTION mean. mean CONJUNCTION weighted sum of CVaR. VaR CONJUNCTION weighted sum of CVaR. weighted sum of CVaR CONJUNCTION VaR. CVaR CONJUNCTION VaR. VaR CONJUNCTION CVaR. VaR CONJUNCTION mean. mean CONJUNCTION VaR. latter USED-FOR risk - return trade - off. risk - return trade - off FEATURE-OF finance. optimal δcorrect algorithm USED-FOR arms. heavy - tailed distributions FEATURE-OF arms. non - convex optimization problem USED-FOR algorithm. probability measures FEATURE-OF non - convex optimization problem. OtherScientificTerm are probability distributions, and arm. ","This paper studies tail-risk measures such as value-at-risk (VaR) and CVaR in the finance and insurance industries. The authors show that the optimal δcorrect algorithm for both of these arms is a non-convex optimization problem over probability distributions. They also show that for VaR, VaR and mean, the latter is optimal for the risk-return trade-off in finance. Finally, they provide an algorithm for the case where the arms have heavy-tailed distributions. ","This paper studies tail-risk measures such as value-at-risk (VaR) and CVaR in the finance and insurance industries. The authors show that the optimal δcorrect algorithm for both of these arms is a non-convex optimization problem over probability distributions. They also show that for VaR, VaR and mean, the latter is optimal for the risk-return trade-off in finance. Finally, they provide an algorithm for the case where the arms have heavy-tailed distributions. "
10251,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"Transformers USED-FOR computer vision tasks. Transformers USED-FOR modeling long - range dependency. self - attention mechanism USED-FOR modeling long - range dependency. intrinsic inductive bias ( IB ) USED-FOR modeling local visual structures. vision transformers USED-FOR image. 1D sequence of visual tokens USED-FOR image. 1D sequence of visual tokens USED-FOR vision transformers. training schedules USED-FOR IB. large - scale training data CONJUNCTION training schedules. training schedules CONJUNCTION large - scale training data. training schedules USED-FOR they. large - scale training data USED-FOR they. intrinsic IB USED-FOR Vision Transformer. ViTAE HYPONYM-OF convolutions. convolutions USED-FOR intrinsic IB. dilation rates FEATURE-OF convolutions. spatial pyramid reduction modules PART-OF ViTAE. it USED-FOR robust feature representation. it USED-FOR intrinsic scale invariance IB. convolution block PART-OF multi - head selfattention module. ViTAE PART-OF transformer layer. convolution block PART-OF transformer layer. convolution block PART-OF ViTAE. local features CONJUNCTION global dependencies. global dependencies CONJUNCTION local features. it USED-FOR global dependencies. it USED-FOR local features. intrinsic locality IB FEATURE-OF it. ImageNet EVALUATE-FOR ViTAE. downstream tasks EVALUATE-FOR ViTAE. ViTAE COMPARE concurrent works. concurrent works COMPARE ViTAE. ViTAE COMPARE baseline transformer. baseline transformer COMPARE ViTAE. downstream tasks EVALUATE-FOR concurrent works. downstream tasks EVALUATE-FOR baseline transformer. ImageNet CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet. baseline transformer CONJUNCTION concurrent works. concurrent works CONJUNCTION baseline transformer. ImageNet EVALUATE-FOR baseline transformer. ImageNet EVALUATE-FOR concurrent works. OtherScientificTerm are local visual structures, scale variance, and rich multi - scale context. Method are feed - forward network, and pretrained models. ","Transformers have been widely used for computer vision tasks, and the self-attention mechanism is particularly useful for modeling long-range dependency through the intrinsic inductive bias (IB) of the image. However, vision transformers typically learn an image from a 1D sequence of visual tokens, and they are trained with large-scale training data and training schedules that are insensitive to the intrinsic IB of the input image. This paper proposes Vision Transformer (ViTAE), which uses intrinsic IB to model local visual structures.  ViTAE consists of two convolutions: (1) convolutions with dilation rates that are invariant to scale variance, and (2) convolution block in the multi-head selfattention module. The convolutions have spatial pyramid reduction modules, which are used to reduce the scale variance in the input images.  The authors show that the convolutions can be learned in a feed-forward network, and that it can be used to learn robust feature representation. They also show that it is able to learn intrinsic scale invariance, and it can learn both local features and global dependencies.  Experiments on ImageNet, ImageNet and downstream tasks show that ViTAe outperforms a baseline transformer, concurrent works, and a number of recent pretrained models. The paper also shows that it also learns intrinsic locality IB, and can learn local features in a way that it does not rely on the rich multi-scale context. ","Transformers have been widely used for computer vision tasks, and the self-attention mechanism is particularly useful for modeling long-range dependency through the intrinsic inductive bias (IB) of the image. However, vision transformers typically learn an image from a 1D sequence of visual tokens, and they are trained with large-scale training data and training schedules that are insensitive to the intrinsic IB of the input image. This paper proposes Vision Transformer (ViTAE), which uses intrinsic IB to model local visual structures.  ViTAE consists of two convolutions: (1) convolutions with dilation rates that are invariant to scale variance, and (2) convolution block in the multi-head selfattention module. The convolutions have spatial pyramid reduction modules, which are used to reduce the scale variance in the input images.  The authors show that the convolutions can be learned in a feed-forward network, and that it can be used to learn robust feature representation. They also show that it is able to learn intrinsic scale invariance, and it can learn both local features and global dependencies.  Experiments on ImageNet, ImageNet and downstream tasks show that ViTAe outperforms a baseline transformer, concurrent works, and a number of recent pretrained models. The paper also shows that it also learns intrinsic locality IB, and can learn local features in a way that it does not rely on the rich multi-scale context. "
10276,SP:5e3572a386f890c5864437985cf63b13844f338f,fine - tuning USED-FOR NLP fields. pre - trained language models USED-FOR NLP fields. pre - trained language models USED-FOR fine - tuning. adversarial examples USED-FOR it. synonyms USED-FOR word substitution attacks. word substitution attacks HYPONYM-OF adversarial examples. adversarial training HYPONYM-OF defense technique. adversarial training USED-FOR fine - tuning scenario. catastrophic forgetting FEATURE-OF it. pre - trained model USED-FOR generic and robust linguistic features. Robust Informative Fine - Tuning ( RIFT ) HYPONYM-OF adversarial fine - tuning method. objective model USED-FOR features. RIFT USED-FOR objective model. pre - trained model USED-FOR features. pre - trained weights USED-FOR one. sentiment analysis CONJUNCTION natural language inference. natural language inference CONJUNCTION sentiment analysis. RIFT COMPARE state - of - the - arts. state - of - the - arts COMPARE RIFT. NLP tasks EVALUATE-FOR state - of - the - arts. NLP tasks EVALUATE-FOR RIFT. natural language inference HYPONYM-OF NLP tasks. sentiment analysis HYPONYM-OF NLP tasks. Method is BERT - based sentiment analysis model. Task is fine - tuning process. ,"This paper studies the problem of fine-tuning BERT-based sentiment analysis model. The authors propose a new adversarial training technique called Robust Informative Fine-Tuning (RIFT) to improve the robustness of pre-trained language models in NLP fields. Specifically, it uses adversarial examples (i.e., word substitution attacks using synonyms) to fine-tune the model and it aims to prevent catastrophic forgetting.   The authors show that adversarial attacks can be used as a defense technique in the fine tuning scenario. They also show that the pre-training of the BERT model can lead to the loss of generic and robust linguistic features.  The paper proposes a novel adversarial fine tuning method called RIFT, which is an extension of the Robust-Informative-Fine-Tuneing (RFT) method. RIFT uses an objective model based on RIFT to learn the features of a pre-trainable model, and then uses the learned features to fine tune a BERT based sentiment model. In addition, the authors propose two variants of RIFT. The first one is based on the use of a few pre-tuned weights, while the second one uses the weights of the original model.  Experiments show that RIFT outperforms the state-of-the-arts on several NLP tasks (sentiment analysis, natural language inference, and natural language reasoning) and outperforms RIFT in terms of robustness. The paper also shows that the authors also demonstrate that their method can be applied to a variety of other fine tuning methods. ","This paper studies the problem of fine-tuning BERT-based sentiment analysis model. The authors propose a new adversarial training technique called Robust Informative Fine-Tuning (RIFT) to improve the robustness of pre-trained language models in NLP fields. Specifically, it uses adversarial examples (i.e., word substitution attacks using synonyms) to fine-tune the model and it aims to prevent catastrophic forgetting.   The authors show that adversarial attacks can be used as a defense technique in the fine tuning scenario. They also show that the pre-training of the BERT model can lead to the loss of generic and robust linguistic features.  The paper proposes a novel adversarial fine tuning method called RIFT, which is an extension of the Robust-Informative-Fine-Tuneing (RFT) method. RIFT uses an objective model based on RIFT to learn the features of a pre-trainable model, and then uses the learned features to fine tune a BERT based sentiment model. In addition, the authors propose two variants of RIFT. The first one is based on the use of a few pre-tuned weights, while the second one uses the weights of the original model.  Experiments show that RIFT outperforms the state-of-the-arts on several NLP tasks (sentiment analysis, natural language inference, and natural language reasoning) and outperforms RIFT in terms of robustness. The paper also shows that the authors also demonstrate that their method can be applied to a variety of other fine tuning methods. "
10301,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"acceleration method USED-FOR fixed - point iterations. Anderson mixing ( AM ) HYPONYM-OF acceleration method. convergence theory FEATURE-OF AM. Stochastic Anderson Mixing ( SAM ) scheme USED-FOR nonconvex stochastic optimization problems. damped projection and adaptive regularization USED-FOR AM. damped projection and adaptive regularization USED-FOR Stochastic Anderson Mixing ( SAM ) scheme. almost sure convergence CONJUNCTION worst - case iteration complexity. worst - case iteration complexity CONJUNCTION almost sure convergence. convergence theory FEATURE-OF SAM. almost sure convergence FEATURE-OF stationary points. almost sure convergence PART-OF convergence theory. worst - case iteration complexity PART-OF convergence theory. variance reduction technique PART-OF SAM. preconditioned mixing strategy USED-FOR SAM. faster convergence CONJUNCTION generalization ability. generalization ability CONJUNCTION faster convergence. preconditioned mixing strategy USED-FOR faster convergence. generalization ability EVALUATE-FOR preconditioned mixing strategy. DenseNet CONJUNCTION LSTM. LSTM CONJUNCTION DenseNet. ResNeXt CONJUNCTION DenseNet. DenseNet CONJUNCTION ResNeXt. vanilla CNN CONJUNCTION ResNets. ResNets CONJUNCTION vanilla CNN. WideResNet CONJUNCTION ResNeXt. ResNeXt CONJUNCTION WideResNet. ResNets CONJUNCTION WideResNet. WideResNet CONJUNCTION ResNets. SAM method USED-FOR neural networks. LSTM HYPONYM-OF neural networks. DenseNet HYPONYM-OF neural networks. vanilla CNN HYPONYM-OF neural networks. ResNeXt HYPONYM-OF neural networks. WideResNet HYPONYM-OF neural networks. ResNets HYPONYM-OF neural networks. image classification and language model EVALUATE-FOR method. Task are scientific computing, and machine learning problems. Metric is complexity bound. ","This paper proposes Anderson mixing (AM), an acceleration method to accelerate fixed-point iterations in scientific computing. AM is based on damped projection and adaptive regularization, and the authors show that the Stochastic Anderson Mixing (SAM) scheme can be applied to nonconvex stochastic optimization problems. The convergence theory of AM in the convergence theory includes both almost sure convergence to stationary points and the worst-case iteration complexity. The authors also propose a variance reduction technique to speed up the convergence of SAM.   The paper also proposes a preconditioned mixing strategy to accelerate SAM, which is shown to achieve faster convergence and better generalization ability. Experiments on image classification and language model demonstrate the effectiveness of the proposed method. The SAM method is applied to several neural networks (vanilla CNN, ResNets, WideResNet, ResNeXt, DenseNet, LSTM) and neural networks trained with the same parameters (e.g., weights). The paper shows that SAM achieves a complexity bound of O(1/\sqrt{T} where T is the number of parameters). ","This paper proposes Anderson mixing (AM), an acceleration method to accelerate fixed-point iterations in scientific computing. AM is based on damped projection and adaptive regularization, and the authors show that the Stochastic Anderson Mixing (SAM) scheme can be applied to nonconvex stochastic optimization problems. The convergence theory of AM in the convergence theory includes both almost sure convergence to stationary points and the worst-case iteration complexity. The authors also propose a variance reduction technique to speed up the convergence of SAM.   The paper also proposes a preconditioned mixing strategy to accelerate SAM, which is shown to achieve faster convergence and better generalization ability. Experiments on image classification and language model demonstrate the effectiveness of the proposed method. The SAM method is applied to several neural networks (vanilla CNN, ResNets, WideResNet, ResNeXt, DenseNet, LSTM) and neural networks trained with the same parameters (e.g., weights). The paper shows that SAM achieves a complexity bound of O(1/\sqrt{T} where T is the number of parameters). "
10326,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"posterior distribution USED-FOR linear inverse problem. SNIPS HYPONYM-OF stochastic algorithm. Langevin dynamics CONJUNCTION Newton ’s method. Newton ’s method CONJUNCTION Langevin dynamics. Newton ’s method USED-FOR solution. Langevin dynamics USED-FOR solution. singular value decomposition ( SVD ) USED-FOR degradation operator. singular value decomposition ( SVD ) PART-OF posterior score function. paradigm USED-FOR image deblurring. image deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION image deblurring. super - resolution CONJUNCTION compressive sensing. compressive sensing CONJUNCTION super - resolution. paradigm USED-FOR super - resolution. paradigm USED-FOR compressive sensing. OtherScientificTerm are additive white Gaussian noise, and noisy observation. Generic are approach, and algorithm. Method is iterative algorithm. Task is inverse problem. ","This paper considers the linear inverse problem with additive white Gaussian noise, where the posterior distribution of the data is known. The authors propose a stochastic algorithm called SNIPS, which is an iterative algorithm for solving the inverse problem. The solution is based on Langevin dynamics and Newton’s method. The approach is novel.    The key idea of the proposed algorithm is to learn the posterior score function by incorporating the singular value decomposition (SVD) of the degradation operator into the prior score function. The paper shows that this paradigm can be applied to image deblurring, super-resolution, and compressive sensing. However, the paper does not consider noisy observation.","This paper considers the linear inverse problem with additive white Gaussian noise, where the posterior distribution of the data is known. The authors propose a stochastic algorithm called SNIPS, which is an iterative algorithm for solving the inverse problem. The solution is based on Langevin dynamics and Newton’s method. The approach is novel.    The key idea of the proposed algorithm is to learn the posterior score function by incorporating the singular value decomposition (SVD) of the degradation operator into the prior score function. The paper shows that this paradigm can be applied to image deblurring, super-resolution, and compressive sensing. However, the paper does not consider noisy observation."
10351,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"Instagram HYPONYM-OF social media. techniques USED-FOR illicit drug trades. meta - learning technique USED-FOR MetaHG. multimodal content CONJUNCTION relational structured information. relational structured information CONJUNCTION multimodal content. holistic framework USED-FOR illicit drug traffickers. MetaHG USED-FOR illicit drug trafficker detection. relational structured information USED-FOR illicit drug trafficker detection. MetaHG USED-FOR multimodal content. MetaHG USED-FOR relational structured information. MetaHG USED-FOR illicit drug traffickers. social media FEATURE-OF relational structured information. MetaHG HYPONYM-OF holistic framework. social media USED-FOR illicit drug traffickers. Instagram HYPONYM-OF social media. heterogeneous graph ( HG ) USED-FOR MetaHG. relation - based graph convolutional neural network USED-FOR node ( i.e., user ) representations. graph structure refinement USED-FOR sparse connection among entities. graph structure refinement USED-FOR node representation learning. sparse connection among entities PART-OF HG. graph structure refinement USED-FOR HG. HG USED-FOR relation - based graph convolutional neural network. HG USED-FOR node ( i.e., user ) representations. meta - learning algorithm USED-FOR model optimization. self - supervised module CONJUNCTION knowledge distillation module. knowledge distillation module CONJUNCTION self - supervised module. unlabeled data USED-FOR model. knowledge distillation module USED-FOR model. knowledge distillation module USED-FOR unlabeled data. self - supervised module USED-FOR model. self - supervised module USED-FOR unlabeled data. MetaHG COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MetaHG. real - world data EVALUATE-FOR MetaHG. real - world data EVALUATE-FOR state - of - the - art methods. Instagram FEATURE-OF real - world data. Task are crime of drug trafficking, online drug trafficking, and model training. Material is post content. ","This paper proposes MetaHG, a meta-learning framework for drug-trafficking detection based on the heterogeneous graph (HG) framework. The proposed framework is based on a graph structure refinement approach to learn a graph convolutional neural network (GCNN) for multi-modal content and relational information. The authors also propose a self-supervised meta-training method to improve the performance of the GCNN. Experiments show that the proposed method outperforms state-of-the-art methods on a number of real-world datasets. ","This paper proposes MetaHG, a meta-learning framework for drug-trafficking detection based on the heterogeneous graph (HG) framework. The proposed framework is based on a graph structure refinement approach to learn a graph convolutional neural network (GCNN) for multi-modal content and relational information. The authors also propose a self-supervised meta-training method to improve the performance of the GCNN. Experiments show that the proposed method outperforms state-of-the-art methods on a number of real-world datasets. "
10376,SP:242da1384f48260d58a0e7949438611c05079197,"ReLU activations CONJUNCTION architecture. architecture CONJUNCTION ReLU activations. architecture USED-FOR neural network. ReLU activations USED-FOR neural network. neural network USED-FOR class of functions. polyhedral theory CONJUNCTION tropical geometry. tropical geometry CONJUNCTION polyhedral theory. mixed - integer optimization CONJUNCTION polyhedral theory. polyhedral theory CONJUNCTION mixed - integer optimization. hidden layer USED-FOR learning tasks. techniques USED-FOR mathematical counterbalance. mathematical counterbalance USED-FOR universal approximation theorems. polyhedral theory USED-FOR techniques. mixed - integer optimization USED-FOR techniques. upper bounds FEATURE-OF neural networks. neural networks USED-FOR neural hypothesis classes. OtherScientificTerm are layers, and neural network literature. Task is algorithmic and statistical aspects. ","This paper studies the universal approximation theorems of neural networks with ReLU activations and architecture for a class of functions. The authors use techniques from mixed-integer optimization, polyhedral theory, and tropical geometry to provide a mathematical counterbalance to the commonly used universal approximation results in the neural network literature. They also provide upper bounds on the number of layers in neural networks for neural hypothesis classes, and show that a single hidden layer can be used as a counterbalance for many learning tasks. The paper is well-written and well-motivated, and the algorithmic and statistical aspects are clear. ","This paper studies the universal approximation theorems of neural networks with ReLU activations and architecture for a class of functions. The authors use techniques from mixed-integer optimization, polyhedral theory, and tropical geometry to provide a mathematical counterbalance to the commonly used universal approximation results in the neural network literature. They also provide upper bounds on the number of layers in neural networks for neural hypothesis classes, and show that a single hidden layer can be used as a counterbalance for many learning tasks. The paper is well-written and well-motivated, and the algorithmic and statistical aspects are clear. "
10401,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"worst - case training principle USED-FOR maximal adversarial loss. min - max optimization USED-FOR AT. min - max optimization PART-OF adversarial context. framework USED-FOR adversarial attacks. min - max optimization USED-FOR adversarial attacks. framework USED-FOR min - max optimization. probability simplex FEATURE-OF domain weights. unified framework USED-FOR attack generation problems. unified framework USED-FOR crafting attacks. crafting attacks USED-FOR data transformations. crafting attacks HYPONYM-OF attack generation problems. attacking model ensembles HYPONYM-OF attack generation problems. approach COMPARE heuristic strategies. heuristic strategies COMPARE approach. robustness EVALUATE-FOR defense methods. heuristic strategies COMPARE defense methods. defense methods COMPARE heuristic strategies. approach COMPARE defense methods. defense methods COMPARE approach. robustness EVALUATE-FOR heuristic strategies. robustness EVALUATE-FOR approach. self - adjusted domain weights USED-FOR difficulty level of attack. min - max framework USED-FOR self - adjusted domain weights. Method is adversarial training ( AT ). Task are adversarial robustness, and min - max problem. OtherScientificTerm are risk sources, and universal perturbation. Metric is worst - case attack loss. ","This paper studies the problem of adversarial training (AT) and adversarial robustness. The authors propose a new worst-case training principle for maximizing the maximal adversarial loss, which they call ""worst-case attack loss"". They show that the min-max optimization in the adversarial context can be seen as a special case of min- max optimization in AT. They then propose a unified framework for both adversarial attacks and crafting attacks, which are attack generation problems (e.g. attacking model ensembles). The authors show that under certain conditions, the worst case attack loss can be minimized. They also show that self-adjusted domain weights (i.e. the probability simplex of the domain weights) can be used to improve the performance of the proposed framework for adversarial optimization. Finally, they show that their approach outperforms heuristic strategies and other defense methods in terms of robustness to adversarial perturbations. ","This paper studies the problem of adversarial training (AT) and adversarial robustness. The authors propose a new worst-case training principle for maximizing the maximal adversarial loss, which they call ""worst-case attack loss"". They show that the min-max optimization in the adversarial context can be seen as a special case of min- max optimization in AT. They then propose a unified framework for both adversarial attacks and crafting attacks, which are attack generation problems (e.g. attacking model ensembles). The authors show that under certain conditions, the worst case attack loss can be minimized. They also show that self-adjusted domain weights (i.e. the probability simplex of the domain weights) can be used to improve the performance of the proposed framework for adversarial optimization. Finally, they show that their approach outperforms heuristic strategies and other defense methods in terms of robustness to adversarial perturbations. "
10426,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. model USED-FOR sparse PCA. model USED-FOR tensor PCA. Wigner form FEATURE-OF sparse PCA. polynomial - time algorithm CONJUNCTION exponential - time exhaustive search algorithm. exponential - time exhaustive search algorithm CONJUNCTION polynomial - time algorithm. polynomial - time algorithm USED-FOR algorithms. exponential - time exhaustive search algorithm USED-FOR algorithms. algorithms USED-FOR sparse vector. signal - tonoise ratio λ FEATURE-OF sparse vector. algorithms USED-FOR sparse vectors. algorithms COMPARE algorithms. algorithms COMPARE algorithms. λ FEATURE-OF sparse vectors. algorithms USED-FOR sparse PCA. signal - to - noise ratio CONJUNCTION running time. running time CONJUNCTION signal - to - noise ratio. sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. lower bound USED-FOR lower bounds. lower bounds USED-FOR sparse PCA. lower bound USED-FOR sparse PCA. lower bounds USED-FOR tensor PCA. Task is sparse tensor principal component analysis. OtherScientificTerm are i.i.d. Gaussian entries, k - sparse unit vector, k - sparse signals, and sparsity k. Generic is matrix settings. Metric is low - degree likelihood ratio. ","This paper studies the problem of sparse tensor principal component analysis, where the i.i.d. Gaussian entries of a k-sparse unit vector are assumed to be sparse. The authors propose a new model for sparse PCA and tensor PCA based on the Wigner form. The algorithms are based on a polynomial-time algorithm and an exponential-time exhaustive search algorithm. They show that under certain matrix settings, the algorithms converge to a sparse vector with a signal-tonoise ratio λ, which is a low-degree likelihood ratio. They also show that the algorithms that converge to the sparse vectors with λ outperform existing algorithms in terms of both the signal-to-noise ratio and running time. Finally, the authors provide lower bounds for the lower bound of the generalization error of the algorithms for both the case of k-sparse signals and the case where the sparsity k is small.   ","This paper studies the problem of sparse tensor principal component analysis, where the i.i.d. Gaussian entries of a k-sparse unit vector are assumed to be sparse. The authors propose a new model for sparse PCA and tensor PCA based on the Wigner form. The algorithms are based on a polynomial-time algorithm and an exponential-time exhaustive search algorithm. They show that under certain matrix settings, the algorithms converge to a sparse vector with a signal-tonoise ratio λ, which is a low-degree likelihood ratio. They also show that the algorithms that converge to the sparse vectors with λ outperform existing algorithms in terms of both the signal-to-noise ratio and running time. Finally, the authors provide lower bounds for the lower bound of the generalization error of the algorithms for both the case of k-sparse signals and the case where the sparsity k is small.   "
10451,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"Multilayer - perceptrons ( MLP ) USED-FOR learning functions of high - frequencies. spatially adaptive progressive encoding ( SAPE ) scheme USED-FOR MLP networks. spatially adaptive progressive encoding ( SAPE ) scheme USED-FOR them. feedback loop USED-FOR neural optimization process. feedback loop USED-FOR progressive exposure of frequencies. regression of low dimensional signals CONJUNCTION images. images CONJUNCTION regression of low dimensional signals. representation learning of occupancy networks CONJUNCTION geometric task of mesh transfer. geometric task of mesh transfer CONJUNCTION representation learning of occupancy networks. regression of low dimensional signals CONJUNCTION representation learning of occupancy networks. representation learning of occupancy networks CONJUNCTION regression of low dimensional signals. SAPE USED-FOR applications. geometric task of mesh transfer HYPONYM-OF applications. regression of low dimensional signals HYPONYM-OF applications. images HYPONYM-OF applications. representation learning of occupancy networks HYPONYM-OF applications. OtherScientificTerm are wide frequency bands, and 3D shapes. Metric is training stability. Method is domain specific preprocessing. ","Multilayer-perceptrons (MLP) have been widely used to learn learning functions of high-frequencies in wide frequency bands. This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for training MLP networks that allows them to adaptively expose different frequencies of the input signal to each layer of the MLP. The key idea is to use a feedback loop in the neural optimization process to guide the progressive exposure of frequencies. The authors show that SAPE can be applied to several applications including regression of low dimensional signals and images, representation learning of occupancy networks, and the geometric task of mesh transfer between 3D shapes. The paper also shows that the training stability of SAPE is improved when the domain specific preprocessing is used. ","Multilayer-perceptrons (MLP) have been widely used to learn learning functions of high-frequencies in wide frequency bands. This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for training MLP networks that allows them to adaptively expose different frequencies of the input signal to each layer of the MLP. The key idea is to use a feedback loop in the neural optimization process to guide the progressive exposure of frequencies. The authors show that SAPE can be applied to several applications including regression of low dimensional signals and images, representation learning of occupancy networks, and the geometric task of mesh transfer between 3D shapes. The paper also shows that the training stability of SAPE is improved when the domain specific preprocessing is used. "
10476,SP:b03063fa82d76db341076e5f282176f4c007a202,"probability simplex constraints FEATURE-OF constrained saddle - point optimization problem. constrained saddle - point optimization problem USED-FOR equilibrium of competitive games. methods USED-FOR constrained settings. unconstrained setting FEATURE-OF extragradient methods. entropy regularization USED-FOR single - agent reinforcement learning and game theory. entropy regularization FEATURE-OF zero - sum two - player matrix games. algorithms USED-FOR approximate Nash equilibrium. approximate Nash equilibrium FEATURE-OF unregularized matrix game. knob of entropy regularization USED-FOR algorithms. policy extragradient algorithms USED-FOR entropy - regularized zero - sum Markov games. methods USED-FOR policy extragradient algorithms. linear rate FEATURE-OF policy extragradient algorithms. entropy regularization USED-FOR accelerating convergence. logarithm factors FEATURE-OF state and action spaces. OtherScientificTerm are multiplicative updates, objective function, sublinear rate, and Nash equilibrium. Method is symmetric and multiplicative updates. Metric is convergence rates. ","This paper studies a constrained saddle-point optimization problem under probability simplex constraints for the equilibrium of competitive games under the assumption that there are no multiplicative updates to the objective function. The authors show that existing extragradient methods in the unconstrained setting converge at a sublinear rate, and extend these methods to the constrained settings. They also show that the optimal solution of the constrained saddle point optimization problem is the Nash equilibrium of a matrix game under the same assumption.    The authors extend the work on entropy regularization in single-agent reinforcement learning and game theory to zero-sum two-player matrix games, and show that there exists an approximate Nash equilibrium for the unregularized matrix game. They then provide algorithms that converge to this Nash equilibrium with a linear rate for both symmetric and multiplicative upsampling and downsampling, and they show that these algorithms can also converge with the knob of entropy regularisation. Finally, the authors provide a theoretical analysis of the convergence rates of these methods for policy extragadient algorithms in the case of entropy-regularized Markov games. They show that under certain assumptions on the logarithm factors of the state and action spaces, these algorithms converge at the rate of $O(1/\sqrt{T})$ for all policies, and $O(\log T)$ for policies that have $T=1/T$ or more.  Finally, they provide an empirical analysis that shows that the algorithms converge to the optimal Nash equilibrium at the linear rate of the optimal policy in the setting where the entropy regularized zero sum Markov game is non-convex.  They also provide an experimental analysis of their results, showing that their algorithms outperform existing methods in practice. ","This paper studies a constrained saddle-point optimization problem under probability simplex constraints for the equilibrium of competitive games under the assumption that there are no multiplicative updates to the objective function. The authors show that existing extragradient methods in the unconstrained setting converge at a sublinear rate, and extend these methods to the constrained settings. They also show that the optimal solution of the constrained saddle point optimization problem is the Nash equilibrium of a matrix game under the same assumption.    The authors extend the work on entropy regularization in single-agent reinforcement learning and game theory to zero-sum two-player matrix games, and show that there exists an approximate Nash equilibrium for the unregularized matrix game. They then provide algorithms that converge to this Nash equilibrium with a linear rate for both symmetric and multiplicative upsampling and downsampling, and they show that these algorithms can also converge with the knob of entropy regularisation. Finally, the authors provide a theoretical analysis of the convergence rates of these methods for policy extragadient algorithms in the case of entropy-regularized Markov games. They show that under certain assumptions on the logarithm factors of the state and action spaces, these algorithms converge at the rate of $O(1/\sqrt{T})$ for all policies, and $O(\log T)$ for policies that have $T=1/T$ or more.  Finally, they provide an empirical analysis that shows that the algorithms converge to the optimal Nash equilibrium at the linear rate of the optimal policy in the setting where the entropy regularized zero sum Markov game is non-convex.  They also provide an experimental analysis of their results, showing that their algorithms outperform existing methods in practice. "
10501,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"remote cooperation CONJUNCTION online education. online education CONJUNCTION remote cooperation. screen sharing CONJUNCTION remote cooperation. remote cooperation CONJUNCTION screen sharing. HR display USED-FOR super - resolution ( SR ). image SR methods USED-FOR natural images. image SR methods USED-FOR SCIs. pixel values USED-FOR continuous SR. implicit transformer USED-FOR image features. image features USED-FOR pixel values. LR and HR SCI pairs USED-FOR SCI1 K and SCI1K - compression datasets. continuous and discrete SR methods USED-FOR compressed and uncompressed SCIs. ITSRN COMPARE continuous and discrete SR methods. continuous and discrete SR methods COMPARE ITSRN. ITSRN USED-FOR compressed and uncompressed SCIs. Material is screen contents. OtherScientificTerm are limited terminal bandwidth, high - resolution ( HR ) screen contents, and image characteristics. Task is SCI browsing. Method are SCISR, and implicit position encoding scheme. ","This paper proposes SCISR, a method for super-resolution (SR) compression of high-res (HR) images in the presence of limited terminal bandwidth. SCI browsing is an important problem in the context of screen sharing, remote cooperation, online education, and online education. SCIs are typically generated by image SR methods that can be used to compress natural images from a HR display, but cannot be used for SCIs due to the limited bandwidth. In this paper, the authors propose an implicit position encoding scheme, where pixel values for continuous SR are encoded using image features extracted from an implicit transformer. The authors show that the resulting SCI1K-compression datasets can be decomposed into LR and HR SCI pairs. They also show that ITSRN outperforms previous continuous and discrete SR methods for both compressed and uncompressed SCIs.  ","This paper proposes SCISR, a method for super-resolution (SR) compression of high-res (HR) images in the presence of limited terminal bandwidth. SCI browsing is an important problem in the context of screen sharing, remote cooperation, online education, and online education. SCIs are typically generated by image SR methods that can be used to compress natural images from a HR display, but cannot be used for SCIs due to the limited bandwidth. In this paper, the authors propose an implicit position encoding scheme, where pixel values for continuous SR are encoded using image features extracted from an implicit transformer. The authors show that the resulting SCI1K-compression datasets can be decomposed into LR and HR SCI pairs. They also show that ITSRN outperforms previous continuous and discrete SR methods for both compressed and uncompressed SCIs.  "
10526,SP:3751625929b707ced417c3eb10064e4917866048,"probabilistic models USED-FOR causality. sumproduct networks ( SPNs ) USED-FOR learning interventional distributions. gate functions USED-FOR sumproduct networks ( SPNs ). neural networks HYPONYM-OF gate functions. gate function USED-FOR SPN. structural causal model USED-FOR interventional SPNs. personal health FEATURE-OF structural causal model. generative and causal modelling USED-FOR methods. Generic is so. Task is intractability of inference. Method is causal models. OtherScientificTerm are interventional distributions, arbitrarily intervened causal graph, and Pearl ’s do - operator. ","This paper proposes a new class of probabilistic models for learning causality. The authors propose sumproduct networks (SPNs) for learning interventional distributions, which are based on gate functions (similar to neural networks). The authors argue that the intractability of inference is due to the fact that existing causal models can only learn the interventional distribution, which is intractable. To overcome this problem, the authors propose to learn a gate function for the SPN, which they call a structural causal model. This gate function can be applied to any arbitrary intervened causal graph, and the authors use Pearl’s do-operator. The proposed methods are tested on both generative and causal modelling, and are shown to be able to learn interventional SPNs that can be used to infer the underlying structure of the underlying causal model in personal health. ","This paper proposes a new class of probabilistic models for learning causality. The authors propose sumproduct networks (SPNs) for learning interventional distributions, which are based on gate functions (similar to neural networks). The authors argue that the intractability of inference is due to the fact that existing causal models can only learn the interventional distribution, which is intractable. To overcome this problem, the authors propose to learn a gate function for the SPN, which they call a structural causal model. This gate function can be applied to any arbitrary intervened causal graph, and the authors use Pearl’s do-operator. The proposed methods are tested on both generative and causal modelling, and are shown to be able to learn interventional SPNs that can be used to infer the underlying structure of the underlying causal model in personal health. "
10551,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"Factor analysis methods USED-FOR low dimensional, ideally interpretable representations. Factor analysis methods USED-FOR neuroimaging. Factor analysis methods USED-FOR high dimensional imaging data. high dimensional imaging data USED-FOR low dimensional, ideally interpretable representations. deep Markov factor analysis ( DMFA ) HYPONYM-OF generative model. Markov property USED-FOR low dimensional temporal embeddings. Markov property CONJUNCTION spatial inductive assumptions. spatial inductive assumptions CONJUNCTION Markov property. temporal dynamics FEATURE-OF functional magnetic resonance imaging ( fMRI ) data. Markov property USED-FOR generative model. discrete latent USED-FOR DMFA. DMFA USED-FOR fMRI data. low dimensional temporal embedding USED-FOR DMFA. DMFA USED-FOR interpretable clusters. DMFA USED-FOR nonlinear temporal dependencies. synthetic and real fMRI data EVALUATE-FOR DMFA. nonlinear temporal dependencies FEATURE-OF high dimensional imaging data. Generic is methods. OtherScientificTerm are nonlinear and complex temporal dynamics of neural processes, high spatial dimensionality, and subject and cognitive state variability. Material is imaging data. Method is neural networks. Task are fMRI - driven neuroscientific hypotheses, and capturing nonlinear temporal dependencies. ","Factor analysis methods for neuroimaging have been widely used to learn low dimensional, ideally interpretable representations from high dimensional imaging data. However, these methods can be problematic due to the nonlinear and complex temporal dynamics of neural processes. This paper proposes a generative model called deep Markov factor analysis (DMFA), which uses the Markov property of the generative process to learn a low dimensional temporal embeddings. The authors show that under certain conditions, DMFA can capture nonlinear temporal dependencies in functional magnetic resonance imaging (fMRI) data with temporal dynamics that are non-trivial to capture in high spatial dimensionality. They show that DMFA is able to capture fMRI data with a discrete latent, and can be trained with neural networks. They also show that the DMFA captures the subject and cognitive state variability in fMRI-driven neuroscientific hypotheses. Finally, they show that using DMFA, they can learn interpretable clusters that are more likely to be non-linear in nature. They evaluate DMFA on both synthetic and real fMI data, and show that their DMFA achieves state-of-the-art performance on capturing nonlinear spatial dependencies in high-dimensional imaging data, especially in the case of imaging data that has a low spatial dimension. ","Factor analysis methods for neuroimaging have been widely used to learn low dimensional, ideally interpretable representations from high dimensional imaging data. However, these methods can be problematic due to the nonlinear and complex temporal dynamics of neural processes. This paper proposes a generative model called deep Markov factor analysis (DMFA), which uses the Markov property of the generative process to learn a low dimensional temporal embeddings. The authors show that under certain conditions, DMFA can capture nonlinear temporal dependencies in functional magnetic resonance imaging (fMRI) data with temporal dynamics that are non-trivial to capture in high spatial dimensionality. They show that DMFA is able to capture fMRI data with a discrete latent, and can be trained with neural networks. They also show that the DMFA captures the subject and cognitive state variability in fMRI-driven neuroscientific hypotheses. Finally, they show that using DMFA, they can learn interpretable clusters that are more likely to be non-linear in nature. They evaluate DMFA on both synthetic and real fMI data, and show that their DMFA achieves state-of-the-art performance on capturing nonlinear spatial dependencies in high-dimensional imaging data, especially in the case of imaging data that has a low spatial dimension. "
10576,SP:855dcaa42868a29a14619d63221169495ed5dd54,"spheres CONJUNCTION tori. tori CONJUNCTION spheres. generative models USED-FOR complex geometries. tori CONJUNCTION implicit surfaces. implicit surfaces CONJUNCTION tori. manifolds USED-FOR complex geometries. implicit surfaces HYPONYM-OF manifolds. spheres HYPONYM-OF complex geometries. spheres HYPONYM-OF manifolds. tori HYPONYM-OF complex geometries. tori HYPONYM-OF manifolds. Moser Flow ( MF ) HYPONYM-OF generative models. continuous normalizing flows ( CNF ) FEATURE-OF generative models. MF USED-FOR CNF. source ( prior ) density CONJUNCTION divergence. divergence CONJUNCTION source ( prior ) density. CNF methods COMPARE model ( learned ) density. model ( learned ) density COMPARE CNF methods. divergence PART-OF neural network ( NN ). source ( prior ) density USED-FOR model ( learned ) density. divergence HYPONYM-OF local, linear differential operator. CNFs COMPARE MF. MF COMPARE CNFs. divergence USED-FOR model density. NN USED-FOR model density. MF USED-FOR universal density approximator. flow models USED-FOR sampling from general curved surfaces. training complexity EVALUATE-FOR CNFs. sample quality EVALUATE-FOR CNFs. sample quality CONJUNCTION training complexity. training complexity CONJUNCTION sample quality. synthetic geometries CONJUNCTION real - world benchmarks. real - world benchmarks CONJUNCTION synthetic geometries. density estimation CONJUNCTION sample quality. sample quality CONJUNCTION density estimation. flow models COMPARE CNFs. CNFs COMPARE flow models. density estimation EVALUATE-FOR CNFs. earth and climate sciences FEATURE-OF synthetic geometries. earth and climate sciences FEATURE-OF real - world benchmarks. earth and climate sciences EVALUATE-FOR CNFs. real - world benchmarks EVALUATE-FOR CNFs. synthetic geometries EVALUATE-FOR CNFs. Method are Euclidean ) generative models, and ODE solver. OtherScientificTerm are change - of -","This paper proposes a new family of generative models for complex geometries on manifolds (spheres, tori, implicit surfaces, etc.). The authors propose continuous normalizing flows (CNF) which are a generalization of previous (Euclidean) generative model, the Moser Flow (MF). The authors show that CNFs can be seen as a special case of the MF, which is a generalisation of the ODE solver. The authors also show that MF is a universal density approximator, and that MF can be used as a universal normalizing flow.   The authors demonstrate that the CNF methods are more efficient than the original model (learned) density of a neural network (NN) based on the source (prior) density and the divergence of a local, linear differential operator. They show that the model density of the NN is a function of the divergence between the prior and the change-of-divergence of the source density. They also show how the divergence can be incorporated into the training complexity of the Neural Network to improve the performance of the model. Finally, the authors show how flow models can be applied to sampling from general curved surfaces, and show that flow models are able to achieve better density estimation, sample quality, and training complexity compared to the original CNF. They demonstrate how the density estimation can be improved by the use of flow models, and demonstrate that C NFs outperform the original MF in terms of training complexity and training time, and also outperform MF. They further show how to use flow models to sample from general manifolds such as spheres and tori and show how they can be combined with the MF to obtain better performance. They evaluate CNFMs on synthetic geometrie and real-world benchmarks from the earth and climate sciences, showing that the proposed CNF models outperform existing flow models. ","This paper proposes a new family of generative models for complex geometries on manifolds (spheres, tori, implicit surfaces, etc.). The authors propose continuous normalizing flows (CNF) which are a generalization of previous (Euclidean) generative model, the Moser Flow (MF). The authors show that CNFs can be seen as a special case of the MF, which is a generalisation of the ODE solver. The authors also show that MF is a universal density approximator, and that MF can be used as a universal normalizing flow.   The authors demonstrate that the CNF methods are more efficient than the original model (learned) density of a neural network (NN) based on the source (prior) density and the divergence of a local, linear differential operator. They show that the model density of the NN is a function of the divergence between the prior and the change-of-divergence of the source density. They also show how the divergence can be incorporated into the training complexity of the Neural Network to improve the performance of the model. Finally, the authors show how flow models can be applied to sampling from general curved surfaces, and show that flow models are able to achieve better density estimation, sample quality, and training complexity compared to the original CNF. They demonstrate how the density estimation can be improved by the use of flow models, and demonstrate that C NFs outperform the original MF in terms of training complexity and training time, and also outperform MF. They further show how to use flow models to sample from general manifolds such as spheres and tori and show how they can be combined with the MF to obtain better performance. They evaluate CNFMs on synthetic geometrie and real-world benchmarks from the earth and climate sciences, showing that the proposed CNF models outperform existing flow models. "
10601,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"framework USED-FOR unsupervised representation learning. Independent component analysis USED-FOR unsupervised representation learning. observed variables PART-OF generative process. independent causal mechanisms USED-FOR causality. assumptions USED-FOR independent causal mechanisms. approach USED-FOR nonidentifiability issues. nonidentifiability issues FEATURE-OF nonlinear blind source separation. OtherScientificTerm are latent code, mixing, statistical independence, Identifiability, and mixing process. Generic is model. Method is independent mechanism analysis. ","This paper proposes a new framework for unsupervised representation learning based on Independent component analysis. The key idea is to model the generative process as a set of observed variables that are independent of the latent code, and to use independent causal mechanisms to model causality between the observed variables and the latent codes. The authors show that under certain assumptions on the mixing process, there exists independent mechanisms that are responsible for the mixing. They also show that this approach avoids nonidentifiability issues in the case of nonlinear blind source separation.    Identifiability is defined as the statistical independence between the data points in the model and the underlying latent code. The paper also shows that independent mechanism analysis can be used to identify the source of the non-identifiable data points. ","This paper proposes a new framework for unsupervised representation learning based on Independent component analysis. The key idea is to model the generative process as a set of observed variables that are independent of the latent code, and to use independent causal mechanisms to model causality between the observed variables and the latent codes. The authors show that under certain assumptions on the mixing process, there exists independent mechanisms that are responsible for the mixing. They also show that this approach avoids nonidentifiability issues in the case of nonlinear blind source separation.    Identifiability is defined as the statistical independence between the data points in the model and the underlying latent code. The paper also shows that independent mechanism analysis can be used to identify the source of the non-identifiable data points. "
10626,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"Annealed Importance Sampling ( AIS ) HYPONYM-OF method. Hamiltonian MCMC USED-FOR Annealed Importance Sampling ( AIS ). non - differentiable transition kernels USED-FOR it. AIS - like procedure USED-FOR framework. Uncorrected Hamiltonian MCMC USED-FOR AIS - like procedure. Uncorrected Hamiltonian Annealing HYPONYM-OF AIS - like procedure. method COMPARE approaches. approaches COMPARE method. method USED-FOR tight and differentiable lower bounds. method COMPARE approaches. approaches COMPARE method. OtherScientificTerm are unnormalized target distribution, tight lower bound, and reparameterization gradients. ","This paper proposes a new method called Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The idea is that it uses non-differentiable transition kernels to sample from an unnormalized target distribution. The paper also proposes an AIS-like procedure called Uncorrected Hamiltonian Annealing, which is a generalization of the AIS framework. The proposed method is shown to provide tight and differentiable lower bounds on the number of samples needed to achieve a tight lower bound. The authors also show that the proposed method outperforms existing approaches when the reparameterization gradients of the target distribution are small. ","This paper proposes a new method called Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The idea is that it uses non-differentiable transition kernels to sample from an unnormalized target distribution. The paper also proposes an AIS-like procedure called Uncorrected Hamiltonian Annealing, which is a generalization of the AIS framework. The proposed method is shown to provide tight and differentiable lower bounds on the number of samples needed to achieve a tight lower bound. The authors also show that the proposed method outperforms existing approaches when the reparameterization gradients of the target distribution are small. "
10651,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"deep neural networks USED-FOR safetycritical applications. training algorithms USED-FOR neural network. training algorithms USED-FOR robustness. Certified robustness FEATURE-OF deep neural networks. robustness EVALUATE-FOR neural network. Lipschitz constant FEATURE-OF global bound. global bound USED-FOR training algorithms. non - convexity FEATURE-OF network. natural and certified accuracy EVALUATE-FOR tighter Lipschitz bound. activation functions CONJUNCTION weight matrices. weight matrices CONJUNCTION activation functions. induced norm FEATURE-OF weight matrix. global Lipschitz constant FEATURE-OF neural network. method USED-FOR plug - in module. plug - in module USED-FOR Lipschitz bound. method USED-FOR Lipschitz bound. Lipschitz bound FEATURE-OF certifiable training algorithms. upper threshold CONJUNCTION sparsity loss. sparsity loss CONJUNCTION upper threshold. ReLU CONJUNCTION MaxMin. MaxMin CONJUNCTION ReLU. sparsity loss USED-FOR network. network USED-FOR local Lipschitz bound. upper threshold USED-FOR activation functions. MaxMin HYPONYM-OF activation functions. ReLU HYPONYM-OF activation functions. method COMPARE methods. methods COMPARE method. network architectures USED-FOR method. TinyImageNet datasets EVALUATE-FOR methods. clean and certified accuracy EVALUATE-FOR methods. MNIST EVALUATE-FOR methods. TinyImageNet datasets EVALUATE-FOR method. clean and certified accuracy EVALUATE-FOR method. MNIST EVALUATE-FOR method. Generic are bound, and it. Metric is natural accuracy. OtherScientificTerm are local Lipschitz upper bound, and activation function. ","Certified robustness is an important problem in deep neural networks for safetycritical applications. In this paper, the authors propose a new bound on the Lipschitz constant of the global bound of training algorithms for a neural network to achieve robustness. This bound is based on the observation that the natural accuracy of a network with non-convexity is bounded by the global upper bound of the neural network. The authors then propose a tighter Lipshitz bound for both natural and certified accuracy, which is defined as the difference between the local Lipsshitz upper bound and the global Lipsschitz bound.    The main contribution of the paper is a new method for computing a plug-in module to compute the global and certified bound of any training algorithms. The key idea is to use activation functions (e.g., ReLU, MaxMin) and weight matrices (eigenvectors of the induced norm of the weight matrix) as input to compute a neural net's global Lippingchitz bound, and then use the method to compute an upper threshold and a sparsity loss for each layer of the network.  The authors show that the proposed method converges to a Lipsz-based bound for any certifiable training algorithms when the activation functions are either ReLU or MaxMin. They also show that if the activation function is not ReLU (or MaxMin), then the network will converge to a local Lippingz bound when the network is trained with the same activation function.  Experiments on MNIST, CIFAR10, and TinyImageNet show that their method outperforms existing methods in terms of clean and certify accuracy on the MNIST and Cifar10 datasets. The method is also shown to be applicable to different network architectures, and the method can be applied to any network architectures. ","Certified robustness is an important problem in deep neural networks for safetycritical applications. In this paper, the authors propose a new bound on the Lipschitz constant of the global bound of training algorithms for a neural network to achieve robustness. This bound is based on the observation that the natural accuracy of a network with non-convexity is bounded by the global upper bound of the neural network. The authors then propose a tighter Lipshitz bound for both natural and certified accuracy, which is defined as the difference between the local Lipsshitz upper bound and the global Lipsschitz bound.    The main contribution of the paper is a new method for computing a plug-in module to compute the global and certified bound of any training algorithms. The key idea is to use activation functions (e.g., ReLU, MaxMin) and weight matrices (eigenvectors of the induced norm of the weight matrix) as input to compute a neural net's global Lippingchitz bound, and then use the method to compute an upper threshold and a sparsity loss for each layer of the network.  The authors show that the proposed method converges to a Lipsz-based bound for any certifiable training algorithms when the activation functions are either ReLU or MaxMin. They also show that if the activation function is not ReLU (or MaxMin), then the network will converge to a local Lippingz bound when the network is trained with the same activation function.  Experiments on MNIST, CIFAR10, and TinyImageNet show that their method outperforms existing methods in terms of clean and certify accuracy on the MNIST and Cifar10 datasets. The method is also shown to be applicable to different network architectures, and the method can be applied to any network architectures. "
10676,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"scalable methods USED-FOR conformal Bayesian predictive intervals. Bayesian posterior predictive distributions USED-FOR subjective beliefs. finite sample frequentist guarantees FEATURE-OF predictive confidence intervals. conformal inference USED-FOR predictive confidence intervals. conformal inference USED-FOR finite sample frequentist guarantees. add - one - in ’ importance sampling USED-FOR conformal Bayesian predictive intervals. re - weighted posterior samples of model parameters USED-FOR conformal Bayesian predictive intervals. refitting of models CONJUNCTION data - splitting. data - splitting CONJUNCTION refitting of models. approach COMPARE conformal methods. conformal methods COMPARE approach. refitting of models USED-FOR conformal methods. computational efficiency EVALUATE-FOR conformal methods. data - splitting USED-FOR conformal methods. hierarchical models HYPONYM-OF partially exchangeable settings. OtherScientificTerm are finite sample calibration guarantees, predictors, predictive intervals, and model fidelity. Method is Bayesian prediction. Metric is empirical coverage. Generic is examples. ","This paper proposes scalable methods to compute conformal Bayesian predictive intervals for Bayesian posterior predictive distributions for subjective beliefs. Conformal inference provides finite sample frequentist guarantees on the predictive confidence intervals obtained by conformal inference, and the authors propose to use ‘add-one-in’ importance sampling to obtain conformal predictive intervals based on re-weighted posterior samples of model parameters. The authors show that the proposed approach outperforms existing conformal methods based on refitting of models and data-splitting in terms of computational efficiency. They also provide finite sample calibration guarantees for the case where the predictors are partially exchangeable (e.g., hierarchical models) and the predictive intervals are non-convex (i.e., Bayesian prediction). The authors also provide empirical coverage for the proposed method, and show that their approach is able to achieve better predictive intervals than existing methods. The paper also shows that their method can be used to improve model fidelity in the case of partially exchangeability settings, and that their methods can be applied to a variety of examples.   ","This paper proposes scalable methods to compute conformal Bayesian predictive intervals for Bayesian posterior predictive distributions for subjective beliefs. Conformal inference provides finite sample frequentist guarantees on the predictive confidence intervals obtained by conformal inference, and the authors propose to use ‘add-one-in’ importance sampling to obtain conformal predictive intervals based on re-weighted posterior samples of model parameters. The authors show that the proposed approach outperforms existing conformal methods based on refitting of models and data-splitting in terms of computational efficiency. They also provide finite sample calibration guarantees for the case where the predictors are partially exchangeable (e.g., hierarchical models) and the predictive intervals are non-convex (i.e., Bayesian prediction). The authors also provide empirical coverage for the proposed method, and show that their approach is able to achieve better predictive intervals than existing methods. The paper also shows that their method can be used to improve model fidelity in the case of partially exchangeability settings, and that their methods can be applied to a variety of examples.   "
10701,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"denoisers USED-FOR general inverse problems. priors FEATURE-OF explicit likelihood functions. regularization - by - denoising ( RED ) HYPONYM-OF frameworks. RED USED-FOR imaging tasks. RED CONJUNCTION PnP. PnP CONJUNCTION RED. PnP USED-FOR imaging tasks. convolutional neural networks ( CNNs ) HYPONYM-OF denoisers. maximum a posteriori ( MAP ) CONJUNCTION minimum mean square error ( MMSE ) estimators. minimum mean square error ( MMSE ) estimators CONJUNCTION maximum a posteriori ( MAP ). convergence FEATURE-OF RED and PnP methods. CNN denoisers USED-FOR maximum a posteriori ( MAP ). Lipschitz constant FEATURE-OF CNN denoisers. denoisers PART-OF RED and PnP schemes. denoisers USED-FOR MAP and MMSE estimators interpretation. symmetric Jacobians FEATURE-OF denoisers. backtracking step size USED-FOR RED and PnP schemes. backtracking step size USED-FOR denoisers. denoisers USED-FOR inversion method. method COMPARE RED and PnP methods. RED and PnP methods COMPARE method. imaging experiments EVALUATE-FOR RED and PnP methods. imaging experiments EVALUATE-FOR method. Method are denoising algorithms, MAP or MMSE estimators, inverse algorithms, and image denoisers. Generic is they. OtherScientificTerm are potentials, and objective function. ","This paper considers the problem of denoising algorithms for inverse problems where the priors of explicit likelihood functions are known. In particular, the authors consider two frameworks: (1) regularization-by-denoising (RED) and (2) inverse algorithms (PnP). The authors show that denoisers for general inverse problems can be seen as a special case of convolutional neural networks (CNNs) and that they can be viewed as a generalization of RED and PnP for imaging tasks. The authors also show that the convergence of the RED and PI-RED and Pp methods can be interpreted as a result of the use of CNN denoiser, which they call maximum a posteriori (MAP) and minimum mean square error (MMSE) estimators.    The authors further show that MAP and MMSE estimators interpretation can be derived from the choice of denoising algorithms, and that the MAP or MMSE estimation can be approximated by the number of potentials in the denoiser.  The paper also shows that the Lipschitz constant of the denisers in RED and Pi-RED is the same as that of the corresponding CNN deniser.  Finally, the paper shows that inversion method can be formulated as an inversion of the sum of a pair of denisering algorithms, where the objective function is a convex combination of two denoisers, one of which has symmetric Jacobians, and the other of the other. The paper further shows that by using the backtracking step size in the proposed by the authors for RED and pi-RED, denoisering and inverse algorithms, the corresponding to the corresponding objective function can be invertible.  In addition, the proposed method is shown to be more robust than the original RED and PU-RED methods, and can be applied to a variety of imaging experiments, and outperforms both RED and U-RED in terms of convergence. ","This paper considers the problem of denoising algorithms for inverse problems where the priors of explicit likelihood functions are known. In particular, the authors consider two frameworks: (1) regularization-by-denoising (RED) and (2) inverse algorithms (PnP). The authors show that denoisers for general inverse problems can be seen as a special case of convolutional neural networks (CNNs) and that they can be viewed as a generalization of RED and PnP for imaging tasks. The authors also show that the convergence of the RED and PI-RED and Pp methods can be interpreted as a result of the use of CNN denoiser, which they call maximum a posteriori (MAP) and minimum mean square error (MMSE) estimators.    The authors further show that MAP and MMSE estimators interpretation can be derived from the choice of denoising algorithms, and that the MAP or MMSE estimation can be approximated by the number of potentials in the denoiser.  The paper also shows that the Lipschitz constant of the denisers in RED and Pi-RED is the same as that of the corresponding CNN deniser.  Finally, the paper shows that inversion method can be formulated as an inversion of the sum of a pair of denisering algorithms, where the objective function is a convex combination of two denoisers, one of which has symmetric Jacobians, and the other of the other. The paper further shows that by using the backtracking step size in the proposed by the authors for RED and pi-RED, denoisering and inverse algorithms, the corresponding to the corresponding objective function can be invertible.  In addition, the proposed method is shown to be more robust than the original RED and PU-RED methods, and can be applied to a variety of imaging experiments, and outperforms both RED and U-RED in terms of convergence. "
10726,SP:da92e936f88b3842ca82c2914413b129ca35890f,rhythmic features FEATURE-OF activities. rhythmic features FEATURE-OF musical soundtrack. system USED-FOR soundtrack. them USED-FOR rhythmic sounds. models USED-FOR rhythmic sounds. human movements USED-FOR RhythmicNet. skeleton keypoints USED-FOR RhythmicNet. rhythm CONJUNCTION melody. melody CONJUNCTION rhythm. natural process of music improvisation USED-FOR RhythmicNet. RhythmicNet USED-FOR rhythm. RhythmicNet USED-FOR style pattern. body keypoints USED-FOR rhythm. body keypoints USED-FOR RhythmicNet. body keypoints USED-FOR style pattern. U - net based model USED-FOR velocity. U - net based model USED-FOR it. transformerbased model USED-FOR it. inherit sound association FEATURE-OF body movements. body movements PART-OF large scale video datasets. dance HYPONYM-OF body movements. dance HYPONYM-OF inherit sound association. large scale video datasets EVALUATE-FOR RhythmicNet. Task is video. OtherScientificTerm is free body movements. Generic is method. ,"This paper proposes RhythmicNet, a system that learns to predict the rhythmic features of musical soundtrack based on human movements and activities. The system is trained on a large video dataset of people moving around in a video. The authors train two models to predict rhythmic sounds, one based on skeleton keypoints and the other based on free body movements. The method is evaluated on a number of video datasets with a focus on the natural process of music improvisation, and the system is shown to be able to reconstruct the entire soundtrack.    The main contribution of the paper is that the system learns a style pattern based on the body keypoints, and uses them to predict a set of rhythmic and melodic sounds.  The authors show that RhythicNet is able to predict both the rhythm and the melody based on a single body keypoint, and that it can be trained using a U-net based model for velocity and a transformerbased model for the melody. They also show that the style pattern can be predicted based on body movements (e.g. dance) and that the inherited sound association between body movements and the inherit sound association (i.e. music and dance). ","This paper proposes RhythmicNet, a system that learns to predict the rhythmic features of musical soundtrack based on human movements and activities. The system is trained on a large video dataset of people moving around in a video. The authors train two models to predict rhythmic sounds, one based on skeleton keypoints and the other based on free body movements. The method is evaluated on a number of video datasets with a focus on the natural process of music improvisation, and the system is shown to be able to reconstruct the entire soundtrack.    The main contribution of the paper is that the system learns a style pattern based on the body keypoints, and uses them to predict a set of rhythmic and melodic sounds.  The authors show that RhythicNet is able to predict both the rhythm and the melody based on a single body keypoint, and that it can be trained using a U-net based model for velocity and a transformerbased model for the melody. They also show that the style pattern can be predicted based on body movements (e.g. dance) and that the inherited sound association between body movements and the inherit sound association (i.e. music and dance). "
10751,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"approaches USED-FOR offline reinforcement learning ( RL ). iterative actor - critic approach USED-FOR approaches. off - policy evaluation PART-OF iterative actor - critic approach. on - policy Q estimate USED-FOR behavior policy. on - policy Q estimate USED-FOR constrained / regularized policy improvement. onestep algorithm COMPARE iterative algorithms. iterative algorithms COMPARE onestep algorithm. D4RL benchmark EVALUATE-FOR iterative algorithms. D4RL benchmark EVALUATE-FOR onestep algorithm. one - step baseline COMPARE iterative algorithms. iterative algorithms COMPARE one - step baseline. OtherScientificTerm is hyperparameters. Method are iterative approaches, and one - step algorithm. Task is repeated optimization of policies. Generic is estimates. ","This paper proposes two approaches for offline reinforcement learning (RL) based on an iterative actor-critic approach that combines the off-policy evaluation with the iterative Q-function estimation. The authors show that the on-policy Q estimate for a behavior policy can be used as a constrained/regularized policy improvement. They also show that their onestep algorithm outperforms other iterative algorithms on the D4RL benchmark.    The main contribution of this paper is that the authors propose a one-step algorithm that does not rely on hyperparameters that are commonly used in iterative approaches. This avoids the need for repeated optimization of policies, which can be prohibitively expensive in practice.  The authors also provide a theoretical analysis of their estimates and show that, in practice, their one-steep baseline outperforms the state-of-the-art iterative methods. ","This paper proposes two approaches for offline reinforcement learning (RL) based on an iterative actor-critic approach that combines the off-policy evaluation with the iterative Q-function estimation. The authors show that the on-policy Q estimate for a behavior policy can be used as a constrained/regularized policy improvement. They also show that their onestep algorithm outperforms other iterative algorithms on the D4RL benchmark.    The main contribution of this paper is that the authors propose a one-step algorithm that does not rely on hyperparameters that are commonly used in iterative approaches. This avoids the need for repeated optimization of policies, which can be prohibitively expensive in practice.  The authors also provide a theoretical analysis of their estimates and show that, in practice, their one-steep baseline outperforms the state-of-the-art iterative methods. "
10776,SP:0346eba4f587acbe3492d039066f1737360fd870,"statistics CONJUNCTION machine learning. machine learning CONJUNCTION statistics. tasks PART-OF machine learning. Low - rank and nonsmooth matrix optimization problems USED-FOR tasks. tasks PART-OF statistics. Low - rank and nonsmooth matrix optimization problems USED-FOR statistics. methods USED-FOR smooth low - rank optimization problems. convex relaxations USED-FOR problems. extragradient method USED-FOR optimal solution. maximum of smooth functions USED-FOR nonsmooth objective. initializations USED-FOR extragradient method. full - rank SVDs CONJUNCTION SVDs of rank. SVDs of rank CONJUNCTION full - rank SVDs. OtherScientificTerm are high - rank matrices, high - rank SVDs, natural generalized strict complementarity condition, low - rank SVDs, and SVDs. Task are nonsmooth problems, and nonsmooth low - rank matrix recovery tasks. Generic is method. ","Low-rank and nonsmooth matrix optimization problems are popular tasks in statistics and machine learning. The authors propose two methods for solving smooth low-rank optimization problems. The main idea is to use convex relaxations to solve these problems. In particular, the authors propose an extragradient method for finding the optimal solution to a nonsmootooth problem, which is a natural generalized generalized strict complementarity condition between high-rank matrices and high-ranks SVDs.   The authors show that the nonssooth objective is equivalent to the maximum of smooth functions, and that the authors prove that under certain initializations, the extragadient method converges to an optimal solution. They also show that this method can be used to recover the original matrix.  The main contribution of the paper is to show that nonsmoth-low-rank matrix recovery tasks are non-trivial, and the authors provide a theoretical analysis of their method. They show that under some assumptions on the number of samples, the number and the rank of the matrix is non-zero, and they show that if the number is sufficiently large enough, then the number can be polynomial in the rank.  In addition, they also provide an analysis of the generalization properties of their methods.  Finally, they provide some numerical experiments to demonstrate that their method performs well on a number of datasets. ","Low-rank and nonsmooth matrix optimization problems are popular tasks in statistics and machine learning. The authors propose two methods for solving smooth low-rank optimization problems. The main idea is to use convex relaxations to solve these problems. In particular, the authors propose an extragradient method for finding the optimal solution to a nonsmootooth problem, which is a natural generalized generalized strict complementarity condition between high-rank matrices and high-ranks SVDs.   The authors show that the nonssooth objective is equivalent to the maximum of smooth functions, and that the authors prove that under certain initializations, the extragadient method converges to an optimal solution. They also show that this method can be used to recover the original matrix.  The main contribution of the paper is to show that nonsmoth-low-rank matrix recovery tasks are non-trivial, and the authors provide a theoretical analysis of their method. They show that under some assumptions on the number of samples, the number and the rank of the matrix is non-zero, and they show that if the number is sufficiently large enough, then the number can be polynomial in the rank.  In addition, they also provide an analysis of the generalization properties of their methods.  Finally, they provide some numerical experiments to demonstrate that their method performs well on a number of datasets. "
10801,SP:d39f1d77d9919f897ccf82958b71be8798523923,graphs CONJUNCTION images. images CONJUNCTION graphs. images CONJUNCTION texts. texts CONJUNCTION images. texts HYPONYM-OF structured treatments. graphs HYPONYM-OF structured treatments. images HYPONYM-OF structured treatments. arbitrary models USED-FOR learning. generalized Robinson decomposition USED-FOR causal estimand. mild assumptions FEATURE-OF quasi - oracle convergence guarantee. approach COMPARE prior work. prior work COMPARE approach. small - world and molecular graphs EVALUATE-FOR approach. prior work USED-FOR CATE estimation. CATE estimation EVALUATE-FOR approach. OtherScientificTerm is regularization bias. ,"This paper considers the problem of learning from structured treatments (graphs, images, texts, etc.) with arbitrary models. The authors propose a novel causal estimand based on generalized Robinson decomposition, which they show provides a quasi-oracle convergence guarantee under mild assumptions. The proposed approach is shown to outperform prior work on CATE estimation on small-world and molecular graphs, and the authors also show that the regularization bias is not present in the learning. ","This paper considers the problem of learning from structured treatments (graphs, images, texts, etc.) with arbitrary models. The authors propose a novel causal estimand based on generalized Robinson decomposition, which they show provides a quasi-oracle convergence guarantee under mild assumptions. The proposed approach is shown to outperform prior work on CATE estimation on small-world and molecular graphs, and the authors also show that the regularization bias is not present in the learning. "
10826,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"qualitative assumptions CONJUNCTION distributions. distributions CONJUNCTION qualitative assumptions. system USED-FOR distributions. causal graph HYPONYM-OF qualitative assumptions. probability axioms CONJUNCTION do - calculus. do - calculus CONJUNCTION probability axioms. do - calculus CONJUNCTION c - factorization. c - factorization CONJUNCTION do - calculus. probability axioms USED-FOR graphical criteria. graphical criteria USED-FOR identification algorithms. matrix equations USED-FOR proxy variables. graphical criteria CONJUNCTION matrix equations. matrix equations CONJUNCTION graphical criteria. graphical criteria USED-FOR causal identification algorithm. matrix equations USED-FOR causal identification algorithm. graphically - driven formulae CONJUNCTION matrix multiplications. matrix multiplications CONJUNCTION graphically - driven formulae. enriched matrix - based criteria PART-OF graphical identification approach. marginal, conditional, and interventional distributions USED-FOR causal effect identification algorithm. Task is Causal effect identification. OtherScientificTerm are causal effect, proxy variable based identification conditions, and intermediary criteria. ","Causal effect identification is an important problem in the context of causal effect identification, where the goal is to identify the causal effect under a set of qualitative assumptions (e.g., causal graph, do-calculus, c-factorization) and distributions over a system. The paper proposes a graphical identification approach that incorporates enriched matrix-based criteria into the graphical criteria for identification algorithms based on probability axioms and do-scalarization. The graphical criteria and matrix equations for proxy variables are used to guide the design of a causal identification algorithm based on graphical criteria. The authors show that graphically-driven formulae and matrix multiplications can be used to improve the performance of the proposed causal ID algorithm under marginal, conditional, and interventional distributions. They also show that the proxy variable based identification conditions do not require any intermediary criteria. ","Causal effect identification is an important problem in the context of causal effect identification, where the goal is to identify the causal effect under a set of qualitative assumptions (e.g., causal graph, do-calculus, c-factorization) and distributions over a system. The paper proposes a graphical identification approach that incorporates enriched matrix-based criteria into the graphical criteria for identification algorithms based on probability axioms and do-scalarization. The graphical criteria and matrix equations for proxy variables are used to guide the design of a causal identification algorithm based on graphical criteria. The authors show that graphically-driven formulae and matrix multiplications can be used to improve the performance of the proposed causal ID algorithm under marginal, conditional, and interventional distributions. They also show that the proxy variable based identification conditions do not require any intermediary criteria. "
10851,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"Unsupervised Environment Design ( UED ) HYPONYM-OF selfsupervised RL paradigm. random levels USED-FOR PLR. Dual Curriculum Design ( DCD ) HYPONYM-OF UED methods. PLR CONJUNCTION UED algorithm. UED algorithm CONJUNCTION PLR. UED algorithm CONJUNCTION PAIRED. PAIRED CONJUNCTION UED algorithm. PLR CONJUNCTION PAIRED. PAIRED CONJUNCTION PLR. PAIRED PART-OF DCD. PLR PART-OF DCD. UED algorithm PART-OF DCD. theory USED-FOR PLR. robustness guarantee FEATURE-OF Nash equilibria. theory USED-FOR PLR. Nash equilibria FEATURE-OF convergence. PLR⊥ USED-FOR PAIRED. PLR⊥ HYPONYM-OF method. Method are Deep reinforcement learning ( RL ) agents, Prioritized Level Replay ( PLR ), UED, and theoretical framework. OtherScientificTerm are environment and task configurations, diverse training environments, randomly - generated training levels, and theoretical guarantees. Generic is it. ","This paper considers the problem of unsupervised learning of RL agents in a multi-agent setting, where the agent is trained in a self-supervised way, and the goal is to learn a policy that is robust to a variety of environment and task configurations. The authors propose a new method, Prioritized Level Replay (PLR), which is a variant of the Unsupervised Environment Design (UED) paradigm, which is an extension of the selfsupervised RL paradigm.    The authors show that PLR can be applied to a wide range of environments and tasks, and that it converges to Nash equilibria in the presence of diverse training environments and diverse training levels. They also provide theoretical guarantees on the robustness of PLR to randomly-generated training levels, and show that it can converge to a Nash equilibrium at random levels. Previous UED methods, such as Dual Curriculum Design (DCCD) and PAIRED, have been shown to converge to the Nash equilibrium in the absence of diverse environments.  The main contribution of this paper is to extend DCD to PLR and UED by introducing a theoretical framework that provides a robustness guarantee on the convergence of the two methods.  In addition, the authors propose to combine PLR with the UED algorithm in DCD, which consists of two steps: 1) PLR, 2) PAIRED. The theoretical framework is applied to the case of DCD and PLR. The main result of the paper is that the PLR� is a generalization of the PAIRED algorithm. The paper also provides theoretical guarantees for the case where the target environment is diverse and the target task is not. ","This paper considers the problem of unsupervised learning of RL agents in a multi-agent setting, where the agent is trained in a self-supervised way, and the goal is to learn a policy that is robust to a variety of environment and task configurations. The authors propose a new method, Prioritized Level Replay (PLR), which is a variant of the Unsupervised Environment Design (UED) paradigm, which is an extension of the selfsupervised RL paradigm.    The authors show that PLR can be applied to a wide range of environments and tasks, and that it converges to Nash equilibria in the presence of diverse training environments and diverse training levels. They also provide theoretical guarantees on the robustness of PLR to randomly-generated training levels, and show that it can converge to a Nash equilibrium at random levels. Previous UED methods, such as Dual Curriculum Design (DCCD) and PAIRED, have been shown to converge to the Nash equilibrium in the absence of diverse environments.  The main contribution of this paper is to extend DCD to PLR and UED by introducing a theoretical framework that provides a robustness guarantee on the convergence of the two methods.  In addition, the authors propose to combine PLR with the UED algorithm in DCD, which consists of two steps: 1) PLR, 2) PAIRED. The theoretical framework is applied to the case of DCD and PLR. The main result of the paper is that the PLR� is a generalization of the PAIRED algorithm. The paper also provides theoretical guarantees for the case where the target environment is diverse and the target task is not. "
10876,SP:9ed528da4b67f22678303cfd975aafe678db6411,"( ε, δ)-differentially private algorithm USED-FOR multi - armed bandit ( MAB ) problem. shuffle model FEATURE-OF multi - armed bandit ( MAB ) problem. upper bound COMPARE regret. regret COMPARE upper bound. Metric are distribution - dependent regret, and distribution - independent regret. OtherScientificTerm is suboptimality gap. Method are centralized model, and local model. ","This paper studies the multi-armed bandit (MAB) problem under the shuffle model. The authors propose a ( ε, δ)-differentially private algorithm for the (approximate) (distribution-dependent) suboptimality gap. They prove an upper bound on the distribution-dependent regret, which is a lower bound of the regret of the centralized model. They also prove a distribution-independent regret that matches the upper bound. ","This paper studies the multi-armed bandit (MAB) problem under the shuffle model. The authors propose a ( ε, δ)-differentially private algorithm for the (approximate) (distribution-dependent) suboptimality gap. They prove an upper bound on the distribution-dependent regret, which is a lower bound of the regret of the centralized model. They also prove a distribution-independent regret that matches the upper bound. "
10901,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"probabilistic forecasts USED-FOR decision rules. threshold calibration HYPONYM-OF calibration. algorithm USED-FOR threshold - calibrated forecaster. uncalibrated forecaster USED-FOR algorithm. threshold loss function USED-FOR threshold decision. hospital scheduling decisions CONJUNCTION resource allocation decisions. resource allocation decisions CONJUNCTION hospital scheduling decisions. threshold calibration USED-FOR decision loss prediction. real - world settings EVALUATE-FOR threshold calibration. resource allocation decisions HYPONYM-OF real - world settings. hospital scheduling decisions HYPONYM-OF real - world settings. OtherScientificTerm are forecasted probabilities, predicted losses, cutoff, decision loss, and threshold decisions. Task are regression setting, and loss of threshold decisions. Generic is procedure. ","This paper studies the problem of learning decision rules from probabilistic forecasts. In the regression setting, the goal is to learn a set of decision rules that are robust to changes in the forecasted probabilities. The authors propose a procedure called ""threshold calibration"", which is a form of calibration based on the notion of ""calibration"". In particular, the authors propose an algorithm that learns a threshold-calibrated forecaster that is robust to the change in the predicted losses, and then uses this algorithm to train an algorithm to calibrate the loss of threshold decisions. The idea is that the threshold loss function for a given threshold decision is a function of the number of times that the cutoff for the decision loss is changed, and that the algorithm learns an algorithm for calibrating the loss for a threshold decision that is sensitive to this change.  The authors evaluate the performance of threshold calibration in two real-world settings: hospital scheduling decisions and resource allocation decisions, and show that threshold calibration improves decision loss prediction in both cases.  ","This paper studies the problem of learning decision rules from probabilistic forecasts. In the regression setting, the goal is to learn a set of decision rules that are robust to changes in the forecasted probabilities. The authors propose a procedure called ""threshold calibration"", which is a form of calibration based on the notion of ""calibration"". In particular, the authors propose an algorithm that learns a threshold-calibrated forecaster that is robust to the change in the predicted losses, and then uses this algorithm to train an algorithm to calibrate the loss of threshold decisions. The idea is that the threshold loss function for a given threshold decision is a function of the number of times that the cutoff for the decision loss is changed, and that the algorithm learns an algorithm for calibrating the loss for a threshold decision that is sensitive to this change.  The authors evaluate the performance of threshold calibration in two real-world settings: hospital scheduling decisions and resource allocation decisions, and show that threshold calibration improves decision loss prediction in both cases.  "
10926,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,method USED-FOR centroid approximation. centroid approximation USED-FOR random function. argmax distribution HYPONYM-OF random function. method USED-FOR argmax distribution. centroid points USED-FOR argmax distribution. centroid points USED-FOR method. objective function USED-FOR method. objective function USED-FOR argmax distribution. personalized dialogue systems CONJUNCTION multi - target domain adaptation. multi - target domain adaptation CONJUNCTION personalized dialogue systems. few - shot image classification CONJUNCTION personalized dialogue systems. personalized dialogue systems CONJUNCTION few - shot image classification. real - world multitask learning applications EVALUATE-FOR method. few - shot image classification HYPONYM-OF real - world multitask learning applications. multi - target domain adaptation HYPONYM-OF real - world multitask learning applications. personalized dialogue systems HYPONYM-OF real - world multitask learning applications. Task is machine learning. Method is argmax centroid method. OtherScientificTerm is Wasserstein distance. ,"This paper proposes a new method for centroid approximation to the random function, the argmax distribution, in machine learning. The proposed method uses centroid points as the objective function to approximate the arg max distribution of a random function. The method is evaluated on three real-world multitask learning applications: few-shot image classification, personalized dialogue systems, and multi-target domain adaptation. The argmax centroid method is based on the Wasserstein distance between the target distribution and the centroid distribution. ","This paper proposes a new method for centroid approximation to the random function, the argmax distribution, in machine learning. The proposed method uses centroid points as the objective function to approximate the arg max distribution of a random function. The method is evaluated on three real-world multitask learning applications: few-shot image classification, personalized dialogue systems, and multi-target domain adaptation. The argmax centroid method is based on the Wasserstein distance between the target distribution and the centroid distribution. "
10951,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"adversarial manner USED-FOR preferences. preference vector USED-FOR reward function. pre - specified multi - objective reward functions FEATURE-OF preference vector. episodic learning problem USED-FOR problem. Markov decision process USED-FOR episodic learning problem. nearly minimax optimal regret bound EVALUATE-FOR model - based algorithm. nearly optimal trajectory complexity EVALUATE-FOR algorithm. Task are multi - objective reinforcement learning, and online setting. OtherScientificTerm are transitions, ( adversarial ) preference, policies, and preference - free exploration. ","This paper considers the problem of multi-objective reinforcement learning, where the preferences are learned in an adversarial manner in an online setting. In this setting, the transitions are assumed to be non-convex and the goal is to learn a (adversarial) preference that maximizes the expected return of the policy. The authors consider the online setting where the preference vector of the reward function is learned in a pre-specified multi-optimal reward functions. The problem is formulated as an episodic learning problem with a Markov decision process, and the authors propose an algorithm with nearly minimax optimal regret bound. They also provide a model-based algorithm that achieves a nearly optimal trajectory complexity, and show that their algorithm can be used to learn policies that are robust to adversarial attacks. Finally, the authors show that preference-free exploration is also possible. ","This paper considers the problem of multi-objective reinforcement learning, where the preferences are learned in an adversarial manner in an online setting. In this setting, the transitions are assumed to be non-convex and the goal is to learn a (adversarial) preference that maximizes the expected return of the policy. The authors consider the online setting where the preference vector of the reward function is learned in a pre-specified multi-optimal reward functions. The problem is formulated as an episodic learning problem with a Markov decision process, and the authors propose an algorithm with nearly minimax optimal regret bound. They also provide a model-based algorithm that achieves a nearly optimal trajectory complexity, and show that their algorithm can be used to learn policies that are robust to adversarial attacks. Finally, the authors show that preference-free exploration is also possible. "
10976,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"classification models COMPARE explanation of sequence generation models. explanation of sequence generation models COMPARE classification models. model - agnostic explanations USED-FOR text generation task. dialogue response generation HYPONYM-OF text generation task. open - ended sentences USED-FOR Dialog response generation. LERG USED-FOR sequence prediction. LERG USED-FOR explanations. unbiased approximation CONJUNCTION consistency. consistency CONJUNCTION unbiased approximation. consistency CONJUNCTION cause identification. cause identification CONJUNCTION consistency. explanation USED-FOR text generation. LERG USED-FOR text generation. explanation USED-FOR LERG. consistency HYPONYM-OF explanation. automaticand humanevaluation metrics EVALUATE-FOR task. method COMPARE methods. methods COMPARE method. task EVALUATE-FOR methods. task EVALUATE-FOR method. automaticand humanevaluation metrics EVALUATE-FOR methods. automaticand humanevaluation metrics EVALUATE-FOR method. LERG USED-FOR explicit and implicit relations. Method are generation model, and local explanation of response generation ( LERG ). OtherScientificTerm is human response. ","This paper proposes a local explanation of response generation (LERG) method for dialogue response generation. Dialog response generation is a text generation task with open-ended sentences, where the goal is to generate a sequence of sentences that describe a human response to a question. The paper argues that existing model-agnostic explanations are not always sufficient to explain the performance of classification models, and proposes LERG, which is an explanation of explanation of sequence generation models. LERG uses the explanation of a generation model to explain a sequence prediction, and then uses this explanation to guide the text generation. The authors show that LERG is able to explain both explicit and implicit relations between the explanation and the human response, and that the explanation can be unbiased approximation, consistency, and cause identification. The method is evaluated on this task using automaticand humanevaluation metrics, and is shown to outperform existing methods on the task.   ","This paper proposes a local explanation of response generation (LERG) method for dialogue response generation. Dialog response generation is a text generation task with open-ended sentences, where the goal is to generate a sequence of sentences that describe a human response to a question. The paper argues that existing model-agnostic explanations are not always sufficient to explain the performance of classification models, and proposes LERG, which is an explanation of explanation of sequence generation models. LERG uses the explanation of a generation model to explain a sequence prediction, and then uses this explanation to guide the text generation. The authors show that LERG is able to explain both explicit and implicit relations between the explanation and the human response, and that the explanation can be unbiased approximation, consistency, and cause identification. The method is evaluated on this task using automaticand humanevaluation metrics, and is shown to outperform existing methods on the task.   "
11001,SP:965413b1726617006317bbbec55673dd5d21812a,"distributed methods USED-FOR compressor. contraction property FEATURE-OF compressor. RandK HYPONYM-OF biased compressors. error compensation CONJUNCTION error feedback. error feedback CONJUNCTION error compensation. gradient compression CONJUNCTION acceleration. acceleration CONJUNCTION gradient compression. error compensation CONJUNCTION acceleration. acceleration CONJUNCTION error compensation. error compensation USED-FOR gradient compression. method COMPARE error compensated algorithms. error compensated algorithms COMPARE method. communication rounds EVALUATE-FOR method. Method are Gradient compression, error compensated gradient compression methods, and error compensated loopless Katyusha method. Metric are communication cost, and accelerated linear convergence rate. OtherScientificTerm is divergence. ","Gradient compression is a popular technique to reduce the communication cost between the server and the client. However, existing distributed methods for training a compressor are computationally expensive due to the need to compute the divergence between the compression property of the compressor and the error compensation. In this paper, the authors show that biased compressors, such as RandK, can be used to reduce this communication cost. The authors also show that error compensated gradient compression methods can be more efficient than standard error compensation and error feedback.  The authors propose a new error compensated loopless Katyusha method, where the divergence is computed based on the convergence of the gradient compression, error compensation, and acceleration. They show that the proposed method has an accelerated linear convergence rate and can reduce the number of communication rounds by a factor of $\sqrt{\log n}$. They also show empirically that their method outperforms existing error compensated algorithms. ","Gradient compression is a popular technique to reduce the communication cost between the server and the client. However, existing distributed methods for training a compressor are computationally expensive due to the need to compute the divergence between the compression property of the compressor and the error compensation. In this paper, the authors show that biased compressors, such as RandK, can be used to reduce this communication cost. The authors also show that error compensated gradient compression methods can be more efficient than standard error compensation and error feedback.  The authors propose a new error compensated loopless Katyusha method, where the divergence is computed based on the convergence of the gradient compression, error compensation, and acceleration. They show that the proposed method has an accelerated linear convergence rate and can reduce the number of communication rounds by a factor of $\sqrt{\log n}$. They also show empirically that their method outperforms existing error compensated algorithms. "
11026,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"machine learning framework USED-FOR edge and neuromorphic computing paradigms. training complexity CONJUNCTION biological plausibility. biological plausibility CONJUNCTION training complexity. machine learning framework USED-FOR it. biological plausibility FEATURE-OF liquid state machine ( LSM ). training complexity EVALUATE-FOR liquid state machine ( LSM ). LSM USED-FOR internal weights. LSM COMPARE multi - layer neural networks. multi - layer neural networks COMPARE LSM. LSM USED-FOR model of brain computation. synaptic plasticity CONJUNCTION brain dynamics. brain dynamics CONJUNCTION synaptic plasticity. astrocytes USED-FOR synaptic plasticity. astrocytes USED-FOR brain dynamics. neuron - astrocyte liquid state machine ( NALSM)1 USED-FOR under - performance. self - organized near - critical dynamics USED-FOR neuron - astrocyte liquid state machine ( NALSM)1. self - organized near - critical dynamics USED-FOR under - performance. astrocyte model USED-FOR global feedback. NALSM COMPARE LSM methods. LSM methods COMPARE NALSM. accuracy EVALUATE-FOR LSM methods. accuracy EVALUATE-FOR NALSM. MNIST CONJUNCTION N - MNIST. N - MNIST CONJUNCTION MNIST. accuracy EVALUATE-FOR NALSM. N - MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION N - MNIST. braininspired machine learning methods USED-FOR deep learning. deep learning USED-FOR neuromorphic computing. Task is brain computation. Method are backpropagation of gradients, and backpropagation. OtherScientificTerm are brain networks, computationally optimal critical phase transition, neuronal activity, NALSM dynamics, branching factor, edge - of - chaos, and data - specific hand - tuning. ","This paper proposes a new machine learning framework for both edge and neuromorphic computing paradigms that combines the advantages of the training complexity and biological plausibility of the liquid state machine (LSM) in terms of training complexity. The authors show that the internal weights of a LSM can be decomposed into two parts: (1) the backpropagation of gradients, and (2) the computationally optimal critical phase transition between the two parts of the LSM, which is similar to multi-layer neural networks.  The authors also show that LSM is a model of brain computation, and that it can be applied to a number of existing machine learning frameworks.  In particular, the authors use astrocytes to model the synaptic plasticity and brain dynamics of neurons, and show that a neuron-astrocyte liquid-state machine (NALSM)1 with self-organized near-critical dynamics is able to avoid under-performance due to the lack of neuronal activity.  NALSM dynamics can be seen as a function of the number of neurons and the branching factor of the neurons.   The paper also shows that an astrocyclic model can be used to provide global feedback to a neuron.  Experiments are conducted on MNIST, N-MNIST, and N-NLI datasets, and the authors show the superiority of the NalSM dynamics over other LSM methods.  Finally, experiments are performed on deep learning for deep learning using deep learning on neuromorphic processing, and on deep neural networks using braininspired machine learning methods. The results show that NNALM outperforms LSM in accuracy and accuracy, but the NALM is more robust to edge-of-chaos (e.g. edge of chaos) and less sensitive to data-specific hand-tuning.","This paper proposes a new machine learning framework for both edge and neuromorphic computing paradigms that combines the advantages of the training complexity and biological plausibility of the liquid state machine (LSM) in terms of training complexity. The authors show that the internal weights of a LSM can be decomposed into two parts: (1) the backpropagation of gradients, and (2) the computationally optimal critical phase transition between the two parts of the LSM, which is similar to multi-layer neural networks.  The authors also show that LSM is a model of brain computation, and that it can be applied to a number of existing machine learning frameworks.  In particular, the authors use astrocytes to model the synaptic plasticity and brain dynamics of neurons, and show that a neuron-astrocyte liquid-state machine (NALSM)1 with self-organized near-critical dynamics is able to avoid under-performance due to the lack of neuronal activity.  NALSM dynamics can be seen as a function of the number of neurons and the branching factor of the neurons.   The paper also shows that an astrocyclic model can be used to provide global feedback to a neuron.  Experiments are conducted on MNIST, N-MNIST, and N-NLI datasets, and the authors show the superiority of the NalSM dynamics over other LSM methods.  Finally, experiments are performed on deep learning for deep learning using deep learning on neuromorphic processing, and on deep neural networks using braininspired machine learning methods. The results show that NNALM outperforms LSM in accuracy and accuracy, but the NALM is more robust to edge-of-chaos (e.g. edge of chaos) and less sensitive to data-specific hand-tuning."
11051,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"class imbalance problem HYPONYM-OF learning node representations. asymmetric topological properties FEATURE-OF labeled nodes. graph data USED-FOR imbalance. Label Propagation algorithm USED-FOR node influence shift phenomenon. model - agnostic method ReNode USED-FOR topology - imbalance issue. influence conflict detection – based metric Totoro USED-FOR graph topology imbalance. method USED-FOR topology - imbalance issue. method USED-FOR semi - supervised node classification. topology - imbalance issue CONJUNCTION semi - supervised node classification. semi - supervised node classification CONJUNCTION topology - imbalance issue. graph neural networks ( GNNs ) USED-FOR topology imbalance. OtherScientificTerm are quantity imbalance, graph ( topology imbalance ), and class boundaries. Task are unknown topology - imbalance issue, semisupervised node classification learning, and quantityand topologyimbalance issues. ","This paper studies the class imbalance problem in learning node representations, i.e., the problem of class imbalance in the presence of quantity imbalance. The imbalance is caused by the imbalance of the graph data between two classes of nodes in the same graph (topology imbalance). The authors propose a new model-agnostic method ReNode to address the topology-imbalance issue. They propose a label Propagation algorithm to mitigate the node influence shift phenomenon, where the labeled nodes have asymmetric topological properties. The authors also propose an influence conflict detection –based metric Totoro to detect the graph topology imbalance. They show that the proposed method can be applied to both the topologies imbalance issue in graph neural networks (GNNs) and the semi-supervised node classification problem. They also show that their method is applicable to the unknown topology - imbalance issue, which is a common problem in the context of semisupervised graph classification learning.    The paper is well-written, well-motivated, and well-structured. The idea of the authors is interesting. However, there are a few issues in the paper. First, there is a lack of discussion of the relation between quantity imbalance and the graph (Topology imbalance) imbalance. Second, the authors do not discuss the relationship between the quantityand topologyimbalance issues. Third, the class boundaries are not clearly defined. ","This paper studies the class imbalance problem in learning node representations, i.e., the problem of class imbalance in the presence of quantity imbalance. The imbalance is caused by the imbalance of the graph data between two classes of nodes in the same graph (topology imbalance). The authors propose a new model-agnostic method ReNode to address the topology-imbalance issue. They propose a label Propagation algorithm to mitigate the node influence shift phenomenon, where the labeled nodes have asymmetric topological properties. The authors also propose an influence conflict detection –based metric Totoro to detect the graph topology imbalance. They show that the proposed method can be applied to both the topologies imbalance issue in graph neural networks (GNNs) and the semi-supervised node classification problem. They also show that their method is applicable to the unknown topology - imbalance issue, which is a common problem in the context of semisupervised graph classification learning.    The paper is well-written, well-motivated, and well-structured. The idea of the authors is interesting. However, there are a few issues in the paper. First, there is a lack of discussion of the relation between quantity imbalance and the graph (Topology imbalance) imbalance. Second, the authors do not discuss the relationship between the quantityand topologyimbalance issues. Third, the class boundaries are not clearly defined. "
11076,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"d - dimensional lattice FEATURE-OF additive Gaussian noise. additive Gaussian noise USED-FOR piece - wise constant signals. partition recovery USED-FOR partition of the lattice. DCART - based procedure USED-FOR partition. regularity conditions USED-FOR DCART - based procedure. recursive dyadic partitions USED-FOR signal partition. recursive dyadic partitions USED-FOR rectangular sub - graphs. NP - hard exhaustive search method USED-FOR one. optimal regression tree estimator ( ORT ) USED-FOR partition estimator. DCART USED-FOR partition recovery. Task are signal detection or testing, and estimation. OtherScientificTerm are constancy regions of the unknown signal, and noise variance. Generic is method. ","This paper considers the problem of signal detection or testing for piece-wise constant signals with additive Gaussian noise on a d-dimensional lattice. The authors propose a DCART-based procedure to recover the partition of the lattice under certain regularity conditions. The main idea is to use recursive dyadic partitions to partition rectangular sub-graphs into rectangular subgraphs, and then use partition recovery to recover a signal partition from each of the subgraph. The proposed partition estimator is based on the optimal regression tree estimator (ORT). The authors also propose an NP-hard exhaustive search method to search for the best one, which is more computationally efficient than the one based on DCART.    The authors provide a theoretical analysis of their method, which shows that the constancy regions of the unknown signal can be recovered from the noise variance. They also show that DCART can be used to perform partition recovery in the case that the noise is low-variance, and that the estimation can be done efficiently. ","This paper considers the problem of signal detection or testing for piece-wise constant signals with additive Gaussian noise on a d-dimensional lattice. The authors propose a DCART-based procedure to recover the partition of the lattice under certain regularity conditions. The main idea is to use recursive dyadic partitions to partition rectangular sub-graphs into rectangular subgraphs, and then use partition recovery to recover a signal partition from each of the subgraph. The proposed partition estimator is based on the optimal regression tree estimator (ORT). The authors also propose an NP-hard exhaustive search method to search for the best one, which is more computationally efficient than the one based on DCART.    The authors provide a theoretical analysis of their method, which shows that the constancy regions of the unknown signal can be recovered from the noise variance. They also show that DCART can be used to perform partition recovery in the case that the noise is low-variance, and that the estimation can be done efficiently. "
11101,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"tasks EVALUATE-FOR deep learning models. causality - based training framework USED-FOR spurious correlations. interventional distribution COMPARE observational distribution. observational distribution COMPARE interventional distribution. Maximum Likelihood Estimation ( MLE ) USED-FOR interventional distribution. algorithms USED-FOR causal predictions. Implicit CMLE CONJUNCTION Explicit CMLE. Explicit CMLE CONJUNCTION Implicit CMLE. Implicit CMLE USED-FOR causal predictions. algorithms USED-FOR deep learning models. Explicit CMLE USED-FOR causal predictions. deep learning models USED-FOR causal predictions. Implicit CMLE HYPONYM-OF algorithms. Explicit CMLE HYPONYM-OF algorithms. observational data USED-FOR algorithms. observational data USED-FOR deep learning models. observational data USED-FOR causal predictions. observational data USED-FOR interventional distribution. Natural Language Inference ( NLI ) CONJUNCTION Image Captioning. Image Captioning CONJUNCTION Natural Language Inference ( NLI ). simulated data CONJUNCTION real - world tasks. real - world tasks CONJUNCTION simulated data. Natural Language Inference ( NLI ) HYPONYM-OF real - world tasks. Image Captioning HYPONYM-OF real - world tasks. CMLE methods COMPARE regular MLE method. regular MLE method COMPARE CMLE methods. CMLE methods USED-FOR spurious correlations. out - of - domain generalization EVALUATE-FOR regular MLE method. out - of - domain generalization EVALUATE-FOR CMLE methods. Generic is they. OtherScientificTerm are predictive clues, observed confounders, and expected negative log - likelihood. Method are general Structural Causal Model ( SCM ), and Counterfactual Maximum Likelihood Estimation ( CMLE ). ","This paper proposes a causal-based training framework to tackle the problem of spurious correlations in tasks where the interventional distribution is different from the observational distribution using Maximum Likelihood Estimation (MLE). The authors propose two algorithms, Implicit CMLE and Explicit CMLE, to learn causal predictions from observational data for deep learning models. The authors show that they are able to learn a general Structural Causal Model (SCM) that is more robust to spurious correlations than existing methods. They also show that their algorithms can be used to learn two types of causal predictions using observational data: (1) causal predictions based on observational data, and (2) predictive clues based on observed confounders.  Experiments are conducted on simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. They show that the proposed CMLE methods outperform the regular MLE method in terms of out-of-domain generalization, and that the expected negative log-likelihood is lower than that of Counterfactual Maximum Unlikelihood Estation (CMLE). ","This paper proposes a causal-based training framework to tackle the problem of spurious correlations in tasks where the interventional distribution is different from the observational distribution using Maximum Likelihood Estimation (MLE). The authors propose two algorithms, Implicit CMLE and Explicit CMLE, to learn causal predictions from observational data for deep learning models. The authors show that they are able to learn a general Structural Causal Model (SCM) that is more robust to spurious correlations than existing methods. They also show that their algorithms can be used to learn two types of causal predictions using observational data: (1) causal predictions based on observational data, and (2) predictive clues based on observed confounders.  Experiments are conducted on simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. They show that the proposed CMLE methods outperform the regular MLE method in terms of out-of-domain generalization, and that the expected negative log-likelihood is lower than that of Counterfactual Maximum Unlikelihood Estation (CMLE). "
11126,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"multi - task learning USED-FOR learning. multi - task learning COMPARE single task learning. single task learning COMPARE multi - task learning. learning COMPARE single task learning. single task learning COMPARE learning. multi - task learning objective USED-FOR average loss. gradients FEATURE-OF task objectives. heuristics USED-FOR problem. heuristics USED-FOR task gradients. Conflict - Averse Gradient descent ( CAGrad ) USED-FOR average loss function. regular gradient descent ( GD ) CONJUNCTION multiple gradient descent algorithm ( MGDA ). multiple gradient descent algorithm ( MGDA ) CONJUNCTION regular gradient descent ( GD ). multiple gradient descent algorithm ( MGDA ) PART-OF multi - objective optimization ( MOO ) literature. multiple gradient descent algorithm ( MGDA ) PART-OF It. regular gradient descent ( GD ) PART-OF It. CAGrad COMPARE multi - objective gradient manipulation methods. multi - objective gradient manipulation methods COMPARE CAGrad. OtherScientificTerm are model structures, conflicting gradients, average gradient direction, convergence guarantee, Pareto - stationary point, and worst local improvement. Generic is objective. Method is multi - task model. ","This paper studies the problem of multi-task learning, where the goal is to improve the performance of learning in multi-tasks in the presence of conflicting gradients. In particular, the authors consider the setting where there are multiple tasks with different model structures, and the objective is to minimize the average loss of all tasks. In this setting, they show that the average of the gradients of all task objectives can be computed in a Pareto-stationary direction, which is the average gradient direction in the direction of the worst local improvement. They also show a convergence guarantee for this objective.    The authors propose a new method called Conflict-Averse Gradient descent (CAGrad) to approximate the average function of the multi-target learning objective. It is a combination of regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) from the multi -objective optimization (MOO) literature. CAGrad uses heuristics to manipulate the task gradients in order to solve the problem. The authors also show that their method can be used to learn a single-task model that is more robust to the conflicting gradient of each task.  The paper also shows that the convergence guarantee is tighter than that of GD and MGDA. Finally, the paper shows that, when the number of tasks is large enough, C AGrad converges faster than other multi-objective gradient manipulation methods, and that it is more stable than GD. ","This paper studies the problem of multi-task learning, where the goal is to improve the performance of learning in multi-tasks in the presence of conflicting gradients. In particular, the authors consider the setting where there are multiple tasks with different model structures, and the objective is to minimize the average loss of all tasks. In this setting, they show that the average of the gradients of all task objectives can be computed in a Pareto-stationary direction, which is the average gradient direction in the direction of the worst local improvement. They also show a convergence guarantee for this objective.    The authors propose a new method called Conflict-Averse Gradient descent (CAGrad) to approximate the average function of the multi-target learning objective. It is a combination of regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) from the multi -objective optimization (MOO) literature. CAGrad uses heuristics to manipulate the task gradients in order to solve the problem. The authors also show that their method can be used to learn a single-task model that is more robust to the conflicting gradient of each task.  The paper also shows that the convergence guarantee is tighter than that of GD and MGDA. Finally, the paper shows that, when the number of tasks is large enough, C AGrad converges faster than other multi-objective gradient manipulation methods, and that it is more stable than GD. "
11151,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"Large language models USED-FOR few - shot learning. language models USED-FOR simple algorithmic concepts. strong priors FEATURE-OF teaching problem. program induction systems CONJUNCTION humans. humans CONJUNCTION program induction systems. GPT architectures CONJUNCTION program induction systems. program induction systems CONJUNCTION GPT architectures. complexity EVALUATE-FOR concept. GPT architectures CONJUNCTION humans. humans CONJUNCTION GPT architectures. artificial intelligence CONJUNCTION machine learning. machine learning CONJUNCTION artificial intelligence. language models CONJUNCTION machine teaching. machine teaching CONJUNCTION language models. machine teaching USED-FOR artificial intelligence. OtherScientificTerm are patterns of algorithmic nature, and Occam ’s razor. Generic is models. Task is learning. ","This paper studies the problem of few-shot learning with large language models. The authors show that language models are able to learn simple algorithmic concepts, which are patterns of algorithmic nature. They show that the learning problem with strong priors is similar to the teaching problem in the context of Occam’s razor, and that the complexity of this concept is a function of the number of training examples and the size of the models. They also show that GPT architectures, program induction systems, and humans can be trained with this concept.   The paper is well-written and well-motivated. The connection between language models and machine teaching in artificial intelligence and machine learning is clear. ","This paper studies the problem of few-shot learning with large language models. The authors show that language models are able to learn simple algorithmic concepts, which are patterns of algorithmic nature. They show that the learning problem with strong priors is similar to the teaching problem in the context of Occam’s razor, and that the complexity of this concept is a function of the number of training examples and the size of the models. They also show that GPT architectures, program induction systems, and humans can be trained with this concept.   The paper is well-written and well-motivated. The connection between language models and machine teaching in artificial intelligence and machine learning is clear. "
11176,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"perturbation USED-FOR Adversarial examples. feature representation USED-FOR robust and non - robust features. Information Bottleneck USED-FOR feature representation. Information Bottleneck USED-FOR way. noise variation USED-FOR feature unit. information flow PART-OF feature representation. noise variation magnitude USED-FOR information flow. human - perceptible semantic information FEATURE-OF they. attack mechanism USED-FOR gradient of non - robust features. OtherScientificTerm are adversarial examples, feature space, and distilled features. Task are adversarial prediction, model prediction, and model robustness. ","Adversarial examples are generated by perturbation to the input feature space. The paper proposes a new way to attack the Information Bottleneck in the feature representation to distinguish between robust and non-robust features. The proposed way is based on the idea that adversarial examples can be generated by distilling the input features from the original feature space to a single feature unit. The idea is that the noise variation of the feature unit is related to the information flow of the original and distilled feature unit, and that the adversarial prediction can be made from the distilled features. This information flow in a feature representation is modeled as a function of noise variation magnitude, and the authors show that they can attack the model prediction of a model robustly. The authors also propose an attack mechanism to accelerate the gradient of non-robotically robust features as they are more likely to contain human-perceptible semantic information.","Adversarial examples are generated by perturbation to the input feature space. The paper proposes a new way to attack the Information Bottleneck in the feature representation to distinguish between robust and non-robust features. The proposed way is based on the idea that adversarial examples can be generated by distilling the input features from the original feature space to a single feature unit. The idea is that the noise variation of the feature unit is related to the information flow of the original and distilled feature unit, and that the adversarial prediction can be made from the distilled features. This information flow in a feature representation is modeled as a function of noise variation magnitude, and the authors show that they can attack the model prediction of a model robustly. The authors also propose an attack mechanism to accelerate the gradient of non-robotically robust features as they are more likely to contain human-perceptible semantic information."
11201,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"support vector machine ( SVM ) CONJUNCTION minimum Euclidean norm least squares regression. minimum Euclidean norm least squares regression CONJUNCTION support vector machine ( SVM ). models USED-FOR high - dimensional data. support vector machine ( SVM ) HYPONYM-OF approaches. approaches USED-FOR linear models. minimum Euclidean norm least squares regression HYPONYM-OF approaches. they PART-OF models. support vector proliferation USED-FOR independent feature models. super - linear lower bound USED-FOR support vector proliferation. dimension USED-FOR support vector proliferation. super - linear lower bound FEATURE-OF dimension. sharp phase transition FEATURE-OF Gaussian feature models. geometric characterization of the problem USED-FOR lp case. l1 variant PART-OF SVM. OtherScientificTerm are support vector, upper bounds, and phase transition. Generic is transition. ","This paper proposes two approaches to train linear models, support vector machine (SVM) and minimum Euclidean norm least squares regression, which are models that are well suited for high-dimensional data. The authors show that the support vector proliferation of independent feature models is a result of the fact that they share the same dimension across different models. They also provide a super-linear lower bound on the number of support vectors, which shows that the dimension is a function of the support vectors. The paper also shows that Gaussian feature models have a sharp phase transition, which is the result of a sharp transition between the support of a support vector and a support of the true support vector.   The authors also provide upper bounds on the phase transition.  Finally, the authors provide a geometric characterization of the problem in the lp case, showing that the transition depends on the dimension of the data, and that the l1 variant of the SVM is a special case of this. ","This paper proposes two approaches to train linear models, support vector machine (SVM) and minimum Euclidean norm least squares regression, which are models that are well suited for high-dimensional data. The authors show that the support vector proliferation of independent feature models is a result of the fact that they share the same dimension across different models. They also provide a super-linear lower bound on the number of support vectors, which shows that the dimension is a function of the support vectors. The paper also shows that Gaussian feature models have a sharp phase transition, which is the result of a sharp transition between the support of a support vector and a support of the true support vector.   The authors also provide upper bounds on the phase transition.  Finally, the authors provide a geometric characterization of the problem in the lp case, showing that the transition depends on the dimension of the data, and that the l1 variant of the SVM is a special case of this. "
11226,SP:99f226a63902863c429cb7baefab09626d13921e,"Markov Decision Processes USED-FOR active pure exploration problem. instance - specific sample complexity EVALUATE-FOR algorithm. algorithm USED-FOR communicating MDPs. reduced exploration rate CONJUNCTION faster convergence. faster convergence CONJUNCTION reduced exploration rate. reduced exploration rate FEATURE-OF variant. ergodicity assumption USED-FOR variant. online setting USED-FOR navigation constraints. ergodic theorem USED-FOR non - homogeneous Markov chains. ergodic theorem USED-FOR analysis. OtherScientificTerm are system trajectory, and problem - dependent lower bound. Task are generative setting, and analysis of Markov Decision Processes. ","This paper studies the active pure exploration problem in Markov Decision Processes, where the goal is to explore the entire system trajectory. The authors provide an instance-specific sample complexity bound for the algorithm for communicating MDPs, and a problem-dependent lower bound for a generative setting. They also provide a variant with reduced exploration rate and faster convergence under an ergodicity assumption. The analysis is based on the ergodic theorem for non-homogeneous Markov chains, and the authors show that in the online setting, the navigation constraints can be enforced in an online setting. ","This paper studies the active pure exploration problem in Markov Decision Processes, where the goal is to explore the entire system trajectory. The authors provide an instance-specific sample complexity bound for the algorithm for communicating MDPs, and a problem-dependent lower bound for a generative setting. They also provide a variant with reduced exploration rate and faster convergence under an ergodicity assumption. The analysis is based on the ergodic theorem for non-homogeneous Markov chains, and the authors show that in the online setting, the navigation constraints can be enforced in an online setting. "
11251,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"entities CONJUNCTION first - order logical ( FOL ) queries. first - order logical ( FOL ) queries CONJUNCTION entities. low - dimensional spaces FEATURE-OF first - order logical ( FOL ) queries. low - dimensional spaces FEATURE-OF entities. conjunction CONJUNCTION disjunction. disjunction CONJUNCTION conjunction. disjunction CONJUNCTION negation. negation CONJUNCTION disjunction. geometry - based QE model USED-FOR FOL operations. query embedding model USED-FOR FOL operations. Cone Embeddings ( ConE ) HYPONYM-OF geometry - based QE model. Cone Embeddings ( ConE ) HYPONYM-OF query embedding model. negation HYPONYM-OF FOL operations. disjunction HYPONYM-OF FOL operations. conjunction HYPONYM-OF FOL operations. geometric complement operators USED-FOR negation operations. embedding space FEATURE-OF geometric complement operators. ConE COMPARE state - of - the - art methods. state - of - the - art methods COMPARE ConE. benchmark datasets EVALUATE-FOR state - of - the - art methods. benchmark datasets EVALUATE-FOR ConE. Task is multi - hop reasoning over knowledge graphs. OtherScientificTerm are knowledge graphs, geometric shapes, Cartesian products of two - dimensional cones, conjunction and disjunction operations, closure of complement of cones, and cones. Method is geometry - based models. ","This paper addresses the problem of multi-hop reasoning over knowledge graphs, where entities and first-order logical (FOL) queries are embedded in low-dimensional spaces. The authors propose a geometry-based QE model called Cone Embeddings (ConE) which is a query embedding model for FOL operations such as conjunction, disjunction, negation, etc. ConE is based on Cartesian products of two-dimensional cones, which are geometric shapes. The geometric complement operators in the embedding space are used to encode negation operations, and the authors show that ConE outperforms previous state-of-the-art methods on several benchmark datasets. The main contribution of the paper is the introduction of geometric complement operations, which can be seen as an extension of previous work on the closure of complement of cones. The paper also shows that the geometric-based models can be used to embed FOL operators in a similar way to previous work.","This paper addresses the problem of multi-hop reasoning over knowledge graphs, where entities and first-order logical (FOL) queries are embedded in low-dimensional spaces. The authors propose a geometry-based QE model called Cone Embeddings (ConE) which is a query embedding model for FOL operations such as conjunction, disjunction, negation, etc. ConE is based on Cartesian products of two-dimensional cones, which are geometric shapes. The geometric complement operators in the embedding space are used to encode negation operations, and the authors show that ConE outperforms previous state-of-the-art methods on several benchmark datasets. The main contribution of the paper is the introduction of geometric complement operations, which can be seen as an extension of previous work on the closure of complement of cones. The paper also shows that the geometric-based models can be used to embed FOL operators in a similar way to previous work."
11276,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"linear - time Legendre transform USED-FOR numerical scheme. numerical scheme USED-FOR value iteration ( VI ) algorithm. linear - time Legendre transform USED-FOR value iteration ( VI ) algorithm. conjugate domain FEATURE-OF value iteration ( VI ) algorithm. time complexity EVALUATE-FOR algorithm. error EVALUATE-FOR algorithm. convergence EVALUATE-FOR algorithm. convergence CONJUNCTION time complexity. time complexity CONJUNCTION convergence. time complexity CONJUNCTION error. error CONJUNCTION time complexity. minimization operation PART-OF primal domain. discretization USED-FOR state and input spaces. discretization USED-FOR approach. time complexity EVALUATE-FOR approach. Task is stochastic nonlinear systems. OtherScientificTerm are state and input variables, and O(X + U ). Method is VI algorithm. ",This paper considers stochastic nonlinear systems where the state and input variables are nonconvex. The authors propose a numerical scheme based on the linear-time Legendre transform to approximate the value iteration (VI) algorithm in the conjugate domain. They show that the convergence and time complexity of the proposed algorithm are O(X + U) and O(U) respectively. The paper also shows that the minimization operation in the primal domain of the VI algorithm can be decomposed into a minimization of the sum of a minimizer and a minimiser. The proposed approach is based on discretization for both the states and input spaces. ,This paper considers stochastic nonlinear systems where the state and input variables are nonconvex. The authors propose a numerical scheme based on the linear-time Legendre transform to approximate the value iteration (VI) algorithm in the conjugate domain. They show that the convergence and time complexity of the proposed algorithm are O(X + U) and O(U) respectively. The paper also shows that the minimization operation in the primal domain of the VI algorithm can be decomposed into a minimization of the sum of a minimizer and a minimiser. The proposed approach is based on discretization for both the states and input spaces. 
11301,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"framework USED-FOR multimodal representations. unlabeled data USED-FOR framework. unlabeled data USED-FOR multimodal representations. convolution - free Transformer architectures USED-FOR framework. multimodal representations USED-FOR downstream tasks. VideoAudio - Text Transformer ( VATT ) USED-FOR multimodal representations. raw signals USED-FOR VideoAudio - Text Transformer ( VATT ). audio event classification CONJUNCTION image classification. image classification CONJUNCTION audio event classification. image classification CONJUNCTION text - to - video retrieval. text - to - video retrieval CONJUNCTION image classification. video action recognition CONJUNCTION audio event classification. audio event classification CONJUNCTION video action recognition. video action recognition HYPONYM-OF downstream tasks. text - to - video retrieval HYPONYM-OF downstream tasks. image classification HYPONYM-OF downstream tasks. audio event classification HYPONYM-OF downstream tasks. multimodal contrastive losses USED-FOR VATT. convolution - free VATT COMPARE ConvNet - based architectures. ConvNet - based architectures COMPARE convolution - free VATT. ConvNet - based architectures USED-FOR downstream tasks. convolution - free VATT USED-FOR downstream tasks. Kinetics-400 EVALUATE-FOR VATT ’s vision Transformer. top-1 accuracy EVALUATE-FOR VATT ’s vision Transformer. videos CONJUNCTION images. images CONJUNCTION videos. VATT ’s audio Transformer USED-FOR waveform - based audio event recognition. mAP EVALUATE-FOR VATT ’s audio Transformer. Method are modality - agnostic, single - backbone Transformer, and supervised pre - training. Material are Kinetics-600, Kinetics-700, ImageNet, and AudioSet. Generic are Transformer, and model. ","This paper proposes a new framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. The proposed VideoAudio-Text Transformer (VATT) is a modality-agnostic, single-backbone Transformer that is trained on raw signals. The authors show that the proposed framework is able to achieve state-of-the-art performance on several downstream tasks, including video action recognition, audio event classification, image classification, and text-to-video retrieval. VATT is trained with multi-modal contrastive losses, and the authors also show that VATT’s vision Transformer achieves the top-1 accuracy on Kinetics-600, Kinetic-700, and ImageNet, and achieves the best mAP on the mAP of any Transformer. The paper also shows that the convolution free VATT outperforms ConvNet-based architectures on most of the downstream tasks. Finally, the authors show how VATT can be used to train a waveform-based audio event recognition model on both videos and images, and demonstrate that the model can also be used for the task of audio-text retrieval.    The authors also provide a thorough ablation study on the effect of supervised pre-training. ","This paper proposes a new framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. The proposed VideoAudio-Text Transformer (VATT) is a modality-agnostic, single-backbone Transformer that is trained on raw signals. The authors show that the proposed framework is able to achieve state-of-the-art performance on several downstream tasks, including video action recognition, audio event classification, image classification, and text-to-video retrieval. VATT is trained with multi-modal contrastive losses, and the authors also show that VATT’s vision Transformer achieves the top-1 accuracy on Kinetics-600, Kinetic-700, and ImageNet, and achieves the best mAP on the mAP of any Transformer. The paper also shows that the convolution free VATT outperforms ConvNet-based architectures on most of the downstream tasks. Finally, the authors show how VATT can be used to train a waveform-based audio event recognition model on both videos and images, and demonstrate that the model can also be used for the task of audio-text retrieval.    The authors also provide a thorough ablation study on the effect of supervised pre-training. "
11326,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"quadratic time and space complexity EVALUATE-FOR self - attention mechanism. computation bottleneck FEATURE-OF pairwise dot products. computation bottleneck FEATURE-OF kernel machines. computational cost EVALUATE-FOR approximation schemes. accuracy EVALUATE-FOR approximation schemes. computation methods USED-FOR kernel machines. Nyström method USED-FOR computation. Nyström method USED-FOR non - positive semidefinite matrix. softmax structure CONJUNCTION Gaussian kernel. Gaussian kernel CONJUNCTION softmax structure. computation methods USED-FOR computational cost. matrix approximation error EVALUATE-FOR method. spectral norm FEATURE-OF matrix approximation error. spectral norm EVALUATE-FOR method. method COMPARE full self - attention. full self - attention COMPARE method. Long Range Arena benchmark EVALUATE-FOR method. Long Range Arena benchmark EVALUATE-FOR full self - attention. Method are Transformers, and Skyformer. OtherScientificTerm is computation resources. ","This paper studies the quadratic time and space complexity of the self-attention mechanism in Transformers. The authors show that the computation bottleneck of pairwise dot products has a significant impact on the computational cost and accuracy of approximation schemes. To alleviate this issue, the authors propose to use Nyström method to reduce the computation of the non-positive semidefinite matrix. They also propose several computation methods for kernel machines that can be applied to reduce computational cost.  The authors demonstrate that the softmax structure of a Gaussian kernel can be approximated by the proposed method. The proposed method is shown to have a lower matrix approximation error with respect to the spectral norm of the true matrix compared to other methods. The method is also shown to be more computationally efficient than full self-ention on the Long Range Arena benchmark.    The paper is well-written and well-motivated. The contribution of the paper is that the authors provide a theoretical analysis of the computation resources. The paper also shows that Skyformer can be used as an efficient alternative to the proposed methods. ","This paper studies the quadratic time and space complexity of the self-attention mechanism in Transformers. The authors show that the computation bottleneck of pairwise dot products has a significant impact on the computational cost and accuracy of approximation schemes. To alleviate this issue, the authors propose to use Nyström method to reduce the computation of the non-positive semidefinite matrix. They also propose several computation methods for kernel machines that can be applied to reduce computational cost.  The authors demonstrate that the softmax structure of a Gaussian kernel can be approximated by the proposed method. The proposed method is shown to have a lower matrix approximation error with respect to the spectral norm of the true matrix compared to other methods. The method is also shown to be more computationally efficient than full self-ention on the Long Range Arena benchmark.    The paper is well-written and well-motivated. The contribution of the paper is that the authors provide a theoretical analysis of the computation resources. The paper also shows that Skyformer can be used as an efficient alternative to the proposed methods. "
11351,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"setting PART-OF problems. image - based data augmentation USED-FOR invariance. image - based data augmentation CONJUNCTION expert - aware offline data augmentation approach. expert - aware offline data augmentation approach CONJUNCTION image - based data augmentation. expert - aware offline data augmentation approach USED-FOR feedback - sensitivity. image - based data augmentation USED-FOR image perturbations. image - based data augmentation PART-OF augmented policy cloning ( APC ) approach. method USED-FOR transfer 12 of complex high - DoF behaviors. method USED-FOR policy cloning. data - efficiency EVALUATE-FOR policy cloning. approach USED-FOR algorithms. policy cloning USED-FOR transfer 12 of complex high - DoF behaviors. policy cloning PART-OF algorithms. data - efficiency EVALUATE-FOR method. Method are data - augmentation technique, and behavioral cloning. Task is policy 3 cloning setting. Metric is data efficiency. OtherScientificTerm are expert, student policy, and expert trajectories. ","This paper proposes a data-augmentation technique to improve the performance of policy cloning in the policy 3 cloning setting, where the goal is to achieve data efficiency. This setting is a generalization of two existing problems: (1) behavioral cloning, where an expert is trained to imitate the behavior of a student policy, and (2) policy cloning, in which an expert has access to a dataset of expert trajectories.  The paper proposes an augmented policy cloning (APC) approach that incorporates image-based data augmentation to improve invariance to image perturbations, and an expert-aware offline data augmentation approach to improve feedback-sensitivity. The proposed method is shown to improve data-efficiency in policy cloning for transfer 12 of complex high-DoF behaviors, and the proposed approach is applied to several algorithms that combine policy cloning with policy cloning.  ","This paper proposes a data-augmentation technique to improve the performance of policy cloning in the policy 3 cloning setting, where the goal is to achieve data efficiency. This setting is a generalization of two existing problems: (1) behavioral cloning, where an expert is trained to imitate the behavior of a student policy, and (2) policy cloning, in which an expert has access to a dataset of expert trajectories.  The paper proposes an augmented policy cloning (APC) approach that incorporates image-based data augmentation to improve invariance to image perturbations, and an expert-aware offline data augmentation approach to improve feedback-sensitivity. The proposed method is shown to improve data-efficiency in policy cloning for transfer 12 of complex high-DoF behaviors, and the proposed approach is applied to several algorithms that combine policy cloning with policy cloning.  "
11376,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,benchmarks CONJUNCTION simulated robotics environment. simulated robotics environment CONJUNCTION benchmarks. simulated robotics environment EVALUATE-FOR framework. benchmarks EVALUATE-FOR framework. Task is computer vision settings. Method is deep networks. ,This paper proposes a new framework for training deep neural networks in computer vision settings. The proposed framework is evaluated on several benchmarks and a simulated robotics environment. Experiments show that the proposed framework can achieve state-of-the-art performance. The paper also shows that deep networks can be trained to generalize to unseen tasks. ,This paper proposes a new framework for training deep neural networks in computer vision settings. The proposed framework is evaluated on several benchmarks and a simulated robotics environment. Experiments show that the proposed framework can achieve state-of-the-art performance. The paper also shows that deep networks can be trained to generalize to unseen tasks. 
11401,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"agents USED-FOR tasks. Reinforcement Learning ( RL ) USED-FOR agents. visual observations USED-FOR agents. visual observations USED-FOR tasks. data augmentation USED-FOR generalization. generalization FEATURE-OF RL. data augmentation USED-FOR off - policy RL algorithms. data augmentation USED-FOR instability. technique USED-FOR algorithms. ConvNets CONJUNCTION Vision Transformers ( ViT ). Vision Transformers ( ViT ) CONJUNCTION ConvNets. benchmarks CONJUNCTION robotic manipulation tasks. robotic manipulation tasks CONJUNCTION benchmarks. benchmarks EVALUATE-FOR Vision Transformers ( ViT ). benchmarks EVALUATE-FOR image - based RL. DeepMind Control Suite USED-FOR benchmarks. Vision Transformers ( ViT ) USED-FOR image - based RL. ConvNets USED-FOR image - based RL. state - of - the - art methods USED-FOR image - based RL. stability CONJUNCTION sample efficiency. sample efficiency CONJUNCTION stability. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. generalization EVALUATE-FOR state - of - the - art methods. stability FEATURE-OF ConvNets. sample efficiency FEATURE-OF ConvNets. method USED-FOR ConvNets. augmentation FEATURE-OF ConvNets. sample efficiency EVALUATE-FOR method. stability EVALUATE-FOR method. generalization EVALUATE-FOR method. method USED-FOR RL. ViT - based architectures USED-FOR method. ViT - based architectures USED-FOR RL. OtherScientificTerm are divergence, and high - variance Q - targets. Generic is problems. ","This paper proposes a data augmentation method for off-policy reinforcement learning (off-policy RL) to improve the stability and sample efficiency of RL algorithms. Specifically, the authors propose to use a convolutional neural network (ConvNets) and a vision transformer (ViT) to augment the Q-values of a policy to improve its generalization performance. The authors show that the proposed method can improve the generalization of RL methods on a variety of tasks. The paper also shows that the method improves the sample efficiency and stability of existing algorithms.","This paper proposes a data augmentation method for off-policy reinforcement learning (off-policy RL) to improve the stability and sample efficiency of RL algorithms. Specifically, the authors propose to use a convolutional neural network (ConvNets) and a vision transformer (ViT) to augment the Q-values of a policy to improve its generalization performance. The authors show that the proposed method can improve the generalization of RL methods on a variety of tasks. The paper also shows that the method improves the sample efficiency and stability of existing algorithms."
11426,SP:f8ca9d92c45adc4512381035856b445029e3080a,"local data USED-FOR joint model. minibatch sizes CONJUNCTION number of local updates. number of local updates CONJUNCTION minibatch sizes. WNs ’ and the server ’s update directions CONJUNCTION minibatch sizes. minibatch sizes CONJUNCTION WNs ’ and the server ’s update directions. communication rounds USED-FOR WNs. WNs USED-FOR local updates. algorithm USED-FOR ✏ -stationary solution. Õ ( ✏ 3/2 ) samples CONJUNCTION Õ ( ✏ 1 ) communication rounds. Õ ( ✏ 1 ) communication rounds CONJUNCTION Õ ( ✏ 3/2 ) samples. stochastic momentum estimator USED-FOR WN ’s and the server ’s directions. Õ ( ✏ 1 ) communication rounds USED-FOR algorithm. Õ ( ✏ 3/2 ) samples USED-FOR algorithm. near - optimal sample and communication complexities EVALUATE-FOR FL algorithm. WNs ’ and server ’s update directions CONJUNCTION minibatch sizes. minibatch sizes CONJUNCTION WNs ’ and server ’s update directions. Method are Federated Learning ( FL ), stochastic algorithms, and FL algorithms. Task is non - convex FL problem. Metric is sample and communication complexities. OtherScientificTerm is trade - off curve. ","This paper studies the problem of Federated Learning (FL) in the non-convex FL problem, where the joint model is trained on local data, and the goal is to minimize the total number of communication rounds between the WNs’ and the server’s update directions, minibatch sizes and the number of local updates. This is a well-studied problem, and existing stochastic algorithms have been shown to achieve near-optimal sample and communication complexities. This paper proposes a new algorithm, called FL algorithm with Õ(3/2) samples and Ú(／ 1) communication rounds, to achieve a ✏-stationary solution. The algorithm is based on the observation that the trade-off curve of FL algorithms is convex if and only if WNs and servers share the same update directions. The authors propose to use a simple and elegant way to compute the tradeoff between WNs' and server's update directions and minibatches, and then use a stochedastic momentum estimator to estimate WN's and servers' directions.  ","This paper studies the problem of Federated Learning (FL) in the non-convex FL problem, where the joint model is trained on local data, and the goal is to minimize the total number of communication rounds between the WNs’ and the server’s update directions, minibatch sizes and the number of local updates. This is a well-studied problem, and existing stochastic algorithms have been shown to achieve near-optimal sample and communication complexities. This paper proposes a new algorithm, called FL algorithm with Õ(3/2) samples and Ú(／ 1) communication rounds, to achieve a ✏-stationary solution. The algorithm is based on the observation that the trade-off curve of FL algorithms is convex if and only if WNs and servers share the same update directions. The authors propose to use a simple and elegant way to compute the tradeoff between WNs' and server's update directions and minibatches, and then use a stochedastic momentum estimator to estimate WN's and servers' directions.  "
11451,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"information - theoretical framework USED-FOR non - vacuous generalization bounds. non - vacuous generalization bounds USED-FOR large models. isotropic noise USED-FOR Stochastic Gradient Langevin Dynamics ( SGLD ). Stochastic Gradient Langevin Dynamics ( SGLD ) USED-FOR large models. noise structure USED-FOR SGLD. noise structure USED-FOR information - theoretical generalization bound. expected gradient covariance USED-FOR optimal noise covariance. optimal noise COMPARE empirical gradient covariance. empirical gradient covariance COMPARE optimal noise. information - theoretical bound USED-FOR optimization analysis. matrix analysis USED-FOR optimal noise covariance. OtherScientificTerm are constraint, and prior. ","This paper proposes an information-theoretic framework to derive non-vacuous generalization bounds for large models trained with Stochastic Gradient Langevin Dynamics (SGLD) with isotropic noise. The key idea is to use the noise structure of SGLD to derive the information-towards the optimal noise covariance, which is a constraint on the prior. The optimal noise is defined as the expected gradient covariance that minimizes the expected expected gradient of the prior, and the authors show that this optimal noise can be found by a matrix analysis. The authors also provide an optimization analysis based on this information-termed generalization bound. ","This paper proposes an information-theoretic framework to derive non-vacuous generalization bounds for large models trained with Stochastic Gradient Langevin Dynamics (SGLD) with isotropic noise. The key idea is to use the noise structure of SGLD to derive the information-towards the optimal noise covariance, which is a constraint on the prior. The optimal noise is defined as the expected gradient covariance that minimizes the expected expected gradient of the prior, and the authors show that this optimal noise can be found by a matrix analysis. The authors also provide an optimization analysis based on this information-termed generalization bound. "
11476,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,Learned video compression methods COMPARE video codecs. video codecs COMPARE Learned video compression methods. prediction mode CONJUNCTION fixed network framework. fixed network framework CONJUNCTION prediction mode. prediction mode USED-FOR learned video compression schemes. fixed network framework USED-FOR learned video compression schemes. model USED-FOR prediction modes. 3D motion vector fields USED-FOR weighted 9 trilinear warping. voxel flows USED-FOR weighted 9 trilinear warping. spatial - temporal space FEATURE-OF weighted 9 trilinear warping. 3D motion vector fields USED-FOR motion compensation 8 module. motion compensation 8 module USED-FOR versatile compression. voxel flows HYPONYM-OF 3D motion vector fields. temporal reference position FEATURE-OF voxel flows. flow prediction module USED-FOR motion trajectories. flow prediction module USED-FOR multiple - reference - frame predic12 tion. unified polynomial function USED-FOR flow prediction module. unified polynomial function USED-FOR motion trajectories. flow prediction module USED-FOR voxel flows. VLVC USED-FOR versatile compression. VLVC COMPARE Versatile Video Coding 17 ( VVC ) standard. Versatile Video Coding 17 ( VVC ) standard COMPARE VLVC. R - D performance EVALUATE-FOR Versatile Video Coding 17 ( VVC ) standard. MS - SSIM EVALUATE-FOR R - D performance. R - D performance EVALUATE-FOR VLVC. MS - SSIM EVALUATE-FOR Versatile Video Coding 17 ( VVC ) standard. OtherScientificTerm is inter prediction modes. ,"Learned video compression methods have been shown to outperform state-of-the-art video codecs. However, most learned video compression schemes rely on either a single prediction mode or a fixed network framework. In this paper, the authors propose a model that can be applied to both prediction modes. The motion compensation 8 module uses 3D motion vector fields (i.e., voxel flows with temporal reference position) to compute a weighted 9 trilinear warping in spatial-temporal space, which can be used for versatile compression. The flow prediction module predicts motion trajectories using a unified polynomial function, which is used to predict the flow of the voxels. The authors also propose a flow prediction for multiple-reference-frame predic12 tion, where the flow prediction is applied to multiple frames and the inter prediction modes can be combined. Experiments show that VLVC outperforms the Versatile Video Coding 17 (VVC) standard in terms of R-D performance on MS-SSIM. ","Learned video compression methods have been shown to outperform state-of-the-art video codecs. However, most learned video compression schemes rely on either a single prediction mode or a fixed network framework. In this paper, the authors propose a model that can be applied to both prediction modes. The motion compensation 8 module uses 3D motion vector fields (i.e., voxel flows with temporal reference position) to compute a weighted 9 trilinear warping in spatial-temporal space, which can be used for versatile compression. The flow prediction module predicts motion trajectories using a unified polynomial function, which is used to predict the flow of the voxels. The authors also propose a flow prediction for multiple-reference-frame predic12 tion, where the flow prediction is applied to multiple frames and the inter prediction modes can be combined. Experiments show that VLVC outperforms the Versatile Video Coding 17 (VVC) standard in terms of R-D performance on MS-SSIM. "
11501,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"regret EVALUATE-FOR MT - OMD. geometry FEATURE-OF regularizer. geometry FEATURE-OF task 3 variance. OMDs USED-FOR √ NT bound. Online Gradient Descent CONJUNCTION Exponentiated 7 Gradient. Exponentiated 7 Gradient CONJUNCTION Online Gradient Descent. Exponentiated 7 Gradient HYPONYM-OF OMD. Method are Online Mirror 1 Descent ( OMD ), and closed - form updates. OtherScientificTerm are time horizon, and σ. Generic is them. Material is real - world datasets. ","This paper studies the regret of the Online Mirror 1 Descent (OMD) algorithm, which is a variant of the well-known Online Gradient Descent algorithm. In particular, the paper shows that under certain assumptions, the regret bound of the MT-OMD algorithm can be improved to the NT bound of $O(\sqrt{T}^T})$ for any $T$-th time horizon. This is achieved by considering the geometry of the regularizer $t$ and the task 3 variance $T(T)$, which depends on the geometry $T$. The paper also shows that for any $\tilde{O}(\log T)$ and $T^{-1}$, there exists a closed-form update of the OMD algorithm.    The main contribution of the paper is to show that for a number of OMDs (e.g., OMD, Exponentiated 7-Gradient, Online Mirror-1 Descent, etc.), the $\log T$-NT bound is upper bounded by $O(T^T)$ when $T=1/\sqrt{\log T}$. The authors also show that this is the case for any OMD with a closed form update.  The paper further shows that this result holds for any two OMD algorithms with closed form updates (online gradient Descent and Exponentated 7 Gradient) and shows that it holds for all of them. Finally, the authors show that under some assumptions on the time horizon, the σ of the update is bounded by $\log t$.   Experiments are conducted on two real-world datasets, and the results are shown to be consistent with the theoretical results. ","This paper studies the regret of the Online Mirror 1 Descent (OMD) algorithm, which is a variant of the well-known Online Gradient Descent algorithm. In particular, the paper shows that under certain assumptions, the regret bound of the MT-OMD algorithm can be improved to the NT bound of $O(\sqrt{T}^T})$ for any $T$-th time horizon. This is achieved by considering the geometry of the regularizer $t$ and the task 3 variance $T(T)$, which depends on the geometry $T$. The paper also shows that for any $\tilde{O}(\log T)$ and $T^{-1}$, there exists a closed-form update of the OMD algorithm.    The main contribution of the paper is to show that for a number of OMDs (e.g., OMD, Exponentiated 7-Gradient, Online Mirror-1 Descent, etc.), the $\log T$-NT bound is upper bounded by $O(T^T)$ when $T=1/\sqrt{\log T}$. The authors also show that this is the case for any OMD with a closed form update.  The paper further shows that this result holds for any two OMD algorithms with closed form updates (online gradient Descent and Exponentated 7 Gradient) and shows that it holds for all of them. Finally, the authors show that under some assumptions on the time horizon, the σ of the update is bounded by $\log t$.   Experiments are conducted on two real-world datasets, and the results are shown to be consistent with the theoretical results. "
11526,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,ε - error USED-FOR approximating d - dimensional ULD. finite summation of N smooth components PART-OF stronglyconvex potential. stronglyconvex potential USED-FOR underdamped Langevin diffusion ( ULD ). gradient evaluations USED-FOR discretization method. method USED-FOR strongly - log - concave distribution. gradient complexity EVALUATE-FOR gradient based sampling algorithms. method COMPARE gradient based sampling algorithms. gradient based sampling algorithms COMPARE method. gradient complexity EVALUATE-FOR method. synthetic and real - world data EVALUATE-FOR method. method COMPARE ULD approaches. ULD approaches COMPARE method. synthetic and real - world data EVALUATE-FOR ULD approaches. ,"This paper studies the problem of approximating underdamped Langevin diffusion (ULD) with a stronglyconvex potential with finite summation of N smooth components. The authors propose a discretization method based on gradient evaluations. They show that approximating d-dimensional ULD with Eq. (2) with the eigenvectors of the log-concave distribution is equivalent to approximating the log of the Eigenvalue of Eq (2). The authors also show that the method can be used to approximate a strongly-log-concentrated distribution. Finally, the authors compare the gradient complexity of their method with gradient based sampling algorithms and show that their method outperforms existing ULD approaches on both synthetic and real-world data. ","This paper studies the problem of approximating underdamped Langevin diffusion (ULD) with a stronglyconvex potential with finite summation of N smooth components. The authors propose a discretization method based on gradient evaluations. They show that approximating d-dimensional ULD with Eq. (2) with the eigenvectors of the log-concave distribution is equivalent to approximating the log of the Eigenvalue of Eq (2). The authors also show that the method can be used to approximate a strongly-log-concentrated distribution. Finally, the authors compare the gradient complexity of their method with gradient based sampling algorithms and show that their method outperforms existing ULD approaches on both synthetic and real-world data. "
11551,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"recurrent networks USED-FOR neural dynamics. neural dynamics USED-FOR biological continual learning. feedforward and recurrent neural networks EVALUATE-FOR methods. weight regularization CONJUNCTION projected gradient descent. projected gradient descent CONJUNCTION weight regularization. projected gradient descent PART-OF method. weight regularization PART-OF method. catastrophic forgetting FEATURE-OF optimization. prior precision USED-FOR catastrophic forgetting. gradient projection USED-FOR NCL. prior precision USED-FOR gradient projection. Bayesian weight regularization USED-FOR NCL. projection based approaches USED-FOR continual learning problems. weight regularization techniques CONJUNCTION projection based approaches. projection based approaches CONJUNCTION weight regularization techniques. method USED-FOR continual learning problems. method COMPARE weight regularization techniques. weight regularization techniques COMPARE method. method COMPARE projection based approaches. projection based approaches COMPARE method. feedforward and recurrent networks USED-FOR continual learning problems. networks USED-FOR task - specific dynamics. Method are Biological agents, artificial agents, specific parameter regularizers, and Natural Continual Learning ( NCL ). OtherScientificTerm are parameter space, gradients, and biological circuits. Task is optimization journey. ","This paper studies biological continual learning from the perspective of neural dynamics in recurrent networks. The authors propose a method called Natural Continual Learning (NCL) that combines weight regularization and projected gradient descent to improve the performance of existing methods on feedforward and recurrent neural networks.  Biological agents are able to learn a task-specific dynamics in a continual learning setting, while artificial agents are not able to do so.  NCL is based on the observation that in the parameter space of a neural network, the gradients of the current task are similar to those of a biological circuit, and that the optimization process is similar to that of biological circuits.  The authors then propose to use specific parameter regularizers to ensure that the gradient projection in NCL uses prior precision to avoid catastrophic forgetting in optimization.  They also propose a Bayesian weight regularisation for NCL.  Experiments show that the proposed method outperforms existing weight regularized techniques and projection based approaches for continual learning problems on both feedforward networks and recurrent networks, and is able to capture task- specific dynamics in networks that are not trained on a single task. ","This paper studies biological continual learning from the perspective of neural dynamics in recurrent networks. The authors propose a method called Natural Continual Learning (NCL) that combines weight regularization and projected gradient descent to improve the performance of existing methods on feedforward and recurrent neural networks.  Biological agents are able to learn a task-specific dynamics in a continual learning setting, while artificial agents are not able to do so.  NCL is based on the observation that in the parameter space of a neural network, the gradients of the current task are similar to those of a biological circuit, and that the optimization process is similar to that of biological circuits.  The authors then propose to use specific parameter regularizers to ensure that the gradient projection in NCL uses prior precision to avoid catastrophic forgetting in optimization.  They also propose a Bayesian weight regularisation for NCL.  Experiments show that the proposed method outperforms existing weight regularized techniques and projection based approaches for continual learning problems on both feedforward networks and recurrent networks, and is able to capture task- specific dynamics in networks that are not trained on a single task. "
11576,SP:26de056be14962312c759be5d284ef235d660f9c,"Normalizing flows HYPONYM-OF invertible neural networks. change - of - volume terms FEATURE-OF invertible neural networks. low - dimensional manifold PART-OF high - dimensional ambient space. heuristics USED-FOR approaches. heuristics USED-FOR term. methods USED-FOR gradient. automatic differentiation CONJUNCTION numerical linear algebra. numerical linear algebra CONJUNCTION automatic differentiation. gradient FEATURE-OF term. automatic differentiation USED-FOR methods. approaches USED-FOR end - to - end nonlinear manifold learning. manifolds CONJUNCTION distributions. distributions CONJUNCTION manifolds. Method is maximum likelihood. OtherScientificTerm are modelling mismatch, invertibility requirement, Injective flows, lowto high - dimensional spaces, and volume - change term. Generic are manifold, and model. Task is out - of - distribution detection. ","Normalizing flows are a class of invertible neural networks with change-of-volume terms. Injective flows can be seen as a mapping from low-dimensional manifold to a high-dimensional ambient space, where the maximum likelihood is a function of the modelling mismatch. In this paper, the authors focus on the invertibility requirement and propose two heuristics to compute this term. The first is based on existing methods for computing the gradient of the term, which uses automatic differentiation and numerical linear algebra. The second is a generalization of existing approaches for end-to-end nonlinear manifold learning.    The authors provide a theoretical analysis of their approach. They show that if the manifold is non-invertible, then the model will not be able to learn on it. In contrast, if the manifolds and the distributions are non-infinitesimal, the model can learn on them. The authors also provide an empirical analysis that shows that their approach is able to generalize from lowto high-dimensions, and that the volume-change term can be computed on any manifold. Finally, they provide an application of their method to the problem of out-of distribution detection. ","Normalizing flows are a class of invertible neural networks with change-of-volume terms. Injective flows can be seen as a mapping from low-dimensional manifold to a high-dimensional ambient space, where the maximum likelihood is a function of the modelling mismatch. In this paper, the authors focus on the invertibility requirement and propose two heuristics to compute this term. The first is based on existing methods for computing the gradient of the term, which uses automatic differentiation and numerical linear algebra. The second is a generalization of existing approaches for end-to-end nonlinear manifold learning.    The authors provide a theoretical analysis of their approach. They show that if the manifold is non-invertible, then the model will not be able to learn on it. In contrast, if the manifolds and the distributions are non-infinitesimal, the model can learn on them. The authors also provide an empirical analysis that shows that their approach is able to generalize from lowto high-dimensions, and that the volume-change term can be computed on any manifold. Finally, they provide an application of their method to the problem of out-of distribution detection. "
11601,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"response time FEATURE-OF physical computational elements. inference CONJUNCTION learning. learning CONJUNCTION inference. framework USED-FOR inference. framework USED-FOR learning. inference CONJUNCTION learning. learning CONJUNCTION inference. networks of slow components USED-FOR learning. principle USED-FOR quasi - instantaneous inference. phased plasticity CONJUNCTION network relaxation phases. network relaxation phases CONJUNCTION phased plasticity. prospective energy function USED-FOR disentangled neuron and synapse dynamics. continuous - time, leaky neuronal dynamics CONJUNCTION continuously active, local plasticity. continuously active, local plasticity CONJUNCTION continuous - time, leaky neuronal dynamics. error backpropagation USED-FOR deep cortical networks. continuous - time, leaky neuronal dynamics USED-FOR error backpropagation. benchmark datasets EVALUATE-FOR learning. fully - connected and convolutional architectures USED-FOR learning. robustness EVALUATE-FOR model. model USED-FOR physical realization. spatio - temporal substrate imperfections FEATURE-OF robustness. spatio - temporal substrate imperfections FEATURE-OF model. OtherScientificTerm are neurons, response lag, delayed processing of stimuli, timing mismatch, instructive signals, biological neurons, membrane potential, and network depth. Method are hierarchical models of cortical networks, physical dynamical systems, and Latent Equilibrium. ","This paper proposes a new framework for inference and learning in networks of slow components, where the physical computational elements have a slow response time due to the response lag due to delayed processing of stimuli. The authors propose a principle for quasi-instantaneous inference based on the principle that hierarchical models of cortical networks can be seen as physical dynamical systems. They show that this principle holds for a number of biological neurons, including those with delayed processing due to a timing mismatch between the instructive signals and the response of the neurons. They also show that the disentangled neuron and synapse dynamics can be modeled as a prospective energy function, which they call the Latent Equilibrium.   The authors show that in biological neurons there is a phase of phased plasticity and a series of network relaxation phases, where each phase corresponds to a different type of membrane potential. They then show that error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity can be understood as a result of this phase.  Finally, they show that for learning in fully-connected and convolutional architectures with learning on standard benchmark datasets, their model is robust to spatio-temporal substrate imperfections and robust to robustness to physical realization. ","This paper proposes a new framework for inference and learning in networks of slow components, where the physical computational elements have a slow response time due to the response lag due to delayed processing of stimuli. The authors propose a principle for quasi-instantaneous inference based on the principle that hierarchical models of cortical networks can be seen as physical dynamical systems. They show that this principle holds for a number of biological neurons, including those with delayed processing due to a timing mismatch between the instructive signals and the response of the neurons. They also show that the disentangled neuron and synapse dynamics can be modeled as a prospective energy function, which they call the Latent Equilibrium.   The authors show that in biological neurons there is a phase of phased plasticity and a series of network relaxation phases, where each phase corresponds to a different type of membrane potential. They then show that error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity can be understood as a result of this phase.  Finally, they show that for learning in fully-connected and convolutional architectures with learning on standard benchmark datasets, their model is robust to spatio-temporal substrate imperfections and robust to robustness to physical realization. "
11626,SP:b937901e3230b14e36975fbab0658a52bdac4977,"Graph neural network ( GNN ) USED-FOR graph classification. node representation USED-FOR rooted subtree. GNN USED-FOR node representation. 1 - WL USED-FOR node representation. 1 - WL CONJUNCTION GNN. GNN CONJUNCTION 1 - WL. representation USED-FOR graph. rooted subtree representations PART-OF representation. rooted subtrees USED-FOR nontree graph. NGNN USED-FOR graph. rooted subgraphs COMPARE rooted subtrees. rooted subtrees COMPARE rooted subgraphs. rooted subgraphs FEATURE-OF graph. GNN USED-FOR subgraph representation. NGNN USED-FOR subgraph representation. GNN USED-FOR subgraph. NGNN USED-FOR local subgraph. GNN USED-FOR NGNN. subgraph representations USED-FOR whole - graph representation. NGNN COMPARE 1 - WL. 1 - WL COMPARE NGNN. NGNN USED-FOR r - regular graphs. NGNN COMPARE GNNs. GNNs COMPARE NGNN. GNNs COMPARE NGNN. NGNN COMPARE GNNs. time complexity EVALUATE-FOR GNNs. time complexity EVALUATE-FOR NGNN. NGNN HYPONYM-OF plug - and - play framework. plug - and - play framework CONJUNCTION base GNNs. base GNNs CONJUNCTION plug - and - play framework. NGNN CONJUNCTION base GNNs. base GNNs CONJUNCTION NGNN. NGNN COMPARE base GNNs. base GNNs COMPARE NGNN. benchmark datasets EVALUATE-FOR base GNNs. benchmark datasets EVALUATE-FOR NGNN. OtherScientificTerm are neighboring node features, subtrees, and subtree. Method is Nested Graph Neural Networks ( NGNNs ). ","Graph neural network (GNN) for graph classification can be seen as an extension of 1-WL, where a node representation for a rooted subtree is learned from neighboring node features. In this paper, the authors propose Nested Graph Neural Networks (NGNNs), where the representation of a graph is decomposed into two subgraphs, one for each node in the subtree, and each subgraph is represented by a GNN. The representation of the graph is composed of two rooted subtrees, where each subtree corresponds to a node in a nontree graph, and the nodes in a subtree are represented as a subgraph of the original graph. The authors show that the representation can be decomposed as a combination of the two root-subgraph representations, and that the subgraph representations of the root subtrees can be used to learn a whole-graph representation.  The authors also show that NGNN can be applied to any r-regular graphs, where the root subgraph representation of each node is learned by the GNN and the node representation is learned via 1-WSL or a Gnn. The main contribution of the paper is the development of NGNN, which is a plug-and-play framework, where NGNN is used to represent a graph as a set of rooted subgraph, rather than a single subgraph. In particular, NGNN first learns a local subgraph from the root nodes, and then uses the local GNN to learn the sub graph representation for each sub-graph. The proposed NGNN outperforms GNNs in terms of time complexity, and is shown to be more efficient than GNN, and can also be used in combination with other GNN methods. The paper also shows that NGnn outperforms 1-wL and GNN on several benchmark datasets. ","Graph neural network (GNN) for graph classification can be seen as an extension of 1-WL, where a node representation for a rooted subtree is learned from neighboring node features. In this paper, the authors propose Nested Graph Neural Networks (NGNNs), where the representation of a graph is decomposed into two subgraphs, one for each node in the subtree, and each subgraph is represented by a GNN. The representation of the graph is composed of two rooted subtrees, where each subtree corresponds to a node in a nontree graph, and the nodes in a subtree are represented as a subgraph of the original graph. The authors show that the representation can be decomposed as a combination of the two root-subgraph representations, and that the subgraph representations of the root subtrees can be used to learn a whole-graph representation.  The authors also show that NGNN can be applied to any r-regular graphs, where the root subgraph representation of each node is learned by the GNN and the node representation is learned via 1-WSL or a Gnn. The main contribution of the paper is the development of NGNN, which is a plug-and-play framework, where NGNN is used to represent a graph as a set of rooted subgraph, rather than a single subgraph. In particular, NGNN first learns a local subgraph from the root nodes, and then uses the local GNN to learn the sub graph representation for each sub-graph. The proposed NGNN outperforms GNNs in terms of time complexity, and is shown to be more efficient than GNN, and can also be used in combination with other GNN methods. The paper also shows that NGnn outperforms 1-wL and GNN on several benchmark datasets. "
11651,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"nesting FEATURE-OF forward or reverse KL divergence. NVI USED-FOR importance sampling strategies. heuristics USED-FOR sampler. NVI USED-FOR intermediate densities. NVI USED-FOR multimodal distribution. amortized inference USED-FOR hierarchical deep generative models. heuristics USED-FOR amortized inference. heuristics USED-FOR hidden Markov model. learned annealing path USED-FOR NVI. log average weight CONJUNCTION effective sample size. effective sample size CONJUNCTION log average weight. log average weight FEATURE-OF sample quality. effective sample size FEATURE-OF sample quality. sample quality EVALUATE-FOR nested objectives. Method are nested variational inference ( NVI ), and nested importance samplers. ","This paper proposes nested variational inference (NVI), which is a generalization of NVI to importance sampling strategies. The key idea is that the forward or reverse KL divergence between the true distribution and that of the nested distribution is related to the importance of the sampler in the nested. The authors show that nested importance samplers are amortized in the sense that they can be used to learn a multimodal distribution. They also show that NVI is able to learn the intermediate densities of the multi-modal distribution through a learned annealing path. Finally, they show that heuristics can be applied to train a sampler by learning a hidden Markov model using NVI.    The authors also show how to use nested objectives to improve the sample quality in terms of the log average weight and effective sample size of the sampled samples. They demonstrate that nested objectives can improve sample quality for hierarchical deep generative models via amortization of the learned heuristic in nested inference, and they also demonstrate how to apply nested objectives in the case of nested importance sampling. ","This paper proposes nested variational inference (NVI), which is a generalization of NVI to importance sampling strategies. The key idea is that the forward or reverse KL divergence between the true distribution and that of the nested distribution is related to the importance of the sampler in the nested. The authors show that nested importance samplers are amortized in the sense that they can be used to learn a multimodal distribution. They also show that NVI is able to learn the intermediate densities of the multi-modal distribution through a learned annealing path. Finally, they show that heuristics can be applied to train a sampler by learning a hidden Markov model using NVI.    The authors also show how to use nested objectives to improve the sample quality in terms of the log average weight and effective sample size of the sampled samples. They demonstrate that nested objectives can improve sample quality for hierarchical deep generative models via amortization of the learned heuristic in nested inference, and they also demonstrate how to apply nested objectives in the case of nested importance sampling. "
11676,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"packing bound USED-FOR Piyavskii - Shubert algorithm. packing bound USED-FOR upper bound. local worst - case analysis USED-FOR learning tasks. instance - dependent lower bound COMPARE worst - case lower bounds. worst - case lower bounds COMPARE instance - dependent lower bound. Lipschitz setting FEATURE-OF worst - case lower bounds. local worst - case analysis USED-FOR instance - dependent lower bound. OtherScientificTerm is Lipschitz function f. Metric is optimal sample complexity. Method are computationally tractable DOO algorithm, and packing and integral bounds. ","This paper provides a packing bound for the Piyavskipperi-shubert algorithm for the Lipschitz setting. The authors show that the optimal sample complexity of the algorithm is $O(1/\sqrt{n})$, where $n$ is the number of samples and $N$ is a function of $N$. The upper bound is based on the packing bound of Shubert et al. (2018) and the lower bound is a lower bound based on a local worst-case analysis for learning tasks. In particular, the authors provide an instance-dependent lower bound that matches the best known best-case lower bounds for the worst case of the Pii-Shubert-Piyavskyi algorithm in the Lapsing setting. They also provide a computationally tractable DOO algorithm based on their packing and integral bounds.","This paper provides a packing bound for the Piyavskipperi-shubert algorithm for the Lipschitz setting. The authors show that the optimal sample complexity of the algorithm is $O(1/\sqrt{n})$, where $n$ is the number of samples and $N$ is a function of $N$. The upper bound is based on the packing bound of Shubert et al. (2018) and the lower bound is a lower bound based on a local worst-case analysis for learning tasks. In particular, the authors provide an instance-dependent lower bound that matches the best known best-case lower bounds for the worst case of the Pii-Shubert-Piyavskyi algorithm in the Lapsing setting. They also provide a computationally tractable DOO algorithm based on their packing and integral bounds."
11701,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"Deep neural networks ( DNNs ) USED-FOR tasks. Credible uncertainty estimation USED-FOR risk - sensitive applications. network USED-FOR uncertainty estimation. attack COMPARE adversarial attacks. adversarial attacks COMPARE attack. white - box setting USED-FOR scenario. Deep Ensembles CONJUNCTION MC - Dropout. MC - Dropout CONJUNCTION Deep Ensembles. vanilla softmax score CONJUNCTION Deep Ensembles. Deep Ensembles CONJUNCTION vanilla softmax score. uncertainty estimation methods USED-FOR attacks. vanilla softmax score HYPONYM-OF uncertainty estimation methods. MC - Dropout HYPONYM-OF uncertainty estimation methods. Deep Ensembles HYPONYM-OF uncertainty estimation methods. SelectiveNet CONJUNCTION selective classification architecture. selective classification architecture CONJUNCTION SelectiveNet. SelectiveNet USED-FOR attack. MobileNetV2 CONJUNCTION EfficientNetB0. EfficientNetB0 CONJUNCTION MobileNetV2. architectures EVALUATE-FOR attack. EfficientNetB0 HYPONYM-OF architectures. MobileNetV2 HYPONYM-OF architectures. OtherScientificTerm are network ’s capacity, and uncertainty estimation damage. Method is DNN. Material are black - box regime, and ImageNet. Task is uncertainty estimations. ","This paper studies the problem of adversarial attacks on deep neural networks (DNNs) for tasks where the network’s capacity is limited. Credible uncertainty estimation is important for risk-sensitive applications, but it is not well-studied in the black-box regime. In this scenario, the authors consider a white-box setting where the uncertainty estimation of the network is not available. They show that an attack on a DNN can be more powerful than adversarial examples. They also show that the attack can be applied to any uncertainty estimation methods (vanilla softmax score, Deep Ensembles, MC-Dropout, etc.).   The authors show that this attack is more effective than previous attacks on the same type of DNNs. The attack is based on the fact that the uncertainty estimations of the DNN are not available in this case. The authors further show that attacks are more effective when the network can be trained to be robust to uncertainty estimation damage.   They evaluate the attack on three different architectures: MobileNetV2, EfficientNetB0, and SelectiveNet (a selective classification architecture). They also evaluate their attack on ImageNet. ","This paper studies the problem of adversarial attacks on deep neural networks (DNNs) for tasks where the network’s capacity is limited. Credible uncertainty estimation is important for risk-sensitive applications, but it is not well-studied in the black-box regime. In this scenario, the authors consider a white-box setting where the uncertainty estimation of the network is not available. They show that an attack on a DNN can be more powerful than adversarial examples. They also show that the attack can be applied to any uncertainty estimation methods (vanilla softmax score, Deep Ensembles, MC-Dropout, etc.).   The authors show that this attack is more effective than previous attacks on the same type of DNNs. The attack is based on the fact that the uncertainty estimations of the DNN are not available in this case. The authors further show that attacks are more effective when the network can be trained to be robust to uncertainty estimation damage.   They evaluate the attack on three different architectures: MobileNetV2, EfficientNetB0, and SelectiveNet (a selective classification architecture). They also evaluate their attack on ImageNet. "
11726,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"stochastic block models USED-FOR community detection. network information USED-FOR they. model USED-FOR networks. Task are community detection problem, and real - world applications. OtherScientificTerm are network, well - connected ‘ communities ’, and network structure. Method are detection algorithm, voting approaches, streaming stochastic block model ( StSBM ), voting algorithms, and streaming belief - propagation ( STREAMBP ) approach. Material is synthetic and real data. ","This paper studies the community detection problem, where the goal is to detect whether a network is a ‘community’ or not. The authors consider the setting where well-connected ‘communities’ are known to exist in the network, and the detection algorithm is based on voting approaches. In particular, they consider stochastic block models for community detection, and they use the network information to train a voting algorithm.    The authors propose a streaming stochedastic block model (StSBM) and show that voting algorithms can be used to select a subset of ‘members’ of a community based on the network structure. They also show that a streaming belief-propagation (STREAMBP) approach can be applied to train the model to select networks that are most likely to belong to a particular community.  The paper is well-written and well-motivated, and is clearly written. The results are well-supported by experiments on both synthetic and real data, and are applicable to real-world applications. ","This paper studies the community detection problem, where the goal is to detect whether a network is a ‘community’ or not. The authors consider the setting where well-connected ‘communities’ are known to exist in the network, and the detection algorithm is based on voting approaches. In particular, they consider stochastic block models for community detection, and they use the network information to train a voting algorithm.    The authors propose a streaming stochedastic block model (StSBM) and show that voting algorithms can be used to select a subset of ‘members’ of a community based on the network structure. They also show that a streaming belief-propagation (STREAMBP) approach can be applied to train the model to select networks that are most likely to belong to a particular community.  The paper is well-written and well-motivated, and is clearly written. The results are well-supported by experiments on both synthetic and real data, and are applicable to real-world applications. "
11751,SP:b1163857a6b06047c3531ab762642fcbed6dd294,predictor space FEATURE-OF regularization cost. l2 regularization USED-FOR regularization cost. l2 regularization USED-FOR predictor space. linear neural networks USED-FOR parameterizations of linear predictors. sparse linear ConvNets CONJUNCTION residual networks. residual networks CONJUNCTION sparse linear ConvNets. representation cost FEATURE-OF sparse linear ConvNets. representation cost FEATURE-OF residual networks. lp quasi - norms CONJUNCTION group quasi - norms. group quasi - norms CONJUNCTION lp quasi - norms. architecture CONJUNCTION parameterization. parameterization CONJUNCTION architecture. group quasi - norms CONJUNCTION k - support - norm. k - support - norm CONJUNCTION group quasi - norms. k - support - norm CONJUNCTION elastic net. elastic net CONJUNCTION k - support - norm. parameterization USED-FOR representation cost. regularizers USED-FOR linear predictors. architecture USED-FOR representation cost. group quasi - norms CONJUNCTION elastic net. elastic net CONJUNCTION group quasi - norms. l2 regularization USED-FOR representation cost. elastic net HYPONYM-OF regularizers. k - support - norm HYPONYM-OF regularizers. group quasi - norms HYPONYM-OF regularizers. lp quasi - norms HYPONYM-OF regularizers. Method is parameterizations. Task is reverse problem. ,"This paper studies the regularization cost in the predictor space of linear neural networks with l2 regularization. The authors show that the parameterizations of linear predictors can be approximated by linear neural network, and that the regularizations of these parameterizations can be decomposed into two parts: (1) sparse linear ConvNets and (2) residual networks with low representation cost. The paper also shows that the representation cost of sparse linear convNets is related to the number of layers in the residual networks.   The paper further shows that regularizers such as lp quasi-norms, group quasi-Norms, k-support-norm, and elastic net can be used as regularizers to improve the performance of linear predictions. The main contribution of the paper is that the paper shows that any architecture, parameterization, and the choice of regularizer can be reduced to a lower representation cost by l2-regularization. This paper also provides a theoretical analysis of the reverse problem, showing that the choice for the architecture and the parameterization of the regularizer is critical to the reduction in representation cost due to the use of regularizers.","This paper studies the regularization cost in the predictor space of linear neural networks with l2 regularization. The authors show that the parameterizations of linear predictors can be approximated by linear neural network, and that the regularizations of these parameterizations can be decomposed into two parts: (1) sparse linear ConvNets and (2) residual networks with low representation cost. The paper also shows that the representation cost of sparse linear convNets is related to the number of layers in the residual networks.   The paper further shows that regularizers such as lp quasi-norms, group quasi-Norms, k-support-norm, and elastic net can be used as regularizers to improve the performance of linear predictions. The main contribution of the paper is that the paper shows that any architecture, parameterization, and the choice of regularizer can be reduced to a lower representation cost by l2-regularization. This paper also provides a theoretical analysis of the reverse problem, showing that the choice for the architecture and the parameterization of the regularizer is critical to the reduction in representation cost due to the use of regularizers."
11776,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"approach USED-FOR knowledge graphs ( KGs ). approach COMPARE case - based reasoning. case - based reasoning COMPARE approach. case - based reasoning USED-FOR artificial intelligence ( AI ). non - parametric approach USED-FOR crisp logical rules. graph path patterns USED-FOR non - parametric approach. NELL-995 CONJUNCTION FB-122. FB-122 CONJUNCTION NELL-995. accuracy EVALUATE-FOR method. NELL-995 EVALUATE-FOR models. FB-122 EVALUATE-FOR models. model USED-FOR low data settings. OtherScientificTerm are binary relation, and relation. ","This paper proposes a novel approach to learning knowledge graphs (KGs) that can be seen as a binary relation between two entities. The approach is similar to case-based reasoning in artificial intelligence (AI) but uses a non-parametric approach to learn crisp logical rules based on graph path patterns. The proposed method achieves state-of-the-art accuracy on two datasets NELL-995 and FB-122, outperforming existing models. The authors also show that the proposed model can be applied to low data settings. ","This paper proposes a novel approach to learning knowledge graphs (KGs) that can be seen as a binary relation between two entities. The approach is similar to case-based reasoning in artificial intelligence (AI) but uses a non-parametric approach to learn crisp logical rules based on graph path patterns. The proposed method achieves state-of-the-art accuracy on two datasets NELL-995 and FB-122, outperforming existing models. The authors also show that the proposed model can be applied to low data settings. "
11780,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,Transformer architecture PART-OF entity linking model. CoNLL CONJUNCTION TAC - KBP. TAC - KBP CONJUNCTION CoNLL. entity linking datasets EVALUATE-FOR model. Transformer architecture CONJUNCTION input perturbations. input perturbations CONJUNCTION Transformer architecture. negative entity candidates CONJUNCTION Transformer architecture. Transformer architecture CONJUNCTION negative entity candidates. end - to - end entity linking CONJUNCTION entity linking. entity linking CONJUNCTION end - to - end entity linking. entity linking HYPONYM-OF settings. end - to - end entity linking HYPONYM-OF settings. in - domain training data USED-FOR settings. Material is Wikipedia links. ,"This paper proposes a new entity linking model that incorporates the Transformer architecture into the training process. The proposed model is evaluated on two standard entity linking datasets, CoNLL and TAC-KBP. The authors show that the proposed model outperforms the state-of-the-art on both datasets. The paper also shows that the model can be trained on both negative entity candidates and input perturbations.   The paper is well-written and well-motivated, and the experiments are well-designed. The experiments are conducted on two settings: end-to-end entity linking and entity linking with in-domain training data, and on Wikipedia links. ","This paper proposes a new entity linking model that incorporates the Transformer architecture into the training process. The proposed model is evaluated on two standard entity linking datasets, CoNLL and TAC-KBP. The authors show that the proposed model outperforms the state-of-the-art on both datasets. The paper also shows that the model can be trained on both negative entity candidates and input perturbations.   The paper is well-written and well-motivated, and the experiments are well-designed. The experiments are conducted on two settings: end-to-end entity linking and entity linking with in-domain training data, and on Wikipedia links. "
11784,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"ensemble Active Learning methods USED-FOR acquisition. intermediate training checkpoints USED-FOR ensembles. training data subset search USED-FOR large labeled datasets. acquisition functions CONJUNCTION ensemble configurations. ensemble configurations CONJUNCTION acquisition functions. initialization schemes CONJUNCTION acquisition functions. acquisition functions CONJUNCTION initialization schemes. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. ImageNet HYPONYM-OF image classification benchmarks. CIFAR-10 HYPONYM-OF image classification benchmarks. CIFAR-100 HYPONYM-OF image classification benchmarks. ResNet-101 CONJUNCTION DenseNet121. DenseNet121 CONJUNCTION ResNet-101. data subsets USED-FOR deep models. ResNet-18 ensemble USED-FOR data subsets. DenseNet121 HYPONYM-OF deep models. ResNet-101 HYPONYM-OF deep models. training data distribution USED-FOR large scale vision tasks. Method are Deep Neural Networks ( DNNs ), and DNNs. Generic are datasets, they, approach, and dataset. Task is DNN ’s optimization. OtherScientificTerm is training distribution. Metric is training time. ","This paper studies the problem of training Deep Neural Networks (DNNs) on large datasets. The authors propose a new approach, called ensemble Active Learning methods, to accelerate the acquisition of ensembles at intermediate training checkpoints. The key idea is to use training data subset search for large labeled datasets, where the training distribution of the training data is not available. The approach is based on the observation that DNNs can be trained on a large number of subsets of training data, which can be used to speed up the DNN’s optimization.  The authors evaluate the performance of the proposed approach on image classification benchmarks such as CIFAR-10, CIFar-100, and ImageNet. They evaluate different initialization schemes, acquisition functions, and ensemble configurations. They show that deep models trained with data subsets from a ResNet-18 ensemble are able to achieve state-of-the-art performance. They also show that training data distribution for large scale vision tasks can be learned from a single training data set, and that training time can be significantly reduced. ","This paper studies the problem of training Deep Neural Networks (DNNs) on large datasets. The authors propose a new approach, called ensemble Active Learning methods, to accelerate the acquisition of ensembles at intermediate training checkpoints. The key idea is to use training data subset search for large labeled datasets, where the training distribution of the training data is not available. The approach is based on the observation that DNNs can be trained on a large number of subsets of training data, which can be used to speed up the DNN’s optimization.  The authors evaluate the performance of the proposed approach on image classification benchmarks such as CIFAR-10, CIFar-100, and ImageNet. They evaluate different initialization schemes, acquisition functions, and ensemble configurations. They show that deep models trained with data subsets from a ResNet-18 ensemble are able to achieve state-of-the-art performance. They also show that training data distribution for large scale vision tasks can be learned from a single training data set, and that training time can be significantly reduced. "
11788,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"routing algorithm USED-FOR capsule networks. mechanism USED-FOR routing. sequential iterative routing CONJUNCTION concurrent iterative routing. concurrent iterative routing CONJUNCTION sequential iterative routing. inverted dot - product attention USED-FOR routing. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. routing algorithms COMPARE method. method COMPARE routing algorithms. method COMPARE CNN. CNN COMPARE method. it COMPARE CNN. CNN COMPARE it. method COMPARE it. it COMPARE method. CIFAR-10 HYPONYM-OF benchmark datasets. CIFAR-100 HYPONYM-OF benchmark datasets. benchmark datasets EVALUATE-FOR method. capsule model COMPARE CNNs. CNNs COMPARE capsule model. task EVALUATE-FOR CNNs. task EVALUATE-FOR capsule model. task EVALUATE-FOR recognizing digits. neurons per layer USED-FOR CNNs. capsule model USED-FOR recognizing digits. overlayed digit images USED-FOR task. overlayed digit images USED-FOR recognizing digits. capsule networks USED-FOR complex real - world tasks. Method are Layer Normalization, and Capsules - Inverted - Attention - Routing. OtherScientificTerm is normalization. ","This paper proposes a new routing algorithm for capsule networks. The key idea is to use Layer Normalization to normalize the weights of each layer of a capsule network, and then use this mechanism to improve the routing of the capsule network. Capsules-Inverted-Attention-Routing (Capsules-IATR) is based on the idea of inverted dot-product attention, which combines sequential iterative routing and concurrent iterative routed. The proposed method is evaluated on two benchmark datasets (CIFAR-10 and CIFAR100) and compared with other routing algorithms. The authors show that the proposed capsule model outperforms CNNs on the task of recognizing digits from overlayed digit images. They also show that capsule networks can be used to solve complex real-world tasks, and that it outperforms a standard CNN even when the number of neurons per layer is limited.","This paper proposes a new routing algorithm for capsule networks. The key idea is to use Layer Normalization to normalize the weights of each layer of a capsule network, and then use this mechanism to improve the routing of the capsule network. Capsules-Inverted-Attention-Routing (Capsules-IATR) is based on the idea of inverted dot-product attention, which combines sequential iterative routing and concurrent iterative routed. The proposed method is evaluated on two benchmark datasets (CIFAR-10 and CIFAR100) and compared with other routing algorithms. The authors show that the proposed capsule model outperforms CNNs on the task of recognizing digits from overlayed digit images. They also show that capsule networks can be used to solve complex real-world tasks, and that it outperforms a standard CNN even when the number of neurons per layer is limited."
11792,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"statistical physics FEATURE-OF parallel tempering technique. history PART-OF joint hyperparameter / model - parameter space. method USED-FOR dropout and learning rate optimization. Task are Hyperparameter optimization, and training of deep architectures. Method is deep architectures. Generic are methods, and model. OtherScientificTerm are hyperparameter space, nonlocal paths, hyperparameters, correlated noise, temperature, and overfitting. Metric are computational cost, resistance, and absolute validation error. ","Hyperparameter optimization is an important problem in deep architectures, but existing methods are computationally expensive. This paper proposes a parallel tempering technique inspired by statistical physics, where the history of the joint hyperparameter/model-parameter space is shared across all layers of the network. The authors show that existing methods do not consider nonlocal paths between hyperparameters, which can be problematic for training of deep architectures due to the computational cost and resistance to correlated noise. To address this issue, the authors propose a new method, which is based on the observation that the temperature of the model is a function of the number of times the model has been trained in the past. The proposed method is applied to both dropout and learning rate optimization, and is shown to achieve a resistance that matches the absolute validation error. The paper also shows that the proposed method does not suffer from overfitting or over-fitting. ","Hyperparameter optimization is an important problem in deep architectures, but existing methods are computationally expensive. This paper proposes a parallel tempering technique inspired by statistical physics, where the history of the joint hyperparameter/model-parameter space is shared across all layers of the network. The authors show that existing methods do not consider nonlocal paths between hyperparameters, which can be problematic for training of deep architectures due to the computational cost and resistance to correlated noise. To address this issue, the authors propose a new method, which is based on the observation that the temperature of the model is a function of the number of times the model has been trained in the past. The proposed method is applied to both dropout and learning rate optimization, and is shown to achieve a resistance that matches the absolute validation error. The paper also shows that the proposed method does not suffer from overfitting or over-fitting. "
11796,SP:beba754d96cc441712a5413c41e98863c8abf605,"Minimum Risk Training ( MRT ) CONJUNCTION Generative Adversarial Networks ( GAN ). Generative Adversarial Networks ( GAN ) CONJUNCTION Minimum Risk Training ( MRT ). Reinforcement learning ( RL ) USED-FOR text generation tasks. machine translation ( MT ) HYPONYM-OF text generation tasks. methods USED-FOR MT. RL methods USED-FOR MT. OtherScientificTerm are expected reward, pre - trained parameters, translation, training signal, and distribution curve. ","This paper studies the use of Reinforcement learning (RL) for text generation tasks such as machine translation (MT), where the expected reward is a function of the pre-trained parameters. The authors combine the ideas of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN) to combine the benefits of both of these methods for MT. They show that RL methods can improve the performance of MT using RL methods, especially when the target task is a translation task. They also show that the performance improves when the training signal is noisy (i.e. when the distribution curve is skewed). ","This paper studies the use of Reinforcement learning (RL) for text generation tasks such as machine translation (MT), where the expected reward is a function of the pre-trained parameters. The authors combine the ideas of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN) to combine the benefits of both of these methods for MT. They show that RL methods can improve the performance of MT using RL methods, especially when the target task is a translation task. They also show that the performance improves when the training signal is noisy (i.e. when the distribution curve is skewed). "
11800,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,reinforcement learning algorithms CONJUNCTION applications. applications CONJUNCTION reinforcement learning algorithms. closed - form characterizations FEATURE-OF asymptotic variances. closed - form characterizations USED-FOR Q - value estimates. policies USED-FOR estimation errors. confidence regions USED-FOR Q - value and optimal value functions. exploration strategy COMPARE benchmark approaches. benchmark approaches COMPARE exploration strategy. Task is statistical inference. Method is policy exploration strategy. OtherScientificTerm is Q estimates. ,This paper considers the problem of statistical inference in the context of reinforcement learning algorithms and applications. The authors propose a policy exploration strategy where the Q-value estimates are based on closed-form characterizations of the asymptotic variances of the policy’s Q-values. They show that the estimation errors of policies trained with these policies can be reduced to zero when the Q estimates are close to the optimal value function. They also show that this exploration strategy is more effective than standard benchmark approaches when the confidence regions of the Q -value and optimal value functions are sufficiently large. ,This paper considers the problem of statistical inference in the context of reinforcement learning algorithms and applications. The authors propose a policy exploration strategy where the Q-value estimates are based on closed-form characterizations of the asymptotic variances of the policy’s Q-values. They show that the estimation errors of policies trained with these policies can be reduced to zero when the Q estimates are close to the optimal value function. They also show that this exploration strategy is more effective than standard benchmark approaches when the confidence regions of the Q -value and optimal value functions are sufficiently large. 
11804,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"Top - k recommendation USED-FOR largescale recommender systems. Cold - start and efficiency issues USED-FOR largescale recommender systems. Cold - start and efficiency issues FEATURE-OF Top - k recommendation. hybrid recommendation methods USED-FOR cold - start issues. efficiency EVALUATE-FOR online recommendation. online recommendation EVALUATE-FOR they. real latent space FEATURE-OF similarity search. cold - start items CONJUNCTION warm - start ones. warm - start ones CONJUNCTION cold - start items. cold - start users CONJUNCTION cold - start items. cold - start items CONJUNCTION cold - start users. collaborative generated hashing ( CGH ) USED-FOR efficiency. it USED-FOR recommendation settings. CGH USED-FOR hash functions. Minimum Description Length ( MDL ) principle USED-FOR CGH. Minimum Description Length ( MDL ) principle USED-FOR hash functions. CGH USED-FOR marketing strategy. generative step USED-FOR CGH. MDL principle USED-FOR compact and informative binary codes. content data USED-FOR compact and informative binary codes. recommendations COMPARE baselines. baselines COMPARE recommendations. public datasets EVALUATE-FOR recommendations. application USED-FOR marketing. public datasets EVALUATE-FOR baselines. OtherScientificTerm are side information, and binary codes. ","Cold-start and efficiency issues for largescale recommender systems have been observed for Top-k recommendation, and hybrid recommendation methods have been proposed to address cold-start issues. However, they have not been shown to improve efficiency for online recommendation.  This paper proposes collaborative generated hashing (CGH) to address the issues of efficiency and compactness of the binary codes. The idea is that the similarity search in the real latent space can be improved by taking into account side information, and that it can be applied to all recommendation settings. CGH is based on the Minimum Description Length (MDL) principle for hash functions, which is used to generate compact and informative binary codes based on content data. The authors show that CGH can be used as a marketing strategy, and can be combined with any generative step to improve the performance. The paper also shows that the proposed recommendations outperform the baselines on public datasets, and the application to marketing is interesting. ","Cold-start and efficiency issues for largescale recommender systems have been observed for Top-k recommendation, and hybrid recommendation methods have been proposed to address cold-start issues. However, they have not been shown to improve efficiency for online recommendation.  This paper proposes collaborative generated hashing (CGH) to address the issues of efficiency and compactness of the binary codes. The idea is that the similarity search in the real latent space can be improved by taking into account side information, and that it can be applied to all recommendation settings. CGH is based on the Minimum Description Length (MDL) principle for hash functions, which is used to generate compact and informative binary codes based on content data. The authors show that CGH can be used as a marketing strategy, and can be combined with any generative step to improve the performance. The paper also shows that the proposed recommendations outperform the baselines on public datasets, and the application to marketing is interesting. "
11808,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"adversarial domain adaptation CONJUNCTION multi - task learning. multi - task learning CONJUNCTION adversarial domain adaptation. adversarial domain adaptation USED-FOR AITL. multi - task learning USED-FOR AITL. genomic information USED-FOR drug response. large pre - clinical pharmacogenomics datasets CONJUNCTION clinical datasets. clinical datasets CONJUNCTION large pre - clinical pharmacogenomics datasets. transfer learning USED-FOR large pre - clinical pharmacogenomics datasets. drug response outcome FEATURE-OF clinical data. cancer cell lines HYPONYM-OF large pre - clinical pharmacogenomics datasets. AITL USED-FOR input and output discrepancies. adversarial inductive transfer learning method USED-FOR input and output discrepancies. AITL HYPONYM-OF adversarial inductive transfer learning method. AITL USED-FOR precision oncology. AITL COMPARE pharmacogenomics and transfer learning baselines. pharmacogenomics and transfer learning baselines COMPARE AITL. Method is Adversarial Inductive Transfer Learning ( AITL ). Generic is method. OtherScientificTerm are input and output spaces, and output space. Task is pharmacogenomics. Material is pre - clinical and clinical datasets. ","This paper proposes Adversarial Inductive Transfer Learning (AITL), a method that learns to transfer between input and output spaces. AITL is a combination of adversarial domain adaptation and multi-task learning, where the goal is to transfer genomic information about a drug response from one domain to another. The authors consider large pre-clinical pharmacogenomics datasets (i.e., cancer cell lines) and clinical datasets for transfer learning (drug response outcome in clinical data). They show that the adversarial inductive transfer learning method is able to learn to transfer information from input space to the output space. They also show that AitL can learn to correct the input/output discrepancies between pre-clinically and clinical data, and can be used for precision oncology as well.    The authors also provide a theoretical analysis of the performance of the proposed method. The paper also shows that the performance is comparable to the state-of-the-art in both pharmacogenomic and transfer learning baselines. ","This paper proposes Adversarial Inductive Transfer Learning (AITL), a method that learns to transfer between input and output spaces. AITL is a combination of adversarial domain adaptation and multi-task learning, where the goal is to transfer genomic information about a drug response from one domain to another. The authors consider large pre-clinical pharmacogenomics datasets (i.e., cancer cell lines) and clinical datasets for transfer learning (drug response outcome in clinical data). They show that the adversarial inductive transfer learning method is able to learn to transfer information from input space to the output space. They also show that AitL can learn to correct the input/output discrepancies between pre-clinically and clinical data, and can be used for precision oncology as well.    The authors also provide a theoretical analysis of the performance of the proposed method. The paper also shows that the performance is comparable to the state-of-the-art in both pharmacogenomic and transfer learning baselines. "
11812,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,"sample complexity EVALUATE-FOR model - free approaches. fictitious trajectory rollouts USED-FOR dynamics model. fictitious trajectory rollouts USED-FOR model - free approaches. stochastic dynamics model USED-FOR uncertainty quantification. erroneously optimistic regions PART-OF dynamics model. uncertainty aware ensemble of dynamics models USED-FOR next state predictions. ensemble of dynamics models USED-FOR policy update. simulated robotic locomotion HYPONYM-OF benchmark tests. benchmark tests EVALUATE-FOR approach. approach COMPARE model - based one. model - based one COMPARE approach. approach COMPARE model - free algorithms. model - free algorithms COMPARE approach. model - free algorithms COMPARE model - based one. model - based one COMPARE model - free algorithms. learning rates CONJUNCTION asymptotic behaviour. asymptotic behaviour CONJUNCTION learning rates. asymptotic behaviour EVALUATE-FOR MBPGE. learning rates EVALUATE-FOR MBPGE. Method are RL methods, and policy gradient methods. OtherScientificTerm are next state prediction, and real and virtual total reward. ","This paper studies the sample complexity of model-free approaches that rely on fictitious trajectory rollouts to train a dynamics model. The authors propose to use a stochastic dynamics model for uncertainty quantification, and use RL methods to mitigate the issue of erroneously optimistic regions in the dynamics model, which is common in policy gradient methods. They propose an uncertainty aware ensemble of dynamics models to make next state predictions, and then use this ensemble to perform a policy update based on the next state prediction. The proposed approach is evaluated on several benchmark tests, including simulated robotic locomotion, where the authors show that the proposed approach outperforms the previous model-based one, and other model -free algorithms, in terms of learning rates and asymptotic behaviour. They also show that MBPGE achieves better learning rates with respect to the real and virtual total reward.","This paper studies the sample complexity of model-free approaches that rely on fictitious trajectory rollouts to train a dynamics model. The authors propose to use a stochastic dynamics model for uncertainty quantification, and use RL methods to mitigate the issue of erroneously optimistic regions in the dynamics model, which is common in policy gradient methods. They propose an uncertainty aware ensemble of dynamics models to make next state predictions, and then use this ensemble to perform a policy update based on the next state prediction. The proposed approach is evaluated on several benchmark tests, including simulated robotic locomotion, where the authors show that the proposed approach outperforms the previous model-based one, and other model -free algorithms, in terms of learning rates and asymptotic behaviour. They also show that MBPGE achieves better learning rates with respect to the real and virtual total reward."
11816,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"neural network models USED-FOR visual features. adversarial examples CONJUNCTION corrupted images. corrupted images CONJUNCTION adversarial examples. class - conditional reconstruction of the input USED-FOR adversarial examples. class - conditional reconstruction of the input USED-FOR corrupted images. misclassification CONJUNCTION reconstruction error. reconstruction error CONJUNCTION misclassification. Reconstructive Attack USED-FOR detection mechanism. reconstructive attack USED-FOR undetected adversarial examples. success rate EVALUATE-FOR reconstructive attack. CapsNets COMPARE convolutional networks. convolutional networks COMPARE CapsNets. adversarial examples USED-FOR CapsNets. visual similarity USED-FOR reconstructive attack. features USED-FOR CapsNets. Material is Adversarial examples. Method is class - conditional reconstruction. Generic is attacks. OtherScientificTerm are perturbations, and human perception. ",This paper studies the problem of adversarial attacks on convolutional neural networks. The authors propose a novel adversarial attack method called Reconstructive Attack (RA) that is based on class-conditional reconstruction of the input image. They show that the proposed method can be used to attack the classifier of a neural network. They also show that it can be applied to any classifier.   ,This paper studies the problem of adversarial attacks on convolutional neural networks. The authors propose a novel adversarial attack method called Reconstructive Attack (RA) that is based on class-conditional reconstruction of the input image. They show that the proposed method can be used to attack the classifier of a neural network. They also show that it can be applied to any classifier.   
11820,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"gradient descent USED-FOR parameter space. function space FEATURE-OF kernel gradient descent. gradient descent USED-FOR neural network. linear model USED-FOR wide networks. linear model USED-FOR neural network. full batch gradient descent USED-FOR neural network. Edge of Chaos HYPONYM-OF initialization. initialization CONJUNCTION activation function. activation function CONJUNCTION initialization. activation function USED-FOR NTK. initialization USED-FOR NTK. OtherScientificTerm are Neural Tangent Kernel ( NTK ), and network depth. ","This paper studies the Neural Tangent Kernel (NTK) of kernel gradient descent in the function space of a neural network. The authors show that gradient descent converges to the NTK of the parameter space of the neural network under a linear model for wide networks. They also show that under a certain initialization (e.g., Edge of Chaos) and a certain activation function, the neural networks trained with full batch gradient descent converge to a neural tangent kernel. The NTK depends on the initialization and activation function and the network depth.   ","This paper studies the Neural Tangent Kernel (NTK) of kernel gradient descent in the function space of a neural network. The authors show that gradient descent converges to the NTK of the parameter space of the neural network under a linear model for wide networks. They also show that under a certain initialization (e.g., Edge of Chaos) and a certain activation function, the neural networks trained with full batch gradient descent converge to a neural tangent kernel. The NTK depends on the initialization and activation function and the network depth.   "
11824,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"self - supervised method USED-FOR sentence representations. injection of linguistic knowledge USED-FOR self - supervised method. sentence structures USED-FOR semantic meaning. Multiple linguistic frameworks USED-FOR sentence structures. compositional words operations USED-FOR semantic meaning. embeddings USED-FOR semantic. linguistic views USED-FOR embeddings. OtherScientificTerm are linguist diversity, views, and sentence outward form. ","This paper proposes a self-supervised method for learning sentence representations based on injection of linguistic knowledge. Multiple linguistic frameworks are used to learn sentence structures that encode semantic meaning from compositional words operations. The authors show that embeddings learned from linguistic views of the semantic are more likely to capture the semantic meaning of the sentence, and that this is a result of linguist diversity across different views. The paper also shows that the learned embedding of a sentence in this way can capture the sentence outward form. ","This paper proposes a self-supervised method for learning sentence representations based on injection of linguistic knowledge. Multiple linguistic frameworks are used to learn sentence structures that encode semantic meaning from compositional words operations. The authors show that embeddings learned from linguistic views of the semantic are more likely to capture the semantic meaning of the sentence, and that this is a result of linguist diversity across different views. The paper also shows that the learned embedding of a sentence in this way can capture the sentence outward form. "
11828,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"methods USED-FOR niche domains. product or movie review datasets EVALUATE-FOR sentiment classification solutions. finance HYPONYM-OF niche domains. Transfer learning USED-FOR new domains. NLP transfer learning USED-FOR financial sentiment classification. FinBERT HYPONYM-OF language model. financial sentiment classification task EVALUATE-FOR language model. FinancialPhrasebank dataset USED-FOR financial sentiment classification task. BERT USED-FOR language model. OtherScientificTerm is domainspecific language. Generic is models. Material are labeled data, specific domain, and large training data sets. ","This paper proposes methods to transfer existing methods to niche domains (e.g. product or movie review datasets) from one domain to another using a domainspecific language. The authors argue that existing sentiment classification solutions are not transferable to new niche domains, e.g., finance. Transfer learning to new domains is challenging because models are trained on labeled data from a specific domain, and large training data sets are not available. To address this problem, the authors propose FinBERT, a language model based on BERT for financial sentiment classification on the FinancialPhrasebank dataset. NLP transfer learning is applied to the task of NLP sentiment classification.","This paper proposes methods to transfer existing methods to niche domains (e.g. product or movie review datasets) from one domain to another using a domainspecific language. The authors argue that existing sentiment classification solutions are not transferable to new niche domains, e.g., finance. Transfer learning to new domains is challenging because models are trained on labeled data from a specific domain, and large training data sets are not available. To address this problem, the authors propose FinBERT, a language model based on BERT for financial sentiment classification on the FinancialPhrasebank dataset. NLP transfer learning is applied to the task of NLP sentiment classification."
11832,SP:31c9c3a693922d5c3448e80ade920391dce261f9,musical scores CONJUNCTION text lyrics. text lyrics CONJUNCTION musical scores. Generative models USED-FOR singing voice. Generative models USED-FOR singing voice synthesis. singing voice synthesis USED-FOR singing voice waveforms. musical scores USED-FOR singing voice waveforms. text lyrics USED-FOR singing voice waveforms. pre - assigned scores CONJUNCTION lyrics. lyrics CONJUNCTION pre - assigned scores. pre - assigned scores USED-FOR singing voice generation. singing voice generation HYPONYM-OF alternative. training and inference time EVALUATE-FOR singing voice generation. pipeline USED-FOR tasks. source separation and transcription models USED-FOR data preparation. adversarial networks USED-FOR audio generation. source separation and transcription models CONJUNCTION adversarial networks. adversarial networks CONJUNCTION source separation and transcription models. adversarial networks CONJUNCTION metrics. metrics CONJUNCTION adversarial networks. Method is unconditioned or weakly conditioned singing voice generation schemes. ,"Generative models for singing voice synthesis have been proposed in the past. This paper proposes a new alternative, singing voice generation, where singing voice waveforms are generated from musical scores and text lyrics. The authors propose to use pre-assigned scores and/or lyrics as input to the generative models. They show that the proposed pipeline can be applied to a variety of tasks, and that the training and inference time is significantly faster than existing unconditioned or weakly conditioned singing voicegeneration schemes. They also show that adversarial networks for audio generation, source separation and transcription models for data preparation can be used to speed up the training. ","Generative models for singing voice synthesis have been proposed in the past. This paper proposes a new alternative, singing voice generation, where singing voice waveforms are generated from musical scores and text lyrics. The authors propose to use pre-assigned scores and/or lyrics as input to the generative models. They show that the proposed pipeline can be applied to a variety of tasks, and that the training and inference time is significantly faster than existing unconditioned or weakly conditioned singing voicegeneration schemes. They also show that adversarial networks for audio generation, source separation and transcription models for data preparation can be used to speed up the training. "
11836,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,computer vision CONJUNCTION audio - understanding. audio - understanding CONJUNCTION computer vision. deep neural networks USED-FOR computer vision. deep neural networks USED-FOR audio - understanding. adversarial attacks FEATURE-OF they. defensive tensorization HYPONYM-OF adversarial defence technique. latent high order factorization of the network USED-FOR adversarial defence technique. Randomization USED-FOR latent subspace. Randomization USED-FOR dense reconstructed weights. sparsity CONJUNCTION perturbations. perturbations CONJUNCTION sparsity. randomization USED-FOR perturbations. approach CONJUNCTION techniques. techniques CONJUNCTION approach. approach CONJUNCTION neural architecture. neural architecture CONJUNCTION approach. adversarial training HYPONYM-OF techniques. image classification benchmarks EVALUATE-FOR approach. audio classification task CONJUNCTION binary networks. binary networks CONJUNCTION audio classification task. binary networks EVALUATE-FOR approach. audio classification task EVALUATE-FOR approach. Generic is network. ,"This paper proposes a defense against adversarial attacks on deep neural networks for computer vision and audio-understanding. The authors propose a new adversarial defence technique, called defensive tensorization, based on the latent high order factorization of the network. Randomization is applied to the latent subspace of dense reconstructed weights. The idea is that sparsity and perturbations from randomization can be used to improve the robustness of the trained network. The proposed approach is evaluated on image classification benchmarks and on the audio classification task and binary networks. The approach is combined with existing techniques, such as adversarial training, as well as a new neural architecture.","This paper proposes a defense against adversarial attacks on deep neural networks for computer vision and audio-understanding. The authors propose a new adversarial defence technique, called defensive tensorization, based on the latent high order factorization of the network. Randomization is applied to the latent subspace of dense reconstructed weights. The idea is that sparsity and perturbations from randomization can be used to improve the robustness of the trained network. The proposed approach is evaluated on image classification benchmarks and on the audio classification task and binary networks. The approach is combined with existing techniques, such as adversarial training, as well as a new neural architecture."
11840,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,Deep learning based approaches USED-FOR urban spatiotemporal forecasting problems. graph attention network CONJUNCTION transformer. transformer CONJUNCTION graph attention network. clustered graph transformer framework USED-FOR unsmoothness issue. transformer PART-OF encoder - decoder architecture. transformer PART-OF clustered graph transformer framework. graph attention network PART-OF clustered graph transformer framework. structural components USED-FOR architectures. structural components USED-FOR deep learning models. architectures PART-OF deep learning models. gradient - based clustering method USED-FOR feature extractors. gradient - based clustering method USED-FOR spatial domain. multi - view position encoding USED-FOR periodicity and closeness of urban time series data. multi - view position encoding USED-FOR temporal domain. real datasets EVALUATE-FOR method. real datasets EVALUATE-FOR baselines. method COMPARE baselines. baselines COMPARE method. ride - hailing business FEATURE-OF real datasets. OtherScientificTerm is unsmoothness issue of urban data. Material is urban data. ,"This paper proposes a clustered graph transformer framework to address the unsmoothness issue of urban spatiotemporal forecasting problems. Deep learning based approaches to solve the problem have been recently shown to be effective for many applications, but there is a lack of work on the unssmooths issue in the context of urban data. This paper proposes to use structural components from existing deep learning models to improve the performance of existing architectures. Specifically, the proposed clustered graph transformers (CGT) consists of a graph attention network and a transformer in an encoder-decoder architecture. In the spatial domain, a gradient-based clustering method is used to train feature extractors, and a multi-view position encoding is used in the temporal domain to capture the periodicity and closeness of urban time series data. Experiments on two real datasets from the ride-hailing business show that the proposed method outperforms several baselines.","This paper proposes a clustered graph transformer framework to address the unsmoothness issue of urban spatiotemporal forecasting problems. Deep learning based approaches to solve the problem have been recently shown to be effective for many applications, but there is a lack of work on the unssmooths issue in the context of urban data. This paper proposes to use structural components from existing deep learning models to improve the performance of existing architectures. Specifically, the proposed clustered graph transformers (CGT) consists of a graph attention network and a transformer in an encoder-decoder architecture. In the spatial domain, a gradient-based clustering method is used to train feature extractors, and a multi-view position encoding is used in the temporal domain to capture the periodicity and closeness of urban time series data. Experiments on two real datasets from the ride-hailing business show that the proposed method outperforms several baselines."
11844,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. deep neural networks USED-FOR fitted Q iteration ( FQI ) algorithm. algorithmic and statistical rates of convergence FEATURE-OF action - value functions. action - value functions FEATURE-OF iterative policy sequence. FQI USED-FOR iterative policy sequence. geometric rate FEATURE-OF algorithmic error. deep neural network USED-FOR action - value function. experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. Minimax - DQN algorithm USED-FOR zero - sum Markov game. DQN USED-FOR Minimax - DQN algorithm. Method are deep reinforcement learning, and deep Q - network ( DQN ) algorithm. OtherScientificTerm is statistical error. ","This paper studies deep reinforcement learning and proposes a new deep Q-network (DQN) algorithm. DQN is an extension of the fitted Q iteration (FQI) algorithm, which uses deep neural networks to learn the action-value function of a deep neural network. The authors show that DQI converges to an iterative policy sequence under FQI with algorithmic and statistical rates of convergence of the action -value functions. The algorithmic error has a geometric rate of $O(1/\sqrt{T})$ and the statistical error is $O(\sqrt{\log T})$.  The authors also show that the Minimax-dQN algorithm can be used to solve a zero-sum Markov game with experience replay and a target network.   The main contribution of this paper is that the authors show the convergence of DQM to a state-of-the-art statistical error of $\Omega(\log T)$ when the experience replay is combined with the target network in order to achieve the convergence rate.  The paper also shows that the min-max-min-max algorithm can also be used with DQn to solve the Min-max Markov Game.","This paper studies deep reinforcement learning and proposes a new deep Q-network (DQN) algorithm. DQN is an extension of the fitted Q iteration (FQI) algorithm, which uses deep neural networks to learn the action-value function of a deep neural network. The authors show that DQI converges to an iterative policy sequence under FQI with algorithmic and statistical rates of convergence of the action -value functions. The algorithmic error has a geometric rate of $O(1/\sqrt{T})$ and the statistical error is $O(\sqrt{\log T})$.  The authors also show that the Minimax-dQN algorithm can be used to solve a zero-sum Markov game with experience replay and a target network.   The main contribution of this paper is that the authors show the convergence of DQM to a state-of-the-art statistical error of $\Omega(\log T)$ when the experience replay is combined with the target network in order to achieve the convergence rate.  The paper also shows that the min-max-min-max algorithm can also be used with DQn to solve the Min-max Markov Game."
11848,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"weighted sum USED-FOR semantics. attention mechanism USED-FOR natural language. PhraseTransformer HYPONYM-OF attention architecture. second phase USED-FOR inductive bias. WMT16 English - German translation task EVALUATE-FOR Transformer. WMT16 English - German translation task EVALUATE-FOR PhraseTransformer. BLEU EVALUATE-FOR Transformer. BLEU EVALUATE-FOR PhraseTransformer. Transformer COMPARE PhraseTransformer. PhraseTransformer COMPARE Transformer. WMT16 English - German translation task EVALUATE-FOR BLEU. OtherScientificTerm are self - attention, word compositions, compositional attentions, hypernodes, and non - linearity. Task is attention. Generic is first phase. Method is non - linear attention. ","This paper proposes a new attention architecture called PhraseTransformer, which is an extension of self-attention. The key idea is that the attention mechanism in natural language can be seen as a weighted sum that captures the semantics of the input word, and the semantics can be decomposed into two phases. The first phase, called compositional attention, maps word compositions to compositional attentions. The second phase is designed to capture the inductive bias of the first phase. The authors show that the non-linearity of the attention can be explained by the fact that the hypernodes of the second phase are not linearized. The paper also shows that the BLEU of Transformer trained on the WMT16 English-German translation task is comparable to that of the original Transformer, while the performance of Phrasetransformer is significantly better.","This paper proposes a new attention architecture called PhraseTransformer, which is an extension of self-attention. The key idea is that the attention mechanism in natural language can be seen as a weighted sum that captures the semantics of the input word, and the semantics can be decomposed into two phases. The first phase, called compositional attention, maps word compositions to compositional attentions. The second phase is designed to capture the inductive bias of the first phase. The authors show that the non-linearity of the attention can be explained by the fact that the hypernodes of the second phase are not linearized. The paper also shows that the BLEU of Transformer trained on the WMT16 English-German translation task is comparable to that of the original Transformer, while the performance of Phrasetransformer is significantly better."
11852,SP:622b0593972296a95b630a4ece1e959b60fec56c,"modular neural network architecture MAIN USED-FOR algorithms. input - output examples USED-FOR algorithms. neural controller USED-FOR variable - length input tape. neural controller PART-OF MAIN. general domain - agnostic mechanism USED-FOR selection of modules. general domain - agnostic mechanism USED-FOR MAIN. input tape layout CONJUNCTION parallel history tape. parallel history tape CONJUNCTION input tape layout. parallel history tape USED-FOR It. input tape layout USED-FOR It. memoryless controller USED-FOR it. input - output examples USED-FOR reinforcement learning. reinforcement learning USED-FOR MAIN architecture. it USED-FOR policies. algorithmic tasks EVALUATE-FOR MAIN. OtherScientificTerm are modules, random access, and tape locations. ","This paper proposes a modular neural network architecture MAIN for learning algorithms with input-output examples. MAIN consists of a neural controller that selects modules from a variable-length input tape, and a general domain-agnostic mechanism for the selection of modules. It is trained using input tape layout and parallel history tape. The authors show that the proposed MAIN architecture can be used for reinforcement learning with a large number of input-outlet examples, and that it can learn policies that are robust to random access to different types of data and different tape locations. They also show that MAIN achieves state-of-the-art performance on several algorithmic tasks. ","This paper proposes a modular neural network architecture MAIN for learning algorithms with input-output examples. MAIN consists of a neural controller that selects modules from a variable-length input tape, and a general domain-agnostic mechanism for the selection of modules. It is trained using input tape layout and parallel history tape. The authors show that the proposed MAIN architecture can be used for reinforcement learning with a large number of input-outlet examples, and that it can learn policies that are robust to random access to different types of data and different tape locations. They also show that MAIN achieves state-of-the-art performance on several algorithmic tasks. "
11856,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"Quantization USED-FOR Deep Neural Networks. Quantized floating point representations COMPARE fixed point representations. fixed point representations COMPARE Quantized floating point representations. Quantized floating point representations USED-FOR dynamic range. fixed point representations USED-FOR dynamic range. technique USED-FOR Deep Neural Networks. floating point arithmetic FEATURE-OF quantization. quantization FEATURE-OF Deep Neural Networks. floating point arithmetic USED-FOR Deep Neural Networks. Monte Carlo Arithmetic USED-FOR inference computation. relative standard deviation FEATURE-OF neural network loss. CIFAR-10 and ImageNet datasets EVALUATE-FOR pre - trained image classification models. pre - trained image classification models EVALUATE-FOR method. CIFAR-10 and ImageNet datasets EVALUATE-FOR method. loss of significance FEATURE-OF weight parameter sets. Metric is accuracy. OtherScientificTerm are parameter distributions, network topology, Monte Carlo trials, and network topologies. Method is MCDA. ","Quantization in Deep Neural Networks has been a popular topic of interest recently. Quantized floating point representations have been shown to have a dynamic range that is more expressive than fixed point representations. However, quantization with floating point arithmetic can be computationally expensive. This paper proposes a technique to reduce the computational cost of quantization in deep Neural Networks with floating-point arithmetic.    The authors propose to use Monte Carlo Arithmetic to perform inference computation. The idea is to use the relative standard deviation of the neural network loss as a metric to measure the accuracy of a neural network. The authors show that this method can be applied to pre-trained image classification models on CIFAR-10 and ImageNet datasets, and that the proposed method achieves better performance than the state-of-the-art in terms of accuracy.  The main contribution of the paper is that the authors propose a new way to compute the relative accuracy of the parameter distributions, which is based on Monte Carlo trials. The paper also shows that the loss of significance of the weight parameter sets can be controlled by the network topology and the number of parameters in the MCDA. The proposed method is shown to achieve better performance on the Cifar-10/ImageNet datasets compared to the state of the art. ","Quantization in Deep Neural Networks has been a popular topic of interest recently. Quantized floating point representations have been shown to have a dynamic range that is more expressive than fixed point representations. However, quantization with floating point arithmetic can be computationally expensive. This paper proposes a technique to reduce the computational cost of quantization in deep Neural Networks with floating-point arithmetic.    The authors propose to use Monte Carlo Arithmetic to perform inference computation. The idea is to use the relative standard deviation of the neural network loss as a metric to measure the accuracy of a neural network. The authors show that this method can be applied to pre-trained image classification models on CIFAR-10 and ImageNet datasets, and that the proposed method achieves better performance than the state-of-the-art in terms of accuracy.  The main contribution of the paper is that the authors propose a new way to compute the relative accuracy of the parameter distributions, which is based on Monte Carlo trials. The paper also shows that the loss of significance of the weight parameter sets can be controlled by the network topology and the number of parameters in the MCDA. The proposed method is shown to achieve better performance on the Cifar-10/ImageNet datasets compared to the state of the art. "
11860,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"Model - based reinforcement learning ( MBRL ) USED-FOR data - efficiently learning control of continuous tasks. function approximators CONJUNCTION planning schemes. planning schemes CONJUNCTION function approximators. function approximators USED-FOR MBRL. planning schemes USED-FOR MBRL. dynamics models USED-FOR task. method USED-FOR mismatch issue. dynamics model training USED-FOR method. Method are MBRL framework, and forward dynamics model. Task are objective mismatch issue, and downstream control task. OtherScientificTerm are Objective mismatch, and objective mismatch. Metric is downstream control performance. ","Model-based reinforcement learning (MBRL) is a popular approach for data-efficient learning control of continuous tasks. However, the MBRL framework has been widely criticized for the objective mismatch issue. Objective mismatch is defined as the difference between the performance of a forward dynamics model trained on a downstream control task and that of the dynamics models trained on the original task. This paper studies the problem of objective mismatch in MBRL with function approximators and planning schemes. The paper proposes a novel method to address the mismatch issue through dynamics model training. The proposed method is shown to improve downstream control performance. ","Model-based reinforcement learning (MBRL) is a popular approach for data-efficient learning control of continuous tasks. However, the MBRL framework has been widely criticized for the objective mismatch issue. Objective mismatch is defined as the difference between the performance of a forward dynamics model trained on a downstream control task and that of the dynamics models trained on the original task. This paper studies the problem of objective mismatch in MBRL with function approximators and planning schemes. The paper proposes a novel method to address the mismatch issue through dynamics model training. The proposed method is shown to improve downstream control performance. "
11864,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"CNN classifiers USED-FOR adversarial attacks. targeted blackbox transfer - based attack EVALUATE-FOR undefended ImageNet models. intermediate feature distributions FEATURE-OF CNNs. adversarial attacks USED-FOR intermediate feature distributions. Generic are network, and methodology. Task are adversarial attack, and attacking process. Method is CNN architecture. Metric is transferability. ","This paper studies the transferability of adversarial attacks on CNN classifiers. The authors propose a targeted blackbox transfer-based attack on undefended ImageNet models, where the goal is to transfer the adversarial attack to a different part of the network. The paper proposes a methodology to identify which parts of the input image are most likely to be targeted by an adversarial adversary, and then design a CNN architecture that is robust to this transferability. Experiments are conducted to demonstrate the effectiveness of the proposed methodology.    The paper also shows that adversarial examples are transferable across different CNNs with different intermediate feature distributions, and that the attacking process is transfer-agnostic. The main contribution of the paper is that the authors show that the transferable nature of the attacks is not due to the architecture of the CNN architecture, but rather to the fact that the adversary has access to the intermediate feature distribution of the image.","This paper studies the transferability of adversarial attacks on CNN classifiers. The authors propose a targeted blackbox transfer-based attack on undefended ImageNet models, where the goal is to transfer the adversarial attack to a different part of the network. The paper proposes a methodology to identify which parts of the input image are most likely to be targeted by an adversarial adversary, and then design a CNN architecture that is robust to this transferability. Experiments are conducted to demonstrate the effectiveness of the proposed methodology.    The paper also shows that adversarial examples are transferable across different CNNs with different intermediate feature distributions, and that the attacking process is transfer-agnostic. The main contribution of the paper is that the authors show that the transferable nature of the attacks is not due to the architecture of the CNN architecture, but rather to the fact that the adversary has access to the intermediate feature distribution of the image."
11868,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,approach USED-FOR reinforcement learning policies. latent behavioral space FEATURE-OF Wasserstein distances ( WDs ). Wasserstein distances ( WDs ) USED-FOR approach. dual formulation USED-FOR score functions. dual formulation USED-FOR WD. score functions USED-FOR policy optimization. dual formulation USED-FOR algorithms. smoothed WDs CONJUNCTION dual formulation. dual formulation CONJUNCTION smoothed WDs. WD regularizers USED-FOR algorithms. Behavior - Guided Policy Gradient CONJUNCTION Behavior - Guided Evolution Strategies. Behavior - Guided Evolution Strategies CONJUNCTION Behavior - Guided Policy Gradient. regularizers PART-OF on - policy algorithms. Behavior - Guided Evolution Strategies COMPARE methods. methods COMPARE Behavior - Guided Evolution Strategies. Behavior - Guided Policy Gradient HYPONYM-OF on - policy algorithms. Behavior - Guided Evolution Strategies HYPONYM-OF on - policy algorithms. Method is demo1. ,"This paper proposes a new approach to learn reinforcement learning policies based on Wasserstein distances (WD) in the latent behavioral space. The proposed approach is based on smoothed WDs and a dual formulation of the score functions of the WD for policy optimization. The authors also propose two algorithms based on the dual formulation that can be used as regularizers in existing on-policy algorithms, namely, Behavior-Guided Policy Gradient and Behavioural Guided Evolution Strategies. Experiments show that the proposed algorithms outperform existing algorithms with and without the WD regularizers. ","This paper proposes a new approach to learn reinforcement learning policies based on Wasserstein distances (WD) in the latent behavioral space. The proposed approach is based on smoothed WDs and a dual formulation of the score functions of the WD for policy optimization. The authors also propose two algorithms based on the dual formulation that can be used as regularizers in existing on-policy algorithms, namely, Behavior-Guided Policy Gradient and Behavioural Guided Evolution Strategies. Experiments show that the proposed algorithms outperform existing algorithms with and without the WD regularizers. "
11872,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"deep neural networks PART-OF supervised learning. optimization algorithm USED-FOR deep learning. interpolation property USED-FOR optimization algorithm. it USED-FOR adaptive learning - rate. SGD USED-FOR ALI - G. learning - rate EVALUATE-FOR ALI - G. SGD COMPARE ALI - G. ALI - G COMPARE SGD. constant hyper - parameter USED-FOR ALI - G. constant hyper - parameter USED-FOR learning - rate. convergence guarantees FEATURE-OF ALI - G. ALI - G USED-FOR stochastic convex setting. convergence guarantees FEATURE-OF stochastic convex setting. wide residual networks CONJUNCTION densely connected networks. densely connected networks CONJUNCTION wide residual networks. architectures CONJUNCTION tasks. tasks CONJUNCTION architectures. SVHN data set EVALUATE-FOR wide residual network. CIFAR data sets EVALUATE-FOR wide residual networks. SNLI data set EVALUATE-FOR Bi - LSTM. CIFAR data sets EVALUATE-FOR densely connected networks. ALI - G COMPARE SGD. SGD COMPARE ALI - G. ALI - G COMPARE adaptive methods. adaptive methods COMPARE ALI - G. manually tuned learning - rate schedules USED-FOR SGD. ALI - G USED-FOR drop - in replacement. ALI - G PART-OF deep learning framework. OtherScientificTerm are empirical loss, Adaptive Learning - rates, and decay schedule. Method is differentiable neural computer. ","This paper proposes a new optimization algorithm for deep learning based on the interpolation property of deep neural networks in supervised learning. Adaptive Learning-ratio (ALI-G) is a differentiable neural computer and it can be used to adjust the adaptive learning-rate when the empirical loss is large enough. The authors show that ALI-g has better convergence guarantees in the stochastic convex setting compared to SGD, and that it can also be used as a drop-in replacement for SGD when the decay schedule is too slow. They also show that under certain assumptions on the constant hyper-parameter of the learning rate, ALI -G can match or outperform SGD in terms of the convergence guarantees.   The authors evaluate wide residual networks and densely connected networks on CIFAR data sets, wide residual network on SVHN data set, and Bi-LSTM on the SNLI data set.  They also evaluate different architectures and different tasks, and compare with other adaptive methods.  The main contribution of this paper is to propose a new deep learning framework that combines the advantages of SGD and ALI with the benefits of manually tuned learning-rates.  In particular, the authors show how ALI can be incorporated into existing deep learning frameworks, and how it can provide a drop in replacement for the standard SGD.","This paper proposes a new optimization algorithm for deep learning based on the interpolation property of deep neural networks in supervised learning. Adaptive Learning-ratio (ALI-G) is a differentiable neural computer and it can be used to adjust the adaptive learning-rate when the empirical loss is large enough. The authors show that ALI-g has better convergence guarantees in the stochastic convex setting compared to SGD, and that it can also be used as a drop-in replacement for SGD when the decay schedule is too slow. They also show that under certain assumptions on the constant hyper-parameter of the learning rate, ALI -G can match or outperform SGD in terms of the convergence guarantees.   The authors evaluate wide residual networks and densely connected networks on CIFAR data sets, wide residual network on SVHN data set, and Bi-LSTM on the SNLI data set.  They also evaluate different architectures and different tasks, and compare with other adaptive methods.  The main contribution of this paper is to propose a new deep learning framework that combines the advantages of SGD and ALI with the benefits of manually tuned learning-rates.  In particular, the authors show how ALI can be incorporated into existing deep learning frameworks, and how it can provide a drop in replacement for the standard SGD."
11876,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,Forming perceptual groups CONJUNCTION individuating objects in visual scenes. individuating objects in visual scenes CONJUNCTION Forming perceptual groups. Forming perceptual groups USED-FOR visual intelligence. computations USED-FOR ability. connections USED-FOR perceptual grouping. high - level object cues USED-FOR perceptual grouping. synthetic visual tasks EVALUATE-FOR neural network architectures. learning USED-FOR networks. bottom - up connections USED-FOR networks. Horizontal connections USED-FOR straining. top - down connections USED-FOR learning. high - level object cues USED-FOR learning. model USED-FOR perceptual groups. interactions USED-FOR perceptual groups. interactions PART-OF model. Material is visual scenes. Generic is task. OtherScientificTerm is Gestalt cues. Task is incremental grouping. ,"This paper studies the problem of forming perceptual groups and individuating objects in visual scenes. Forming perceptual groups is an important problem in visual intelligence, and this paper proposes a novel way to improve the performance of neural network architectures. The authors propose a new task, called incremental grouping, which is based on the observation that existing computations can strangle the ability of a network to learn a good perceptual grouping. To alleviate the straining, the authors propose to use top-down connections instead of bottom-up connections for learning for networks with high-level object cues. Horizontal connections are also used to alleviate straining. Experiments on synthetic visual tasks show that the proposed method outperforms state-of-the-art networks on most of the tasks. The model is able to learn perceptual groups based on interactions between different perceptual groups. Gestalt cues are used to guide the learning. ","This paper studies the problem of forming perceptual groups and individuating objects in visual scenes. Forming perceptual groups is an important problem in visual intelligence, and this paper proposes a novel way to improve the performance of neural network architectures. The authors propose a new task, called incremental grouping, which is based on the observation that existing computations can strangle the ability of a network to learn a good perceptual grouping. To alleviate the straining, the authors propose to use top-down connections instead of bottom-up connections for learning for networks with high-level object cues. Horizontal connections are also used to alleviate straining. Experiments on synthetic visual tasks show that the proposed method outperforms state-of-the-art networks on most of the tasks. The model is able to learn perceptual groups based on interactions between different perceptual groups. Gestalt cues are used to guide the learning. "
11880,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"` 1 or ` 0 regularizers USED-FOR weight sparsity. ` 0 regularizer USED-FOR parameter sparsity. complex optimization techniques USED-FOR it. gradient descent USED-FOR ` 1 regularizer. Hoyer measure USED-FOR compressed sensing problems. DeepHoyer HYPONYM-OF sparsity - inducing regularizers. DeepHoyer regularizers USED-FOR sparser neural network models. DeepHoyer USED-FOR element - wise and structural pruning. Task is sparse and efficient neural network models. Method is neural network models. OtherScientificTerm are scaling of parameter values, gradients, and sparsity. Metric are shrinking rate, and accuracy level. ","This paper studies the problem of learning sparse and efficient neural network models. The authors propose two new regularizers for weight sparsity: 1) `1 or `0 regularizers that encourage the scaling of parameter values, and 2) `Hoyer regularizers, which encourage the shrinking rate of gradients.   The authors show that the `1 regularizer can be learned via gradient descent, and that it can be optimized using complex optimization techniques. They also show that a ` 0 regularizer induces parameter sparsity.  The paper also shows that the Hoyer measure can be applied to compressed sensing problems.  Finally, the authors propose DeepHoyer, which is a family of sparsity-inducing regularizers.  In particular, they show that for element-wise and structural pruning, the proposed Deephoyer can be used to learn sparser neural net models. They show that this sparsity can be achieved without compromising the accuracy level. ","This paper studies the problem of learning sparse and efficient neural network models. The authors propose two new regularizers for weight sparsity: 1) `1 or `0 regularizers that encourage the scaling of parameter values, and 2) `Hoyer regularizers, which encourage the shrinking rate of gradients.   The authors show that the `1 regularizer can be learned via gradient descent, and that it can be optimized using complex optimization techniques. They also show that a ` 0 regularizer induces parameter sparsity.  The paper also shows that the Hoyer measure can be applied to compressed sensing problems.  Finally, the authors propose DeepHoyer, which is a family of sparsity-inducing regularizers.  In particular, they show that for element-wise and structural pruning, the proposed Deephoyer can be used to learn sparser neural net models. They show that this sparsity can be achieved without compromising the accuracy level. "
11884,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"data - dependent O(log T ) regret bound FEATURE-OF strongly convex functions. Adam USED-FOR data - dependent O(log T ) regret bound. controlled step size USED-FOR strong convexity. hyperparameters USED-FOR SAdam. RMSprop USED-FOR strongly convex functions. SC - RMSprop HYPONYM-OF RMSprop. SC - RMSprop USED-FOR SAdam. optimizing strongly convex functions CONJUNCTION deep networks. deep networks CONJUNCTION optimizing strongly convex functions. OtherScientificTerm are O ( √ T ) regret bound, and time horizon. Metric is data - dependent logarithmic regret bound. Generic is method. ","This paper studies the data-dependent O(log T) regret bound of Adam for strongly convex functions. The authors show that the O(T) regret of Adam is O(√T) if and only if the strong convexity of the function is maintained and the controlled step size is constant. They also show that SAdam with the same hyperparameters achieves O(O(T^2) regret, which is the same as RMSprop (RMSprop), which is a variant of SAdam. The main contribution of this paper is that the authors propose a new method, called SC-RSprop, to improve the O(\log T^2/\sqrt{T}) regret bound. The idea is to use the SC-rMSprop as a generalization of the previous work, which shows that SC-Rsprop is a special case of the existing SAdam, and that it can be used for both optimizing strongly-convex functions and deep networks. In addition, the authors also provide a data-dependant logarithmic regret bound for the case where the time horizon is long enough. ","This paper studies the data-dependent O(log T) regret bound of Adam for strongly convex functions. The authors show that the O(T) regret of Adam is O(√T) if and only if the strong convexity of the function is maintained and the controlled step size is constant. They also show that SAdam with the same hyperparameters achieves O(O(T^2) regret, which is the same as RMSprop (RMSprop), which is a variant of SAdam. The main contribution of this paper is that the authors propose a new method, called SC-RSprop, to improve the O(\log T^2/\sqrt{T}) regret bound. The idea is to use the SC-rMSprop as a generalization of the previous work, which shows that SC-Rsprop is a special case of the existing SAdam, and that it can be used for both optimizing strongly-convex functions and deep networks. In addition, the authors also provide a data-dependant logarithmic regret bound for the case where the time horizon is long enough. "
11888,SP:9f89501e6319280b4a14b674632a300805aa485c,BlockBERT HYPONYM-OF BERT model. BERT model USED-FOR modeling long - distance dependencies. BlockBERT USED-FOR modeling long - distance dependencies. memory consumption CONJUNCTION training time. training time CONJUNCTION memory consumption. attention heads USED-FOR shortor long - range contextual information. sparse block structures PART-OF attention matrix. BERT USED-FOR model. sparse block structures USED-FOR model. paragraph lengths FEATURE-OF benchmark question answering datasets. BlockBERT COMPARE BERT - based model. BERT - based model COMPARE BlockBERT. prediction accuracy EVALUATE-FOR BERT - based model. BlockBERT COMPARE RoBERTa. RoBERTa COMPARE BlockBERT. memory USED-FOR BlockBERT. RoBERTa HYPONYM-OF BERT - based model. training time EVALUATE-FOR BlockBERT. prediction accuracy EVALUATE-FOR BlockBERT. ,"This paper proposes BlockBERT, a BERT model that aims at modeling long-distance dependencies in the input space. The authors propose to use sparse block structures in the attention matrix of BERT to reduce the memory consumption and training time. The attention heads are trained to capture shortor long-range contextual information. The proposed model is evaluated on a number of benchmark question answering datasets with different paragraph lengths, and the results show that BlockBERTs outperforms RoBERTa (a BERT-based model) in terms of prediction accuracy. The main contribution of the paper is that the authors propose a model that can be trained with less memory, and that the model is able to be trained in an unsupervised way.","This paper proposes BlockBERT, a BERT model that aims at modeling long-distance dependencies in the input space. The authors propose to use sparse block structures in the attention matrix of BERT to reduce the memory consumption and training time. The attention heads are trained to capture shortor long-range contextual information. The proposed model is evaluated on a number of benchmark question answering datasets with different paragraph lengths, and the results show that BlockBERTs outperforms RoBERTa (a BERT-based model) in terms of prediction accuracy. The main contribution of the paper is that the authors propose a model that can be trained with less memory, and that the model is able to be trained in an unsupervised way."
11892,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"test accuracy EVALUATE-FOR pruning approaches. test accuracy EVALUATE-FOR pruning. generalization EVALUATE-FOR pruning. pruning CONJUNCTION regularizing. regularizing CONJUNCTION pruning. generalization FEATURE-OF over - parameterized networks. noise USED-FOR regularizing. noise USED-FOR pruning. OtherScientificTerm are Pruning neural network parameters, overfitting, and instability. Method is pruned model. ","This paper studies the problem of over-parameterized neural networks. Pruning neural network parameters is a popular technique to improve the test accuracy of a neural network. However, there are several issues with existing pruning approaches: (1) overfitting, (2) instability, and (3) poor generalization. To address these issues, this paper proposes a new pruning approach that is based on the observation that test accuracy can be improved by reducing the number of parameters in the pruned model. Experiments show that pruning and regularizing with noise improves the generalization performance of the pruning. ","This paper studies the problem of over-parameterized neural networks. Pruning neural network parameters is a popular technique to improve the test accuracy of a neural network. However, there are several issues with existing pruning approaches: (1) overfitting, (2) instability, and (3) poor generalization. To address these issues, this paper proposes a new pruning approach that is based on the observation that test accuracy can be improved by reducing the number of parameters in the pruned model. Experiments show that pruning and regularizing with noise improves the generalization performance of the pruning. "
11896,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"framework USED-FOR automatic discovering process of neural architectures. neural architecture search ( NAS ) algorithm USED-FOR automatic discovering process of neural architectures. neural architecture search ( NAS ) algorithm CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION neural architecture search ( NAS ) algorithm. neural architecture search ( NAS ) algorithm USED-FOR framework. architecture encoders CONJUNCTION decoders. decoders CONJUNCTION architecture encoders. reinforcement learning USED-FOR embedding space. reinforcement learning USED-FOR approach. decoders USED-FOR reinforcement learning. architecture encoders USED-FOR reinforcement learning. decoders USED-FOR approach. architecture encoders USED-FOR approach. NAS approaches USED-FOR image classification task. architecture network COMPARE NAS approaches. NAS approaches COMPARE architecture network. image classification task EVALUATE-FOR architecture network. CIFAR10 USED-FOR image classification task. CIFAR10 EVALUATE-FOR NAS approaches. NASES procedure USED-FOR architecture network. architecture - embedding searching CONJUNCTION pre - training controller. pre - training controller CONJUNCTION architecture - embedding searching. architecture - embedding searching USED-FOR NASES. pre - training controller USED-FOR NASES. parameter sharing HYPONYM-OF NAS tricks. architecture USED-FOR NASES procedure. Method are neural architectures, NAS in embedding space ( NASES ), and NAS with reinforcement learning approaches. OtherScientificTerm are noncontinuous and highdimensional search spaces, and discrete and high - dimensional architecture space. Task is optimization. ","This paper proposes a new framework for the automatic discovering process of neural architectures based on the neural architecture search (NAS) algorithm and reinforcement learning, NAS in embedding space (NASES). The approach uses architecture encoders and decoders from reinforcement learning to learn the embedding representation of the architecture in a discrete and high-dimensional architecture space. The authors show that the NASES procedure can find an architecture network that outperforms existing NAS approaches on an image classification task on CIFAR10. NASES is based on architecture-embedding searching and a pre-training controller, and the authors also propose a few NAS tricks such as parameter sharing, which is a common practice in NAS with reinforcement learning approaches. The paper also shows that the architecture network trained with the proposed architecture network outperforms other NAS approaches. ","This paper proposes a new framework for the automatic discovering process of neural architectures based on the neural architecture search (NAS) algorithm and reinforcement learning, NAS in embedding space (NASES). The approach uses architecture encoders and decoders from reinforcement learning to learn the embedding representation of the architecture in a discrete and high-dimensional architecture space. The authors show that the NASES procedure can find an architecture network that outperforms existing NAS approaches on an image classification task on CIFAR10. NASES is based on architecture-embedding searching and a pre-training controller, and the authors also propose a few NAS tricks such as parameter sharing, which is a common practice in NAS with reinforcement learning approaches. The paper also shows that the architecture network trained with the proposed architecture network outperforms other NAS approaches. "
11900,SP:e2e5bebccc76a51df3cb8b64572720da97174604,"Homotopy Training Algorithm ( HTA ) USED-FOR optimization problems. neural networks USED-FOR optimization problems. decoupled systems USED-FOR HTA. low dimensional structure FEATURE-OF decoupled systems. HTA USED-FOR continuous homotopy path. continuous homotopy path USED-FOR system. homotopy solution path USED-FOR convex case. HTA USED-FOR non - convex case. CIFAR-10 USED-FOR VGG models. VGG models HYPONYM-OF examples. accuracy EVALUATE-FOR HTA. examples EVALUATE-FOR HTA. HTA USED-FOR neural networks. HTA CONJUNCTION dropout technique. dropout technique CONJUNCTION HTA. dropout technique USED-FOR neural networks. OtherScientificTerm are high dimensional coupled system, and low dimensionality. ","This paper proposes the Homotopy Training Algorithm (HTA) for optimization problems with neural networks. HTA trains decoupled systems with a low dimensional structure, and then trains a continuous homotopy path through the system. In the convex case, HTA learns a homotopic solution path for a high dimensional coupled system, while in the non-convex case HTA is able to learn a non-homotopy solution path. Experiments show that HTA improves the accuracy of neural networks on a number of examples (e.g., VGG models trained on CIFAR-10). HTA and the dropout technique are also applied to neural networks with low dimensionality. ","This paper proposes the Homotopy Training Algorithm (HTA) for optimization problems with neural networks. HTA trains decoupled systems with a low dimensional structure, and then trains a continuous homotopy path through the system. In the convex case, HTA learns a homotopic solution path for a high dimensional coupled system, while in the non-convex case HTA is able to learn a non-homotopy solution path. Experiments show that HTA improves the accuracy of neural networks on a number of examples (e.g., VGG models trained on CIFAR-10). HTA and the dropout technique are also applied to neural networks with low dimensionality. "
11904,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,attention USED-FOR entity representations. dot - product attention USED-FOR higher - dimensional attention. higher - dimensional attention PART-OF Transformer. tensor products of value vectors USED-FOR entity representations. inductive bias USED-FOR logical reasoning. architecture USED-FOR inductive bias. logical reasoning PART-OF deep reinforcement learning. Method is 2 - simplicial Transformer. ,"This paper proposes a new architecture for deep reinforcement learning (DRL) based on the 2-simplicial Transformer. Specifically, the authors propose a higher-dimensional attention in the Transformer based on dot-product attention. This attention is used to learn entity representations based on tensor products of value vectors. The authors show that the proposed architecture has an inductive bias towards logical reasoning in deep RL. ","This paper proposes a new architecture for deep reinforcement learning (DRL) based on the 2-simplicial Transformer. Specifically, the authors propose a higher-dimensional attention in the Transformer based on dot-product attention. This attention is used to learn entity representations based on tensor products of value vectors. The authors show that the proposed architecture has an inductive bias towards logical reasoning in deep RL. "
11908,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,classification and regression approaches USED-FOR task. labelled data USED-FOR classification and regression approaches. labelled data USED-FOR pose estimation. circular latent representations USED-FOR 2D rotations. datasets USED-FOR method. labelled images FEATURE-OF datasets. Task is Pose estimation. OtherScientificTerm is fixed frame of reference. Method is Conditional Variational Autoencoders ( CVAEs ). ,"This paper proposes a new method for pose estimation based on a set of labelled data. Pose estimation is a challenging problem, and existing classification and regression approaches for this task are limited to a fixed frame of reference. This paper proposes to learn circular latent representations for 2D rotations, which are then used to train Conditional Variational Autoencoders (CVAEs). The proposed method is tested on two datasets with labelled images, and the results show that the proposed method performs well. ","This paper proposes a new method for pose estimation based on a set of labelled data. Pose estimation is a challenging problem, and existing classification and regression approaches for this task are limited to a fixed frame of reference. This paper proposes to learn circular latent representations for 2D rotations, which are then used to train Conditional Variational Autoencoders (CVAEs). The proposed method is tested on two datasets with labelled images, and the results show that the proposed method performs well. "
11912,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"Evolutionary Population Curriculum ( EPC ) HYPONYM-OF curriculum learning paradigm. evolutionary approach USED-FOR objective misalignment issue. evolutionary approach USED-FOR EPC. approach COMPARE baselines. baselines COMPARE approach. MARL algorithm EVALUATE-FOR EPC. MADDPG HYPONYM-OF MARL algorithm. Task is multi - agent games. Metric is complexity. OtherScientificTerm are policies, agent population, curriculum, and scaled populations. Method is MultiAgent Reinforcement Learning ( MARL ). ","This paper proposes a curriculum learning paradigm called Evolutionary Population Curriculum (EPC) for multi-agent games, which aims to reduce the complexity of learning policies in the presence of a large number of agents. The authors propose an evolutionary approach to address the objective misalignment issue, where the agent population changes over the course of training and the goal is to learn policies that maximize the return of all agents in the population. The curriculum is based on the idea of MultiAgent Reinforcement Learning (MARL). The authors show that their approach outperforms several baselines on a number of benchmarks. They also show that EPC outperforms an existing MARL algorithm, MADDPG, in terms of performance on scaled populations. ","This paper proposes a curriculum learning paradigm called Evolutionary Population Curriculum (EPC) for multi-agent games, which aims to reduce the complexity of learning policies in the presence of a large number of agents. The authors propose an evolutionary approach to address the objective misalignment issue, where the agent population changes over the course of training and the goal is to learn policies that maximize the return of all agents in the population. The curriculum is based on the idea of MultiAgent Reinforcement Learning (MARL). The authors show that their approach outperforms several baselines on a number of benchmarks. They also show that EPC outperforms an existing MARL algorithm, MADDPG, in terms of performance on scaled populations. "
11916,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,MULTIPLEX GRAPH NETWORKS USED-FOR DIAGRAMMATIC REASONING. ,This paper presents a new method for generating explanations for DIAGRAMMATIC REASONING from multi-headed graph neural networks (MIGNN). The method is based on multi-head graph neural network (MGNNN) and is trained on a large collection of multi-heads graphs. The authors show that the proposed method is able to generate explanations that are more interpretable and interpretable than existing methods. The paper is well-written and easy to follow.,This paper presents a new method for generating explanations for DIAGRAMMATIC REASONING from multi-headed graph neural networks (MIGNN). The method is based on multi-head graph neural network (MGNNN) and is trained on a large collection of multi-heads graphs. The authors show that the proposed method is able to generate explanations that are more interpretable and interpretable than existing methods. The paper is well-written and easy to follow.
11920,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,"Bayesian inference USED-FOR deep neural networks. agent exploration CONJUNCTION prediction fairness. prediction fairness CONJUNCTION agent exploration. decision making CONJUNCTION agent exploration. agent exploration CONJUNCTION decision making. calibrated measure of uncertainty USED-FOR decision making. It USED-FOR calibrated measure of uncertainty. calibrated measure of uncertainty USED-FOR agent exploration. calibrated measure of uncertainty USED-FOR prediction fairness. It USED-FOR overfitting. decision making CONJUNCTION prediction fairness. prediction fairness CONJUNCTION decision making. Markov Chain Monte Carlo ( MCMC ) methods USED-FOR Bayesian inference. model parameters FEATURE-OF posterior distribution. optimization methods USED-FOR large scale deep learning tasks. sampling methods COMPARE optimization methods. optimization methods COMPARE sampling methods. MCMC CONJUNCTION optimization methods. optimization methods CONJUNCTION MCMC. ATMC HYPONYM-OF adaptive noise MCMC algorithm. momentum CONJUNCTION noise. noise CONJUNCTION momentum. noise USED-FOR parameter update. momentum USED-FOR parameter update. momentum USED-FOR ATMC. noise USED-FOR ATMC. classification accuracy CONJUNCTION test log - likelihood. test log - likelihood CONJUNCTION classification accuracy. Cifar10 benchmark CONJUNCTION large scale ImageNet benchmark. large scale ImageNet benchmark CONJUNCTION Cifar10 benchmark. ATMC COMPARE optimization baseline. optimization baseline COMPARE ATMC. ResNet architecture USED-FOR ATMC. ResNet architecture CONJUNCTION batch normalization. batch normalization CONJUNCTION ResNet architecture. test log - likelihood EVALUATE-FOR optimization baseline. large scale ImageNet benchmark EVALUATE-FOR ATMC. classification accuracy EVALUATE-FOR optimization baseline. Cifar10 benchmark EVALUATE-FOR ATMC. test log - likelihood EVALUATE-FOR ATMC. classification accuracy EVALUATE-FOR ATMC. ATMC COMPARE optimization baseline. optimization baseline COMPARE ATMC. calibrated measure of uncertainty EVALUATE-FOR optimization baseline. ATMC USED-FOR overfitting. ATMC COMPARE ATMC. ATMC COMPARE ATMC. calibrated measure of uncertainty EVALUATE-FOR ATMC. OtherScientificTerm are hyperparameters, and","This paper studies the problem of Bayesian inference for deep neural networks. It proposes a calibrated measure of uncertainty for decision making, agent exploration, prediction fairness, and agent exploration. It shows that existing Markov Chain Monte Carlo (MCMC) methods do not work well for this problem. The authors propose an adaptive noise MCMC algorithm, called ATMC, which uses momentum and noise to adjust the posterior distribution of the model parameters. It is shown to avoid overfitting and to be more robust to hyperparameters than existing optimization methods for large scale deep learning tasks. The paper also shows that ATMC can be used with momentum or noise to speed up the parameter update. ATMC is tested on the Cifar10 benchmark and the large scale ImageNet benchmark, where it outperforms the optimization baseline in terms of classification accuracy and test log-likelihood, and outperforms a baseline that uses a ResNet architecture and batch normalization. The main contribution of the paper is the introduction of an adaptive uncertainty measure and the use of momentum to improve the performance of ATMC. ","This paper studies the problem of Bayesian inference for deep neural networks. It proposes a calibrated measure of uncertainty for decision making, agent exploration, prediction fairness, and agent exploration. It shows that existing Markov Chain Monte Carlo (MCMC) methods do not work well for this problem. The authors propose an adaptive noise MCMC algorithm, called ATMC, which uses momentum and noise to adjust the posterior distribution of the model parameters. It is shown to avoid overfitting and to be more robust to hyperparameters than existing optimization methods for large scale deep learning tasks. The paper also shows that ATMC can be used with momentum or noise to speed up the parameter update. ATMC is tested on the Cifar10 benchmark and the large scale ImageNet benchmark, where it outperforms the optimization baseline in terms of classification accuracy and test log-likelihood, and outperforms a baseline that uses a ResNet architecture and batch normalization. The main contribution of the paper is the introduction of an adaptive uncertainty measure and the use of momentum to improve the performance of ATMC. "
11924,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"accuracy EVALUATE-FOR latter. winning tickets USED-FOR dense, randomly initialized networks. small but critical subnetworks HYPONYM-OF winning tickets. early stopping CONJUNCTION low - precision training. low - precision training CONJUNCTION early stopping. large learning rates FEATURE-OF lowcost training schemes. low - precision training HYPONYM-OF lowcost training schemes. early stopping HYPONYM-OF lowcost training schemes. lowcost training schemes USED-FOR winning tickets. connectivity patterns FEATURE-OF neural networks. mask distance metric USED-FOR EB tickets. computational overhead EVALUATE-FOR mask distance metric. mask distance USED-FOR training methods. low - cost schemes USED-FOR EB tickets. method USED-FOR deep network training. mask distance USED-FOR them. deep networks CONJUNCTION datasets. datasets CONJUNCTION deep networks. accuracy EVALUATE-FOR training methods. Method is train - prune - retrain process. OtherScientificTerm is Early - Bird ( EB ) tickets. Metric is energy savings. Generic is state - ofthe - art training methods. ","This paper proposes a method for early-bird ticketing for training deep neural networks. The idea is that winning tickets for training dense, randomly initialized networks (i.e. small but critical subnetworks) can be pruned during the train-prune-retrain process, and that the latter can improve the accuracy of the latter.   The paper proposes two lowcost training schemes for winning tickets: early stopping and low-precision training with large learning rates. The paper also proposes a mask distance metric for EB tickets that can be used to compare the performance of different training methods based on the mask distance between the winning tickets and the early-Bird (EB) tickets.  The authors show that the proposed method can be applied to any deep network training and that it can be combined with other low-cost schemes (e.g. early stopping, low-probability training, etc.) to produce EB tickets. They also show that their method achieves energy savings in terms of the number of training epochs and the computational overhead in comparison to state-of-the-art training methods.  Finally, the paper shows that this method is applicable to both deep networks and datasets, and is able to identify winning tickets based on connectivity patterns of the neural networks, and prune them based on mask distance. ","This paper proposes a method for early-bird ticketing for training deep neural networks. The idea is that winning tickets for training dense, randomly initialized networks (i.e. small but critical subnetworks) can be pruned during the train-prune-retrain process, and that the latter can improve the accuracy of the latter.   The paper proposes two lowcost training schemes for winning tickets: early stopping and low-precision training with large learning rates. The paper also proposes a mask distance metric for EB tickets that can be used to compare the performance of different training methods based on the mask distance between the winning tickets and the early-Bird (EB) tickets.  The authors show that the proposed method can be applied to any deep network training and that it can be combined with other low-cost schemes (e.g. early stopping, low-probability training, etc.) to produce EB tickets. They also show that their method achieves energy savings in terms of the number of training epochs and the computational overhead in comparison to state-of-the-art training methods.  Finally, the paper shows that this method is applicable to both deep networks and datasets, and is able to identify winning tickets based on connectivity patterns of the neural networks, and prune them based on mask distance. "
11928,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"adversarial examples FEATURE-OF deep convolutional neural networks. intrinsic dimension COMPARE pixel space dimension. pixel space dimension COMPARE intrinsic dimension. low - dimensional space USED-FOR classification. intrinsic dimension FEATURE-OF image data. image data COMPARE pixel space dimension. pixel space dimension COMPARE image data. high - dimensional input images USED-FOR classification. high - dimensional input images USED-FOR low - dimensional space. vulnerability EVALUATE-FOR neural networks. robustness EVALUATE-FOR deep neural networks. adversarial robustness EVALUATE-FOR classifier. benchmark datasets EVALUATE-FOR framework. benchmark datasets EVALUATE-FOR strong adversarial attack methods. strong adversarial attack methods EVALUATE-FOR framework. OtherScientificTerm are input dimension, lowdimensional space, regularization, and embedding regularization. Method is Embedding Regularized Classifier ( ER - Classifier ). ","This paper studies the vulnerability of deep convolutional neural networks to adversarial examples. The authors propose an Embedding Regularized Classifier (ER-Classifier) that learns a low-dimensional space in which the intrinsic dimension of the image data is much smaller than the pixel space dimension. They show that the input dimension can be reduced to a lowdimensional space, and that the classifier can be trained in this low-dimension space with high-dimensional input images for classification. They also show that this regularization can be used to improve the robustness of deep neural networks against adversarial attacks. Finally, the authors propose a new embedding regularization that encourages the embedding to be more robust. The proposed framework is tested on several benchmark datasets for strong adversarial attack methods and shows that the proposed framework improves the adversarial robustness against a classifier trained with the proposed regularization. ","This paper studies the vulnerability of deep convolutional neural networks to adversarial examples. The authors propose an Embedding Regularized Classifier (ER-Classifier) that learns a low-dimensional space in which the intrinsic dimension of the image data is much smaller than the pixel space dimension. They show that the input dimension can be reduced to a lowdimensional space, and that the classifier can be trained in this low-dimension space with high-dimensional input images for classification. They also show that this regularization can be used to improve the robustness of deep neural networks against adversarial attacks. Finally, the authors propose a new embedding regularization that encourages the embedding to be more robust. The proposed framework is tested on several benchmark datasets for strong adversarial attack methods and shows that the proposed framework improves the adversarial robustness against a classifier trained with the proposed regularization. "
11932,SP:efd68097f47dbfdd0208573071686a62240d1b12,"Named entity recognition ( NER ) CONJUNCTION relation extraction ( RE ). relation extraction ( RE ) CONJUNCTION Named entity recognition ( NER ). Named entity recognition ( NER ) HYPONYM-OF tasks. relation extraction ( RE ) HYPONYM-OF tasks. dependency parsers HYPONYM-OF external natural language processing ( NLP ) tools. external natural language processing ( NLP ) tools USED-FOR joint models. neural, end - to - end model USED-FOR jointly extracting entities. large, pre - trained language model USED-FOR neural, end - to - end model. recurrence USED-FOR self - attention. OtherScientificTerm is propagation of error. Method are pipeline - based systems, neural, end - to - end models, and external NLP tools. Generic are tools, and model. ","This paper proposes a method to jointly train two tasks, named entity recognition (NER) and relation extraction (RE), using external natural language processing (NLP) tools, such as dependency parsers. The authors propose a neural, end-to-end model for jointly extracting entities from a large, pre-trained language model. The main idea is to use a pipeline-based systems to train the two tasks jointly, and then use external NLP tools to train joint models. The key idea is that the propagation of error between two tasks can be reduced by using a neural model that can be trained with external tools. The paper also proposes to use recurrence for self-attention, which can be used as a way to improve the performance of the neural model.   ","This paper proposes a method to jointly train two tasks, named entity recognition (NER) and relation extraction (RE), using external natural language processing (NLP) tools, such as dependency parsers. The authors propose a neural, end-to-end model for jointly extracting entities from a large, pre-trained language model. The main idea is to use a pipeline-based systems to train the two tasks jointly, and then use external NLP tools to train joint models. The key idea is that the propagation of error between two tasks can be reduced by using a neural model that can be trained with external tools. The paper also proposes to use recurrence for self-attention, which can be used as a way to improve the performance of the neural model.   "
11936,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"network USED-FOR task. informative representation USED-FOR tasks. network USED-FOR informative representation. image context FEATURE-OF RGB values of images. RGB values of images HYPONYM-OF low - level feature representation. DNNs USED-FOR Euclidean representation. DNNs USED-FOR algorithm. problem PART-OF machine learning. Ordinal Embedding HYPONYM-OF machine learning. approaches USED-FOR problem. approach COMPARE methods. methods COMPARE approach. real - world large datasets EVALUATE-FOR methods. real - world large datasets EVALUATE-FOR approach. Method are supervised / unsupervised DNNs, and neural networks. OtherScientificTerm is triplet comparisons. Task is unsupervised learning problems. ","This paper studies the problem of learning a low-level feature representation (i.e., RGB values of images in an image context) from a network trained on a single task to an informative representation for other tasks. This problem is an important problem in machine learning (e.g., Ordinal Embedding) and has been studied extensively in both supervised/unsupervised DNNs. The authors propose an algorithm that uses DNN to learn a Euclidean representation of the input image, which is then used to learn the informative representation of a network for a new task.  The authors show that the proposed approach outperforms existing methods on several real-world large datasets, including triplet comparisons. They also show that their approach can be applied to unsupervised learning problems, where neural networks are not trained on the same data as in the supervised setting.    The paper is well-written and well-motivated, and the paper is clearly written. The approach is well motivated and the results are interesting. However, there are a few issues that prevent me from recommending the paper be accepted. ","This paper studies the problem of learning a low-level feature representation (i.e., RGB values of images in an image context) from a network trained on a single task to an informative representation for other tasks. This problem is an important problem in machine learning (e.g., Ordinal Embedding) and has been studied extensively in both supervised/unsupervised DNNs. The authors propose an algorithm that uses DNN to learn a Euclidean representation of the input image, which is then used to learn the informative representation of a network for a new task.  The authors show that the proposed approach outperforms existing methods on several real-world large datasets, including triplet comparisons. They also show that their approach can be applied to unsupervised learning problems, where neural networks are not trained on the same data as in the supervised setting.    The paper is well-written and well-motivated, and the paper is clearly written. The approach is well motivated and the results are interesting. However, there are a few issues that prevent me from recommending the paper be accepted. "
11940,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"domain adaptation CONJUNCTION corrupted sample discovery. corrupted sample discovery CONJUNCTION domain adaptation. building insights about the learning task CONJUNCTION domain adaptation. domain adaptation CONJUNCTION building insights about the learning task. corrupted sample discovery CONJUNCTION robust learning. robust learning CONJUNCTION corrupted sample discovery. Data Valuation HYPONYM-OF meta learning framework. Reinforcement Learning ( DVRL ) USED-FOR meta learning framework. deep neural network USED-FOR data value estimator. reinforcement signal USED-FOR data value estimator. DVRL COMPARE methods. methods COMPARE DVRL. DVRL USED-FOR data value estimates. data value estimates EVALUATE-FOR methods. domain adaptation CONJUNCTION robust learning. robust learning CONJUNCTION domain adaptation. DVRL COMPARE state - of - the - art. state - of - the - art COMPARE DVRL. DVRL USED-FOR corrupted sample discovery. DVRL USED-FOR robust learning. DVRL USED-FOR domain adaptation. Task are machine learning, and Data valuation. OtherScientificTerm is data values. Method are task predictor model, and predictor model. ","This paper proposes a meta learning framework called Data Valuation (DVRL) which is based on Reinforcement Learning (RL) to learn a data value estimator using a deep neural network. Data valuation is defined as the difference between the true value of the data and the value of a task predictor model. The paper proposes to use a reinforcement signal to guide the training of this data value estimation, which can be used for building insights about the learning task, domain adaptation, corrupted sample discovery, and robust learning. Experiments show that DVRL outperforms existing methods in terms of data value estimates, and outperforms the state-of-the-art in domains where DVRL is applied to domains where the predictor model is not available. ","This paper proposes a meta learning framework called Data Valuation (DVRL) which is based on Reinforcement Learning (RL) to learn a data value estimator using a deep neural network. Data valuation is defined as the difference between the true value of the data and the value of a task predictor model. The paper proposes to use a reinforcement signal to guide the training of this data value estimation, which can be used for building insights about the learning task, domain adaptation, corrupted sample discovery, and robust learning. Experiments show that DVRL outperforms existing methods in terms of data value estimates, and outperforms the state-of-the-art in domains where DVRL is applied to domains where the predictor model is not available. "
11944,SP:e2c3374629cfd654b7b35e88507e65646d70470e,"network ’s architecture CONJUNCTION initialization parameters. initialization parameters CONJUNCTION network ’s architecture. gradient FEATURE-OF random fully connected ReLU networks. finite sized networks FEATURE-OF per layer Jacobian. initialization parameters PART-OF network. ResNets CONJUNCTION DenseNets. DenseNets CONJUNCTION ResNets. vanilla networks CONJUNCTION ResNets. ResNets CONJUNCTION vanilla networks. vanilla networks HYPONYM-OF architectures. DenseNets HYPONYM-OF architectures. ResNets HYPONYM-OF architectures. arbitrary depths FEATURE-OF norm. architecture CONJUNCTION layer ’s size. layer ’s size CONJUNCTION architecture. Method are neural networks, initialization strategy, and deep networks. Task is gradient steps post - initialization. OtherScientificTerm are Jacobian squared norm, exploding or decaying gradients, per layer Jacobian norm, and layer ’s depth. ","This paper considers the problem of exploding or decaying gradients in neural networks. The authors consider random fully connected ReLU networks with a fixed gradient, and show that the Jacobian squared norm of the per layer Jacobian in finite sized networks is a function of the network’s architecture and the initialization parameters of the entire network. They also show that if the initialization strategy is differentiable enough, the gradient steps post-initialization can be reduced to zero.   The authors show that this phenomenon happens for a variety of architectures (vanilla networks, ResNets, DenseNets) and architectures (ResNets and DenseNet). They show that exploding/decaying gradients are not caused by the architecture, but rather by the per-layer Jacobian norm, which depends on the depth of the layer and the number of initialization parameters in the network. In particular, they show that for deep networks with arbitrary depths, the norm depends only on the architecture and not on the layer's size, which is a result of the architecture. ","This paper considers the problem of exploding or decaying gradients in neural networks. The authors consider random fully connected ReLU networks with a fixed gradient, and show that the Jacobian squared norm of the per layer Jacobian in finite sized networks is a function of the network’s architecture and the initialization parameters of the entire network. They also show that if the initialization strategy is differentiable enough, the gradient steps post-initialization can be reduced to zero.   The authors show that this phenomenon happens for a variety of architectures (vanilla networks, ResNets, DenseNets) and architectures (ResNets and DenseNet). They show that exploding/decaying gradients are not caused by the architecture, but rather by the per-layer Jacobian norm, which depends on the depth of the layer and the number of initialization parameters in the network. In particular, they show that for deep networks with arbitrary depths, the norm depends only on the architecture and not on the layer's size, which is a result of the architecture. "
11948,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"hand - optimized libraries CONJUNCTION compilation heuristics. compilation heuristics CONJUNCTION hand - optimized libraries. genetic algorithms CONJUNCTION stochastic methods. stochastic methods CONJUNCTION genetic algorithms. compilation heuristics CONJUNCTION genetic algorithms. genetic algorithms CONJUNCTION compilation heuristics. hand - optimized libraries USED-FOR neural networks. solution USED-FOR code optimization. reinforcement learning USED-FOR CHAMELEON. reinforcement learning USED-FOR solution. domain - knowledge inspired logic USED-FOR adaptive sampling algorithm. CHAMELEON COMPARE AutoTVM. AutoTVM COMPARE CHAMELEON. optimization time EVALUATE-FOR AutoTVM. real hardware EVALUATE-FOR CHAMELEON. inference time EVALUATE-FOR deep networks. optimization time EVALUATE-FOR CHAMELEON. Metric is compilation time. Generic are methods, and them. OtherScientificTerm are hardware measurements, design space, and real hardware measurements. Task is search. ","This paper proposes a new method to speed up the training of neural networks using hand-optimized libraries, compilation heuristics, genetic algorithms, and stochastic methods. The authors propose a solution for code optimization based on reinforcement learning, called CHAMELEON. The core idea is to use domain-knowledge inspired logic to learn an adaptive sampling algorithm that can be applied to any hardware measurements. The paper shows that the proposed method CHameLEON outperforms AutoTVM in terms of optimization time and inference time on real hardware.    The paper is well-written, well-motivated, and easy to follow. The contribution of the paper is that the authors provide a thorough comparison of existing methods, and show that all of them perform similarly in the sense that the search space is not too large. They also show that real hardware measurements can be used to improve the performance. ","This paper proposes a new method to speed up the training of neural networks using hand-optimized libraries, compilation heuristics, genetic algorithms, and stochastic methods. The authors propose a solution for code optimization based on reinforcement learning, called CHAMELEON. The core idea is to use domain-knowledge inspired logic to learn an adaptive sampling algorithm that can be applied to any hardware measurements. The paper shows that the proposed method CHameLEON outperforms AutoTVM in terms of optimization time and inference time on real hardware.    The paper is well-written, well-motivated, and easy to follow. The contribution of the paper is that the authors provide a thorough comparison of existing methods, and show that all of them perform similarly in the sense that the search space is not too large. They also show that real hardware measurements can be used to improve the performance. "
11952,SP:df8483206bb88debeb24b04eb31e016368792a84,adversarial perturbations FEATURE-OF classifiers. certified robustnesses FEATURE-OF top-1 predictions. top - k predictions PART-OF real - world applications. certified robustness USED-FOR top - k predictions. randomized smoothing USED-FOR certified robustness. it USED-FOR large - scale neural networks. it USED-FOR classifier. randomized smoothing USED-FOR classifier. ` 2 norm FEATURE-OF topk predictions. tight robustness FEATURE-OF topk predictions. ` 2 norm FEATURE-OF tight robustness. Gaussian noise USED-FOR randomized smoothing. randomized smoothing USED-FOR tight robustness. top - k predictions FEATURE-OF certified robustness. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. ImageNet EVALUATE-FOR method. CIFAR10 EVALUATE-FOR method. method USED-FOR ImageNet classifier. ` 2 - norms FEATURE-OF adversarial perturbations. top-5 accuracy EVALUATE-FOR ImageNet classifier. OtherScientificTerm is noise. ,"This paper studies the problem of certified robustness of classifiers to adversarial perturbations. The authors consider certified robustnesses of the top-1 predictions and top-k predictions in real-world applications. They show that randomized smoothing can improve the certification of top-K predictions, and that it can be applied to large-scale neural networks. They also show that it improves the classifier’s top-5 accuracy when the noise is Gaussian. Finally, they propose a method to improve the top 5 accuracy of the ImageNet classifier with randomized smoothed with Gaussian noise.   ","This paper studies the problem of certified robustness of classifiers to adversarial perturbations. The authors consider certified robustnesses of the top-1 predictions and top-k predictions in real-world applications. They show that randomized smoothing can improve the certification of top-K predictions, and that it can be applied to large-scale neural networks. They also show that it improves the classifier’s top-5 accuracy when the noise is Gaussian. Finally, they propose a method to improve the top 5 accuracy of the ImageNet classifier with randomized smoothed with Gaussian noise.   "
11956,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,gradient signal to noise ratio ( GSNR ) USED-FOR DNNs. GSNR EVALUATE-FOR parameter. GSNR CONJUNCTION generalization gap. generalization gap CONJUNCTION GSNR. logistic regression CONJUNCTION support vector machines. support vector machines CONJUNCTION logistic regression. gradient descent optimization dynamics USED-FOR DNNs. gradient descent optimization dynamics USED-FOR GSNR. support vector machines HYPONYM-OF shallow models. logistic regression HYPONYM-OF shallow models. Method is deep neural networks ( DNNs ). OtherScientificTerm is data distribution. ,"This paper studies the gradient signal to noise ratio (GSNR) of deep neural networks (DNNs) and its relationship to the generalization performance of DNNs. The authors show that the parameter GSNR can be used as a measure of generalization ability of a DNN. They also show that if the data distribution is sufficiently shallow, then the GSNRR of a deep network can be approximated by a simple linear combination of GSNRs. The paper also shows that the gradient descent optimization dynamics of a standard DNN can be seen as a function of the parameter. ","This paper studies the gradient signal to noise ratio (GSNR) of deep neural networks (DNNs) and its relationship to the generalization performance of DNNs. The authors show that the parameter GSNR can be used as a measure of generalization ability of a DNN. They also show that if the data distribution is sufficiently shallow, then the GSNRR of a deep network can be approximated by a simple linear combination of GSNRs. The paper also shows that the gradient descent optimization dynamics of a standard DNN can be seen as a function of the parameter. "
11960,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"approach USED-FOR problem. KG entities PART-OF vector space. conjunctions CONJUNCTION existential quantifiers. existential quantifiers CONJUNCTION conjunctions. existential quantifiers USED-FOR queries. conjunctions USED-FOR queries. logical disjunctions ( ∨ ) USED-FOR Handling queries. QUERY2BOX HYPONYM-OF embedding - based framework. embedding - based framework USED-FOR massive and incomplete KGs. QUERY2BOX USED-FOR arbitrary logical queries. Disjunctive Normal Form USED-FOR QUERY2BOX. Disjunctive Normal Form FEATURE-OF queries. ∧ FEATURE-OF arbitrary logical queries. QUERY2BOX COMPARE state of the art. state of the art COMPARE QUERY2BOX. QUERY2BOX COMPARE QUERY2BOX. QUERY2BOX COMPARE QUERY2BOX. KGs EVALUATE-FOR QUERY2BOX. Task is Answering complex logical queries. OtherScientificTerm are complex query, hyper - rectangles, and disjunctions. ","This paper tackles the problem of Answering complex logical queries. The authors propose a novel approach to this problem. They propose a new embedding-based framework called QUERY2BOX, which is able to handle massive and incomplete KGs by embedding KG entities into a vector space. The queries are represented as conjunctions and existential quantifiers. Handling queries using logical disjunctions (∨) is an important problem. The paper proposes to use Disjunctive Normal Form (DNF) to represent queries in the context of a complex query. The key idea is to use hyper-rectangles to represent the query as a set of hyper-triangles, and then use disjunitions to encode the query. Experiments on KGs are conducted to demonstrate the effectiveness of QUBER2BOX in answering arbitrary logical queries (in the form of a query in the form  of a disjunctive normal form). The results show that QUBER1BOX outperforms the state of the art in terms of the number of queries, and that QUER2BOX performs significantly better than QUBER.  ","This paper tackles the problem of Answering complex logical queries. The authors propose a novel approach to this problem. They propose a new embedding-based framework called QUERY2BOX, which is able to handle massive and incomplete KGs by embedding KG entities into a vector space. The queries are represented as conjunctions and existential quantifiers. Handling queries using logical disjunctions (∨) is an important problem. The paper proposes to use Disjunctive Normal Form (DNF) to represent queries in the context of a complex query. The key idea is to use hyper-rectangles to represent the query as a set of hyper-triangles, and then use disjunitions to encode the query. Experiments on KGs are conducted to demonstrate the effectiveness of QUBER2BOX in answering arbitrary logical queries (in the form of a query in the form  of a disjunctive normal form). The results show that QUBER1BOX outperforms the state of the art in terms of the number of queries, and that QUER2BOX performs significantly better than QUBER.  "
11964,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"approaches USED-FOR stochastic optimization. Stochastic gradient descent ( SGD ) USED-FOR stochastic optimization. Stochastic gradient descent ( SGD ) HYPONYM-OF approaches. machine learning USED-FOR convex loss functions. machine learning USED-FOR nonconvex deep neural networks. machine learning USED-FOR SGD. unbiased estimator COMPARE full gradient. full gradient COMPARE unbiased estimator. consistent estimators COMPARE unbiased ones. unbiased ones COMPARE consistent estimators. convergence behavior EVALUATE-FOR consistent estimators. training algorithms USED-FOR large - scale graphs. consistent estimators USED-FOR SGD updates. Method are unbiased gradient estimator, empirical risk minimization, and consistent gradient estimator. OtherScientificTerm are graphs, and convex, convex, and nonconvex objectives. Material is synthetic and real - world data. ","This paper studies two approaches to stochastic optimization, namely Stochastic gradient descent (SGD) and the unbiased gradient estimator. In particular, the authors consider SGD with machine learning for convex loss functions and nonconvex deep neural networks. They show that the unbiased estimator converges faster than the full gradient when the number of graphs is large enough. They also show that consistent estimators of SGD updates have a similar convergence behavior as the unbiased ones. Finally, they propose two training algorithms for large-scale graphs that are consistent with the empirical risk minimization.    The paper is well-written, well-motivated, and well-structured. The authors provide a theoretical analysis of the convergence behavior of the unbiased and consistent gradient estimators. The paper also provides empirical experiments on both synthetic and real-world data to show the convergence of the consistent estimator in the case of convex, convex, convex convex-concave, and nonconsistent gradient estimations.  The authors also provide a detailed discussion of the properties of the training algorithms. ","This paper studies two approaches to stochastic optimization, namely Stochastic gradient descent (SGD) and the unbiased gradient estimator. In particular, the authors consider SGD with machine learning for convex loss functions and nonconvex deep neural networks. They show that the unbiased estimator converges faster than the full gradient when the number of graphs is large enough. They also show that consistent estimators of SGD updates have a similar convergence behavior as the unbiased ones. Finally, they propose two training algorithms for large-scale graphs that are consistent with the empirical risk minimization.    The paper is well-written, well-motivated, and well-structured. The authors provide a theoretical analysis of the convergence behavior of the unbiased and consistent gradient estimators. The paper also provides empirical experiments on both synthetic and real-world data to show the convergence of the consistent estimator in the case of convex, convex, convex convex-concave, and nonconsistent gradient estimations.  The authors also provide a detailed discussion of the properties of the training algorithms. "
11968,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,resource constraints FEATURE-OF inference. neural architecture search ( NAS ) USED-FOR specialized neural network. neural architecture search ( NAS ) USED-FOR approaches. OFA network USED-FOR specialized sub - network. width CONJUNCTION kernel size. kernel size CONJUNCTION width. depth CONJUNCTION width. width CONJUNCTION depth. kernel size CONJUNCTION resolution. resolution CONJUNCTION kernel size. progressive shrinking algorithm HYPONYM-OF generalized pruning method. generalized pruning method COMPARE pruning. pruning COMPARE generalized pruning method. depth HYPONYM-OF pruning. progressive shrinking algorithm USED-FOR OFA networks. width HYPONYM-OF pruning. kernel size HYPONYM-OF pruning. It USED-FOR subnetworks. hardware platforms CONJUNCTION latency constraints. latency constraints CONJUNCTION hardware platforms. accuracy EVALUATE-FOR It. MobileNetV3 COMPARE EfficientNet. EfficientNet COMPARE MobileNetV3. OFA COMPARE MobileNetV3. MobileNetV3 COMPARE OFA. OFA COMPARE EfficientNet. EfficientNet COMPARE OFA. ImageNet top1 accuracy EVALUATE-FOR MobileNetV3. MobileNetV3 COMPARE MobileNetV3. MobileNetV3 COMPARE MobileNetV3. CO2 emission EVALUATE-FOR OFA. ImageNet top1 accuracy EVALUATE-FOR OFA. accuracy EVALUATE-FOR OFA. SOTA EVALUATE-FOR OFA. DSP classification track CONJUNCTION LPCVC. LPCVC CONJUNCTION DSP classification track. classification track CONJUNCTION detection track. detection track CONJUNCTION classification track. LPCVC CONJUNCTION classification track. classification track CONJUNCTION LPCVC. detection track HYPONYM-OF LPCVC. LPCVC EVALUATE-FOR OFA. DSP classification track EVALUATE-FOR OFA. Code CONJUNCTION pre - trained models. pre - trained models CONJUNCTION Code. Material is edge devices. Generic is it. OtherScientificTerm is diverse architectural settings. Metric is ImageNet top-1 accuracy. ,This paper proposes a novel approach to pruning a neural network in the presence of resource constraints. The approach is based on the idea of neural architecture search (NAS) to search for a specialized sub-networks that can be efficiently pruned. The authors show that this approach can be applied to a wide variety of architectures and architectures with different resource constraints for inference. They also show that the proposed approach is more efficient than existing approaches.  ,This paper proposes a novel approach to pruning a neural network in the presence of resource constraints. The approach is based on the idea of neural architecture search (NAS) to search for a specialized sub-networks that can be efficiently pruned. The authors show that this approach can be applied to a wide variety of architectures and architectures with different resource constraints for inference. They also show that the proposed approach is more efficient than existing approaches.  
11972,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"discrete, symbolic operations USED-FOR they. Neural module networks ( NMNs ) USED-FOR questions. Neural module networks ( NMNs ) USED-FOR executable programs. modules PART-OF executable programs. synthetic visual QA domains EVALUATE-FOR Neural module networks ( NMNs ). models USED-FOR non - synthetic questions. model USED-FOR reasoning. model USED-FOR natural language. open - domain text USED-FOR non - synthetic questions. sorting CONJUNCTION counting. counting CONJUNCTION sorting. arithmetic CONJUNCTION sorting. sorting CONJUNCTION arithmetic. modules USED-FOR symbolic reasoning. probabilistic and differentiable manner USED-FOR symbolic reasoning. counting HYPONYM-OF symbolic reasoning. arithmetic HYPONYM-OF symbolic reasoning. sorting HYPONYM-OF symbolic reasoning. heuristically - obtained question program CONJUNCTION intermediate module output supervision. intermediate module output supervision CONJUNCTION heuristically - obtained question program. inductive bias USED-FOR learning. heuristically - obtained question program USED-FOR learning. intermediate module output supervision USED-FOR inductive bias. heuristically - obtained question program USED-FOR inductive bias. intermediate module output supervision USED-FOR learning. model COMPARE models. models COMPARE model. DROP dataset EVALUATE-FOR models. DROP dataset EVALUATE-FOR model. Task is Answering compositional questions. Method are NMNs, and unsupervised auxiliary loss. ","This paper proposes Neural module networks (NMNs) for compositional QA (QA) problems, where the goal is to answer questions that are composed of discrete, symbolic operations. Answering compositional questions is an important problem, and NMNs have been shown to perform well in synthetic visual QA domains, but they do not perform well when they are used for real-world QA problems. This paper argues that the reason for this is that neural module networks can be seen as executable programs that contain modules that can be interpreted as discrete operations, and that they can be used to perform symbolic reasoning in a probabilistic and differentiable manner.    The authors show that the models are able to answer non-synthetic questions from open-domain text, and they also show that a model trained on natural language is able to perform reasoning in an unsupervised way.  The model is trained on the DROP dataset, where it is shown that the model performs better than other models. The authors also demonstrate that the inductive bias of the learning is enhanced by a heuristically-referred question program and intermediate module output supervision. They also show how to train a model to solve a symbolic reasoning problem (e.g., arithmetic, sorting, and counting) using modules, and how to learn an inductive biases for symbolic reasoning using a heiristically-semi-supervised auxiliary loss. ","This paper proposes Neural module networks (NMNs) for compositional QA (QA) problems, where the goal is to answer questions that are composed of discrete, symbolic operations. Answering compositional questions is an important problem, and NMNs have been shown to perform well in synthetic visual QA domains, but they do not perform well when they are used for real-world QA problems. This paper argues that the reason for this is that neural module networks can be seen as executable programs that contain modules that can be interpreted as discrete operations, and that they can be used to perform symbolic reasoning in a probabilistic and differentiable manner.    The authors show that the models are able to answer non-synthetic questions from open-domain text, and they also show that a model trained on natural language is able to perform reasoning in an unsupervised way.  The model is trained on the DROP dataset, where it is shown that the model performs better than other models. The authors also demonstrate that the inductive bias of the learning is enhanced by a heuristically-referred question program and intermediate module output supervision. They also show how to train a model to solve a symbolic reasoning problem (e.g., arithmetic, sorting, and counting) using modules, and how to learn an inductive biases for symbolic reasoning using a heiristically-semi-supervised auxiliary loss. "
11976,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"Network pruning USED-FOR compressing deep neural networks. approach USED-FOR pruning. connection sensitivity HYPONYM-OF saliency criterion. saliency criterion USED-FOR pruning. initialization conditions USED-FOR connection sensitivity measurements. gradient FEATURE-OF connection sensitivity. signal propagation properties FEATURE-OF pruned networks. image classification tasks EVALUATE-FOR network models. signal propagation perspective CONJUNCTION unsupervised pruning. unsupervised pruning CONJUNCTION signal propagation perspective. pruning USED-FOR arbitrarily - designed architectures. supervision USED-FOR pruning. Method are deep neural networks, data - free method, and pruning at initialization method. Generic is model. OtherScientificTerm is redundant parameters. ","Network pruning is a popular technique for compressing deep neural networks. This paper proposes an approach to pruning based on the saliency criterion, called connection sensitivity, which is a simple yet effective approach to compressing a model. The authors propose a data-free method that prunes the network at initialization, removing redundant parameters. The connection sensitivity measurements are obtained under certain initialization conditions, and the authors show that pruning at initialization can be performed without any additional supervision. They also show that the signal propagation properties of the pruned networks are similar to that of the original model. Experiments on image classification tasks are conducted to demonstrate the effectiveness of the proposed network models. The paper also shows that the pruning can be applied to arbitrarily-designed architectures, and that the connection sensitivity of the model is correlated with the gradient of the gradients of the weights of the network.    The paper is well-motivated from a signal propagation perspective and from an unsupervised pruning perspective. ","Network pruning is a popular technique for compressing deep neural networks. This paper proposes an approach to pruning based on the saliency criterion, called connection sensitivity, which is a simple yet effective approach to compressing a model. The authors propose a data-free method that prunes the network at initialization, removing redundant parameters. The connection sensitivity measurements are obtained under certain initialization conditions, and the authors show that pruning at initialization can be performed without any additional supervision. They also show that the signal propagation properties of the pruned networks are similar to that of the original model. Experiments on image classification tasks are conducted to demonstrate the effectiveness of the proposed network models. The paper also shows that the pruning can be applied to arbitrarily-designed architectures, and that the connection sensitivity of the model is correlated with the gradient of the gradients of the weights of the network.    The paper is well-motivated from a signal propagation perspective and from an unsupervised pruning perspective. "
11980,SP:d5899cba36329d863513b91c2db57675086abc49,"deep neural networks PART-OF machine learning research. training ‘ a priori ’ sparse networks USED-FOR method. training ‘ a priori ’ sparse networks USED-FOR layers. information bandwidth FEATURE-OF layers. sparse topology USED-FOR networks. data - free heuristic USED-FOR topology. Task is fast training. OtherScientificTerm are dense and convolutional layers, memory, and intra - layer topology. Method is sparse neural network initialization scheme. Generic are topologies, and network. ","This paper proposes a method for fast training of deep neural networks in machine learning research. The method is based on training ‘a priori’ sparse networks, where dense and convolutional layers are used to reduce the memory and increase the information bandwidth of the layers. The authors propose a sparse neural network initialization scheme, where the layers are initialized with a sparse topology, and the topologies are learned in an end-to-end manner. They show that this method is able to improve the performance of the network in terms of speed, memory, and performance. They also show that a data-free heuristic can be used to learn the topology of the networks.","This paper proposes a method for fast training of deep neural networks in machine learning research. The method is based on training ‘a priori’ sparse networks, where dense and convolutional layers are used to reduce the memory and increase the information bandwidth of the layers. The authors propose a sparse neural network initialization scheme, where the layers are initialized with a sparse topology, and the topologies are learned in an end-to-end manner. They show that this method is able to improve the performance of the network in terms of speed, memory, and performance. They also show that a data-free heuristic can be used to learn the topology of the networks."
11984,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"recurrent neural networks ( RNNs ) USED-FOR long sequence tasks. exponential explosion CONJUNCTION vanishing of signals. vanishing of signals CONJUNCTION exponential explosion. LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. RNN architectures COMPARE vanilla RNN cells. vanilla RNN cells COMPARE RNN architectures. GRU HYPONYM-OF RNN architectures. LSTM HYPONYM-OF RNN architectures. LSTMs CONJUNCTION GRUs. GRUs CONJUNCTION LSTMs. time scales CONJUNCTION spectral properties. spectral properties CONJUNCTION time scales. spectral properties FEATURE-OF state - to - state Jacobians. mean field theory USED-FOR signal propagation. mean field theory USED-FOR time scales. mean field theory USED-FOR GRUs. time scales USED-FOR signal propagation. LSTMs FEATURE-OF signal propagation. initialization scheme USED-FOR training instabilities. quantities USED-FOR initialization scheme. initialization hyperparameters USED-FOR quantities. it COMPARE initialization. initialization COMPARE it. it EVALUATE-FOR initialization scheme. multiple sequence tasks EVALUATE-FOR initialization scheme. generalization performance EVALUATE-FOR initialization. Generic are network, and they. Method is algorithmic and architectural modifications. OtherScientificTerm is instabilities. ","This paper studies the generalization performance of recurrent neural networks (RNNs) for long sequence tasks. It shows that RNN architectures such as LSTM, GRU, and GRU are more general than vanilla RNN cells in terms of exponential explosion and vanishing of signals. The authors propose algorithmic and architectural modifications to RNNs that mitigate these instabilities. They use mean field theory on time scales, spectral properties of state-to-state Jacobians, and signal propagation in LSTMs and GRUs. They also propose a new initialization scheme to mitigate the training instabilities, and show that it outperforms the standard initialization when it is applied to multiple sequence tasks, and that it is more robust to initialization hyperparameters. ","This paper studies the generalization performance of recurrent neural networks (RNNs) for long sequence tasks. It shows that RNN architectures such as LSTM, GRU, and GRU are more general than vanilla RNN cells in terms of exponential explosion and vanishing of signals. The authors propose algorithmic and architectural modifications to RNNs that mitigate these instabilities. They use mean field theory on time scales, spectral properties of state-to-state Jacobians, and signal propagation in LSTMs and GRUs. They also propose a new initialization scheme to mitigate the training instabilities, and show that it outperforms the standard initialization when it is applied to multiple sequence tasks, and that it is more robust to initialization hyperparameters. "
11988,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,"intra - class diversity CONJUNCTION inter - class similarity. inter - class similarity CONJUNCTION intra - class diversity. deep learning algorithms USED-FOR classification tasks. intra - class diversity FEATURE-OF remote sensing scene image data sets. neural network USED-FOR smoothing operation. neural network USED-FOR approaches. neighboring scene images USED-FOR deep features. siamese network USED-FOR discriminative power. siamese network USED-FOR convolutional neural networks. discriminative power FEATURE-OF convolutional neural networks. neighboring scene images USED-FOR convolutional neural networks. siamese network USED-FOR approach. semantic coherence USED-FOR feature vector. It USED-FOR feature vector. semantic coherence USED-FOR It. approach COMPARE methods. methods COMPARE approach. model COMPARE baseline. baseline COMPARE model. mean squared error value EVALUATE-FOR baseline. disease density estimation task EVALUATE-FOR baseline. mean squared error value EVALUATE-FOR model. disease density estimation task EVALUATE-FOR model. prediction accuracy EVALUATE-FOR model. Metric is accuracy. Method are post - classification methods, and cleanup model. OtherScientificTerm is overhead. Generic is task. ","This paper proposes a method to improve the intra-class diversity and inter-class similarity of remote sensing scene image data sets using deep learning algorithms for classification tasks. Previous approaches rely on a neural network to perform a smoothing operation, which can be expensive and slow to train. The authors propose to use a siamese network to increase the discriminative power of convolutional neural networks trained on neighboring scene images to learn deep features. It is based on semantic coherence to learn a feature vector, which is then used to train a post-classification methods. The proposed approach is evaluated on a disease density estimation task, where the authors show that the proposed approach outperforms existing methods in terms of mean squared error value and prediction accuracy. They also show that their approach is able to reduce the overhead of post-training by removing the need for a cleanup model, and that their model outperforms a baseline on this task.","This paper proposes a method to improve the intra-class diversity and inter-class similarity of remote sensing scene image data sets using deep learning algorithms for classification tasks. Previous approaches rely on a neural network to perform a smoothing operation, which can be expensive and slow to train. The authors propose to use a siamese network to increase the discriminative power of convolutional neural networks trained on neighboring scene images to learn deep features. It is based on semantic coherence to learn a feature vector, which is then used to train a post-classification methods. The proposed approach is evaluated on a disease density estimation task, where the authors show that the proposed approach outperforms existing methods in terms of mean squared error value and prediction accuracy. They also show that their approach is able to reduce the overhead of post-training by removing the need for a cleanup model, and that their model outperforms a baseline on this task."
11992,SP:99c10e038939aa88fc112db10fe801b42360c8dc,"Self - supervised learning USED-FOR monocular depth estimation. geometry USED-FOR supervision. geometry USED-FOR Self - supervised learning. Depth networks USED-FOR representations. representations USED-FOR visual appearance. category - level patterns USED-FOR representations. semantic structure USED-FOR geometric representation learning. fixed pretrained semantic segmentation networks USED-FOR self - supervised representation learning. semantic labels CONJUNCTION proxy losses. proxy losses CONJUNCTION semantic labels. architecture USED-FOR self - supervised representation learning. semantic labels USED-FOR multi - task approach. proxy losses USED-FOR multi - task approach. semantic labels USED-FOR architecture. pixel - adaptive convolutions USED-FOR architecture. fixed pretrained semantic segmentation networks USED-FOR architecture. pixel - adaptive convolutions USED-FOR self - supervised representation learning. two - stage training process USED-FOR common semantic bias. common semantic bias FEATURE-OF dynamic objects. resampling USED-FOR two - stage training process. resampling USED-FOR common semantic bias. method COMPARE state of the art. state of the art COMPARE method. state of the art USED-FOR self - supervised monocular depth prediction. method USED-FOR self - supervised monocular depth prediction. OtherScientificTerm are 3D properties, self - supervised regime, and fine - grained details. ","Self-supervised learning for monocular depth estimation relies on geometry as a form of supervision. Depth networks are trained to learn representations that capture visual appearance and category-level patterns. This paper proposes a new architecture based on pixel-adaptive convolutions that uses fixed pretrained semantic segmentation networks to perform self-supervision representation learning using semantic labels and proxy losses. The authors also propose a multi-task approach that uses semantic labels as well as a two-stage training process to address the common semantic bias in dynamic objects through resampling. Experiments show that the proposed method outperforms the state of the art on the task of self-surrogate depth prediction, and is able to capture 3D properties that are not present in the original self supervised regime.   ","Self-supervised learning for monocular depth estimation relies on geometry as a form of supervision. Depth networks are trained to learn representations that capture visual appearance and category-level patterns. This paper proposes a new architecture based on pixel-adaptive convolutions that uses fixed pretrained semantic segmentation networks to perform self-supervision representation learning using semantic labels and proxy losses. The authors also propose a multi-task approach that uses semantic labels as well as a two-stage training process to address the common semantic bias in dynamic objects through resampling. Experiments show that the proposed method outperforms the state of the art on the task of self-surrogate depth prediction, and is able to capture 3D properties that are not present in the original self supervised regime.   "
11996,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,label - flipping attacks HYPONYM-OF data poisoning attack. deep features USED-FOR linear classifiers. test - time robustness EVALUATE-FOR technique. randomized smoothing USED-FOR approach. Dogfish binary classification task EVALUATE-FOR baseline undefended classifier. ImageNet FEATURE-OF Dogfish binary classification task. certified accuracy EVALUATE-FOR classifier. accuracy EVALUATE-FOR baseline undefended classifier. multi - class classification algorithm USED-FOR label - flipping attacks. Task is label - flipping. Method is classification. Material is multi - class case. ,"Label-flipping attacks are a data poisoning attack in which labels are flipped on a subset of the training data. This paper studies the problem of label flipping, which is a variant of data poisoning where a label is flipped on one of the classes in the training set, but the classification is changed on the other classes. In this paper, the authors consider the multi-class case, where the labels are not flipped on all classes, but only on a small subset of them. The authors propose a new attack method based on randomized smoothing, and show that this technique improves the test-time robustness of the proposed approach. They also show that linear classifiers trained with deep features are more robust to label-flip attacks. They show that the certified accuracy of a baseline undefended classifier trained on the Dogfish binary classification task on ImageNet matches the accuracy of the baseline classifier on the original dataset. They further show that a multi -class classification algorithm is more robust against label-Flipping attacks.","Label-flipping attacks are a data poisoning attack in which labels are flipped on a subset of the training data. This paper studies the problem of label flipping, which is a variant of data poisoning where a label is flipped on one of the classes in the training set, but the classification is changed on the other classes. In this paper, the authors consider the multi-class case, where the labels are not flipped on all classes, but only on a small subset of them. The authors propose a new attack method based on randomized smoothing, and show that this technique improves the test-time robustness of the proposed approach. They also show that linear classifiers trained with deep features are more robust to label-flip attacks. They show that the certified accuracy of a baseline undefended classifier trained on the Dogfish binary classification task on ImageNet matches the accuracy of the baseline classifier on the original dataset. They further show that a multi -class classification algorithm is more robust against label-Flipping attacks."
12000,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"Outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION Outlier detection. novelty detection USED-FOR anomaly detection. Outlier detection PART-OF anomaly detection. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. backdoor poisoning attacks USED-FOR machine learning models. Differential privacy USED-FOR aggregated analysis. dataset USED-FOR aggregated analysis. random noise USED-FOR It. differential privacy USED-FOR outlier detection. differential privacy USED-FOR novelty detection. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. extension USED-FOR poisoning samples. poisoning samples PART-OF backdoor attacks. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. novelty detection CONJUNCTION backdoor attack detection. backdoor attack detection CONJUNCTION novelty detection. differential privacy USED-FOR detection. differential privacy USED-FOR outlier detection. differential privacy USED-FOR backdoor attack detection. differential privacy USED-FOR novelty detection. OtherScientificTerm are distribution, Outliers, novelties, and outliers. Method is aggregation mechanism. ","This paper studies the problem of backdoor poisoning attacks on machine learning models. Outlier detection, novelty detection and anomaly detection are considered in the context of outlier detection and novelty detection. Differential privacy is used for aggregated analysis on a dataset. It is based on random noise. Outliers and novelties are considered to be outliers. The authors propose an extension to poisoning samples in backdoor attacks that allows for differential privacy to be used for outlier detections and to use novelty detection with differential privacy for backdoor attack detection. They also propose a new aggregation mechanism to aggregate the poisoning samples.  ","This paper studies the problem of backdoor poisoning attacks on machine learning models. Outlier detection, novelty detection and anomaly detection are considered in the context of outlier detection and novelty detection. Differential privacy is used for aggregated analysis on a dataset. It is based on random noise. Outliers and novelties are considered to be outliers. The authors propose an extension to poisoning samples in backdoor attacks that allows for differential privacy to be used for outlier detections and to use novelty detection with differential privacy for backdoor attack detection. They also propose a new aggregation mechanism to aggregate the poisoning samples.  "
12004,SP:a5f0e531afd970144169823971d2d039bff752fb,"applications CONJUNCTION safety - critical ones. safety - critical ones CONJUNCTION applications. calibration of uncertainty prediction USED-FOR regression tasks. real - world systems FEATURE-OF regression tasks. definition USED-FOR regression uncertainty. reliability diagrams USED-FOR classification tasks. histogram - based approach USED-FOR classification tasks. definition CONJUNCTION evaluation method. evaluation method CONJUNCTION definition. reliability diagrams USED-FOR histogram - based approach. histogram - based approach USED-FOR evaluation method. synthetic, controlled problem CONJUNCTION object detection bounding - box regression task. object detection bounding - box regression task CONJUNCTION synthetic, controlled problem. Generic are method, and examples. OtherScientificTerm are uncertainty prediction, and empirical uncertainty. Method is scaling - based calibration. ","This paper proposes a new method for calibrating the calibration of uncertainty prediction for regression tasks in real-world systems, both applications and safety-critical ones. The authors propose a new definition of regression uncertainty and an evaluation method based on a histogram-based approach based on reliability diagrams for classification tasks. The paper also introduces a synthetic, controlled problem and an object detection bounding-box regression task, where the authors show that the proposed method is able to calibrate the uncertainty prediction on both synthetic and real world datasets. They also show that scaling-based calibration can be applied to more realistic settings where empirical uncertainty is higher.","This paper proposes a new method for calibrating the calibration of uncertainty prediction for regression tasks in real-world systems, both applications and safety-critical ones. The authors propose a new definition of regression uncertainty and an evaluation method based on a histogram-based approach based on reliability diagrams for classification tasks. The paper also introduces a synthetic, controlled problem and an object detection bounding-box regression task, where the authors show that the proposed method is able to calibrate the uncertainty prediction on both synthetic and real world datasets. They also show that scaling-based calibration can be applied to more realistic settings where empirical uncertainty is higher."
12008,SP:c422afd1df1ac98e23235830585dd0d45513064c,BERT HYPONYM-OF bidirectional Transformer language model. Tensor - Product Representations ( TPRs ) CONJUNCTION BERT. BERT CONJUNCTION Tensor - Product Representations ( TPRs ). structured - representational power CONJUNCTION Tensor - Product Representations ( TPRs ). Tensor - Product Representations ( TPRs ) CONJUNCTION structured - representational power. structured - representational power CONJUNCTION BERT. BERT CONJUNCTION structured - representational power. Tensor - Product Representations ( TPRs ) USED-FOR HUBERT1. BERT PART-OF HUBERT1. structured - representational power PART-OF HUBERT1. HUBERT COMPARE BERT. BERT COMPARE HUBERT. GLUE benchmark CONJUNCTION HANS dataset. HANS dataset CONJUNCTION GLUE benchmark. HANS dataset EVALUATE-FOR model. GLUE benchmark EVALUATE-FOR model. general language structure USED-FOR untangling data - specific semantics. Material is NLP datasets. ,"This paper proposes a bidirectional Transformer language model, called HUBERT1, which is an extension of BERT (BERT). The authors show that HUBBER1 combines structured-representational power, Tensor-Product Representations (TPRs), and BERT in HUBERBERT. The model is evaluated on the GLUE benchmark and the HANS dataset. The authors also show that the proposed model outperforms BERT on both NLP datasets. The paper also shows that untangling data-specific semantics from a general language structure is possible. ","This paper proposes a bidirectional Transformer language model, called HUBERT1, which is an extension of BERT (BERT). The authors show that HUBBER1 combines structured-representational power, Tensor-Product Representations (TPRs), and BERT in HUBERBERT. The model is evaluated on the GLUE benchmark and the HANS dataset. The authors also show that the proposed model outperforms BERT on both NLP datasets. The paper also shows that untangling data-specific semantics from a general language structure is possible. "
12012,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,Multi - agent reinforcement learning HYPONYM-OF problem. communication CONJUNCTION centralized training. centralized training CONJUNCTION communication. particle - based agents USED-FOR cooperative and competitive environments. communication USED-FOR particle - based agents. centralized training USED-FOR particle - based agents. dynamics CONJUNCTION humanoid navigation strategies. humanoid navigation strategies CONJUNCTION dynamics. interaction CONJUNCTION dynamics. dynamics CONJUNCTION interaction. Multi - Agent Reinforcement Learning CONJUNCTION Hierarchical Reinforcement Learning. Hierarchical Reinforcement Learning CONJUNCTION Multi - Agent Reinforcement Learning. multi - agent models USED-FOR simulated humanoid navigation. decentralized methods USED-FOR learning. structure USED-FOR optimization problem. goal - conditioned policies USED-FOR low - level physical controllers. balance CONJUNCTION walking. walking CONJUNCTION balance. low - level physical controllers USED-FOR balance. low - level physical controllers USED-FOR walking. lower - level controllers CONJUNCTION higher - level policies. higher - level policies CONJUNCTION lower - level controllers. decentralized heterogeneous policies USED-FOR multi - agent goal - directed collision avoidance. goal conditioned policies USED-FOR decentralized heterogeneous policies. RL techniques USED-FOR policies. methods USED-FOR RL techniques. Method is partial parameter sharing approach. OtherScientificTerm is hierarchy. Task is multi - agent problem. Material is multi - agent pursuit environment. ,"This paper considers the problem of multi-agent reinforcement learning, which is a special case of the problem known as Multi-agent Reinforcement Learning (MARL). The authors propose a partial parameter sharing approach, where each agent learns a hierarchy of agents that share information about their interactions, dynamics, and humanoid navigation strategies. They show that particle-based agents trained with communication and centralized training in cooperative and competitive environments outperform centralized training. They also show that the learning can be done using decentralized methods.    The authors also demonstrate that the multi- agent models can be used for simulated humanoid navigation, where the agents share information on their interaction, dynamics and dynamics of the environment. The optimization problem is formulated as an optimization problem, and the structure is similar to that of a multi-manual pursuit environment. In this paper, the authors propose to use goal-conditioned policies to train low-level physical controllers for balance and walking, and lower-level controllers and higher-level policies for goal-directed collision avoidance. The authors show that decentralized heterogeneous policies can be trained with goal conditioned policies for multi-agents goal-directed collision avoidance, and that RL techniques can be applied to learn these policies using existing RL techniques. ","This paper considers the problem of multi-agent reinforcement learning, which is a special case of the problem known as Multi-agent Reinforcement Learning (MARL). The authors propose a partial parameter sharing approach, where each agent learns a hierarchy of agents that share information about their interactions, dynamics, and humanoid navigation strategies. They show that particle-based agents trained with communication and centralized training in cooperative and competitive environments outperform centralized training. They also show that the learning can be done using decentralized methods.    The authors also demonstrate that the multi- agent models can be used for simulated humanoid navigation, where the agents share information on their interaction, dynamics and dynamics of the environment. The optimization problem is formulated as an optimization problem, and the structure is similar to that of a multi-manual pursuit environment. In this paper, the authors propose to use goal-conditioned policies to train low-level physical controllers for balance and walking, and lower-level controllers and higher-level policies for goal-directed collision avoidance. The authors show that decentralized heterogeneous policies can be trained with goal conditioned policies for multi-agents goal-directed collision avoidance, and that RL techniques can be applied to learn these policies using existing RL techniques. "
12016,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"spatial convolution layer USED-FOR Graph Neural Networks ( GNNs ). spatial convolution layer USED-FOR feature vector. convolution layer USED-FOR nodes. convolution layer USED-FOR feature vectors. continuous feature space FEATURE-OF feature vectors. GNN USED-FOR graphs. convolution layers USED-FOR local structures. solution USED-FOR GNNs. spatial representation of the graph USED-FOR approach. point - cloud representation of the graph USED-FOR spatial representation. graph embedding method USED-FOR spatial representation. GNN USED-FOR topological structure. local feature extractor USED-FOR GNN. approach USED-FOR local feature extractor. spatial distribution of the locally extracted feature vectors USED-FOR GNN. spatial distribution of the locally extracted feature vectors USED-FOR topological structure. spatial representation USED-FOR graph down - sampling problem. pooling method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE pooling method. Method are neural network, and graph pooling method. OtherScientificTerm is graph. ","Graph Neural Networks (GNNs) are a class of deep learning models that can be seen as a special case of graph convolutional networks (GCNs). In this paper, the authors propose a novel spatial convolution layer for GNNs. The proposed method is based on graph pooling, where a local feature extractor is used to pool features from different nodes of the same graph. The authors show that the proposed pooling method can be used to improve the performance of existing GNN models. ","Graph Neural Networks (GNNs) are a class of deep learning models that can be seen as a special case of graph convolutional networks (GCNs). In this paper, the authors propose a novel spatial convolution layer for GNNs. The proposed method is based on graph pooling, where a local feature extractor is used to pool features from different nodes of the same graph. The authors show that the proposed pooling method can be used to improve the performance of existing GNN models. "
12020,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,it USED-FOR image processing. shift - equivalent prior of images USED-FOR Convolutional layer. max - pooling CONJUNCTION averagepooling. averagepooling CONJUNCTION max - pooling. averagepooling CONJUNCTION stride convolution. stride convolution CONJUNCTION averagepooling. down sampling methods USED-FOR convolutional neural networks ( CNNs ). max - pooling HYPONYM-OF convolutional neural networks ( CNNs ). stride convolution HYPONYM-OF down sampling methods. max - pooling HYPONYM-OF down sampling methods. averagepooling HYPONYM-OF down sampling methods. down sampling USED-FOR shift - equivalent. image classifications EVALUATE-FOR frequency pooling. accuracy CONJUNCTION robustness. robustness CONJUNCTION accuracy. CNNs EVALUATE-FOR frequency pooling. accuracy EVALUATE-FOR frequency pooling. robustness EVALUATE-FOR frequency pooling. ,"This paper proposes a new convolutional layer based on a shift-equivalent prior of images, and proposes to use it for image processing. The authors propose three down sampling methods for convolution neural networks (CNNs): max-pooling, averagepooling and stride convolution. They show that frequency pooling improves the accuracy and robustness of CNNs on image classifications. They also show that down sampling is also beneficial for shift-equalization. ","This paper proposes a new convolutional layer based on a shift-equivalent prior of images, and proposes to use it for image processing. The authors propose three down sampling methods for convolution neural networks (CNNs): max-pooling, averagepooling and stride convolution. They show that frequency pooling improves the accuracy and robustness of CNNs on image classifications. They also show that down sampling is also beneficial for shift-equalization. "
12024,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"high - dimensional state spaces USED-FOR they. images HYPONYM-OF high - dimensional state spaces. technique USED-FOR deep RL agents. technique USED-FOR generalization ability. generalization ability EVALUATE-FOR deep RL agents. randomized ( convolutional ) neural network USED-FOR technique. Monte Carlo approximation USED-FOR inference method. 2D CoinRun CONJUNCTION 3D DeepMind Lab exploration. 3D DeepMind Lab exploration CONJUNCTION 2D CoinRun. 3D DeepMind Lab exploration CONJUNCTION 3D robotics control tasks. 3D robotics control tasks CONJUNCTION 3D DeepMind Lab exploration. it COMPARE regularization and data augmentation methods. regularization and data augmentation methods COMPARE it. 3D robotics control tasks EVALUATE-FOR method. 3D DeepMind Lab exploration EVALUATE-FOR method. 2D CoinRun EVALUATE-FOR method. Method is Deep reinforcement learning ( RL ) agents. Generic are agents, and It. OtherScientificTerm are robust features, varied and randomized environments, and randomization. ","Deep reinforcement learning (RL) agents have been shown to generalize well when they are trained on high-dimensional state spaces (e.g. images). This paper proposes a technique to improve the generalization ability of deep RL agents by training a randomized (convolutional) neural network. The idea is to train the agents to learn robust features that are invariant to perturbations in the input space. The authors propose an inference method based on Monte Carlo approximation. It is shown that the proposed technique is able to improve generalization performance in varied and randomized environments. The method is evaluated on 2D CoinRun, 3D DeepMind Lab exploration, and 3D robotics control tasks, and it outperforms existing regularization and data augmentation methods. ","Deep reinforcement learning (RL) agents have been shown to generalize well when they are trained on high-dimensional state spaces (e.g. images). This paper proposes a technique to improve the generalization ability of deep RL agents by training a randomized (convolutional) neural network. The idea is to train the agents to learn robust features that are invariant to perturbations in the input space. The authors propose an inference method based on Monte Carlo approximation. It is shown that the proposed technique is able to improve generalization performance in varied and randomized environments. The method is evaluated on 2D CoinRun, 3D DeepMind Lab exploration, and 3D robotics control tasks, and it outperforms existing regularization and data augmentation methods. "
12028,SP:31772a9122ec998c7c829bc4813f6147cdc30145,"image classification CONJUNCTION visual question answering. visual question answering CONJUNCTION image classification. models USED-FOR tasks. visual question answering HYPONYM-OF tasks. image classification HYPONYM-OF tasks. explanation approach USED-FOR image similarity models. score USED-FOR model. saliency map USED-FOR explanation method. saliency map USED-FOR image regions. explanations USED-FOR attribute recognition. diverse domains FEATURE-OF datasets. Polyvore Outfits HYPONYM-OF datasets. datasets EVALUATE-FOR approach. Polyvore Outfits HYPONYM-OF diverse domains. Method are deep learning model, and saliency maps. Task is classification. Generic are task, and methods. ","This paper proposes a new explanation approach for image similarity models for two tasks: image classification and visual question answering. The explanation method is based on the saliency map of a deep learning model. The saliency maps are used as a score for the model to determine whether a region of the image should be classified as similar or not. The authors show that the proposed explanation method can be used as an explanation method for any explanation method. They also show that using the proposed approach on two datasets with diverse domains (e.g., Polyvore Outfits) and datasets with different types of explanations (e., attribute recognition).    The main contribution of this paper is that the authors propose an explanation approach that can be applied to any explanation methods.  The idea is to use the proposed method for classification as an example of an explanation for a particular task, and then to use this explanation method to identify the image regions that are most likely to be classified correctly by the proposed methods.","This paper proposes a new explanation approach for image similarity models for two tasks: image classification and visual question answering. The explanation method is based on the saliency map of a deep learning model. The saliency maps are used as a score for the model to determine whether a region of the image should be classified as similar or not. The authors show that the proposed explanation method can be used as an explanation method for any explanation method. They also show that using the proposed approach on two datasets with diverse domains (e.g., Polyvore Outfits) and datasets with different types of explanations (e., attribute recognition).    The main contribution of this paper is that the authors propose an explanation approach that can be applied to any explanation methods.  The idea is to use the proposed method for classification as an example of an explanation for a particular task, and then to use this explanation method to identify the image regions that are most likely to be classified correctly by the proposed methods."
12032,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,WaveFlow HYPONYM-OF small - footprint generative flow. auxiliary losses USED-FOR Parallel WaveNet. small - footprint generative flow USED-FOR raw audio. flowbased models USED-FOR raw audio. autoregressive flow CONJUNCTION bipartite flow. bipartite flow CONJUNCTION autoregressive flow. WaveNet HYPONYM-OF autoregressive flow. autoregressive flow PART-OF flowbased models. WaveGlow HYPONYM-OF bipartite flow. test likelihood CONJUNCTION speech fidelity. speech fidelity CONJUNCTION test likelihood. likelihood - based generative models USED-FOR raw waveforms. test likelihood EVALUATE-FOR likelihood - based generative models. speech fidelity EVALUATE-FOR likelihood - based generative models. WaveFlow USED-FOR high - fidelity speech. WaveFlow COMPARE WaveNet. WaveNet COMPARE WaveFlow. likelihood EVALUATE-FOR WaveFlow. small - footprint WaveFlow COMPARE real - time. real - time COMPARE small - footprint WaveFlow. GPU without engineered inference kernels USED-FOR real - time. Method is maximum likelihood. ,"This paper proposes WaveFlow, a small- footprint generative flow for raw audio that can be applied to existing flowbased models to generate high-fidelity audio. The authors propose to use parallel WaveNet with auxiliary losses, which is a variant of Parallel WaveNet. The autoregressive flow and bipartite flow (WaveNet and WaveGlow) are two popular flow based models for generating raw waveforms, and the authors show that the test likelihood and speech fidelity of such likelihood-based generative models can be improved by the use of WaveFlow. They also show that WaveFlow can be used for high-idelity speech, and that the maximum likelihood of the generated speech is better than that of a real-time model on a GPU without engineered inference kernels.   The authors also demonstrate that the small-size WaveFlow outperforms the state-of-the-art WaveNet in terms of likelihood. ","This paper proposes WaveFlow, a small- footprint generative flow for raw audio that can be applied to existing flowbased models to generate high-fidelity audio. The authors propose to use parallel WaveNet with auxiliary losses, which is a variant of Parallel WaveNet. The autoregressive flow and bipartite flow (WaveNet and WaveGlow) are two popular flow based models for generating raw waveforms, and the authors show that the test likelihood and speech fidelity of such likelihood-based generative models can be improved by the use of WaveFlow. They also show that WaveFlow can be used for high-idelity speech, and that the maximum likelihood of the generated speech is better than that of a real-time model on a GPU without engineered inference kernels.   The authors also demonstrate that the small-size WaveFlow outperforms the state-of-the-art WaveNet in terms of likelihood. "
12036,SP:963e85369978dddcd9e3130bc11453696066bbf3,"GT - GAN USED-FOR translation mapping. graph translator USED-FOR translation mapping. graph convolution and deconvolution layers USED-FOR translation mapping. graph convolution and deconvolution layers PART-OF graph translator. graph convolution and deconvolution layers PART-OF GT - GAN. graph translator PART-OF GT - GAN. GT - GAN COMPARE baseline methods. baseline methods COMPARE GT - GAN. synthetic and realworld datasets EVALUATE-FOR GT - GAN. synthetic and realworld datasets EVALUATE-FOR baseline methods. scalability EVALUATE-FOR GT - GAN. effectiveness EVALUATE-FOR GT - GAN. scalability EVALUATE-FOR baseline methods. effectiveness EVALUATE-FOR baseline methods. GT - GAN COMPARE GraphRNN. GraphRNN COMPARE GT - GAN. GraphRNN CONJUNCTION RandomVAE. RandomVAE CONJUNCTION GraphRNN. GT - GAN COMPARE RandomVAE. RandomVAE COMPARE GT - GAN. Method are Deep graph generation models, unconditioned generative models, and conditional graph discriminator. OtherScientificTerm is global and local features. ","Deep graph generation models can be seen as an extension of unconditioned generative models. This paper proposes a new GAN called GT-GAN to learn a translation mapping using a graph translator. The graph translator consists of graph convolution and deconvolution layers to learn the translation mapping, and a conditional graph discriminator to discriminate between global and local features. The authors show that GT-GAN outperforms baseline methods on both synthetic and realworld datasets, and achieves better scalability and effectiveness than baseline methods. The paper also shows that the proposed GT-GAAN is able to outperform GraphRNN and RandomVAE. ","Deep graph generation models can be seen as an extension of unconditioned generative models. This paper proposes a new GAN called GT-GAN to learn a translation mapping using a graph translator. The graph translator consists of graph convolution and deconvolution layers to learn the translation mapping, and a conditional graph discriminator to discriminate between global and local features. The authors show that GT-GAN outperforms baseline methods on both synthetic and realworld datasets, and achieves better scalability and effectiveness than baseline methods. The paper also shows that the proposed GT-GAAN is able to outperform GraphRNN and RandomVAE. "
12040,SP:962caffd236630c4079bfc7292403c1cc6861c3b,"METAGROSS ( Meta Gated Recursive Controller ) HYPONYM-OF neural sequence modeling unit. gating mechanisms PART-OF METAGROSS. inductive bias USED-FOR learning. hierarchically - structured sequence data USED-FOR learning. language HYPONYM-OF hierarchically - structured sequence data. code generation CONJUNCTION machine translation. machine translation CONJUNCTION code generation. tree traversal CONJUNCTION logical inference. logical inference CONJUNCTION tree traversal. sequential pixel - by - pixel classification CONJUNCTION semantic parsing. semantic parsing CONJUNCTION sequential pixel - by - pixel classification. sorting CONJUNCTION tree traversal. tree traversal CONJUNCTION sorting. semantic parsing CONJUNCTION code generation. code generation CONJUNCTION semantic parsing. machine translation CONJUNCTION polyphonic music modeling. polyphonic music modeling CONJUNCTION machine translation. logical inference CONJUNCTION sequential pixel - by - pixel classification. sequential pixel - by - pixel classification CONJUNCTION logical inference. tasks EVALUATE-FOR approach. polyphonic music modeling HYPONYM-OF recursive logic tasks. sequential pixel - by - pixel classification HYPONYM-OF recursive logic tasks. machine translation HYPONYM-OF recursive logic tasks. logical inference HYPONYM-OF recursive logic tasks. code generation HYPONYM-OF recursive logic tasks. semantic parsing HYPONYM-OF recursive logic tasks. sorting HYPONYM-OF recursive logic tasks. tree traversal HYPONYM-OF recursive logic tasks. Generic is unit. OtherScientificTerm are gating functions, and meta - gating. Method is recurrent model. ","This paper proposes METAGROSS (Meta Gated Recursive Controller) which is a neural sequence modeling unit. The proposed unit is based on the idea of gating mechanisms, where the gating functions are gated by a meta-gating. The authors argue that this inductive bias is useful for learning on hierarchically-structured sequence data (e.g. language). The proposed approach is evaluated on a variety of tasks, including several types of recursive logic tasks (sorting, tree traversal, logical inference, sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation, and polyphonic music modeling). ","This paper proposes METAGROSS (Meta Gated Recursive Controller) which is a neural sequence modeling unit. The proposed unit is based on the idea of gating mechanisms, where the gating functions are gated by a meta-gating. The authors argue that this inductive bias is useful for learning on hierarchically-structured sequence data (e.g. language). The proposed approach is evaluated on a variety of tasks, including several types of recursive logic tasks (sorting, tree traversal, logical inference, sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation, and polyphonic music modeling). "
12044,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,local prior matching ( LPM ) HYPONYM-OF self - supervised objective. self - supervised objective USED-FOR speech recognition. language model USED-FOR learning signal. unlabeled speech USED-FOR learning signal. language model USED-FOR LPM objective. unpaired text CONJUNCTION speech. speech CONJUNCTION unpaired text. it USED-FOR LPM. unpaired text USED-FOR it. speech USED-FOR it. unpaired text USED-FOR LPM. speech USED-FOR LPM. language model USED-FOR LPM. model USED-FOR LPM. unlabeled data USED-FOR model. labeled speech USED-FOR model. WER EVALUATE-FOR LPM. clean and noisy test set EVALUATE-FOR LPM. test sets EVALUATE-FOR fully supervised model. WER EVALUATE-FOR fully supervised model. noisy test set EVALUATE-FOR WER. WER EVALUATE-FOR LPM. noisy data USED-FOR LPM. Method is self - supervised approach. ,"This paper proposes a new self-supervised objective for speech recognition, called local prior matching (LPM). The LPM objective is based on a language model that predicts the learning signal from unlabeled speech. The authors show that it can be applied to both unpaired text and speech, and that LPM can be used to learn from the language model. They also show that the model can be trained from only the labeled speech.   The authors evaluate LPM on WER on a clean and noisy test set, and on a fully supervised model on both test sets. The results show that for the noisy data, LPM is able to learn a good WER, and for the clean data, the model learns well from the unlabeling data. The paper also shows that the proposed LPM approach is more robust to noise in the test set.","This paper proposes a new self-supervised objective for speech recognition, called local prior matching (LPM). The LPM objective is based on a language model that predicts the learning signal from unlabeled speech. The authors show that it can be applied to both unpaired text and speech, and that LPM can be used to learn from the language model. They also show that the model can be trained from only the labeled speech.   The authors evaluate LPM on WER on a clean and noisy test set, and on a fully supervised model on both test sets. The results show that for the noisy data, LPM is able to learn a good WER, and for the clean data, the model learns well from the unlabeling data. The paper also shows that the proposed LPM approach is more robust to noise in the test set."
12048,SP:e6af249608633f1776b608852a00946a5c09a357,"data poisoning USED-FOR fair and robust model training. FR - GAN USED-FOR fair and robust model training. generative adversarial networks ( GANs ) USED-FOR FR - GAN. generative adversarial networks ( GANs ) USED-FOR fair and robust model training. fairness discriminator CONJUNCTION robustness discriminator. robustness discriminator CONJUNCTION fairness discriminator. robustness discriminator HYPONYM-OF discriminators. fairness discriminator HYPONYM-OF discriminators. disparate impact CONJUNCTION equalized odds. equalized odds CONJUNCTION disparate impact. equalized odds CONJUNCTION equal opportunity. equal opportunity CONJUNCTION equalized odds. fairness measures FEATURE-OF framework. disparate impact HYPONYM-OF fairness measures. equalized odds HYPONYM-OF fairness measures. equal opportunity HYPONYM-OF fairness measures. FR - GAN USED-FOR fairness. FR - GAN COMPARE fairness methods. fairness methods COMPARE FR - GAN. fairness CONJUNCTION accuracy. accuracy CONJUNCTION fairness. accuracy EVALUATE-FOR FR - GAN. fairness EVALUATE-FOR FR - GAN. accuracy CONJUNCTION fairness. fairness CONJUNCTION accuracy. fairness EVALUATE-FOR FR - GAN. accuracy EVALUATE-FOR FR - GAN. OtherScientificTerm is bias. Method are model fairness techniques, and generator. Material is validation set. ","This paper proposes a new method for fair and robust training of generative adversarial networks (GANs). The proposed method, called FR-GAN, is based on the idea of data poisoning, which is used to train a discriminator to distinguish between fair and not-fairly-trained GANs. The discriminator is trained in a two-stage fashion: the fairness discriminator discriminates the discriminator and the robustness discriminator, and the two discriminators are trained separately. The authors show that the proposed method outperforms existing methods in terms of accuracy and fairness.  ","This paper proposes a new method for fair and robust training of generative adversarial networks (GANs). The proposed method, called FR-GAN, is based on the idea of data poisoning, which is used to train a discriminator to distinguish between fair and not-fairly-trained GANs. The discriminator is trained in a two-stage fashion: the fairness discriminator discriminates the discriminator and the robustness discriminator, and the two discriminators are trained separately. The authors show that the proposed method outperforms existing methods in terms of accuracy and fairness.  "
12052,SP:6306417f5a300629ec856495781515c6af05a363,"physics - inspired deep learning approach USED-FOR point cloud processing. static background grid CONJUNCTION Lagrangian material space. Lagrangian material space CONJUNCTION static background grid. moving particles USED-FOR Lagrangian material space. Lagrangian material space USED-FOR learning architecture. moving particles USED-FOR learning architecture. static background grid USED-FOR learning architecture. Eulerian - Lagrangian representation USED-FOR particle features. generalized, high - dimensional force field USED-FOR flow velocities. generalized, high - dimensional force field USED-FOR particle features. flow velocities USED-FOR particle features. point cloud classification and segmentation problems EVALUATE-FOR system. geometric machine learning CONJUNCTION physical simulation. physical simulation CONJUNCTION geometric machine learning. PIC / FLIP scheme USED-FOR natural flow. Task is natural flow phenomena in fluid mechanics. OtherScientificTerm are Eulerian world space, and geometric reservoir. ","This paper proposes a physics-inspired deep learning approach for point cloud processing, inspired by natural flow phenomena in fluid mechanics. The learning architecture is based on a combination of a static background grid and Lagrangian material space with moving particles. The Eulerian-Lagrangian representation is used to learn particle features from a generalized, high-dimensional force field to capture flow velocities. The system is evaluated on point cloud classification and segmentation problems, and the results show that the system is able to achieve state-of-the-art performance. The paper also shows that the PIC/FLIP scheme can be applied to natural flow, and that the Eulerians in Euler world space can be used as a geometric reservoir for learning. This is a nice combination of geometric machine learning and physical simulation. ","This paper proposes a physics-inspired deep learning approach for point cloud processing, inspired by natural flow phenomena in fluid mechanics. The learning architecture is based on a combination of a static background grid and Lagrangian material space with moving particles. The Eulerian-Lagrangian representation is used to learn particle features from a generalized, high-dimensional force field to capture flow velocities. The system is evaluated on point cloud classification and segmentation problems, and the results show that the system is able to achieve state-of-the-art performance. The paper also shows that the PIC/FLIP scheme can be applied to natural flow, and that the Eulerians in Euler world space can be used as a geometric reservoir for learning. This is a nice combination of geometric machine learning and physical simulation. "
12056,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"clipping USED-FOR dynamics of iterates. local minimum FEATURE-OF rate of convergence. clipping COMPARE vanilla gradient descent. vanilla gradient descent COMPARE clipping. clipping USED-FOR noise. lens USED-FOR gradient clipping. robustness HYPONYM-OF lens. label noise FEATURE-OF classification. robustness EVALUATE-FOR gradient clipping. gradient clipping USED-FOR label noise. Method are Gradient clipping, deep networks, and optimisation lens. OtherScientificTerm are loss function, and cross - entropy loss. ","Gradient clipping is a popular technique to improve the robustness of deep networks. In this paper, the authors show that clipping can improve the dynamics of iterates in deep networks, and that clipping improves the rate of convergence to the local minimum of the loss function. The authors also propose a new optimisation lens, called cross-entropy loss, which is based on the observation that clipping is more robust to noise than vanilla gradient descent. They also show that gradient clipping can be used as a lens to improve gradient clipping to improve robustness to label noise in classification. ","Gradient clipping is a popular technique to improve the robustness of deep networks. In this paper, the authors show that clipping can improve the dynamics of iterates in deep networks, and that clipping improves the rate of convergence to the local minimum of the loss function. The authors also propose a new optimisation lens, called cross-entropy loss, which is based on the observation that clipping is more robust to noise than vanilla gradient descent. They also show that gradient clipping can be used as a lens to improve gradient clipping to improve robustness to label noise in classification. "
12060,SP:414b06d86e132357a54eb844036b78a232571301,they USED-FOR imitating actions. imitation learning methods USED-FOR imitating actions. state alignment based imitation learning method USED-FOR imitator. expert demonstrations FEATURE-OF state sequences. them PART-OF reinforcement learning framework. local and global perspectives USED-FOR state alignment. regularized policy update objective USED-FOR them. regularized policy update objective USED-FOR reinforcement learning framework. imitation learning settings CONJUNCTION imitation learning settings. imitation learning settings CONJUNCTION imitation learning settings. imitation learning settings EVALUATE-FOR method. imitation learning settings EVALUATE-FOR method. Task is imitation learning problem. Method is dynamics models. ,"This paper proposes a new state alignment based imitation learning method to learn an imitator from expert demonstrations. The authors argue that existing imitation learning methods are limited because they focus on imitating actions and not on the underlying dynamics of the environment. To solve the imitation learning problem, the authors propose to learn a state alignment between the expert demonstrations and the state sequences generated by the dynamics models. The state alignment is learned from both local and global perspectives, and the authors integrate them into a reinforcement learning framework with a regularized policy update objective. The proposed method is evaluated on a variety of imitation learning settings and in both standard and non-simulated environments.","This paper proposes a new state alignment based imitation learning method to learn an imitator from expert demonstrations. The authors argue that existing imitation learning methods are limited because they focus on imitating actions and not on the underlying dynamics of the environment. To solve the imitation learning problem, the authors propose to learn a state alignment between the expert demonstrations and the state sequences generated by the dynamics models. The state alignment is learned from both local and global perspectives, and the authors integrate them into a reinforcement learning framework with a regularized policy update objective. The proposed method is evaluated on a variety of imitation learning settings and in both standard and non-simulated environments."
12064,SP:91761d68086330ce378507c152e72218ed7b2196,"Stochastic gradient descent ( SGD ) USED-FOR deep neural networks. deep gradient boosting ( DGB ) HYPONYM-OF SGD. pseudo - residual targets USED-FOR gradient boosting problem. chain rule USED-FOR back - propagated gradients. boosting problem USED-FOR weight update. linear base learner USED-FOR boosting problem. normalization procedure USED-FOR weight update formula. architecture COMPARE architecture. architecture COMPARE architecture. image recognition tasks EVALUATE-FOR architecture. input normalization layer ( INN ) USED-FOR architecture. normalization layers PART-OF architecture. image recognition tasks EVALUATE-FOR architecture. batch normalization ( BN ) COMPARE INN. INN COMPARE batch normalization ( BN ). CIFAR10 CONJUNCTION ImageNet classification tasks. ImageNet classification tasks CONJUNCTION CIFAR10. ImageNet classification tasks EVALUATE-FOR it. CIFAR10 EVALUATE-FOR it. Method are neural network, and DGB. OtherScientificTerm are intrinsic generalization properties, and forward pass. ","This paper studies the intrinsic generalization properties of Stochastic gradient descent (SGD) for deep neural networks, specifically deep gradient boosting (DGB) of SGD. The authors propose a novel gradient boosting problem based on pseudo-residual targets, where the weights of the neural network are boosted during the forward pass, and the back-propagated gradients are computed using a chain rule. The boosting problem for the weight update is solved by a linear base learner, and a normalization procedure is used to regularize the original weight update formula. The proposed architecture is shown to outperform the previous architecture based on the input normalization layer (INN) on CIFAR10 and ImageNet classification tasks, and it outperforms batch normalization (BN) and INN. The architecture also contains two additional normalization layers, one for each layer of the architecture. ","This paper studies the intrinsic generalization properties of Stochastic gradient descent (SGD) for deep neural networks, specifically deep gradient boosting (DGB) of SGD. The authors propose a novel gradient boosting problem based on pseudo-residual targets, where the weights of the neural network are boosted during the forward pass, and the back-propagated gradients are computed using a chain rule. The boosting problem for the weight update is solved by a linear base learner, and a normalization procedure is used to regularize the original weight update formula. The proposed architecture is shown to outperform the previous architecture based on the input normalization layer (INN) on CIFAR10 and ImageNet classification tasks, and it outperforms batch normalization (BN) and INN. The architecture also contains two additional normalization layers, one for each layer of the architecture. "
12068,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"Differentiable architecture search ( DARTS ) USED-FOR network architectures. reduced memory cost USED-FOR PC - DARTS. training stability EVALUATE-FOR PC - DARTS. batch size USED-FOR PC - DARTS. GPU - days USED-FOR search. top-1 error rate EVALUATE-FOR search. ImageNet EVALUATE-FOR top-1 error rate. Metric are memory and computing overheads, and error rate. OtherScientificTerm are super - network, edges of super - net, and edge - level parameters. Generic are approach, strategy, method, and code. Method are Partially - Connected DARTS, operation search, edge normalization, and architecture search. Material is CIFAR10. ","This paper proposes a differentiable architecture search (DARTS) method for network architectures. The main idea of DARTS is to use differentiable search to reduce memory and computing overheads. The proposed approach is called Partially-Connected DARTS. The key idea of the proposed strategy is to partition the super-net into two sub-supernets, each with a different number of edges. Each super-network is partitioned into two subsets, one for operation search and one for architecture search. The idea is that the edges of super-nets are connected via edge normalization, and that the operation search is performed on the top-1 error rate of each sub-network.    The paper shows that PC-DARTs achieves a reduced memory cost with a reduced error rate, and a reduced training stability with a smaller batch size. The paper also shows that the search can be run in GPU-days with a small number of GPU-hours. The method is also shown to be computationally efficient. The authors also show that the architecture search is computationally tractable.  The code is well-written and easy to follow. Experiments are conducted on CIFAR10 and ImageNet, showing that the proposed PC-ARTS achieves the best results in terms of training stability. The results are also shown on Cifar-10, where the results show that there is a trade-off between the error rate and the number of edge-level parameters. ","This paper proposes a differentiable architecture search (DARTS) method for network architectures. The main idea of DARTS is to use differentiable search to reduce memory and computing overheads. The proposed approach is called Partially-Connected DARTS. The key idea of the proposed strategy is to partition the super-net into two sub-supernets, each with a different number of edges. Each super-network is partitioned into two subsets, one for operation search and one for architecture search. The idea is that the edges of super-nets are connected via edge normalization, and that the operation search is performed on the top-1 error rate of each sub-network.    The paper shows that PC-DARTs achieves a reduced memory cost with a reduced error rate, and a reduced training stability with a smaller batch size. The paper also shows that the search can be run in GPU-days with a small number of GPU-hours. The method is also shown to be computationally efficient. The authors also show that the architecture search is computationally tractable.  The code is well-written and easy to follow. Experiments are conducted on CIFAR10 and ImageNet, showing that the proposed PC-ARTS achieves the best results in terms of training stability. The results are also shown on Cifar-10, where the results show that there is a trade-off between the error rate and the number of edge-level parameters. "
12072,SP:724870046e990376990ba9f73d63d331f61788d7,"control strategies USED-FOR complex control tasks. model - predictive control ( MPC ) CONJUNCTION trajectory optimization. trajectory optimization CONJUNCTION model - predictive control ( MPC ). Model - based control algorithms USED-FOR control tasks. gradients of underlying system dynamics USED-FOR control tasks. gradients of underlying system dynamics USED-FOR Model - based control algorithms. sample efficiency EVALUATE-FOR control tasks. trajectory optimization HYPONYM-OF Model - based control algorithms. model - predictive control ( MPC ) HYPONYM-OF Model - based control algorithms. gradient - based numerical optimization methods COMPARE model - based control methods. model - based control methods COMPARE gradient - based numerical optimization methods. sampling USED-FOR solution space. gradient - based methods CONJUNCTION DRL. DRL CONJUNCTION gradient - based methods. gradient - based methods USED-FOR hybrid method. DRL USED-FOR hybrid method. true gradients USED-FOR convergence rate. convergence rate FEATURE-OF actor. differentiable physical simulator USED-FOR true gradients. convergence rate EVALUATE-FOR modification. differentiable physical simulator USED-FOR modification. true gradients USED-FOR modification. deep deterministic policy gradients ( DDPG ) algorithm USED-FOR algorithm. differentiable half cheetah HYPONYM-OF one. 2D robot control tasks EVALUATE-FOR algorithm. hard contact constraints FEATURE-OF differentiable half cheetah. method USED-FOR DDPG. robustness EVALUATE-FOR method. OtherScientificTerm are initialization, and local minima. Method is Deep reinforcement learning ( DRL ). Metric is computational cost. ","Model-based control algorithms, such as model-predictive control (MPC) and trajectory optimization, have been shown to achieve state-of-the-art sample efficiency on a variety of control strategies for complex control tasks. Model-based algorithms are typically based on the gradients of underlying system dynamics, which can be used to improve the sample efficiency of existing control tasks in terms of the number of samples required to reach the optimal solution.    This paper proposes a hybrid method that combines the benefits of gradient-based numerical optimization methods with model-based methods and DRL. Deep reinforcement learning (DRL) is used to learn a good initialization, which is then used to optimize the solution space by sampling from a solution space. The authors propose a modification to the deep deterministic policy gradients (DDPG) algorithm, which uses true gradients from a differentiable physical simulator to obtain a better convergence rate of the actor to local minima. The proposed algorithm is tested on two 2D robot control tasks, one of which is a one with hard contact constraints, and the other one with soft contact constraints. The results show that the proposed method is able to converge faster than DDPG, and achieves better robustness to perturbations in the environment. ","Model-based control algorithms, such as model-predictive control (MPC) and trajectory optimization, have been shown to achieve state-of-the-art sample efficiency on a variety of control strategies for complex control tasks. Model-based algorithms are typically based on the gradients of underlying system dynamics, which can be used to improve the sample efficiency of existing control tasks in terms of the number of samples required to reach the optimal solution.    This paper proposes a hybrid method that combines the benefits of gradient-based numerical optimization methods with model-based methods and DRL. Deep reinforcement learning (DRL) is used to learn a good initialization, which is then used to optimize the solution space by sampling from a solution space. The authors propose a modification to the deep deterministic policy gradients (DDPG) algorithm, which uses true gradients from a differentiable physical simulator to obtain a better convergence rate of the actor to local minima. The proposed algorithm is tested on two 2D robot control tasks, one of which is a one with hard contact constraints, and the other one with soft contact constraints. The results show that the proposed method is able to converge faster than DDPG, and achieves better robustness to perturbations in the environment. "
12076,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"task - agnostic world graphs USED-FOR complex environment. nodes PART-OF world graph. hierarchical RL framework USED-FOR exploration. world graph nodes CONJUNCTION edges. edges CONJUNCTION world graph nodes. structural and connectivity knowledge USED-FOR exploration. trajectory data USED-FOR binary recurrent variational autoencoder ( VAE ). world graph USED-FOR structural and connectivity knowledge. learning phases PART-OF framework. structural and connectivity knowledge USED-FOR hierarchical RL framework. hierarchical RL framework HYPONYM-OF learning phases. world graphs USED-FOR RL. reward CONJUNCTION learning. learning CONJUNCTION reward. maze tasks EVALUATE-FOR approach. OtherScientificTerm are complex environments, and agents. Method is reinforcement learning ( RL ) agents. ","This paper proposes to use task-agnostic world graphs to learn to explore a complex environment using reinforcement learning (RL) agents. The authors propose a hierarchical RL framework that combines two learning phases: structural and connectivity knowledge from the world graph (i.e., world graph nodes and edges) and a binary recurrent variational autoencoder (VAE) on trajectory data. The proposed approach is evaluated on maze tasks and shows that RL agents trained on world graphs are able to generalize well to complex environments. ","This paper proposes to use task-agnostic world graphs to learn to explore a complex environment using reinforcement learning (RL) agents. The authors propose a hierarchical RL framework that combines two learning phases: structural and connectivity knowledge from the world graph (i.e., world graph nodes and edges) and a binary recurrent variational autoencoder (VAE) on trajectory data. The proposed approach is evaluated on maze tasks and shows that RL agents trained on world graphs are able to generalize well to complex environments. "
12080,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,Neural networks USED-FOR real - world tasks. neural network USED-FOR approximation of any type of continuous functions. non - interpretable black box model USED-FOR neural network. white box network HYPONYM-OF function constructing network. network USED-FOR function blocks. discretized layers USED-FOR network. discretization USED-FOR end - to - end PathNet structure. OtherScientificTerm is continuous functions. Task is reverse engineering. Method is neural networks. ,"Neural networks have been shown to perform well on many real-world tasks. However, it is not well known that a neural network trained on a non-interpretable black box model can be used to perform an approximation of any type of continuous functions. This paper proposes a new function constructing network, called a white box network, that can be applied to any continuous functions, and the authors show that the neural network is able to learn function blocks that are interpretable by reverse engineering. The authors also show that a network trained with discretized layers can learn a network that is interpretable. The paper also shows that the end-to-end PathNet structure can be learned using discretization.  ","Neural networks have been shown to perform well on many real-world tasks. However, it is not well known that a neural network trained on a non-interpretable black box model can be used to perform an approximation of any type of continuous functions. This paper proposes a new function constructing network, called a white box network, that can be applied to any continuous functions, and the authors show that the neural network is able to learn function blocks that are interpretable by reverse engineering. The authors also show that a network trained with discretized layers can learn a network that is interpretable. The paper also shows that the end-to-end PathNet structure can be learned using discretization.  "
12084,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"approach USED-FOR control policies. Imitation learning algorithms USED-FOR control policies. Imitation learning algorithms USED-FOR approach. supervised learning methods USED-FOR control policies. supervised imitation learning USED-FOR policies. imitation learning USED-FOR policies. algorithm USED-FOR behaviors. user - provided reward functions CONJUNCTION reinforcement learning methods. reinforcement learning methods CONJUNCTION user - provided reward functions. user - provided reward functions USED-FOR algorithm. method USED-FOR goal - reaching policies. approach USED-FOR method. approach USED-FOR goal - reaching policies. approach USED-FOR imitation learning settings. self - supervised imitation learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION self - supervised imitation learning. it COMPARE reinforcement learning methods. reinforcement learning methods COMPARE it. reinforcement learning methods USED-FOR goal reaching problems. goal reaching problems EVALUATE-FOR it. OtherScientificTerm are expert demonstrator, expert demonstrations, and demonstrations. Task is multi - task setting. Method is suboptimal policy. Generic is tasks. ","This paper proposes an approach to learn control policies using Imitation learning algorithms. The approach is based on the observation that existing supervised learning methods for learning control policies can be suboptimal in a multi-task setting, where the goal is to reach a goal that the expert demonstrator is unable to reach. The authors propose to use supervised imitation learning to learn policies that are more likely to reach the goal in the presence of expert demonstrations. They show that this approach can be used to learn goal-reaching policies in this setting. The algorithm learns to learn behaviors that are similar to the expert demonstrations, and then uses these learned behaviors to train a suboptimally performing policy. The proposed algorithm can be applied to both user-provided reward functions and reinforcement learning methods.  The authors show that the proposed approach works well in imitation learning settings where the demonstrations are not available (e.g. in the multi-tasks setting). They also show that their approach is able to learn to reach goals that are difficult to reach using a single expert demonstration, and that it can be combined with reinforcement learning to solve goal reaching problems. Finally, they show that in a few tasks, their approach outperforms both self-supervised imitation learning and traditional reinforcement learning.  ","This paper proposes an approach to learn control policies using Imitation learning algorithms. The approach is based on the observation that existing supervised learning methods for learning control policies can be suboptimal in a multi-task setting, where the goal is to reach a goal that the expert demonstrator is unable to reach. The authors propose to use supervised imitation learning to learn policies that are more likely to reach the goal in the presence of expert demonstrations. They show that this approach can be used to learn goal-reaching policies in this setting. The algorithm learns to learn behaviors that are similar to the expert demonstrations, and then uses these learned behaviors to train a suboptimally performing policy. The proposed algorithm can be applied to both user-provided reward functions and reinforcement learning methods.  The authors show that the proposed approach works well in imitation learning settings where the demonstrations are not available (e.g. in the multi-tasks setting). They also show that their approach is able to learn to reach goals that are difficult to reach using a single expert demonstration, and that it can be combined with reinforcement learning to solve goal reaching problems. Finally, they show that in a few tasks, their approach outperforms both self-supervised imitation learning and traditional reinforcement learning.  "
12088,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"Zeno++ USED-FOR non - convex problems. Zeno++ COMPARE approaches. approaches COMPARE Zeno++. OtherScientificTerm are workerserver communications, Byzantine workers, candidate gradient, optimization progress, and Byzantine failures. ","This paper proposes a new algorithm, called Zeno++, for solving non-convex optimization problems with Byzantine workers. The idea is to use workerserver communications between Byzantine workers to update the candidate gradient of the solution of the problem, which is then used to guide the optimization progress. The authors show that the proposed algorithm is more robust to Byzantine failures than previous approaches.  ","This paper proposes a new algorithm, called Zeno++, for solving non-convex optimization problems with Byzantine workers. The idea is to use workerserver communications between Byzantine workers to update the candidate gradient of the solution of the problem, which is then used to guide the optimization progress. The authors show that the proposed algorithm is more robust to Byzantine failures than previous approaches.  "
12092,SP:d16ed9bd4193d99774840783347137e938955b87,"deep neural networks ( DNNs ) HYPONYM-OF Machine learning models. defenses USED-FOR adversarial impact. Lp norm USED-FOR adversarial perturbations. unrestricted ” perturbations USED-FOR image - based visual descriptors. feature squeezing CONJUNCTION adversarially trained model. adversarially trained model CONJUNCTION feature squeezing. JPEG compression CONJUNCTION feature squeezing. feature squeezing CONJUNCTION JPEG compression. semantically aware perturbations USED-FOR JPEG compression. semantically aware perturbations USED-FOR adversarially trained model. semantically aware perturbations USED-FOR feature squeezing. ImageNet CONJUNCTION MSCOCO. MSCOCO CONJUNCTION ImageNet. image classification CONJUNCTION image captioning tasks. image captioning tasks CONJUNCTION image classification. methods USED-FOR image captioning tasks. methods USED-FOR image classification. complex datasets USED-FOR methods. complex datasets USED-FOR image captioning tasks. MSCOCO HYPONYM-OF complex datasets. ImageNet HYPONYM-OF complex datasets. OtherScientificTerm are adversarial examples, and large magnitude perturbations. Method is user studies. Material is semantic adversarial examples. Generic is attacks. ","Machine learning models such as deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples. However, existing defenses to the adversarial impact are not robust to large magnitude perturbations. This paper proposes to use “unrestricted” perturbation to the image-based visual descriptors, which is defined as the Lp norm of the perturbed image. The paper shows that semantically aware perturbed examples are more robust to JPEG compression, feature squeezing, and an adversarially trained model. Experiments are conducted on complex datasets such as ImageNet, MSCOCO, and ImageNet to demonstrate the effectiveness of the proposed methods for image classification and image captioning tasks.    The paper is well-written and well-motivated, and the user studies are well-designed. The authors provide a detailed analysis of the effect of semantic adversarial attacks, and show that the attacks are robust to a wide range of attacks. ","Machine learning models such as deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples. However, existing defenses to the adversarial impact are not robust to large magnitude perturbations. This paper proposes to use “unrestricted” perturbation to the image-based visual descriptors, which is defined as the Lp norm of the perturbed image. The paper shows that semantically aware perturbed examples are more robust to JPEG compression, feature squeezing, and an adversarially trained model. Experiments are conducted on complex datasets such as ImageNet, MSCOCO, and ImageNet to demonstrate the effectiveness of the proposed methods for image classification and image captioning tasks.    The paper is well-written and well-motivated, and the user studies are well-designed. The authors provide a detailed analysis of the effect of semantic adversarial attacks, and show that the attacks are robust to a wide range of attacks. "
12096,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,large datasets USED-FOR latent representations. neural networks USED-FOR latent representations. large datasets USED-FOR neural networks. events CONJUNCTION locations. locations CONJUNCTION events. natural language understanding ( NLU ) system USED-FOR emerging entities. RL trainable controller USED-FOR representation learning. representation learning PART-OF neural encoder. neural encoder CONJUNCTION memory management role. memory management role CONJUNCTION neural encoder. controller USED-FOR read and write operations. external memory USED-FOR read and write operations. named Learning to Control ( LTC ) USED-FOR approach. memory plasticity USED-FOR few - shot learning. system USED-FOR few - shot learning of entity recognition. Stanford Task - Oriented Dialogue dataset USED-FOR few - shot learning of entity recognition. Metric is loss function. Generic is solution. ,This paper proposes a method for few-shot learning of entity recognition from natural language understanding (NLU) systems. The method is based on the observation that NLU systems are able to learn to identify entities from a large number of events and locations. The authors propose to use RL to train a controller to guide the read and write operations from an external memory to a neural encoder and a memory management role. The controller is trained in an end-to-end manner. The proposed method is evaluated on the Stanford Task-Oriented Dialogue (STOD) dataset.  ,This paper proposes a method for few-shot learning of entity recognition from natural language understanding (NLU) systems. The method is based on the observation that NLU systems are able to learn to identify entities from a large number of events and locations. The authors propose to use RL to train a controller to guide the read and write operations from an external memory to a neural encoder and a memory management role. The controller is trained in an end-to-end manner. The proposed method is evaluated on the Stanford Task-Oriented Dialogue (STOD) dataset.  
12105,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,approach USED-FOR recomposable motor primitives. large - scale and diverse manipulation demonstrations FEATURE-OF recomposable motor primitives. approaches USED-FOR primitives. manually defined primitives USED-FOR approaches. manually defined primitives USED-FOR primitives. approaches USED-FOR primitive discovery. complexity EVALUATE-FOR primitive. latent representation USED-FOR primitives. motor primitives CONJUNCTION latent representation. latent representation CONJUNCTION motor primitives. hierarchical reinforcement learning setup USED-FOR robotic manipulation tasks. primitives USED-FOR robotic manipulation tasks. primitives PART-OF hierarchical reinforcement learning setup. ,"This paper proposes an approach to learn recomposable motor primitives from large-scale and diverse manipulation demonstrations. Previous approaches to learning such primitives are based on manually defined primitives, which can be problematic due to the complexity of primitive discovery. This paper proposes to use a hierarchical reinforcement learning setup to learn such primitive discovery, where the primitives can be learned from a latent representation. The main idea is to learn a set of primitive primitives that can be combined with the latent representation, and then use these primitives to solve robotic manipulation tasks.   ","This paper proposes an approach to learn recomposable motor primitives from large-scale and diverse manipulation demonstrations. Previous approaches to learning such primitives are based on manually defined primitives, which can be problematic due to the complexity of primitive discovery. This paper proposes to use a hierarchical reinforcement learning setup to learn such primitive discovery, where the primitives can be learned from a latent representation. The main idea is to learn a set of primitive primitives that can be combined with the latent representation, and then use these primitives to solve robotic manipulation tasks.   "
12114,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"Transfer CONJUNCTION adaptation. adaptation CONJUNCTION Transfer. adaptation USED-FOR unknown environmental dynamics. Transfer PART-OF reinforcement learning ( RL ). adaptation PART-OF reinforcement learning ( RL ). Transfer USED-FOR unknown environmental dynamics. methods USED-FOR adaptation. experience rollouts USED-FOR adaptation. experience rollouts USED-FOR methods. general algorithm USED-FOR probe. general algorithm USED-FOR inference model. general algorithm USED-FOR latent variables of test dynamics. general algorithm USED-FOR single episode transfer. algorithms USED-FOR RL. algorithms USED-FOR variational inference. variational inference CONJUNCTION RL. RL CONJUNCTION variational inference. algorithms USED-FOR modular approach. method COMPARE adaptive approaches. adaptive approaches COMPARE method. baselines USED-FOR robust transfer. method COMPARE baselines. baselines COMPARE method. method USED-FOR robust transfer. single episode test constraint EVALUATE-FOR method. OtherScientificTerm are dense rewards, and rewards. Method is universal control policy. Generic are approach, and it. ","This paper considers the problem of transfer in reinforcement learning (RL) and adaptation to unknown environmental dynamics in the presence of dense rewards. The authors propose two methods for adaptation based on experience rollouts. The first approach is to learn a universal control policy that can be applied to any environment. The second approach is a modular approach, where a general algorithm is used to perform a probe and learn the latent variables of test dynamics. A general algorithm for single episode transfer is also proposed to learn the inference model. Both algorithms are applied to RL and adapted to the modular approach. Experiments show that the proposed method outperforms the baselines for robust transfer under the single episode test constraint and outperforms other adaptive approaches. ","This paper considers the problem of transfer in reinforcement learning (RL) and adaptation to unknown environmental dynamics in the presence of dense rewards. The authors propose two methods for adaptation based on experience rollouts. The first approach is to learn a universal control policy that can be applied to any environment. The second approach is a modular approach, where a general algorithm is used to perform a probe and learn the latent variables of test dynamics. A general algorithm for single episode transfer is also proposed to learn the inference model. Both algorithms are applied to RL and adapted to the modular approach. Experiments show that the proposed method outperforms the baselines for robust transfer under the single episode test constraint and outperforms other adaptive approaches. "
12123,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"AlphaZero COMPARE AlphaGo Zero. AlphaGo Zero COMPARE AlphaZero. generalization EVALUATE-FOR neural net. three - head network architecture USED-FOR action - value head. action - value head USED-FOR Monte Carlo tree search ( MCTS ). search efficiency EVALUATE-FOR action - value head. three - head network USED-FOR AlphaZero style learning paradigm. threehead network architecture USED-FOR AlpahZero learning. game of Hex EVALUATE-FOR threehead network architecture. architecture USED-FOR zero - style iterative learning. neural network models COMPARE those. those COMPARE neural network models. two - head counterpart PART-OF MCTS. MCTS EVALUATE-FOR neural network models. two - head counterpart USED-FOR those. two - head counterpart USED-FOR neural network models. Method are search - based reinforcement learning algorithm AlphaZero, two - head network architecture, and two - head net. Material is chess. OtherScientificTerm is policy. ","This paper proposes a new search-based reinforcement learning algorithm AlphaZero, which is based on the two-head network architecture of AlphaZero. The authors show that AlphaZero outperforms AlphaGo Zero in terms of generalization of a neural net. The main contribution of the paper is the use of a three-head neural network architecture for the action-value head in Monte Carlo tree search (MCTS) to improve the search efficiency. Specifically, the authors propose AlpahZero learning with the three-headed network architecture in the AlphaZero style learning paradigm, where the policy is trained with a two-headed neural network. The threehead network is tested on a game of chess and on the game of Hex, where it is shown that the proposed architecture is effective for zero-style iterative learning. The paper also shows that the neural network models trained with MCTS with the proposed three head architecture outperform those trained with the two -head counterpart in MCTT.  ","This paper proposes a new search-based reinforcement learning algorithm AlphaZero, which is based on the two-head network architecture of AlphaZero. The authors show that AlphaZero outperforms AlphaGo Zero in terms of generalization of a neural net. The main contribution of the paper is the use of a three-head neural network architecture for the action-value head in Monte Carlo tree search (MCTS) to improve the search efficiency. Specifically, the authors propose AlpahZero learning with the three-headed network architecture in the AlphaZero style learning paradigm, where the policy is trained with a two-headed neural network. The threehead network is tested on a game of chess and on the game of Hex, where it is shown that the proposed architecture is effective for zero-style iterative learning. The paper also shows that the neural network models trained with MCTS with the proposed three head architecture outperform those trained with the two -head counterpart in MCTT.  "
12132,SP:89d6d55107b6180109affe7522265c751640ad96,policy transfer PART-OF reinforcement learning. warm initialization CONJUNCTION imitation. imitation CONJUNCTION warm initialization. Policy transfer USED-FOR Reinforcement Learning ( RL ) tasks. imitation USED-FOR Policy transfer. warm initialization USED-FOR Policy transfer. biological world FEATURE-OF behavior transfer. adaptation reward CONJUNCTION environmental reward. environmental reward CONJUNCTION adaptation reward. method USED-FOR policies. sample complexity EVALUATE-FOR policies. sample complexity EVALUATE-FOR method. Material is randomized instances. Task is transfer of policies. Generic is mechanism. OtherScientificTerm is transition differences. ,"Policy transfer is an important problem in reinforcement learning. Policy transfer in Reinforcement Learning (RL) tasks is typically based on the transfer of policies from one environment to another, but this paper proposes a novel mechanism that allows for behavior transfer in the biological world, where the transition differences between the two environments are known. The authors propose to use warm initialization and imitation to achieve Policy transfer through the use of randomized instances. The proposed method is shown to improve the sample complexity of the learned policies in terms of the number of transitions, the adaptation reward, and the environmental reward. ","Policy transfer is an important problem in reinforcement learning. Policy transfer in Reinforcement Learning (RL) tasks is typically based on the transfer of policies from one environment to another, but this paper proposes a novel mechanism that allows for behavior transfer in the biological world, where the transition differences between the two environments are known. The authors propose to use warm initialization and imitation to achieve Policy transfer through the use of randomized instances. The proposed method is shown to improve the sample complexity of the learned policies in terms of the number of transitions, the adaptation reward, and the environmental reward. "
12141,SP:626021101836a635ad2d896bd66951aff31aa846,"scale changes FEATURE-OF tasks. steerable filters USED-FOR scale - equivariant convolutional networks. numerical stability EVALUATE-FOR method. computational efficiency EVALUATE-FOR method. computational efficiency CONJUNCTION numerical stability. numerical stability CONJUNCTION computational efficiency. scale equivariance CONJUNCTION local scale invariance. local scale invariance CONJUNCTION scale equivariance. models COMPARE methods. methods COMPARE models. methods USED-FOR scale equivariance. methods USED-FOR local scale invariance. models USED-FOR scale equivariance. models USED-FOR local scale invariance. MNIST - scale dataset CONJUNCTION STL-10 dataset. STL-10 dataset CONJUNCTION MNIST - scale dataset. supervised learning setting FEATURE-OF STL-10 dataset. Method are Convolutional Neural Networks ( CNNs ), CNNs, and scale - convolution. OtherScientificTerm are translation equivariance, and scale - equivariant. Generic is transformations. ","This paper proposes a new approach to scale-equivariant convolutional neural networks (CNNs).   Convolutional Neural Networks (CNN) have been shown to be equivariant to translation equivariance, but not to scale changes in tasks.  This paper proposes to use steerable filters in order to make scale-Equivariant CNNs more robust to such transformations.  The authors show that scale-convolution can be made to be more robust by adding a steerable filter.  They also show that the proposed method improves computational efficiency, numerical stability, and computational efficiency.  Experiments are conducted on the MNIST-scale dataset and the STL-10 dataset in a supervised learning setting.  Results show that models with these models outperform existing methods in terms of scale equivariancy and local scale invariance.","This paper proposes a new approach to scale-equivariant convolutional neural networks (CNNs).   Convolutional Neural Networks (CNN) have been shown to be equivariant to translation equivariance, but not to scale changes in tasks.  This paper proposes to use steerable filters in order to make scale-Equivariant CNNs more robust to such transformations.  The authors show that scale-convolution can be made to be more robust by adding a steerable filter.  They also show that the proposed method improves computational efficiency, numerical stability, and computational efficiency.  Experiments are conducted on the MNIST-scale dataset and the STL-10 dataset in a supervised learning setting.  Results show that models with these models outperform existing methods in terms of scale equivariancy and local scale invariance."
12150,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,deep learning setups USED-FOR scan completion. paired training data FEATURE-OF supervision. supervision USED-FOR methods. synthetic data EVALUATE-FOR methods. real scans USED-FOR scan completion. point clouds USED-FOR approach. Matterport3D CONJUNCTION KITTI. KITTI CONJUNCTION Matterport3D. ScanNet CONJUNCTION Matterport3D. Matterport3D CONJUNCTION ScanNet. incompleteness USED-FOR realistic completions. ScanNet HYPONYM-OF real - world datasets. real - world datasets EVALUATE-FOR approach. 3D - EPN shape completion dataset EVALUATE-FOR approach. KITTI HYPONYM-OF real - world datasets. Matterport3D HYPONYM-OF real - world datasets. Task is 3D scanning solutions. Material is raw scans. OtherScientificTerm is partial scans. Generic is approaches. ,"This paper proposes a new approach for 3D scanning solutions that can be applied to any deep learning setups for the task of scan completion. The proposed methods are based on the observation that existing methods require supervision from paired training data, which is not always available for raw scans. The authors propose to use synthetic data to validate the effectiveness of the proposed methods, and then use real scans from point clouds to train the proposed approach. The approach is evaluated on three real-world datasets: ScanNet, Matterport3D and KITTI, and is shown to perform well on the 3D-EPN shape completion dataset. It is also shown that incompleteness is not necessary for realistic completions, and that partial scans can be used to improve the performance of existing approaches. ","This paper proposes a new approach for 3D scanning solutions that can be applied to any deep learning setups for the task of scan completion. The proposed methods are based on the observation that existing methods require supervision from paired training data, which is not always available for raw scans. The authors propose to use synthetic data to validate the effectiveness of the proposed methods, and then use real scans from point clouds to train the proposed approach. The approach is evaluated on three real-world datasets: ScanNet, Matterport3D and KITTI, and is shown to perform well on the 3D-EPN shape completion dataset. It is also shown that incompleteness is not necessary for realistic completions, and that partial scans can be used to improve the performance of existing approaches. "
12159,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"models USED-FOR systems. authentication CONJUNCTION anomaly detection. anomaly detection CONJUNCTION authentication. sensor data USED-FOR anomaly detection. sensor data USED-FOR authentication. systems USED-FOR authentication. systems USED-FOR anomaly detection. sensor data USED-FOR systems. learning systems USED-FOR private data. authentication system USED-FOR generative impersonation attacks. optimal strategies USED-FOR attacker. optimal strategies USED-FOR Gaussian source distributions. maximin game USED-FOR problem. they USED-FOR models. real - world data USED-FOR models. Method are Generative neural models, and practical learning approaches. Generic is it. Task are learning, and information theory. Material is nominally - looking artificial data. OtherScientificTerm is optimal strategy. ","Generative neural models are a promising area of research in machine learning, but it is not well-studied because of the lack of practical applications. This paper proposes to study the problem of generative impersonation attacks on models that are used to train systems on sensor data for authentication and anomaly detection.    The authors consider the problem that learning systems should be able to protect private data. They show that a given authentication system is vulnerable to generative impostor attacks. They also show that the optimal strategies for generating Gaussian source distributions that are close to the optimal strategy of the attacker can be found by playing a maximin game, and that they can also be used to fool models trained on real-world data. The authors also provide a theoretical analysis of practical learning approaches that can be applied to the problem.  The paper is well-written and well-motivated. The idea of the problem is interesting, and the idea of learning from nominally-looking artificial data is interesting. However, there are a few concerns that need to be addressed in order for the paper to be accepted as a work of information theory. ","Generative neural models are a promising area of research in machine learning, but it is not well-studied because of the lack of practical applications. This paper proposes to study the problem of generative impersonation attacks on models that are used to train systems on sensor data for authentication and anomaly detection.    The authors consider the problem that learning systems should be able to protect private data. They show that a given authentication system is vulnerable to generative impostor attacks. They also show that the optimal strategies for generating Gaussian source distributions that are close to the optimal strategy of the attacker can be found by playing a maximin game, and that they can also be used to fool models trained on real-world data. The authors also provide a theoretical analysis of practical learning approaches that can be applied to the problem.  The paper is well-written and well-motivated. The idea of the problem is interesting, and the idea of learning from nominally-looking artificial data is interesting. However, there are a few concerns that need to be addressed in order for the paper to be accepted as a work of information theory. "
12168,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,robustness CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robustness. natural accuracy CONJUNCTION robustness. robustness CONJUNCTION natural accuracy. limited data samples USED-FOR high dimensional distribution. sensible adversary USED-FOR defense model. Bayes rule HYPONYM-OF multi - class classifier. 0 - 1 loss FEATURE-OF Bayes rule. 0 - 1 loss FEATURE-OF multi - class classifier. sensible adversarial learning USED-FOR Bayes rule. algorithm USED-FOR robust model. sensible adversarial examples USED-FOR robust model. robust accuracy EVALUATE-FOR PGD attacks. model USED-FOR attacks. CIFAR10 EVALUATE-FOR model. perturbations USED-FOR attacks. Generic is problem. ,"This paper studies the problem of robustness and standard accuracy in the presence of a sensible adversary. The authors consider the setting where a high dimensional distribution over limited data samples is known, and the goal is to balance natural accuracy and robustness. To this end, the authors propose a defense model based on the notion of sensible adversary, which is a multi-class classifier called the Bayes rule, with a 0-1 loss on the loss of a single classifier. They show that under the setting of sensible adversarial learning, the proposed algorithm can learn a robust model that is robust to PGD attacks. They also show that the proposed model can defend against attacks on CIFAR10 under perturbations that are not present in the training data.","This paper studies the problem of robustness and standard accuracy in the presence of a sensible adversary. The authors consider the setting where a high dimensional distribution over limited data samples is known, and the goal is to balance natural accuracy and robustness. To this end, the authors propose a defense model based on the notion of sensible adversary, which is a multi-class classifier called the Bayes rule, with a 0-1 loss on the loss of a single classifier. They show that under the setting of sensible adversarial learning, the proposed algorithm can learn a robust model that is robust to PGD attacks. They also show that the proposed model can defend against attacks on CIFAR10 under perturbations that are not present in the training data."
12177,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,image intensity CONJUNCTION spatial correlation. spatial correlation CONJUNCTION image intensity. spatial correlation FEATURE-OF Feature maps. image intensity FEATURE-OF Feature maps. class probabilities USED-FOR online knowledge distillation methods. adversarial training framework USED-FOR online knowledge distillation method. discriminators USED-FOR feature map distributions. discriminators USED-FOR networks. discriminator USED-FOR feature map. discriminator PART-OF network. network USED-FOR feature map distribution. network USED-FOR discriminator. Discriminators CONJUNCTION networks. networks CONJUNCTION Discriminators. Discriminators PART-OF minimax twoplayer game. minimax twoplayer game USED-FOR networks. cyclic learning scheme USED-FOR networks. method USED-FOR network architectures. method USED-FOR classification task. classification task EVALUATE-FOR network architectures. Generic is small network. ,"This paper proposes a new online knowledge distillation method based on the adversarial training framework. Feature maps are learned based on image intensity and spatial correlation, and class probabilities are used to train the discriminator of a small network. Discriminators and networks are trained in a minimax twoplayer game, where the discriminators are trained to learn feature map distributions. The discriminator is trained on the feature map of the network, and the network is trained to predict the featuremap distribution of a given class. The networks are then trained using a cyclic learning scheme. The proposed method is applied to a variety of network architectures, and is shown to outperform existing methods on a classification task.","This paper proposes a new online knowledge distillation method based on the adversarial training framework. Feature maps are learned based on image intensity and spatial correlation, and class probabilities are used to train the discriminator of a small network. Discriminators and networks are trained in a minimax twoplayer game, where the discriminators are trained to learn feature map distributions. The discriminator is trained on the feature map of the network, and the network is trained to predict the featuremap distribution of a given class. The networks are then trained using a cyclic learning scheme. The proposed method is applied to a variety of network architectures, and is shown to outperform existing methods on a classification task."
12186,SP:e43fc8747f823be6497224696adb92d45150b02d,"natural language processing tasks EVALUATE-FOR word embedding models. word embedding models USED-FOR rich semantic meanings. maximum likelihood estimation CONJUNCTION Bayesian estimation. Bayesian estimation CONJUNCTION maximum likelihood estimation. maximum likelihood estimation USED-FOR model. Bayesian estimation USED-FOR model. model COMPARE baseline methods. baseline methods COMPARE model. baseline methods USED-FOR sentiment analysis. low - frequency words FEATURE-OF sentiment analysis. sentiment analysis EVALUATE-FOR model. it USED-FOR semantic and sentiment analysis tasks. OtherScientificTerm is sentiment information. Method are sentiment word embedding model, and parameter estimating method. Task is semantic and sentiment embeddings. ","This paper studies the problem of learning word embedding models for natural language processing tasks that can capture rich semantic meanings. The authors propose a new model based on maximum likelihood estimation and Bayesian estimation. They show that the proposed model outperforms baseline methods for sentiment analysis on low-frequency words, and that it can be applied to both semantic and sentiment analysis tasks. They also show that their model is able to capture sentiment information more accurately than baseline methods.    The authors also propose a parameter estimating method that can be used to improve the performance of sentiment word embeddings.  The paper is well-written and well-motivated, and the authors have done a good job of explaining their work.  However, the paper suffers from a lack of experimental results, which makes it difficult to judge the effectiveness of the proposed method. The paper also lacks a detailed discussion of the relationship between the proposed parameter estimation method and the performance.  I have read the authors' response and other reviewers' comments, and I am happy to increase my score if the authors can address my concerns.  Overall, I think this is a good paper, but there are a few concerns that need to be addressed. ","This paper studies the problem of learning word embedding models for natural language processing tasks that can capture rich semantic meanings. The authors propose a new model based on maximum likelihood estimation and Bayesian estimation. They show that the proposed model outperforms baseline methods for sentiment analysis on low-frequency words, and that it can be applied to both semantic and sentiment analysis tasks. They also show that their model is able to capture sentiment information more accurately than baseline methods.    The authors also propose a parameter estimating method that can be used to improve the performance of sentiment word embeddings.  The paper is well-written and well-motivated, and the authors have done a good job of explaining their work.  However, the paper suffers from a lack of experimental results, which makes it difficult to judge the effectiveness of the proposed method. The paper also lacks a detailed discussion of the relationship between the proposed parameter estimation method and the performance.  I have read the authors' response and other reviewers' comments, and I am happy to increase my score if the authors can address my concerns.  Overall, I think this is a good paper, but there are a few concerns that need to be addressed. "
12195,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"Noisy labels PART-OF real - world training data. overfitting FEATURE-OF noisy labels. maximal safe set USED-FOR early stopped network. two - phase training method USED-FOR noise - free training. Prestopping HYPONYM-OF two - phase training method. image benchmark data sets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. image benchmark data sets EVALUATE-FOR state - of - the - art methods. test error EVALUATE-FOR state - of - the - art methods. test error EVALUATE-FOR method. Method is deep neural network. OtherScientificTerm are label noise, and real - world noise. ","This paper studies the problem of overfitting to noisy labels in real-world training data. In particular, the authors consider the setting where a deep neural network is trained with label noise, and the goal is to avoid overfitting in the presence of noisy labels. The authors propose a two-phase training method, called Prestopping, to achieve the goal of noise-free training. In the first phase, an early stopped network is used to reach the maximal safe set, and in the second phase, the weights of the earlier stage of the network are used to stop the training. Experiments on image benchmark data sets show that the proposed method outperforms state-of-the-art methods in terms of test error in the absence of label noise. ","This paper studies the problem of overfitting to noisy labels in real-world training data. In particular, the authors consider the setting where a deep neural network is trained with label noise, and the goal is to avoid overfitting in the presence of noisy labels. The authors propose a two-phase training method, called Prestopping, to achieve the goal of noise-free training. In the first phase, an early stopped network is used to reach the maximal safe set, and in the second phase, the weights of the earlier stage of the network are used to stop the training. Experiments on image benchmark data sets show that the proposed method outperforms state-of-the-art methods in terms of test error in the absence of label noise. "
12204,SP:8316872d8b388587bf25f724c80155b25b6cb68e,framework USED-FOR generalization. reinforcement learning USED-FOR actions. regularization metrics USED-FOR generalization. generalization USED-FOR policy. action representations USED-FOR reinforcement learning architecture. policy USED-FOR zero - shot generalization. representation learning method CONJUNCTION policy. policy CONJUNCTION representation learning method. representation learning method USED-FOR zero - shot generalization. sequential decision - making environments USED-FOR zero - shot generalization. Task is intelligence. OtherScientificTerm is action ’s functionality. Method is unsupervised representation learning. ,This paper proposes a new framework to study generalization in reinforcement learning for actions that are learned via reinforcement learning. The generalization is based on regularization metrics that measure how well a policy is able to generalize to unseen states. The authors propose a new reinforcement learning architecture that learns action representations that are invariant to changes in the action’s functionality. They show that the representation learning method and the policy are able to achieve zero-shot generalization on sequential decision-making environments. They also show that unsupervised representation learning can be used to improve generalization. ,This paper proposes a new framework to study generalization in reinforcement learning for actions that are learned via reinforcement learning. The generalization is based on regularization metrics that measure how well a policy is able to generalize to unseen states. The authors propose a new reinforcement learning architecture that learns action representations that are invariant to changes in the action’s functionality. They show that the representation learning method and the policy are able to achieve zero-shot generalization on sequential decision-making environments. They also show that unsupervised representation learning can be used to improve generalization. 
12213,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"word2vec CONJUNCTION GloVe. GloVe CONJUNCTION word2vec. vector word embeddings USED-FOR Deep learning natural language processing models. word2vec HYPONYM-OF vector word embeddings. GloVe HYPONYM-OF vector word embeddings. continuous vectors USED-FOR it. embedding vectors USED-FOR dictionary. word embedding matrix USED-FOR inference. word2ket CONJUNCTION word2ketXS. word2ketXS CONJUNCTION word2ket. word embedding matrix USED-FOR training. quantum computing USED-FOR approaches. accuracy EVALUATE-FOR natural language processing tasks. natural language processing tasks EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. OtherScientificTerm are discrete sequence of words, and GPU memory. Material is text corpus. Generic is embeddings. ","Deep learning natural language processing models use vector word embeddings, such as word2vec and GloVe, to encode a discrete sequence of words in a text corpus. This paper proposes to use it as a set of continuous vectors, which can be stored in a GPU memory. The embedding vectors are then used to construct a dictionary, which is then used for training a word embedding matrix for inference. Experiments are conducted on word2ket, word2k, and word2KXS, and show that the proposed approach improves the accuracy of the natural language language processing tasks. The paper also shows that the embedding of a word in the text corpus can be used as a proxy for the training of a new word, and that the learned embedding can also be used to improve the performance of existing approaches based on quantum computing. ","Deep learning natural language processing models use vector word embeddings, such as word2vec and GloVe, to encode a discrete sequence of words in a text corpus. This paper proposes to use it as a set of continuous vectors, which can be stored in a GPU memory. The embedding vectors are then used to construct a dictionary, which is then used for training a word embedding matrix for inference. Experiments are conducted on word2ket, word2k, and word2KXS, and show that the proposed approach improves the accuracy of the natural language language processing tasks. The paper also shows that the embedding of a word in the text corpus can be used as a proxy for the training of a new word, and that the learned embedding can also be used to improve the performance of existing approaches based on quantum computing. "
12222,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"machine learning approach USED-FOR policy. Imitation Learning ( IL ) HYPONYM-OF machine learning approach. Imitation Learning ( IL ) USED-FOR policy. human players PART-OF video games. IL USED-FOR reinforcement learning ( RL ). they USED-FOR average ” policy. dataset USED-FOR average ” policy. behavioral descriptions USED-FOR state - action pairs. approach USED-FOR neural network policy. behavior description USED-FOR neural network policy. approach USED-FOR policy. human demonstrations USED-FOR build - order planning task. StarCraft II FEATURE-OF build - order planning task. StarCraft II FEATURE-OF human demonstrations. human demonstrations USED-FOR policy. Dimensionality reduction techniques USED-FOR lowdimensional behavioral space. high - dimensional army unit composition USED-FOR lowdimensional behavioral space. UCB1 algorithm USED-FOR policy. policy COMPARE IL baseline approach. IL baseline approach COMPARE policy. Generic is it. Method are IL approaches, and Behavioral Repertoire Imitation Learning ( BRIL ). OtherScientificTerm is repertoire of behaviors. ","This paper proposes a new machine learning approach, called Imitation Learning (IL), to learn a policy from human players in video games.   The key idea of IL for reinforcement learning (RL) is that it learns a set of behavioral descriptions for state-action pairs, and then uses these behavioral descriptions to train a neural network policy conditioned on a behavior description.  The paper shows that IL approaches can be applied to a wide range of behaviors, and that they can learn a “average” policy from a dataset.  This approach is applied to learning a policy based on human demonstrations in a build-order planning task in StarCraft II, where the goal is to learn from human demonstrations.  Experiments show that the policy learned using the UCB1 algorithm outperforms the IL baseline approach, which is based on Behavioral Repertoire Imitation learning (BRIL).    Contributions:  1. Dimensionality reduction techniques are used to reduce the lowdimensional behavioral space from a high-dimensional army unit composition to a low dimensional behavioral space.  2. The policy is trained on a large dataset of human demonstrations, where it is shown to outperform BRIL.","This paper proposes a new machine learning approach, called Imitation Learning (IL), to learn a policy from human players in video games.   The key idea of IL for reinforcement learning (RL) is that it learns a set of behavioral descriptions for state-action pairs, and then uses these behavioral descriptions to train a neural network policy conditioned on a behavior description.  The paper shows that IL approaches can be applied to a wide range of behaviors, and that they can learn a “average” policy from a dataset.  This approach is applied to learning a policy based on human demonstrations in a build-order planning task in StarCraft II, where the goal is to learn from human demonstrations.  Experiments show that the policy learned using the UCB1 algorithm outperforms the IL baseline approach, which is based on Behavioral Repertoire Imitation learning (BRIL).    Contributions:  1. Dimensionality reduction techniques are used to reduce the lowdimensional behavioral space from a high-dimensional army unit composition to a low dimensional behavioral space.  2. The policy is trained on a large dataset of human demonstrations, where it is shown to outperform BRIL."
12231,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"stochastic gradient descent optimization USED-FOR lottery ticket. weight magnitude FEATURE-OF model. gradual pruning COMPARE one - shot pruning. one - shot pruning COMPARE gradual pruning. accuracy EVALUATE-FOR one - shot pruning. accuracy EVALUATE-FOR gradual pruning. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. ImageNet EVALUATE-FOR ResNet architectures. CIFAR10 EVALUATE-FOR ResNet architectures. ImageNet FEATURE-OF ResNet50. Task is lottery ticket hypothesis. Method are small, sparsified neural networks, sparsified model, iterative pruning, and memorization capacity analysis. Generic is network. OtherScientificTerm are rewinding, transferability of the winning lottery tickets, winning ticket, structure of winning lottery tickets, lottery tickets, Pruning, complex patterns, and pruning rate. Metric is Top-1 accuracy. ","This paper investigates the lottery ticket hypothesis, which claims that small, sparsified neural networks are able to transfer winning tickets to larger, sparser ones. The authors propose to use stochastic gradient descent optimization to find the winning lottery ticket for each layer of the network. They show that the transferability of winning lottery tickets is a function of the weight magnitude of the model, and that the winning ticket can be found by rewinding the weights of the sparsification layer. The paper also shows that the structure of winning tickets is similar to that of lottery tickets.  The authors also show that gradual pruning is more effective than one-shot pruning in terms of accuracy.  Experiments on ResNet architectures on CIFAR10 and ImageNet for ResNet50 on ImageNet show that gradually pruning can achieve Top-1 accuracy. Pruning can also be performed to remove complex patterns in the training data.    In addition, the authors show that iterative pruning, where the network is pruned in a step-by-step fashion, can be used as a way to reduce the amount of memorization in the network, which can be done in a single step. The pruning rate can be chosen to be as low as possible.  Finally, they provide a memorization capacity analysis, which shows that if the network can memorize the entire training set, then the sparsity of the training set can be reduced.","This paper investigates the lottery ticket hypothesis, which claims that small, sparsified neural networks are able to transfer winning tickets to larger, sparser ones. The authors propose to use stochastic gradient descent optimization to find the winning lottery ticket for each layer of the network. They show that the transferability of winning lottery tickets is a function of the weight magnitude of the model, and that the winning ticket can be found by rewinding the weights of the sparsification layer. The paper also shows that the structure of winning tickets is similar to that of lottery tickets.  The authors also show that gradual pruning is more effective than one-shot pruning in terms of accuracy.  Experiments on ResNet architectures on CIFAR10 and ImageNet for ResNet50 on ImageNet show that gradually pruning can achieve Top-1 accuracy. Pruning can also be performed to remove complex patterns in the training data.    In addition, the authors show that iterative pruning, where the network is pruned in a step-by-step fashion, can be used as a way to reduce the amount of memorization in the network, which can be done in a single step. The pruning rate can be chosen to be as low as possible.  Finally, they provide a memorization capacity analysis, which shows that if the network can memorize the entire training set, then the sparsity of the training set can be reduced."
12240,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"low - confidence detections USED-FOR unknowns. CNNs USED-FOR product operation. UDN USED-FOR product operation. CNNs USED-FOR UDN. convolutional layers USED-FOR features. product operations USED-FOR UDN. UDN USED-FOR detecting unknowns. learning process USED-FOR UDN. information - theoretic regularization strategy USED-FOR learning process. information - theoretic regularization strategy USED-FOR UDN. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. SVHN HYPONYM-OF benchmark image datasets. MNIST HYPONYM-OF benchmark image datasets. CIFAR-100 HYPONYM-OF benchmark image datasets. CIFAR-10 HYPONYM-OF benchmark image datasets. UDN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE UDN. accuracy EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR UDN. classification accuracy EVALUATE-FOR UDN. Method is image classification systems. Generic are they, and problem. ","This paper proposes a novel method for detecting unknown unknowns in image classification systems. The key idea is to use low-confidence detections to detect unknowns. The authors propose UDN, which uses CNNs to model the product operation of a product operation and then uses convolutional layers to extract features from the product operations. The learning process of UDN is regularized with an information-theoretic regularization strategy. Experiments are conducted on several benchmark image datasets (MNIST, CIFAR-10, CifAR-100, and SVHN). The authors show that UDN outperforms state-of-the-art methods in terms of accuracy and classification accuracy. ","This paper proposes a novel method for detecting unknown unknowns in image classification systems. The key idea is to use low-confidence detections to detect unknowns. The authors propose UDN, which uses CNNs to model the product operation of a product operation and then uses convolutional layers to extract features from the product operations. The learning process of UDN is regularized with an information-theoretic regularization strategy. Experiments are conducted on several benchmark image datasets (MNIST, CIFAR-10, CifAR-100, and SVHN). The authors show that UDN outperforms state-of-the-art methods in terms of accuracy and classification accuracy. "
12249,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"approach USED-FOR approximate Bayesian inference. Variational inference ( VI ) USED-FOR approximate Bayesian inference. approximate Bayesian inference USED-FOR highly parameterized models. Variational inference ( VI ) HYPONYM-OF approach. deep neural networks HYPONYM-OF highly parameterized models. method USED-FOR highly flexible variational distributions. coarse approximation USED-FOR method. benchmark tasks EVALUATE-FOR method. log - likelihood CONJUNCTION ELBO. ELBO CONJUNCTION log - likelihood. variational inference methods USED-FOR deep learning. method COMPARE variational inference methods. variational inference methods COMPARE method. method USED-FOR deep learning. log - likelihood EVALUATE-FOR variational inference methods. ELBO EVALUATE-FOR variational inference methods. log - likelihood EVALUATE-FOR method. ELBO EVALUATE-FOR method. CIFAR10 EVALUATE-FOR residual networks. Task is variational inference. Generic are distribution, and it. OtherScientificTerm are variational families, and Evidence Lower BOund. Method are larger scale models, and VI. ","This paper proposes a new approach to approximate Bayesian inference, Variational inference (VI) for highly parameterized models (e.g., deep neural networks). The authors propose a method for learning highly flexible variational distributions, where the distribution can be learned in an unsupervised way. The method is based on a coarse approximation to the distribution, and the authors show that this method can be applied to larger scale models, and that it can be combined with existing variational inference methods for deep learning. The proposed method is evaluated on a number of benchmark tasks, and is shown to outperform the existing state-of-the-art in terms of log-likelihood and ELBO. Experiments on residual networks trained on CIFAR10 show that the proposed method outperforms the state of the art on the task of deep learning, and also outperforms other popular variational methods.    The main contribution of the paper is the introduction of a new family of variational families, called Evidence Lower BOund, which is a generalization of the existing VI family. ","This paper proposes a new approach to approximate Bayesian inference, Variational inference (VI) for highly parameterized models (e.g., deep neural networks). The authors propose a method for learning highly flexible variational distributions, where the distribution can be learned in an unsupervised way. The method is based on a coarse approximation to the distribution, and the authors show that this method can be applied to larger scale models, and that it can be combined with existing variational inference methods for deep learning. The proposed method is evaluated on a number of benchmark tasks, and is shown to outperform the existing state-of-the-art in terms of log-likelihood and ELBO. Experiments on residual networks trained on CIFAR10 show that the proposed method outperforms the state of the art on the task of deep learning, and also outperforms other popular variational methods.    The main contribution of the paper is the introduction of a new family of variational families, called Evidence Lower BOund, which is a generalization of the existing VI family. "
12258,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,user - defined metrics USED-FOR reinforcement learning. reinforcement learning USED-FOR Sequence generation models. correlated Monte Carlo ( MC ) rollouts USED-FOR variance control. correlated Monte Carlo ( MC ) rollouts USED-FOR policy gradient estimator. policy gradient estimator USED-FOR contextual generation of categorical sequences. rollouts USED-FOR model uncertainty. correlated MC rollouts USED-FOR binary - tree softmax models. large vocabulary scenarios FEATURE-OF high generation cost. binary actions FEATURE-OF categorical action. neural program synthesis CONJUNCTION image captioning. image captioning CONJUNCTION neural program synthesis. image captioning EVALUATE-FOR methods. neural program synthesis EVALUATE-FOR methods. gradient variance EVALUATE-FOR methods. Generic is method. OtherScientificTerm is correlation. ,This paper proposes a method for learning a policy gradient estimator for the contextual generation of categorical sequences using reinforcement learning using user-defined metrics. Sequence generation models have been widely used in reinforcement learning. This paper proposes to use correlated Monte Carlo (MC) rollouts to improve variance control in policy gradient estimation for contextual generation. The authors show that correlated MC rollouts can be used to train binary-tree softmax models and that the rollouts are able to control the model uncertainty.  The authors also show that the proposed method can be applied to large vocabulary scenarios with high generation cost in the presence of large correlation between the categorical action and the binary actions. The proposed methods are evaluated on neural program synthesis and image captioning and show that their methods can reduce the gradient variance. ,This paper proposes a method for learning a policy gradient estimator for the contextual generation of categorical sequences using reinforcement learning using user-defined metrics. Sequence generation models have been widely used in reinforcement learning. This paper proposes to use correlated Monte Carlo (MC) rollouts to improve variance control in policy gradient estimation for contextual generation. The authors show that correlated MC rollouts can be used to train binary-tree softmax models and that the rollouts are able to control the model uncertainty.  The authors also show that the proposed method can be applied to large vocabulary scenarios with high generation cost in the presence of large correlation between the categorical action and the binary actions. The proposed methods are evaluated on neural program synthesis and image captioning and show that their methods can reduce the gradient variance. 
12267,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"Goal recognition USED-FOR applications. goal recognition design USED-FOR online goal recognition process. goal recognition control HYPONYM-OF stages. deceptive opponent modeling HYPONYM-OF stages. proactively static interdiction USED-FOR goal recognition control. worst case distinctiveness ( wcd ) USED-FOR nondistinctive path. approach USED-FOR goal recognition process. opponent ’s deceptive behavior USED-FOR approach. opponent ’s deceptive behavior USED-FOR goal recognition process. OtherScientificTerm are hard action removal, and wcd. Method is S - GRC. ","Goal recognition is an important problem in many applications. This paper proposes a new goal recognition design for the online goal recognition process. Two stages are considered: (1) goal recognition control, where proactively static interdiction is used, and (2) deceptive opponent modeling, where hard action removal is used. The main contribution of the paper is the use of worst case distinctiveness (wcd) to identify a nondistinctive path, which is used in both stages. The paper also proposes an approach to improve the performance of the proposed approach based on the opponent’s deceptive behavior. The proposed approach, S-GRC, is based on wcd. ","Goal recognition is an important problem in many applications. This paper proposes a new goal recognition design for the online goal recognition process. Two stages are considered: (1) goal recognition control, where proactively static interdiction is used, and (2) deceptive opponent modeling, where hard action removal is used. The main contribution of the paper is the use of worst case distinctiveness (wcd) to identify a nondistinctive path, which is used in both stages. The paper also proposes an approach to improve the performance of the proposed approach based on the opponent’s deceptive behavior. The proposed approach, S-GRC, is based on wcd. "
12276,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,large mini - batch sizes USED-FOR Generative Adversarial Networks ( GANs ). Coreset - selection USED-FOR active learning. Coreset - selection USED-FOR method. Coreset - selection USED-FOR batch. training time CONJUNCTION memory usage. memory usage CONJUNCTION training time. it USED-FOR GANs. GANs USED-FOR anomaly detection. training time EVALUATE-FOR GAN variants. it USED-FOR dropped modes. technique USED-FOR GAN variants. memory usage EVALUATE-FOR GAN variants. it USED-FOR anomaly detection. dropped modes PART-OF synthetic dataset. memory usage EVALUATE-FOR technique. training time EVALUATE-FOR technique. synthetic dataset EVALUATE-FOR it. Method is GAN. Material is real ’ images. Generic is them. ,"This paper proposes a method to reduce the memory footprint of Generative Adversarial Networks (GANs) with large mini-batch sizes. The method is based on Coreset-selection for active learning, where a GAN is trained on a set of ‘real’ images, and the goal is to select a subset of them to be used in the next batch. The authors show that Coreset -selection can be applied to any batch, and that it can reduce the training time and memory usage of GANs for anomaly detection. The technique is applied to several GAN variants, and it is shown to significantly reduce the number of dropped modes in a synthetic dataset, while maintaining the same training time.","This paper proposes a method to reduce the memory footprint of Generative Adversarial Networks (GANs) with large mini-batch sizes. The method is based on Coreset-selection for active learning, where a GAN is trained on a set of ‘real’ images, and the goal is to select a subset of them to be used in the next batch. The authors show that Coreset -selection can be applied to any batch, and that it can reduce the training time and memory usage of GANs for anomaly detection. The technique is applied to several GAN variants, and it is shown to significantly reduce the number of dropped modes in a synthetic dataset, while maintaining the same training time."
12285,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,artificial neurons PART-OF recurrent neural networks ( RNNs ). maximum likelihood USED-FOR recurrent neural networks ( RNNs ). restorative Brownian motion CONJUNCTION hand - drawn sketch dataset. hand - drawn sketch dataset CONJUNCTION restorative Brownian motion. information plane FEATURE-OF RNNs. hand - drawn sketch dataset HYPONYM-OF datasets. restorative Brownian motion HYPONYM-OF datasets. RNNs USED-FOR predictive information. maximum likelihood CONJUNCTION contrastive loss training. contrastive loss training CONJUNCTION maximum likelihood. noise USED-FOR hidden state. predictive information USED-FOR maximum likelihood. RNNs USED-FOR contrastive loss training. predictive information USED-FOR contrastive loss training. noise USED-FOR past information. Method is biological neurons. ,"This paper investigates the role of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood and contrastive loss on the information plane of RNNs. The authors present experiments on two datasets: restorative Brownian motion and the hand-drawn sketch dataset. They show that biological neurons are able to encode predictive information that can be used to improve the performance of maximum likelihood or contrastive losses. They also show that the predictive information from RNN can be leveraged to improve performance of contrastive training using maximum likelihood.   The main contribution of this paper is to show that noise in the hidden state of a RNN is able to capture the past information, and that the noise can also be used as a proxy for past information in contrastive learning.","This paper investigates the role of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood and contrastive loss on the information plane of RNNs. The authors present experiments on two datasets: restorative Brownian motion and the hand-drawn sketch dataset. They show that biological neurons are able to encode predictive information that can be used to improve the performance of maximum likelihood or contrastive losses. They also show that the predictive information from RNN can be leveraged to improve performance of contrastive training using maximum likelihood.   The main contribution of this paper is to show that noise in the hidden state of a RNN is able to capture the past information, and that the noise can also be used as a proxy for past information in contrastive learning."
12294,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"techniques USED-FOR delusion. methods USED-FOR delusional bias. Q - approximators USED-FOR methods. penalization scheme USED-FOR Q - labels. search framework USED-FOR premature ( implicit ) policy commitments. search framework USED-FOR Q - approximators. methods USED-FOR Q - learning. Atari games EVALUATE-FOR Q - learning. Atari games EVALUATE-FOR methods. Task is Delusional bias. Method are approximate Q - learning, and tabular value estimates. OtherScientificTerm are greedy policy class, and expressible policy class. ","Delusional bias is a well-studied problem in approximate Q-learning. This paper proposes a number of techniques to tackle the problem of delusion. The authors propose two methods to address the delusional bias based on Q-approximators. They propose a penalization scheme to penalize the Q-labels of the greedy policy class, and a search framework to find the best Q-absorbators to avoid premature (implicit) policy commitments. They evaluate their methods on several Atari games and show that their methods outperform the state-of-the-art methods for Q-learning on a variety of Atari games. They also show that tabular value estimates can be used as a way to measure the extent to which a policy is in fact in a greedy class. ","Delusional bias is a well-studied problem in approximate Q-learning. This paper proposes a number of techniques to tackle the problem of delusion. The authors propose two methods to address the delusional bias based on Q-approximators. They propose a penalization scheme to penalize the Q-labels of the greedy policy class, and a search framework to find the best Q-absorbators to avoid premature (implicit) policy commitments. They evaluate their methods on several Atari games and show that their methods outperform the state-of-the-art methods for Q-learning on a variety of Atari games. They also show that tabular value estimates can be used as a way to measure the extent to which a policy is in fact in a greedy class. "
12303,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"approaches USED-FOR unsupervised object - oriented scene representation learning. spatial - attention CONJUNCTION scene - mixture approaches. scene - mixture approaches CONJUNCTION spatial - attention. scene - mixture approaches USED-FOR approaches. spatial - attention USED-FOR approaches. spatial - attention CONJUNCTION scene - mixture approaches. scene - mixture approaches CONJUNCTION spatial - attention. generative latent variable model USED-FOR unified probabilistic modeling framework. scene - mixture approaches USED-FOR unified probabilistic modeling framework. spatial - attention USED-FOR unified probabilistic modeling framework. SPACE HYPONYM-OF generative latent variable model. factorized object representations USED-FOR foreground objects. SPACE USED-FOR factorized object representations. SPACE USED-FOR methods. SPACE USED-FOR scalability problems. parallel spatial - attention USED-FOR methods. IODINE CONJUNCTION GENESIS. GENESIS CONJUNCTION IODINE. SPACE COMPARE SPAIR. SPAIR COMPARE SPACE. Atari CONJUNCTION 3D - Rooms. 3D - Rooms CONJUNCTION Atari. SPACE COMPARE IODINE. IODINE COMPARE SPACE. SPAIR CONJUNCTION IODINE. IODINE CONJUNCTION SPAIR. SPACE COMPARE GENESIS. GENESIS COMPARE SPACE. OtherScientificTerm are complex multi - object scenes, higher - level cognition, and complex morphology. Task is modeling real - world scenes. Generic is models. ","This paper proposes a unified probabilistic modeling framework based on spatial-attention and scene-mixture approaches for unsupervised object-oriented scene representation learning. The proposed approach, called SPACE, is based on a generative latent variable model that is used to model a unified approach to modeling real-world scenes with complex multi-object scenes. The authors claim that existing approaches are limited by the limitations of the spatial -attention, scene-mixing, and other existing approaches in the context of unsupervisory object-orientation learning.    The authors propose to use SPACE as a factorized object representations for foreground objects, which can be applied to existing methods that rely on parallel spatial attention. They show that the proposed SPACE can be used to solve scalability problems that existing methods cannot, and demonstrate that SPACE outperforms SPAIR, IODINE, and GENESIS on Atari and 3D-Rooms. They also show that their models are able to capture higher-level cognition, which is important for modeling complex morphology. ","This paper proposes a unified probabilistic modeling framework based on spatial-attention and scene-mixture approaches for unsupervised object-oriented scene representation learning. The proposed approach, called SPACE, is based on a generative latent variable model that is used to model a unified approach to modeling real-world scenes with complex multi-object scenes. The authors claim that existing approaches are limited by the limitations of the spatial -attention, scene-mixing, and other existing approaches in the context of unsupervisory object-orientation learning.    The authors propose to use SPACE as a factorized object representations for foreground objects, which can be applied to existing methods that rely on parallel spatial attention. They show that the proposed SPACE can be used to solve scalability problems that existing methods cannot, and demonstrate that SPACE outperforms SPAIR, IODINE, and GENESIS on Atari and 3D-Rooms. They also show that their models are able to capture higher-level cognition, which is important for modeling complex morphology. "
12312,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,classification tasks EVALUATE-FOR Convolutional Neural Networks ( CNN ). depthwise convolution CONJUNCTION pointwise convolution. pointwise convolution CONJUNCTION depthwise convolution. convolution CONJUNCTION depthwise convolution. depthwise convolution CONJUNCTION convolution. convolution PART-OF depthwise separable convolution. accuracies EVALUATE-FOR convolution. method USED-FOR compressing CNN. FALCON HYPONYM-OF method. FALCON USED-FOR compressing CNN. mathematical formulation USED-FOR convolution kernel. FALCON USED-FOR convolution methods. depthwise separable convolution USED-FOR convolution methods. EHP USED-FOR convolution methods. compression CONJUNCTION computation reduction rates. computation reduction rates CONJUNCTION compression. computation reduction rates EVALUATE-FOR generalized version rank - k FALCON. accuracy EVALUATE-FOR generalized version rank - k FALCON. compression EVALUATE-FOR generalized version rank - k FALCON. FALCON PART-OF convolution unit ShuffleUnitV2. FALCON USED-FOR FALCON - branch. FALCON - branch COMPARE methods. methods COMPARE FALCON - branch. FALCON COMPARE methods. methods COMPARE FALCON. FALCON CONJUNCTION FALCON - branch. FALCON - branch CONJUNCTION FALCON. FALCON - branch COMPARE CNN models. CNN models COMPARE FALCON - branch. methods COMPARE CNN models. CNN models COMPARE methods. depthwise separable convolution USED-FOR methods. compression EVALUATE-FOR CNN models. accuracy EVALUATE-FOR CNN models. parameters CONJUNCTION floating - point operations. floating - point operations CONJUNCTION parameters. rank - k FALCON COMPARE convolution. convolution COMPARE rank - k FALCON. accuracy EVALUATE-FOR convolution. floating - point operations USED-FOR rank - k FALCON. parameters USED-FOR rank - k FALCON. accuracy EVALUATE-FOR rank - k FALCON. Generic is they. Method is heuristic approaches. ,"This paper proposes a new method for compressing convolutional neural networks (CNN) for classification tasks. The proposed method, called FALCON, is based on a mathematical formulation of the convolution kernel, which is a generalization of depthwise separable convolution, which combines convolution with depthwise convolution and pointwise convolutions. The authors show that FALcon can be applied to existing convolution methods based on EHP, and show that they can achieve comparable accuracies to standard convolution.  The authors also propose a new convolution unit ShuffleUnitV2, where FAL CON is incorporated into the original Convolutional Neural Networks (CNN). The authors further show that the proposed method can be used to improve the compression and computation reduction rates of a generalized version of convolution (FALCON-branch).  Experiments are conducted on several heuristic approaches to compress CNNs, and the authors compare the performance of the proposed methods with standard CNN models in terms of compression, computation, and accuracy. The results show that rank-k FALCon outperforms convolution in all cases, and can be combined with convolution to achieve better performance.  Finally, the authors also show that a variant of FALON, which adds FALCON to convolution units, is able to reduce the number of parameters and floating-point operations in convolution by a factor of k.  ","This paper proposes a new method for compressing convolutional neural networks (CNN) for classification tasks. The proposed method, called FALCON, is based on a mathematical formulation of the convolution kernel, which is a generalization of depthwise separable convolution, which combines convolution with depthwise convolution and pointwise convolutions. The authors show that FALcon can be applied to existing convolution methods based on EHP, and show that they can achieve comparable accuracies to standard convolution.  The authors also propose a new convolution unit ShuffleUnitV2, where FAL CON is incorporated into the original Convolutional Neural Networks (CNN). The authors further show that the proposed method can be used to improve the compression and computation reduction rates of a generalized version of convolution (FALCON-branch).  Experiments are conducted on several heuristic approaches to compress CNNs, and the authors compare the performance of the proposed methods with standard CNN models in terms of compression, computation, and accuracy. The results show that rank-k FALCon outperforms convolution in all cases, and can be combined with convolution to achieve better performance.  Finally, the authors also show that a variant of FALON, which adds FALCON to convolution units, is able to reduce the number of parameters and floating-point operations in convolution by a factor of k.  "
12321,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"Batch Normalization HYPONYM-OF normalization layers. normalization algorithm USED-FOR small batch sizes. Ghost Batch Normalization USED-FOR small and medium batch sizes. scaling and shifting parameters FEATURE-OF weight decay regularization. Batch and Group Normalization USED-FOR normalization algorithm. SVHN CONJUNCTION Caltech-256. Caltech-256 CONJUNCTION SVHN. CUB-2011 CONJUNCTION ImageNet. ImageNet CONJUNCTION CUB-2011. Oxford Flowers-102 CONJUNCTION CUB-2011. CUB-2011 CONJUNCTION Oxford Flowers-102. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. Caltech-256 CONJUNCTION Oxford Flowers-102. Oxford Flowers-102 CONJUNCTION Caltech-256. ImageNet HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. CUB-2011 HYPONYM-OF datasets. Caltech-256 HYPONYM-OF datasets. Oxford Flowers-102 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. Method are neural network architectures, and deep architectures. Generic are they, and method. Task is inference normalization statistics. OtherScientificTerm is training vs. inference discrepancy. ","This paper proposes a new normalization layer, called batch normalization layers, for training neural network architectures. Batch Normalization is an extension of previous work that normalizes the normalization weights of the weights of normalized layers. The authors propose a normalization algorithm for small and medium batch sizes based on Batch and Group Normalization, and show that they are more robust to scaling and shifting parameters of the weight decay regularization. They also propose a variant of the previous work, called Ghost Batch normalization, which is able to adaptively adapt the training vs. inference normalization statistics to adapt to small and large batch sizes. They show that the proposed method is more robust in terms of training vs inference discrepancy.  Experiments are conducted on several datasets (CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet).  ","This paper proposes a new normalization layer, called batch normalization layers, for training neural network architectures. Batch Normalization is an extension of previous work that normalizes the normalization weights of the weights of normalized layers. The authors propose a normalization algorithm for small and medium batch sizes based on Batch and Group Normalization, and show that they are more robust to scaling and shifting parameters of the weight decay regularization. They also propose a variant of the previous work, called Ghost Batch normalization, which is able to adaptively adapt the training vs. inference normalization statistics to adapt to small and large batch sizes. They show that the proposed method is more robust in terms of training vs inference discrepancy.  Experiments are conducted on several datasets (CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet).  "
12330,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"outliers CONJUNCTION misclassifications. misclassifications CONJUNCTION outliers. manual data inspection USED-FOR privacy - sensitive datasets. metrics CONJUNCTION model parameters. model parameters CONJUNCTION metrics. model parameters HYPONYM-OF aggregated outputs. metrics HYPONYM-OF aggregated outputs. federated methods CONJUNCTION formal differential privacy guarantees. formal differential privacy guarantees CONJUNCTION federated methods. formal differential privacy guarantees FEATURE-OF generative models. federated methods USED-FOR generative models. algorithm USED-FOR differentially private federated GANs. text with differentially private federated RNNs CONJUNCTION images. images CONJUNCTION text with differentially private federated RNNs. algorithm USED-FOR images. Task are machine learning, Manual inspection of raw data, and federated learning. Generic are models, two, and methods. OtherScientificTerm are modeling hypotheses, and human - provided labels. ","This paper considers the problem of machine learning in federated learning, where the goal is to train models that are differentially private in the presence of outliers, misclassifications, and privacy-sensitive datasets. The authors propose two methods: 1) manual data inspection of raw data and 2) aggregated outputs (i.e., metrics and model parameters). The first two methods are based on the assumption that there are two sets of models that share the same training data. The second one is based on an assumption that the two models share a common training dataset.   The authors show that both federated methods and formal differential privacy guarantees for generative models can be obtained for both of these two cases. They also propose an algorithm that can be used to train differential private federated GANs for both text and images. The main contribution of this paper is that it proposes a new algorithm for aggregating aggregated data from two models, and shows that the algorithm can be applied to both training and testing datasets.  2) The authors also show that their algorithm can also be used for training GAN-based models for images.  3) They show that modeling hypotheses can be learned in a federated setting without human-provided labels.  4) They also provide a theoretical analysis of their algorithm. ","This paper considers the problem of machine learning in federated learning, where the goal is to train models that are differentially private in the presence of outliers, misclassifications, and privacy-sensitive datasets. The authors propose two methods: 1) manual data inspection of raw data and 2) aggregated outputs (i.e., metrics and model parameters). The first two methods are based on the assumption that there are two sets of models that share the same training data. The second one is based on an assumption that the two models share a common training dataset.   The authors show that both federated methods and formal differential privacy guarantees for generative models can be obtained for both of these two cases. They also propose an algorithm that can be used to train differential private federated GANs for both text and images. The main contribution of this paper is that it proposes a new algorithm for aggregating aggregated data from two models, and shows that the algorithm can be applied to both training and testing datasets.  2) The authors also show that their algorithm can also be used for training GAN-based models for images.  3) They show that modeling hypotheses can be learned in a federated setting without human-provided labels.  4) They also provide a theoretical analysis of their algorithm. "
12339,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"Learning diverse and natural behaviors USED-FOR intelligent characters. method USED-FOR generating long range diverse and distinctive behaviors. method USED-FOR motion of human. non - parametric techniques CONJUNCTION parametric ones. parametric ones CONJUNCTION non - parametric techniques. non - parametric techniques USED-FOR method. parametric ones USED-FOR method. memory bank USED-FOR motion references. deep network USED-FOR synthesis. skeleton datasets EVALUATE-FOR method. method COMPARE parametric and non - parametric baselines. parametric and non - parametric baselines COMPARE method. skeleton datasets EVALUATE-FOR parametric and non - parametric baselines. OtherScientificTerm are long range diverse and distinctive behaviors, and starting and ending state. Material is animated world. ","This paper proposes a method for learning diverse and natural behaviors for intelligent characters. The proposed method aims at generating long range diverse and distinctive behaviors. The method is based on non-parametric techniques and parametric ones, and is able to capture the motion of human. The key idea is to use a memory bank to store motion references from the current state of the animated world, and to learn a starting and ending state for each reference. The synthesis is performed by a deep network, and the method is evaluated on two skeleton datasets, where the method outperforms both parametric and non parametric baselines. ","This paper proposes a method for learning diverse and natural behaviors for intelligent characters. The proposed method aims at generating long range diverse and distinctive behaviors. The method is based on non-parametric techniques and parametric ones, and is able to capture the motion of human. The key idea is to use a memory bank to store motion references from the current state of the animated world, and to learn a starting and ending state for each reference. The synthesis is performed by a deep network, and the method is evaluated on two skeleton datasets, where the method outperforms both parametric and non parametric baselines. "
12348,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,neural networks USED-FOR natural language processing tasks. model USED-FOR semantic compositions. hierarchies FEATURE-OF non - additivity and context independent importance attributions. inconsistent explanation quality EVALUATE-FOR models. contextual decomposition HYPONYM-OF hierarchical explanations. Sampling and Contextual Decomposition ( SCD ) algorithm CONJUNCTION Sampling and Occlusion ( SOC ) algorithm. Sampling and Occlusion ( SOC ) algorithm CONJUNCTION Sampling and Contextual Decomposition ( SCD ) algorithm. algorithms COMPARE hierarchical explanation algorithms. hierarchical explanation algorithms COMPARE algorithms. LSTM models CONJUNCTION BERT Transformer models. BERT Transformer models CONJUNCTION LSTM models. Human and metrics evaluation EVALUATE-FOR BERT Transformer models. Human and metrics evaluation EVALUATE-FOR algorithms. Human and metrics evaluation EVALUATE-FOR LSTM models. BERT Transformer models EVALUATE-FOR algorithms. Human and metrics evaluation EVALUATE-FOR hierarchical explanation algorithms. algorithms USED-FOR semantic composition. algorithms USED-FOR classification rules. models USED-FOR semantic composition. Task is hierarchical explanation of neural network predictions. OtherScientificTerm is word and phrase compositions. ,"This paper studies the problem of hierarchical explanation of neural network predictions. The authors propose a new model for learning semantic compositions of word and phrase compositions in natural language processing tasks using neural networks. They show that existing hierarchical explanations (e.g., contextual decomposition) suffer from inconsistent explanation quality due to non-additivity and context independent importance attributions across hierarchies. They propose two algorithms: Sampling and Contextual Decomposition (SCD) algorithm and the Sampling & Occlusion (SOC) algorithm. Human and metrics evaluation on LSTM models and BERT Transformer models show that the proposed algorithms outperform existing hierarchical explanation algorithms for learning the semantic composition of a word or phrase. They also show that these algorithms are able to learn classification rules that are more interpretable and interpretable. ","This paper studies the problem of hierarchical explanation of neural network predictions. The authors propose a new model for learning semantic compositions of word and phrase compositions in natural language processing tasks using neural networks. They show that existing hierarchical explanations (e.g., contextual decomposition) suffer from inconsistent explanation quality due to non-additivity and context independent importance attributions across hierarchies. They propose two algorithms: Sampling and Contextual Decomposition (SCD) algorithm and the Sampling & Occlusion (SOC) algorithm. Human and metrics evaluation on LSTM models and BERT Transformer models show that the proposed algorithms outperform existing hierarchical explanation algorithms for learning the semantic composition of a word or phrase. They also show that these algorithms are able to learn classification rules that are more interpretable and interpretable. "
12357,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"AI saliency map method USED-FOR deep convolutional neural networks ( CNN ). AI saliency map method COMPARE gradient methods. gradient methods COMPARE AI saliency map method. deep convolutional neural networks ( CNN ) COMPARE gradient methods. gradient methods COMPARE deep convolutional neural networks ( CNN ). accuracy EVALUATE-FOR It. Saliency Map Order Equivalence USED-FOR saliency measures. Layer Ordered Visualization of Information USED-FOR scale / layer contributions. scale information contributions PART-OF network. it COMPARE Guided Backprop. Guided Backprop COMPARE it. layers PART-OF network. forward pass USED-FOR layers. forward pass USED-FOR method. Grad - CAM++ CONJUNCTION Smooth Grad - CAM++. Smooth Grad - CAM++ CONJUNCTION Grad - CAM++. Grad - CAM CONJUNCTION Grad - CAM++. Grad - CAM++ CONJUNCTION Grad - CAM. method COMPARE Guided Backprop. Guided Backprop COMPARE method. method COMPARE class activation methods. class activation methods COMPARE method. Guided Backprop COMPARE class activation methods. class activation methods COMPARE Guided Backprop. Smooth Grad - CAM++ HYPONYM-OF class activation methods. Grad - CAM HYPONYM-OF class activation methods. Grad - CAM++ HYPONYM-OF class activation methods. cell phones CONJUNCTION low cost industrial devices. low cost industrial devices CONJUNCTION cell phones. robots CONJUNCTION cell phones. cell phones CONJUNCTION robots. resource limited platforms USED-FOR methods. low cost industrial devices HYPONYM-OF resource limited platforms. robots HYPONYM-OF resource limited platforms. cell phones HYPONYM-OF resource limited platforms. method USED-FOR CNNs 1. Generic is technique. OtherScientificTerm are network scale, and memory footprint. Method are saliency map, and saliency map methods. Task is satellite image processing. ","This paper proposes an AI saliency map method for deep convolutional neural networks (CNN) that outperforms standard gradient methods in accuracy. The technique is motivated by the observation that saliency measures based on Saliency Map Order Equivalence (SME) do not scale well with the network scale. The authors propose to use Layer Ordered Visualization of Information (LVI) to measure the scale/layer contributions of each layer in a network, which is based on the idea that the scale information contributions of a network can be estimated from a single image. The proposed method uses a forward pass to update the layers of the network, and it is shown to outperform Guided Backprop in terms of accuracy and memory footprint. The method is applied to CNNs 1.5 and 2.5 layers, and compared to other class activation methods such as Grad-CAM, Grad-AM++, Smooth Grad- CAM++, etc. on several resource limited platforms (robots, cell phones, and low cost industrial devices). The authors also perform satellite image processing and show that the proposed method can be used to improve the performance of CNNs on a variety of applications. ","This paper proposes an AI saliency map method for deep convolutional neural networks (CNN) that outperforms standard gradient methods in accuracy. The technique is motivated by the observation that saliency measures based on Saliency Map Order Equivalence (SME) do not scale well with the network scale. The authors propose to use Layer Ordered Visualization of Information (LVI) to measure the scale/layer contributions of each layer in a network, which is based on the idea that the scale information contributions of a network can be estimated from a single image. The proposed method uses a forward pass to update the layers of the network, and it is shown to outperform Guided Backprop in terms of accuracy and memory footprint. The method is applied to CNNs 1.5 and 2.5 layers, and compared to other class activation methods such as Grad-CAM, Grad-AM++, Smooth Grad- CAM++, etc. on several resource limited platforms (robots, cell phones, and low cost industrial devices). The authors also perform satellite image processing and show that the proposed method can be used to improve the performance of CNNs on a variety of applications. "
12366,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"Non - autoregressive models USED-FOR text generation tasks. position modeling PART-OF non - autoregressive text generation. PNAT USED-FOR text generative process. positions USED-FOR PNAT. PNAT COMPARE baselines. baselines COMPARE PNAT. machine translation CONJUNCTION paraphrase generation tasks. paraphrase generation tasks CONJUNCTION machine translation. paraphrase generation tasks EVALUATE-FOR PNAT. machine translation EVALUATE-FOR PNAT. OtherScientificTerm are positions of generated words, and latent variable. ","Non-autoregressive models for text generation tasks are becoming more and more popular in recent years. In this paper, the authors consider the problem of position modeling in non-autorgressive text generation. The authors propose PNAT, a method that uses positions of generated words as a latent variable to guide the text generation process. They show that PNAT can be used as a regularizer in the text generative process, and that the positions of the generated words can be learned from the latent variable. PNAT is evaluated on machine translation and paraphrase generation tasks, and compared with several baselines. ","Non-autoregressive models for text generation tasks are becoming more and more popular in recent years. In this paper, the authors consider the problem of position modeling in non-autorgressive text generation. The authors propose PNAT, a method that uses positions of generated words as a latent variable to guide the text generation process. They show that PNAT can be used as a regularizer in the text generative process, and that the positions of the generated words can be learned from the latent variable. PNAT is evaluated on machine translation and paraphrase generation tasks, and compared with several baselines. "
12375,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,GANs USED-FOR generative model analysis. Random Path Generative Adversarial Network ( RPGAN ) HYPONYM-OF GANs. random paths PART-OF generator network. latent space FEATURE-OF GAN. latent space PART-OF RPGAN. random paths PART-OF latent space. design USED-FOR factors of variation. natural interpretability FEATURE-OF factors of variation. generator layers USED-FOR factors of variation. layers USED-FOR image generation process. RPGAN USED-FOR image generation process. RPGAN model USED-FOR incremental learning. interpretability EVALUATE-FOR RPGAN model. generation quality EVALUATE-FOR RPGAN model. OtherScientificTerm is Gaussian distribution. ,"This paper proposes Random Path Generative Adversarial Network (RPGAN), an extension of GANs for generative model analysis. In RPGAN, the latent space of a GAN consists of a set of random paths in the generator network, each of which is drawn from a Gaussian distribution. The design of RPGAN aims to capture factors of variation in the generated images through the generator layers, which are important for natural interpretability. The resulting image generation process is trained with multiple layers, and the resulting RPGAN model is shown to improve the generation quality and the interpretability of the resulting images. The paper also shows that the proposed RPGan model can be used for incremental learning.","This paper proposes Random Path Generative Adversarial Network (RPGAN), an extension of GANs for generative model analysis. In RPGAN, the latent space of a GAN consists of a set of random paths in the generator network, each of which is drawn from a Gaussian distribution. The design of RPGAN aims to capture factors of variation in the generated images through the generator layers, which are important for natural interpretability. The resulting image generation process is trained with multiple layers, and the resulting RPGAN model is shown to improve the generation quality and the interpretability of the resulting images. The paper also shows that the proposed RPGan model can be used for incremental learning."
12384,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"convolution USED-FOR spherical neural network. efficiency CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION efficiency. DeepSphere HYPONYM-OF method. graph representation of the sampled sphere USED-FOR method. graph USED-FOR equivariance. Generic is formulation. Method are anisotropic filters, and deepsphere. ","This paper proposes a spherical neural network with convolution that is equivariant to rotation and rotation equivariance. The formulation is based on anisotropic filters. The proposed method, DeepSphere, is a method based on the graph representation of the sampled sphere. The authors show that equivariation is achieved through the use of a graph, and that deepsphere can be used for efficient and efficient training. ","This paper proposes a spherical neural network with convolution that is equivariant to rotation and rotation equivariance. The formulation is based on anisotropic filters. The proposed method, DeepSphere, is a method based on the graph representation of the sampled sphere. The authors show that equivariation is achieved through the use of a graph, and that deepsphere can be used for efficient and efficient training. "
12393,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"Learning Invariant Representations USED-FOR deep classifiers. invariance USED-FOR compression of representations. weighting representations USED-FOR representation distributions. adaptability EVALUATE-FOR weighting representations. compression CONJUNCTION invariance of learned representations. invariance of learned representations CONJUNCTION compression. adaptability EVALUATE-FOR representation. compression risk EVALUATE-FOR representation. learning weighted representations USED-FOR constraint of invariance. Metric are minimal combined domain error, and risk of compression. OtherScientificTerm are representation invariance, and constraint. Generic is bound. Material is domain adaptation benchmark. Method is adaptation methods. ","This paper studies the problem of Learning Invariant Representations for deep classifiers. The authors show that learning invariance to compression of representations leads to a minimal combined domain error, and that representation invariance can be used to reduce the risk of compression. They also show that weighting representations for different representation distributions improves the adaptability of the learned representations. Finally, they show that the compression and invariance of learned representations can be combined to improve the compression risk of the representation.    The authors also provide a bound on the number of times that learning weighted representations satisfy the constraint of invariance, and show that adaptation methods that do not satisfy this constraint do not perform well on the domain adaptation benchmark. The paper is well-written and easy to follow. ","This paper studies the problem of Learning Invariant Representations for deep classifiers. The authors show that learning invariance to compression of representations leads to a minimal combined domain error, and that representation invariance can be used to reduce the risk of compression. They also show that weighting representations for different representation distributions improves the adaptability of the learned representations. Finally, they show that the compression and invariance of learned representations can be combined to improve the compression risk of the representation.    The authors also provide a bound on the number of times that learning weighted representations satisfy the constraint of invariance, and show that adaptation methods that do not satisfy this constraint do not perform well on the domain adaptation benchmark. The paper is well-written and easy to follow. "
12402,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"learning USED-FOR user interface attributes. it USED-FOR synthetic training dataset. colors CONJUNCTION border radius. border radius CONJUNCTION colors. border radius CONJUNCTION shadow or text properties. shadow or text properties CONJUNCTION border radius. shadow or text properties HYPONYM-OF attributes. border radius HYPONYM-OF attributes. colors HYPONYM-OF attributes. imitation learning USED-FOR neural policy. approach USED-FOR inferring Android Button attribute values. accuracy EVALUATE-FOR dataset. real - world Google Play Store applications FEATURE-OF dataset. dataset EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Task is user interface implementation. Method are black box rendering engine, and neural models. Metric is pixel - level accuracy. OtherScientificTerm are attribute space, and Android Button attribute values. ","This paper proposes learning to infer user interface attributes from a user interface implementation. The key idea is to train a black box rendering engine and use it as a synthetic training dataset, where attributes (e.g., colors, border radius, shadow or text properties, etc.) can be extracted from the attribute space. Then, a neural policy is trained using imitation learning. The approach is evaluated on a dataset of real-world Google Play Store applications, where it is shown to achieve pixel-level accuracy. In addition, the approach is also applied to inferring Android Button attribute values. ","This paper proposes learning to infer user interface attributes from a user interface implementation. The key idea is to train a black box rendering engine and use it as a synthetic training dataset, where attributes (e.g., colors, border radius, shadow or text properties, etc.) can be extracted from the attribute space. Then, a neural policy is trained using imitation learning. The approach is evaluated on a dataset of real-world Google Play Store applications, where it is shown to achieve pixel-level accuracy. In addition, the approach is also applied to inferring Android Button attribute values. "
12411,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"Transfer learning USED-FOR neural network classifiers. data scarcity CONJUNCTION computational limitations. computational limitations CONJUNCTION data scarcity. robust feature extractors PART-OF robust networks. feature extractors USED-FOR classifiers. strategies USED-FOR models. Generic are network, and model. Method are full - scale training, robust transfer learning, lifelong learning strategies, and adversarial training. Metric are robustness, accuracy, and generalization of adversarially trained models. ","This paper studies the problem of transfer learning for neural network classifiers in the presence of data scarcity, computational limitations, and adversarial attacks. The authors show that robust feature extractors in robust networks can be used to improve the generalization performance of classifiers. They also show that the robustness of a network can be improved by learning robust features that are transferable across different tasks.    The authors also propose two lifelong learning strategies to improve robust transfer learning. The main idea is to train a model with adversarial training and then transfer the robust features from one task to another. They show that these strategies can improve the robust transfer performance of models that have been adversarially trained. The paper also shows that the transferability of robust features across tasks is improved when the number of tasks is limited and the accuracy of the model is high.","This paper studies the problem of transfer learning for neural network classifiers in the presence of data scarcity, computational limitations, and adversarial attacks. The authors show that robust feature extractors in robust networks can be used to improve the generalization performance of classifiers. They also show that the robustness of a network can be improved by learning robust features that are transferable across different tasks.    The authors also propose two lifelong learning strategies to improve robust transfer learning. The main idea is to train a model with adversarial training and then transfer the robust features from one task to another. They show that these strategies can improve the robust transfer performance of models that have been adversarially trained. The paper also shows that the transferability of robust features across tasks is improved when the number of tasks is limited and the accuracy of the model is high."
12420,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,"natural language USED-FOR complex concepts. compositionality USED-FOR complex concepts. natural language USED-FOR compositionality. neural agents USED-FOR language games. compositionality FEATURE-OF language. neural agents USED-FOR communication protocols. neural iterated learning ( NIL ) algorithm USED-FOR structured type of language. neural iterated learning ( NIL ) algorithm USED-FOR interacting neural agents. OtherScientificTerm are limited vocabulary, and compositional language. Generic is languages. Method are NIL, probabilistic model of NIL, and neural agent communication. ","This paper studies the problem of communication between neural agents playing language games. The authors show that natural language can capture complex concepts and that compositionality in natural language allows for complex concepts to be expressed in a natural language. They then propose a neural iterated learning (NIL) algorithm for interacting neural agents that learns a structured type of language, where the agents are able to communicate with each other using a limited vocabulary. They show that neural agents can learn communication protocols by iteratively iteratively learning a language with compositionality. They also show that NIL can be used to learn a language that is more expressive than other languages.    The authors also propose a probabilistic model of NIL, which they use to show that the neural agent communication can be improved by learning a compositional language. ","This paper studies the problem of communication between neural agents playing language games. The authors show that natural language can capture complex concepts and that compositionality in natural language allows for complex concepts to be expressed in a natural language. They then propose a neural iterated learning (NIL) algorithm for interacting neural agents that learns a structured type of language, where the agents are able to communicate with each other using a limited vocabulary. They show that neural agents can learn communication protocols by iteratively iteratively learning a language with compositionality. They also show that NIL can be used to learn a language that is more expressive than other languages.    The authors also propose a probabilistic model of NIL, which they use to show that the neural agent communication can be improved by learning a compositional language. "
12429,SP:add48154b31c13f48aef740e665f23694fa83681,inference CONJUNCTION learning. learning CONJUNCTION inference. black - box algorithm USED-FOR inference. black - box algorithm USED-FOR learning. Adversarial Variational Inference and Learning ( AdVIL ) USED-FOR inference. Adversarial Variational Inference and Learning ( AdVIL ) USED-FOR learning. learning USED-FOR Markov random field ( MRF ). Adversarial Variational Inference and Learning ( AdVIL ) HYPONYM-OF black - box algorithm. variational distributions USED-FOR latent variables. AdVIL USED-FOR partition function. variational distributions USED-FOR partition function. partition function FEATURE-OF MRF. variational distributions USED-FOR AdVIL. variational distributions USED-FOR negative log - likelihood. negative log - likelihood FEATURE-OF MRF. stochastic gradient descent USED-FOR minimax optimization problem. minimax optimization problem USED-FOR negative log - likelihood. contrastive divergence COMPARE AdVIL. AdVIL COMPARE contrastive divergence. AdVIL USED-FOR MRFs. black - box methods COMPARE AdVIL. AdVIL COMPARE black - box methods. AdVIL USED-FOR log partition function. OtherScientificTerm is model structure. ,"This paper proposes Adversarial Variational Variational Inference and Learning (AdVIL), a black-box algorithm for inference and learning for Markov random field (MRF). AdVIL learns the partition function of the MRF using variational distributions for the latent variables. The negative log-likelihood of MRF is obtained by minimizing the sum of the variational distribution of the latent variable and the log partition function. This is done by solving a minimax optimization problem via stochastic gradient descent. Compared to contrastive divergence, the authors show that AdVil can learn MRFs that are more robust to changes in the model structure. The authors also show that compared to other black- box methods, AdVilt can learn a more robust and more robust log partitionfunction.   ","This paper proposes Adversarial Variational Variational Inference and Learning (AdVIL), a black-box algorithm for inference and learning for Markov random field (MRF). AdVIL learns the partition function of the MRF using variational distributions for the latent variables. The negative log-likelihood of MRF is obtained by minimizing the sum of the variational distribution of the latent variable and the log partition function. This is done by solving a minimax optimization problem via stochastic gradient descent. Compared to contrastive divergence, the authors show that AdVil can learn MRFs that are more robust to changes in the model structure. The authors also show that compared to other black- box methods, AdVilt can learn a more robust and more robust log partitionfunction.   "
12438,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"low - dimensional state of the environment USED-FOR robot manipulation tasks. state estimator USED-FOR reward function. reinforcement learning USED-FOR policy. high - dimensional observations USED-FOR reward function. indicator reward function USED-FOR goal - conditioned reinforcement learning. continuous state spaces FEATURE-OF indicator reward function. reward balancing CONJUNCTION reward filtering. reward filtering CONJUNCTION reward balancing. methods USED-FOR convergence. indicator rewards USED-FOR methods. reward filtering HYPONYM-OF methods. indicator rewards USED-FOR convergence. reward balancing HYPONYM-OF methods. method USED-FOR tasks. continuous state spaces USED-FOR method. continuous state spaces FEATURE-OF tasks. RGB - D images HYPONYM-OF continuous state spaces. RGB - D images USED-FOR rope manipulation. rope manipulation HYPONYM-OF continuous state spaces. OtherScientificTerm are deformable objects, high - dimensional sensor inputs, positive reward, positive rewards, ground - truth state, and rewards. Method is end - to - end policy. ","This paper proposes a goal-conditioned reinforcement learning method that uses an indicator reward function to guide the learning of a policy in a low-dimensional state space. The paper proposes to learn a reward function in a continuous state space, where the reward function is a function of the state of the environment. The reward function can be learned from high-dimensional sensor inputs. The method is tested on a number of robot manipulation tasks with deformable objects, and is shown to outperform existing methods based on reward balancing and reward filtering.","This paper proposes a goal-conditioned reinforcement learning method that uses an indicator reward function to guide the learning of a policy in a low-dimensional state space. The paper proposes to learn a reward function in a continuous state space, where the reward function is a function of the state of the environment. The reward function can be learned from high-dimensional sensor inputs. The method is tested on a number of robot manipulation tasks with deformable objects, and is shown to outperform existing methods based on reward balancing and reward filtering."
12447,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"Robustness verification USED-FOR prediction behavior. Robustness verification USED-FOR safety guarantees. Robustness verification USED-FOR neural networks. architectures USED-FOR neural networks. robustness verification problem USED-FOR Transformers. cross - nonlinearity CONJUNCTION cross - position dependency. cross - position dependency CONJUNCTION cross - nonlinearity. self - attention layers FEATURE-OF Transformers. robustness verification algorithm USED-FOR Transformers. method COMPARE naive Interval Bound Propagation. naive Interval Bound Propagation COMPARE method. method COMPARE those. those COMPARE method. certified robustness bounds COMPARE those. those COMPARE certified robustness bounds. naive Interval Bound Propagation USED-FOR those. naive Interval Bound Propagation USED-FOR certified robustness bounds. method USED-FOR certified robustness bounds. bounds USED-FOR Transformers. OtherScientificTerm is model behavior. Task are verification, and sentiment analysis. Generic is they. ","Robustness verification is an important technique for verifying the safety guarantees of neural networks with different architectures and different prediction behavior. This paper considers the robustness verification problem for Transformers, where the goal is to certify that the model behavior is robust to perturbations in the input space.    The authors consider Transformers with cross-nonlinearity, cross-position dependency, and self-attention layers. The authors propose a robustness verify algorithm for Transformers. The verification is based on sentiment analysis, and the authors show that the proposed method is more robust than naive Interval Bound Propagation, and achieves certified robustness bounds that are tighter than those based on the same method for Transformers with the same number of layers as they have.","Robustness verification is an important technique for verifying the safety guarantees of neural networks with different architectures and different prediction behavior. This paper considers the robustness verification problem for Transformers, where the goal is to certify that the model behavior is robust to perturbations in the input space.    The authors consider Transformers with cross-nonlinearity, cross-position dependency, and self-attention layers. The authors propose a robustness verify algorithm for Transformers. The verification is based on sentiment analysis, and the authors show that the proposed method is more robust than naive Interval Bound Propagation, and achieves certified robustness bounds that are tighter than those based on the same method for Transformers with the same number of layers as they have."
12456,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"self - supervised learning USED-FOR natural language processing ( NLP ) tasks. pretrained language models USED-FOR self - supervised learning. syntactic and semantic NLP tasks EVALUATE-FOR pretrained models. real - world knowledge FEATURE-OF tasks. tasks EVALUATE-FOR pretrained models. zero - shot fact completion task USED-FOR pretrained models. BERT HYPONYM-OF pretrained models. weakly supervised pretraining objective USED-FOR model. objective USED-FOR Models. fact completion task EVALUATE-FOR objective. fact completion task EVALUATE-FOR Models. TriviaQA CONJUNCTION SearchQA. SearchQA CONJUNCTION TriviaQA. SearchQA CONJUNCTION Quasar - T. Quasar - T CONJUNCTION SearchQA. WebQuestions CONJUNCTION TriviaQA. TriviaQA CONJUNCTION WebQuestions. model COMPARE BERT. BERT COMPARE model. model USED-FOR downstream tasks. FIGER HYPONYM-OF fine - grained entity typing dataset. fine - grained entity typing dataset EVALUATE-FOR model. entity - related question answering datasets EVALUATE-FOR BERT. entity - related question answering datasets EVALUATE-FOR model. WebQuestions HYPONYM-OF entity - related question answering datasets. Quasar - T HYPONYM-OF entity - related question answering datasets. TriviaQA HYPONYM-OF entity - related question answering datasets. SearchQA HYPONYM-OF entity - related question answering datasets. Method is large - scale language modeling. OtherScientificTerm are knowledge, and real - world entities. ","This paper studies the problem of self-supervised learning for natural language processing (NLP) tasks using pretrained language models. The authors show that pretrained models trained on syntactic and semantic NLP tasks with a zero-shot fact completion task are able to generalize to new tasks that require real-world knowledge. Models trained with this objective are shown to generalise well to a fact-of-the-moment task.   The authors also propose a weakly supervised pretraining objective to further improve the performance of a model trained with a pretrained model.  They show that their model outperforms BERT on three entity-related question answering datasets (WebQuestions, TriviaQA, SearchQA and Quasar-T) and on a fine-grained entity typing dataset ( FIGER). They also show that the model generalizes well to other downstream tasks (e.g. to large-scale language modeling). ","This paper studies the problem of self-supervised learning for natural language processing (NLP) tasks using pretrained language models. The authors show that pretrained models trained on syntactic and semantic NLP tasks with a zero-shot fact completion task are able to generalize to new tasks that require real-world knowledge. Models trained with this objective are shown to generalise well to a fact-of-the-moment task.   The authors also propose a weakly supervised pretraining objective to further improve the performance of a model trained with a pretrained model.  They show that their model outperforms BERT on three entity-related question answering datasets (WebQuestions, TriviaQA, SearchQA and Quasar-T) and on a fine-grained entity typing dataset ( FIGER). They also show that the model generalizes well to other downstream tasks (e.g. to large-scale language modeling). "
12465,SP:4395d6f3e197df478eee84e092539dc370babd97,"image collection USED-FOR discovering novel classes. setting COMPARE semi - supervised learning. semi - supervised learning COMPARE setting. self - supervised learning USED-FOR representation. supervised classification of the labelled data CONJUNCTION clustering of the unlabelled data. clustering of the unlabelled data CONJUNCTION supervised classification of the labelled data. rank statistics USED-FOR model. rank statistics USED-FOR clustering the unlabelled images. model USED-FOR clustering the unlabelled images. labelled and unlabelled subsets of the data USED-FOR joint objective function. labeled data USED-FOR image representation. joint objective function USED-FOR data representation. classification benchmarks EVALUATE-FOR methods. approach COMPARE methods. methods COMPARE approach. methods USED-FOR novel category discovery. classification benchmarks EVALUATE-FOR novel category discovery. approach USED-FOR novel category discovery. classification benchmarks EVALUATE-FOR approach. OtherScientificTerm is labelled images. Method is general - purpose clustering model. Generic is latter. Material are unlabelled data, and labelled and unlabelled data. ","This paper considers the problem of discovering novel classes from an image collection. The setting is similar to semi-supervised learning, but the representation is learned via self-supervision. The authors propose a general-purpose clustering model, where the unlabelled data is split into two subsets. The latter is used to learn a joint objective function between the supervised classification of the labelled data and the clustering of the unlabeled data. The model is trained using rank statistics from the labelled and unlabelling subsets of the data, and the latter is then used to train a model for clustering the two sets of unlabeling data.  The authors show that the proposed approach outperforms existing methods on several classification benchmarks for novel category discovery. The main contribution of the paper is that the data representation of the image representation can be learned from both the labeled data as well as the unlabeled ones, and that the authors propose to use a more general objective function that can be used for both the joint objective of the supervised and unlabelEDs. ","This paper considers the problem of discovering novel classes from an image collection. The setting is similar to semi-supervised learning, but the representation is learned via self-supervision. The authors propose a general-purpose clustering model, where the unlabelled data is split into two subsets. The latter is used to learn a joint objective function between the supervised classification of the labelled data and the clustering of the unlabeled data. The model is trained using rank statistics from the labelled and unlabelling subsets of the data, and the latter is then used to train a model for clustering the two sets of unlabeling data.  The authors show that the proposed approach outperforms existing methods on several classification benchmarks for novel category discovery. The main contribution of the paper is that the data representation of the image representation can be learned from both the labeled data as well as the unlabeled ones, and that the authors propose to use a more general objective function that can be used for both the joint objective of the supervised and unlabelEDs. "
12474,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"selfsupervised robot interaction USED-FOR images. images HYPONYM-OF dynamical system. VP algorithms USED-FOR robotic manipulation and navigation domains. data - driven perception and planning USED-FOR VP algorithms. approach USED-FOR VP. nodes PART-OF graph. connectivity PART-OF graph. image samples PART-OF graph. semiparametric topological memory ( SPTM ) method HYPONYM-OF approach. deep image classification USED-FOR connectivity. topological connectivity FEATURE-OF graph. graph search methods USED-FOR planning. loss function USED-FOR connectivity classifier. manual tuning USED-FOR loss function. loss function USED-FOR SPTM. discriminative classifier USED-FOR energy function. contrastive predictive coding USED-FOR discriminative classifier. contrastive predictive coding USED-FOR energy function. hallucinated samples USED-FOR connectivity graph. connectivity graph USED-FOR zero - shot generalization. domain changes FEATURE-OF zero - shot generalization. simulated domains EVALUATE-FOR HTM. SPTM CONJUNCTION visual foresight methods. visual foresight methods CONJUNCTION SPTM. simulated domains EVALUATE-FOR visual foresight methods. HTM COMPARE SPTM. SPTM COMPARE HTM. HTM COMPARE visual foresight methods. visual foresight methods COMPARE HTM. simulated domains EVALUATE-FOR SPTM. visual foresight methods USED-FOR long - horizon planning. long - horizon planning EVALUATE-FOR HTM. plan quality EVALUATE-FOR visual foresight methods. plan quality EVALUATE-FOR HTM. Task is visual planning ( VP ). Method are Hallucinative Topological Memory ( HTM ), and conditional VAE model. OtherScientificTerm is context image of the domain. ","This paper proposes a new approach to visual planning (VP) called the Hallucinative Topological Memory (HTM). The proposed approach builds upon the semiparametric topological memory (SPTM) method, which is a recent approach to improve the performance of existing VP algorithms in robotic manipulation and navigation domains through data-driven perception and planning.   The key idea of SPTM is to construct a graph that contains nodes that are connected by a topological connectivity. The connectivity graph is constructed from image samples from a dynamical system (e.g., images from a selfsupervised robot interaction) and a context image of the domain.  The connectivity is learned by deep image classification, and the energy function is learned via contrastive predictive coding. The authors propose a loss function to train the connectivity classifier, which can be used in conjunction with any existing graph search methods for planning. The proposed loss function is based on manual tuning, where the discriminative classifier is used to learn an energy function, and a conditional VAE model is trained to predict the energy of the classifier.  Experiments on simulated domains show that the proposed SPTM outperforms SPTM and other visual foresight methods in terms of zero-shot generalization to domain changes, and that the connectivity graph can be learned from hallucinated samples.  In addition, the authors also show that HTM improves the plan quality of long-horizon planning compared to SPTM, as well as other state-of-the-art visual forethought methods.","This paper proposes a new approach to visual planning (VP) called the Hallucinative Topological Memory (HTM). The proposed approach builds upon the semiparametric topological memory (SPTM) method, which is a recent approach to improve the performance of existing VP algorithms in robotic manipulation and navigation domains through data-driven perception and planning.   The key idea of SPTM is to construct a graph that contains nodes that are connected by a topological connectivity. The connectivity graph is constructed from image samples from a dynamical system (e.g., images from a selfsupervised robot interaction) and a context image of the domain.  The connectivity is learned by deep image classification, and the energy function is learned via contrastive predictive coding. The authors propose a loss function to train the connectivity classifier, which can be used in conjunction with any existing graph search methods for planning. The proposed loss function is based on manual tuning, where the discriminative classifier is used to learn an energy function, and a conditional VAE model is trained to predict the energy of the classifier.  Experiments on simulated domains show that the proposed SPTM outperforms SPTM and other visual foresight methods in terms of zero-shot generalization to domain changes, and that the connectivity graph can be learned from hallucinated samples.  In addition, the authors also show that HTM improves the plan quality of long-horizon planning compared to SPTM, as well as other state-of-the-art visual forethought methods."
12483,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"Large - scale benchmark datasets USED-FOR AI. benchmarks USED-FOR realistic problem distributions. spurious biases FEATURE-OF them. similar ( thus non - tail ) problems FEATURE-OF benchmarks. long - tail problems PART-OF real world problems. Adversarial Filters USED-FOR model - based reduction of dataset biases. AFLITE HYPONYM-OF iterative greedy algorithm. AFLITE USED-FOR reduced dataset. realistic problem distributions CONJUNCTION spurious biases. spurious biases CONJUNCTION realistic problem distributions. spurious biases FEATURE-OF reduced dataset. realistic problem distributions FEATURE-OF reduced dataset. AFOPTIMUM USED-FOR optimum bias reduction. AFLITE USED-FOR task. MNLI CONJUNCTION QNLI. QNLI CONJUNCTION MNLI. SNLI CONJUNCTION MNLI. MNLI CONJUNCTION SNLI. ImageNet CONJUNCTION SNLI. SNLI CONJUNCTION ImageNet. it USED-FOR benchmarks. SNLI CONJUNCTION QNLI. QNLI CONJUNCTION SNLI. ImageNet HYPONYM-OF benchmarks. QNLI HYPONYM-OF benchmarks. SNLI HYPONYM-OF benchmarks. MNLI HYPONYM-OF benchmarks. AFLITE USED-FOR measurable dataset biases. measurable dataset biases FEATURE-OF synthetic and real datasets. synthetic and real datasets EVALUATE-FOR AFLITE. K - nearest - neighbors USED-FOR dataset biases. Generic are filtered counterparts, and model. Metric is human performance. Task is bias reduction. ","Large-scale benchmark datasets are of great importance for AI. However, most of these benchmarks are not designed for realistic problem distributions, and many of them have spurious biases due to long-tail problems. This paper proposes a model-based reduction of dataset biases using Adversarial Filters. The authors propose AFLITE, an iterative greedy algorithm that uses AFOPTIMUM to find the optimum bias reduction. They show that AFLITE is able to find a reduced dataset with realistic problem samples and spurious biases, and that it can be applied to several popular benchmarks such as ImageNet, SNLI, MNLI, QNLI, etc.    The authors also show that the bias reduction performance of AFLITE matches human performance, and it can also be applied on synthetic and real datasets as well. They also show how AFLITE can reduce the measurable dataset biases on synthetic datasets using K-nearest-neighbor, and show that it is also able to reduce the bias on real-world datasets. ","Large-scale benchmark datasets are of great importance for AI. However, most of these benchmarks are not designed for realistic problem distributions, and many of them have spurious biases due to long-tail problems. This paper proposes a model-based reduction of dataset biases using Adversarial Filters. The authors propose AFLITE, an iterative greedy algorithm that uses AFOPTIMUM to find the optimum bias reduction. They show that AFLITE is able to find a reduced dataset with realistic problem samples and spurious biases, and that it can be applied to several popular benchmarks such as ImageNet, SNLI, MNLI, QNLI, etc.    The authors also show that the bias reduction performance of AFLITE matches human performance, and it can also be applied on synthetic and real datasets as well. They also show how AFLITE can reduce the measurable dataset biases on synthetic datasets using K-nearest-neighbor, and show that it is also able to reduce the bias on real-world datasets. "
12492,SP:82777947d2377efa897c6905261f5375b29a4c19,"decision metric USED-FOR models. matching networks CONJUNCTION prototypical networks. prototypical networks CONJUNCTION matching networks. prototypical networks PART-OF few - shot models. matching networks PART-OF few - shot models. softmax USED-FOR relative distance. batch normalization USED-FOR centering. Gaussian layer USED-FOR distance calculation. Gaussian layer USED-FOR prototypical network. support examples ’ distribution COMPARE centroid. centroid COMPARE support examples ’ distribution. distance calculation PART-OF prototypical network. support examples ’ distribution USED-FOR Gaussian layer. Method are Few - shot models, and prototypical few - shot models. Generic are They, and extension. OtherScientificTerm are class belongings, and null class ”. Material are Omniglot data set, matched test set, unmatched MNIST data, and MiniImageNet data set. Metric are classification accuracy, and test accuracy. ","This paper proposes a new decision metric for few-shot learning models. Few-shot models are models that are trained on the Omniglot data set, but not on the matched test set. They can be seen as a generalization of matching networks and prototypical networks. The authors propose to extend the notion of class belongings to the case where there is a “null class”, i.e. a set of classes that have not been seen before in the training set. This extension allows them to define a new definition of “class belongings” that can be used to measure the classification accuracy of a model. They also propose a novel way to define the relative distance between a pair of classes, which is based on softmax. They show that the distance calculation in a prototypical network is performed by a Gaussian layer on the support examples’ distribution rather than the centroid, and that the centering is done by batch normalization. They evaluate their method on the unmatched MNIST data set and on the MiniImageNet data set.   ","This paper proposes a new decision metric for few-shot learning models. Few-shot models are models that are trained on the Omniglot data set, but not on the matched test set. They can be seen as a generalization of matching networks and prototypical networks. The authors propose to extend the notion of class belongings to the case where there is a “null class”, i.e. a set of classes that have not been seen before in the training set. This extension allows them to define a new definition of “class belongings” that can be used to measure the classification accuracy of a model. They also propose a novel way to define the relative distance between a pair of classes, which is based on softmax. They show that the distance calculation in a prototypical network is performed by a Gaussian layer on the support examples’ distribution rather than the centroid, and that the centering is done by batch normalization. They evaluate their method on the unmatched MNIST data set and on the MiniImageNet data set.   "
12501,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"Graph embedding techniques USED-FOR applications. learning on non - Euclidean data USED-FOR applications. node attribute information PART-OF graph embedding models. computational complexity CONJUNCTION memory usage. memory usage CONJUNCTION computational complexity. graph fusion USED-FOR graph. topology FEATURE-OF graph. topology CONJUNCTION node attribute information. node attribute information CONJUNCTION topology. GraphZoom HYPONYM-OF multi - level framework. graph CONJUNCTION node attribute information. node attribute information CONJUNCTION graph. graph fusion USED-FOR GraphZoom. it USED-FOR embeddings. embedding methods USED-FOR coarsened graph. GraphZoom USED-FOR embedding methods. graph datasets USED-FOR transductive and inductive tasks. transductive and inductive tasks EVALUATE-FOR approach. graph datasets EVALUATE-FOR approach. GraphZoom COMPARE unsupervised embedding methods. unsupervised embedding methods COMPARE GraphZoom. GraphZoom USED-FOR graph embedding process. graph embedding process COMPARE unsupervised embedding methods. unsupervised embedding methods COMPARE graph embedding process. classification accuracy EVALUATE-FOR GraphZoom. OtherScientificTerm is node attribute noise. Metric are accuracy, and scalability. Generic is them. ","Graph embedding techniques are of great interest for many applications, especially for applications that require learning on non-Euclidean data. However, the computational complexity and memory usage of existing graph embedding models can be prohibitively expensive due to node attribute noise. This paper proposes a multi-level framework called GraphZoom, which combines the advantages of graph fusion for learning a graph with topology and node attribute information in order to achieve a better trade-off between accuracy and scalability.    The key idea is to combine the topology of the graph with the node attribute of each node in the graph, and then use graph fusion to map the graph to a coarsened graph, where it can be used to learn embeddings for each node independently.  The paper shows that Graphzoom outperforms existing embedding methods on a number of graph datasets for both transductive and inductive tasks, and it also shows that it is able to improve classification accuracy. The paper further shows that the graphZoom can be applied to any unsupervised embedding process, and that it outperforms the state-of-the-art in terms of classification accuracy on a variety of tasks. ","Graph embedding techniques are of great interest for many applications, especially for applications that require learning on non-Euclidean data. However, the computational complexity and memory usage of existing graph embedding models can be prohibitively expensive due to node attribute noise. This paper proposes a multi-level framework called GraphZoom, which combines the advantages of graph fusion for learning a graph with topology and node attribute information in order to achieve a better trade-off between accuracy and scalability.    The key idea is to combine the topology of the graph with the node attribute of each node in the graph, and then use graph fusion to map the graph to a coarsened graph, where it can be used to learn embeddings for each node independently.  The paper shows that Graphzoom outperforms existing embedding methods on a number of graph datasets for both transductive and inductive tasks, and it also shows that it is able to improve classification accuracy. The paper further shows that the graphZoom can be applied to any unsupervised embedding process, and that it outperforms the state-of-the-art in terms of classification accuracy on a variety of tasks. "
12510,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,predictive coding USED-FOR image compression models. neural autoregressive image generation models USED-FOR absolute pixel intensities. prediction COMPARE absolute counterpart. absolute counterpart COMPARE prediction. model USED-FOR sharp transitions. model USED-FOR smooth transitions. absolute predictor USED-FOR model. relative predictor USED-FOR generating smooth transitions. mechanism COMPARE absolute prediction counterparts. absolute prediction counterparts COMPARE mechanism. unconditional image generation CONJUNCTION image colorization. image colorization CONJUNCTION unconditional image generation. image colorization CONJUNCTION super - resolution. super - resolution CONJUNCTION image colorization. benchmarks EVALUATE-FOR mechanism. benchmarks EVALUATE-FOR super - resolution. likelihood EVALUATE-FOR absolute prediction counterparts. benchmarks EVALUATE-FOR unconditional image generation. benchmarks EVALUATE-FOR absolute prediction counterparts. image colorization HYPONYM-OF benchmarks. likelihood EVALUATE-FOR mechanism. Material is natural images. Method is unified probabilistic model. ,"This paper proposes a new approach to learn a predictive coding for image compression models. The idea is to use neural autoregressive image generation models to learn absolute pixel intensities for natural images. The authors propose a unified probabilistic model that can be used to train a model to generate sharp transitions and smooth transitions. The model is trained with an absolute predictor and a relative predictor. The proposed mechanism is evaluated on three benchmarks for unconditional image generation, image colorization, and super-resolution, and shows better likelihood than its absolute prediction counterparts. ","This paper proposes a new approach to learn a predictive coding for image compression models. The idea is to use neural autoregressive image generation models to learn absolute pixel intensities for natural images. The authors propose a unified probabilistic model that can be used to train a model to generate sharp transitions and smooth transitions. The model is trained with an absolute predictor and a relative predictor. The proposed mechanism is evaluated on three benchmarks for unconditional image generation, image colorization, and super-resolution, and shows better likelihood than its absolute prediction counterparts. "
12519,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"reinforcement learning CONJUNCTION Monte Carlo methods. Monte Carlo methods CONJUNCTION reinforcement learning. stationary distribution FEATURE-OF Markov chain. stationary distribution USED-FOR estimating quantities. Markov chain USED-FOR estimating quantities. estimation USED-FOR applications. variational divergence minimization USED-FOR constraint reformulations. off - line PageRank CONJUNCTION off - policy policy evaluation. off - policy policy evaluation CONJUNCTION off - line PageRank. off - policy policy evaluation HYPONYM-OF benchmark problems. off - line PageRank HYPONYM-OF benchmark problems. Task are real - world applications, and consistent estimation. OtherScientificTerm are transition operator, and stationary and empirical distributions. Generic are approach, and algorithm. Method are GenDICE, and error analysis. ","This paper considers the problem of estimating quantities from a Markov chain with a stationary distribution, which is a common problem in both reinforcement learning and Monte Carlo methods. The authors consider two real-world applications where the transition operator is non-convex and the objective is to learn a consistent estimation. In both these applications, the authors propose a new approach, called GenDICE, that is based on variational divergence minimization. The key idea is that the stationary and empirical distributions of the transition operators can be approximated by the stationary distribution of the Markov chains, which can be used for estimating quantities that are non-trivial to compute.   The authors show that the proposed approach, which they refer to as “consistent estimation”, is computationally tractable, and can be applied to a variety of benchmark problems such as off-line PageRank, off-policy policy evaluation, and constraint reformulations. They also provide a theoretical analysis of the algorithm and provide an error analysis. ","This paper considers the problem of estimating quantities from a Markov chain with a stationary distribution, which is a common problem in both reinforcement learning and Monte Carlo methods. The authors consider two real-world applications where the transition operator is non-convex and the objective is to learn a consistent estimation. In both these applications, the authors propose a new approach, called GenDICE, that is based on variational divergence minimization. The key idea is that the stationary and empirical distributions of the transition operators can be approximated by the stationary distribution of the Markov chains, which can be used for estimating quantities that are non-trivial to compute.   The authors show that the proposed approach, which they refer to as “consistent estimation”, is computationally tractable, and can be applied to a variety of benchmark problems such as off-line PageRank, off-policy policy evaluation, and constraint reformulations. They also provide a theoretical analysis of the algorithm and provide an error analysis. "
12528,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"machine learning systems USED-FOR spurious patterns. statistical frameworks USED-FOR term. models USED-FOR spurious patterns. sentiment analysis CONJUNCTION natural language inference tasks. natural language inference tasks CONJUNCTION sentiment analysis. classifiers USED-FOR natural language inference tasks. classifiers USED-FOR sentiment analysis. classifiers COMPARE models. models COMPARE classifiers. classifiers USED-FOR spurious features. original or manipulated data USED-FOR classifiers. OtherScientificTerm are spurious associations, confounding, direct or indirect causal effects, and internal coherence. Task is natural language processing. Method is Classifiers. ","This paper studies the problem of spurious associations in machine learning systems. The authors propose a new term, confounding, which is defined as the phenomenon where there are direct or indirect causal effects in natural language processing that are not present in the training data. The term is defined in the context of statistical frameworks, and the authors show that existing machine learning models are susceptible to spurious patterns. Classifiers are trained on original or manipulated data, and are shown to be more sensitive to confounding than other classifiers. They show that classifiers trained on sentiment analysis and natural language inference tasks are more robust to spurious features than other models. They also show that these classifiers are sensitive to the presence of spurious features, and that the internal coherence of the classifiers is more important to the spurious features.","This paper studies the problem of spurious associations in machine learning systems. The authors propose a new term, confounding, which is defined as the phenomenon where there are direct or indirect causal effects in natural language processing that are not present in the training data. The term is defined in the context of statistical frameworks, and the authors show that existing machine learning models are susceptible to spurious patterns. Classifiers are trained on original or manipulated data, and are shown to be more sensitive to confounding than other classifiers. They show that classifiers trained on sentiment analysis and natural language inference tasks are more robust to spurious features than other models. They also show that these classifiers are sensitive to the presence of spurious features, and that the internal coherence of the classifiers is more important to the spurious features."
12537,SP:b720eb5b6e44473a9392cc572af89270019d4c42,super - resolution CONJUNCTION image restoration. image restoration CONJUNCTION super - resolution. CNN based image quality assessment CONJUNCTION super - resolution. super - resolution CONJUNCTION CNN based image quality assessment. full - reference perceptual quality features USED-FOR CNN based image quality assessment. image restoration CONJUNCTION imageto - image translation problems. imageto - image translation problems CONJUNCTION image restoration. CNN based image quality assessment CONJUNCTION image restoration. image restoration CONJUNCTION CNN based image quality assessment. frequency and orientation tuning of channels PART-OF trained image classification deep CNNs. spatial frequencies FEATURE-OF grating stimuli. VGG-16 HYPONYM-OF trained image classification deep CNNs. CNN channels USED-FOR human visual perception models. CNN channels USED-FOR spatial frequency and orientation selective filters. deep CNN representations USED-FOR perceptual quality features. contrast masking thresholds FEATURE-OF human visual perception. orientation selectivity FEATURE-OF deep CNN channels. perceptual quality features FEATURE-OF deep CNN channels. contrast masking thresholds FEATURE-OF spatial frequencies. ,"This paper studies the frequency and orientation tuning of channels in trained image classification deep CNNs (e.g. VGG-16). The authors consider CNN based image quality assessment, super-resolution, image restoration, and imageto-image translation problems. They show that the spatial frequencies of grating stimuli are highly correlated with spatial frequencies in human visual perception models. They also show that CNN channels that are tuned to be spatial frequency and orientational selective filters are more sensitive to spatial frequency than those that are not tuned. Finally, they show that deep CNN representations with perceptual quality features are highly sensitive to the orientation selectivity and contrast masking thresholds of spatial frequencies. ","This paper studies the frequency and orientation tuning of channels in trained image classification deep CNNs (e.g. VGG-16). The authors consider CNN based image quality assessment, super-resolution, image restoration, and imageto-image translation problems. They show that the spatial frequencies of grating stimuli are highly correlated with spatial frequencies in human visual perception models. They also show that CNN channels that are tuned to be spatial frequency and orientational selective filters are more sensitive to spatial frequency than those that are not tuned. Finally, they show that deep CNN representations with perceptual quality features are highly sensitive to the orientation selectivity and contrast masking thresholds of spatial frequencies. "
12546,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,drug development CONJUNCTION medical practice. medical practice CONJUNCTION drug development. graph neural networks USED-FOR task. nodes CONJUNCTION drug - drug interactions. drug - drug interactions CONJUNCTION nodes. drugs CONJUNCTION drug - drug interactions. drug - drug interactions CONJUNCTION drugs. drugs CONJUNCTION nodes. nodes CONJUNCTION drugs. graph neural networks USED-FOR DDI predictions. link prediction problems USED-FOR DDI predictions. graph energy neural network ( GENN ) USED-FOR link type correlations. structure prediction problem USED-FOR DDI prediction task. graph neural networks USED-FOR energy function. real world DDI datasets EVALUATE-FOR GENN. GENN COMPARE baselines. baselines COMPARE GENN. real world DDI datasets EVALUATE-FOR baselines. PR - AUC EVALUATE-FOR datasets. datasets EVALUATE-FOR GENN. PR - AUC EVALUATE-FOR GENN. GENN USED-FOR DDI correlations. GENN COMPARE baseline models. baseline models COMPARE GENN. Task is drug - drug interactions ( DDIs ). OtherScientificTerm is DDI types. Method is energy - based model. ,"This paper studies the problem of drug-drug interactions (DDIs) prediction in drug development and medical practice. In this task, graph neural networks are used to make DDI predictions based on link prediction problems. The authors propose a graph energy neural network (GENN) to predict link type correlations between drugs, nodes, drug - drug interactions, and other DDI types. The DDI prediction task is formulated as a structure prediction problem, and the energy function is learned by graph Neural Networks (GNNs). Experiments on three real world DDI datasets show that GENN outperforms several baselines in terms of PR-AUC on these datasets, and GENN is able to predict DDI correlations between two types of drugs, drugs, and nodes.","This paper studies the problem of drug-drug interactions (DDIs) prediction in drug development and medical practice. In this task, graph neural networks are used to make DDI predictions based on link prediction problems. The authors propose a graph energy neural network (GENN) to predict link type correlations between drugs, nodes, drug - drug interactions, and other DDI types. The DDI prediction task is formulated as a structure prediction problem, and the energy function is learned by graph Neural Networks (GNNs). Experiments on three real world DDI datasets show that GENN outperforms several baselines in terms of PR-AUC on these datasets, and GENN is able to predict DDI correlations between two types of drugs, drugs, and nodes."
12555,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,vq - wav2vec USED-FOR discrete representations of audio segments. online k - means clustering USED-FOR dense representations. Gumbel - Softmax CONJUNCTION online k - means clustering. online k - means clustering CONJUNCTION Gumbel - Softmax. algorithm USED-FOR dense representations. online k - means clustering USED-FOR algorithm. Gumbel - Softmax USED-FOR algorithm. Discretization USED-FOR algorithms. discrete inputs USED-FOR algorithms. TIMIT phoneme classification EVALUATE-FOR BERT pre - training. ,This paper proposes a new algorithm called vq-wav2vec to learn discrete representations of audio segments. The algorithm is based on Gumbel-Softmax and online k-means clustering to learn dense representations. Discretization is used to improve the performance of existing algorithms on discrete inputs. Experiments on TIMIT phoneme classification and BERT pre-training are conducted.,This paper proposes a new algorithm called vq-wav2vec to learn discrete representations of audio segments. The algorithm is based on Gumbel-Softmax and online k-means clustering to learn dense representations. Discretization is used to improve the performance of existing algorithms on discrete inputs. Experiments on TIMIT phoneme classification and BERT pre-training are conducted.
12564,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,methods USED-FOR collaborative filtering models. methods USED-FOR ranking - based objective functions. actor - critic reinforcement learning USED-FOR methods. critic network USED-FOR ranking - based metrics. actor network USED-FOR metrics. critic - based method USED-FOR approximate ) ranking scores. critic - based method USED-FOR scoring process. learning - to - rank methods COMPARE critic - based method. critic - based method COMPARE learning - to - rank methods. neural network USED-FOR critic - based method. optimization procedure USED-FOR learning - to - rank methods. neural network USED-FOR scoring process. actor - critic COMPARE baselines. baselines COMPARE actor - critic. prediction models COMPARE baselines. baselines COMPARE prediction models. actor - critic USED-FOR prediction models. large - scale datasets EVALUATE-FOR actor - critic. large - scale datasets EVALUATE-FOR baselines. ,"This paper proposes two methods to train collaborative filtering models based on actor-critic reinforcement learning. The authors propose two methods for learning ranking-based objective functions. The first method uses a critic network to learn the ranking -based metrics, and the second method uses an actor network to train the metrics. The critic-based method is able to learn (approximate) ranking scores, and can be used to guide the scoring process using a neural network. The learning-to-rank methods based on the optimization procedure are shown to outperform the critic -based method for the scoring of the scores. The results show that the proposed method outperforms the baselines on large-scale datasets, and that the prediction models trained with the learned prediction models are more robust than baselines trained with a single critic. ","This paper proposes two methods to train collaborative filtering models based on actor-critic reinforcement learning. The authors propose two methods for learning ranking-based objective functions. The first method uses a critic network to learn the ranking -based metrics, and the second method uses an actor network to train the metrics. The critic-based method is able to learn (approximate) ranking scores, and can be used to guide the scoring process using a neural network. The learning-to-rank methods based on the optimization procedure are shown to outperform the critic -based method for the scoring of the scores. The results show that the proposed method outperforms the baselines on large-scale datasets, and that the prediction models trained with the learned prediction models are more robust than baselines trained with a single critic. "
12573,SP:2444a83ae08181b125a325d893789f074d6db8ee,"off - policy reinforcement learning methods USED-FOR robot control. data - efficiency EVALUATE-FOR Deep Q - learning. multi - step TD - learning USED-FOR data - efficiency. Truncated Q - functions HYPONYM-OF TemporalDifference formulations. Shifted Q - functions USED-FOR farsighted return. Model - based Value Expansion CONJUNCTION TD3(∆ ). TD3(∆ ) CONJUNCTION Model - based Value Expansion. TD3 CONJUNCTION Model - based Value Expansion. Model - based Value Expansion CONJUNCTION TD3. approach COMPARE TD3. TD3 COMPARE approach. TD3(∆ ) HYPONYM-OF off - policy variant of TD(∆ ). approach COMPARE Model - based Value Expansion. Model - based Value Expansion COMPARE approach. approach USED-FOR function - approximation setting. function - approximation setting CONJUNCTION TD3. TD3 CONJUNCTION function - approximation setting. tabular case FEATURE-OF Composite Q - learning. simulated robot tasks EVALUATE-FOR Composite TD3. Composite TD3 COMPARE TD3. TD3 COMPARE Composite TD3. Composite TD3 COMPARE off - policy multi - step approaches. off - policy multi - step approaches COMPARE Composite TD3. simulated robot tasks EVALUATE-FOR off - policy multi - step approaches. simulated robot tasks EVALUATE-FOR TD3. TD3 COMPARE off - policy multi - step approaches. off - policy multi - step approaches COMPARE TD3. data - efficiency EVALUATE-FOR TD3. data - efficiency EVALUATE-FOR off - policy multi - step approaches. data - efficiency EVALUATE-FOR Composite TD3. Task is realworld applications. OtherScientificTerm are off - policy, target - policy rollout, truncated rollout, and shortand long - term predictions. Method is Composite Q - learning algorithm. ","This paper studies the problem of improving the data-efficiency of Deep Q-learning in the context of off-policy reinforcement learning methods for robot control. In realworld applications, multi-step TD-learning is commonly used to improve data-efficient in order to reduce the number of steps required to learn a target policy. However, the authors point out that the current TemporalDifference formulations, such as Truncated Q-functions, do not scale well to real-world applications. To address this issue, they propose a new approach, called Composite TD3, which extends TD3 to a function-approximation setting, where the target-policy rollout is truncated. The authors propose a farsighted return based on Shifted Q-function, which is a variant of TD(∆). The proposed approach is shown to outperform TD3 in the function-assumption setting, Model-based Value Expansion, and TD3(Ł) which is an off-policymaker variant of the truncated rollout. They also show that the proposed approach can be extended to the tabular case, and that the Composite Q-learning algorithm outperforms TD3 on a number of simulated robot tasks. Finally, they compare the performance of the proposed composite TD3 with TD3 and other off-pole multi-stage approaches in terms of data-efficacy and show that it outperforms them in both shortand long-term predictions. ","This paper studies the problem of improving the data-efficiency of Deep Q-learning in the context of off-policy reinforcement learning methods for robot control. In realworld applications, multi-step TD-learning is commonly used to improve data-efficient in order to reduce the number of steps required to learn a target policy. However, the authors point out that the current TemporalDifference formulations, such as Truncated Q-functions, do not scale well to real-world applications. To address this issue, they propose a new approach, called Composite TD3, which extends TD3 to a function-approximation setting, where the target-policy rollout is truncated. The authors propose a farsighted return based on Shifted Q-function, which is a variant of TD(∆). The proposed approach is shown to outperform TD3 in the function-assumption setting, Model-based Value Expansion, and TD3(Ł) which is an off-policymaker variant of the truncated rollout. They also show that the proposed approach can be extended to the tabular case, and that the Composite Q-learning algorithm outperforms TD3 on a number of simulated robot tasks. Finally, they compare the performance of the proposed composite TD3 with TD3 and other off-pole multi-stage approaches in terms of data-efficacy and show that it outperforms them in both shortand long-term predictions. "
12582,SP:64564b09bd68e7af17845019193825794f08e99b,"reinforcement learning USED-FOR real world robotics. dexterous manipulation USED-FOR system. manually designed resets USED-FOR learning. on - board perception USED-FOR manually designed resets. on - board perception USED-FOR learning. dexterous robotic manipulation tasks EVALUATE-FOR system. system USED-FOR vision - based skills. real - world three - fingered hand FEATURE-OF vision - based skills. Material is instrumented laboratory scenarios. Method are continuous learning, and robotic learning system. OtherScientificTerm are hand - engineered reward functions, and human intervention. Generic are solutions, and learning paradigm. ","This paper presents a new approach to reinforcement learning for real world robotics. The system is trained on dexterous manipulation tasks with the goal of learning a system that is able to learn a set of skills in instrumented laboratory scenarios. The authors propose to use continuous learning, where the goal is to avoid hand-engineered reward functions that require human intervention. To this end, the authors propose a robotic learning system that uses on-board perception and manually designed resets to guide learning. The proposed system is evaluated on a series of dexterous robotic manipulation tasks and is shown to learn vision-based skills on a real-world three-fingered hand.    The authors show that the proposed solutions are effective in the sense that the learning paradigm does not require any additional human intervention, and that the system can be trained to learn the skills in an unsupervised way. ","This paper presents a new approach to reinforcement learning for real world robotics. The system is trained on dexterous manipulation tasks with the goal of learning a system that is able to learn a set of skills in instrumented laboratory scenarios. The authors propose to use continuous learning, where the goal is to avoid hand-engineered reward functions that require human intervention. To this end, the authors propose a robotic learning system that uses on-board perception and manually designed resets to guide learning. The proposed system is evaluated on a series of dexterous robotic manipulation tasks and is shown to learn vision-based skills on a real-world three-fingered hand.    The authors show that the proposed solutions are effective in the sense that the learning paradigm does not require any additional human intervention, and that the system can be trained to learn the skills in an unsupervised way. "
12591,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"adversarial examples USED-FOR Neural network robustness. labeled data USED-FOR adversarially robust generalization. perturbed test data EVALUATE-FOR networks. unlabeled data USED-FOR model. adversarially robust generalization FEATURE-OF model. classification accuracy EVALUATE-FOR accuracy part. accuracy part HYPONYM-OF parts. unlabeled data USED-FOR part. adversarially robust generalization COMPARE generalization. generalization COMPARE adversarially robust generalization. generalization PART-OF supervised learning. adversarially robust generalization USED-FOR Gaussian mixture problem. adversarial training algorithm USED-FOR adversarial robust generalization. MNIST CONJUNCTION Cifar-10. Cifar-10 CONJUNCTION MNIST. MNIST USED-FOR adversarial robust generalization. Cifar-10 USED-FOR adversarial robust generalization. unlabeled data USED-FOR adversarial training algorithm. Method is risk decomposition theorem. OtherScientificTerm are expected robust risk, prediction stability, perturbations, and label information. Metric is stability part. ","This paper studies the problem of adversarial examples in the context of Neural network robustness. The authors prove a risk decomposition theorem that shows that adversarially robust generalization under the presence of labeled data on perturbed test data is a function of the expected robust risk of a model trained on unlabeled data. They show that under the assumption that the prediction stability (i.e., prediction stability of the model on the perturbations) and the classification accuracy of the trained model is the same, two parts of the stability part can be decomposed as follows: (1) the accuracy part depends on the number of perturbed examples and (2) on the label information. The paper then shows that under this assumption, the adversarial generalization of a Gaussian mixture problem can be seen as the generalization part of supervised learning.  The paper also shows that an adversarial training algorithm trained on the same level of perturbation as the one trained with unlabeling is robust to adversarial attacks on MNIST and Cifar-10.   ","This paper studies the problem of adversarial examples in the context of Neural network robustness. The authors prove a risk decomposition theorem that shows that adversarially robust generalization under the presence of labeled data on perturbed test data is a function of the expected robust risk of a model trained on unlabeled data. They show that under the assumption that the prediction stability (i.e., prediction stability of the model on the perturbations) and the classification accuracy of the trained model is the same, two parts of the stability part can be decomposed as follows: (1) the accuracy part depends on the number of perturbed examples and (2) on the label information. The paper then shows that under this assumption, the adversarial generalization of a Gaussian mixture problem can be seen as the generalization part of supervised learning.  The paper also shows that an adversarial training algorithm trained on the same level of perturbation as the one trained with unlabeling is robust to adversarial attacks on MNIST and Cifar-10.   "
12600,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"estimation of advantage USED-FOR reinforcement learning algorithms. promotion focus CONJUNCTION prevention focus. prevention focus CONJUNCTION promotion focus. order statistics FEATURE-OF path ensemble. promotion focus FEATURE-OF learning process. prevention focus FEATURE-OF learning process. promotion focus COMPARE prevention focus. prevention focus COMPARE promotion focus. Terrain locomotion CONJUNCTION Atari games. Atari games CONJUNCTION Terrain locomotion. Atari games CONJUNCTION sparse - reward environments. sparse - reward environments CONJUNCTION Atari games. MuJoCo continuous control CONJUNCTION Terrain locomotion. Terrain locomotion CONJUNCTION MuJoCo continuous control. benchmarks EVALUATE-FOR schemes. schemes COMPARE mainstream methods. mainstream methods COMPARE schemes. benchmarks EVALUATE-FOR mainstream methods. MuJoCo continuous control HYPONYM-OF benchmarks. sparse - reward environments HYPONYM-OF benchmarks. Atari games HYPONYM-OF benchmarks. Terrain locomotion HYPONYM-OF benchmarks. Generic are it, and formulation. OtherScientificTerm are regulatory focuses, regulatory focus, and sparse rewards. ","This paper studies the estimation of advantage for reinforcement learning algorithms. The authors propose two regulatory focuses: promotion focus and prevention focus. They show that the learning process with promotion focus is more stable than that with prevention focus, and that it is more robust to regulatory focuses. They also show that regulatory focus can be applied to sparse rewards. They further show that path ensemble with respect to order statistics is a regulatory focus. Finally, they show that these schemes outperform mainstream methods on three benchmarks: MuJoCo continuous control, Terrain locomotion, and Atari games. ","This paper studies the estimation of advantage for reinforcement learning algorithms. The authors propose two regulatory focuses: promotion focus and prevention focus. They show that the learning process with promotion focus is more stable than that with prevention focus, and that it is more robust to regulatory focuses. They also show that regulatory focus can be applied to sparse rewards. They further show that path ensemble with respect to order statistics is a regulatory focus. Finally, they show that these schemes outperform mainstream methods on three benchmarks: MuJoCo continuous control, Terrain locomotion, and Atari games. "
12609,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"precision gating ( PG ) USED-FOR deep neural networks. approach USED-FOR DNN architectures. computational cost EVALUATE-FOR DNN execution. PG USED-FOR CNNs. PG USED-FOR statically compressed mobile - friendly networks. statically compressed mobile - friendly networks HYPONYM-OF CNNs. ShuffleNet HYPONYM-OF statically compressed mobile - friendly networks. prediction - based quantization schemes COMPARE PG. PG COMPARE prediction - based quantization schemes. compute EVALUATE-FOR PG. ImageNet EVALUATE-FOR PG. accuracy EVALUATE-FOR PG. PG USED-FOR RNNs. 8 - bit uniform quantization COMPARE PG. PG COMPARE 8 - bit uniform quantization. computational cost reduction EVALUATE-FOR LSTM. Penn Tree Bank dataset EVALUATE-FOR LSTM. computational cost reduction EVALUATE-FOR PG. perplexity EVALUATE-FOR PG. Metric are precision, and accuracy loss. ","This paper proposes a precision gating (PG) for training deep neural networks. The approach is applied to a variety of DNN architectures and aims to reduce the computational cost for DNN execution. The paper shows that PG can be applied to CNNs (statically compressed mobile-friendly networks, such as ShuffleNet) and RNNs (staticly compressed RNN). Compared to previous prediction-based quantization schemes, PG is more efficient in terms of compute and accuracy. PG is evaluated on ImageNet and is shown to be competitive with 8-bit uniform quantization. PG can also be used to improve the accuracy of RNN and LSTM on the Penn Tree Bank dataset with computational cost reduction.   The paper also shows that the precision of PG is not only competitive with the accuracy loss, but that PG is also competitive with a number of state-of-the-art methods. The authors also show that PG leads to a significant decrease in perplexity, which is a measure of how well the accuracy is preserved.","This paper proposes a precision gating (PG) for training deep neural networks. The approach is applied to a variety of DNN architectures and aims to reduce the computational cost for DNN execution. The paper shows that PG can be applied to CNNs (statically compressed mobile-friendly networks, such as ShuffleNet) and RNNs (staticly compressed RNN). Compared to previous prediction-based quantization schemes, PG is more efficient in terms of compute and accuracy. PG is evaluated on ImageNet and is shown to be competitive with 8-bit uniform quantization. PG can also be used to improve the accuracy of RNN and LSTM on the Penn Tree Bank dataset with computational cost reduction.   The paper also shows that the precision of PG is not only competitive with the accuracy loss, but that PG is also competitive with a number of state-of-the-art methods. The authors also show that PG leads to a significant decrease in perplexity, which is a measure of how well the accuracy is preserved."
12618,SP:0c2c9b80c087389168acdd42af15877fb499449b,"clean labeled data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION clean labeled data. unlabeled data PART-OF TD. classifiers PART-OF unsupervised domain adaptation ( UDA ). unlabeled data USED-FOR classifiers. clean labeled data USED-FOR classifiers. noisy labeled data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION noisy labeled data. unlabeled data PART-OF TD. unlabeled data USED-FOR classifiers. noisy labeled data USED-FOR classifiers. Butterfly framework USED-FOR WUDA. solution USED-FOR WUDA. label noise FEATURE-OF SD. Butterfly framework HYPONYM-OF solution. WUDA COMPARE UDA methods. UDA methods COMPARE WUDA. classification USED-FOR TD. models PART-OF Butterfly. deep networks HYPONYM-OF models. Butterfly USED-FOR WUDA. Butterfly COMPARE baseline methods. baseline methods COMPARE Butterfly. WUDA EVALUATE-FOR Butterfly. WUDA EVALUATE-FOR baseline methods. OtherScientificTerm are noisy - to - clean, and SD - to - TD - distributional. ","This paper studies the problem of unsupervised domain adaptation (UDA), where the goal is to train classifiers on both clean labeled data and unlabeled data from the source domain. The authors propose a novel solution called ""WUDA"" based on the Butterfly framework, where the classifiers are trained on noisy labeled data from both the source and target domain. They show that under the assumption that the label noise in the target domain is low-rank, WUDA outperforms existing UDA methods. They also show that classifiers trained on the noisy-to-clean are more robust to label noise than on the clean data.    The main contribution of the paper is that the proposed solution, called ""Butterfly"", is a solution to the problem in which the classifier is trained on both the target and source domains. The main idea is to learn classifiers that are robust to noisy labeled and clean labels, and then use these classifiers to train a classifier on the source (clean) and target (unlabeled) domain. This is done by learning a SD- to-TD-distributional, which is a weighted sum of the SD of the clean and noisy labels. The paper shows that under this setting, the proposed method is more robust than existing methods. In addition, the paper also shows that the classification for TD is also more robust.  The paper also proposes two models in the proposed Butterfly, namely ""deep networks"" and ""deep neural networks"". The authors show that Butterfly outperforms other baseline methods in W UDA on WUVAE and WU DAE, and outperforms baseline methods when the target data is clean.","This paper studies the problem of unsupervised domain adaptation (UDA), where the goal is to train classifiers on both clean labeled data and unlabeled data from the source domain. The authors propose a novel solution called ""WUDA"" based on the Butterfly framework, where the classifiers are trained on noisy labeled data from both the source and target domain. They show that under the assumption that the label noise in the target domain is low-rank, WUDA outperforms existing UDA methods. They also show that classifiers trained on the noisy-to-clean are more robust to label noise than on the clean data.    The main contribution of the paper is that the proposed solution, called ""Butterfly"", is a solution to the problem in which the classifier is trained on both the target and source domains. The main idea is to learn classifiers that are robust to noisy labeled and clean labels, and then use these classifiers to train a classifier on the source (clean) and target (unlabeled) domain. This is done by learning a SD- to-TD-distributional, which is a weighted sum of the SD of the clean and noisy labels. The paper shows that under this setting, the proposed method is more robust than existing methods. In addition, the paper also shows that the classification for TD is also more robust.  The paper also proposes two models in the proposed Butterfly, namely ""deep networks"" and ""deep neural networks"". The authors show that Butterfly outperforms other baseline methods in W UDA on WUVAE and WU DAE, and outperforms baseline methods when the target data is clean."
12627,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"agent USED-FOR control tasks. high - dimensional images USED-FOR agent. model - free reinforcement learning ( RL ) USED-FOR agent. model - free reinforcement learning ( RL ) USED-FOR control tasks. high - dimensional images USED-FOR control tasks. control policy USED-FOR task. latent representation CONJUNCTION control policy. control policy CONJUNCTION latent representation. latent representation USED-FOR task. scarce reward signal USED-FOR high - capacity encoder. relevant features USED-FOR task. representation learning USED-FOR image - based RL. image reconstruction loss USED-FOR representation learning. auxiliary decoder USED-FOR end - to - end. auxiliary decoder USED-FOR off - policy actor - critic algorithm. control tasks EVALUATE-FOR model - free and model - based algorithms. OtherScientificTerm are suboptimal convergence, and latent features. Metric is sample efficiency. Method is off - policy algorithms. Task is image - based RL1. ","This paper proposes a model-free reinforcement learning (RL) algorithm that learns a high-capacity encoder and a low-capacity decoder from high-dimensional images to solve control tasks. The agent is trained to solve a set of control tasks using high-dimensionality of the input data, and the agent learns a latent representation and a control policy to solve the task using the learned latent representation. The paper shows that the agent can achieve suboptimal convergence to the optimal solution in the presence of a scarce reward signal. The authors also show that the sample efficiency of the agent is improved when the latent features are highly correlated with the reward.  The paper also shows that representation learning in image-based RL can be improved by using an additional image reconstruction loss to encourage the representation learning to capture relevant features for the task. The off-policy actor-critic algorithm uses an auxiliary decoder to guide the end-to-end learning of the decoder and uses the learned decoder as an auxiliary encoder in order to improve sample efficiency. Experiments are conducted on several control tasks and show that both model- free and model-based algorithms perform well on the control tasks, outperforming the state-of-the-art of the art in terms of sample efficiency, and outperforming off-policies. The main contribution of the paper is the introduction of a novel algorithm, which is based on the idea of learning a high capacity encoder from scarce reward signals, and using the auxiliary decoders to guide representation learning.    The authors provide a theoretical analysis of the proposed algorithm, and provide experimental results showing that the proposed method achieves better sample efficiency compared to the state of art. The experimental results are also show improved sample efficiency when the number of latent features is small.  Finally, the authors conduct an ablation study on the performance of their algorithm, showing that their algorithm is able to outperform a number of existing algorithms, including image-free RL1, which does not use any auxiliary features. ","This paper proposes a model-free reinforcement learning (RL) algorithm that learns a high-capacity encoder and a low-capacity decoder from high-dimensional images to solve control tasks. The agent is trained to solve a set of control tasks using high-dimensionality of the input data, and the agent learns a latent representation and a control policy to solve the task using the learned latent representation. The paper shows that the agent can achieve suboptimal convergence to the optimal solution in the presence of a scarce reward signal. The authors also show that the sample efficiency of the agent is improved when the latent features are highly correlated with the reward.  The paper also shows that representation learning in image-based RL can be improved by using an additional image reconstruction loss to encourage the representation learning to capture relevant features for the task. The off-policy actor-critic algorithm uses an auxiliary decoder to guide the end-to-end learning of the decoder and uses the learned decoder as an auxiliary encoder in order to improve sample efficiency. Experiments are conducted on several control tasks and show that both model- free and model-based algorithms perform well on the control tasks, outperforming the state-of-the-art of the art in terms of sample efficiency, and outperforming off-policies. The main contribution of the paper is the introduction of a novel algorithm, which is based on the idea of learning a high capacity encoder from scarce reward signals, and using the auxiliary decoders to guide representation learning.    The authors provide a theoretical analysis of the proposed algorithm, and provide experimental results showing that the proposed method achieves better sample efficiency compared to the state of art. The experimental results are also show improved sample efficiency when the number of latent features is small.  Finally, the authors conduct an ablation study on the performance of their algorithm, showing that their algorithm is able to outperform a number of existing algorithms, including image-free RL1, which does not use any auxiliary features. "
12636,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"it USED-FOR tasks. Dropout HYPONYM-OF regularization technique. it USED-FOR DNNs. regularization technique USED-FOR generalization. regularization technique USED-FOR deep neural networks ( DNNs ). DNNs USED-FOR tasks. dropout USED-FOR training. dropout technique USED-FOR accelerating training. accelerating training CONJUNCTION generalization. generalization CONJUNCTION accelerating training. dropout technique USED-FOR generalization. multi - sample dropout HYPONYM-OF dropout technique. dropout USED-FOR generalization. multi - sample dropout USED-FOR dropout samples. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. ILSVRC 2012 ( ImageNet ) CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION ILSVRC 2012 ( ImageNet ). multi - sample dropout USED-FOR training. CIFAR-100 CONJUNCTION SVHN datasets. SVHN datasets CONJUNCTION CIFAR-100. multi - sample dropout USED-FOR image classification tasks. ILSVRC 2012 ( ImageNet ) CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION ILSVRC 2012 ( ImageNet ). CIFAR-10 USED-FOR image classification tasks. ILSVRC 2012 ( ImageNet ) USED-FOR image classification tasks. SVHN datasets USED-FOR image classification tasks. convolution layers CONJUNCTION dropout layer. dropout layer CONJUNCTION convolution layers. computation time FEATURE-OF convolution layers. training set CONJUNCTION validation set. validation set CONJUNCTION training set. error rates CONJUNCTION losses. losses CONJUNCTION error rates. validation set EVALUATE-FOR networks. losses EVALUATE-FOR networks. training set EVALUATE-FOR networks. error rates EVALUATE-FOR networks. multi - sample dropout USED-FOR networks. OtherScientificTerm are overfitting, dropout sample, sample losses, and fully connected layers. Metric is loss. Generic are technique, operator, and network. Method","Dropout is a popular regularization technique for deep neural networks (DNNs) and it has been shown that it can accelerate training of DNNs on a variety of tasks. However, dropout can be problematic for training as it can lead to overfitting. This paper proposes a new dropout technique, called multi-sample dropout, to address the issue of accelerating training and improving generalization. The technique is based on the observation that the dropout sample is not always the best one for a particular task. To address this issue, the authors propose to use multi-sampling dropout to generate dropout samples for all dropout layers of the network. The authors show that this technique accelerates training on image classification tasks on ILSVRC 2012 (ImageNet), CIFAR-10, CifAR-100, and SVHN datasets. They also show that the training set and validation set of the networks trained with multi-Sample dropout have similar error rates and losses. The main contribution of this paper is that the authors consider the computation time of convolution layers and dropout layer, and show that for fully connected layers, the number of samples per dropout is the same as that of a fully connected layer. The paper also shows that the sample losses are the same across all layers. ","Dropout is a popular regularization technique for deep neural networks (DNNs) and it has been shown that it can accelerate training of DNNs on a variety of tasks. However, dropout can be problematic for training as it can lead to overfitting. This paper proposes a new dropout technique, called multi-sample dropout, to address the issue of accelerating training and improving generalization. The technique is based on the observation that the dropout sample is not always the best one for a particular task. To address this issue, the authors propose to use multi-sampling dropout to generate dropout samples for all dropout layers of the network. The authors show that this technique accelerates training on image classification tasks on ILSVRC 2012 (ImageNet), CIFAR-10, CifAR-100, and SVHN datasets. They also show that the training set and validation set of the networks trained with multi-Sample dropout have similar error rates and losses. The main contribution of this paper is that the authors consider the computation time of convolution layers and dropout layer, and show that for fully connected layers, the number of samples per dropout is the same as that of a fully connected layer. The paper also shows that the sample losses are the same across all layers. "
12645,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"filters CONJUNCTION features. features CONJUNCTION filters. filter ambiguity FEATURE-OF CNNs. interpretability EVALUATE-FOR models. strategy USED-FOR interpretable CNNs. model USED-FOR disentangled filters. Label Sensitive Gate ( LSG ) structure USED-FOR model. supervised manner USED-FOR model. sparsity regularization USED-FOR LSG. LSG USED-FOR redundant filters. training strategy COMPARE model. model COMPARE training strategy. redundancy CONJUNCTION interpretability. interpretability CONJUNCTION redundancy. redundancy EVALUATE-FOR model. interpretability EVALUATE-FOR training strategy. interpretability EVALUATE-FOR model. Method is Convolutional neural networks ( CNNs ). Generic are pre - trained model, and method. OtherScientificTerm are redundant channels, and periodical shutdown. ",This paper studies the problem of filter ambiguity in Convolutional neural networks (CNNs). The authors propose a strategy to learn interpretable CNNs by disentangling the filters and features of a pre-trained model. They propose a model that uses the Label Sensitive Gate (LSG) structure to learn disentangled filters. The model is trained in a supervised manner. The LSG is trained with sparsity regularization to prevent the redundant channels from being removed during training. The authors show that the LSG can be used to disentangle redundant filters and that the proposed training strategy outperforms the original model in terms of redundancy and interpretability. They also show that their method is more robust to periodical shutdown. ,This paper studies the problem of filter ambiguity in Convolutional neural networks (CNNs). The authors propose a strategy to learn interpretable CNNs by disentangling the filters and features of a pre-trained model. They propose a model that uses the Label Sensitive Gate (LSG) structure to learn disentangled filters. The model is trained in a supervised manner. The LSG is trained with sparsity regularization to prevent the redundant channels from being removed during training. The authors show that the LSG can be used to disentangle redundant filters and that the proposed training strategy outperforms the original model in terms of redundancy and interpretability. They also show that their method is more robust to periodical shutdown. 
12654,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"scalability CONJUNCTION non - stationarity. non - stationarity CONJUNCTION scalability. value function factorization learning USED-FOR collaborative multiagent systems. communication USED-FOR tasks. framework USED-FOR nearly decomposable Q - functions ( NDQ ). communication minimization USED-FOR framework. value function factorization learning CONJUNCTION communication learning. communication learning CONJUNCTION value function factorization learning. value function factorization learning USED-FOR framework. communication learning USED-FOR framework. information - theoretic regularizers USED-FOR framework. mutual information EVALUATE-FOR regularizers. regularizers COMPARE value function factorization methods. value function factorization methods COMPARE regularizers. QMIX HYPONYM-OF value function factorization methods. StarCraft unit micromanagement benchmark EVALUATE-FOR framework. framework COMPARE baseline methods. baseline methods COMPARE framework. StarCraft unit micromanagement benchmark EVALUATE-FOR baseline methods. Method is Reinforcement learning. Task is multi - agent settings. OtherScientificTerm are decentralized value functions, coordination, action selection, and communication messages. ","This paper studies the problem of value function factorization learning in collaborative multiagent systems. Reinforcement learning in multi-agent settings is challenging due to scalability and non-stationarity. The authors propose a new framework for nearly decomposable Q-functions (NDQ) based on communication minimization. The key idea is to learn decentralized value functions for each agent in a cooperative setting, where communication between agents is important to perform well on different tasks. The proposed framework combines the benefits of both value function factoring and communication learning. The framework uses information-theoretic regularizers based on mutual information to ensure that the coordination between agents does not suffer due to lack of action selection. The regularizers are evaluated on the StarCraft unit micromanagement benchmark and show that the proposed regularizers outperform existing value function factors (e.g. QMIX) in terms of mutual information.  ","This paper studies the problem of value function factorization learning in collaborative multiagent systems. Reinforcement learning in multi-agent settings is challenging due to scalability and non-stationarity. The authors propose a new framework for nearly decomposable Q-functions (NDQ) based on communication minimization. The key idea is to learn decentralized value functions for each agent in a cooperative setting, where communication between agents is important to perform well on different tasks. The proposed framework combines the benefits of both value function factoring and communication learning. The framework uses information-theoretic regularizers based on mutual information to ensure that the coordination between agents does not suffer due to lack of action selection. The regularizers are evaluated on the StarCraft unit micromanagement benchmark and show that the proposed regularizers outperform existing value function factors (e.g. QMIX) in terms of mutual information.  "
12663,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"AI USED-FOR complex programs. programming puzzles USED-FOR computers programming. programming puzzle HYPONYM-OF Boolean function. input - output pairs CONJUNCTION English problem descriptions. English problem descriptions CONJUNCTION input - output pairs. GAN - like algorithm USED-FOR puzzles. Troublemaker USED-FOR puzzles. Troublemaker HYPONYM-OF GAN - like algorithm. GAN - like algorithm USED-FOR automatic puzzle generation. Generic are problems, and it. OtherScientificTerm is Puzzles. Task is program synthesis. Method are puzzle - solver, and puzzle - solving techniques. ","This paper studies the problem of solving programming puzzles in the context of computers programming, where AI is used to solve complex programs. In particular, the authors consider a Boolean function called a programming puzzle, which is a set of problems where the goal is to solve a sequence of programs. Puzzles can be solved by solving a program synthesis problem. The authors propose a GAN-like algorithm called ""Troublemaker"" to solve such puzzles, and show that it can be trained to solve the problem efficiently. They also show that the GAN can be used for automatic puzzle generation.    The paper is well-written, well-motivated, and well-structured. The problem of program synthesis is interesting, and the idea of using input-output pairs and English problem descriptions is interesting. However, the paper suffers from a lack of technical novelty, and it is not clear to me that the proposed puzzle-solving techniques are novel. ","This paper studies the problem of solving programming puzzles in the context of computers programming, where AI is used to solve complex programs. In particular, the authors consider a Boolean function called a programming puzzle, which is a set of problems where the goal is to solve a sequence of programs. Puzzles can be solved by solving a program synthesis problem. The authors propose a GAN-like algorithm called ""Troublemaker"" to solve such puzzles, and show that it can be trained to solve the problem efficiently. They also show that the GAN can be used for automatic puzzle generation.    The paper is well-written, well-motivated, and well-structured. The problem of program synthesis is interesting, and the idea of using input-output pairs and English problem descriptions is interesting. However, the paper suffers from a lack of technical novelty, and it is not clear to me that the proposed puzzle-solving techniques are novel. "
12672,SP:627b515cc893ff33914dff255f5d6e136441d2e2,"structured decomposition of their behavior USED-FOR Reinforcement learning agents. lower - level primitives CONJUNCTION higher - level meta - policy. higher - level meta - policy CONJUNCTION lower - level primitives. lower - level primitives PART-OF policy. primitives PART-OF hierarchical reinforcement learning. primitives PART-OF policy design. information - theoretic mechanism USED-FOR decentralized decision. natural competition CONJUNCTION specialization. specialization CONJUNCTION natural competition. policy architecture COMPARE flat and hierarchical policies. flat and hierarchical policies COMPARE policy architecture. generalization EVALUATE-FOR policy architecture. Method is meta - policy. OtherScientificTerm are high - level meta - policy, and primitive. ","This paper proposes a structured decomposition of their behavior for Reinforcement learning agents. The policy is decomposed into a set of lower-level primitives and a higher-level meta-policy. The primitives are learned in hierarchical reinforcement learning, and the policy design incorporates these primitives into policy design. The meta-policies are trained to maximize the mutual information between the primitive of the lower level and the high-level. The paper also proposes an information-theoretic mechanism to learn a decentralized decision. Experiments show that the proposed policy architecture outperforms flat and hierarchical policies in terms of generalization, natural competition, and specialization. ","This paper proposes a structured decomposition of their behavior for Reinforcement learning agents. The policy is decomposed into a set of lower-level primitives and a higher-level meta-policy. The primitives are learned in hierarchical reinforcement learning, and the policy design incorporates these primitives into policy design. The meta-policies are trained to maximize the mutual information between the primitive of the lower level and the high-level. The paper also proposes an information-theoretic mechanism to learn a decentralized decision. Experiments show that the proposed policy architecture outperforms flat and hierarchical policies in terms of generalization, natural competition, and specialization. "
12681,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"models USED-FOR highdimensional state spaces. Model - based reinforcement learning methods USED-FOR models. rewards USED-FOR latent dynamics model. model - based planning framework USED-FOR latent reward prediction model. multi - step reward prediction USED-FOR planning. multi - step reward prediction USED-FOR latent representation. concise model - free representation USED-FOR framework. multi - pendulum and multi - cheetah environments FEATURE-OF framework. concise latent representation USED-FOR environments. method USED-FOR latent reward prediction model. method COMPARE model - based methods. model - based methods COMPARE method. sample efficiency EVALUATE-FOR model - free and model - based baselines. Planning COMPARE model - free and model - based baselines. model - free and model - based baselines COMPARE Planning. latent state - space FEATURE-OF Planning. sample efficiency EVALUATE-FOR Planning. Method are model - free reinforcement learning, and model - based algorithms. OtherScientificTerm are pendulums, and irrelevant information. Generic is them. ","This paper proposes a model-free reinforcement learning method for high-dimensional state-space planning. Model-based reinforcement learning methods have been shown to learn models for highdimensional state spaces, but model-based algorithms do not work well in environments with pendulums and cheetah environments. The authors propose a model based planning framework that learns a latent dynamics model of the environment and uses rewards to guide the learning of a latent reward prediction model. The proposed framework is based on the idea of a multi-step reward prediction to learn a latent representation for planning. The framework is tested in the multi-pendulum and multi-cheetah environment, and the authors show that the proposed framework can learn a clear and concise model -free representation for both environments. They also show that their method can be used to learn the latent reward of a particular environment in a single step. Planning is shown to improve sample efficiency over both model- free and model- based baselines in both environments, with the proposed method outperforming both of them in terms of sample efficiency. Planning also shows that in the case of a single latent state -space, it is possible to learn an efficient method that can be applied to a large number of environments. This is achieved by removing irrelevant information in the latent state space, which can be removed during training.  ","This paper proposes a model-free reinforcement learning method for high-dimensional state-space planning. Model-based reinforcement learning methods have been shown to learn models for highdimensional state spaces, but model-based algorithms do not work well in environments with pendulums and cheetah environments. The authors propose a model based planning framework that learns a latent dynamics model of the environment and uses rewards to guide the learning of a latent reward prediction model. The proposed framework is based on the idea of a multi-step reward prediction to learn a latent representation for planning. The framework is tested in the multi-pendulum and multi-cheetah environment, and the authors show that the proposed framework can learn a clear and concise model -free representation for both environments. They also show that their method can be used to learn the latent reward of a particular environment in a single step. Planning is shown to improve sample efficiency over both model- free and model- based baselines in both environments, with the proposed method outperforming both of them in terms of sample efficiency. Planning also shows that in the case of a single latent state -space, it is possible to learn an efficient method that can be applied to a large number of environments. This is achieved by removing irrelevant information in the latent state space, which can be removed during training.  "
12690,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"GPU / TPU memory limitations CONJUNCTION training times. training times CONJUNCTION GPU / TPU memory limitations. parameterreduction techniques USED-FOR BERT. parameterreduction techniques USED-FOR memory consumption. training speed EVALUATE-FOR BERT. parameterreduction techniques USED-FOR training speed. methods USED-FOR models. models COMPARE BERT. BERT COMPARE models. it USED-FOR downstream tasks. self - supervised loss USED-FOR inter - sentence coherence. multi - sentence inputs USED-FOR it. multi - sentence inputs USED-FOR downstream tasks. model COMPARE BERT - large. BERT - large COMPARE model. GLUE, RACE, and SQuAD benchmarks EVALUATE-FOR model. OtherScientificTerm are model size, and parameters. Method are natural language representations, and pretrained models. ","This paper proposes to reduce the memory footprint of BERT (and BERT-large) by reducing the number of parameters in the model size. This is motivated by GPU/TPU memory limitations and training times. The authors show that existing parameterreduction techniques can be used to reduce memory consumption and improve training speed for BERT. They also show that these methods can be applied to other models as well, and that these models perform better than BERT in terms of training speed.   The authors also propose a self-supervised loss to improve the inter-sentence coherence between natural language representations and pretrained models. The proposed model is evaluated on GLUE, RACE, and SQuAD benchmarks, and it is shown to perform well on downstream tasks with multi-sentences inputs. ","This paper proposes to reduce the memory footprint of BERT (and BERT-large) by reducing the number of parameters in the model size. This is motivated by GPU/TPU memory limitations and training times. The authors show that existing parameterreduction techniques can be used to reduce memory consumption and improve training speed for BERT. They also show that these methods can be applied to other models as well, and that these models perform better than BERT in terms of training speed.   The authors also propose a self-supervised loss to improve the inter-sentence coherence between natural language representations and pretrained models. The proposed model is evaluated on GLUE, RACE, and SQuAD benchmarks, and it is shown to perform well on downstream tasks with multi-sentences inputs. "
12699,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"Transformer HYPONYM-OF neural network architecture. neural network architecture USED-FOR language understanding. Transformer USED-FOR language understanding. text CONJUNCTION videos. videos CONJUNCTION text. image CONJUNCTION text. text CONJUNCTION image. modalities FEATURE-OF tasks. image HYPONYM-OF modalities. text HYPONYM-OF modalities. videos HYPONYM-OF modalities. temporal input sequence FEATURE-OF hidden states. model USED-FOR tasks. architecture USED-FOR model. model USED-FOR asynchronous multi - task learning. multiple input modalities CONJUNCTION asynchronous multi - task learning. asynchronous multi - task learning CONJUNCTION multiple input modalities. model USED-FOR multiple input modalities. architecture USED-FOR tasks. tasks CONJUNCTION asynchronous multi - task learning. asynchronous multi - task learning CONJUNCTION tasks. multiple input modalities FEATURE-OF tasks. image captioning CONJUNCTION visual question answering. visual question answering CONJUNCTION image captioning. visual question answering CONJUNCTION video activity recognition. video activity recognition CONJUNCTION visual question answering. OmniNet USED-FOR tasks. part - of - speech tagging CONJUNCTION image captioning. image captioning CONJUNCTION part - of - speech tagging. OmniNet USED-FOR part - of - speech tagging. part - of - speech tagging CONJUNCTION visual question answering. visual question answering CONJUNCTION part - of - speech tagging. video activity recognition HYPONYM-OF tasks. part - of - speech tagging HYPONYM-OF tasks. visual question answering HYPONYM-OF tasks. image captioning HYPONYM-OF tasks. compressed model EVALUATE-FOR tasks. video captioning CONJUNCTION video question answering. video question answering CONJUNCTION video captioning. neural network USED-FOR tasks. modalities USED-FOR neural network. video question answering HYPONYM-OF tasks. video captioning HYPONYM-OF tasks. spatio - temporal cache PART-OF OmniNet. spatio - temporal cache USED-FOR self - attention mechanism. Method is spatio - temporal cache mechanism. Generic are it, and them. ","This paper proposes a new neural network architecture called OmniNet, which is an extension of the Transformer architecture for language understanding. The proposed model is able to handle multiple modalities (image, text, videos) and multiple tasks with different modalities in a single architecture. The model can be used for tasks with multiple input modalities and asynchronous multi-task learning. The authors propose a spatio-temporal cache mechanism, where the hidden states are stored in the temporal input sequence. OmniNet can be applied to a variety of tasks including part-of-speech tagging, image captioning, visual question answering, and video activity recognition.  The authors show that the compressed model performs well on all the tasks. They also show that a neural network trained with multiple inputs across modalities can be trained with a single neural network, and that it can generalize to new modalities.    The main contribution of the paper is that the authors propose to add a spatial component to the self-attention mechanism in OmniNet. Specifically, the authors introduce a spatial feature in the form of a spatiotemporal cache that is shared across all modalities, and they use it to share information between different layers of the neural network. They show that OmniNet is capable of performing well on multiple tasks, and can generalise to new tasks in a compressed model. The paper also shows that the spatio temporal cache is a key component of OmniNet and the authors show how to use it for multiple tasks. ","This paper proposes a new neural network architecture called OmniNet, which is an extension of the Transformer architecture for language understanding. The proposed model is able to handle multiple modalities (image, text, videos) and multiple tasks with different modalities in a single architecture. The model can be used for tasks with multiple input modalities and asynchronous multi-task learning. The authors propose a spatio-temporal cache mechanism, where the hidden states are stored in the temporal input sequence. OmniNet can be applied to a variety of tasks including part-of-speech tagging, image captioning, visual question answering, and video activity recognition.  The authors show that the compressed model performs well on all the tasks. They also show that a neural network trained with multiple inputs across modalities can be trained with a single neural network, and that it can generalize to new modalities.    The main contribution of the paper is that the authors propose to add a spatial component to the self-attention mechanism in OmniNet. Specifically, the authors introduce a spatial feature in the form of a spatiotemporal cache that is shared across all modalities, and they use it to share information between different layers of the neural network. They show that OmniNet is capable of performing well on multiple tasks, and can generalise to new tasks in a compressed model. The paper also shows that the spatio temporal cache is a key component of OmniNet and the authors show how to use it for multiple tasks. "
12708,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"tasks EVALUATE-FOR Deep learning models. predictive accuracy EVALUATE-FOR Deep learning models. methods USED-FOR uncertainty quantification. Bayesian neural networks USED-FOR methods. discriminative accuracy EVALUATE-FOR approximate posterior inference. frequentist approach USED-FOR uncertainty quantification. formal inference procedure USED-FOR predictive confidence intervals. discriminative jackknife ( DJ ) HYPONYM-OF formal inference procedure. formal inference procedure USED-FOR regression models. higher - order influence functions ( HOIFs ) FEATURE-OF model parameters. higher - order influence functions ( HOIFs ) USED-FOR DJ procedure. loss gradients CONJUNCTION Hessian - vector products. Hessian - vector products CONJUNCTION loss gradients. oracle access USED-FOR loss gradients. DJ USED-FOR HOIFs. oracle access CONJUNCTION Hessian - vector products. Hessian - vector products CONJUNCTION oracle access. oracle access USED-FOR recursive formula. model accuracy EVALUATE-FOR it. recursive formula USED-FOR DJ. recursive formula USED-FOR HOIFs. DJ COMPARE Bayesian and non - Bayesian baselines. Bayesian and non - Bayesian baselines COMPARE DJ. OtherScientificTerm are predictive uncertainty, and Bayesian credible intervals. Material is highand low - confidence prediction instances. Method is Bayesian methods. Metric is frequentist coverage. Task is model training. ","Deep learning models have been shown to achieve state-of-the-art predictive accuracy on a variety of tasks. However, uncertainty quantification of the predictive uncertainty is difficult due to the high and low-confidence prediction instances. This paper proposes a frequentist approach to improve the predictive accuracy of Bayesian neural networks. The authors propose a formal inference procedure called discriminative jackknife (DJ) to compute the predictive confidence intervals for regression models. The proposed DJ procedure is based on higher-order influence functions (HOIFs) on the model parameters, which are used in previous Bayesian methods.    The authors show that DJ can be used to compute HOIFs using a recursive formula based on oracle access to the loss gradients and Hessian-vector products, and that it improves model accuracy in the presence of frequentist coverage. In addition, the authors also demonstrate that DJ improves the model training performance on a number of datasets and outperforms Bayesian and non-Bayesian baselines in terms of predictive uncertainty. ","Deep learning models have been shown to achieve state-of-the-art predictive accuracy on a variety of tasks. However, uncertainty quantification of the predictive uncertainty is difficult due to the high and low-confidence prediction instances. This paper proposes a frequentist approach to improve the predictive accuracy of Bayesian neural networks. The authors propose a formal inference procedure called discriminative jackknife (DJ) to compute the predictive confidence intervals for regression models. The proposed DJ procedure is based on higher-order influence functions (HOIFs) on the model parameters, which are used in previous Bayesian methods.    The authors show that DJ can be used to compute HOIFs using a recursive formula based on oracle access to the loss gradients and Hessian-vector products, and that it improves model accuracy in the presence of frequentist coverage. In addition, the authors also demonstrate that DJ improves the model training performance on a number of datasets and outperforms Bayesian and non-Bayesian baselines in terms of predictive uncertainty. "
12717,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,Generative Adversarial Networks USED-FOR video samples. complexity CONJUNCTION fidelity. fidelity CONJUNCTION complexity. complex Kinetics-600 dataset EVALUATE-FOR Generative Adversarial Networks. decomposition of its discriminator USED-FOR model. video synthesis CONJUNCTION video prediction. video prediction CONJUNCTION video synthesis. baseline USED-FOR synthesis. Fréchet Inception Distance USED-FOR prediction. Inception Score USED-FOR synthesis. Fréchet Inception Distance CONJUNCTION Inception Score. Inception Score CONJUNCTION Fréchet Inception Distance. prediction USED-FOR Kinetics600. UCF-101 dataset EVALUATE-FOR Inception Score. UCF-101 dataset EVALUATE-FOR synthesis. Kinetics-600 USED-FOR baseline. Kinetics-600 USED-FOR synthesis. Method is Generative models of natural images. Task is video modeling. ,"Generative Adversarial Networks (GANs) are used to generate video samples. Generative models of natural images have been shown to be very powerful in terms of complexity and fidelity, but video modeling has not yet been considered. This paper studies the problem of generating video samples from the complex Kinetics-600 dataset. The authors propose a new model based on the decomposition of its discriminator. They also propose a baseline for video synthesis and video prediction on Kinetics600 based on Fréchet Inception Distance and Inception Score on the UCF-101 dataset.","Generative Adversarial Networks (GANs) are used to generate video samples. Generative models of natural images have been shown to be very powerful in terms of complexity and fidelity, but video modeling has not yet been considered. This paper studies the problem of generating video samples from the complex Kinetics-600 dataset. The authors propose a new model based on the decomposition of its discriminator. They also propose a baseline for video synthesis and video prediction on Kinetics600 based on Fréchet Inception Distance and Inception Score on the UCF-101 dataset."
12726,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"few - shot classification benchmarks EVALUATE-FOR representation. base class data USED-FOR representation. stages USED-FOR representation. spatial attention map USED-FOR background clutter. pre - trained classifier USED-FOR spatial attention map. Method are Few - shot learning, pre - trained network, and meta - learning. OtherScientificTerm is prior knowledge. Material is large - scale dataset. Generic is network. ","This paper studies the problem of few-shot learning, where the goal is to learn a representation that generalizes well on a few few classes without prior knowledge. Few-shot classification benchmarks have been widely used to evaluate the performance of a representation learned on base class data. This paper proposes a meta-learning approach where the representation is learned in two stages. First, a pre-trained network is trained on a large-scale dataset. Second, a spatial attention map is learned on the background clutter generated by a different classifier. The spatial attention maps are then used to train a new network. The network is then fine-tuned on the new dataset.","This paper studies the problem of few-shot learning, where the goal is to learn a representation that generalizes well on a few few classes without prior knowledge. Few-shot classification benchmarks have been widely used to evaluate the performance of a representation learned on base class data. This paper proposes a meta-learning approach where the representation is learned in two stages. First, a pre-trained network is trained on a large-scale dataset. Second, a spatial attention map is learned on the background clutter generated by a different classifier. The spatial attention maps are then used to train a new network. The network is then fine-tuned on the new dataset."
12735,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"molecules CONJUNCTION game maps. game maps CONJUNCTION molecules. generative adversarial networks ( GANs ) USED-FOR structured objects. molecules HYPONYM-OF structured objects. game maps HYPONYM-OF structured objects. structural requirements FEATURE-OF objects. molecules HYPONYM-OF structural requirements. constraints PART-OF model. approach USED-FOR logical constraints. knowledge compilation techniques USED-FOR expected disagreement. knowledge compilation techniques USED-FOR approach. setup USED-FOR hybrid logical - neural constraints. hybrid logical - neural constraints USED-FOR complex requirements. setup USED-FOR complex requirements. graph reachability HYPONYM-OF complex requirements. constrained images CONJUNCTION molecules. molecules CONJUNCTION constrained images. molecules CONJUNCTION video game levels. video game levels CONJUNCTION molecules. Method are constrained adversarial networks ( CANs ), generator, unconstrained GANs, and CANs. ","This paper proposes a new approach to learning logical constraints for constrained adversarial networks (CANs). The authors propose to use generative adversarial neural networks (GANs) to learn constraints for structured objects (e.g., molecules, game maps, etc.). The idea is that the structural requirements of the objects (i.e., molecules and game maps) can be represented as constraints in the model. The approach is based on knowledge compilation techniques to capture the expected disagreement between the generator and discriminator. The authors show that the proposed setup is able to learn hybrid logical-neural constraints for complex requirements such as graph reachability, which is not possible for unconstrained GANs. Experiments are conducted on constrained images, molecules, and video game levels.   ","This paper proposes a new approach to learning logical constraints for constrained adversarial networks (CANs). The authors propose to use generative adversarial neural networks (GANs) to learn constraints for structured objects (e.g., molecules, game maps, etc.). The idea is that the structural requirements of the objects (i.e., molecules and game maps) can be represented as constraints in the model. The approach is based on knowledge compilation techniques to capture the expected disagreement between the generator and discriminator. The authors show that the proposed setup is able to learn hybrid logical-neural constraints for complex requirements such as graph reachability, which is not possible for unconstrained GANs. Experiments are conducted on constrained images, molecules, and video game levels.   "
12744,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"Deep neural networks COMPARE deep neural networks. deep neural networks COMPARE Deep neural networks. fixed activation functions USED-FOR deep neural networks. learnable activation functions USED-FOR Deep neural networks. adaptability of learnable activation functions USED-FOR model. expressive power FEATURE-OF model. expressive power FEATURE-OF adaptability of learnable activation functions. Adaptive Piecewise Linear units ( APL ) USED-FOR learnable activation function. stages PART-OF activation functions. gradient information PART-OF activation functions. Symmetric - APL activations USED-FOR deep neural networks. robustness EVALUATE-FOR deep neural networks. deep neural networks USED-FOR adversarial attacks. Network - in - Network CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION Network - in - Network. Lenet CONJUNCTION Network - in - Network. Network - in - Network CONJUNCTION Lenet. activation functions COMPARE ReLUs. ReLUs COMPARE activation functions. activation functions USED-FOR architectures. ResNet-18 HYPONYM-OF architectures. activation functions USED-FOR adversarial fooling. Lenet HYPONYM-OF architectures. Network - in - Network HYPONYM-OF architectures. OtherScientificTerm are positive and negative halves, zero - centered continuous non - linearity, and SymmetricAPL function. Task is ablation studies. ","Deep neural networks with fixed activation functions have been shown to be more robust to adversarial attacks than deep neural networks trained with learnable activation functions. This paper studies the expressive power of the adaptability of the model with respect to the number of stages in the model. The authors propose Adaptive Piecewise Linear units (APL) as a learnable activations for the positive and negative halves of a deep neural network.    The authors show that the zero-centered continuous non-linearity of the Symmetric-APL activations can be used to improve the robustness of a model.  They also show that a number of existing architectures (e.g., Network-in-Network, ResNet-18, etc.) can be trained with such activation functions that are more robust against adversarial fooling than ReLUs, and that these activation functions can be adapted to a variety of architectures (i.e., LSTMs, ResNets, etc.).   Finally, the authors conduct ablation studies to show that activation functions incorporating gradient information in the first two stages of the activation functions are the most effective for adversarial training. They also provide a theoretical analysis that shows that the adaptable APL function is more robust than the ReLU function. ","Deep neural networks with fixed activation functions have been shown to be more robust to adversarial attacks than deep neural networks trained with learnable activation functions. This paper studies the expressive power of the adaptability of the model with respect to the number of stages in the model. The authors propose Adaptive Piecewise Linear units (APL) as a learnable activations for the positive and negative halves of a deep neural network.    The authors show that the zero-centered continuous non-linearity of the Symmetric-APL activations can be used to improve the robustness of a model.  They also show that a number of existing architectures (e.g., Network-in-Network, ResNet-18, etc.) can be trained with such activation functions that are more robust against adversarial fooling than ReLUs, and that these activation functions can be adapted to a variety of architectures (i.e., LSTMs, ResNets, etc.).   Finally, the authors conduct ablation studies to show that activation functions incorporating gradient information in the first two stages of the activation functions are the most effective for adversarial training. They also provide a theoretical analysis that shows that the adaptable APL function is more robust than the ReLU function. "
12753,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"models CONJUNCTION proprietary data. proprietary data CONJUNCTION models. third party components CONJUNCTION models. models CONJUNCTION third party components. application programming interfaces ( APIs ) USED-FOR pre - trained encapsulated models. wrapping deep learning model USED-FOR classification black - box. wrapping deep learning model USED-FOR measure of uncertainty. Dirichlet layer USED-FOR fusion layer. black - box classifier USED-FOR probabilistic neural network. Dirichlet layer USED-FOR probabilistic neural network. Dirichlet layer USED-FOR distribution. Dirichlet layer USED-FOR estimation of aleatoric uncertainty. multinomial output parameters USED-FOR distribution. uncertainty measure USED-FOR rejection system. NLP CONJUNCTION computer vision. computer vision CONJUNCTION NLP. technique CONJUNCTION methodology. methodology CONJUNCTION technique. wrapper USED-FOR uncertainty. Method are machine learning models, classifier, and simulated API based. Generic are external components, and components. Metric are auditability, and trustability. OtherScientificTerm are uncontrolled potential risks, black - box, aleatoric uncertainty, and misclassifications. ","This paper proposes a method to audit machine learning models in a black-box setting, where there is no access to external components (e.g. third party components, models, proprietary data, etc.). The authors propose to use application programming interfaces (APIs) to train pre-trained encapsulated models that are robust to uncontrolled potential risks. They propose a new measure of uncertainty, aleatoric uncertainty, based on a wrapping deep learning model that is trained to classify a classification black- box. The Dirichlet layer of the fusion layer is used to learn a probabilistic neural network, which is then used to train a classifier on the input data. The authors show that the Dirichlett layer is able to capture the distribution over the multinomial output parameters of the classifier, and thus can be used as an uncertainty measure for a rejection system. They also show that their technique and methodology can be applied to a variety of applications, including NLP, computer vision, and NLP.    The paper is well-written and well-motivated. The idea of using external components to auditability is interesting, and the paper is clearly written. However, there are a number of issues with the paper:   1. The paper does not provide sufficient discussion of how to achieve trustability.  2. It is not clear to me that the proposed method is practical.  3. There is a lack of discussion of the use of Dirichlets in the paper.  4. The method is not simulated API based.  5. The proposed method does not seem to work well in practice.  The authors do not provide a comprehensive analysis of their method. ","This paper proposes a method to audit machine learning models in a black-box setting, where there is no access to external components (e.g. third party components, models, proprietary data, etc.). The authors propose to use application programming interfaces (APIs) to train pre-trained encapsulated models that are robust to uncontrolled potential risks. They propose a new measure of uncertainty, aleatoric uncertainty, based on a wrapping deep learning model that is trained to classify a classification black- box. The Dirichlet layer of the fusion layer is used to learn a probabilistic neural network, which is then used to train a classifier on the input data. The authors show that the Dirichlett layer is able to capture the distribution over the multinomial output parameters of the classifier, and thus can be used as an uncertainty measure for a rejection system. They also show that their technique and methodology can be applied to a variety of applications, including NLP, computer vision, and NLP.    The paper is well-written and well-motivated. The idea of using external components to auditability is interesting, and the paper is clearly written. However, there are a number of issues with the paper:   1. The paper does not provide sufficient discussion of how to achieve trustability.  2. It is not clear to me that the proposed method is practical.  3. There is a lack of discussion of the use of Dirichlets in the paper.  4. The method is not simulated API based.  5. The proposed method does not seem to work well in practice.  The authors do not provide a comprehensive analysis of their method. "
12762,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"Stochastic methods USED-FOR deep neural networks. RMSprop CONJUNCTION Adam. Adam CONJUNCTION RMSprop. coordinate - wise adaptive stepsize FEATURE-OF Stochastic methods. Adam HYPONYM-OF coordinate - wise adaptive stepsize. RMSprop HYPONYM-OF coordinate - wise adaptive stepsize. Adam HYPONYM-OF Stochastic methods. RMSprop HYPONYM-OF Stochastic methods. they COMPARE stochastic gradient descent. stochastic gradient descent COMPARE they. blockwise adaptivity COMPARE adaptivity. adaptivity COMPARE blockwise adaptivity. adaptivity CONJUNCTION generalization. generalization CONJUNCTION adaptivity. blockwise adaptive gradient descent USED-FOR optimizing nonconvex objective. convergence rate FEATURE-OF optimizing nonconvex objective. blockwise adaptive gradient descent USED-FOR online convex learning. regret CONJUNCTION convergence rate. convergence rate CONJUNCTION regret. convergence rate EVALUATE-FOR blockwise adaptive gradient descent. regret EVALUATE-FOR blockwise adaptive gradient descent. blockwise adaptivity COMPARE coordinate - wise adaptivity. coordinate - wise adaptivity COMPARE blockwise adaptivity. generalization error EVALUATE-FOR coordinate - wise adaptivity. generalization error EVALUATE-FOR blockwise adaptivity. Nesterov ’s accelerated gradient CONJUNCTION Adam. Adam CONJUNCTION Nesterov ’s accelerated gradient. blockwise adaptive gradient descent COMPARE Adam. Adam COMPARE blockwise adaptive gradient descent. blockwise adaptive gradient descent COMPARE Nesterov ’s accelerated gradient. Nesterov ’s accelerated gradient COMPARE blockwise adaptive gradient descent. Method is Adagrad. OtherScientificTerm are network parameters, blockwise adaptive stepsize, and nonconvex objective. Metric is uniform stability. ","This paper studies the adaptive adaptive stepsize of Stochastic methods for training deep neural networks, such as RMSprop, Adam, and Adagrad. The authors show that blockwise adaptivity (i.e., adaptive stepsizes of the network parameters) outperforms adaptivity and generalization in terms of both regret and convergence rate for optimizing nonconvex objective in online convex learning, and that they are more stable than stochastic gradient descent. In particular, blockwise adaptive gradient descent outperforms Nesterov’s accelerated gradient and Adam when the number of parameters increases linearly with the size of the training set.    The authors also provide a theoretical analysis of the uniform stability of blockwiseadaptive gradient descent, showing that the regret and the convergence rate are both upper bounded by a constant factor. The paper also provides a theoretical explanation for why blockwise Adaptive Gradient Descent (Blockwise Adaptation Descent) works better than adaptivity.  Finally, the authors provide some numerical experiments to show the effectiveness of the blockwise adaptation and the generalization error. ","This paper studies the adaptive adaptive stepsize of Stochastic methods for training deep neural networks, such as RMSprop, Adam, and Adagrad. The authors show that blockwise adaptivity (i.e., adaptive stepsizes of the network parameters) outperforms adaptivity and generalization in terms of both regret and convergence rate for optimizing nonconvex objective in online convex learning, and that they are more stable than stochastic gradient descent. In particular, blockwise adaptive gradient descent outperforms Nesterov’s accelerated gradient and Adam when the number of parameters increases linearly with the size of the training set.    The authors also provide a theoretical analysis of the uniform stability of blockwiseadaptive gradient descent, showing that the regret and the convergence rate are both upper bounded by a constant factor. The paper also provides a theoretical explanation for why blockwise Adaptive Gradient Descent (Blockwise Adaptation Descent) works better than adaptivity.  Finally, the authors provide some numerical experiments to show the effectiveness of the blockwise adaptation and the generalization error. "
12771,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"large - scale supervised datasets USED-FOR deep features. datasets CONJUNCTION spatiotemporal models. spatiotemporal models CONJUNCTION datasets. scene and object structure FEATURE-OF implicit biases. implicit biases FEATURE-OF video datasets. observable and controllable object and scene bias FEATURE-OF video dataset. spatiotemporal understanding USED-FOR video dataset. 3D objects USED-FOR dataset. diagnostic tools USED-FOR spatiotemporal video architectures. CATER USED-FOR diagnostic tools. CATER HYPONYM-OF dataset. CATER USED-FOR deep video architectures. Task are Computer vision, static image analysis, and video understanding. Method are frame - by - frame classification methods, and long - term reasoning. OtherScientificTerm is temporal structure. ","This paper proposes a new dataset CATER, a large-scale supervised dataset for learning deep features in video. Computer vision has been shown to have implicit biases in terms of scene and object structure, but this paper investigates the relationship between these implicit biases and video datasets, datasets and spatiotemporal models. The paper proposes to use observable and controllable object and scene bias in a video dataset as an indicator of the implicit biases. The dataset is constructed from 3D objects, and the authors propose to use a dataset based on CATER as a diagnostic tool for understanding the implicit bias in deep video architectures.    The paper shows that existing frame-by-frame classification methods do not capture the temporal structure of a video, which is a common problem in static image analysis. The authors also show that a video understanding can be improved by learning spatiotemic understanding of the video dataset, which can be used as a tool for long-term reasoning.  CATER is a dataset that is designed to provide diagnostic tools to understand the bias in spatiotmporal video architectures, and it is a good idea to use CATER for this purpose. ","This paper proposes a new dataset CATER, a large-scale supervised dataset for learning deep features in video. Computer vision has been shown to have implicit biases in terms of scene and object structure, but this paper investigates the relationship between these implicit biases and video datasets, datasets and spatiotemporal models. The paper proposes to use observable and controllable object and scene bias in a video dataset as an indicator of the implicit biases. The dataset is constructed from 3D objects, and the authors propose to use a dataset based on CATER as a diagnostic tool for understanding the implicit bias in deep video architectures.    The paper shows that existing frame-by-frame classification methods do not capture the temporal structure of a video, which is a common problem in static image analysis. The authors also show that a video understanding can be improved by learning spatiotemic understanding of the video dataset, which can be used as a tool for long-term reasoning.  CATER is a dataset that is designed to provide diagnostic tools to understand the bias in spatiotmporal video architectures, and it is a good idea to use CATER for this purpose. "
12780,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"distribution USED-FOR synthetic data. Generative Adversarial Networks ( GANs ) HYPONYM-OF models. distribution USED-FOR models. image data USED-FOR visual applications. model compatibility problem HYPONYM-OF task. generating near - boundary data USED-FOR classifiers. generating ‘ easier ’ synthetic data USED-FOR GANs. pre - trained classifiers USED-FOR boundary information. Boundary - Calibration GANs ( BCGANs ) USED-FOR GAN. boundary information USED-FOR Boundary - Calibration GANs ( BCGANs ). pre - trained classifiers USED-FOR Boundary - Calibration GANs ( BCGANs ). GAN variants USED-FOR model compatibility. BC - loss CONJUNCTION GAN variants. GAN variants CONJUNCTION BC - loss. BC - loss USED-FOR model compatibility. BCGANs COMPARE GANs. GANs COMPARE BCGANs. model compatibility EVALUATE-FOR GANs. BCGANs USED-FOR realistic images. BCGANs COMPARE GANs. GANs COMPARE BCGANs. GANs USED-FOR realistic images. model compatibility EVALUATE-FOR BCGANs. Material is near - boundary data. Method is generator of GAN. OtherScientificTerm are posterior distributions, and boundaries of the pre - trained classifiers. ","This paper studies the model compatibility problem of GANs, which is the problem of generating near-boundary data for training classifiers. The authors propose two models, called Generative Adversarial Networks (GANs) and Boundary-CalibrationGANs (BCGANs), which are models that learn a distribution for generating synthetic data from image data for visual applications. The main contribution of the paper is that the authors show that the problem can be solved by simply generating ‘easier’ synthetic data, which can be used to train a classifier on the generated data.    The authors also show that a GAN trained on the boundary information from pre-trained classifiers can be trained on a set of samples from the distribution of the posterior distributions of the samples generated by the generator of the GAN, and that the boundaries of the pre-trained classifiers are not too far from the true distribution.  The paper also shows that the boundary-calibration GAN models can be learned from a large number of samples generated from a small number of classes.  In addition, the authors propose a new GAN called Boundary Calibration-based GAN that uses boundary information generated from the boundary of the classifier to train the generator. The BC-loss and other GAN variants are used to improve model compatibility. BCGANs are shown to be able to generate realistic images that are more similar to real images than BCGAN, and to achieve better model compatibility compared to other models that do not use boundary information. ","This paper studies the model compatibility problem of GANs, which is the problem of generating near-boundary data for training classifiers. The authors propose two models, called Generative Adversarial Networks (GANs) and Boundary-CalibrationGANs (BCGANs), which are models that learn a distribution for generating synthetic data from image data for visual applications. The main contribution of the paper is that the authors show that the problem can be solved by simply generating ‘easier’ synthetic data, which can be used to train a classifier on the generated data.    The authors also show that a GAN trained on the boundary information from pre-trained classifiers can be trained on a set of samples from the distribution of the posterior distributions of the samples generated by the generator of the GAN, and that the boundaries of the pre-trained classifiers are not too far from the true distribution.  The paper also shows that the boundary-calibration GAN models can be learned from a large number of samples generated from a small number of classes.  In addition, the authors propose a new GAN called Boundary Calibration-based GAN that uses boundary information generated from the boundary of the classifier to train the generator. The BC-loss and other GAN variants are used to improve model compatibility. BCGANs are shown to be able to generate realistic images that are more similar to real images than BCGAN, and to achieve better model compatibility compared to other models that do not use boundary information. "
12789,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,approach USED-FOR robust neural networks. Adversarial training USED-FOR robust neural networks. Adversarial training USED-FOR approach. minimax robust optimization problem USED-FOR adversarial training. outer minimization USED-FOR robust classifier. inner maximization USED-FOR adversarial samples. outer minimization CONJUNCTION inner maximization. inner maximization CONJUNCTION outer minimization. hand - designed algorithms USED-FOR inner problem. convolutional neural network USED-FOR optimizer. robust classifier USED-FOR adversarial attack. optimizer USED-FOR adversarial attack. L2L COMPARE adversarial training methods. adversarial training methods COMPARE L2L. CIFAR-10 and CIFAR-100 datasets EVALUATE-FOR L2L. classification accuracy CONJUNCTION computational efficiency. computational efficiency CONJUNCTION classification accuracy. classification accuracy EVALUATE-FOR adversarial training methods. computational efficiency EVALUATE-FOR adversarial training methods. computational efficiency EVALUATE-FOR L2L. classification accuracy EVALUATE-FOR L2L. L2L framework USED-FOR generative adversarial imitation learning. Task is minimax problem. OtherScientificTerm is convex - concave structure. Method is adversarial training method. ,"This paper proposes a novel approach to train robust neural networks using Adversarial training. The approach is based on the minimax robust optimization problem in adversarial training, which is a minimax problem with a convex-concave structure. The outer minimization of the robust classifier and the inner maximization of adversarial samples are solved by hand-designed algorithms. The authors propose to solve the inner problem using hand-designed algorithms, and then use a convolutional neural network as the optimizer to train an adversarial attack against the robust classification classifier. L2L is evaluated on the CIFAR-10 and CifAR-100 datasets, and compared with several state-of-the-art adversarial learning methods, and shows that L1L improves classification accuracy and computational efficiency. The paper also proposes a generative adversarial imitation learning using the L2l framework.   ","This paper proposes a novel approach to train robust neural networks using Adversarial training. The approach is based on the minimax robust optimization problem in adversarial training, which is a minimax problem with a convex-concave structure. The outer minimization of the robust classifier and the inner maximization of adversarial samples are solved by hand-designed algorithms. The authors propose to solve the inner problem using hand-designed algorithms, and then use a convolutional neural network as the optimizer to train an adversarial attack against the robust classification classifier. L2L is evaluated on the CIFAR-10 and CifAR-100 datasets, and compared with several state-of-the-art adversarial learning methods, and shows that L1L improves classification accuracy and computational efficiency. The paper also proposes a generative adversarial imitation learning using the L2l framework.   "
12798,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"approaches USED-FOR Inverse Reinforcement Learning ( IRL ). reward function USED-FOR expert agent ’s policy. reward function USED-FOR control task. reward CONJUNCTION hard constraints. hard constraints CONJUNCTION reward. reward USED-FOR behavior. Markov Decision Processes ( MDPs ) USED-FOR IRL. Maximum Entropy IRL framework USED-FOR approach. algorithm USED-FOR Maximum Likelihood Constraint. algorithm USED-FOR observed behavior. Maximum Likelihood Constraint USED-FOR observed behavior. OtherScientificTerm are cumulative rewards, nominal reward function, and MDP. Generic is method. Material is simulated behavior. ","This paper proposes a new approach to Inverse Reinforcement learning based on Markov Decision Processes (MDPs). The main idea is to use the Maximum Entropy IRL framework, where the reward function for an expert agent’s policy is the sum of the cumulative rewards of the expert and a nominal reward function. The reward function is then used for the control task, and the reward and hard constraints are used to constrain the behavior of the agent. The proposed method is evaluated on simulated behavior, and is shown to outperform existing approaches for Inverse reinforcement Learning (IRL).","This paper proposes a new approach to Inverse Reinforcement learning based on Markov Decision Processes (MDPs). The main idea is to use the Maximum Entropy IRL framework, where the reward function for an expert agent’s policy is the sum of the cumulative rewards of the expert and a nominal reward function. The reward function is then used for the control task, and the reward and hard constraints are used to constrain the behavior of the agent. The proposed method is evaluated on simulated behavior, and is shown to outperform existing approaches for Inverse reinforcement Learning (IRL)."
12807,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,deep recurrent neural network architecture USED-FOR visual cortical circuits. architecture USED-FOR contour detection tasks. architecture COMPARE feedforward networks. feedforward networks COMPARE architecture. sample efficiency EVALUATE-FOR feedforward networks. γ - Net HYPONYM-OF architecture. sample efficiency EVALUATE-FOR architecture. orientation - tilt illusion HYPONYM-OF perceptual illusion. lowlevel edges COMPARE high - level object boundary contours. high - level object boundary contours COMPARE lowlevel edges. it USED-FOR lowlevel edges. γ - Net contour detection accuracy EVALUATE-FOR illusion. neural circuits USED-FOR biological visual systems. circuits PART-OF artificial neural networks. circuits USED-FOR computer vision. neural circuits USED-FOR contour detection. artificial neural networks USED-FOR computer vision. biological visual systems USED-FOR contour detection. neural circuits USED-FOR orientation - tilt illusion. ,"This paper proposes a deep recurrent neural network architecture that is inspired by visual cortical circuits. The architecture, called γ-Net, improves sample efficiency over feedforward networks on contour detection tasks. In particular, the authors show that the lowlevel edges of the network are more informative than the high-level object boundary contours, and that it is able to detect the orientation-tilt illusion, which is a perceptual illusion that is similar to the one observed in biological visual systems. The authors also show that this optical illusion can be reduced to a more realistic version of the same optical illusion in the case of neural circuits.   The paper also shows that the proposed architecture (i.e., the architecture of the proposed    - a variant of the architecture called  - Net) improves the sample efficiency of feedforward neural networks on a variety of contour recognition tasks. The main contribution of the paper is that the authors demonstrate that the optical illusion is a result of the fact that neural circuits in artificial neural networks (e.g. circuits in computer vision) can be seen as circuits in biological neural networks, and the authors also demonstrate that this perceptual illusion can also be observed in neural circuits for contour detectors.  The authors further show that, in addition to improving the  -N-N scale of the   of the layer, they also improve the -N -N of the layers in the network, and show that it can be used to learn to detect lowlevel edge information.  In addition, they show that their  -Net contour detections are more robust to the optical illusions, and are able to recover the original orientation-tilting optical illusion.  Finally, they demonstrate that their _i.i.d. _i_-Net_ achieves the state-of-the-art _i-N-Net _scaled_ performance on the _Orientation-Tilt_ and _O_Net_ datasets for the _orientation_-tilted_ optical illusion, outperforming the previous state of the art on all three.","This paper proposes a deep recurrent neural network architecture that is inspired by visual cortical circuits. The architecture, called γ-Net, improves sample efficiency over feedforward networks on contour detection tasks. In particular, the authors show that the lowlevel edges of the network are more informative than the high-level object boundary contours, and that it is able to detect the orientation-tilt illusion, which is a perceptual illusion that is similar to the one observed in biological visual systems. The authors also show that this optical illusion can be reduced to a more realistic version of the same optical illusion in the case of neural circuits.   The paper also shows that the proposed architecture (i.e., the architecture of the proposed    - a variant of the architecture called  - Net) improves the sample efficiency of feedforward neural networks on a variety of contour recognition tasks. The main contribution of the paper is that the authors demonstrate that the optical illusion is a result of the fact that neural circuits in artificial neural networks (e.g. circuits in computer vision) can be seen as circuits in biological neural networks, and the authors also demonstrate that this perceptual illusion can also be observed in neural circuits for contour detectors.  The authors further show that, in addition to improving the  -N-N scale of the   of the layer, they also improve the -N -N of the layers in the network, and show that it can be used to learn to detect lowlevel edge information.  In addition, they show that their  -Net contour detections are more robust to the optical illusions, and are able to recover the original orientation-tilting optical illusion.  Finally, they demonstrate that their _i.i.d. _i_-Net_ achieves the state-of-the-art _i-N-Net _scaled_ performance on the _Orientation-Tilt_ and _O_Net_ datasets for the _orientation_-tilted_ optical illusion, outperforming the previous state of the art on all three."
12816,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,object detection methods USED-FOR detecting and classifying objects. deep convolutional neural networks ( CNNs ) USED-FOR object detection methods. semantics context constraints FEATURE-OF CNN - based object detector. conditional random field ( CRF ) model USED-FOR CNN. mean - field inference method USED-FOR CRF. context - aware module USED-FOR mean - field inference method. context - aware module PART-OF conCNN. stack of common CNN operations USED-FOR mean - field inference method. stack of common CNN operations USED-FOR conCNN. It PART-OF region - based object detection paradigm. average precision ( AP ) EVALUATE-FOR object detection. COCO datasets EVALUATE-FOR conCNN. conCNN USED-FOR object detection. average precision ( AP ) EVALUATE-FOR conCNN. Generic is methods. OtherScientificTerm is semantic context. ,This paper proposes a new class of object detection methods based on deep convolutional neural networks (CNNs) for detecting and classifying objects. Previous methods rely on a conditional random field (CRF) model for training a CNN-based object detector with semantics context constraints. The authors propose a new mean-field inference method for the CRF based on a context-aware module in the proposed conCNN. The conCNN uses a stack of common CNN operations to learn a CRF. It is an extension of the region-based area detection paradigm. ConCNN achieves state-of-the-art performance on the COCO datasets and achieves average precision (AP) for object detection. ,This paper proposes a new class of object detection methods based on deep convolutional neural networks (CNNs) for detecting and classifying objects. Previous methods rely on a conditional random field (CRF) model for training a CNN-based object detector with semantics context constraints. The authors propose a new mean-field inference method for the CRF based on a context-aware module in the proposed conCNN. The conCNN uses a stack of common CNN operations to learn a CRF. It is an extension of the region-based area detection paradigm. ConCNN achieves state-of-the-art performance on the COCO datasets and achieves average precision (AP) for object detection. 
12825,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"Batch Normalization ( BatchNorm ) USED-FOR deep neural networks. adversarial perturbations FEATURE-OF it. adversarial vulnerability FEATURE-OF BatchNorm. training CONJUNCTION inference. inference CONJUNCTION training. normalization statistics USED-FOR training. normalization statistics USED-FOR inference. normalization statistics USED-FOR adversarial vulnerability. adversarial vulnerability FEATURE-OF BatchNorm layer. mini - batch statistics HYPONYM-OF normalization statistics. Method are neural network architectures, and Robust Normalization ( RobustNorm ). OtherScientificTerm is adversarial perturbation. ","This paper studies the adversarial vulnerability of Batch Normalization (BatchNorm) in deep neural networks. The authors show that it is vulnerable to adversarial perturbations on a variety of neural network architectures. They also show that BatchNorm has an adversarial vulnerable layer. They then propose a new normalization statistics (called Robust Normalisation (RobustNorm)) that can be used to improve the robustness of the Batchnorm layer.    The authors also show empirically that the normalisation statistics (i.e., mini-batch statistics) that are used during training and inference can help to improve B batchNorm's robustness.  Finally, the authors provide a theoretical analysis that shows that RobustNorm is more robust to the perturbation that is added to the input. ","This paper studies the adversarial vulnerability of Batch Normalization (BatchNorm) in deep neural networks. The authors show that it is vulnerable to adversarial perturbations on a variety of neural network architectures. They also show that BatchNorm has an adversarial vulnerable layer. They then propose a new normalization statistics (called Robust Normalisation (RobustNorm)) that can be used to improve the robustness of the Batchnorm layer.    The authors also show empirically that the normalisation statistics (i.e., mini-batch statistics) that are used during training and inference can help to improve B batchNorm's robustness.  Finally, the authors provide a theoretical analysis that shows that RobustNorm is more robust to the perturbation that is added to the input. "
12834,SP:f16d3e61eda162dfee39396abbd594425f47f625,"Over - parameterized deep neural networks USED-FOR labeling of data. first - order methods USED-FOR Over - parameterized deep neural networks. over - fitting ability USED-FOR generalization. clean test data EVALUATE-FOR regularization methods. early - stopping HYPONYM-OF regularization methods. trainable auxiliary variable USED-FOR network output. regularization HYPONYM-OF regularization methods. generalization guarantee FEATURE-OF clean data distribution. gradient descent training USED-FOR generalization guarantee. methods USED-FOR gradient descent training. wide neural network CONJUNCTION neural tangent kernel ( NTK ). neural tangent kernel ( NTK ) CONJUNCTION wide neural network. noisily labeled datasets EVALUATE-FOR methods. OtherScientificTerm are network parameters, noisy labels, and label noise. Metric is generalization bound. ","This paper studies the over-parameterized deep neural networks trained with first-order methods for the labeling of data. Over-parametrized neural networks have been shown to suffer from over-fitting in the presence of noisy labels. This paper shows that the generalization ability of OOD neural networks can be explained by the fact that the network parameters tend to overfit to noisy labels, which is a result of a trainable auxiliary variable that can be used to regularize the network output. The paper also shows that regularization methods (e.g., early-stopping, regularization on clean test data, etc.) can be seen as a way to mitigate this over-filing ability.    The paper provides a generalization bound for OOD networks. The generalization guarantee for a clean data distribution is obtained by considering the training of a wide neural network and a neural tangent kernel (NTK). The paper further shows that gradient descent training with these methods leads to an improved generalization performance on noisily labeled datasets when the label noise is low. ","This paper studies the over-parameterized deep neural networks trained with first-order methods for the labeling of data. Over-parametrized neural networks have been shown to suffer from over-fitting in the presence of noisy labels. This paper shows that the generalization ability of OOD neural networks can be explained by the fact that the network parameters tend to overfit to noisy labels, which is a result of a trainable auxiliary variable that can be used to regularize the network output. The paper also shows that regularization methods (e.g., early-stopping, regularization on clean test data, etc.) can be seen as a way to mitigate this over-filing ability.    The paper provides a generalization bound for OOD networks. The generalization guarantee for a clean data distribution is obtained by considering the training of a wide neural network and a neural tangent kernel (NTK). The paper further shows that gradient descent training with these methods leads to an improved generalization performance on noisily labeled datasets when the label noise is low. "
12843,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"convolutional neural networks ( CNNs ) USED-FOR regression. Convolutional Neural Tangent Kernel ( CNTK ) USED-FOR regression. algorithm USED-FOR CNTK. classification accuracy EVALUATE-FOR CNTK. CNTK COMPARE CNN architecture. CNN architecture COMPARE CNTK. classification accuracy EVALUATE-FOR CNN architecture. CIFAR-10 EVALUATE-FOR CNTK. Local Average Pooling ( LAP ) HYPONYM-OF operation. pixel shifts USED-FOR data augmentation. operation USED-FOR kernel. quadratic training cost FEATURE-OF kernel regression. CNN - GP CONJUNCTION CNTK. CNTK CONJUNCTION CNN - GP. full translation data augmentation USED-FOR CNTK. single convolutional layer USED-FOR pre - processing technique. random image patches PART-OF single convolutional layer. CIFAR-10 EVALUATE-FOR kernel. classifier COMPARE trained neural network. trained neural network COMPARE classifier. AlexNet COMPARE classifier. classifier COMPARE AlexNet. CNN - GP HYPONYM-OF kernel. horizontal flip data augmentation USED-FOR kernel. horizontal flip data augmentation USED-FOR CNN - GP. accuracy EVALUATE-FOR kernel. OtherScientificTerm are ` 2 loss, convolutional layers, and fixed kernel. Generic are layer, and kernels. Method are naive data augmentation, Global Average Pooling ( GAP ), and Fashion - MNIST. ","This paper proposes a new convolutional neural networks (CNNs) for regression, named Convolutional Neural Tangent Kernel (CNTK). CNTK is an algorithm for learning the `2 loss, which is the difference between the classification accuracy of a standard CNN architecture and a CNN architecture with a fixed kernel. The authors propose an algorithm to learn the CNTk, and show that the proposed algorithm can achieve better classification accuracy than AlexNet and AlexNet on CIFAR-10 and Fashion-MNIST.    The main contribution of the paper is that the authors propose a new pre-processing technique, called Global Average Pooling (GAP), which is a preprocessing technique that applies a pre-processed image patch to a single convolutionan layer, where the patch is augmented with random image patches, and the layer is trained with a naive data augmentation.  The authors show that GAP can be used to learn a kernel with quadratic training cost for kernel regression, and that this kernel can be learned with any operation (e.g., Local AveragePooling (LAP)). The authors also show that this operation can be applied to any kernel, even if the input image has pixel shifts.  In addition, the authors demonstrate that the classifier trained with GAP outperforms a trained neural network trained with AlexNet in terms of accuracy.  Finally, they show that their kernel, called CNN-GP, and its variants CNN-CNTk (CNN-GP and CNN-CTN) outperform AlexNet, CNN-NTK, and AlexN on Fashion-N on the same dataset.  They also demonstrate that CNN-N outperforms AlexNet when the input is a translation of the original input image, but not the original image.  On the other hand, their kernel performs much worse when the inputs are not translation-based, and perform much better when they are translation based. They show that a simple horizontal flip data augmentation (i.e., adding random patches to the input) can improve the accuracy of the kernel.  This paper also shows that a single image patch can be preprocessed with the proposed GAP, and it is shown that the resulting kernel is asymptotically equivalent to the original kernel. Finally, it shows that the performance of the proposed kernel is comparable to that of AlexNet (which does not use GAP) on the original inputs, and is even better when the images are translated.","This paper proposes a new convolutional neural networks (CNNs) for regression, named Convolutional Neural Tangent Kernel (CNTK). CNTK is an algorithm for learning the `2 loss, which is the difference between the classification accuracy of a standard CNN architecture and a CNN architecture with a fixed kernel. The authors propose an algorithm to learn the CNTk, and show that the proposed algorithm can achieve better classification accuracy than AlexNet and AlexNet on CIFAR-10 and Fashion-MNIST.    The main contribution of the paper is that the authors propose a new pre-processing technique, called Global Average Pooling (GAP), which is a preprocessing technique that applies a pre-processed image patch to a single convolutionan layer, where the patch is augmented with random image patches, and the layer is trained with a naive data augmentation.  The authors show that GAP can be used to learn a kernel with quadratic training cost for kernel regression, and that this kernel can be learned with any operation (e.g., Local AveragePooling (LAP)). The authors also show that this operation can be applied to any kernel, even if the input image has pixel shifts.  In addition, the authors demonstrate that the classifier trained with GAP outperforms a trained neural network trained with AlexNet in terms of accuracy.  Finally, they show that their kernel, called CNN-GP, and its variants CNN-CNTk (CNN-GP and CNN-CTN) outperform AlexNet, CNN-NTK, and AlexN on Fashion-N on the same dataset.  They also demonstrate that CNN-N outperforms AlexNet when the input is a translation of the original input image, but not the original image.  On the other hand, their kernel performs much worse when the inputs are not translation-based, and perform much better when they are translation based. They show that a simple horizontal flip data augmentation (i.e., adding random patches to the input) can improve the accuracy of the kernel.  This paper also shows that a single image patch can be preprocessed with the proposed GAP, and it is shown that the resulting kernel is asymptotically equivalent to the original kernel. Finally, it shows that the performance of the proposed kernel is comparable to that of AlexNet (which does not use GAP) on the original inputs, and is even better when the images are translated."
12852,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"approach USED-FOR model - free Q - learning. MCTS USED-FOR state - action values. real experience USED-FOR prior. Q - estimates USED-FOR prior. Q - estimates CONJUNCTION real experience. real experience CONJUNCTION Q - estimates. model - free learning CONJUNCTION model - based search. model - based search CONJUNCTION model - free learning. MCTS USED-FOR value computation. physical reasoning tasks CONJUNCTION Atari. Atari CONJUNCTION physical reasoning tasks. agents USED-FOR physical reasoning tasks. agents USED-FOR Atari. it PART-OF agents. model USED-FOR Q - learning agent. Q - learning agent USED-FOR SAVE. SAVE COMPARE model - based search approaches. model - based search approaches COMPARE SAVE. training steps USED-FOR SAVE. real experience USED-FOR SAVE. model - free learning CONJUNCTION planning. planning CONJUNCTION model - free learning. Method is Search with Amortized Value Estimates. OtherScientificTerm are search budgets, and search. ","This paper proposes a new approach to model-free Q-learning, Search with Amortized Value Estimates (SAVE). The key idea is to use MCTS to compute state-action values, and then use these Q-estimates and real experience to train a prior based on the current state of the game. The value computation is done using MCTT, and it is then incorporated into the training of agents on physical reasoning tasks and Atari. The authors show that SAVE outperforms other model-based search approaches in terms of search budgets and training time. They also show that the Q-learning agent can be trained with a single model, and that it can be combined with other agents in a way that it is able to learn from the experience of other agents.   The authors also provide a theoretical analysis that shows that the SAVE can be learned with fewer training steps and with more real experience, which is an important property of the search.  The paper is well-written and well-motivated, and the experiments are well-designed. It is a good combination of model free learning and planning, which can be a useful combination for both.","This paper proposes a new approach to model-free Q-learning, Search with Amortized Value Estimates (SAVE). The key idea is to use MCTS to compute state-action values, and then use these Q-estimates and real experience to train a prior based on the current state of the game. The value computation is done using MCTT, and it is then incorporated into the training of agents on physical reasoning tasks and Atari. The authors show that SAVE outperforms other model-based search approaches in terms of search budgets and training time. They also show that the Q-learning agent can be trained with a single model, and that it can be combined with other agents in a way that it is able to learn from the experience of other agents.   The authors also provide a theoretical analysis that shows that the SAVE can be learned with fewer training steps and with more real experience, which is an important property of the search.  The paper is well-written and well-motivated, and the experiments are well-designed. It is a good combination of model free learning and planning, which can be a useful combination for both."
12861,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"deep learning USED-FOR low - level vision tasks. it USED-FOR 3D visualization of the 2D input scenery. network architecture USED-FOR stereoscopic view synthesis. network architecture USED-FOR Deep 3D Pan. globally and locally adaptive dilations USED-FOR t - shaped ” adaptive kernels. local 3D geometries USED-FOR synthesis of naturally looking 3D panned views. globally and locally adaptive dilation FEATURE-OF t - shaped adaptive kernel. t - shaped adaptive kernel USED-FOR monster - net. t - shaped adaptive kernel USED-FOR network architecture. monster - net HYPONYM-OF network architecture. 2 - D input image USED-FOR synthesis of naturally looking 3D panned views. VICLAB STEREO indoors dataset EVALUATE-FOR method. PSNR CONJUNCTION SSIM. SSIM CONJUNCTION PSNR. RMSE CONJUNCTION PSNR. PSNR CONJUNCTION RMSE. PSNR EVALUATE-FOR monster - net. SSIM EVALUATE-FOR monster - net. RMSE EVALUATE-FOR monster - net. monster - net USED-FOR image structures. synthesized images FEATURE-OF image structures. coherent geometry FEATURE-OF synthesized images. coherent geometry FEATURE-OF image structures. SOTA USED-FOR unsupervised monocular depth estimation task. that USED-FOR unsupervised monocular depth estimation task. disparity information COMPARE that. that COMPARE disparity information. disparity information USED-FOR unsupervised monocular depth estimation task. disparity information COMPARE SOTA. SOTA COMPARE disparity information. t - shaped ” kernel COMPARE SOTA. SOTA COMPARE t - shaped ” kernel. SOTA USED-FOR that. t - shaped ” kernel USED-FOR disparity information. Task is single - image - based view synthesis. OtherScientificTerm are parallel camera views, arbitrary camera positions, X - axis, and global camera shift. Material is KITTI. ","This paper proposes a new method for single-image-based view synthesis. The authors propose a new network architecture called Deep 3D Pan, a network architecture for 3D visualization of the 2D input scenery, and apply it to a variety of low-level vision tasks using deep learning. The network architecture is based on a t-shaped adaptive kernel, which is a combination of globally and locally adaptive dilations for different “t-shaped” adaptive kernels.  The authors show that this network architecture, called “monster-net”, is able to synthesize 3D panned views from a 2-D input image with arbitrary camera positions and local 3D geometries, and that it can be applied to a wide variety of applications, including single-view view synthesis, monocular depth estimation, and parallel camera views. The proposed method is evaluated on the VICLAB STEREO indoors dataset, and is shown to be able to achieve state-of-the-art results on RMSE, PSNR, and SSIM. It is also shown that the proposed monster-net can learn image structures of the synthesized images with a coherent geometry, and the disparity information from the t-shaped “kernels” is better than that from a single “regular” kernel, and better than SOTA for an unsupervised monocular depths estimation task. ","This paper proposes a new method for single-image-based view synthesis. The authors propose a new network architecture called Deep 3D Pan, a network architecture for 3D visualization of the 2D input scenery, and apply it to a variety of low-level vision tasks using deep learning. The network architecture is based on a t-shaped adaptive kernel, which is a combination of globally and locally adaptive dilations for different “t-shaped” adaptive kernels.  The authors show that this network architecture, called “monster-net”, is able to synthesize 3D panned views from a 2-D input image with arbitrary camera positions and local 3D geometries, and that it can be applied to a wide variety of applications, including single-view view synthesis, monocular depth estimation, and parallel camera views. The proposed method is evaluated on the VICLAB STEREO indoors dataset, and is shown to be able to achieve state-of-the-art results on RMSE, PSNR, and SSIM. It is also shown that the proposed monster-net can learn image structures of the synthesized images with a coherent geometry, and the disparity information from the t-shaped “kernels” is better than that from a single “regular” kernel, and better than SOTA for an unsupervised monocular depths estimation task. "
12870,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"representation CONJUNCTION generative model. generative model CONJUNCTION representation. Variational AutoEncoder ( VAE ) USED-FOR generative model. Variational AutoEncoder ( VAE ) USED-FOR representation. tasks PART-OF VAE. capacity FEATURE-OF variational network. information properties FEATURE-OF network. information properties USED-FOR tasks. network HYPONYM-OF information properties. generative model HYPONYM-OF information properties. theoretical objective USED-FOR maximal informative generative model. one HYPONYM-OF generative model. Capacity - Constrained InfoMax ( CCIM ) HYPONYM-OF generative model. one USED-FOR clustered and robust representation. one USED-FOR sharper samples. variational lower bound USED-FOR sharper samples. variational lower bound USED-FOR CCIM. variational lower bound USED-FOR one. one HYPONYM-OF VAE. OtherScientificTerm are informative one, and network capacity. Method is Variational InfoMax AutoEncoder ( VIMAE ). ","This paper proposes a Variational AutoEncoder (VAE) for learning a representation and a generative model based on the Variational InfoMax (VIMAE) framework. The authors consider two tasks in the VAE: (1) learning the capacity of a variational network, and (2) learning an informative one, where the capacity is a function of the information properties of the network (e.g., the number of samples, the number the network takes as input, the network capacity, etc.). The authors propose a theoretical objective for learning the maximal informative generative generative models, which they call the capacity-constrained InfoMax. They propose two variants of the variational variational autoencoder, one is a VAE-based one that learns a clustered and robust representation, and the other one is an improved version of the original one, called the Capacity-Constrained Information Maximization (CCIM). The authors show that the one based on a variant of variational lower bound is able to learn sharper samples, and that it is more robust than the original VAE. They also propose a variant called Variational InformationMax Auto Encoder (VIAE), which is based on VIMAE.   ","This paper proposes a Variational AutoEncoder (VAE) for learning a representation and a generative model based on the Variational InfoMax (VIMAE) framework. The authors consider two tasks in the VAE: (1) learning the capacity of a variational network, and (2) learning an informative one, where the capacity is a function of the information properties of the network (e.g., the number of samples, the number the network takes as input, the network capacity, etc.). The authors propose a theoretical objective for learning the maximal informative generative generative models, which they call the capacity-constrained InfoMax. They propose two variants of the variational variational autoencoder, one is a VAE-based one that learns a clustered and robust representation, and the other one is an improved version of the original one, called the Capacity-Constrained Information Maximization (CCIM). The authors show that the one based on a variant of variational lower bound is able to learn sharper samples, and that it is more robust than the original VAE. They also propose a variant called Variational InformationMax Auto Encoder (VIAE), which is based on VIMAE.   "
12879,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"approach COMPARE Skip - gram. Skip - gram COMPARE approach. disconnected networks USED-FOR latent feature identification. latent feature identification HYPONYM-OF applications. embeddings USED-FOR matrices of node - feature pointwise mutual information. algorithms COMPARE models. models COMPARE algorithms. social, web and citation network datasets EVALUATE-FOR models. social, web and citation network datasets EVALUATE-FOR algorithms. Method are network embedding algorithms, random walks, and multiscale approach ( MUSAE ). OtherScientificTerm are local distribution, and attribute - neighborhood relationships. ","This paper proposes a multiscale approach for learning embeddings for disconnected networks. The idea is to learn a set of matrices of node-feature pointwise mutual information (MSE) between two nodes in the local distribution, and then to learn the embedding of each node based on these matrices. The paper shows that the proposed approach outperforms Skip-gram, which is based on random walks, in two applications: latent feature identification on disconnected networks, and attribute-neighborhood relationships.   The paper also proposes a new algorithm for learning the MSE matrices, called the multi-scale approach (MUSAE), which is an extension of previous work on network embedding algorithms. The main difference is that the authors propose to learn these embedding matrices in an unsupervised way, by learning the matrices based on a set (or a subset) of random walks. The authors also show that the learned matrices can be used to learn matrices that are more robust to changes in local distribution.  Experiments are conducted on social, web and citation network datasets, and the proposed algorithms outperform existing models. ","This paper proposes a multiscale approach for learning embeddings for disconnected networks. The idea is to learn a set of matrices of node-feature pointwise mutual information (MSE) between two nodes in the local distribution, and then to learn the embedding of each node based on these matrices. The paper shows that the proposed approach outperforms Skip-gram, which is based on random walks, in two applications: latent feature identification on disconnected networks, and attribute-neighborhood relationships.   The paper also proposes a new algorithm for learning the MSE matrices, called the multi-scale approach (MUSAE), which is an extension of previous work on network embedding algorithms. The main difference is that the authors propose to learn these embedding matrices in an unsupervised way, by learning the matrices based on a set (or a subset) of random walks. The authors also show that the learned matrices can be used to learn matrices that are more robust to changes in local distribution.  Experiments are conducted on social, web and citation network datasets, and the proposed algorithms outperform existing models. "
12888,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"multiple phase transitions FEATURE-OF IB objective. definition USED-FOR IB phase transitions. formula USED-FOR IB phase transitions. secondorder calculus of variations USED-FOR formula. Fisher information matrix USED-FOR parameterized models. canonical - correlation analysis ( CCA ) USED-FOR linear settings. maximum ( nonlinear ) correlation USED-FOR IB phase transition. theory USED-FOR algorithm. theory USED-FOR phase transitions. algorithm USED-FOR phase transitions. algorithm USED-FOR prominent phase transitions. theory CONJUNCTION algorithm. algorithm CONJUNCTION theory. theory USED-FOR prominent phase transitions. algorithm USED-FOR class difficulty. class difficulty FEATURE-OF MNIST. categorical datasets EVALUATE-FOR theory. categorical datasets USED-FOR phase transitions. CIFAR10 USED-FOR prominent phase transitions. Task is Information Bottleneck ( IB ). Generic are terms, dataset, and transitions. OtherScientificTerm are encoding distribution, β, and IB loss landscape. Metric is prediction accuracy. ","This paper studies the Information Bottleneck (IB) problem. The authors propose a new definition of the IB objective that considers multiple phase transitions, i.e., the number of times an input is sampled from an encoding distribution that has a certain number of phase transitions.  The authors define the IB phase transitions as a function of the Fisher information matrix of the parameterized models, and define two terms: (1) the maximum (nonlinear) correlation between the input and the label of the dataset, and (2) the minimum (non-convex) correlation of the label and the encoding distribution of the input.  This definition is used to define a formula for the IB phases, which is based on the secondorder calculus of variations.  In particular, the authors use canonical-correlation analysis (CCA) in linear settings to derive the formula.  They show that the optimal IB phase transition is the one that maximizes the minimum correlation between a pair of samples from the dataset and the corresponding label.  Finally, they propose an algorithm that uses this theory and algorithm to identify prominent phase transitions in categorical datasets (e.g., CIFAR10) and demonstrate that the algorithm is able to identify phase transitions that lead to class difficulty in MNIST. They also show that this algorithm can be used to improve the prediction accuracy in the presence of a prominent phase transition.   The paper also shows that the IB loss landscape can be decomposed into two parts. The first part is a linear, and the second part is non-linear, and it is shown that the authors show that for a certain class of transitions, the algorithm can find a solution that minimizes the loss. ","This paper studies the Information Bottleneck (IB) problem. The authors propose a new definition of the IB objective that considers multiple phase transitions, i.e., the number of times an input is sampled from an encoding distribution that has a certain number of phase transitions.  The authors define the IB phase transitions as a function of the Fisher information matrix of the parameterized models, and define two terms: (1) the maximum (nonlinear) correlation between the input and the label of the dataset, and (2) the minimum (non-convex) correlation of the label and the encoding distribution of the input.  This definition is used to define a formula for the IB phases, which is based on the secondorder calculus of variations.  In particular, the authors use canonical-correlation analysis (CCA) in linear settings to derive the formula.  They show that the optimal IB phase transition is the one that maximizes the minimum correlation between a pair of samples from the dataset and the corresponding label.  Finally, they propose an algorithm that uses this theory and algorithm to identify prominent phase transitions in categorical datasets (e.g., CIFAR10) and demonstrate that the algorithm is able to identify phase transitions that lead to class difficulty in MNIST. They also show that this algorithm can be used to improve the prediction accuracy in the presence of a prominent phase transition.   The paper also shows that the IB loss landscape can be decomposed into two parts. The first part is a linear, and the second part is non-linear, and it is shown that the authors show that for a certain class of transitions, the algorithm can find a solution that minimizes the loss. "
12897,SP:fecfd5e98540e2d146a726f94802d96472455111,"advantage function estimation methods USED-FOR reinforcement learning. independence property USED-FOR importance sampling advantage estimator. close - to - zero variance FEATURE-OF importance sampling advantage estimator. it COMPARE Monte - Carlo estimator. Monte - Carlo estimator COMPARE it. reward decomposition model USED-FOR Monte - Carlo estimator. method COMPARE advantage estimation methods. advantage estimation methods COMPARE method. sample efficiency EVALUATE-FOR advantage estimation methods. advantage estimation methods USED-FOR complex environments. complex environments EVALUATE-FOR method. sample efficiency EVALUATE-FOR method. OtherScientificTerm are time horizon, Monte - Carlo return signal, and estimation variance. Method is advantage estimation. Generic is estimator. ","This paper studies the problem of advantage function estimation methods in reinforcement learning. The authors consider the independence property of the importance sampling advantage estimator with close-to-zero variance. They show that it is equivalent to a Monte-Carlo estimator under a reward decomposition model, where the time horizon is independent of the reward and the reward function. They also show that the advantage estimation is independent when the reward is a linear function of the return signal.  The authors show that their method outperforms existing advantage estimation methods on a variety of tasks and sample efficiency in complex environments. The main contribution of the paper is that the estimator does not require any additional assumptions on the reward or reward function, and that it does not need to rely on any prior knowledge of the underlying reward.   ","This paper studies the problem of advantage function estimation methods in reinforcement learning. The authors consider the independence property of the importance sampling advantage estimator with close-to-zero variance. They show that it is equivalent to a Monte-Carlo estimator under a reward decomposition model, where the time horizon is independent of the reward and the reward function. They also show that the advantage estimation is independent when the reward is a linear function of the return signal.  The authors show that their method outperforms existing advantage estimation methods on a variety of tasks and sample efficiency in complex environments. The main contribution of the paper is that the estimator does not require any additional assumptions on the reward or reward function, and that it does not need to rely on any prior knowledge of the underlying reward.   "
12906,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"approach USED-FOR infinite horizon off - policy evaluation. value function USED-FOR accuracy. value function USED-FOR bias - reduced augmentation of their method. density ratio CONJUNCTION value function estimation. value function estimation CONJUNCTION density ratio. method COMPARE methods. methods COMPARE method. Task is Infinite horizon off - policy policy evaluation. OtherScientificTerm are stationary density ratio, biases, and bias. Method is density ratio estimation. Generic is them. ","This paper proposes a novel approach to infinite horizon off-policy evaluation, where the goal is to estimate the stationary density ratio of the optimal policy. The authors propose a bias-reduced augmentation of their method, where a value function is used to improve the accuracy of the density ratio estimation. They show that their method outperforms existing methods in terms of the number of samples required to achieve the desired density ratio and value function estimation.    The paper is well-written and well-motivated, and the contributions of the paper are clear and convincing.  I have a few concerns about the paper:   1. I am not convinced that this paper is a new direction in the field of bias reduction.  2. I do not think that the bias reduced augmentation is a novel idea.  3. The bias reduction method is not novel.  4. The paper does not compare the method to existing methods, and does not show that it outperforms them. ","This paper proposes a novel approach to infinite horizon off-policy evaluation, where the goal is to estimate the stationary density ratio of the optimal policy. The authors propose a bias-reduced augmentation of their method, where a value function is used to improve the accuracy of the density ratio estimation. They show that their method outperforms existing methods in terms of the number of samples required to achieve the desired density ratio and value function estimation.    The paper is well-written and well-motivated, and the contributions of the paper are clear and convincing.  I have a few concerns about the paper:   1. I am not convinced that this paper is a new direction in the field of bias reduction.  2. I do not think that the bias reduced augmentation is a novel idea.  3. The bias reduction method is not novel.  4. The paper does not compare the method to existing methods, and does not show that it outperforms them. "
12915,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"large - scale base classes USED-FOR generalization of prior knowledge. base data USED-FOR task - agnostic feature. Metric - Softmax loss USED-FOR task - agnostic feature. Metric - Softmax loss USED-FOR discriminative feature. Metric - Softmax loss COMPARE episodic training. episodic training COMPARE Metric - Softmax loss. episodic training USED-FOR discriminative feature. task - adaptive transformation USED-FOR classifier. classifier USED-FOR few - shot setting. task - adaptive transformation USED-FOR few - shot setting. approach COMPARE state - of - the - arts. state - of - the - arts COMPARE approach. mini - ImageNet CONJUNCTION CUB-200 - 2011 benchmarks. CUB-200 - 2011 benchmarks CONJUNCTION mini - ImageNet. CUB-200 - 2011 benchmarks EVALUATE-FOR state - of - the - arts. mini - ImageNet EVALUATE-FOR state - of - the - arts. CUB-200 - 2011 benchmarks EVALUATE-FOR approach. mini - ImageNet EVALUATE-FOR approach. Task is Few - shot classification. Generic are task, and two - stage framework. Method are Metric - Softmax classifier, and fine - tuning scheme. ","Few-shot classification is a challenging task where the generalization of prior knowledge is limited to large-scale base classes. This paper proposes a two-stage framework to tackle this problem. First, a task-agnostic feature is learned from the base data. Then, a Metric-Softmax loss is used to train a discriminative feature that is similar to episodic training. In the few-shot setting, the classifier is fine-tuned using the task-adaptive transformation of the base class.    The authors show that their approach outperforms the state-of-the-arts on mini-ImageNet and CUB-200-2011 benchmarks. They also show that the Metric -Softmax classifier can be trained in an end-to-end manner. The authors also show how to apply a fine-tune scheme to further improve the performance.","Few-shot classification is a challenging task where the generalization of prior knowledge is limited to large-scale base classes. This paper proposes a two-stage framework to tackle this problem. First, a task-agnostic feature is learned from the base data. Then, a Metric-Softmax loss is used to train a discriminative feature that is similar to episodic training. In the few-shot setting, the classifier is fine-tuned using the task-adaptive transformation of the base class.    The authors show that their approach outperforms the state-of-the-arts on mini-ImageNet and CUB-200-2011 benchmarks. They also show that the Metric -Softmax classifier can be trained in an end-to-end manner. The authors also show how to apply a fine-tune scheme to further improve the performance."
12924,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,fluid flow USED-FOR simulations of complex flow phenomena. fluid flow HYPONYM-OF problems. data - driven approach USED-FOR numerical solvers. accuracy EVALUATE-FOR numerical solvers. fine simulation resolution FEATURE-OF numerical scheme. numerical scheme USED-FOR method. neural network USED-FOR correction. fully supervised learning methods CONJUNCTION unsupervised learning method. unsupervised learning method CONJUNCTION fully supervised learning methods. naive and an optimized data acquisition USED-FOR fully supervised learning methods. differentiable Navier - Stokes solver USED-FOR unsupervised learning method. fully supervised learning methods HYPONYM-OF learning approaches. learning approaches USED-FOR targeted learning problem. unsupervised learning method HYPONYM-OF learning approaches. approach USED-FOR arbitrary partial differential equation models. accuracy EVALUATE-FOR fluid flow simulations. Method is numerical methods. Task is nonlinear simulation problems. ,"This paper proposes a data-driven approach to train numerical solvers to improve the accuracy of numerical methods for nonlinear simulation problems. The authors focus on two problems: (1) fluid flow for simulations of complex flow phenomena, and (2) problems where the numerical scheme has a fine simulation resolution. The proposed method is based on a numerical scheme with a fixed numerical scheme and a neural network that performs a correction at the end of the training of the neural network.  The authors compare two learning approaches for the targeted learning problem: fully supervised learning methods based on naive and an optimized data acquisition, and an unsupervised learning method based on the differentiable Navier-Stokes solver. The paper shows that the proposed approach can be applied to arbitrary partial differential equation models, and shows that it improves the accuracy for fluid flow simulations. ","This paper proposes a data-driven approach to train numerical solvers to improve the accuracy of numerical methods for nonlinear simulation problems. The authors focus on two problems: (1) fluid flow for simulations of complex flow phenomena, and (2) problems where the numerical scheme has a fine simulation resolution. The proposed method is based on a numerical scheme with a fixed numerical scheme and a neural network that performs a correction at the end of the training of the neural network.  The authors compare two learning approaches for the targeted learning problem: fully supervised learning methods based on naive and an optimized data acquisition, and an unsupervised learning method based on the differentiable Navier-Stokes solver. The paper shows that the proposed approach can be applied to arbitrary partial differential equation models, and shows that it improves the accuracy for fluid flow simulations. "
12933,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"problem USED-FOR downstream online continual learning tasks. resource constrained data collection USED-FOR learning methods. discrete autoencoders PART-OF Quantization Modules ( SQM ). Quantization Modules ( SQM ) PART-OF architecture. discrete autoencoders PART-OF architecture. module USED-FOR latent space. latent space FEATURE-OF module. module USED-FOR module. methods COMPARE approach. approach COMPARE methods. method USED-FOR applications. episodic memory USED-FOR Experience Replay. episodic memory USED-FOR SQM. fixed memory budget USED-FOR continual learning benchmarks. method USED-FOR online compression of larger images. it USED-FOR modalities. Imagenet USED-FOR online compression of larger images. LiDAR data HYPONYM-OF modalities. Task is Online Continual Compression. Generic are learned representation, and modularity. OtherScientificTerm are moderate compressions, and pretraining. ","This paper tackles the problem of online continual learning in the context of resource constrained data collection for learning methods with a focus on online continual compression. The authors propose a new architecture, called Quantization Modules (SQM), which consists of discrete autoencoders and a learned representation that is modular. This problem is important for downstream online continual continual learning tasks where the learned representations are not available for moderate compressions. The proposed architecture is based on the idea of modularity, where each module in the learned representation is modular in the latent space. Unlike previous methods, the proposed approach does not require a fixed memory budget and can be applied to a wide range of learning methods.   The authors demonstrate that the proposed method is applicable to a variety of applications, including online compression of larger images on Imagenet, online compression for continual learning benchmarks, and the use of an episodic memory for Experience Replay. In addition, the authors show that SQM can also be used to learn a module that can be used for a fixed number of times, and that it can be extended to other modalities (e.g. LiDAR data). The authors also show that the method is able to learn to compress larger images in the presence of pretraining, which is an important problem. ","This paper tackles the problem of online continual learning in the context of resource constrained data collection for learning methods with a focus on online continual compression. The authors propose a new architecture, called Quantization Modules (SQM), which consists of discrete autoencoders and a learned representation that is modular. This problem is important for downstream online continual continual learning tasks where the learned representations are not available for moderate compressions. The proposed architecture is based on the idea of modularity, where each module in the learned representation is modular in the latent space. Unlike previous methods, the proposed approach does not require a fixed memory budget and can be applied to a wide range of learning methods.   The authors demonstrate that the proposed method is applicable to a variety of applications, including online compression of larger images on Imagenet, online compression for continual learning benchmarks, and the use of an episodic memory for Experience Replay. In addition, the authors show that SQM can also be used to learn a module that can be used for a fixed number of times, and that it can be extended to other modalities (e.g. LiDAR data). The authors also show that the method is able to learn to compress larger images in the presence of pretraining, which is an important problem. "
12942,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"framework USED-FOR multi - label problem. Metric Learning USED-FOR k nearest neighbor algorithms. k nearest neighbor algorithms USED-FOR multi - label problem. k nearest neighbor algorithms HYPONYM-OF framework. deep representation approach USED-FOR metric learning. neural networks USED-FOR feature data. raw image data USED-FOR neural networks. neural networks USED-FOR deep representation approach. deep convolutional networks USED-FOR image data. Bidirectional Representation learning USED-FOR network architecture. deep neural networks USED-FOR metric space. metric space FEATURE-OF testing data. deep neural networks USED-FOR model. approach COMPARE methods. methods COMPARE approach. multi - labels tasks EVALUATE-FOR approach. multi - labels tasks EVALUATE-FOR methods. systematic metric EVALUATE-FOR approach. systematic metric EVALUATE-FOR methods. Task is Multi - Label Learning task. Method are multiple - label metric learning, and multi - label metric learning. OtherScientificTerm are application restriction, and label dependency. Material is multi - label data set. ","This paper proposes a framework called Metric Learning for k nearest neighbor algorithms to solve the multi-label problem, i.e., k nearest neighbour algorithms for the Multi-Label Learning task. The authors propose a deep representation approach for metric learning based on neural networks trained on raw image data, where neural networks are used to learn feature data. The proposed model is based on deep convolutional networks for image data and uses Bidirectional Representation learning to learn the network architecture. The model is trained using deep neural networks to learn a metric space for testing data in the metric space, where the application restriction is that the label dependency on the label of the test data is not too high. The approach is evaluated on multi-labels tasks and compared with other methods on a systematic metric. The results show that the proposed approach outperforms the other methods in terms of performance on a number of tasks. The paper also shows that the model is able to generalize well to a multi- label data set, which is an important contribution of the paper.    The paper is well-written, well-motivated, and well-structured. However, there are a few issues in the paper: (1) the multiple-label metric learning is not clearly stated, (2) it is not clear how the proposed method is differentiable, (3) there is no discussion of the application of bidirectional representation learning, and (4) the paper does not provide any discussion of how the model can generalize to a new task. ","This paper proposes a framework called Metric Learning for k nearest neighbor algorithms to solve the multi-label problem, i.e., k nearest neighbour algorithms for the Multi-Label Learning task. The authors propose a deep representation approach for metric learning based on neural networks trained on raw image data, where neural networks are used to learn feature data. The proposed model is based on deep convolutional networks for image data and uses Bidirectional Representation learning to learn the network architecture. The model is trained using deep neural networks to learn a metric space for testing data in the metric space, where the application restriction is that the label dependency on the label of the test data is not too high. The approach is evaluated on multi-labels tasks and compared with other methods on a systematic metric. The results show that the proposed approach outperforms the other methods in terms of performance on a number of tasks. The paper also shows that the model is able to generalize well to a multi- label data set, which is an important contribution of the paper.    The paper is well-written, well-motivated, and well-structured. However, there are a few issues in the paper: (1) the multiple-label metric learning is not clearly stated, (2) it is not clear how the proposed method is differentiable, (3) there is no discussion of the application of bidirectional representation learning, and (4) the paper does not provide any discussion of how the model can generalize to a new task. "
12951,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,large batch sizes CONJUNCTION data parallelism. data parallelism CONJUNCTION large batch sizes. large batch sizes USED-FOR neural networks training. convergence rate EVALUATE-FOR asynchronous methods. dynamical stability USED-FOR asynchronous training. asynchronous stochastic gradient descent algorithm USED-FOR minima. closed - form rules USED-FOR learning rate. momentum PART-OF analysis. momentum USED-FOR training stability. OtherScientificTerm is delay. Task is training. Metric is generalization. Generic is algorithm. ,"This paper considers the problem of neural networks training with large batch sizes and data parallelism, where the delay between training and test time can be very large. The authors propose an asynchronous stochastic gradient descent algorithm that is able to reach minima that are asymptotically stable as possible. They show that the convergence rate of the proposed asynchronous methods can be improved by using dynamical stability for asynchronous training. They also show that momentum in the analysis can be used to improve the training stability and generalization performance.   The authors also propose an algorithm that uses closed-form rules to control the learning rate and the number of epochs.","This paper considers the problem of neural networks training with large batch sizes and data parallelism, where the delay between training and test time can be very large. The authors propose an asynchronous stochastic gradient descent algorithm that is able to reach minima that are asymptotically stable as possible. They show that the convergence rate of the proposed asynchronous methods can be improved by using dynamical stability for asynchronous training. They also show that momentum in the analysis can be used to improve the training stability and generalization performance.   The authors also propose an algorithm that uses closed-form rules to control the learning rate and the number of epochs."
12960,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"Value estimation PART-OF reinforcement learning ( RL ) paradigm. rich transition structure USED-FOR Model learning. weak scalar signal USED-FOR model - free methods. approach USED-FOR representation learning. representation learning USED-FOR RL. OtherScientificTerm are predictors, reward function, features, and value function. Generic are approaches, and task. Task are value prediction, and policy evaluation settings. Material is Atari 2600 games. ","Value estimation is an important component of the reinforcement learning (RL) paradigm. Model learning with rich transition structure is challenging due to the fact that predictors can be biased. Model-free methods rely on a weak scalar signal, which can be prohibitively expensive to compute. This paper proposes two approaches to address this issue. The first approach is based on the observation that the reward function is highly correlated with the features of the environment. The second approach is to learn a representation learning for RL based on this observation. The paper shows that both approaches can be applied to the same task, but that the value prediction is more robust to changes in the environment, which is important for policy evaluation settings. Experiments are conducted on Atari 2600 games. ","Value estimation is an important component of the reinforcement learning (RL) paradigm. Model learning with rich transition structure is challenging due to the fact that predictors can be biased. Model-free methods rely on a weak scalar signal, which can be prohibitively expensive to compute. This paper proposes two approaches to address this issue. The first approach is based on the observation that the reward function is highly correlated with the features of the environment. The second approach is to learn a representation learning for RL based on this observation. The paper shows that both approaches can be applied to the same task, but that the value prediction is more robust to changes in the environment, which is important for policy evaluation settings. Experiments are conducted on Atari 2600 games. "
12969,SP:6388fb91f2eaac02d9406672760a237f78735452,node classification CONJUNCTION graph classification. graph classification CONJUNCTION node classification. Graph Neural Networks ( GNNs ) USED-FOR graph related tasks. graph classification HYPONYM-OF graph related tasks. node classification HYPONYM-OF graph related tasks. graph neural networks USED-FOR adversarial attacks. graph rewiring operation USED-FOR graph. graph rewiring operation COMPARE operators. operators COMPARE graph rewiring operation. reinforcement learning USED-FOR attack strategy. rewiring operation USED-FOR reinforcement learning. rewiring operation USED-FOR attack strategy. real world graphs EVALUATE-FOR framework. perturbation FEATURE-OF graph structure. OtherScientificTerm is edges. ,"Graph Neural Networks (GNNs) are widely used for graph related tasks, such as node classification and graph classification, and adversarial attacks on graph neural networks are common. This paper proposes a novel attack strategy based on reinforcement learning using the rewiring operation of the graph to be rewired. The authors show that the graph rewires operation is more robust to perturbation in the graph structure compared to existing operators. The proposed framework is tested on two real world graphs and shows that the proposed attack strategy is more effective than existing ones. ","Graph Neural Networks (GNNs) are widely used for graph related tasks, such as node classification and graph classification, and adversarial attacks on graph neural networks are common. This paper proposes a novel attack strategy based on reinforcement learning using the rewiring operation of the graph to be rewired. The authors show that the graph rewires operation is more robust to perturbation in the graph structure compared to existing operators. The proposed framework is tested on two real world graphs and shows that the proposed attack strategy is more effective than existing ones. "
12978,SP:233b12d422d0ac40026efdf7aab9973181902d70,"selected data USED-FOR neural networks. Stein ’s unbiased risk estimator ( SURE ) USED-FOR denoising problems. divergence term PART-OF SURE. neural network framework USED-FOR divergence term. close form expression USED-FOR unbiased estimator. unbiased estimator USED-FOR prediction error. close form expression USED-FOR prediction error. piecewise linear representation USED-FOR encoderdecoder CNN. bootstrap and aggregation scheme USED-FOR neural network. close form representation USED-FOR bootstrap and aggregation scheme. neural network USED-FOR identity mapping. inverse problems EVALUATE-FOR algorithm. Generic are architectures, and it. ","This paper proposes Stein’s unbiased risk estimator (SURE) for denoising problems, which is a generalization of the work of Stein. The key idea of SURE is to add a divergence term to the divergence term in the neural network framework, which allows to train neural networks on selected data. The authors show that the unbiased estimator of the prediction error can be expressed as a close form expression of the divergence between the encoderdecoder CNN and the piecewise linear representation of the input. They also propose a bootstrap and aggregation scheme to train a neural network with this close form representation, and show that it can be applied to a variety of architectures. Finally, they show that their algorithm can be used to solve inverse problems where the identity mapping between the input and the output of a given neural network is unknown. ","This paper proposes Stein’s unbiased risk estimator (SURE) for denoising problems, which is a generalization of the work of Stein. The key idea of SURE is to add a divergence term to the divergence term in the neural network framework, which allows to train neural networks on selected data. The authors show that the unbiased estimator of the prediction error can be expressed as a close form expression of the divergence between the encoderdecoder CNN and the piecewise linear representation of the input. They also propose a bootstrap and aggregation scheme to train a neural network with this close form representation, and show that it can be applied to a variety of architectures. Finally, they show that their algorithm can be used to solve inverse problems where the identity mapping between the input and the output of a given neural network is unknown. "
12987,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"meta - learning approaches USED-FOR few - shot classification. meta - knowledge CONJUNCTION task - specific learning. task - specific learning CONJUNCTION meta - knowledge. Bayesian inference framework USED-FOR objective. variational inference USED-FOR objective. variational inference USED-FOR it. it COMPARE meta - learning approaches. meta - learning approaches COMPARE it. balancing component CONJUNCTION Bayesian learning framework. Bayesian learning framework CONJUNCTION balancing component. OtherScientificTerm are distributional difference, task relatedness, and balancing variables. Method are meta - learning model, and meta - learning. Material is multiple realistic taskand class - imbalanced datasets. ",This paper proposes a new meta-learning approach for few-shot classification. The authors argue that meta-learners should consider the distributional difference between the meta-training and meta-test distribution as a metric to measure the task relatedness between two tasks. The paper proposes to use a Bayesian learning framework to balance the distribution of meta-knowledge and task-specific information across tasks. This is done by using a variational inference framework to optimize the objective function. The proposed approach is evaluated on a number of benchmark datasets. ,This paper proposes a new meta-learning approach for few-shot classification. The authors argue that meta-learners should consider the distributional difference between the meta-training and meta-test distribution as a metric to measure the task relatedness between two tasks. The paper proposes to use a Bayesian learning framework to balance the distribution of meta-knowledge and task-specific information across tasks. This is done by using a variational inference framework to optimize the objective function. The proposed approach is evaluated on a number of benchmark datasets. 
12996,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"high - dimensional, continuous observations CONJUNCTION unknown dynamics. unknown dynamics CONJUNCTION high - dimensional, continuous observations. distribution shift USED-FOR Supervised learning methods. behavioral cloning ( BC ) USED-FOR Supervised learning methods. inverse RL CONJUNCTION generative adversarial imitation learning ( GAIL ). generative adversarial imitation learning ( GAIL ) CONJUNCTION inverse RL. generative adversarial imitation learning ( GAIL ) HYPONYM-OF methods. inverse RL HYPONYM-OF methods. reinforcement learning ( RL ) USED-FOR methods. generative adversarial imitation learning ( GAIL ) HYPONYM-OF reinforcement learning ( RL ). inverse RL HYPONYM-OF reinforcement learning ( RL ). methods USED-FOR reward function. reward function USED-FOR task. adversarial training USED-FOR complex and brittle approximation techniques. complex and brittle approximation techniques USED-FOR reward function. complex and brittle approximation techniques USED-FOR methods. RL USED-FOR alternative. sparsity prior USED-FOR long - horizon imitation. regularized variant of BC USED-FOR SQIL. sparsity prior USED-FOR regularized variant of BC. SQIL COMPARE GAIL. GAIL COMPARE SQIL. Atari CONJUNCTION MuJoCo. MuJoCo CONJUNCTION Atari. Box2D CONJUNCTION Atari. Atari CONJUNCTION Box2D. SQIL COMPARE BC. BC COMPARE SQIL. Atari FEATURE-OF image - based and low - dimensional tasks. Box2D FEATURE-OF image - based and low - dimensional tasks. MuJoCo FEATURE-OF image - based and low - dimensional tasks. image - based and low - dimensional tasks EVALUATE-FOR SQIL. image - based and low - dimensional tasks EVALUATE-FOR GAIL. imitation method COMPARE methods. methods COMPARE imitation method. constant rewards FEATURE-OF RL. constant rewards USED-FOR imitation method. RL USED-FOR imitation method. learned rewards USED-FOR methods. OtherScientificTerm are expert behavior, error accumulation, out - of - distribution states, and constant reward. Method are RL agent, and soft Q imitation learning ( SQIL ). Generic is method. ","Supervised learning methods have been shown to suffer from distribution shift in the presence of expert behavior, which can lead to error accumulation and out-of-distribution states. This paper proposes a new method called soft Q imitation learning (SQIL) to address this issue. Supervised learning algorithms such as behavioral cloning (BC) and RL (inverse RL) have been proposed to address the problem of distribution shift due to high-dimensional, continuous observations and unknown dynamics. Two methods are proposed to learn the reward function for a given task using reinforcement learning (RL) or reinforcement learning with reinforcement learning, i.e., inverse RL and generative adversarial imitation learning [1]. These methods use complex and brittle approximation techniques based on adversarial training [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,19] to learn a reward function. The proposed method SQIL uses a regularized variant of BC with a sparsity prior for long-horizon imitation, and an alternative based on RL is also proposed. SQIL is shown to outperform BC and GAIL on image-based and low-dimensional tasks on Box2D, Atari, and MuJoCo. The authors also show that their imitation method outperforms other methods that use RL with constant rewards.    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]   The paper is well-written and well-motivated.  The main contribution of this paper is to propose a new imitation method that outperforms existing methods that rely on RL with learned rewards [1,2].  ","Supervised learning methods have been shown to suffer from distribution shift in the presence of expert behavior, which can lead to error accumulation and out-of-distribution states. This paper proposes a new method called soft Q imitation learning (SQIL) to address this issue. Supervised learning algorithms such as behavioral cloning (BC) and RL (inverse RL) have been proposed to address the problem of distribution shift due to high-dimensional, continuous observations and unknown dynamics. Two methods are proposed to learn the reward function for a given task using reinforcement learning (RL) or reinforcement learning with reinforcement learning, i.e., inverse RL and generative adversarial imitation learning [1]. These methods use complex and brittle approximation techniques based on adversarial training [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,19] to learn a reward function. The proposed method SQIL uses a regularized variant of BC with a sparsity prior for long-horizon imitation, and an alternative based on RL is also proposed. SQIL is shown to outperform BC and GAIL on image-based and low-dimensional tasks on Box2D, Atari, and MuJoCo. The authors also show that their imitation method outperforms other methods that use RL with constant rewards.    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]   The paper is well-written and well-motivated.  The main contribution of this paper is to propose a new imitation method that outperforms existing methods that rely on RL with learned rewards [1,2].  "
13005,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"Point clouds HYPONYM-OF Lagrangian representation. deep - learning method USED-FOR stable and temporally coherent feature spaces. stable and temporally coherent feature spaces FEATURE-OF points clouds. temporal loss function USED-FOR mingling. higher time derivatives USED-FOR temporal loss function. super - resolution method CONJUNCTION truncation approach. truncation approach CONJUNCTION super - resolution method. techniques CONJUNCTION truncation approach. truncation approach CONJUNCTION techniques. techniques PART-OF super - resolution method. method USED-FOR large, deforming point sets. Generic are approaches, and approach. OtherScientificTerm are time dimension, flickering, undesirable local minima, halo structures, and halos. ","This paper proposes a deep-learning method to learn stable and temporally coherent feature spaces for points clouds (Point clouds are a Lagrangian representation). The authors propose a temporal loss function with higher time derivatives to avoid mingling between different time dimension. They also propose a super-resolution method, a truncation approach, and a number of techniques to improve the performance of existing approaches. The proposed method is able to learn large, deforming point sets without flickering and avoids undesirable local minima. The authors also show that the proposed approach is robust to halo structures and halos. ","This paper proposes a deep-learning method to learn stable and temporally coherent feature spaces for points clouds (Point clouds are a Lagrangian representation). The authors propose a temporal loss function with higher time derivatives to avoid mingling between different time dimension. They also propose a super-resolution method, a truncation approach, and a number of techniques to improve the performance of existing approaches. The proposed method is able to learn large, deforming point sets without flickering and avoids undesirable local minima. The authors also show that the proposed approach is robust to halo structures and halos. "
13014,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"hand - crafted or Euclidean cost USED-FOR cost functions. side information USED-FOR cost function. side information USED-FOR subset correspondence. annotated cell type PART-OF single - cell data. side information USED-FOR cost function. marriage - matching CONJUNCTION single - cell RNA - seq. single - cell RNA - seq CONJUNCTION marriage - matching. images CONJUNCTION marriage - matching. marriage - matching CONJUNCTION images. images EVALUATE-FOR method. single - cell RNA - seq EVALUATE-FOR method. method COMPARE state - of - the - art benchmarks. state - of - the - art benchmarks COMPARE method. marriage - matching EVALUATE-FOR method. images CONJUNCTION single - cell RNA - seq. single - cell RNA - seq CONJUNCTION images. Generic is it. OtherScientificTerm is confounding. Method are Optimal transport ( OT ), OT, transport cost function, and Sinkhorn algorithm. ","This paper proposes Optimal transport (OT) as a new cost function for learning a subset of cell types from single-cell data with an annotated cell type. The cost functions are either hand-crafted or Euclidean cost, and the authors show that it can be used to avoid confounding. The key idea is to use side information from the side information to learn a cost function that maximizes the subset correspondence between each cell type and the optimal transport cost function. The proposed method is evaluated on images, marriage-matching, and single cell RNA-seq, and compared to state-of-the-art benchmarks. The authors also propose a Sinkhorn algorithm. ","This paper proposes Optimal transport (OT) as a new cost function for learning a subset of cell types from single-cell data with an annotated cell type. The cost functions are either hand-crafted or Euclidean cost, and the authors show that it can be used to avoid confounding. The key idea is to use side information from the side information to learn a cost function that maximizes the subset correspondence between each cell type and the optimal transport cost function. The proposed method is evaluated on images, marriage-matching, and single cell RNA-seq, and compared to state-of-the-art benchmarks. The authors also propose a Sinkhorn algorithm. "
13023,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,large datasets USED-FOR method. algorithm USED-FOR representation underlying normal data. clustering technique USED-FOR high dimensional data. clustering CONJUNCTION representation learning. representation learning CONJUNCTION clustering. clustering USED-FOR hypothesizing normal candidate subset. normal data subset USED-FOR autoencoder. representation learning USED-FOR hypothesizing normal candidate subset. reconstruction error USED-FOR scoring function. autoencoder USED-FOR reconstruction error. public benchmark datasets EVALUATE-FOR method. method COMPARE semi - supervised techniques. semi - supervised techniques COMPARE method. method COMPARE unsupervised techniques. unsupervised techniques COMPARE method. public benchmark datasets EVALUATE-FOR unsupervised techniques. unsupervised techniques COMPARE semi - supervised techniques. semi - supervised techniques COMPARE unsupervised techniques. Task is normal data selection. ,"This paper proposes a new algorithm to learn the representation underlying normal data. The proposed method can be applied to large datasets. The idea is to use a clustering technique for high dimensional data. Then, hypothesizing normal candidate subset based on clustering and representation learning is used to train an autoencoder on the normal data subset. The scoring function is based on the reconstruction error of the autoencoders. The method is evaluated on several public benchmark datasets and shows that the proposed method outperforms unsupervised and semi-supervised techniques.   ","This paper proposes a new algorithm to learn the representation underlying normal data. The proposed method can be applied to large datasets. The idea is to use a clustering technique for high dimensional data. Then, hypothesizing normal candidate subset based on clustering and representation learning is used to train an autoencoder on the normal data subset. The scoring function is based on the reconstruction error of the autoencoders. The method is evaluated on several public benchmark datasets and shows that the proposed method outperforms unsupervised and semi-supervised techniques.   "
13032,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"learning control policies USED-FOR reward function. local reward improvement update USED-FOR first step. L norm CONJUNCTION Kullback - Leibler divergence. Kullback - Leibler divergence CONJUNCTION L norm. convergence FEATURE-OF PCPO. metrics EVALUATE-FOR PCPO. L norm HYPONYM-OF metrics. Kullback - Leibler divergence HYPONYM-OF metrics. metrics USED-FOR convergence. control tasks EVALUATE-FOR PCPO. constraint violation CONJUNCTION reward. reward CONJUNCTION constraint violation. reward EVALUATE-FOR PCPO. OtherScientificTerm are constraints, fairness, policy, constraint set, and policy update. Generic are algorithm, and second step. Method is iterative method. Metric is reward improvement. ","This paper considers the problem of learning control policies for a reward function that satisfies a set of constraints. The authors propose an algorithm called PCPO, where the first step is a local reward improvement update, and the second step is an iterative update of the policy. The algorithm is based on the idea of fairness, which is to ensure that the policy does not violate any of the constraints in the constraint set, and that the reward improvement is fair.  The authors show that PCPO achieves convergence on two metrics, the L norm and the Kullback-Leibler divergence. They also show that the convergence of PCPO can be improved when the policy update is done in two steps. The first step updates the policy to satisfy the constraints, while the second steps updates the reward.  Experiments are conducted on several control tasks, and PCPO is shown to achieve good performance on both constraint violation and reward improvement.  ","This paper considers the problem of learning control policies for a reward function that satisfies a set of constraints. The authors propose an algorithm called PCPO, where the first step is a local reward improvement update, and the second step is an iterative update of the policy. The algorithm is based on the idea of fairness, which is to ensure that the policy does not violate any of the constraints in the constraint set, and that the reward improvement is fair.  The authors show that PCPO achieves convergence on two metrics, the L norm and the Kullback-Leibler divergence. They also show that the convergence of PCPO can be improved when the policy update is done in two steps. The first step updates the policy to satisfy the constraints, while the second steps updates the reward.  Experiments are conducted on several control tasks, and PCPO is shown to achieve good performance on both constraint violation and reward improvement.  "
13041,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"analogy structure FEATURE-OF embedding space. low rank transformation USED-FOR embedding. embedding transformation USED-FOR relative distances. α USED-FOR word embedding. Method is word embedding methods. Generic are inner mechanism, and method. OtherScientificTerm is word - context co - occurrence space. Material is real datasets. ","This paper proposes a novel way to improve the performance of word embedding methods. The authors propose an inner mechanism, which is based on the observation that word-context co-occurrence space has analogy structure in the embedding space. They propose to use a low rank transformation to learn an embedding, and use this embedding transformation to reduce the relative distances between two word embeddings. They show that the proposed method, α, can be used to learn a good embedding. Experiments on real datasets are conducted to validate the effectiveness of α.","This paper proposes a novel way to improve the performance of word embedding methods. The authors propose an inner mechanism, which is based on the observation that word-context co-occurrence space has analogy structure in the embedding space. They propose to use a low rank transformation to learn an embedding, and use this embedding transformation to reduce the relative distances between two word embeddings. They show that the proposed method, α, can be used to learn a good embedding. Experiments on real datasets are conducted to validate the effectiveness of α."
13050,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"Similarity measurement USED-FOR data mining and machine learning tasks. approximate Random Projection Trees PART-OF X - Forest. RP Trees USED-FOR similarity measurement. layers PART-OF tree. randomness USED-FOR partition. real - world datasets EVALUATE-FOR model. Euclidean distance - based similarity metrics USED-FOR clustering tasks. model COMPARE RP Trees. RP Trees COMPARE model. X - Forest COMPARE RP Trees. RP Trees COMPARE X - Forest. X - Forest HYPONYM-OF model. efficiency EVALUATE-FOR model. accuracy EVALUATE-FOR RP Trees. Method is similarity measurement solution. Metric are speed, and exalted speed. OtherScientificTerm are prior knowledge, and projection vectors. Task is similarity measurements. ","This paper proposes a new similarity measurement solution for data mining and machine learning tasks. Similarity measurement is an important problem in data mining because of its speed and efficiency. The authors propose a model called X-Forest, which consists of approximate Random Projection Trees. The key idea is to partition a tree into layers and use the randomness of the partition to compute a similarity measurement.  The authors show that the proposed model outperforms the state-of-the-art RP Trees in terms of accuracy and efficiency on several real-world datasets. They also show that Euclidean distance-based similarity metrics can be used for clustering tasks.    The main contribution of the paper is that the authors propose to use prior knowledge to partition the data into a tree, and then use projection vectors to compute the similarity measurements. They show that their model is more efficient than RP Trees. ","This paper proposes a new similarity measurement solution for data mining and machine learning tasks. Similarity measurement is an important problem in data mining because of its speed and efficiency. The authors propose a model called X-Forest, which consists of approximate Random Projection Trees. The key idea is to partition a tree into layers and use the randomness of the partition to compute a similarity measurement.  The authors show that the proposed model outperforms the state-of-the-art RP Trees in terms of accuracy and efficiency on several real-world datasets. They also show that Euclidean distance-based similarity metrics can be used for clustering tasks.    The main contribution of the paper is that the authors propose to use prior knowledge to partition the data into a tree, and then use projection vectors to compute the similarity measurements. They show that their model is more efficient than RP Trees. "
13059,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"Statistical inference methods USED-FOR machine learning. Markov chain Monte Carlo ( MCMC ) CONJUNCTION variational inference ( VI ). variational inference ( VI ) CONJUNCTION Markov chain Monte Carlo ( MCMC ). Markov chain Monte Carlo ( MCMC ) USED-FOR inference algorithms. variational inference ( VI ) USED-FOR inference algorithms. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. simulation bias FEATURE-OF finite - length MCMC chains. hybrid method USED-FOR VI. gradient - based optimisation USED-FOR hybrid method. gradient - based optimisation USED-FOR simulation bias. method USED-FOR low - biased samples. approximation bias CONJUNCTION computational efficiency. computational efficiency CONJUNCTION approximation bias. method USED-FOR MCMC hyper - parameters. method COMPARE hybrid methods. hybrid methods COMPARE method. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. hybrid methods USED-FOR MCMC. hybrid methods USED-FOR VI. Generic is methods. Method are MCMC methods, and VI methods. OtherScientificTerm is MCMC simulation. ","This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation to reduce the simulation bias of finite-length MCMC chains. The authors show that the proposed method can be used to obtain low-biased samples, which are then used to optimize the MCMC hyper-parameters. The proposed method is shown to be more efficient than existing hybrid methods for MCMC and VI.  ","This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation to reduce the simulation bias of finite-length MCMC chains. The authors show that the proposed method can be used to obtain low-biased samples, which are then used to optimize the MCMC hyper-parameters. The proposed method is shown to be more efficient than existing hybrid methods for MCMC and VI.  "
13068,SP:64f2744e938bd62cd47c1066dc404a42134953da,"treatment USED-FOR Inferring causal effects. observational data USED-FOR Inferring causal effects. state - of - the - art methods USED-FOR causal inference. adapted unconfoundedness hypothesis USED-FOR they. variational autoencoders USED-FOR missing values. variational autoencoders USED-FOR latent confounders. them PART-OF multiple imputation strategy. methodology COMPARE competitors. competitors COMPARE methodology. methodology USED-FOR non - linear models. non - linear models COMPARE competitors. competitors COMPARE non - linear models. OtherScientificTerm are covariates, and Missing data. Task are real - world analyses, and causal inference procedures. Generic is They. ","This paper proposes a new treatment for Inferring causal effects from observational data. The authors argue that existing state-of-the-art methods for causal inference are not robust to covariates that are not present in real-world analyses. Missing data is a common problem in causal inference procedures. They argue that they rely on an adapted unconfoundedness hypothesis, and propose to use variational autoencoders to model the missing values of latent confounders, and incorporate them into a multiple imputation strategy. They show that their methodology outperforms competitors on non-linear models. ","This paper proposes a new treatment for Inferring causal effects from observational data. The authors argue that existing state-of-the-art methods for causal inference are not robust to covariates that are not present in real-world analyses. Missing data is a common problem in causal inference procedures. They argue that they rely on an adapted unconfoundedness hypothesis, and propose to use variational autoencoders to model the missing values of latent confounders, and incorporate them into a multiple imputation strategy. They show that their methodology outperforms competitors on non-linear models. "
13077,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"neural architecture search algorithm USED-FOR compact reinforcement learning ( RL ) policies. ENAS CONJUNCTION ES. ES CONJUNCTION ENAS. combinatorial search space FEATURE-OF NAS. edge - partitionings USED-FOR compact architectures. vanilla policies COMPARE compact policies. compact policies COMPARE vanilla policies. compression EVALUATE-FOR vanilla policies. compression EVALUATE-FOR compact policies. colorings USED-FOR policies. colorings USED-FOR RL tasks. Toeplitz matrices USED-FOR compact policies. structured neural network architectures USED-FOR RL problems. approach USED-FOR structured neural network architectures. mobile robotics FEATURE-OF RL problems. limited storage and computational resources FEATURE-OF mobile robotics. OtherScientificTerm are edge - partitionings ( colorings ), same - weight classes, and weight parameters. ","This paper proposes a neural architecture search algorithm for learning compact reinforcement learning (RL) policies. The authors use edge-partitionings (colorings) to search for compact architectures based on edge-policies. They show that existing approaches such as ENAS, ES, etc. can be reduced to the combinatorial search space of NAS. They also show that vanilla policies trained with these colorings perform better in terms of compression compared to compact policies trained using Toeplitz matrices. Finally, they show that colorings can be used to train policies for RL tasks with different colorings, and that the same-weight classes can be learned in the same way.   The authors apply their approach to learn structured neural network architectures for RL problems in mobile robotics with limited storage and computational resources. The main contribution of this paper is that the authors propose a search algorithm that can be applied to a wide range of different weight parameters. ","This paper proposes a neural architecture search algorithm for learning compact reinforcement learning (RL) policies. The authors use edge-partitionings (colorings) to search for compact architectures based on edge-policies. They show that existing approaches such as ENAS, ES, etc. can be reduced to the combinatorial search space of NAS. They also show that vanilla policies trained with these colorings perform better in terms of compression compared to compact policies trained using Toeplitz matrices. Finally, they show that colorings can be used to train policies for RL tasks with different colorings, and that the same-weight classes can be learned in the same way.   The authors apply their approach to learn structured neural network architectures for RL problems in mobile robotics with limited storage and computational resources. The main contribution of this paper is that the authors propose a search algorithm that can be applied to a wide range of different weight parameters. "
13086,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"time - series USED-FOR representation learning. Group Transform approach USED-FOR representation learning. framework USED-FOR time - frequency transformations. Wavelet Transform HYPONYM-OF time - frequency transformations. approach USED-FOR non - linear transformations. affine transformations of a mother filter USED-FOR Wavelet Transform filter - bank. transformations USED-FOR signal representations. maps USED-FOR signal representations. maps USED-FOR transformations. parameterization USED-FOR non - linear map. Deep Neural Network USED-FOR Group Transform. time - series datasets EVALUATE-FOR framework. Generic is representation. OtherScientificTerm are invertible maps, and strictly increasing and continuous functions. ","This paper proposes a novel Group Transform approach for representation learning for time-series. The authors propose a novel framework for learning time-frequency transformations such as Wavelet Transform. The proposed approach is able to learn non-linear transformations that are invertible maps.   The Wavelet transform filter-bank is constructed from affine transformations of a mother filter. These transformations can be used to learn signal representations that are invariant to these maps. The paper also proposes a new parameterization for the nonlinear map, which is based on the idea of strictly increasing and continuous functions. The Group Transform is trained using a Deep Neural Network. Experiments are conducted on several time-sensor datasets to demonstrate the effectiveness of the proposed framework.","This paper proposes a novel Group Transform approach for representation learning for time-series. The authors propose a novel framework for learning time-frequency transformations such as Wavelet Transform. The proposed approach is able to learn non-linear transformations that are invertible maps.   The Wavelet transform filter-bank is constructed from affine transformations of a mother filter. These transformations can be used to learn signal representations that are invariant to these maps. The paper also proposes a new parameterization for the nonlinear map, which is based on the idea of strictly increasing and continuous functions. The Group Transform is trained using a Deep Neural Network. Experiments are conducted on several time-sensor datasets to demonstrate the effectiveness of the proposed framework."
13095,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"inductive biases USED-FOR real - world data properties. scale - free HYPONYM-OF real - world data properties. hyperbolic or spherical HYPONYM-OF non - Euclidean spaces. Euclidean geometry CONJUNCTION vector space operations. vector space operations CONJUNCTION Euclidean geometry. Euclidean geometry USED-FOR graph neural networks. vector space operations USED-FOR graph neural networks. graph convolutional networks ( GCN ) USED-FOR ( products of ) constant curvature spaces. gyro - barycentric coordinates USED-FOR Euclidean concept of the center of mass. models USED-FOR Euclidean counterparts. node classification CONJUNCTION distortion minimization. distortion minimization CONJUNCTION node classification. Euclidean GCNs USED-FOR node classification. Euclidean GCNs USED-FOR distortion minimization. distortion minimization USED-FOR symbolic data. non - Euclidean behavior FEATURE-OF symbolic data. Method is unified formalism. OtherScientificTerm are geometries of constant curvature, curvature, and discrete curvature. ","This paper proposes a unified formalism for learning geometries of constant curvature, i.e., non-Euclidean spaces (e.g., hyperbolic or spherical). The authors show that graph convolutional networks (GCN) can be seen as (products of) constant curvatures spaces, and that their inductive biases lead to real-world data properties such as scale-free. The authors also show that the Euclidean geometry and vector space operations of graph neural networks can be used to model the geometry of the graph.   The main contribution of the paper is the introduction of gyro-barycentric coordinates, which are used to represent the geometry and Euclideans of the center of mass of a ball of radius and radius of curvature. The paper also shows that the curvature of the ball can be modeled as a discrete curvature and that models trained with these models can be viewed as Euclideane counterparts.  The authors then show that EuclideAN GCNs are able to learn node classification and distortion minimization on node classification, and can also be used for learning on symbolic data with non-euclidean behavior.","This paper proposes a unified formalism for learning geometries of constant curvature, i.e., non-Euclidean spaces (e.g., hyperbolic or spherical). The authors show that graph convolutional networks (GCN) can be seen as (products of) constant curvatures spaces, and that their inductive biases lead to real-world data properties such as scale-free. The authors also show that the Euclidean geometry and vector space operations of graph neural networks can be used to model the geometry of the graph.   The main contribution of the paper is the introduction of gyro-barycentric coordinates, which are used to represent the geometry and Euclideans of the center of mass of a ball of radius and radius of curvature. The paper also shows that the curvature of the ball can be modeled as a discrete curvature and that models trained with these models can be viewed as Euclideane counterparts.  The authors then show that EuclideAN GCNs are able to learn node classification and distortion minimization on node classification, and can also be used for learning on symbolic data with non-euclidean behavior."
