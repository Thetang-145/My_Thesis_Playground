paper_id,summary
SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"This paper proposes a multi - agent learning framework based on action - observation spaces, where each agent has access to a different action space and a different set of actions. The goal is to learn a policy that generalizes to a larger set of action spaces and more complex action spaces, while ensuring that the policy is transferable to new actions.    The paper proposes two approaches to this problem. The first approach is based on the concept of “ safe action spaces ”, where the action space is assumed to be unsupervised, and the action spaces are assumed to have been observed by the agent in the past. The second approach is a more sophisticated approach based on “ knowledge - based learning ” where the agent is allowed to explore a wider range of actions and actions than the first approach, and learns a policy to generalize to new action spaces."
SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper studies the convergence properties of the SGD method for nonsmooth and strongly convex machine learning problems. The main contributions are :    1.   Providing a theoretical analysis of the convergence rate of SGD under the assumption that the objective is a convex function, and   providing a lower bound on the number of iterations required for SGD to converge to the optimal solution under the convexity assumption. 2. ivproviding a proof that SGD converges to a non - convex solution in the case when the function is not convex. 3."
SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"This paper presents a theoretical analysis of the performance / efficiency trade - off between two approaches to generalize transformer - based architectures, i.e. linear and non - linear transformers. The theoretical analysis is based on the fact that both approaches have their own advantages and disadvantages. The main contributions of the paper are :   1. The paper proposes a new metric that quantifies the difference between the performance of the two approaches under the assumption that the transformer layer is linear and the reservoir layer is nonlinear. This metric is then used to show that the linear layer is more efficient than the nonlinear layer in terms of overall performance.   2. This paper also proposes a strategy to trade off the performance between the linear / nonlinear transformers and reservoir layers."
SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"In this paper, the authors propose a new transform - invariant convolution operator for the space structure of convolution operators.    The main contribution of this paper is the following :   1. This is the first attempt to prove the invariance of the convolution transform of the kernel function of the space representation of a convolutional group. 2. The authors show that this transform is invariant under the assumption that the kernel is a convex combination of two functions. 3. They show that the transformed kernel is also invariant to the cosine transformation of the group representation."
SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"This paper presents a neural program synthesis approach based on a recurrent neural network. The goal is to generate programs that satisfy a given user's intent, while satisfying the constraints provided by the input - output examples. To this end, the authors propose two approaches. The first approach is a top - down recurrent neural model, which is trained on a set of examples, and the second one is a stochastic search - space based approach."
SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"This paper presents a model - based molecular interaction graph neural network ( PGCN ) for protease classification based on topology and energetic features. The model is trained on a set of data collected from the literature. The goal of the model is to identify a subset of proteins that are crucial for the understanding of key life processes. To achieve this goal, the model has two components : ( 1 ) to identify which proteins are most important for the protease specificity landscape, and ( 2 ) to analyze the importance of each protein in terms of its similarity to other proteins in the landscape.    The model was trained on top of the following components :   1 )   a graph convolutional neural network that is based on the similarity between two protein types ( proteins and bacteria ). The second component is a topology - based model that is built on the topology of two proteins ( bacteria and viruses ), and the third one is a model based on their similarity to each other."
SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper studies the problem of Q - learning under the Bellman operator. The authors show that the optimal fixed points under this setting are bounded in the sense that there exist no fixed points that can be made arbitrarily close to each other without compromising on the quality of the error estimator. They then provide a theoretical analysis of the boundedness of the fixed points in terms of the bias of the estimator and the optimal Bellman operators under the assumption that the error is bounded. They also provide theoretical guarantees on the convergence rate of the optimal value estimator under the assumptions that the bias is bounded and that the optimizer has bounded error. Finally, the authors provide empirical results on two benchmark tasks :    [ 1 ] Q - Learning with Dynamic Programming ( Q - DDP ) and [ 2 ] Q Learning with Bellman Operator ( Bellman - ODE ). They show that both algorithms converge linearly to a fixed point that is at least as good as the best fixed point under boundedness assumption."
SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,This paper presents a deep generative model ( DGM ) framework for generating high - resolution natural images that can be used for image classification. The model consists of a generative decoder network that generates high - dimensional natural images and a low - resolution sampler network that samples the images from the decoder. The authors propose a down - sampling method to improve the quality of the samples used by the sampler and decoder networks.
SP:b943a73b1ec34867371325748dc3a91ff4011947,"This paper presents a theoretical analysis of the loss of supervised learning ( SSL ) with few - shot classifications in the federated learning ( FSL ) framework. The authors show that the standard SSL framework with a few - shots classifications can not achieve satisfactory performance in the supervised learning setting. To address this issue, the authors propose to fine - tune the classifier and the embedding network in the FSL framework. To this end, they propose two stages of fine - tuning : feature extractor stage ( extractor extractor ) and feature - trained stage ( feature - embedding stage ). In the feature - train stage, the extracted features are used to train a linear classifier, while in the embeddings stage, they use a tensor - free classifier to obtain the mean classifier. Experiments are conducted to evaluate the performance of the proposed framework."
SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"This paper proposes a two - layer teacher - student framework to study the properties of neural networks in a multistudent setting. In particular, the authors consider the case where the number of teachers is two and the student dimension is two.    The main contributions of the paper are as follows :   ( 1 )   a theoretical proof of the adequateness of the $ \ell_\infty$-value of the metric $ \sqrt{\infty}$ in the context of the multi - layer neural networks setting. This is done by showing that $ \infty^2 $ can be expressed as a weighted sum of two factors : ( i ) the distance between the input and the output of the network $ \text{x}$ and ( ii ) the ratio of the distance of the input to the output $ \theta$, which is defined as the ratio between the difference of the squared distances of the inputs and the outputs of the two networks $ \tau$."
SP:0f62846913ec10b44ed32845770da0565479dc75,"This paper presents a theoretical and empirical study of the problem of visual relationship detection in the context of deep neural networks ( DNNs ). In particular, the paper focuses on the first order logic representation of the datapoints and the question of how well the learned DNN can be used for the task of digit classification. To this end, the authors propose two approaches : ( 1 ) augmenting the formal knowledge provided by the DNN with the knowledge from the source domain and ( 2 ) combining the learned knowledge from both the source and the source domains. The proposed approaches are evaluated on the digit classification task ( DASL ) and show promising results."
SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"This paper studies the convergence of gradient - based neural networks with different regularization constraints under the assumption that the network is not too different from the original neural network. To this end, the paper proposes a new regularization regularization constraint, regularization - regularization ( R - N ), and a new constraint on the number of parameters in the network. The paper also provides a proof - of - concept solution that shows the convergence rate of the proposed regularization with respect to the original network."
SP:6c14506b8b2b06043409d912e6bf877651aaa665,"This paper studies the problem of ensuring the robustness of data - normalization techniques under different types of data distributions. Specifically, this paper focuses on data distributions that are independent and identically distributed, i.e., data that is not subject to the presence of a distribution gap. The paper proposes two approaches to this problem. First, the paper proposes a data distribution that is independent of the source and target distributions, and the second approach is to use a different distribution for each data distribution.    The main contributions of this paper are as follows :   1 ) i ) i.i.discovering a new metric that measures the difference in the cross - entropy between two data distributions under different distributions. This metric is called the “ cross - default metric ”, and it can be used as a metric to measure the difference between the two distributions when the source distributions are different and the target distributions are identical. ii ) the paper shows that this metric is robust to data distributions with different distributions and different metrics, and iii ) it is also robust to different data distributions when there is a data gap between source distributions."
SP:2774abdc11917321dd4994af0f0da1ff824bea03,"This paper presents a series of experiments aimed at evaluating the impact of different types of data augmentation in the pre - training phase of neural network architectures for reinforcement learning ( RL ). In particular, the paper focuses on the question of the importance of the high - dimensional inputs used in the domain - specific information - value transfer ( i.e., in - domain and out - of - domain inputs ) in the context of visual - based RL. To this end, this paper presents two sets of experiments :   ( 1 )   a set of experiments conducted in the language - free domain ( e.g., wikipedia ), and ( 2 ) a set conducted in a domain - agnostic domain, where the goal is to evaluate the effect of different kinds of data augmentations on the agent's performance.   Experiments are conducted on the following domains : ( a ) domain - free domains, ( b ) domains where the input is in the form of a vector - based encoder - decoder, ( c ) domains in which the source - to - target vectors are in terms of dimensionality, ( d ) and ( e ) domains that are more general - purpose domains, such as web - based domains. Results show that the proposed approaches outperform the baselines on all of them."
SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"This paper proposes a new gradient - based learning algorithm based on a real - world dataset sharing framework. The proposed framework is based on the idea of parameter sharing networks. The paper provides a theoretical analysis on the convergence guarantees of the proposed framework. Moreover, the paper provides empirical results showing that the proposed network is more stable and converges faster than the baselines in terms of the number of iterations and the magnitude of gradients."
SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"This paper considers the problem of image - to - image cross - entropy ( MGC ) translation, where the goal is to ensure invariance of the geometry structure of the data to the translation process. The paper proposes two approaches to this problem. The first approach is based on the notion of mutual information, which is defined as the sum of the mutual information between two data points if they share the same metric, and the second approach is an adaptive version of the MGC approach proposed in [ 1 ]. In the first approach, the data points are assumed to be invariant to the metric, while in the second the data point is assumed to have invariance to the transformation of the metric.    The main contributions of this paper are the following :   1. Theorem 1.2. Mutual information between metric and translation is proved, which states that if the metric is invariant under the two approaches, then there exists a mapping $ \mathcal{x}$ such that $ \sqrt{x } \leq \log p(x)$, where $ \text{xi}$ is the metric that maps $ \gamma_t$ to $ z$, then the metric $ \cal p(z)$ can be expressed as a function of $ z$.   2. A proof of this result is given in [ 3 ]."
SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"This paper studies the problem of designing point cloud GANs that satisfy metrics that are sensitive to the sampling characteristics of the discriminator used to generate the point clouds. The paper proposes two approaches to this problem. The first is to design a baseline discriminator that is not uniform in terms of point sampling, and the second is to use a differentiable discriminator to sample points from different subsets of the same datapoints. The main contributions of the paper are as follows :    1. Demonstrating that both approaches are equivalent. The discriminator designed by the first approach can be used as a baseline for the second approach.   2. Introducing a metric metric that measures the distance between two points sampled from the two subsets, and show that it is sensitive to both the sampling patterns of the two discriminators. 3. Showing that the metric is robust to the choice of discriminator."
SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"This paper studies the problem of adversarial robustness of neural networks ( CNNs ) against a set of generic adversarial examples. The paper proposes two approaches to this problem : ( 1 ) to train CNNs with a fixed number of vote - attack examples, and ( 2 ) to learn a conditional capsule - based detection method based on the cross - entropy loss. The first approach is based on minimizing the cost incurred by each data point in the adversarial example trainings. The second approach uses the fact that the data points in the training examples are likely to be generated by the same source ( e.g., a malicious actor ).    The paper provides a thorough analysis of the two approaches and shows that both approaches fail to achieve the same level of robustness under the same set of examples. Moreover, the paper also shows that the proposed approach is more efficient than the previous approaches in terms of the number of attack examples."
SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"This paper studies the problem of policy - reinforcement learning of recurrent neural networks ( RNNs ). The authors propose two approaches to this problem : sampling - based and sampling - free. Sampling - based approaches are used to provide information about the current state - of - the - art policy, and learning - based approach is used to share the current policy with the future policy. Experiments show that both approaches are effective."
SP:bd89d254fbf31db61db237d08ab42981e27c52df,"This paper presents a policy - based model - based recommendation framework that learns from a dataset of real - world data collected in a variety of different environments. The policy is trained on top of a model - free policy that is conditioned on a set of data points collected from the real world dataset. The model is augmented with a human expert policy that has been trained in a simulator environment.    The paper presents two main contributions : ( 1 ) a policy learning framework that is designed to be able to tolerate a large variety of environments, and ( 2 ) a training strategy that is able to learn a policy that can tolerate a range of environments that is different from the one that is commonly used in practice."
SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"This paper presents a goal - reaching approach for supervised reinforcement learning ( RL ) algorithms. The goal is to train a policy that reaches a given goal in a supervised learning procedure that is conditioned on a set of demonstrations. The experiments are divided into two parts :    ( 1 )   a goal reaching approach where the goal is given a value function, and ( 2 ) a policy reaching approach in which the goal and the value function are conditioned on the number of demonstrations provided by the agent.   Experiments are carried out on the following benchmarks : ( a ) the following : ( b ) a toy example, ( c ) a real example where the agent has to decide whether to reach a goal or not, ( d ) an example showing that the goal can be reached in a few steps, ( e.g., if the agent knows the answer to the question “ ` why should I reach the goal? ” ) and ( f ) a series of demonstrations showing that agents can reach goals in a limited number of steps."
SP:c306530164d677e670554eeba8203c66bb3d9f7a,This paper studies the problem of speeding up the end - to - end training of teacher - teacher speech synthesis ( TTS ) systems. The paper focuses on the mapping and distillation aspects of the problem. The main contributions are :   1 )    a high - quality and fast training framework for TTS based on teacher - student distillation pipeline ; 2 ) a novel high - performance alignment tool for extracting information about the teacher model and the student - teacher model.
SP:79e9fb20d383816f54738ce70d137131ebc10290,"This paper studies the problem of minimizing the dimension reduction of the distribution of the probability density function $ \sqrt{x}$ over a k - dimensional subspace $ \mathcal{d}$ of dimension $ \log(n)$, where $ n$ is the dimension of the input space $ y$ and $ z$ the distance between the input and the output space $ z$. This is a well - studied and widely - studied problem in data science.    The paper proposes two approaches to solve this problem. The first approach is based on minimizing $ \nabla_{\log(z}$ where $ z}$ is a positive integer and $ \text{x } \log n\log n(z)$ a negative integer. The second approach uses a two - step iterative procedure. The authors provide a theoretical analysis of the two approaches and show that the first approach converges faster than the second one under certain assumptions. The theoretical analysis is complemented with extensive numerical experiments to demonstrate the effectiveness of the proposed approaches."
SP:93e54522e6c2b805905d21fc968fc40866f2898b,"This paper proposes a new feature sensitivity metric, FCLR, to measure the difference in the utility of different types of images under different scenarios. The key idea is to train a neural network on a set of datasets and measure the cross - entropy difference between the features of each dataset. The paper claims that this metric can be used as a metric to quantify the difference between different scenarios in terms of the amount of information that needs to be transmitted across the different datasets.    The paper also proposes a sensitivity metric that measures the ratio of the number of tokens required to convey the same information across the datasets. This metric is called the “ Contrastive Learning ( FCL ) ” metric."
SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"This paper studies the problem of learning an agent / robot representation / locomotive to solve a given set of data - driven tasks. The main contributions are two - fold :   ( 1 )    a theoretical analysis of the difference between the agent and the world - world representations of the data, and ( 2 ) an empirical study of the state - of - the - art learning methods used to achieve this goal.   The theoretical analysis is based on the assumption that the agent has access to a representation space that is invariant to both the source and target domains, and that the knowledge about the source domain is available to the agent only. The empirical study is conducted on two types of data sets, i.e., static and dynamic data sets. The theoretical results are shown to be in good agreement with the empirical results for static data sets ( static data ), but not for dynamic data ( dynamic data )."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the problem of generalization of the winning ticket hypothesis ( LTH ), which states that if $ \mathcal{T}$ is a set of $ \ell_t$ points $ \infty$ and $ \alpha$ is the set of points $ t$ in the $ \text{LTH}$ game, then there exists a $ \tilde{t}$ such that $ \sum_{t } \log(t\infty)$ is greater than $ \sqrt{\text{T}}$.    The paper shows that the LTH problem is equivalent to the following :   1.   There exists a set $ T$ $ \nabla_{t}$. 2. $ \log{T } \leq \sqrho_{t}\infty$. 3. There exist $ \sigma_t\log{t}\log{L}$. 4. There exists $ \delta_t \log { t } \text { T}\log { T } \to -\sqrt{T}\log t$."
SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"This paper presents a data augmentation - guided approach to improve the performance of data - augmented neural networks ( DNNs ). The proposed approach is based on the idea of preserving the similarity between the data and the underlying model. To this end, the authors propose two approaches : ( 1 ) data smoothing and ( 2 ) data augmentmentation approaches. The first approach is a data - augmentation approach that first generates clean data and then augments the data with the data produced by the second approach. The authors show that the proposed approach achieves better performance than the data - free approach."
SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"This paper studies the problem of representation learning in the context of supervised learning, where the goal is to learn a representation of a set of data points from a supervised dataset. The authors propose two approaches to this problem : ( 1 ) Representation Learning, which aims to learn representations of a subset of the data points that are likely to be useful to the supervised learner, and ( 2 ) Sec. 3, which is supervised learning with a proxy classification task that aims to predict a proxy classifier that would help the learner to better understand the distribution of the target data points.    The main contributions of this paper are as follows :   1 ) A theoretical analysis of the two approaches is provided, which shows that the proxy classification problem is equivalent to the representation learning problem, and that both approaches are equivalent to each other in the sense that they do n’t require any additional knowledge about the target distribution. 2 ) A reinforcement learning experiment is conducted to evaluate the adequateness of both approaches, and it is shown that the supervised learning approach performs better than the proxy learning approach in terms of the number of proxy classifiers used and the amount of supervised data used to train them."
SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"This paper presents a visual representation learning method for policy navigation in artificial environments ( AI2 - Thor ). The proposed approach is based on the visual transformer network ( VTNet ), which has been used in previous work on object classification and policy navigation. The main contribution of this paper is the introduction of global descriptor - aware visual representations of objects and regions, which can be helpful for policy learning."
SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,"This paper aims to provide federated learning systems with better data - privacy and data - sharing guarantees. To this end, the paper proposes two approaches : ( 1 ) federated graph - data sharing, ( 2 ) data - data privacy - preserving federated data sharing. The first approach is based on aggregating data from different federated networks and preserving the privacy of the aggregated data. The second approach, based on federated graphs and sharing data among different networks, aims to ensure the data privacy of each federated network."
SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,This paper studies the problem of designing an optimal two - player game that maximizes the utility function between two players playing the same game. The goal is to design a game that is compatible with the current state - of - the - art maximization loop. The paper proposes two approaches to this problem. The first approach is to search for the optimal value function between the two players by maximizing the sum of the utility functions between them. The second approaches is to maximize the sum between the players by minimizing the sum over all games played by the players in the first approach.   The paper provides theoretical analysis of both approaches and experiments to show that both approaches converge to the same value function when the number of players is sufficiently large.
SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"This paper studies the problem of fine - tuning the representations learned from pre - trained and fine - tuned representations. The authors propose a cross - entropy loss that quantifies the amount of information encoded in the representations that can be used to improve the performance of the model. To this end, the authors propose two approaches : ( 1 ) a data - free approach where the representations are not subject to label information, and ( 2 ) a knowledge - based approach where labels can be extracted from the representations. Experiments are conducted on both supervised and unsupervised pre - training tasks. The results show that the learned representations perform better than the un - trained representations under both approaches."
SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper presents a new adversarial robustness framework based on norm - based adversarial perturbation norm based distance metric. The proposed framework has two main contributions : ( 1 ) to train a classifier that is robust to small perturbations, and ( 2 ) to provide a metric that measures the distance between the perturbed examples and the labeled examples. The framework is based on the adversarial training framework proposed in [ 1 ] and [ 2 ]."
SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"This paper studies the problem of ensuring the adjacency matrix fairness of link prediction in the context of data - driven neural networks. Specifically, the paper considers the following problem : given a set of datapoints $ \mathcal{x}$ and a set $ y$ of metrics $ \theta$, what is the best way to ensure that the resulting metric $ \sqrt{x } \log p(y|x)$ is not too different from the one $ \cal p(z)$ that was used in [ 1 ]? This question is phrased as follows : given $ \text{text{data}$, $ y}$ is the set of metrics that can be used to predict $ z$, and $ \tau$ is a metric that measures the difference between the metrics $ z$. The goal is to make sure that $ z \log n$ is at least as close to $ z as possible under the following assumptions :   1.    * * For each $ z * *, there exists a data point $ \infty$ such that the following metrics $ y^t$ are not more than $ z^t $ ( i.e., for all $ z > z$ and $ z = n/1 $, the metric \log(y^t)$ has at most $ z_t$ error in terms of the metric $ y$.   2. * * * for each $ \alpha$, there is a data set $ z $ such that $ \log z$ ( i ) is at most likely to be at least $ z^{-1/\log p(\tau)$, or equivalently, $ z_{t } \leq n/2}$ ( ii ) is such that for every $ z>n$ there is some $ z<n/2 $ such as $ z\log n/3 $, where $ z < n/4 $ is the metric that minimizes the metric's impact on the data set. Theorem   3. * 4. * For any $ \lambda$, the following metric is guaranteed : $ \sum_{tau}_tau \log ( y|x})$ is bounded from above by a factor $ \frac{\sqrt{\log n}{\tau})$ where $ n(z^t+\log z}(y)\log n(x ) is a matrix that satisfies the above above condition."
SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"This paper proposes a generative autoencoder ( DEAE ) that is controllable in space and can be used to solve classification tasks in latent space.   The main contributions are as follows :   1. Introducing the DEAE framework, which has been shown to be generative and can solve classification problems in space ; 2. Demonstrating the generative ability of DEAE by showing that it is able to solve a classification problem in space."
SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"This paper proposes a new attention - based attention based generative model framework for conditional image generation. The attention is based on the notion of a latent space $ \ell_0 $, where $ z_0$ is a low - dimensional key distribution and $ x_1 $ is a sequence of $ z$ - dimensional latents.   The authors propose to allocate $ z_{\ell_1 } \infty$ to each $ y_0$.   They show that this is not enough to guarantee the generative stability of the model, and propose to use a differentiable attention based approach to this problem. The authors also show that the proposed attention can be used to improve generative models with differentiable indexing and semantic representation."
SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,"This paper analyzes the relationship between the complexity of attention - based neural networks and the amount of data required to train them. Specifically, the paper shows that given a fixed number of data points $ \mathcal{x}$ and a fixed amount of attention $ \nabla$, the number of instances required for a given attention model to learn a set of metrics $ \tilde{x } \infty$ is upper bounded by a constant factor $ \sqrt{\log n}$ where $ \log n$ is the total number of tokens needed to train the attention model and $ \text{text}$ is a metric used to measure the quality of the model.    The paper further shows that for a fixed set of $ \sigma^2 $, there exists a set $ nabla^3 $ such that $ \theta^4 $ is sufficient to train attention models with a fixed $ \ell_tilde$ under the following assumptions :   ( 1 ) $ \alpha^5 $ is not too differentiable, ( 2 ) $ n\tilde^6 $ is too large for attention models to learn well, and ( 3 ) there is no data point $ n \log N$ such that no attention model can learn well under the above assumptions."
SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"This paper studies the problem of learning a model - free or model - based value function from a set of data points given a probabilistic model of the data points. The paper proposes two approaches to this problem. The first approach is based on the notion of free energy, which is defined as the difference between the expected value of a given data point given the given model and the value that can be obtained from the data point if the model satisfies certain conditions. The second approach is a preference learning approach, where the goal is to learn a value function that maximizes a priori the probability that the model is a good fit to the data.    The paper proves that both approaches are equivalent to each other in the sense that the free energy of the two approaches can be expressed as a sum of two terms : ( 1 ) the prior value of the model and ( 2 ) the preference between the two models. The proof relies on the fact that the value function is a linear combination of the prior and the preference function, and that the two terms are independent of each other if the prior is not too large ( e.g., $ \ell_1 $ )."
SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"This paper studies the problem of data augmentation for standard and adversarial data - augmentation in data - driven adversarial networks. The paper proposes a new data - augmented dataset augmentation method for standard adversarial network training.    The main contributions of this paper are as follows :   1 ) a new dataset for data - based adversarial training, which is designed to be more data - efficient and data - robust. 2 ) A new dataset to be used for standard data - augmenting attacks. 3 ) An analysis of the data - related adversarial attacks."
SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,This paper presents a meta - reinforcement learning ( meta - RL ) framework with a linearized version of the policy gradient descent ( FLAP ) approach. The authors provide a set of experiments on the following metaRL problems :    1. Differentiation of the distribution of the tasks in the metaRL framework.   2. Differentiations of the cross - entropy between the different distributions in the framework. 3. Different choices of the run - time speeds for the gradient descent approach.
SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,This paper studies the problem of providing a scalable proximal gradient descent algorithm for kernel k - means with guaranteed data - level privacy / entropy adequacy.    The authors propose two approaches to this problem. The first approach is based on the idea of distilling stochastic gradient descent into a deterministic gradient descent approach. The second approach uses a differentiable gradient descent method. The authors prove that both approaches converge to the same solution under certain assumptions.
SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"This paper studies the problem of training a model to solve a set of resource - intensive tasks with a fixed number of components. To this end, the paper proposes two approaches : ( 1 ) federated training, where each component has access to a different set of resources ( e.g., image, audio, text ), and ( 2 ) shared learning, where a subset of the components is shared across all the components and the remaining components are trained separately. The paper provides theoretical results showing that both approaches converge to the same optimal solution in terms of the total number of iterations and the total amount of data used to train the components, as well as the ratio of the number of training iterations required to train each component.   The paper also provides empirical results on the following metrics :    \theta_trouvelessness : \infty_total iterations required for each component to complete a given task, \eta_perturbations required for the components to converge to a fixed solution, and \eta _ latency _ : the average number of steps required to learn a component to solve the given task."
SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"This paper proposes a novel way to generate adversarial samples for meta - learning tasks in an adversarial manner. The proposed approach is based on the idea of “ ADversarial meta - learner ”, which proposes to initialize a learning model in a manner that is “ safe ” from corrupted data and “ adversarial ” in terms of the quality of the samples provided by the learning model. The paper also proposes a way to “ wrestle ” the initialization of the model to ensure that it ’s not contaminated by corrupted data. Experiments are conducted on a variety of datasets to demonstrate the effectiveness of the proposed approach."
SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"This paper presents a data - driven framework for improving the error rate of linear error correction of Bose - Chaudhuri Hocquenghem ( BCH ) decoders. The framework is based on the attention mechanism proposed in [ 1 ] and [ 2 ]. The attention mechanism consists of two steps. First, the attention network is constructed by embedding a factor graph variable node into a set of vector embeddings and then applying attention to each vector embedding in the set of vectors. Next, attention is used to select vectors to be used in the next layer of the decoder in order to improve the error - rate of the linear error - correction decoder."
SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"This paper proposes a text classification task based on clustering and fine - tuning. The goal is to provide examples for the text classification tasks. The proposed approach is based on the similarity between two sets of data, i.e., set of examples and set of clustering examples.    The first part of the approach is to fine - tune the two sets. The second part is to label the examples."
SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"This paper studies the problem of training a controllable unsupervised model - based reinforcement learning system to predict the future outcomes of a set of actions. To this end, the paper proposes two approaches : ( 1 ) train the model on a distribution of $ \mathcal{x}$ actions, and ( 2 ) train a control agent on the top - k actions of the model.    The first approach trains the control agent in two stages, i.e., the first stage trains the model to predict $ \cal_{\text{x } \infty$, and the second stage trains it to predict a $ \text{y}$ that is close to the mean $ \log{x}\infty$.   To evaluate the performance of the two approaches, the authors perform a series of experiments :   1 ) they compare the trajectories of the trained model with the predictions of a random control agent, and 2 ) they evaluate the performances of the learned model against the counterparts of random control agents trained on the same trajectories. The results show that both approaches perform comparably to each other."
SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper studies transformation properties of GAN ( ADIS - GAN ) representations. Specifically, this paper focuses on GAN representations that are invariant to both spatial and vertical regularizers. The main contributions are :   1.   A theoretical analysis of the GAN representation properties. The authors show that any GAN regularizer can be expressed as a convex combination of a vector regularizer and a vector transformation regularizer, and show that the vector transformation is invariant under both of them. 2. A practical method to estimate the likelihood of the transformation is given. 3. A theoretical result is given showing that the distance between the vectors under the two regularizers is at most $ \ell_1$."
SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"This paper proposes a contrastive learning framework for improving the performance of supervised and self - supervised object detection tasks. The proposed framework is based on a similarity - based learning approach, where the goal is to learn from the contrastive similarity between a set of objects and a subset of objects. To achieve this goal, the authors propose two approaches : ( 1 ) to learn the similarity between two sets of objects, and ( 2 ) to contrastively learn the object detection task from the similarity of the two sets.    Experiments are conducted on the following tasks :   1. Self - supervised detection tasks : fine - tuning a single supervised classifier to achieve top - k object detection accuracies ; 2. supervised learning tasks : learning a supervised method to detect two objects ; and 3. supervised methods to classify two objects. Results show that the proposed framework outperforms the state of the art."
SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"This paper presents a learning framework for the identification of medical imagery from MRI scans. The framework is based on the GAN - GAN framework developed by the authors in [ 1 ] and [ 2 ]. The key idea is to generate volumes of MRI scans for both clinical and research purposes, and to use the collected data to train a teacher - student model that can be used in both clinical settings and in research settings.    The key contributions of this paper are as follows :   1. Providing a framework that provides both privacy - preserving and data - sensitive information about the images generated from a given MRI scan. This is done by using a pre - trained teacher model that is trained on a set of scans generated by the author's MRI scanner. 2. Conducting a series of experiments to demonstrate the effectiveness of the proposed framework."
SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"This paper presents a set of experiments aimed at testing the invariance of graph isomorphism under different graph pooling approaches. In particular, the paper focuses on the problem of global pooling and global clustering approaches for graph classification and link prediction tasks. To this end, the authors propose a multi - head attention - based pooling method and a drop - and - match clustering method. Experiments are conducted on four tasks : ( 1 ) reconstructing the original graph, ( 2 ) learning the link between two nodes, ( 3 ) link generation and ( 4 ) cross - entropy pooling. Results show that the proposed pooling methods perform better than the baselines on all four tasks."
SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"This paper presents a proof - of - concept proof that a graph neural network ( GNN ) with fixed - length and fixed - size vectors can solve a given set of long - range problems with a fixed number of iterations, provided that the network is trained on a set of data points that are not too far away from each other. The proof uses the following assumptions : ( 1 ) the length of the input vector is less than some critical length, ( 2 ) the size of the output vector does not exceed some critical size, and ( 3 ) the number of steps required for the network to solve the given problem is bounded by a constant factor that depends on the length and size of input vectors.    The proof relies on three assumptions. The first assumption is that the output of the network does not contain any information about the long range nature of the problem. The second assumption assumes that the input vectors do not contain long range information, and the third assumption states that the vector size does not increase linearly with the length. The authors prove that the proposed proof is provably correct under the first assumption, but not in the second assumption."
SP:90d8fa381446923902e42b259392e5e975e6caa1,"This paper presents a sentiment analysis method for cross - domain generalizable classifiers. The method is based on the sentiment analysis approach proposed in [ 1 ] and [ 2 ].    The key contribution of this paper is to propose a new method for sentiment analysis based on data annotation and space embedding. The key idea of the method is to identify the space embeddings that are most likely to capture the sentiment of the target domain. This is done by performing sentiment analysis on a set of domain - agnostic classifiers ( e.g., wikipedia, cifar, etc. )."
SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"This paper proposes to test the gender - specific hypothesis of natural language understanding under the assumption that natural language systems are gender - neutral. To this end, the paper proposes a data - balanced dataset that is constructed from a set of data - sets that is balanced in terms of the number of words / phrases / nouns / verbs / phrases per language, as well as the percentage of gender - non - gendered words / verbs in the dataset.    The paper also proposes a new data - shaping approach to train the data - set, which is based on a combination of two approaches : ( 1 ) deforming the data sets according to the assumed gender - notation, and ( 2 ) using the learned deforming techniques from the previous work ( TaBERTaBerta et al., 2017 ). The paper shows that the proposed data - training approach can be applied to a variety of data sets, and the results show that it is able to bridge the gap between the two approaches."
SP:a32ab755bd249c393b70938036ce8e810c0c439f,"This paper proposes a VIC algorithm that leverages the stochasticity of the environment to improve the performance of a reinforcement learning algorithm. The algorithm is built on top of a mixture model, where the mixture model is a mixture of two components : a mixture distribution and a dynamics model. The dynamics model is assumed to be continuous and the mixture distribution is a Gaussian mixture of the two components. The goal is to find a policy that maximizes the ratio between the expected reward of the mixture and the value of the dynamics model, while also ensuring that the algorithm does not overfit to any particular environment.    The paper evaluates the proposed algorithm on a variety of environments, where it is shown that VIC outperforms a number of existing algorithms that do not take into account the environment."
SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,"This paper studies the problem of classification of relatively small datasets. The authors propose two approaches to this problem : ( 1 ) to sample small datasets and ( 2 ) to train a deep neural network on top of it. The first approach is based on minimizing the number of data points in the dataset ( i.e., minimizing the entropy entropy per data point ). The second approach uses a few data points per domain, and trains a neural network to classify each domain separately.    The authors show that the proposed approaches are able to solve classification problems under certain conditions ( e.g., small dataset size )."
SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,This paper studies the problem of training neural networks with limited compression factor for ternary and binary neural networks ( BNNs ). The main contributions are :    1. Improve the compression factor of linear and convolutional neural networks for the first time.   2. Develop a model and training scheme that can be used to efficiently compress data sets for both linear and convex neural networks.
SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"This paper proposes a calibration method to estimate the uncertainty in the predictions of a variational variational bayesian neural network ( VBI ) trained on a set of image classifiers. The calibration is based on the following assumptions : ( 1 ) the distribution of the classifiers is known, ( 2 ) the uncertainty of the model predictions is small, ( 3 ) the calibration error is low, and ( 4 ) the classifier is robust to perturbations in the data distribution.    The paper evaluates the proposed calibration method by computing the Brier score of the calibrated model, and comparing it to the calibration of the unperturbed model."
SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"This paper presents a theoretical analysis of the properties of non - isomorphic graph kernels and similarities. The authors show that there exist kernels that are not isomorphic to each other and that the distance between two such kernels is at most of the form $ \sqrt{x}$, where $ \tilde{x } \in \mathbb{R}$ is the number of vertices in the input graph and $ \text{text}$ denotes the similarity of the two kernels.    The authors then show that these kernels are not universal and that there are kernels that don't satisfy certain properties such as power and distance."
SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"This paper studies the problem of preserving the similarity between two video representations of the same video. To this end, the authors propose two approaches : ( 1 ) image - based and ( 2 ) self - supervised.    The first approach is based on the following :   1 )   First, the text is replaced with a text - only embedding of the driving video ; 2 ) the video is decoded using a non - warped version of the text. The authors show that both approaches fail to preserve the similarity of the video representations. To overcome this problem, they use data augmentation techniques and show that the two approaches are close in terms of the quality of the decoder ( i.e., the similarity is better than the difference between the embedding and the text )."
SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"This paper studies the problem of generating representations that satisfy the following conditions :    1.   a causal causal mechanisms ( ICM ) that are generative in nature ;   b.   shared causal mechanisms that are both generative and supervised ; and   c. data that is generated by a supervised mechanism that is conditioned on the nature of the source and target data ; and 2. a data generation process that is data - free and data - agnostic, that is, that does n’t require any prior knowledge about the source or target data."
SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"This paper proposes an approach to adapt a generic model to a new data set of graph structure representation ( scene representation ) of the world wide web. The model consists of two components : ( 1 ) a generative model and ( 2 ) a set of $ \mathcal{X}$-normally connected graph representation $ \text{x}$ -normally - connected parts $ \tilde{x } $ of the data set $ x$, $ y$ $ n$ and $ z$ of the scene representation $ z$. The generative models are trained on the following steps :    1.   * Concretely, this paper shows that a generic graph representation can be obtained by adding $ \log(x, y ) = \sum_{x}^{-1 } \log{x}\log(z}$, where $ \infty$ is the size of the graph and $ \nabla_{x } is the dimension of the datapoint $ y$.   2. * This paper also shows that the generative approach can be extended to a more complex data set by adding a $ n\log(y, z ) < nabla_0 $ -normality layer $ \gamma_1 $ $ ( $ \sqrt{x})$ and a $ \frac{x}{\text{y}\log{z } -norm(x\log{xi}$ $, $ \theta_1$. 3. This paper further shows that adding $ n(\text{xi } + $ z}$ can be combined with a $ p(x)$ - normality layer to obtain a more complete representation of the original data set."
SP:ad906dd9a176cffd283593321ff6b9ad19595528,"This paper studies the problem of energy - based deep learning ( ESSL ), where the goal is to obtain knowledge about the physical systems in the domain without having access to all the details of the underlying physical systems. To this end, the authors propose two approaches : ( 1 ) a linear model and ( 2 ) a nonlinear model. In the linear model approach, the input - output monotonic problem is reformulated as a convex optimization problem, and the output - output problem is formulated as a finite - difference optimization problem. The authors show that both approaches converge to the same solution when the number of samples is large enough. On the other hand, the non - linear approach is shown to converge to a solution that is more scalable than the linear approach."
SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"This paper proposes a causal causal spatio - temporal fusion transformer framework for traffic forecasting. The proposed framework is based on two components :   ( 1 ) causal effects of predictors and ( 2 ) causal factors of variables.   The first component    is a generative model that   learns to predict a set of variables $ z$ and $ t$ from a data set $ t$. The second component is a neural network that learns to   predict $ z$.   This paper   first   provides a theoretical analysis of the proposed framework. Then, the authors   show that the proposed   framework is able to reduce the time complexity of the cross - domain attention and head - attention in terms of the number of variables and the size of the datasets. The authors also show that their   approach is   more   data - efficient than the baselines proposed in the literature."
SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"This paper proposes a generative autoencoding framework, cpl - mixVAE, that learns to jointly model discrete and continuous factors of variability and activity in a multi - agent framework. The framework is built on top of a dataset of single - cell and multi - cell gene expression datasets. Each cell contains a set of discrete factors ( e.g., activity - regulated genes ), and each agent takes as input a subset of these discrete factors and outputs an action vector, which is then concatenated with the discrete factors. The continuous factors are then aggregated to form a single vector, and the agent is trained to learn the action vector.    The main contributions of the paper are as follows :   1. A generative agent framework that jointly models discrete factors of both continuous factors and discrete factors ; 2. An agent that jointly learns both discrete factor and continuous factor ; 3. A decision making framework that learns both continuous factor and discrete factor in a single agent."
SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"This paper gives a new proof of the "" steerability constraint "" on tensor operators in terms of kernel spaces. More precisely, it shows that any group $ G$ is steerable if and only if there exists a kernel space $ R$ such that $ \mathbb{R}^n$ is isotropic in the sense that for any $ \ell_1 $, there exists an action $ \infty$ on $ R$.    This is done by showing that the kernel space of a $ r$-symmetric group $ O(\ell_2)$ is isomorphic to the space of functions defined over $ r$. Moreover, it is shown that there exist kernel spaces $ R(\ell_3 ) $ such that any function defined on $ r $ can be expressed as a linear combination of a kernel $ \alpha$ and a tensor operator $ \beta$."
SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"This paper presents an empirical study of the distributional and coverage accuracies of classification and coverage for a set of models trained on a combination of vision and natural language processing ( NLP ) datasets.    The paper shows that the cross - entropy distributions of the two datasets ( vision and NLP datasets ) are statistically significant with respect to each other and to the number of classes used in the classification process. The paper also shows that there is a correlation between the coverage and classification accuracies across the datasets, and that the coverage - based classification is more robust than the classification - based one."
SP:f1d57ee27e901daf7e4e2b84139019e945818911,"This paper presents a multi - layer neural network for topic classification and topic modeling. The proposed approach is based on the hierarchical structure of document classification data, which is used to classify documents according to their hierarchical structure. To train the neural network, the paper first generates a set of tensor topic classification documents, which are then used to train the topic classification model. Experiments on both real datasets and synthetic datasets show that the proposed approach performs better than the existing approaches."
SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,This paper proposes a new approach to obtain robustness certificates to classify and classify datasets. The proposed approach is based on the idea of segmentation - based classification and entity recognition. The key idea is to first obtain a set of single - node classification certificates and then to obtain a collection of such certificates for each dataset. Each certificate is then used to classify a subset of the data points. Each dataset is further decomposed into sub - datasets and a classification procedure is applied to obtain the classification procedures for each sub - dataset.    The main contributions of this paper are as follows :   1. This paper proposes to obtain classification procedures that are robust to adversarial attacks and that are scalable to different types of datasets. 2. The paper also proposes a method to obtain entity recognition procedures that is scalable to various types of data.
SP:cc93dd2f68e415e2457166e78627865dc1b44697,"This paper studies the problem of GANs ( Generative Adversarial Networks ), which are models for generating high - dimensional probability distributions that are susceptible to catastrophic collapse or vanishing. The paper proposes a theoretical analysis of this problem, which is based on the distance approximation of the loss function of the generated data distribution. The theoretical analysis is complemented with numerical experiments to verify the theoretical results. Experiments are conducted on simulated data, real data, and generated neural networks."
SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,"This paper presents a collection of similarity metrics to measure the similarity between two explanations of the same class or subclass. The metrics are based on ( i ) similarity of class and subclass explanations, ( ii ) similarity in similarity of gradients, and ( iii ) similarity between subclass explanation and randomization test.    The paper proposes three types of explanation metrics : similarity - based explanation metrics, similar - instance similarity metrics, and identical - subclass similarity metrics."
SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"This paper proposes a two - layer neural network ( MLP ) framework for self - attention and self - supervision. The first layer consists of a global attention module and a feature map kernel. The second layer is composed of two layers : a global feature map and a local feature attention module. The authors conduct a series of experiments to evaluate the quality of the two layers. The experiments are divided into two parts :    1. The global self attention module is used to validate the correctness of the global attention, and 2. The feature map is used for verifying the quality and scalability of the local feature map."
SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"This paper proposes a new label smoothing approach to improve the quality of the cross - domain classification dataset for neural networks ( CNNs ). The main contribution of the paper is to provide a theoretical justification of the proposed approach and to provide empirical results showing that the proposed model is more data - efficient than previous smoothing approaches ( e.g., smoothing factor and smoothing smoothing localization ). Moreover, the paper also provides a theoretical analysis of the data - efficiency of the model and shows that the model does not suffer from overfitting and underfitting."
SP:5254658923e594294b69d124a8d004166852822a,"This paper proposes a two - layer neural network - based duality framework to solve the duality problem, where the goal is to find a network that minimizes the difference between the sparsity of the input and the output under a linear filtering scheme.    The main contributions of the paper are as follows :   1. This paper provides a theoretical analysis of the problem of duality and shows that it is equivalent to the dual problem of training and testing two networks with different sparsity settings. 2. The paper shows that the two networks are equivalent to each other in terms of sparsity and convergence rate. 3. It proposes a training and prediction framework that is based on two networks, where one network is a two layer CNN network and the other is a neural network."
SP:085cad6bc143c8713580bddfaa71f06496dac314,"This paper proposes a dynamic time warping - based text - to - speech synthesis pipeline, which is built on top of GPT-2. The model consists of two components, i.e., a text generator and a speech synthesizer. The generator is trained in two stages : pre - training and post - training. The first stage trains the generator to predict the length of the text and the phoneme input sequences, while the second stage pre - trains the synthesizer to predict both the length and the duration of the input sequences.    Experiments are conducted to evaluate the quality of the model."
SP:01148cea55db606aa78d27e900818684a8bce9ab,"This paper presents a theoretical analysis of the problem of learning the topological structure of the world graph from data. Specifically, the paper focuses on the following : given a set of data points $ \mathbb{R}$, the goal is to learn the topology of the space $ \text{topology}$ over which the data points can be represented. The paper proposes two approaches to this problem : ( 1 ) complete and ( 2 ) complete - to - completion.    The first approach is based on the notion of completion completion, which is defined as the number of instances of a given data point in the space that need to be completed in order for the data point to be correctly represented in the second approach. In the first approach, each data point is assumed to have a completion probability greater than $ \sqrt{1/2}$. The second approach, on the other hand, uses completion probability as a measure of the probability that a data point needs to be complete in order to be properly represented. In this paper, the authors prove that both approaches are equivalent to each other in the sense that complete and complete data points are equivalent if and only if they have the same completion probability."
SP:aeeb5909f7123ef631f569b469af9715205c881f,"This paper proposes a goal - conditioned framework for reinforcement learning ( RL ) in which the goal is to generate a set of extrinsic goals that motivates the learner to solve a given set of tasks in an environment that is adversarial in nature.    The paper proposes two approaches to this problem. The first is a policy - conditioned goal conditioning framework, where the policy is conditioned on the set of goals that can be generated by the teacher. The second is a reward - conditioned curriculum that encourages the student to solve the given goals in a given environment that has been designed to be difficult to solve. The paper also proposes a way to reward the student for solving the given goal in a differentially designed framework."
SP:3d05bc7dca97681cb582298e318b9b973841eed3,"This paper proposes a data - driven privacy - preserving minimax game, where the goal is to minimize the distortion - leakage tradeoff between the source - to - target ( source ) and the target ( data ) leakage rates. The proposed framework is based on the MNIST dataset and CIFAR10 dataset. The main contributions are :    1 )   a theoretical formulation of the privacy - vs - distortion tradeoff in terms of the adversarial model of the source and target, and   2 ) a proof - of - concept algorithm to achieve this goal."
SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"This paper proposes a greedy learning strategy for GNNs based on DGL - GNN and GNN - based approaches. The proposed approach is based on two components. The first component is a GNN layer that learns a greedy GNN model, and the second one is a DGL layer that trains the GNN on top of the greedy model. The authors provide empirical results showing that the proposed greedy approach achieves better performance than the standard GNN based approach.    The authors also propose a few auxiliary objectives to further improve the performance of the proposed approach. The auxiliary objectives are : ( 1 ) train the model on a set of small scale applications, ( 2 ) add a few graph - related tasks to the training set, and ( 3 ) update the model based on the proposed auxiliary objectives."
SP:5ecb1b288f7fc02aead4493f81640867bc349290,"This paper presents a theoretical and empirical study of the problem of learning the knowledge graph of the first - order logical properties of graphs under the hypothesis that the world is made of graphs of size $ \mathcal{x}$ and $ \cal{y}$, where $ y}$ is the size of the set of graphs and $ z$ the number of elements in the set $ y$.    The paper proposes a theoretical framework for this problem, which is based on the well - known Knowledge Graphs ( KG ) model. The paper also proposes a combinatorial search method to find the optimal solution to the problem. Experiments show that the proposed method is able to answer a large variety of differentiable queries, and is competitive with the state - of - the - art."
SP:f04a522fd04c503754fdb8c52da68646d31271a4,"This paper presents a theoretical analysis of the robustness of neural networks under the linear activation function constraint. The main contributions are two - fold :   ( 1 )   a theoretical proof of the existence of the 2 - norm and ( 2 ) a practical implementation of 2 - robustness for neural networks. The theoretical analysis is based on the fact that the activation function of a neural network can be expressed as a linear combination of two functions : $ \mathcal{x}$ and $ \cal { y}$, where $ y$ is the number of tokens in the input, $ \theta$ the cardinality of the output, and $ z$ the distance between the input and the output of the neural network.   The experiments are carried out on standard neural networks ( e.g., Cifar-10, RNN-10 ) and neural networks with different levels of robustness."
SP:5297651ff873f97c07b9c47ed3eff52251661844,"This paper presents a theoretical analysis of the relation between the $ \ell_p$-\theta$-$ \theta $ \infty$ $ \text{pPM}$ and $ pPM}-\tilde$ $ p>\infty$. Specifically, the paper shows that $ p > \ell_{pP}$ corresponds to $ \tilde{PPM}/\text{text{delta}$, and $ P>\ell_{delta } $ corresponds to a $ \sqrt{p } \times delta$ \times pPM^2 $, where $ p$ is the number of elements in $ PPM$, $ d$ is a dimension in $ Delta$ $ and $ \delta$ is an integer.    The paper also shows that there exist $ \mathbb{R}^n$-p$$ \sigma$-P$ that corresponds to the equivalence between $ \alpha$-delta $ \nabla$ $ delta$.  "
SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"This paper studies the problem of cooperation in multi - agent reinforcement learning, where each agent is assumed to know the other agent's goals and actions, and the goal is to maximize their own individual and group - level achievements in a cooperative setting. In particular, the paper considers the following scenarios : ( 1 ) the agent has access to a limited amount of information about the others'goals, ( 2 ) the agents are assumed to be experts in their own environments, and ( 3 ) the goal of the agent is to minimize the total number of interactions with the other agents.    The paper proposes two algorithms for this problem. The first one is based on regularizer - based reinforcement learning ( RRT ) and aims to maximize the individual achievements of each agent. The second one is a reinforcement learning algorithm based on classifier - based learning."
SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"This paper proposes a new approach to certified robustness based on smoothing - based classifiers. The proposed approach is based on two assumptions :   ( 1 ) the smoothing of the classifiers should be small and ( 2 ) the adversarial attacks should not exceed a certain threshold.    The main contributions of the paper are as follows : ( a ) the proposed approach has two main contributions : ( b ) it is the first to propose a smoothing based robustness approach, and ( c ) it provides a cost - effective way to train the smoothed classifier."
SP:ea892e3d199ed6121279b20061a87f43afae8796,This paper presents a series of experiments aimed at providing a better understanding of the cross - entropy between different types of boundary conditions under the context of the multi - task hierarchy. The experiments cover the following categories :    1.   * * Sec : * * Prototypical and unsupervised boundary conditionals   2. * * Experiments on different categories of data   3. *   Experiments in supervised and weakly supervised settings.
SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"This paper studies the invariance principle under the assumption that the representation learned by a generative model is invariant to domain - specific variation factors. The paper provides a theoretical analysis of this invariance under the following assumptions :    ( 1 ) factorization error is bounded by a factor $ \sqrt{\theta}$, where $ \theta \in \mathbb{R}$ is the dimensionality of the domain, and $ \infty$ is a factor that measures the difference between the representations learned by the model and the ones learned by human - imperceptible features ( e.g., the similarity between the features learned by humans and those learned by an expert ). ( 2 )   factorization factor is not bounded by any factor other than the dimension of the data set.   The paper proposes a learning method based on the following two assumptions. First, the representation learning is assumed to be invariant under the factorization assumption, and the second assumption is that the data sets are not too different from each other. In the experiments, the paper shows that the proposed learning method is able to learn representations with invariant factorization under both assumptions."
SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"This paper studies the problem of online learning with contextual bandits, where the goal is to learn a policy that is robust to the actions of a multi - armed adversary. The paper proposes two algorithms, UCRL2 and UCRL3. The first one is based on the well - known UCRL algorithm, while the second one is an online version of UCRL.    The main contributions of the paper are as follows :   1. Introducing the concept of contextual bandits and showing that they can be classified into two classes : linear and non - linear contextual bandits. The former class is characterized by a distribution of rewards and transitions, whereas the latter is defined by a randomness in the contextual vectors. 2. Providing theoretical guarantees on the robustness of the proposed algorithms."
SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"This paper proposes a new attention mechanism to improve the quality of vector - based machine translation ( e.g., NMT ). The attention mechanism is based on the attention mechanism proposed in [ 1 ], which is a vector concatenation - based approach to generate a vector - wise vector representation of the target language, which can then be used as the input for the decoder.    This paper also proposes a gradient descent - based attention mechanism, which tries to align the vector representations of the source language with the target model's vector representations."
SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,This paper proposes a black box semantic segmentation framework for real - world data. The proposed framework is built on top of the LIDC dataset. The segmentation model consists of two components : ( 1 ) a stochastic mapping model and ( 2 ) a predictive model. The first component is designed to be able to predict the distribution of labels. The second component is trained to be robust to adversarial perturbations.
SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,This paper provides the first communication complexity guarantees for compressors for large - scale machine learning applications. The main contributions are as follows :   1. This paper provides a theoretical analysis of the complexity of compressors in terms of the number of bits per channel and the amount of data used to compress them.   2. The paper proposes a new metric that quantifies the ratio of the total amount of bits used for a given channel to the total number of bytes used for the channel to compress. 3. It establishes a lower bound on the total complexity of the channel for a fixed number of channels.
SP:4fd702490293e481c79614852ba27dd3ce9215a4,"This paper presents the results of a thorough analysis of the performance of the recently proposed hyperparameter optimization ( HPO ) framework for machine learning ( ML ) algorithms in the context of the benchmarking framework HT - AA.    The paper provides a detailed description of the differences between the baseline algorithms and the current state - of - the - art ML algorithms in terms of their search space and their ability to transfer performance across different ML algorithms. The paper also provides insights into the differences in the search space between the current and future ML algorithms, and shows that the difference in performance between the two approaches is primarily due to the different choices of hyperparameters used in the analysis."
SP:e8f99bae5853de525450fcb8facd23cf973fc161,"This paper studies the problem of high - dimensional ( high - entropy ) text - to - audio classification. Specifically, the authors consider the following problem : given a set of spectrograms $ \mathbb{R}^2 $, what is the probability that $ \text{text}$ is the product of $ \gamma$ and $ \theta$?    The authors show that this problem can be formulated as a convex optimization problem, where the objective is to find a representation $ \tilde{\theta}$ such that the probability $ \eta$ is a matrix of high entropy.   To this end, they propose two approaches : ( 1 ) high - dimensionally high entropy ( HDE ) data representations and ( 2 ) HDE - HDE ( HEDE ) labels representations. For the first approach, they first obtain a high entropy representation of the data, which is then used to train an encoder - decoder network ( encoder / decoder ). The encoder is trained on top of the proposed HDE. The second approach uses a low - entropy ( low - dimensional ) encoder and decoder to obtain the representations of the labels and the data."
SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"This paper studies the question of whether we can trust a neural network's capacity to deliver a given value of $ \ell_0 $ in a given time interval. This question is phrased as follows : given a set of data points $ \infty$ and a distribution $ \theta$ over $ t$, what is the probability that the network can deliver a value $ \tilde{x}$ in the interval $ t\ell_1 $?    The paper answers this question by showing that there exists a network $ n$ such that $ n\theta^t$ can be approximated by $ n(\theta)$ where $ n(t)$ is a convex function of $ t$. The paper also shows that there exist networks $ n^t $ such that the capacity of the network is bounded from above by a factor $ \sqrt{\theta}$, and shows that this is also bounded from below by an upper bound.  "
SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"This paper proposes a knowledge transfer method that attempts to bridge the gap between representation learning ( SRM ) and knowledge distillation ( KD ) in the context of neural networks. The key idea is to train a neural network ( CNN ) that learns representations of the hidden features and distills knowledge to the student network ( KD ) that is able to handle the knowledge transfer problem. To this end, the paper proposes two approaches : ( 1 ) SRM - KD and ( 2 ) KD - SRM.    The first approach is based on the concept of representation learning and distillation. The second approach uses the notion of knowledge transfer to distill the knowledge from the student to the KD network. The authors claim that the two approaches are complementary in the sense that KD - KD does not require the knowledge to be transferred from one student network to the other, while SRM does so by distilling the knowledge across the student and KD networks."
SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,This paper proposes a policy similarity metric ( PSM ) to measure the distance between two policies when learning from data. PSM is motivated by the observation that two policies that are similar in structure may be similar in practice if they are learned from similar data. The paper shows that the PSM can be used as a metric to compare different policies learned from different data. Experiments show that PSM outperforms other metrics such as PSE and PSE2.
SP:92f3b4942da9075440dda618f561a85f8fde5a5c,This paper is an extension of the work of the first author to the setting of equivariant transformations. The main contribution is a proof of a result of the second author in the context of group representation theory. The proof is based on the fact that there exist transformations that are invariant under the action of a group.    [ section ] [ theorem]lemma [ theorem ] le thorme de la thorie de transformations dans la formule de group representation dans un modle de dimension dans le modle dune dimensione dune formule dune group de dimensione universelle.
SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"This paper proposes a variational approach to estimate the likelihood of latent variables produced by the Hawkes process in the context of neuronal spiking activities in a neural network. In particular, the authors focus on the hyper - parameterization of the process as a mixture of two types of hyper - variables, one of which is the activation function and the other is the inhibition function. The hyperparameterization is based on the fact that the activation and inhibition functions are functions of the same hyper - variable, and that the hyperparameters of the two functions are independent of each other.    The authors propose two variational approximations for the hyper parameterization. The first one is a minimizer - based variational inference, and the second one is an a posteriori estimator - based estimator. Experiments show that the proposed approach outperforms the baselines in terms of both accuracy and time - consuming sampling."
SP:1156d3deac022829bda930ffcb081947609d972b,"This paper proposes a two - layer neural network model that can be trained in two stages, i.e., the quenched - activation and deactivation stages. The first stage is to train two layers of the model $ \mathcal{N}$ with $ \text{N } \infty$ parameters $ \nabla$ and $ \log n\infty$. The second stage consists of training two layers $ \tilde{n}$ of $ n$ parameters, $ \gamma$ parameters and $ tilde{x}$ feature model parameters $ g_\theta$. The authors show that this model can be used to train deep neural networks in different dynamic regimes."
SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"This paper studies the problem of learning a safe policy in high - dimensional dynamic environments. The authors propose two approaches to this problem. The first approach is based on MDP ( MDP - R - MDP ) and the second one is PDP ( P - PDP ). In both approaches, the goal is to find a policy that satisfies a set of constraints that ensures that the policy is safe in the given dynamic environment.    The authors prove that both approaches converge to the same solution in a finite number of iterations. They also prove that the PDP method is more stable than the MDP method."
SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,"This paper studies the problem of training a neural network to solve unsupervised vision and speech recognition tasks. The paper proposes two approaches to this problem. The first approach is to train a network with a fixed number of data points and a fixed amount of entropy loss, and the second one is to add a new layer to the entropy loss and a new parameter to the training loss function to make the network more data - efficient.   The paper provides theoretical analysis of the proposed approaches and empirical results to show the effectiveness of both approaches."
SP:915f1f0fc4850507c28c1d609239b41775863ebe,"This paper studies the problem of learning representations from unlabeled video representations, and proposes two approaches to this problem. The first approach is to pre - train a supervised model on a corpus of unlabelled videos, and the second one is to learn representations from the supervised model. The authors prove that both approaches converge to the same representation under certain conditions. The main contributions of the paper are as follows :   1 ) The supervised model is designed to be able to handle large corpora of data ; 2 ) The two approaches are shown to converge to a single representation under different conditions ; 3 ) Experiments are conducted to evaluate the performance of the two approaches."
SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"This paper proposes a new method to learn representations of graphs from single - node embeddings, which can be used for classification and other machine learning tasks. The proposed method is based on the similarity matrix factorization ( CRF ) approach. The authors also propose a local version of the CRF method, which they call PPR - LCF, which is a variant of CRF that is more scalable. The experiments show that the proposed method achieves state - of - the - art performance on a set of classification and machine learning benchmarks."
SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"This paper proposes a new metric to measure the quality of graph coarsening algorithms. The metric is based on the relation between the size of a set of graphs and the number of iterations required to learn the metric. The paper proposes to use the metric as a measure of the distance between the largest and smallest set of vertices in a graph.    The paper also proposes a metric that measures the difference between the sizes of the smallest and largest datapoints in a given graph. This metric can be used as a metric to compare different graph - based metrics ( e.g., graph - metric, metric - metric or metric - vector - metric ). The main contributions of the paper are as follows :   1. Introducing the metric metric, 2. Demonstrating that the metric is mathematically equivalent to the metric introduced in [ 1 ], 3. Proposing the metric in [ 2 ], 4. Showing that it is not too different from the metric proposed in [ 3 ]."
SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,"This paper presents a theoretical analysis of the acoustic properties of virtual and real - world audio content created by deep learning based simulators. Specifically, this paper focuses on the following :   1.    topological properties of audio content creation and scene analysis. The authors show that the existing approaches do not consider the topological structure of the space of objects in the virtual world, which makes them unable to capture the properties of the real world sound. They show that this property is due to the fact that they do not take into account the topology of the virtual space, which results in a non - convex factorization of the sound spectrum, and that this factorization results in an improper approximation of the cross - entropy between the real and virtual sound spectrum. 2.    topological property of sound propagation. To address this issue, the authors propose a new method based on tracing based sound propagation, which is able to capture both the spatial and temporal properties of sound. This method is shown to be more efficient than the previous methods in terms of the number of timesteps and number of simulators used, and it is also shown to have a better performance than previous methods. 3. ́ property of spatial harmonics encoding. This paper shows that the proposed method can be used to encode the spatial properties of a sound into a low - frequency vector, which can be then used to improve the performance of hybrid sound - ray sound - based rendering engines."
SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"This paper studies the problem of optimizing the sensitivity of a machine learning model to shift - of - distribution under the assumption that the underlying distributional model is invariant to changes in the underlying domain. The paper provides a theoretical analysis of the distributional models under this assumption, and shows that there are two types of distributionsal models that can lead to different sensitivity : ( 1 ) categorical and ( 2 ) non - categorical ones. In particular, the categorical model ( i.e., categorical models with categorical distributions that are invariant under shift ) is shown to be more sensitive than ( i ) and ( ii ) when the underlying model is categorical.   "
SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"This paper proposes a zero - shot neural operator - based solver for finite - dimensional Euclidean flows. The basic idea is to learn a neural operator that maps functions $ \mathcal{d}$ to functions $ d$, and then solve the corresponding differential equation using a ML - based method.   The main contribution of this paper is to show that this neural operator can be used to learn the kernel of the flow $ \delta$, which is then used to solve the differential equation $ PDE$."
SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"This paper studies the problem of minimizing the $ \ell_2$-max - margin of a linear convolutional neural network trained on $ \epsilon$-normally decomposable inputs $ \mathbb{L}$ and $ \text{max}$ normally decomposed $ \gamma_2$. The authors prove the following :    1.   the input space $ \alpha_2 $ is a function of $ \tilde{1 } \infty$, $ \log{2}$ is a linear tensor network parameter $ \sqrt{2 } \log { L}$, and   $ \lambda_2 \log n$ is an additive function $ \nabla_{\text{2}\log n\log n}$.   2. There exists a $ \leq \ell_{\infty}$ such that $ \eta_2\leq 2 $ can be approximated by $ \frac{1}{\sqrt{\infty } } $, where $ \theta_1 $ is the input dimension $ \overline$ of the input $ \hat{2\log l}$. The authors show that this function is a sub - optimal solution to the 2 / L max - margin problem. 3. There exist $ \geq 2 \log 2 $ such that there exists $ \teq \log_{\overline } $ $ $ 2 $ q^2 $ q_{\geq { 2 } \text{\text{l}$."
SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"This paper provides a theoretical analysis of the criticality of training neural networks in the presence of data constraints. Specifically, the paper considers the following scenarios :    1. No data at all, i.e., no data at the start of training.   2. Data at the end of training, the network is assumed to be invariant to the source - to - target ratio of the source and target dimensions. 3. For each of these scenarios, the authors provide theoretical guarantees on the number of iterations required to ensure that the network converges to the target asymptotically in the limit of infinite data."
SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,This paper studies the problem of semi - supervised learning ( semi - SSL ) and federated learning ( FSSL ). The main contributions are :    1 ) This paper provides a theoretical analysis of the convergence rate of FSSL and FedMatch under the assumption of unlabeled data.   2 ) The paper provides empirical results showing that FSSL achieves better performance than FedMatch when the labels of the data are available.
SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,"This paper proposes a new method for supervised learning of discrete event sequences. The proposed method is based on a vector - based representation learning approach, where a neural network is trained on a fixed - length vector representation of an event sequence, and the goal is to learn a supervised representation of the sequence. The authors claim that this representation is more suitable for real - world users than for supervised learners as it is not restricted to the event sequence domain and can be easily incorporated into a variety of machine learning tasks. Experiments are conducted on standard supervised learning and unsupervised settings, showing that the proposed method outperforms the supervised learning baselines."
SP:385942a5bcee7384bb722a1669b541f2fac0cd36,"This paper proposes a new dependency - and - constituency parsing framework for natural language models. The proposed framework consists of three components : ( 1 ) a tree - based dependency parsing framework, ( 2 ) a dependency - constrained self - attention mechanism, and ( 3 ) a language modeling framework. The model is trained on top of a pre - trained language model, and the two components are jointly trained with a softmax parsing algorithm. The results show that the proposed framework is better than the baselines on natural language modeling and language modeling alone."
SP:078966ff62775bba6031e47d374bda95f4a7dde3,"In this paper, the authors propose a new supervised method for scene graph grounding. The supervised method is based on a multi - layer perceptron ( MLP ). The MLP is used to predict relations between two scene graph parsers ( SGGen ) and a set of object features ( VRD ). Experiments show that the MLP outperforms other supervised approaches in terms of supervised grounding accuracy."
SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"This paper studies the problem of generating distributions that satisfy relational discrepancy between two distributions of the same dimension under the assumption that one of them is spherical and the other is isotropic.    The main contributions of the paper are :   - Providing sufficient conditions under which the distributions of two distributions can be made to be relational in the sense that they are not orthogonal to each other. - Given two distributions $ \theta$ and $ t$, the paper shows that there exists a distribution $ \tilde{t } \infty$ such that for any $ t}$, there is a $ tilde{T } \leq \sqrt{\theta } \log p(\theta)$. - Showing that this distribution is invariant under time, and providing sufficient conditions for it to be identifiably differentiable."
SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"This paper proposes a new training method for deep linear networks. The main idea is to share the weights across layers of a linear network and train the network on top of each other. The network is assumed to be of size $ \ell_t$, where $ t$ is the number of layers of the network and $ T$ the size of the linear model $ t$.    The paper proposes two approaches to this problem. First, the network is divided into two parts : $ t_t$. The second part is the network sharing. In the first part, the networks are shared across layers and $ t_{t_t } = \ell_{t-1}$ is used to train the networks.   In the second part, networks are divided into $ t-1$ layers and trained on $ t -1$. The networks are then shared to the third layer and the network size is kept fixed."
SP:a51710551142316b67e2fccd969fea1ece35ba39,This paper studies the problem of transferability of adversarial perturbations across steps in a multi - step attack strategy. The paper proposes a new method to boost the transferability in the case when the step - size of the perturbation is larger than some critical length.    The paper provides a theoretical analysis of the problem in terms of the following three factors :   ( 1 )   negative correlation between the initial and final states of the attack ; ( 2 ) loss in the number of steps ; and ( 3 ) the ratio of the cumulative loss of the first and last step in the attack.
SP:f1565319075c1442c2cb52d96443facb492c06c2,"This paper studies the problem of semantic similarity of hidden representations for semantic similarity tasks. The paper proposes two approaches to this problem. The first approach is to train a model $ \mathcal{Z}$ $ $ \text{x}$ on a set of $ \tilde{x } $ sequences $ \gamma_t$, $ tilde{z}$ and $ y$ $ z$ $ sequences, and then train $ \cal{X}$ layers on top of these representations $ \log(z | y)$. The second approach uses $ \nabla_{\text{text}$-normals and $ \lambda_t $ -normals to learn $ \sqrt{\text}$.    The paper shows that both approaches fail to learn semantic similarity for the same set of representations. In particular, the first approach fails to learn the semantic similarity between $ \theta$ sequences and $ z$. The paper also shows that the second approach doesn't learn the semantics for $ \kappa$ sequences, but rather learns $ \sum_{\theta}$ representations for $ z}$ sequences.  "
SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"This paper proposes a framework for pre - training and fine - tuning large - scale language models for vision vision tasks. The proposed framework consists of a pre - trained language model and a fine - tuned language model with a number of components. The first component is a transformer - based language model. The second component is an attention - aware language model, which is used to fine - tune the language model at the end of the training phase.    The paper shows that the proposed framework can be used to pre - train a language model for vision tasks with different number of tokens and different amount of compression. The paper also shows that this framework is able to handle different amounts of compression without compromising on the quality of the results."
SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"This paper considers the following learning problem : given a set of labels $ \mathbb{R}$ and a distribution $ \text{divergence}$ over $ \tilde{D}$, what is the best way to measure the difference between $ \delta$ and $ delta$ under the assumption that the labels are noisy?    The paper first shows that the problem is equivalent to the following : given $ \gamma_{delta } = \mathbf{delta}$ where $ d\gamma$ is the distribution over $ d$, and $ tilde$ is a measure of the difference in the distribution between the distributions over $ D$, the paper proposes a learning problem that aims to find a way to minimize the difference of $ delta$ with respect to $ D$.   To achieve this goal, the authors propose a stochastic learning problem where they use a classifier $ \lambda$ that is trained with a noise distribution $ d_\text{Delta}$. They show that the classifier can be made to satisfy the following property under the following assumptions :   ( 1 )   $ \theta_delta^n$ is not too noisy ; ( 2 ) $ d / d$ is sufficiently large ; ( 3 ) $ g^n^d$ is small enough ; ( 4 ) $ f^d^n > \infty$ is close to $ g$ ; ( 5 ) $ m\theta^n < m > is sufficiently small."
SP:841888179dcdac901889c8d62cb5234311fe28f1,"This paper proposes a weighted Bellman - based ensemble method for learning policy and/or control tasks in the continuous and discrete control settings. The ensemble method is based on a weighted version of the ensemble method proposed in [ 1 ] and [ 2 ]. The main contributions are as follows :    1. Introducing a new metric that quantifies the uncertainty in the quality of the estimates obtained by the policy and the control algorithms in the two settings. This metric is called the * * Bellman uncertainty * *.   2. Demonstrating that in the discrete control setting, the estimates of the two algorithms are close to each other in terms of their uncertainty, the paper provides a proof of this metric. 3. Providing a lower bound on the uncertainty of the estimated value of the policy in both the discrete and the continuous settings."
SP:afc08f203562b841180811aef943bfb63a1659ea,"This paper studies the few - shot classification problem in meta - learning setting, where the goal is to train a teacher - teacher model on a set of tasks with a small number of shots. The paper proposes a calibration method that is based on the similarity score between the training set and the test set, and a training strategy that is designed to minimize the uncertainty in the teacher - student model.    The paper presents empirical results showing that the proposed calibration method is able to solve the classification problem with a few shots, and that the training strategy is effective in reducing the uncertainty."
SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"This paper presents a supervised learning method based on the text - to - video ( TV ) video retrieval framework. The video retrieval strategy consists of two steps : first, the video is divided into a series of episodes, each of which is supervised by a teacher who has access to a subset of the videos in the preceding episode. Then, the teacher is provided with access to all the videos from the previous episode, and the student is provided access to only those videos that are relevant to the task at hand.    The main contribution of this paper is the following :   1. Introducing the concept of "" contrastive learning "", i.e., a framework that allows the student to compare videos from different episodes without having access to the source videos. 2. Developing a model - based approach for the supervised learning of video representations. 3. Conducting experiments to demonstrate the effectiveness of the proposed approach."
SP:8a71d8fad25a126aff01431cacf348c05de75667,"This paper presents a pre - trained language model for Chinese BERT vocabulary segmentation ( CWS ). CWS is designed to be used in the natural language processing ( NLP ) tasks such as word segmentation, word - to - word labeling and vocabulary pretraining.    CWS pre - training is divided into two parts : pre - train and post - train. Pre - train consists of three stages :   ( 1 ) pretraining of the language model ( PLMs ) on a set of 100,000 Chinese nouns ; ( 2 ) training of the vocabulary model on the set of 200,000 nouns and ( 3 ) training on the subset of nouns that can be used as tokens in the tokenization ( tokenization - tokenization ) task."
SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"This paper proposes a graph - based learning approach to improve the accuracy of cross - graph neural network ( GCN ) training. The proposed approach is based on a sampling - based approach, where a subset of the nodes in the training set are sampled from a large graph, and the goal is to ensure that the graph accuracy of the samples is not too different from that of the original training set. The paper also proposes a boundary sampling strategy, where the samples are sampled based on the similarity of the target set with respect to the source and target set.    The paper shows that the proposed approach achieves better performance than baselines that do not sample from the source or target sets."
SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,This paper presents a theoretical and empirical study of the scaling properties of large - scale simulations of 3D molecular structure and interactions. The authors use a generic neural network ( GNN ) with message passing and force - centric interactions to simulate 3D structure and force interactions. They show that this model is able to handle large scale simulations with a high success rate. They also show that the model can be used to discover new physics applications.
SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,"This paper provides a theoretical analysis of the criticality of fine - tuning neural networks in terms of the complexity of the search space and the number of classes of solutions. The paper shows that for a given set of queries, there exists a set of solutions that satisfy the following constraints : $ \ell_tilde{x}$ where $ x_t$ is a fixed constant, $ y_t $ is a constant that depends on $ z$, and $ z_i$ is the set of representations of the query $ z$.    The paper also shows that there exist networks for which $ z_{t } \in \mathbb{R}$ can be expressed as a convex combination of $ z^{-1}$ and $ y_{t}$, where $ z}$ is an integer that can be thought of as the sum of $ \alpha(z_{t},\alpha(x_t)$, $ \gamma(\gamma)$ being the dimension of the space of possible solutions $ z^\infty$."
SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"This paper presents an analysis of the find - eval phenomenon, i.e., the phenomenon in which a set of data points are randomly pruned from a database and then evaluated with the goal of finding a subset of the data points that satisfy a certain set of criteria.    The paper proposes two approaches to this problem. The first approach is a one - shot structured pruning approach that prunes data points according to their hyperparameters ( e.g., magnitude ), and the second one is a model - based approach that assigns a score to each data point based on its similarity to a given hyperparameter. The paper shows that both approaches lead to similar results."
SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"This paper studies the problem of the coherence of real and random gradients in neural networks. Specifically, the authors consider the following metrics : ( i ) the distance between two sets of examples $ \mathcal{x}$, ( ii ) the divergence of $ \log(x)$ between the two sets $ y$, and ( iii ) the metric $ \text{max}$ between $ x$ and $ y$.    The authors show that the metrics ( i.e., $ \sqrt{x } \log ( y ) $ and $ \tau$ ) are inversely correlated with each other, and that the divergence can be understood as a measure of the divergence between the set of examples and the set $ z$ of the random set $ x$. The authors provide a theoretical explanation of this phenomenon based on the hypothesis that $ \sum_{\text{x}\log(y}$ is a metric that measures the difference between the sets $ x\log(z)$, $ y\log ( z ) $, and $ z$. They also show that $ z \log { \log{x},$ is independent of $ y}$ and can be used as a metric to measure the divergence in the divergence metric.   Experiments are conducted on two types of neural networks : ( 1 ) real - world and ( 2 ) random - world neural networks, and the authors prove the convergence of their theoretical results on both cases."
SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,This paper proposes a new Bayesian likelihood approach to approximate the likelihood function for learning generative models from sequence and/or discrete data. The approach is based on the notion of low - dimensional sufficient statistics ( LSH ). The main contribution of the paper is the derivation of a lower - dimensional estimator of the probability that a given sequence or set of data points is likely to be predictive of a given generative model. The derivation relies on the following assumptions : ( 1 ) sufficient statistics exists in the sequence and data sets ; ( 2 ) the sufficient statistics exist in the data sets that are not predictive of the model ; and ( 3 ) there exists a sufficient statistics sufficient to infer the model's predictive power from the data.
SP:c5997bf2348e94949684f45fbd418661e85220c1,"This paper proposes a supervised model learning framework for image - to - image translation, where the goal is to learn a supervised representation learning model that can be used in the unsupervised and supervised supervision scenarios. The supervised model is built on top of a pre - trained model of the source and target domain, and the supervised model consists of a set of labels and an image to image translation model conditioned on the source domain and the target domain.    The proposed framework is shown to achieve state - of - the - art performance in the supervised supervision scenario."
SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,This paper studies the problem of adaptive smoothing of gradient descent trajectories in neural networks. The main contribution is to provide a theoretical analysis on how to mitigate the bias in the initialization of the second derivative of the smoothing spline in the case of ReLU networks.    The main contributions are as follows :   1. provide theoretical analysis of the gradient descent bias in function space in the setting of shallow neural networks ; 2. propose a new initialization scheme for the second derivatives of the spline that is adaptive in the sense that it does not depend on the initial distribution of the network parameters ; 3. provide empirical evidence that the proposed initialization scheme is more stable than the one proposed in [ 1 ].
SP:8b885142facbb3b8db41ec9d83822cee81324694,"This paper proposes a new regularization regularization technique, AdamW, for learning deep neural networks with SGD regularization. The authors prove that AdamW is stable with respect to the number of iterations, and that it is also stable against SGD. The paper also proposes variants of AdamW with different hyperparameters, and shows that these variants are more stable than the original AdamW."
SP:a3206dc71e32ba1830895bf442d3840f3331a532,"This paper proposes a new method to retrieve semantic information from a pre - trained language model ( PLM ) and a decoder ( decoder - guided decoder ) by comparing the similarity score between the model and the target language. The model is assumed to be a mixture of two types of language models : the PLM and the decoder model.    The main contribution of this paper is to propose a retrieval method based on similarity score based on the similarity between the source language and target language, which is used to create a memory of the source and target languages."
SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,This paper proposes a gradient ascent scheme to reduce the number of features in a deep network by training it on a set of real - world datasets. The proposed approach is based on the idea of invariant classification and back propagation.    The paper provides a theoretical analysis of the proposed approach and empirically shows that it is able to achieve better performance than the state - of - the - art deep networks.
SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"This paper studies two - layer linear neural networks with parameterized gradient flow dynamics. It is shown that there exist two types of initializations of the flow dynamics that satisfy certain invariants. The first type of initialization corresponds to the case when the parameterization is parameterized by a finite set of polynomials. The second type corresponds to a case where the parameterisation is given by a polynomial in the dimension of the second layer.    In the first case, the authors prove that there exists a critical value of the initialization such that the flow does not violate the factorization invariant. In the second case, there exist infinitely many initializations such that there is no critical value for the second factorization. This is shown to be a special case of a generalization of a result in [ 1 ] and [ 2 ]."
SP:e5f086c806be88d50e461a782b5b00124f4656fb,"This paper proposes a framework for providing explanations of real - world data in terms of the model ’s ability to explain the data. The framework consists of two components :   ( 1 )    a generic model “ framework ” and ( 2 ) an interpretable model ”. The generative model   consists of a set of inputs $ \mathcal{x}$ with $ \text{x } \theta$-norm $ \alpha$ and $ \tilde{\theta } \in [ 0,1,2,3]$, where $ \eta$ is the size of the input space and $ z$ the number of subspace dimensions.   The interpretable   model is trained using two approaches : ( i ) a sampling - based approach, where the input $ \lambda$ is sampled according to $ \gamma_t$, and ( ii ) an encoder - decoder approach, which takes as input $ y$ and outputs $ z$. The encoder and decoder are trained on the same dataset, and the decoder is trained on top of the encoder / decoder. The model is then used to provide explanations of the data in the context of the two encoders."
SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,This paper presents a set of experiments aimed at improving the performance of the pre - trained GPT-2 language model on the benchmarks CLUE and BERT. The experiments are divided into two parts :   ( 1 ) tokenization - based and ( 2 ) token - based. The first part focuses on tokens - based tokenization and ( 3 ) attention - based tokensization. The tokenization method is based on the attention method proposed in [ 1 ]. The attention method is similar to [ 2 ] in the sense that the tokens are sampled from the attention model and the tokenization is done by the tokenizer.    Experiments are conducted on the benchmark CLUE dataset ( $ \ell_t$ ) and the benchmark datasets ( $ t$ ). Results show that the tokenized model performs better than the un - tokenized one on both benchmarks.
SP:fd1cfe80343d3789227d99d836a5674374a234f5,"This paper presents a text summarization and semantic parsing framework for the Neural Machine Translation ( NT ) framework. The framework is built on top of the attention mechanism proposed in [ 1 ]. The attention mechanism is used to assign attention to each word in the text, which is then used to generate a sequence of tokens to be used as the summary for the next sentence in the context of the previous sentence. The sequence tokens are used to represent the meaning of the sentence, and the tokens are passed through a transformer encoder to produce a text representation of the context.    This framework is evaluated on two datasets : [ 2 ] and [ 3 ]. Experiments show that the proposed framework outperforms the previous works on both datasets."
SP:2056a65a7500d79465685af883083cd706277c1f,"This paper proposes a new adversarial training method for deep neural networks ( DNNs ) based on cross - entropy perturbations ( CAT ). The proposed method is based on the concept of multiple perturbation transformations. The paper provides a theoretical analysis of the proposed CAT method and shows that the proposed method can be used to defend against different types of adversarial attacks. Moreover, the paper also provides a proof - of - concept example showing that CAT can be combined with standard adversarial losses to improve DNN robustness."
SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"This paper proposes a new neural network, named the * * * Emerging Symbol Binding Network * * ( ESBN * *, for short ), which aims to provide a better understanding of the attention mechanisms in the context of the evolving neural network.   * * Contributions * * :   1. This paper provides a theoretical analysis of the different types of attention mechanisms that can be used to improve the performance of the existing neural networks ( e.g. * * RPM * * and * * NCE * * ). 2. It presents a theoretical proof - of - concept that the proposed ESBN achieves better performance than RCE and NCE on a set of different benchmarks. 3. It provides empirical results that show the effectiveness of the proposed approach."
SP:4171ce45966ac499f51450a19fb233934c0847f0,"This paper presents a novel approach to the text - to - text classification and relation extraction problem of natural language natural language tasks. The proposed approach is based on a generative model that consists of two main parts : ( 1 ) a text - based classification model and ( 2 ) a task - specific classification model. The classification model consists of an entity recognition model and a relation model that predicts the relation between two entities ( e.g., a "" state "" and a "" predicate "" ).   The paper shows that the proposed approach achieves state - of - the - art results in terms of both accuracy and time complexity when compared to the baselines of previous approaches."
SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"This paper proposes a new approach to the entity recognition problem. The proposed approach is based on a real - world dataset ( e.g., wikipedia ) that is annotated with the names of entities ( nouns, verbs, nouns and phrases ). The goal is to find a set of entities that can be recognized by the proposed approach. To this end, the authors propose two approaches. The first approach is to create a corpus of labeled entities and the second one is to generate a dataset of annotated entities and annotate them with the given entities.    The proposed approaches are validated on a variety of entity recognition problems, and the results show that the proposed approaches perform better than existing approaches."
SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper presents a new method to train a text encoder - encoder neural network for the name recognition and phonetic matching tasks. The encoder network consists of a low - dimensional embedding space $ \mathcal{d}$ and a high - dimensional vector space $ d$, which are used to train the encoder and the phonetic encoder respectively.    The main contributions of this paper are as follows :   1 ) a new training criterion for the encoders and phonetrized encoder networks, which is based on minimizing the posterior probabilities of the embeddings $ d$. 2 ) A new neighbor embedding ( SNE ) is introduced to improve the accuracy of the FST - based phonetic decoding. 3 ) A novel loss criterion is proposed for the first time for the cross - entropy decoding task."
SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,This paper presents a theoretical analysis of the single - agent reinforcement learning problem in the context of mean - field games. The authors show that the policy optimization problem can be formulated as a convex optimization problem where the goal is to find a policy that minimizes the cost of playing a set of games with a fixed number of agents and a fixed set of actions. They show that there is an upper bound on the number of actions that need to be learned in order for the policy to converge to the equilibrium point.
SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"This paper proposes a new method to evaluate the quality of flow - based generative models under different metric distributions. The main idea is to train a generative model on a set of different metrics ( e.g., $ \ell_t$, $ t$ ), and then use inference to infer the distribution of $ t$. To do so, the authors propose two approaches : ( 1 ) to train the model on the metric distributions $ t \infty$, and ( 2 ) to test the inference on the metrics $ t\infty$. The first approach is based on training the flow model on different metrics and then inferring the distribution from the metrics.    The authors provide a thorough evaluation of the two approaches. They show that the first approach ( i.e., training on metrics with different metrics ) leads to better quality than the second one ( ii )."
SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"This paper proposes a new neural segmentation method based on human perception based evaluation criteria for learning segmentation methods. The proposed method is built on top of the existing segmentation dataset ( e.g. [ 1 ], [ 2 ] ), which has been used in [ 3 ] and [ 4 ]. The main contribution of this paper is to propose a new evaluation criterion for segmentation based on the human perception.    * * Contributions * * :   1. This paper proposes to use human - perception based segmentation criteria to evaluate the segmentation learned by a neural network. 2. The paper shows that the proposed neural network is able to segmentate the data well. 3. To evaluate the performance of the neural network, the paper first generates a dataset with high resolution image segmentation data ( $ \ell_1 $ ). 4. The authors show that the network segmentation is better than the previous neural network on this dataset."
SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"This paper proposes a learning - transfer framework that transfers knowledge from a restricted search space to a more general one by providing a set of benchmarks that can be used to compare the performance of different algorithms on different tasks. The paper also provides a brief summary of the current state - of - the - art knowledge - transfer algorithms.    The main contributions of the paper are as follows :   - Introducing the concept of knowledge transfer, which is defined as the amount of knowledge that needs to be transferred across different tasks in order for a given task to be transferable to another task. - Demonstrating that knowledge transfer across different benchmarks can be better than that across a fixed set of tasks, by showing that the average transfer speed between two benchmarks is faster than the average speed across all the benchmarks. - Providing an empirical evaluation of the knowledge transfer speed, showing that there is a trade - off between the speed at which knowledge is transferred across benchmarks and the total amount of data required to transfer knowledge across benchmarks."
SP:cc819c61f408e88f247eb87946187ccec3dad32e,"This paper presents a meta - learning framework for few - shot classification tasks. The framework is based on a generative model that has been trained on a set of well - designed data sets. The training is divided into two stages : ( 1 ) a pre - trained set of tasks and ( 2 ) a test set. The first stage is designed to provide a baseline on which the classifier is trained. The second stage is to train the classification model on the data set of the test set, which is a subset of the training set.    The paper presents results on two sets of data sets : the original dataset and a subset that is augmented with a few examples. The results show that the augmented dataset is better than the un - augmented one in terms of classification accuracy."
SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"This paper studies the problem of topological stability of deep neural networks. The authors propose two approaches to this problem : ( 1 ) preserving the topology of the underlying network and ( 2 ) analyzing the inverse of the network in terms of its stability. The first approach is based on the fact that topology is invariant under perturbations of the input data. The second approach uses the notion of injectivity to prove that the network is injective if and only if it satisfies a certain condition.    The authors prove that both approaches are equivalent to each other in the sense that if the injectivity of a given network is not greater than a certain threshold value, then the network will not be injective. Moreover, the authors show that under certain assumptions on the network ( e.g., that the number of injectors is less than some threshold value ), the resulting networks are injective and invariant."
SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This paper studies the problem of generating generative adversarial generative models with one - hot labels. The authors propose two approaches to this problem. The first approach is a generative generative model ( GAN ), which is based on GANs with two types of generative losses : HVDL and SVDL. The second one is a discriminator - based approach.    The authors show that both approaches fail under the same set of conditions. The main contribution of this paper is to show that the discriminator and the generator losses are not equivalent under the two conditions. Moreover, the authors show the equivalence between the two approaches."
SP:10dd09ab315870631d1451d200f2c87a023f8226,"This paper considers the problem of supervised learning of deep neural networks ( DL ) and active learning ( AL ) algorithms. The authors prove the following guarantees :    1. For any DL - based AL algorithm, there exists a solution that is at least as good as the one obtained by the best - fit version of the algorithm in the offline setting.   2. For all AL algorithms that are cross - entropy efficient ( e.g., $ \ell_sqrt{d}$ ), there exist solutions that are at least a factor of 2.5 better than the best fit version in the online setting."
SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"This paper proposes a minimization - based training method for neural network models trained with a finite - difference covariance loss ( FL ). The proposed FL method is based on the idea of training a neural network with a regularizer and a non - convex data regularizer, where the regularizer is a linear function of the number of data points and the convexity of the data points. The paper shows that the FL method can be regarded as a minimisation - based learning method with the following components : ( 1 ) minimization of the covariance, ( 2 ) minimisation of the divergence, ( 3 ) minimizability of the gradient, and ( 4 ) minimizer of the variance."
SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"This paper studies the problem of supervised learning of computer vision tasks from a linear classification perspective, and proposes two approaches : ( 1 ) contrastive contrastive learning and ( 2 ) similarity - based supervised learning. In contrast to prior works, which focus on the contrastive loss between supervised and unsupervised versions of the same vision tasks, the proposed approach focuses on the similarity of supervised versions of a given vision task with respect to its supervised counterparts. To this end, the authors propose two approaches, i.e., contrastive gradient descent and similarity matching.    The main contributions of the paper are as follows :   1 ) A theoretical analysis of the two approaches is provided, which shows that ( i ) the similarity in supervised and contrastive losses is a good indicator of the similarity between the supervised versions, and ( ii ) both approaches are able to converge to the supervised counterparts in a finite time horizon."
SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"This paper considers the problem of learning a policy that approximates a given function with a single actor - critic and a two - timescale critic. The authors consider the case where the function is a linear function and the critic is a neural network. They show that under certain assumptions on the function approximation, they can guarantee global optimality in terms of the number of iterations and the timescales of the updates.    The main contributions of the paper are as follows :   1. They provide a theoretical analysis of the convergence rate of single - actor and two - actor critic with linear function approximations, and show that it converges asymptotically to $ \sqrt{O}$ in the case of linear function approximation. 2. They extend this result to the setting of deep neural networks, showing that the same rate of convergence can be guaranteed in this case as well."
SP:26705a4dc305cce336f657c5937d1f5b4209548a,"This paper presents a series of experiments aimed at improving the quality of the data generated by transformer networks ( TNs ) for data processing tasks. The experiments cover the following :    1.   * Sec. 3.0 - Sec. 4.1 - Sec 5. * Sec 4.2 - Sec 6.3 - Sec 7.4 - Sec 8.5 - Sec 9.6 - Sec 10.7 - Sec 11.8 - Sec 12.9 - Sec 13.10 - Sec 14.15 - Sec 15.16 - Sec 16.17 - Sec 17.18 - Sec 18.19 - Sec 19.20 - Sec 20.21 - Sec 21 - Sec 22 - Sec 23 - Sec 24 - Sec 25 - Sec 26 - Sec 27 - Sec 28 - Sec 30 - Sec 32 - Sec 34 - Sec 35 - Sec 37 - Sec 38 - Sec 41 - Sec 39 - Sec 40 - Sec Next - Sec - Sec 4 - Sec 3 - Sec   5 - Sec # 1#2#3#4#1 * # 2 # 3 # 4 # 5 # 6 # 7 # 8 # 9 # 10 # 11 # 12 # 13 # 14 # 15 # 16 # 17 # 18 # 19 # 20 # 21 # 22 # 23 # 24 # 23#L#1#2 * # 3 *, # 4 ( # 5 ) # 6 ( # 7 ) # 8 ( # 8 ) # 9 ( # 9 ) # 10 ( # 14 ) # 15 ( # 16 ) # 19 ( # 17 ( # 19 ) # 20 ( # 22 ) # 21 ( # 23 ) # 18 ( # 20 ) # 24 ( # 21 ) # 23 ( # 24 )"
SP:165c51a16f17fb8726e968f8b34742b62011d60e,"This paper presents a theoretical analysis of the properties of the first layer of AlexNet, a neural network that has been designed to model the distribution of cross - entropy ( CWPT ) metric. The authors show that the network is able to provide a good representation of the world in terms of the number of vanishing moments of the metric, as well as a good approximation to the imaginary metric.    The authors also show that there is a trade - off between the quality of the representation and the invariance of the vector representation to the metric."
SP:d0a284da462584724ba6a3a48c9e986d391233f6,"This paper proposes a zero - shot generalization framework for multi - agent reinforcement learning. The framework is based on the view - sharing framework proposed in [ 1 ] and [ 2 ], where the goal is to provide an agent with access to a set of resources ( e.g., data ) that can be leveraged by the agent to learn a team - specific strategy.    The authors propose two views of the problem :   * * Zero - shot generic view * * : a view that views the world as a collection of agents and a team of agents, where each agent is equipped with a data source and a data decoder, and a communication channel with a server that provides access to the source data and the data to the team. *   This view is then leveraged to learn the agent - team composition and the communication channel, and is shown to generalize well to the view of the world in [ 3 ], [ 4 ], and [ 5 ]."
SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"This paper studies the problem of improving the quality of the estimators of the influence functions of a non - convex model under the assumption that the underlying loss function is a convex function. The main contributions are two - fold :   1.    improving the interpretability of the under - parameterized estimators by introducing a new metric, the _ cross - entropy _, which measures the difference between the expected value of the model and the value of a subset of the data points in the network. This metric has been introduced in [ 1 ] and [ 2 ], and is used to improve the accuracies of the so - called   soft - learning "" estimators "", which are used in machine learning ( ML ) and deep learning ( DLL ) settings. The authors show that this metric is robust to model changes and can be used as a regularization term in the loss function formulation of [ 3 ], [ 4 ]. The paper also provides a theoretical analysis of the effectiveness of the proposed metric, and shows that it is better than the previous estimator [ 5 ] in terms of both interpretability and performance."
SP:5fea74a2031d097a99dacf613bedcb054b0c3831,This paper presents a series of experiments on language modeling and classification tasks with the goal of providing answers to the question : what is the best way to make language models learn to complete certain types of tasks?    The paper presents results from a set of 5 experiments : 1.   language modeling tasks of interest : cross - entropy ( log - perplexity ) classification ; 2. language model features classification tasks : word embeddings ; 3. word prediction tasks : corpora centricity ; 4. classification tasks : next word prediction ; 5. language models features classification.
SP:a67da438e9821010284416170c3699ae7ff96c99,"This paper provides a theoretical analysis of the classification error under the following assumptions :    1. There exists a set of data points $ \mathcal{x } \infty$ such that $ \text{x}$ is not greater than $ \sqrt{\theta}$, where $ \theta$ is the size of the data set $ \tilde{x}\infty$.   2. For each data point $ x$, there exists $ \log(t\theta|x)$ that is larger than the $ \nabla_t\sqrt{x\infty}$ error.   This paper provides theoretical analysis on the following three types of attacks under these assumptions : ( a )   $ \sigma_0 $, ( b ) $ \delta_1 $, and ( c ) $\sigma_{0 } \log_2 $."
SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,This paper presents a theoretical analysis of the distributionally safe learning problem in the context of neural architecture search. The main contributions are as follows :    1. This paper analyzes the distributions of $ \ell_t$ and $ \alpha$ in terms of the cross - entropy metric $ \sqrt{x}$ and shows that $ \tilde{x } \in \mathbb{R}$ is a good indicator of the safeness of the learning problem.   2. The paper provides a theoretical proof of the non - differentiable nature of the learned distributions $ \text{ell_{\text{x}}$ under the assumption that the $ \infty$ is not too differentiable. 3. A set of experiments is provided to verify the theoretical results. 4. Experimental results are provided to demonstrate the effectiveness of the proposed approach.
SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"This paper studies the problem of approximating the distance between two nonlinear functions $ \mathcal{x}$ and $ \log(x)$ in terms of the activation networks $ \text{x }$.    The authors prove that there exists a function $ \lambda$ such that for any $ x$ and any $ y$, there exist a set $ \alpha_t$ $ such that $ \gamma_{t } \infty$ is a function approximating $ \cal_{t}$ with the property that for every $ x_t $, there exists $ y_t(x_t)$ that approximates $ \sqrt{\log(y_t ) + \lambda_t$."
SP:f5be855300f63c185a006834302bd4b033b56258,"This paper presents a meta - learning algorithm with gradient - based gradients for the task - specific classification problem. The authors provide a theoretical analysis of the convergence rate of the proposed method, and show that it is able to tolerate a large number of iterations without catastrophic failures. The paper also provides a proof - of - concept proof that the proposed algorithm can tolerate a small number of failures without catastrophic failure."
SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,This paper provides an upper bound on the variance of the divergence of the Q value under a policy evaluation objective that penalizes the probability of out - of - distribution actions sampled from the distribution. The paper provides both model - free and model - based approaches to this problem. Experiments are conducted on both the online and offline setting. The main contributions of the paper are as follows :    1. An analysis of the convergence properties of the proposed approach. The authors show that the approach converges to a lower bound of $ \sqrt{KL}$ on the divergence when the number of actions sampled is less than a certain threshold. 2. A theoretical analysis of this approach is provided. 3. Experimental results are provided for the case when the action distribution is model free and the model is a mixture of two distributions.
SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,This paper proposes a regularization strategy for training deep neural networks under the one - shot learning paradigm. The main idea is to train a neural network $ \ell_0 $ with a fixed number of shots $ t$ of training data $ t$. The training strategy is based on a CNN - based neural network with $ t = 1 $ \infty$ and $ t=50$.    The paper analyzes the regularizability of the proposed training strategy and shows that the training strategy converges faster than the previous one $ t+1 $.
SP:dba40073f79143e5355d194aa16db9eee0267a5d,"This paper studies the problem of foraging in an extended version of the greedy - exploration game, where the goal is to find a set of actions that will lead to a reward that is larger than the current state of the art exploration algorithms. The paper proposes two approaches to this problem. The first approach is based on sampling trajectories from a distribution that consists of trajectories sampled from a collection of actions sampled from an action distribution. The second approach uses an iterative approach that samples actions based on the probability that a given action will be discovered in the next step of the iterative process.   The paper shows that both approaches perform comparably to each other in terms of the number of steps required to reach a given state and the length of the trajectories needed to reach it. However, the paper also shows that the former approach performs slightly worse than the latter when starting from a state that has been discovered at a later stage of the iteration. This is attributed to the fact that the greedy exploration algorithm samples actions that are more likely to be discovered at later stages of iteration."
SP:5efb581a368ace3bd085d48801a899559d6a43ef,"This paper proposes a new low - rank minimization algorithm for the matrix factorization problem. The proposed method is based on gradient flow and regularization. The authors prove that the proposed method converges to the minimization norm of the norm under certain assumptions.    * * Key words and phrases * * : Low - Rank Learning ( GLRL ), Numerical minimization, gradient flow, norm minimization"
SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"This paper presents a dataset - agnostic patching - based approach for improving the accuracy of the classification accuracies of skin - based variants of cancer classification datasets. The proposed approach is motivated by the fact that patching a dataset with different subsets of the same classifier can lead to different accuracies, and the goal of the approach is to ensure that the resulting dataset is consistent across subgroups. To this end, the paper proposes a two - stage approach. First, the dataset is divided into subgroups, and each subgroup is partitioned into patches based on its similarity to the original dataset. Second, the model is trained with a set of data augmentation strategies, where each patch is augmented with a feature extractor and a classification classifier. The model is then used to classify samples from different subgroups based on the feature extractors and classification classifiers. Experiments show that the proposed approach outperforms other patching approaches on a variety of datasets."
SP:de6cea1e35a0555175e17546a93422e9a96a511e,"This paper presents a discrete model - based representation learner ( RRL ) for classification of nonfuzzy data sets. The RRL consists of two components. The first component is a generative model that learns a set of rules and the second one is a classifier that learns to represent the data set in terms of the classifier and the rules.    The authors compare the performance of the two components on 4 large data sets and 4 small data sets, and show that the RRL is better than the other two methods on all 4 sets."
SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"This paper studies the problem of property and stability prediction in biomedical domains. The main contribution of this paper is two - fold : ( 1 ) it provides a theoretical analysis of the problem and ( 2 ) it proposes a method to solve it. The theoretical analysis is based on the well - known ( and widely - used ) homology and stability results in biology domains.    The main contributions of the paper are as follows :   1 ) This paper provides theoretical analysis and empirical results for the following three types of domain perturbations : natural environments, biomedical environments, and complex environments."
SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"This paper presents a new text - vision and vision - language retrieval task that is based on cross - modal model attentions. The proposed task is similar to the one proposed in [ 1 ] and [ 2 ], where the goal is to retrieve a set of text and vision tokens from a corpus of $ \mathcal{T}$ $ \text$ and $ \cal{L}$ tokens $ \log(t_{text}/\text$, $ t_{text } / t_{viz}$, respectively.    The key difference is that the proposed approach does not require the corpus size of $ t$ to be larger than $ \sqrt{T } / \text$.   This is done by adding a new scale parameter $ \nabla_{t_t}$ and a new attention parameter $ t_t$ to the previous one $ \lambda_t$. The proposed approach is shown to outperform the previous approaches by a large margin."
SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"This paper presents actor - critic - free reinforcement learning algorithms for the action spaces $ \mathcal{K}$ and $ \text{text{BipedalWalker}$ with $ k$=1 $ and $ k=2 $, respectively.    The proposed algorithms are based on the following components :   1. Actor - Critic algorithm $ \cal{KT}$, which is based on $ \frac{K}{\sqrt{K } \log n}$ where $ n$ is the number of tokens in the actor's vocabulary and $ K$ is a set of tokens that needs to be observed by the critic. 2. Critic - actor - $ \log{K}\log n }, which uses $ \nabla_{k}$ to predict $ \sqrt{\text{K},\log n$ for $ k.   This paper shows that the proposed algorithms can be used in the MuJoCo and Box2D environments and achieves state - of - the - art performance in both."
SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"This paper presents a new perspective on the problem of aggregating data from different distributions in a federated model ensembles. The main contribution is a theoretical analysis of the problem from the inference perspective, which shows that data from a given distribution can not be aggregated into a single global model under certain conditions. In particular, the paper shows that the distribution of a single data point can not contain more than a few percent of the total number of data points in the federated models. The paper then proposes to aggregate the data from each model ensemble into a global model, which can then be used to regularize the model training.    The paper provides theoretical analysis on the conditions under which the model can be regularized to ensure that it is not too different from a single local model."
SP:3ac5f437fc349a33810d0645664d1c448528af74,This paper presents a multi - partite influence patterns refinement method for contextualizing anaphora. The goal is to find a contextualization model that can be used in conjunction with a number - based approach to discover new patterns.
SP:efa2343ead47263a0d09e1c17f9aa044605b9650,"This paper provides a theoretical analysis of the time - invariant update of the Lyapunov function in the context of neural networks. The main contributions are two - fold :    - Providing theoretical guarantees of the finite - time convergence of the update function of the neural network with respect to the number of iterations in the update process. - Developing a theoretical framework to analyze the convergence of this function in terms of the derivative of its update function, and providing theoretical guarantees for the convergence rate of the perturbation $ \sqrt{\theta}$, under the assumption that $ \theta$ is a convex combination of $ \mathbb{R}$-th order perturbations of the function $ \tau$. - Conducting experiments on a variety of datasets to validate the theoretical results."
SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"This paper proposes a new data - based model - based adversarial defence framework. The proposed framework is based on two components, i.e., data - free and data - driven latent variables selection.    The first component is an extension of the previous work [ 1 ]. The second component is a generalization of [ 2 ]. In the first component, the data is decomposed into a set of disentangled data points and the latent variables are selected according to a differentiable component analysis criterion. The authors show that the proposed method is more robust than the previous one [ 3 ] in terms of outlier detection."
SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"This paper proposes a new pooling method for image classification and semantic segmentation. The proposed method is based on the notion of diversified pooling, where each image is partitioned into subsets of subsets with different dimensionality, and each subsets is further decomposed using a different pooling function. The authors claim that the proposed pooling scheme is more robust than previous pooling methods such as Gumbel - pooling and gradient pooling.   The paper also presents a set of resolution and feature - to - feature challenges for the proposed method."
SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"This paper proposes a new embedding method based on a vector - valued vector - hadamard tensor, which can be used to encode the distance between two vectors with different values. The key idea is to divide the space into a set of vectors $ \mathcal{x}$ and a set $ \text{quant}$. Each vector $ x}$ is assumed to be a Gaussian random matrix whose entries are given by a $ \sqrt{1}$-norm and $ \tilde{1 }-norm. The authors show that if $ x$ is a vector with $ z_1 $ values, then $ \log(z_1)$ can be represented as a linear transformation of $ \nabla_{1 } - $ z_{\text{text}$, where $ z}$ denotes the vector $ \gamma$ and $ 1$ is the scalar vector $ z$.    The authors then show that $ 1 $ - norm vectors can be encoded with the following complexity guarantees : $ \theta_{1}-norm $ \infty$, $ \ell_2 $, $ tilde{2}$ $, and $ p_1$."
SP:f65e229bca3904095743e7a501b1083cc60f1e22,"This paper proposes a new method for learning plasticity rules from data. The key idea is to use a neural network ( RNN ) that has been trained to tolerate different kinds of perturbations in the input data.    The proposed approach is similar to the one proposed in [ 1 ] and [ 2 ], but the difference is that the RNN has the capability to tolerate data that is different from the data that was used in the previous work ( [ 1]and [ 2] ). The main difference between the two approaches is the fact that the data used for the first step is the same as the one used in [ 3 ], while for the second step it is the difference in the magnitude of the perturbation ( $ \ell_t$ ) that is used ( $ t$ )."
SP:f435530146fa975cb27cd375a857df9bcbd87682,"This paper presents a question generation framework based on the VQG paradigm, which aims to generate questions that are referential and meaningful to both humans and non - experts alike. The framework is built on top of the Graph2Seq model, which has been shown to be able to answer a large set of questions in a few hours.    The main contributions of the paper are as follows :   1. A model - free way of generating questions that is end - to - end informative, i.e. not relying on any prior knowledge about the question domain, 2. A way to map the question model to a set of reference questions and answer them in a single step, 3. A method to learn a sequence of questions by annotating each reference question with a question embedding and a relation between the question and the reference question."
SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"This paper presents a theoretical analysis of the Double - descent phenomenon, i.e., the failure of large neural networks with regularized covariates under the assumption of monotonic covariates. The authors show that the data distribution of data under regularized or under - regularized models is monotonically isotropic, and propose a risk metric that quantifies the probability that the test sample size is larger than a certain threshold. They show that this metric is robust to the choice of the regularizer and the number of samples. They also show that under certain assumptions on the covariates, the risk metric converges to monotonicity."
SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"This paper proposes a new method to learn structured and disentangled representations of video sequences for image segmentation and image processing tasks. To this end, the authors propose a hierarchical VAE model that consists of three layers : 1 ) convolutional layer, 2 ) spatial dependency layer, 3 ) decoder layer. The first two layers are used for segmentation, while the third layer is used for representation learning. The authors also propose a gating - based mechanism for spatial dependency modeling. Experiments show that the proposed method achieves state - of - the - art performance on several image classification tasks."
SP:db91512a90e75675af03c2f197751c8526d6f5e9,"This paper studies the problem of learning a policy that approximates the Q - quality of an agent's behavior under the assumption that the agent has access to a well - designed model. The paper proposes two approaches to this problem. The first approach is based on the actor - critic approach, where the agent is provided with a set of actions, and a policy is trained to learn an action that minimizes the probability that this action will be chosen as the answer to the actor's actions. The second approach is a policy - based approach, in which the policy is conditioned on a distribution of actions that is assumed to be well - approximated by the actor critic.    The paper provides a theoretical analysis of both approaches, and empirically shows that the first approach outperforms the second one both in terms of the number of actions used and the quality of the model design."
SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"This paper studies the problem of batch - selection - based optimizer for training deep learning models. The main contribution of this paper is to provide a theoretical analysis of the trade - off between batch - size and batch - quality in terms of the metric of “ parity ”, which quantifies the probability that a given training batch will converge to a target distribution that is similar to another distribution within a fixed batch size range. The paper provides theoretical results showing that the optimal batch size scales linearly with the number of samples, and empirically shows that this is inversely proportional to the batch size.    The paper also provides empirical results on both synthetic and real data to show that the batch selection selection algorithm does not converge linearly to a fixed distribution, and provides theoretical guarantees on the batch quality."
SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"This paper studies the question of whether there exist networks that satisfy the decision boundary of a dequantized neural network ( DEQ ) model. In particular, this question is phrased as follows : given an input mapping $ \ell_0 $ and an output mapping $ x_1 $, what is the probability that the outputs of these two networks are identical?    This paper answers this question by showing that there exists a deep network $ \mathbb R^n$ such that $ x_{\ell_1 } \infty$ has dimension $ \sqrt{\ell_2}$, where $ \text{xi}$ is the dimension of the input set $ z$ and $ z_1$ is a subnetwork of dimension $ z$.   The paper also shows that $ \tilde{xi } \leq \log p(\sqrt{xi})$ can be expressed as a polynomial function of $ \alpha$, and gives bounds on $ \gamma$ that depend on $ p(z_1 | z_2)$, $ z_{\alpha}$ and the dimension $ m$ of the output set $ y_1$."
SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This paper presents a goal - conditioned reinforcement learning framework with two components. The first component aims to learn a goal conditioned value function from a set of data points, and the second one aims to improve the quality of the learned value function.    The goal conditioned learning framework is built on top of the goal conditioned softmax framework proposed in [ 1 ], where the goal function is defined as the sum of two factors : ( 1 ) the distance between the data points and the goal value function, and ( 2 ) the similarity between the goal and the value function of the data point. The second component is based on the concept of bias - conditioned learning, where data points are given to the learner based on their similarity to the goal, and is conditioned on the fact that the goal is inversely related to the similarity of the two data points ( i.e., the similarity is greater for data points that are closer in distance than those that are further away from the goal )."
SP:d57550b2f323b356d7e609acc35ee33039f376b4,"This paper proposes a multi - task learning framework where the goal is to learn a bi - task relation between a set of tasks and a pre - defined set of data points. The authors propose to use a variational inference framework where each of the data points correspond to a single task or set of related tasks. The inference problem is formulated as a mixture of variational posteriors and task - specific inference problems. The paper proposes two approaches to this problem, i.e., variational single task learning ( VMTL ) and variational Multi - task Learning ( MTL ). In the first approach, the authors use a single data point per task and learn the task - related inference problem in the same way as in the second approach, where they use different data points per task to learn the relation between different tasks.    The paper also proposes a way to share the training data between the two approaches."
SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"This paper presents a series of experiments aimed at improving the quality of long - range transformer models. The experiments cover a range of length scales ranging from tens of nanoseconds to tens of tens of seconds.    The paper proposes two types of experiments :   ( 1 ) benchmarking a set of $ \mathcal{x}$ - sized $ \text{max}$ sequence lengths, and ( 2 ) comparing a subset of these sequences with a given set of quality metrics. The paper claims that the two approaches are complementary and complementary, and that the combination of the two experiments leads to better quality metrics than a single approach alone."
SP:e12e410c3335b76133ceda4c865b244fbbab8580,"This paper studies the problem of language - agnostic source - code summarization, and proposes a model - based training framework that can be used to bridge the gap between source - only and language - specific representations of the same source code.   The main contribution of the paper is a set of experiments that compare the performance of the proposed framework against baselines that are based on different source - language representations ( e.g. text - only or code - only ) of the source code, as well as a set that is based on the relative distances between source and target language representations. The paper shows that the proposed approach outperforms baselines based on language - only representations, and also outperforms baseline - based representations based on both source - level and code - level representations."
SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"This paper presents a sound - based navigation framework with the goal of learning to navigate in a world that is unmapped. The framework consists of three components : 1 ) sound source, 2 ) visual source, 3 ) sound - world 3D visual source. The sound source component is designed to provide the user with a set of sound sources and visual data that can be used for navigation. The visual component provides the visual data to guide the user through the world. The goal is to learn a policy that allows the user to navigate the world in a manner that is sound - safe and visual - safe.    The paper presents the following contributions :   1 ) A sound - source component that provides the source data for the navigation. 2 ) A visual component that helps the user understand the world - view of the source. 3 ) A policy that guides the user in learning the visual component."
SP:23bfe317dcef00a91ea92389b3f39d9b93972454,This paper proposes a new way to improve the initialization of networks with the goal to make them more robust to small perturbations. The main idea is to replace the standard initialization with a gradient descent descent approach.    The main contribution of the paper is a proof that the gradient descent approach is more robust than the standard one by showing that the difference between the two approaches is at most $ \ell_1$.
SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"This paper presents a series of benchmarks for supervised learning ( SSL ) approaches with the goal of regularizing the quality of the supervised learning data collected by the learner. To this end, the authors provide a regularization metric that quantifies the amount of data that needs to be collected in order to ensure that the learned function is not too different from the original data. This metric is then used to regularize the SSL approaches with respect to the original and supervised data. The authors also provide a proof that the SSL approach with the regularized data is more efficient than the naive SSL approaches without regularization."
SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"This paper proposes a meta - learning algorithm that optimizes risk minimization in both online and few - shot settings. The paper provides a theoretical analysis of the learning rate of the algorithm in terms of the number of iterations and the level of optimality of the algorithms in the two settings. In addition, the paper provides empirical evidence that the rate of learning in the online setting is higher than that in the zero - shot setting."
SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"This paper studies the problem of constructing transformers that can be used to represent a set of n - grams. The main contributions are :    ( 1 )   the construction of a generic set of tokens that can represent a given n - gram structure, ( 2 ) the design of a framework that allows to embed such a set in a closed form, and ( 3 ) the development of a language that supports such representations.   Experiments are conducted on the following tasks : ( a ) representational invariance, ( b ) distance metric, ( c ) structure invariance and ( d ) position invariance. The proposed framework is shown to outperform the existing works on all three tasks."
SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"This paper presents a GAN - like approach to the problem of generating high - fidelity vector - based representations for classification tasks. The proposed approach is based on the idea of training a feature encoder and a supervised discriminator that learns to synthesize representations for a given set of problems from a set of data points. The discriminator is trained using a mixture of supervised and unsupervised approaches. The supervised approach consists of two stages : ( 1 ) training the discriminator and ( 2 ) supervising the encoder to produce representations that are good enough to be used by the classification task.    The first stage consists of : ( i ) training a discriminator to generate representations that satisfy the following conditions : ( ii ) distilling the data set to a fixed resolution, ( iii ) back - tracking the vectors that are back - propagated to the target domain, ( iv ) and ( v ) predicting the representations that will be used in the supervised approach to solve the classification problem. The authors provide empirical results showing that the proposed approach achieves state - of - the - art performance in terms of both the quality of the representations produced and the number of supervised examples."
SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"This paper studies the problem of verifying the continuity of a linear program with respect to the number of variables in the program. The main contribution of the paper is a theoretical analysis of the problem and a practical implementation of this problem. The theoretical analysis is based on the assumption that the program is invariant under the action of a finite set of variables $ \ell_t$, where $ t$ is a linear function of $ t$.    The main contributions of this paper are as follows :   1.   A theoretical analysis showing that the linear program can not be made invariant to any more variables than $ T$ ; 2. ) A practical implementation that satisfies this assumption is given, which is shown to be equivalent to a trade - off between invariance under $ T$. 3. ) Experimental results show that the proposed approach can be used to verify the invariance of the program against different choices of $ T $ ; 4. ) An empirical study is provided to show that this approach is more robust than previous approaches."
SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"This paper presents a framework for fine - tuning text - to - text transformers for self - supervised learning of commonsense knowledge about text representations. The framework is based on a generative model ( PTLM ) and a contrastive model ( CALM ). To this end, the authors first fine - tune both the generative and contrastive parts of the model. The generative part fine - tunes the model to be more generative, and the contrastive part is to make the model more reasoning - able. The CALM model is fine - tuned to be less generative. The authors also provide a set of experiments to validate the effectiveness of the framework."
SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"This paper presents the results of the Robust Robust Object Discovery Experiments ( RDEs ), which aims to discover and characterize physical objects that can interact with the environment. The experiments are divided into two parts. The first part is designed to discover objects that are partially occluded by the environment, and the second part is to discover real objects that interact with both the environment and the objects in the scene.   Experiments were conducted with the following components :    1. Robust object segmentation, which consists of three stages : 1. object identification, 2. object classification, 3. object dynamics, and 4. object interaction."
SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"This paper studies the problem of adversarial robustness of convolutional neural networks ( DNNs ) against white - box and black - box adversarial attacks. In particular, this paper focuses on the question of how to obtain data - free samples that are robust to the noise level / amplitude of the adversarial attack.    The paper proposes two approaches to this problem. The first approach is based on data augmentation of the original DNN model with a set of $ \ell_0.3 $ random noise - based augmentation samples. The second approach uses a differentiable classifier that is trained on top of the augmentation data. The paper shows that the proposed approach is more robust than the previous one."
SP:276ffd59fbf49e3ee02756da8920218102214917,"This paper proposes a new model distillation method for teacher - agnostic classification tasks. The main idea is to distill the knowledge about the teacher model to the student model by distilling the image and text classification data from the two datasets. The paper also proposes a mirror descent technique to train the model on the intermediate targets.    * * Contributions * * :   1. This paper proposes to distil the knowledge distillation to the teacher and student model using the mirror descent method. 2. To train the teacher, the paper first distills the text and image dataset from the same source and then distills both the source and target datasets separately. 3. For the image distillation, the teacher distillation is first distillation from source to student model, and then the student distillation takes the source to teacher model."
SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"This paper proposes a new channel pruning method for hyper - structure networks. The main idea is to divide the network into two layers, each of which has a different number of layers and a different amount of resources. The goal is to minimize the difference in the number of resources between the two layers.    The main contribution of this paper is to show that there exists a constant factor $ \ell_0 $ such that for any $ t \infty$, there exist $ t$ such that the total number of resource terms in the second layer is at least as large as that in the first layer."
SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"This paper presents a proof - of - concept approach to the problem of reinforcement learning in the context of higher - order logic. The approach is based on the premise that the goal of a learner is to find a set of examples such that the learner can infer a lower - order logical result from the examples.   The proof relies on the following steps :   1. The learner first selects a subset of examples from the set of possible examples. 2. For each of these examples, the proof is performed on the basis of a probabilistic model that assumes the existence of a closed - form upper - bound for the probability that the hypothesis is true. 3. The proof is then performed on top of the probabilistically chosen examples."
SP:88209417a8ad07e6103084e41709be900303ce5f,"This paper proposes a data augmentation approach to improve the performance of deep learning models under data transformation operations. The proposed approach is based on the concept of space augmentation, which is defined as replacing a subset of the input data with a set of data that satisfies a certain set of metrics. The paper proposes two types of data transformation approaches : space - space and space - agnostic data transformation.   The paper provides a theoretical analysis of the proposed approach and shows that both space and agnostic transformations are equivalent to each other in terms of the number of metrics and the amount of resources required to perform the transformation."
SP:6d84670d321b0d584b097c630574bd748e85c9a2,"This paper studies the convergence of three - layer networks under the assumption that the number of layers in the network is less than $ \ell_1 $, where $ \infty$ is the dimension of the input set $ \mathcal{L}$ and $ \text{x}$ is a factor of $ \alpha$-th layer embedding.    The main contributions of the paper are as follows :   1. This paper provides the first global convergence result for three - layers networks under nonlinear and nontrivial dynamical assumptions. 2. The paper provides convergence guarantees for multilayer networks in the same way as in [ 1 ]. 3. This is the first time such a result is given for three layer networks."
SP:b90f893f927db9c439595fd119a565cf43c971f4,"This paper proposes an inverse reinforcement learning approach for policy evaluation in a real - world healthcare setting, where the goal is to learn a policy that minimizes the cost - benefit tradeoff between the expert's actions and the actions provided by the provider. To this end, the paper proposes a combination of two approaches : ( 1 ) a policy evaluation approach based on the counterfactual reasoning approach, and ( 2 ) an action - based evaluation approach, based on a policy evaluative approach. The proposed approach is evaluated on both real and simulated medical settings, and achieves state - of - the - art performance in both cases."
SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"This paper studies the problem of learning a continuous control algorithm that updates the state of a set of tasks in time $ t$ according to a data - dependent metric $ \mathcal{data } \log n}$, where $ n$ is the number of actions in the set $ t$.    The paper proposes a message - passing algorithm that iteratively updates the states of all the tasks in $ T$ by sending messages to each of them at timesteps $ t(n)$. The paper shows that this approach is data - efficient and provides a policy that can be used to ensure that the updates are not too different from each other. The paper also shows that the policy is also data - invariant.   Finally, the paper provides a theoretical analysis of the data - dependence of the stability of the proposed algorithm."
SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper presents an answer to a question posed in VQA 2.0 : given a set of datasets $ \mathcal{x}$, what is the maximum number of objects $ \cal n$ that can be produced in a given dataset $ n$? This question was posed in the context of the following set of questions :    1.   \theorem 1. \ell_tilde{x } \inftyq_{\mathcal n } \times n_{\textrm{x}}$, where $ n_t$ is the number of elements in $ \theta$, and $ z_0 $ is the cardinality of the set $ z$. The paper answers this question by showing that ( 1 ) there exists a set $ \sqrt{\theta}$ such that $ z_{\cal n}$ can be generated in $ n+1 $ steps, and ( 2 ) there exist sets $ \sigma^n$ $ such that for any $ n>1 $, there exist $ n-1 $ elements $ \tilde { x}$ satisfying the following conditions : ( a ) for every $ n > n$, there exists $ n(n)$ elements $ n < n+\sqrt{x},$ ( b ) for each $ n<1 $ element $ n=1 $ ( c)$ there exists an element $ c(n+1 ) such that if $ n \in$ is greater than $ c$, then $ n/n < n$ contains elements of size $ n^n < 1 $ ( < n>0 $ ( e.g.1 )   The paper concludes that the answer to this question is a closed - form answer to the question posed by Qiu et al. ( 2009 )."
SP:c64e77507e562f236cb69361b22fb1a7951ffb22,"This paper studies the problem of learning a convex model - based poisoning attack framework that is provably convex, and provides theoretical guarantees on the maximum loss difference between two classes of data poisoning attacks that can be used to improve the quality of the learned model. In particular, the paper focuses on the case where the data is assumed to consist of a set of data points and the attacker has access to a subset of these data points.    The main contributions of the paper are as follows :   1. Providing theoretical guarantees for the convergence of the loss difference difference between the two classes under the assumption that the data points are assumed to be convex. 2. Further providing sufficient conditions on the classifier used to ensure the convergence. 3. Demonstrating that the obtained theoretical guarantees are provably valid in the case of convex data - based attacks."
SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,This paper studies the problem of learning point clouds on point - point applications with resource - constrained devices. The paper proposes two approaches to this problem. The first approach is to learn a point cloud model that satisfies a constraint on the dimensionality of the data points in the cloud. The second one is to save data points by minimizing the information entropy of the point cloud.    The main contributions of the paper are as follows :   1. A theoretical analysis of the two approaches. The main result shows that the first approach has a lower entropy than the second one. 2. A proof of the lower entropy approach is provided.
SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"This paper presents a multi - hop reasoning approach to improve the performance of Transformer - based encoders for natural language processing ( NLP ) on the GLUE benchmark. The approach consists of three steps :    1. Conducting a series of experiments on a set of natural language modeling tasks ( e.g., answering questions in wikipedia, inferring the grammar of the target language, constructing tokens from wikipedia ). 2. conducting a series   of attention - based experiments on the same set of tasks, i.e., answer questions in the wikipedia context and construct tokens from the source - target context. 3. conducting an analysis of the attention patterns generated by each of these actions.   The paper concludes with a discussion about the impact of each step on the performance."
SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper proposes a new contrastive learning framework based on clustering space and space - based representation learning. The proposed framework has two main contributions : ( 1 ) to improve the quality of the prototypes provided to the learner, and ( 2 ) to increase the likelihood of the estimation of the network parameters in the learning process. The paper provides a theoretical analysis of the proposed framework and shows that the proposed method is more transferable than the existing contrastive clustering - based framework ( PCL ). Experiments are conducted to validate the theoretical results."
SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,This paper studies the problem of learning to tolerate perturbations in neural networks with respect to the number of parameters. The authors propose a multi - path neural network ( OMP ) block - based approach to this problem and show that it is more robust than previous works on whitewashed and black - box neural networks. They also show that the OMP is more resilient to perturbation in terms of the amount of parameters that needs to be corrected.
SP:776df66274ed12449fde8dcef873a593980f397c,"This paper proposes a new attention mechanism that can be used to supervise the design of new types of attention mechanisms. The attention mechanism is based on the similarity between the characteristics of a set of sources and the goal set of the supervised task. The goal is to design a new type of attention mechanism such that the supervision task is easier to solve. To achieve this goal, the authors propose two types of supervision task :    * self - supervision task * and * supervised task recipe *.   The first task is to predict the source characteristics of the source and the target set. The second one is to provide a supervision task that the attention mechanism can be applied to the new source and target sets. To solve both tasks, the attention mechanisms are first designed and then supervised on the source data. The supervision task recipe is then used to design the new attention mechanisms and the attention recipe is used to solve the target task."
SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"This paper proposes an agent - to - agent approach to automatically identify and treat a new type of disease in a data - driven medical system. The proposed approach is based on two modules : ( 1 ) the diagnosis module, which aims to train an agent to identify new types of diseases that are likely to cause a new health problem, and ( 2 ) the learning module that trains the agent to decide whether to treat the new disease or not.    The main contributions of this paper are as follows :   1 ) The authors provide a set of metrics to evaluate the quality of the diagnosis provided by the two modules. They show that both the diagnosis and the learning metrics are inversely proportional to the number of samples used in the decision - making process. They also show that the learned metrics are not affected by the amount of data used to train the agent, and that the metrics do not suffer from the presence of extrinsic factors that hinder the agent from making a correct diagnosis."
SP:10ae09d90d465125433a9b4f15b1405ab017920d,"This paper proposes a new approach to solve the confusion - based energy - based classification problem. The proposed approach is based on batch - wise regularization of the visual classification dataset. The main contributions are as follows :    1. Introducing a new concept of batch confusion norm ( BCN ), which is defined as the probability that a given image is a subset of a given set of head categories. 2. Using this concept, the authors propose two strategies to deal with the entropy - based confusion problem : ( 1 ) to distinguish the batch confusion from the real - world confusion ; ( 2 ) to mitigate the long - tailed nature of the distribution of visual classification.   Experiments are conducted on CIFAR-10 and ImageNet datasets to demonstrate the effectiveness of the proposed approach."
SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"This paper proposes a new variant of the Variational Reward Imitation Learning ( VRIL ) approach to the reinforcement learning problem, where the goal is to learn a policy that minimizes the risk of failure of a given policy in the presence of an unknown environment. The paper proposes two approaches to this problem, i.e., a variational approach and a reward inference approach.    The first approach is based on minimizing the posterior distribution of the expected reward of a policy over a set of data points sampled from a distribution over environments that are assumed to be similar to the data set sampled from the reward distribution of a toy data point. The second approach is an extension of the variational reward approach proposed in ( Xie et al., 2020 ) to the case where the environment is assumed to consist of a large number of environments ( e.g. $ \mathcal{x}$ ) and a small number of policies ( $ \cal n$ ). The authors prove that both approaches are equivalent to each other in terms of risk minimization, and that the risk - minimization approach is more risk - effective than the reward - maximization approach in the case of the toy data."
SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"This paper proposes a belief search method that aims to find the optimal set of agents in a multi - agent environment. The key idea is to train the search algorithm in a way that ensures that the learned belief is not too different from the one obtained in the initial stage of the search. To this end, the paper proposes to divide the search space into two stages : initial stage where the search is conducted in a closed - form, and a second stage where it is performed in an open - form. The paper also proposes a way of training the belief search in a manner that is more data - driven and less data - dependent."
SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"This paper considers the following question : what is the best way to estimate the number of steps in a multi - step expansion of the search space $ \sqrt{\sqrt{x}$? This question is phrased as follows : given a set of $ \mathcal{x } \log n}$ points $ \infty$ and a fixed $ \tilde{x}\log n$, how many steps do you need to take in order to get a good estimate of the total number of queries?    This paper answers this question by showing that for any $ \text{xi}$, $ \nabla_{\text{x}}$ and $ \gamma_t$, there exists a $ \log(x\log n)$ such that $ \sigma(x)$ is greater than $ \sum_{\log(z}$ where $ z}$ is a positive integer and $ z$ is the size of the set of points $ z$.   The paper then goes on to show that there is a trade - off between $ \ell_t \log ( z ) \log p(x)\log n(z)$, where $ p(z ) = \sum_t(x \log z)$ where z is the set size of $ z $ and $ Z$ is an integer.   Finally, the paper shows that for $ \lambda_t $, there exist trees $ \leq \log t(x^2 $ that can be used to estimate $ \pi(z+\log z$ and get $ \alpha_t ( z)$."
SP:5efc271ccc555fd9aa542548838170bd4c98e957,"This paper presents a series of experiments aimed at addressing the problem of understanding the source - to - target flow of knowledge in the natural language processing ( NLP ) community. The paper proposes three approaches to this problem, i.e., data - driven, knowledge - augmented, and knowledge - transfer - based approaches.    The first two approaches are based on data - augmented NLPs, where data points are generated from a pre - trained network, and the third approach is based on learning the content knowledge of the source and target networks, which is then used to train a neural network for the next set of tasks. The experiments show that the proposed approaches outperform the baselines in terms of the number of trained tasks and the amount of data used for training. Moreover, the paper also shows that the learned knowledge can be leveraged to improve the performance of the network in a number of downstream tasks."
SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"This paper studies the following problems :    1.   - Given a smooth smooth vector flow path $ \mathbb{R}$ and a data set $ \text{data}$ $ \infty$, what is the best way to learn the gradient flow $ \rho_{\text{text}$ along $ \nabla_{\tilde{text } } $?   2. - What is the optimal way to minimize the cross - entropy loss $ \sqrt{x}$.   3. - This paper shows that there is a solution $ \sigma_0 $ such that $ \log(x)$ can be approximated by $ \tilde{\text{\text}$. 4. This paper gives a lower bound on the learning rate $ \epsilon$ for this problem and shows that it is at least as good as the previous upper bound $ \epsilon$."
SP:c71f9d2a602516865a0b103028186e83b52e5f00,"This paper proposes a training framework that addresses the problem of collapse of generative adversarial networks ( GANs ) under the assumption that the generative models collapse under certain conditions. To this end, the paper proposes two approaches : ( 1 ) sample - based training and ( 2 ) evaluation - based approach. The first approach is based on the FID framework, while the second one uses IS framework. Experiments are conducted to evaluate the effectiveness of the two approaches."
SP:52c48198c95826e042f9e5a512ef3265daaff882,"This paper proposes a new regularization method for the learning of metric policies in the context of reinforcement learning. The key idea is to learn metric policies based on the concept of score - importance ( score is defined as the ratio of the expected value of a metric policy over the value of the sub - metric policy it is meant to replace ).    The paper proposes two approaches to this problem. The first approach is based on learning metric policies from scratch, and the second one is a combination of metric - based policies and heuristic policies."
SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"This paper studies the problem of learning a robot that satisfies the consistency constraint, i.e., that is, the robot must not deviate from its initial state more than a certain set of times over a fixed period of time. To this end, the paper proposes two approaches : ( 1 ) simulating the initial state and action trajectories of a given robot with randomly collected data, and ( 2 ) fine - tuning the policy of the robot to ensure that it obeys the constraint.    The first approach is based on a monocular video - based simulator and ( 3 ) a policy - based approach is used to fine - tune the policy. The second approach is a combination of the above two approaches."
SP:006434d56992836ab9420d7d4215bc70664de304,This paper studies the problem of explainability of data on the manifold and off - manifolds. The main contributions are :   ( 1 ) provide a theoretical analysis of the manifold explainability problem and show that it is equivalent to the off - manifold one. ( 2 ) provide sufficient conditions for the data to be explainable.
SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,"This paper proposes a reinforcement learning approach for a multi - agent RL agent that learns to take actions in the presence of a non - stationary and controllable world state model. The goal is to learn a policy that maximizes the agent's own reward while minimising the value of actions taken by the controlled agent. To this end, the paper proposes two approaches : ( 1 ) a learned trajectory - based approach based on the observation of the world state of the agent, and ( 2 ) a controlled agent that takes actions based on trajectories of other agents in the world.    The first approach is based on taking actions from other agents and learning a policy based on their trajectories. The second approach uses a learned policy that takes into account the fact that other agents may take actions that are different from the one that is assumed to be optimal. The authors prove that both approaches are equivalent to each other in the sense that the learned policy maximises the agent ’s reward while minimizing the cost of taking actions."
SP:c239bc531bcf7293032748af29a1b786e9d893dd,"This paper presents a visual representation learning framework for semi - supervised classification and object detection. The framework is based on VOCAL VOC, which has been shown to achieve state - of - the - art performance on a variety of visual classification tasks. The main contributions of this paper are as follows :    1. An improved label assignment strategy that is more data - efficient and less biased.   2. A regularization term regularization strategy that aims to ensure that the learned classifier is not too different from the original classifier."
SP:d18bab21790713e2facb053c47298fc9079ab783,"This paper considers the following games :    1.   $ \ell_tilde{O}^2 $, where $ t_t$ is a convex function defined on $ \mathbb{R}$ and $ \text{W}$ is an iterate of $ \alpha$ with $ \beta = \sum_{t_t+\alpha } \infty$, and $ p_t = \alpha$.   2. assume that $ \gamma$ is convex and $ w$ is strongly convex, and that $ w_t^2$ satisfies $ \sqrt{\frac{O}{\sqrt{W}}$ where $ w_{t-\alpha}$ can be expressed as a sum of two factors : ( 1 ) $ \theta$ and ( 2 ) $ p_{t -\theta}$ are convex functions defined on a $ \lambda$-th dimension $ \tau$. The authors prove the following convergence rates for $ \eta$ under these assumptions : $ p(\theta^2)$, $ p(\alpha)$ and   ( 2)$ are smooth functions satisfying $ \sigma^3 $, $ \mu_t + \mu_{t } \leq p(\tau)$. Theorem 2. provides sufficient conditions for the convergence of the above rates under the assumption that the function $ \phi$ is also convex ( and satisfies $ w^{t-2}$."
SP:bbc7f77308b298c332a39747f693bc396f00a89f,"This paper proposes a new method to train unsupervised voice, face, and handwriting encoders for user verification. The key idea is to train the encoder and decoder with a linear combination of linear combinations of user - defined vector embeddings and to provide private and secure training of the vector embedding and the decoder.    The paper provides a theoretical analysis of the proposed approach and shows that the proposed method is more secure than the existing approaches."
SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"This paper studies the robustness of neural network classifiers trained on multi - way class boundary manifolds. The authors prove that the training set size of a neural network is not a measure of its robustness, but rather the ratio of its dimension to the dimension of the input space class partitioning space. Moreover, the authors show that the number of classes per training set is not an indicator of robustness."
SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"This paper proposes a new entropy - aware approach to estimate the level of uncertainty in the entropy of a given state under the assumption that the state is not too different from the previous one. The key idea is to use a differentiator between the state - level entropy ( SAC ) and the level - level uncertainty ( EI ). The paper proposes two approaches to this problem. The first approach is based on the idea of “ soft actor - critic ”, where the goal is to find a state that is similar to the previous state ( e.g., the one that has the lowest entropy ), and the second approach is to learn a policy that minimizes the entropy difference between the two states.    The paper provides a theoretical analysis of the two approaches and empirically shows that the EI approach outperforms the other two approaches in terms of EI and EI."
SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"This paper proposes a meta - reinforcement learning framework for distribution - based reinforcement learning ( DRL ) with a policy - based relabeling framework. The authors provide a series of experiments to evaluate the performance of the proposed framework on a set of distribution tasks.    Experiments are divided into three parts :   1. Experiments with different distribution tasks, 2. experiments with different models, 3. experiments on different relabeled examples."
SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"This paper proposes a self - paced learning ( ISPL ) framework that aims to solve the overfitting problem of learning from data that contains a large amount of label noise. The paper proposes two approaches to this problem. The first approach is to train a set of data points with different levels of noise, i.e. $ \ell_t$, $ t$ and $ p_t$. The second approach uses a different level of noise ( $ p_{t } = \ell_{t}$ ) and a different set of parameters ( $ t_{t},$,   $ t_0 $, $ \infty$ ).    The paper shows that both approaches fail to converge to the same solution under the same set of noise levels, and the authors propose a way to mitigate this problem by sampling data points from different levels with different amounts of noise. In the experiments, the authors show that the proposed approach is able to solve some of the underfitting problems."
SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"This paper investigates adversarial perturbations to the training distribution of the ResNet50 neural network. The paper shows that the training distributions of the original and the modified versions of the network are not identical, and proposes a metric to measure the difference between the two distributions. The metric is based on the similarity between the features in the training data and those in the test data.    The paper also shows that there is a distribution shift between the training and test distributions, which is attributed to the fact that the trained version of the model is more stylized than the un - trained version. The authors show that this is due to the different stylization of the data in the two models, and propose a metric that measures the difference in the stylized and un - stylized distributions."
SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"This paper proposes a new metric metric, VoG, that quantifies the similarity between two distributions of the same metric ( e.g. $ \tilde{x}$. The metric is defined as the ratio of the expected value of the two distributions over a set of metric points $ x$.    The authors claim that this metric can be used as a metric to quantify the human - in - the - loop ( HIL ) quality of data produced by a machine learning system.   This paper provides a theoretical analysis of VoG metric and shows that it is equivalent to a metric that measures the difference between the value of a given metric and the one produced by an expert ( i.e., a human ) who has access to the same data distribution."
SP:074bfacc75837bb19049be8a2890e10de073dd8e,This paper proposes a new method for generating samples from a generative model that is regularized according to the flow of f - divergences. The method is based on the idea of Optimal Transport ( DOT ) and Driven Latent Sampling ( DDLS ). The main contribution of the paper is a proof - of - concept that the samples generated by DOT and DDLS are close to each other in terms of the quality of the generated samples.
SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"This paper presents a series of experiments on cross - lingual pre - training and generation tasks for transformer - only and transformers - only models. In particular, the paper focuses on the following :    1. Pre - training the model on a pre - trained dataset, 2. Conducting experiments on the Transformer - only benchmark, 3. Conduct experiments on a set of transformer variants, 4. Conducted experiments on transformer variants and 5. Conducts experiments on generation tasks."
SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"This paper presents a series of experiments aimed at providing a reinforcement learning ( RL ) objective that motivates the learner to explore the physical world more effectively. In particular, the paper focuses on the following :   1. Exploring the world with visual cues : 1. Showing how visual cues can be leveraged to motivate exploration ; 2. Introducing a learning model that leverages visual cues to encourage exploration ; 3. Providing a reward function that encourages exploration.   Experiments are divided into three parts : ( 1 ) a simulator - based learning, ( 2 ) a goal - based exploration, and ( 3 ) a reward - based approach. The goal of each part is to maximize the difference between the expected value of the learned goal and the one that is empirically proven to be attainable by the agent. The paper presents results that show that ( 1. ) visual cues are more effective than ( 2. ) in motivating exploration ; ( 3. ) learning - based approaches are more efficient than ( 4. ) rewards are more valuable than ( 5 ) rewards when the agent has access to a large number of examples."
SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper presents a multi - modal data - based representation learning framework with the goal to provide a more discriminative and contrastive learning approach. To this end, the paper proposes a pseudo - label - wise hashing algorithm that can be used to assign pseudo - labels to each data point in the representation space. Experiments are conducted on ImageNet - CIFAR-100 and VGG - Sound benchmarks to demonstrate the effectiveness of the proposed framework."
SP:4df640f502e88ddba2d7e183625231d70b083e82,"This paper proposes a novel approach to the supervised metric learning problem. The main idea is to provide a low - level image similarity and high - level semantic similarity between two sets of data points in the supervised segmentation space. To this end, the authors propose a few different approaches :   1 ) image similarity - level classifier model, 2 ) class - wise feature - wise classification model, 3 ) cross - domain classification model and 4 ) annotation - level classification model.   Experiments show that the proposed approaches achieve satisfactory results."
SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,This paper proposes a distillation strategy for self - supervised learning based on small scale models. The main idea is to distill a small set of instances from a dataset to a larger dataset and then evaluate the quality of the two sets by comparing the representation obtained by the two approaches. The authors claim that this approach is more data - agnostic and can be used to improve the convergence rate of supervised learning.    The main contributions of the paper are as follows :   1. A distillation method that learns from a small number of instances to obtain a representation that is better than the one obtained from a large number of examples. 2. A method that distills a small amount of examples to a large set and then evaluates the representation produced by both approaches on the same set.
SP:328866aad6544c81ded8980934df31dc4472435f,"This paper proposes a new approach to estimating the posterior distribution of the likelihood of a model - based posterior estimation of the probability that a given point in a high - dimensional space is located in a lower dimensional space. The proposed approach is based on the following assumptions :    1.   \the posterior distribution over the space $ \theta$ of a set of points in $ \mathbb{R}^n$ is well - calibrated \infty$   2. \the prior distribution over $ \text{theta}$ over the set $ \sqrt{\theta } \log n$ is not too different from the prior distribution $ \nabla_{\theta}\log n(\theta)$. \theorem 1.1 assumes that the posterior distributions of the lower - dimensional spaces $ \tau$ and $ \mu$ of the higher - dimensional data are similarly well calibrated \log N(\tau)$.   Theorem 2.1 claims that the prior distributions of $ \\text{Theta}\log n(\tau ) \log ( \mathbf{ Theta})$ are not different from each other \log(\sqrt{Tau})$ under the assumption that $ \alpha$ is a lower bound on the number of particles in the lower dimension $ \gamma$ and that the upper - bound on $ \eta$ is upper bounded by a constant factor $ \log \log p(\infty$. \tilde(\gamma)$ is proven to be true under the above assumptions. \inversely proportional to $ \lambda$ for the lower and upper - bounded data sets. \log { Theta}(\lambda$, \log tau \log P(\log p(\gamma\log N)$ where $ p(\sqrt {Tau}$ is the dimension of the data set and \lambda\log P(\gammas)$ the upper bound is the prior of the posterior of a lower - bounded set of particles ( $ \sum p(\lambda)$. Theorem 3.1 shows that this approach is equivalent to the prior - based approach of [ 1 ] and [ 2 ]."
SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"This paper proposes a score - based approach to estimate the cross - entropy between two cosmological models ( e.g., the standard cosmology and the quantum mechanical model ). The key idea is to use the fact that the two models are not identical, i.e., one model assumes that the other model has the same cosmology, and the other one assumes the other has a different cosmology.    The main contribution of this paper is to provide a score estimator for the difference between two models, which can be used to make inferences about the cosmology of the second model."
SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"This paper presents a set of experiments aimed to evaluate the effect of interventions in reinforcement learning ( RL ) in the setting of real - world environments. The experiments are conducted on the benchmark EARL1, where the agent has access to real world data ( e.g., the agent is asked to solve a given task ) as well as a simulated version of the same. The paper shows that interventions can have a significant impact on the performance of the algorithm in both the real world and simulated settings.    The paper also presents results on the impact of interventions on the learning time and the amount of supervision provided by the agent. The results show that interventions do not have a major impact on performance in the simulated setting, but they do affect the performance in real world settings."
SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,This paper proposes a question answering ( QA ) framework that leverages knowledge - aware neural networks ( GNNs ) to improve the QA capability of open - book and NLP - based QA systems. The proposed framework consists of three main components : ( 1 ) GNN - based question answering modules ; ( 2 ) neural - counter - based modules ; and ( 3 ) question - reasoning - aware modules.    The main contributions of the paper are as follows :   - Providing a set of well - designed and well - crafted GNN modules that can be leveraged to improve QA capabilities ; - Conducting a series of experiments to demonstrate the effectiveness of the proposed framework ; - Providening a large set of benchmark datasets to demonstrate that the proposed approach can be used for improving the performance of QA - based systems ;
SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"This paper proposes a three - stage compression framework for DNNs. The three stages are :    ( i )   < Sec. 1 > Sec. 2 > Coding Sec. 3 > Inference Sec. 4 > Data Structures Sec. 5   Sec. 6   \infty Sec. 7 > Transforming Sec. 8   Qualitative Constraints Sec. 9   This is done in three stages. The first stage is to provide a set of data structures for the inference stage ( Sec. 10 ). In Sec. 11, the data structure is transformed to a form that can be used for the next stage of the inference. The second stage, Sec. 12, is to obtain the representation for the third stage.   The authors provide a theoretical analysis of Sec. 13 and Sec. 14, and show that Sec. 15 can be translated to Sec. 16 in Sec. 17."
SP:94c395afc794a9cc163e362078769ff83f3d20d0,This paper studies the problem of under - fitting of small neural networks trained with data augmentation and dropout techniques. The main contribution is a theoretical analysis of the problem and a practical solution to it.    The main contributions are :   1. A theoretical analysis showing that the problem is not over - fitting for small networks ; 2. A practical solution for large neural networks to solve the problem ; 3. An empirical study showing the effectiveness of the proposed solution.
SP:9c24549b980e415616f818acbf4cf680ef8edb52,This paper proposes a framework for generating dynamic point cloud sequences that can be used for the learning of temporal coherence and coherence of a dynamical system. The proposed framework is based on the TPU - GAN framework that has been previously proposed in [ 1 ] and [ 2 ]. The main contribution of this paper is the introduction of a learning task that aims to learn a coherence metric that quantifies the difference between the representation of a point cloud point sequence and the one obtained by a human action scanned from the source point cloud. The paper also proposes a masking module that is used to ensure that the learned metric is not too different from the original point cloud representation.
SP:67efe60ad37807505369b7852bc0abed29ffdda8,"This paper presents a series of experiments aimed at bridging the gap between transformer - only and object - only training paradigms for object detection in ImageNet classification. The paper proposes a pre - training strategy that consists of two stages :    1.   a large - scale training phase where the model is trained on a large dataset of object - related data, and 2. a smaller - scale phase where a small - scale dataset is used to train the transformer model on top of the object detection dataset. Experiments show that the proposed strategy outperforms the previous works on both metrics."
SP:a1f9897496303984fc7ad469222106b14b4a6233,"This paper studies the communication complexity of convex and nonconvex convex learning algorithms. The authors prove that the number of communication rounds required for each iteration of the convex iteration is at most $ \sqrt{\sqrt{delta}$, where $ d$ and $ \tilde{D}$ are the communication length and the length of the shortest path, respectively. Similarly, the authors show that the same is true for the non - convex iterations of the page - level navigation algorithm in the setting of SGD - SGD. In particular, they show that $ \sigma(delta)$ can be expressed as a sum of $ \mathcal{Delta}^2 $ where $ \delta$ is a convex factor, $ \log d$ is the distance between the origin and the destination, and $ D$ is an integer that measures the speed at which information is exchanged between the source and destination ( $ delta$ ). The paper also shows that for the case when $ d = SGD-1 $, $ d\log delta$."
SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"This paper studies the problem of adversarial training of deep neural networks in the space of nonlinearity of the input perturbations. Specifically, the paper focuses on the question of how to train a neural network to ensure the robustness to different types of perturbation in the geometrical structure of the domain under adversarial attacks. To this end, the authors propose two approaches : ( 1 ) boundary - linearity analysis and ( 2 ) decision - boundary analysis. The first approach is based on the fact that the input space of the adversarial examples can be decomposed into a set of proximal class labels and ( 3 ) makes a decision on whether to label the examples based on these class labels or not. The second approach is built on the notion of boundary - smoothness. The authors prove that both approaches are equivalent to each other under certain assumptions."
SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"This paper proposes a contrastive learning approach to improve the quality of the learned representations in the context of supervised learning approaches. The proposed approach is based on the idea of constructing clusters of representations that can be used to infuse information into the representations. To this end, the proposed approach first constructs a set of supervised representations and then uses a clustering approach to extract information from each of these representations. The learned representations are then used to label the representations to be used in the downstream supervised learning approach. Experiments are conducted on a variety of supervised and unsupervised learning approaches to evaluate the performance of the proposed method."
SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"This paper studies the following problem : given a set of data points $ \mathcal{x}$ and a given set of parameters $ \theta$, what is the best algorithm that can recover the given data points with the highest probability?    This question is phrased as follows : given $ x$, $ y$ and $ z$, the following algorithms are considered :   1.   * * Based on the following : * * Given a set $ \text{x } \in { x, y }, $ z$.   $ \sum_{\theta } \log p(x, y)$, assume that $ \sqrt{\theta}$ is a positive integer, and $ \infty$ is the probability that $ z is greater than $ \alpha$, and that $ y \leq \log n\infty$. * * The goal is to learn the following algorithm with high probability : $ \tilde{x}\in { z } \to \mathbb R^2 $, where $ \eta$ is an integer such that $ x\in { y}$ can be recovered with a high probability given $ \nabla_{\sqrt{x}}$ ( $ \ell_\text{z}$ ) and $ y\to \log r^2$ can not be recovered ( $ z \to 0 $, which is a negative integer ). * * * This paper studies this problem from the perspective : given some $ \lambda$ and some $ z $, the goal of the following is to find an algorithm that recovers the data points within a certain error interval $ \log(sigma)$ given $ y$. The goal of this algorithm is to minimize the error between $ \gamma$ and the value of $ z(x)\log(y ) $, i.e., the ratio between the error of the learned algorithm with respect to the data point $ z and the error obtained by the given algorithm with the same $ \sigma$ under the following assumptions : $ y(\theta)$   \in \mathbf{x},$ \log sigma$,   [ 1 ] \log ( \log y)$."
SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"This paper proposes a new action space representation for reinforcement learning ( RL ) based on continuous and discrete action space embeddings. The authors claim that the continuous action space space provides a more decodable representation than the discrete space space space, and the continuous space space is more homogeneous than the continuous one.    The authors also propose a VAE - based RL algorithm to learn the continuous and continuous space representation."
SP:5128bf712f6b197de113c7a371b4bec36f978eca,"This paper studies the energy stability property of convex stochastic gradient descent methods for nonconvex convex optimization problems. The main contributions are :    1.   The authors prove that the convex gradient descent method converges to the energy - stable constant in the energy range $ \epsilon$, where $ \epsilon$ is a convex constant and $ \ell_0 $ is a non - convex factor. This is the first time that this property has been proven for convex learning benchmarks, and the second time that it has been shown to be also true for nonlinear convex constants. 2. The authors provide a theoretical analysis of this property, and show that it is equivalent to an energy - stability property in the nonlinear setting. 3. They provide an empirical evidence that the gradient descent approach converges more quickly than the gradient ascent approach, and that the difference between the two approaches is inversely proportional to the number of iterations."
SP:11f49b0a975be87769be29e85d7e3924699cf2c9,This paper proposes a training framework for non - autoregressive ( NAR ) data without distillation. The proposed framework consists of two components : data distillation and data training.   The first component distills the data into NAR models and trains them to be human - level accurate. The second component trains the data to be NAR counterparts.
SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"This paper presents a neural network - based approach to address the problem of time - to - time ( T to T ) efficiency in neural networks. The main contribution of the paper is a time - aware architecture for neural networks that is based on the concept of temporal convolutions.    The paper provides a theoretical analysis of the proposed approach and empirically shows that the proposed architecture is able to achieve txt - time - constant ( txt / txt ) efficiency on a wide range of neural networks with low - power local signal processing ( e.g., $ \mathcal{L}$ ) and low - memory ( $ \cal{T}/\sqrt{T } )."
SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"This paper provides a set of theoretical and empirical guarantees on the fairness of machine learning ( ML ) algorithms under the following assumptions : ( 1 ) data distribution distribution of student data, ( 2 ) algorithmic performance of algorithms, and ( 3 ) student - teacher behavior of ML algorithms.   The main contributions of the paper are as follows :   1. Providing theoretical guarantees on data distribution and ( 4 ) providing empirical guarantees of fairness under the above assumptions."
SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,This paper proposes a neural - based stochastic dual dynamic programming ( SDDP ) problem solving framework. The proposed framework is based on V - functions and aims to learn a neural model that can be used to solve a variety of real - world optimization problems. The main contributions are as follows :    1. This paper provides a theoretical proof that the proposed framework can solve $ \ell_sqrt{\sqrt{d}$ problems in $ \mathcal{D}$-dimensional space $ \infty$ with $ \text{d\infty}$ being a linear function.   2. The paper shows that the learned neural model is able to solve high - dimensional and long - horizon problems $ \nabla_{d}/\ell_symmetric $ \log{d}\infty$.
SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"This paper proposes a privacy - preserving next - token prediction mechanism based on differentially private corpus - language models. The authors claim that the privacy of such models is not well protected under the current data - extraction attacks. To address this issue, the authors propose a different data - dependent privacy accounting mechanism. The proposed framework is based on the following two parts :    1. Each language model has its own privacy protection mechanism, 2. each language model can be used for different kinds of attacks. Experiments are conducted to show the effectiveness of the proposed privacy protection."
SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,This paper proposes a new smoothing - based k - NN density estimator based on an unsupervised method. The proposed method is based on the smoothing of the distribution of a set of data points by a pre - defined smoothing model.    The authors claim that the proposed method can be used to estimate the k - nN density of the data points with high probability.
SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"This paper presents a representation learning framework for differential differential equations that can be used to improve the quality of the representation learned in adversarial training. The key idea is to learn a representation of the time domain by solving a set of differential equations over the space of time $ t$ and space $ d$ variables $ t$. The authors claim that this representation is better than the representation obtained by solving the corresponding differential equations in the un - diffusion - based setting.    To achieve this goal, the authors propose two approaches : ( 1 ) to learn the representation of time - domain differential equations, and ( 2 ) to train a generative adversarial model. The first approach is based on image classification and yields state - of - the - art classification accuracies. The second approach uses score matching to obtain a representation that is more suitable for unsupervised learning."
SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"This paper presents a goal - conditioned reinforcement learning ( RL ) algorithm for navigation and manipulation tasks. The goal is to find a policy that maximizes the distance between the current state and a goal that is reached in the next step. To achieve this goal, the goal is conditioned to be close to the goal at each step, and the policy is also conditioned to plan to reach the goal within a certain period of time.    The authors provide a number of metrics to evaluate the quality of the policy, policy, and planning. They also provide a set of benchmarks that measure the agent ’s ability to navigate through the maze of long - range behavior. They show that the goal conditioned policy achieves better performance than policies that do n’t have this property."
SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"This paper proposes a regularization approach to improve the quality of the cross - entropy data augmentation used in supervised learning. The main contributions are as follows :    1. Introducing a new metric metric, _ mixup metric _, which measures the distance between two points in the training dataset if they belong to the same mixup class. This metric can be used to distinguish between different mixup classes.   2. Using this metric, the authors propose a new regularization procedure to regularize the cross entropy of the training data. The regularization is based on the fact that cross entropy is a metric that measures how far the data points are from each other. 3. Extensive experiments are conducted to validate the effectiveness of the regularization."
SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,This paper presents a deep neural network based approach for natural language understanding ( NLI ) based classification. The proposed approach is based on a kernelized classification layer with a high - dimensional embedding space and a representation learning backbone. The authors show that the proposed approach achieves state - of - the - art performance on a variety of natural language processing tasks.
SP:01ee8ec81619784788eb0ce9785098e437d17a7c,This paper proposes a new GNN - based data augmentation framework for real - world graph neural networks ( GNNs ). The proposed framework is based on the contrastive learning framework proposed in [ 1 ] and [ 2 ]. The main contribution of this paper is the introduction of a new metric to quantify the difference between the representations learned by the two approaches. This metric can be used as a metric to measure the difference in the quality of the learned representations.    The paper also proposes a few data augmentations strategies to improve the performance of the proposed framework. The experiments show that the proposed approaches outperform the state - of - the - art approaches.
SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"This paper considers the problem of estimating the effect of two - stage regression under the assumption that the two stages of the regression are identical. The paper proposes two approaches to this problem : ( 1 ) to estimate the effect in the first stage and ( 2 ) to balance the effects in the second stage. The first approach is based on minimizing the variance of the covariance of the two variables ( i.e., the difference between the two regressors ), while the second approach uses the fact that one of the regressors is more informative than the other.    The paper provides theoretical guarantees for both approaches under the following assumptions :   1. The covariance distribution of the second variable ( ii ) must be balanced with respect to the first variable ( iii ). 2. There must be a separability between the effects of the first and second stage in the case of the linear and nonlinear setting. Theoretical guarantees for the second assumption are provided in the form of an upper bound and a lower bound."
SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"This paper proposes a gradient descent - based adaptive learning ( NAL ) method for solving linear regression problems. The proposed method is based on a two - layer neural network with two layers : $ \alpha$ and $ \beta$. The first layer is used to classify the input images $ \mathcal{L}$, while the second layer is trained to predict the $ \text{x}$-th image $ \gamma$.    Experiments are conducted on both easy and hard regression problems, where the goal is to find a solution that is adaptive to changes in the input $ \lambda$ $ and $ z$. The proposed NAL method is shown to achieve state - of - the - art performance on both hard and easy tasks."
SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"This paper proposes a new source separation algorithm based on linearized linearized minimization ( LPALM ) for solving multispectral sensing problems. The authors provide a theoretical analysis of the convergence rate of the proposed algorithm and show that it converges to the optimal solution in a finite number of iterations. Moreover, the authors propose a training loss function that can be used to choose between unrolled and unrolled versions of the algorithm."
SP:7716315001949ab88c8a216302fe51bae872fc87,"This paper proposes a self - attention module for the study of the properties of the world in the era of the big bang. The main idea is to model the world as a collection of infinitely many entities, each of which is endowed with a finite amount of data. Each entity is assumed to be of size $ \ell_t$, and to have a property property ( e.g., size of the largest set of particles in the universe ) that can be measured by the ratio of its size to its total number of constituents.    The paper proposes to use the notion of "" size "" as a metric to measure the size of entities in the world, and a "" knowledge "" metric that measures the total amount of information encoded in each entity. The paper also proposes a "" memory "" metric, which measures how much information is encoded in a given entity, and how much it takes to retrieve it."
SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"This paper proposes a new GAN - based keypoint encoding and decoding scheme, which is based on a two stage conditioning scheme. The first stage is used to train a keypoint encoder and decoder, while in the second stage, the encoder is trained to decode a background encoding and a set of images.    The keypoints encoding scheme is trained on top of a GAN, and the images are conditioned on the source and target vectors. The encoder, decoder and the decoder are trained on the same network. The main contributions of the paper are as follows :   1. The keypoint embedding scheme is shown to be more stable than the background encoding, and can be used to improve the quality of keypoint detection. 2. The image conditioning scheme is also shown to outperform the background encoder."
SP:9206ae6e31077569313838504ef6daa89ad3b59c,"This paper provides a theoretical analysis of the initialization of fully - connected neural networks in the context of the exploding network setting. The authors show that the naive initialization of neural networks with different values of the preactivation parameter can lead to different initializations with different velocities, leading to different trajectories of the network explosion. The paper also provides a perturbative analysis of this phenomenon."
SP:2177be818b5843c580c787f1b2d725154846feb6,This paper proposes a constant learning rate schedule for improving the quality of the update step sizes of the search approaches for deep learning across models.    The main contributions are as follows :   ( 1 ) a constant update step - size schedule for the update steps of deep learning approaches ; ( 2 ) a regularization scheme to ensure that the updates are not too different for different models ; ( 3 ) an improvement in the test accuracy of the updated step - sizes for each model ; and ( 4 ) a reduction in the batch - size loss for all models when the number of models is increased.
SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"This paper provides a theoretical analysis of gradient descent and contrastive gradient descent ( NCE ), a well - known loss landscape that has been used successfully in various applications in various domains. The main contribution of this paper is to provide theoretical guarantees for the convergence rate of NCE gradient descent under the following assumptions :    1. There exists a $ \ell_t$-th dimension $ \infty$ such that $ \tilde{x}$ is not too different from $ \mathbb{R}$.   2. There exist sufficient conditions under which the gradient descent of $ \nabla_{t } \leq \sqrt{\sqrt{r}$ can be made consistent with $ \text{max}$. 3. For any $ t$, there exists a set $ \alpha_t $ such that gradient descent converges to $ \log(t\infty)$ with a probability $ \epsilon$ \theta where $ \eta$ is the number of iterations of $ t$. 4. For $ t=1 $, there exist a set of conditions on $ t $ such as $ t \lequivare $ \gamma_{t}$ and $ t\lequiver$ $ g_t$."
SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"This paper provides a theoretical analysis of the SGD - SGD algorithm under different kinds of attacks. The main contributions are :    1. Providing theoretical guarantees for the convergence of SGD with respect to the number of attacks and the amount of data used in each attack.   2. Demonstrating that SGD can be made "" resilient "" against different types of attacks under certain assumptions."
SP:bc783f0c829f90931535e63687d13172879631b3,"This paper proposes a multi - extent composition method to learn representations of source code snippets and tree representations. The method is based on the idea of pattern encoding and decoding, where a source code is first decoded into a set of tokens, and then a subset of those tokens are used to decode a tree representation of the source code. The authors claim that this approach is more data - efficient than the single - extent encoding approach proposed in [ 1 ] and [ 2 ]. The paper also proposes a sample matching approach to improve the quality of the representations obtained by the method."
SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"This paper proposes a framework to study the relationship between the structure of the data generated by a generative model and the quality of the generated sequences. The main idea is to use sequence data as a constraint to ensure that the generated sequence is not too different from a human - like structure. To this end, the authors propose two metrics : ( 1 ) high - level structure of sequence data and ( 2 ) relational constraints on the sequence data.    The main contribution of this paper is to show that the two metrics are inversely proportional to the ratio of the number of elements in the sequence and the length of the sequence."
SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper studies the problem of learning hypergraph models to predict set - to - hypergraph representations from low - dimensional input data. The paper proposes two approaches to this problem. The first approach is based on learning a hypergraph model $ p(x, y)$, where $ y$ is the dimension of the input data and $ z$ is a set of hypergraph features $ z_t$. The second approach uses a refinement method to learn $ z_{\textrm{x}$ and $ y_{\tilde{x } \log p}$ by matching $ z^{(x,y)$ to $ y_t$. The paper shows that both approaches converge to the same set of solutions when the number of input dimensions goes to infinity. However, the paper also shows that the refinement method does not scale well to infinity, which motivates the paper to propose a memory - based refinement method.   "
SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"This paper studies the vulnerability of a pre - trained model to the presence of gender bias in the embedding space of the trained model. Specifically, the authors consider the case when the model is assumed to be a mixture of two types of humans : a male and a female, and show that the loss of the model with respect to the two categories can be as large as $ \ell_sqrt{\sqrt{n } \infty$, where $ n$ is the number of the humans in the model and $ n\infty $ is the size of the embeddings space.    To address this problem, the paper proposes two approaches : ( 1 ) to train the model in a manner that is more sensitive to the appearance of the female than the male, and ( 2 ) to allow the model to be trained in a way that is less vulnerable to gender bias. The first approach is based on embedding the model into a space of embedding spaces of $ n \ell_{\text{n}$, $ n_\sqrt { n}$ and $ N_\pi$, and training the model on $ n(\text{infty}/n)$. The second approach uses a neural network that is trained on top of a set of data points $ n^{-1}$ such that $ n(n^{-2}$ is not too different from $ n$.   The authors show that both approaches fail to achieve the same performance under the same set of assumptions."
SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,This paper proposes a data - based incremental learning ( CIL ) method that uses a stream - based approach to distill class knowledge from image stream data. The authors provide a theoretical analysis of the proposed method and empirically show that the CIL method outperforms other methods in terms of the number of new classes learned and the amount of data used to train the algorithm. The paper also provides empirical results showing that CIL outperforms a number of existing methods such as image - based methods and image - free methods.
SP:506e0a888c03a955b708464eed3670c04baf4912,"This paper proposes a new sampling strategy for improving the quality of the data sampled from a stochastic chain monte carlo ( SCMC ) proposal. The proposed approach is based on the following steps :    1.   * Introducing the concept of an energy function ( EFE ), which is defined as the difference between the expected value of a distribution over a set of points in the interval $ \mathbb{R}^n$ and the probability that there exists a point $ \infty$ such that $ \theta$ is greater than $ \sqrt{\theta}$ for all $ n$ points in this interval, 2. * Specifying the distribution $ \text{\theta } \log n$ over $ \tau$, and providing a sampling strategy to ensure that it is less than $ n\sqrt{\tau}$.   3. * Providing a sample strategy that is robust to changes in the eFE, and that is not too different from the one used in the previous sampling strategy ( MCMC ). The main contribution of this paper is to show that this sampling strategy can be improved to a point where it is no longer susceptible to the catastrophic error of sampling from distributions that are too large or too small compared to the ones used in MCMC."
SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"This paper presents a reinforcement learning approach for learning representations of video features. The key idea is to learn a probabilistic inference system that can be used to predict the state - space boundaries of a set of video events. To this end, the paper proposes two approaches. The first approach is based on a model - based reinforcement learning. The second approach is a combination of two approaches : ( 1 ) learning of the underlying representations and ( 2 ) learning the event detection mechanism. Experiments show that the proposed approach achieves state - of - the - art results.    * * Contributions * * :   1. Discovery and learning of an underlying spatiotemporal hierarchy. 2. Learning the detection mechanism and the event boundaries."
SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,"This paper presents a multi - stage image retrieval and local feature learning framework, which consists of three stages : ( 1 ) global features retrieval pipeline, ( 2 ) supervision pipeline and ( 3 ) local feature matching pipeline. The global features and supervision pipeline consists of two stages, i.e., global features - supervision and global information - fusion pipeline. In the supervision stage, the global features are stored in the server and the local features are fed into the server. The local feature - matching stage is the same as in the global feature - supervision stage. The attention stage is similar in the sense that the attention is fed into both server and local features.    Experiments show that the proposed framework achieves state - of - the - art performance."
SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"This paper presents a theoretical and empirical analysis of the convergence rate of gradient - based multi - label classification tasks ( MTL - G ) in terms of the number of labels and the complexity of the gradients used to train them. In particular, this paper focuses on the following :    1.   The authors show that ( i ) the amount of labeled data used in the training process is increasing exponentially with the size of the training set ; ( ii ) the total number of labelled data used for each task is increasing linearly with the network size ; ( iii ) the magnitude of gradients required to solve a given task is decreasing with increasing number of labeled datapoints ; ( iv ) there is a trade - off between the number and complexity of labels used for training. The paper provides theoretical analysis of these issues and provides theoretical guarantees for each of these problems. Empirically, the paper shows that the proposed method is able to converge faster than previous methods ( e.g., SGD - G, GPT-2 ) in most cases."
SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,"This paper addresses the problem of layer - wise model fusion framework bridging the gap between data - free and data - driven model fusion frameworks. The authors propose a new bridging framework, CIFAR10, which is based on the idea of teacher - student model fusion and distillation, where the teacher is provided with a small amount of data, and the student is provided a large amount of training data.   The authors show that the proposed bridging method is able to solve the cross - layer alignment problem without losing performance."
SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"This paper presents a theoretical analysis of the difference learning problem in the setting of deep reinforcement learning ( RL ) with implicit regularizer and action pair representations. The authors show that under certain assumptions on the representations of the regularizer ( e.g. $ \ell_t$ ) and action pairs $ t$, the difference between the two sets of solutions can be bounded by a constant factor $ \sqrt{\theta}$, where $ \theta$ is the distance between the set of solutions and the original set of actions.    The authors provide a theoretical justification of this result in terms of the following assumptions :   ( 1 ) $ \tilde{t } \in \mathbb{R}$ is a convex combination of $ \alpha$ and $ \gamma$, and ( 2 ) the action pair $ \text{t}$ can be re - re - defined as $ \sum_{t}^n$ where $ n\infty$ is $ \infty$."
SP:6fd793b27123bf80504e2ad5957455b7ec311612,This paper presents a hyper - model - free exploration method for solving linear neural network problems. The hypermodel consists of two components :    ( 1 ) an actor - critic and ( 2 ) an action - value function.   The first component provides samples for exploration. The second component provides control samples for bonus and randomized exploration.
SP:b428383660928374c953f659ea1e05852dbdcd6e,"This paper presents a representation learning approach based on causal graph neural networks that aims to improve the generalization ability of graphical representation learning approaches. The authors propose two approaches : ( 1 ) causal graph - based representation learning and ( 2 ) causal information - inspired representation learning. In the first approach, the representation is extracted from a set of causal graph representations and is then used to train a graphical model. The second approach is a data - augmented version of the former, which is used to learn the representation from a larger set of representations. Experiments show that both approaches outperform the baselines in terms of sample complexity reduction."
SP:1258c05a80a17949b50e6dae13deea1d2235f456,This paper proposes an approach to reduce the cost of federated learning in terms of compression and distillation. The proposed approach is based on the idea of segmentation - based generative models. The authors provide a theoretical analysis of the proposed approach and empirically demonstrate that the proposed framework can be applied to various image classification and medical image segmentation tasks. The paper also provides a proof - of - concept example to demonstrate the effectiveness of the approach.
SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"This paper studies the bounds on the Rademacher complexity of two - layer neural networks in the presence of adversarial attacks. Specifically, the paper considers the following settings :    1.   standard adversarial settings where the attacker has access to a set of data points $ \mathbb{R}^n$ and a training set $ \text{text}$ of $ \ell_1$.   The paper shows that the complexity of the two networks is bounded by $ \sqrt{\ell_2}$, where $ \infty$ is the number of $ n$-norms and $ \tilde{text } $ is the dimension of the input $ n$. The paper also shows that for $ n=1 $, the two neural networks can not be made to be as complex as $ \nabla_{\ell_3 } \log n(\sqrt{text})$ where $ n(n)$ is an upper bound on $ n\infty$. This is in contrast to the previous works which showed that $ n(\sigma n)$ can be made as complicated as $ nabla_4 $ by training two networks with $ n>1 $ and $ n<2 $."
SP:925d6bb051e9b384669fb695085b678c11f7c11a,"This paper proposes a kernel - based estimator of differential entropy, which can be used for fine - tuning classification tasks. The main contribution of the paper is to provide a theoretical analysis of the convergence of the estimator with respect to the number of data points and the dimensionality of the data set. The paper also provides a theoretical proof that the entropy estimator is a good general purpose estimator, and an empirical proof that it is not a good estimator for certain classes of networks."
SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"This paper studies the problem of improving the state - action space coverage of the greedy operator in the context of action - value - based RL. The paper proposes two approaches to this problem. First, the paper proposes a new exploration hyperparameter, $ \max_t$, which measures the distance between the current state and the current greedy operator $ \sqrt{max}$, and $ \tilde{max } \infty$, that measures the gap between current and future actions in the space $ t$. The paper shows that $ \mathbb{T}$ can be approximated to $ \sigma^2 $ with $ \ell_t$. The second approach is to learn a policy $ \text{t}$ that approximates $ \nabla_{t } \log(t)$ and $ t$, where $ t \log t$ is the current value of the action $ \theta$. The authors show that both approaches are equivalent to each other in terms of minimizing the gap. The authors also propose a policy iteration strategy that iteratively learns a policy that minimizes $ \alpha^2$ with $ t+1 $, and an off - policy that maximizes $ t + 1 $."
SP:792ae8808aa6902758146aef1548c975492b833c,"This paper proposes a new learning mechanism, Learnability Lock, which aims to provide a universal transformation function that can be used to control the learnability of a set of data formats under the assumption that the data formats used for the transformations are not differentiable.   The key idea of the approach is to use the concept of “ knowledge distillation ” introduced in [ 1 ], where a data format is assumed to be invariant to a certain set of transformations, and then to allow the data to be transformed under a certain perturbation that depends on the data format ( e.g. $ \ell_t$, $ \tilde{x}$ ). The paper shows that this can be viewed as a learning mechanism that is able to prevent the data from deviating from the invariant set of the data under the perturbed data format."
SP:9af10703605e620e563241e2602a50b629f3d37a,"This paper presents a theoretical and empirical study of the energy minimization problem for graph neural networks ( GNNs ) with respect to different types of features and applications. In particular, the paper focuses on node and edge - level features and shows that there are two types of applications ( i.e., energy - differential equation and energy - convex differential equation ) for which there is no known empirical result. The paper also provides a theoretical analysis on the relation between these two kinds of applications.    The main contributions of the paper are as follows :   1. Providing theoretical analysis of the two cases and showing that there is a relationship between them. 2. Demonstrating that both cases are equivalent to each other."
SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"This paper presents a theoretical analysis of the problem of learning from data sets that are not labeled. The main contribution of the paper is that it provides a theoretical framework for designing a data pool that can be used for both active learning and supervised learning. Specifically, the paper focuses on the setting where the goal is to find a set of data points such that the supervised learning algorithm is able to learn from data points that are within a certain budget.    The main contributions of this paper are as follows :   1. A theoretical analysis that shows that a data set that is not labeled can not be used as a data point for active learning ; 2. A collection strategy for selecting data points in the active learning framework that is labeled so that supervised learning algorithms can learn from it ; 3. A sample selection strategy for supervised learning that uses labeled data points to improve the quality of supervised learning ; 4. Experimental results that demonstrate the effectiveness of the proposed approach."
SP:4c72923f78ca6590dc11e10d1a2403076a583718,This paper proposes a new method to generate graphs of human reference sequences for novo novo genome assembly. The method is based on a greedy search algorithm that iteratively finds sequences from a graph convolutional network that has been trained on a large set of generated data. The goal is to find sequences that are most likely to be related to a given human sequence. The authors claim that this approach is more efficient than previous approaches that use a fixed set of sequences and a fixed number of queries.
SP:24de906e4289c9073b6c55c747b0913b8df5e053,"This paper proposes a buffer - buffer approach for continual learning of meta - representations. The buffer is built on top of a pre - trained meta - learning model, and is used to sample from a set of representations at different timesteps ( e.g. $ \ell_t$, $ t$ ) and at different learning rates ( $ t_t$. ). The idea is to ensure that the representations learned at each timestep are not too different from each other, and that the learning rate does not deviate too much from its pre - training rate.    The paper also proposes a sampling strategy that is based on the idea of cross - entropy buffer ( CS buffer )."
SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"This paper considers a multi - agent credit assignment problem, where each agent is provided with a credit assignment, and each agent has to decide whether to cooperate with another agent to obtain the credit assignment. The paper considers the problem as a joint learning problem where the goal is to minimize the total amount of credit that needs to be transferred between the agents in order for the agent to achieve its credit assignment goal. In this paper, the authors propose to train the agents to solve the problem jointly, and then assign the credit to each agent based on the value of the other agent's contribution to the problem.   The paper provides a theoretical analysis of the problem in terms of the Q - learning problem, and shows that it is possible to solve it in a closed form. The authors also provide an empirical study of the convergence rate of the proposed method."
SP:0d2b225ac697679d10df25f371b2a718d4949b42,"This paper proposes a new framework for improving the robustness of the Transformer - based learning - based adversarial training framework. The proposed framework is based on the GPT-2 framework ( GMSA ), which has been proposed in [ 1 ], [ 2 ] and [ 3 ]. In this paper, the authors provide a theoretical analysis of the model space attack and the learned - based defenses in the framework of the GMSA framework.    * * key words and phrases * * : Transformer based attacks, GPT - 2 framework, T - learning based defenses, GMSA"
SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,This paper proposes a new approach to train neural networks on small minibatches. The proposed approach is based on the gradient - based normalization approach proposed in [ 1 ] and [ 2 ]. The idea is to train a neural network on a small set of examples and then train the model on a larger set of small examples.    The main contributions of this paper are as follows :   1. This paper proposes to train the network on small examples by minimizing the number of examples in the training step. 2. The paper shows that the proposed approach can be used to improve the accuracy of the model.
SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"This paper presents a theoretical analysis of the problem of pre - training and post - training of transformer - based natural language models in terms of the low - rank metric. The paper proposes two approaches to this problem, i.e., pre - pre - tuning $ \ell_t$ and pre - switching $ \tilde{x}$, where $ t$ is the number of tokens in the input sequence and $ \text{x } $ is the size of the pre - trained set of tokens.    The paper also proposes a strategy to pre - train the language model $ \infty$ by storing the tokens in a memory buffer $ \mathcal{T}$ and fine - tuning the parameters $ \nabla_{t } $ of the model $ t$. The paper shows that both approaches fail to achieve the same performance under the same set of assumptions, and proposes a way to trade - off between the two objectives."
SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"This paper proposes a language - free neural model that satisfies a set of constraints on the language - based dependencies of the model. In particular, the paper focuses on the following :    1. Constraints that the model must satisfy in order to be able to decode data in a language other than the one under which the model is trained.   2. A set of non - trivial relations that must be satisfied by the model in order for the model to satisfy the above mentioned constraints. 3. A constraint on the number of tokens in the model that needs to be decoded to ensure the model satisfies the above constraints."
SP:74c186a96c12adff178264aa84ace8d04dc7d725,This paper proposes a new approach to improve the performance of deep neural networks with a camera - based vitals measurement. The idea is to train a neural network with a convolutional layer on top of a transformer - based neural network and perform a series of vitals - based interventions on the top of the transformer layer. The experiments show that the proposed approach improves the performance on a variety of different deep neural network architectures.
SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"This paper studies the problem of prioritizing the amount of data that needs to be pruned / updated during the end - to - end network deployment phase of a given application. To this end, the paper proposes two approaches : ( 1 ) prioritize the number of datasets that need to be updated / pruned during the deployment phase, and ( 2 ) optimize the ratio between the total number of data changes required to ensure accuracy ( i.e., the ratio of the total amount of information needed to update / update the entire network ) and the required amount of resources to update the network during deployment. The paper provides empirical results showing that the proposed approaches achieve better performance than the baselines in terms of both ( i ) and ( ii )."
SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"This paper proposes a new graph generation method based on energy - based models ( EBMs ). The main idea is to use a permutation - invariant energy function as the objective. The authors show that this energy function can be used to generate graphs satisfying the following properties : ( 1 ) it is not divergent, ( 2 ) it does not depend on the dimension of the graph space, ( 3 ) there exists a set of graphs such that the energy function of each graph can be expressed as a sum of the energy functions of all other graphs in the set, ( 4 ) there exist a graph such that each graph has the same energy function, ( 5 ) for any graph $ \mathcal{O}$, there exists an energy function $ \alpha$ such that every graph $ O$ can be written as a product of $ \beta$ and $ \gamma$."
SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"This paper presents a policy - based program synthesis framework that is built on top of top - k search - space blowup. The framework has two components : a policy and a data - driven program synthesis strategy. The policy is based on a search strategy that selects a subset of programs that has the highest probability of success in the search space. The data - guided policy is built from a set of data points that are generated by a pre - trained policy that is guided by a learned programming model.    The paper presents empirical results that show that the policy is able to generate programs with high - probability in search space under the following assumptions : ( 1 ) that the number of programs in the program space is less than $ \ell_1 $, ( 2 ) that there is no data point in the domain that is not part of the policy space that has a lower - probability than the policy policy, and ( 3 ) that no data points in the data space have been generated by the policy that has not been guided by the learned program synthesis policy."
SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"This paper studies the problem of improving the quality of the knowledge provided by target - network based learning methods. Specifically, the paper focuses on the following :    1.   The authors provide a theoretical analysis of the prior and current state - of - the - art value - based training methods in terms of the squared Bellman error ( squared error / squared value ) and the quality - to - value ratio ( DQN / QN ) metric. 2. They provide theoretical results showing that the current knowledge - based methods do not provide a satisfactory answer to the question of how to improve the quality / value ratio. 3. They propose a new training method that provides a better answer to this question. 4. They show that the proposed method is independent of the set of parameters used to train the networks."
SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"This paper proposes a learning framework based on graph neural networks ( GNNs ) for the Lehman - Lehman classification problem. The proposed framework is based on the idea of local isomorphism between graph structures and the goal is to ensure that the learned model is not too different from the original one. To this end, the authors propose a message - passing framework that aggregates information from different subgraphs of a GNN in a way that ensures that the learnt model is similar to the original model. The authors also propose a learning aggregation scheme that allows the GNN to be used for different types of classification tasks. Experiments show that the proposed framework outperforms existing approaches on a number of benchmark datasets."
SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"This paper proposes a new method for quantifying the uncertainty in the cross - entropy squared error ( CE ) between two neural networks ( NNs ). The authors claim that the existing approaches do not satisfy the crossing property and propose a new approach that satisfies it. The key idea is to divide the neural networks into piecewise linear regions and assign each region a different CE score, and then to use a weighted average of the squared error between the two networks to obtain a PI interval that can be used to test the uncertainty of the two NNs. To this end, the authors propose a series of different PI intervals and a set of different loss functions.   The main contributions of this paper are as follows :   1. Introducing a new metric that quantifies the CE between two nNs. This metric has the property that it does not depend on the number of nNs but only on the squared errors between them. 2. Developing a distribution - free PI method that satisfies this metric. This method is called PI3NN. 3. Conducting extensive experiments to validate the quality of the obtained PI intervals."
SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"This paper proposes FOML, a meta - learning method that adapts to different types of supervision problems in real - world settings. The main contribution of the paper is the introduction of the concept of boundary boundaries, which is used to define the set of problems that can be handled by the algorithm. The paper also proposes a sampling scheme that allows the algorithm to adapt to different kinds of problems at different timescales.    * * Contributions * * :   1. This paper proposes a learning method to adapt the parameters learned by the meta - learner in a different type of supervision problem. The idea is to divide the problems into discrete and continuous versions and to learn the boundary between the two types of problems by sampling a subset of the solutions from each of them. 2. The authors provide a theoretical analysis of the problem boundaries and show that the proposed method is able to learn a good balance between the number of solutions and the complexity of the problems. 3. They provide empirical results that show the effectiveness of the proposed approach."
SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"This paper presents a gradient - based approach to the problem of designing functional molecules with respect to a set of differentiable generative models. The proposed approach is based on the differentiable scaffolding tree ( DST ) framework, which has been proposed in [ 1 ], [ 2 ] and [ 3 ].    The key contribution of this paper is the introduction of the DST framework in [ 4 ], which is used in [ 5 ] as a basis for the gradient based optimization approach in [ 6 ]."
SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"This paper proposes a knowledge - augmented approach to improve the quality of the responses of medical systems by augmenting the knowledge about the source and target of a set of data with the representation of a patient and a lab response.    To this end, the paper proposes to use a combination of two approaches : ( 1 ) a personalizedized version of the data augmentation approach and ( 2 ) a cross - entropy - augmented version. The proposed approach is based on the idea of “ knowledge augmentation ”, where the data is augmented with a representation of the source / target of interest, and the knowledge of the target is used to extract the information about the sources / targets of the test responses. The authors claim that this approach leads to better quality results than the previous approaches that do not include this information augmentation."
SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"This paper proposes a cross - domain generalization approach for the classification of unknown classes in real - world domains. The proposed approach is based on cross - match method, which is a multi - binary classifier with the goal to identify classes from a set of source and unknown classes from the same datapoints. The main idea of the approach is to augment the data augmentation strategy of the source domain with the knowledge of the classes in the unknown domain.    The main contributions of this paper are as follows :   1. Introducing cross - cross - datastream method for the class identification in the real world domain. 2. Demonstrating the effectiveness of the proposed approach on a variety of domain classification tasks."
SP:126f8ffb855aa22eda4d681a499953879ed3679e,"This paper studies the problem of policy optimization in the context of Robotic locomotion tasks. The paper proposes a duality between policy gradient methods ( SPO ) and trust - region methods ( TTR ). SPO is motivated by the fact that policy updates tend to diverge faster than trust region methods, and TTR aims to bridge the gap between the two. To this end, the paper proposes to combine SPO with TTR. The main contributions of the paper are as follows :    1. Introducing SPO, a new metric that quantifies the divergence speed of policy updates in terms of the difference between the expected value of a policy update and the value of its trust region counterpart. 2. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach."
SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"This paper studies the problem of information distillation in neural network training. The paper proposes two approaches to this problem, i.e., learn - and - relearn and learn - to - fail, respectively. The first approach is to train a neural network with a fixed number of iterations, and the second one is to remove undesirable information at each stage of the training process. This paper analyzes the relationship between the two approaches, and finds that both approaches fail to deliver desirable information at the beginning of the learning stage, and that the information is not removed at the end of training.    The paper also analyzes how the different approaches affect each other in terms of the amount of undesirable information that is removed during the training stage."
SP:2789859517b6624730b14a7e010444a72d3dd3ed,"This paper proposes a new way to collect data from RL agents in an online manner. The goal is to provide agents with sufficient coverage to cover a large variety of domains in an offline manner. To this end, the paper proposes two approaches : ( 1 ) collect data in a data - driven manner, and ( 2 ) collect agents in a manner similar to batch RL.    The first approach is based on the idea of “ data - collected RL ”, where a collection of agents is collected at each time step ( e.g., at time t, t+1 ), and each agent is given access to a subset of the data collected by the other agents in the previous step. In the second approach, the data is collected in a way similar to a batch RL setting, where each agent has access to only a small fraction of the available data in the other step. The paper shows that both approaches lead to similar results."
SP:76625a25e770415599a34122110d61cb3b7e614c,"This paper studies the problem of meta - learning of generalization algorithms over domains. The authors first prove bounds on the discrepancy between the meta - learner and the domain - invariant generalization ( DG ). Then, they show that the DG bounds can be improved to $ \ell_sqrt{\sqrt{DG}$ by training the algorithm on a set of domain samples. Finally, the authors provide a theoretical analysis of the convergence rate of DG bounds."
SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"This paper studies the problem of planning over a set of domains, given a policy $ \theta$ and a set $ z$ of actions $ z$. The goal is to find a way to minimize the sum of the cost of the actions and the value of the action in each domain. The problem is formulated as a two - player game, where each player has access to a subset of the domains $ p(a, b)$, where $ a$ is the set of actions taken by the policy and $ b$ is a sub - domain of $ a$.    The paper proposes two approaches to solve this problem : ( 1 ) best - first search and ( 2 ) value - value search. The first approach is based on a tree - based approach, where the goal is first to find $ z $ and then to perform a value search on $ a(a,b)$. The second approach is a combination of two approaches, one based on the value network and the other on the policy network.   This paper shows that the first approach outperforms the second one in terms of search effectiveness. The paper also shows that there is a trade - off between the cost and the number of actions used in the policy search."
SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"This paper presents an approach to learn a policy from video demonstrations of a humanoid robot. The approach is based on a one - shot vision task, where the goal is to train a policy that is able to imitate the behavior of the humanoid robot, while also being able to learn the model of the robot from the data provided by the video demonstrations. To this end, the paper proposes two approaches :    ( 1 ) A - Cycle - learning, where a policy is trained on a data - augmented version of the video demonstration, and ( 2 ) Experiments - RC - Learning, where data is provided to the policy in the form of a set of video demonstrations, and the policy learns the model from these data samples.   The paper evaluates the effectiveness of both approaches in terms of the following metrics : ( a ) The quality of the data used to train the policy, ( b ) The amount of data used in the experiments, ( c ) The performance of the policy trained on the data samples, ( d ) The number of instances used in each experiment, ( e.g., the average number of times the policy was trained on each data sample )"
SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"This paper studies the problem of improving the quality of the optimizers of deep neural networks with hyper - parameterized neural networks. The main contributions are as follows :   1. This paper proposes a new metric that quantifies the amount of parameter tuning required to make the optimizer perform optimistically with respect to a given set of data points. This metric can be used to estimate the batch size of the training data points required to improve the performance of a given optimizer.   2. The paper shows that regularizing the network weights of the trained networks with a fixed number of hyper - parameters ( e.g. $ \alpha$, $ \beta$, and $ \gamma$ ) leads to a significant reduction in the batch sizes required to achieve the same performance as without such regularization. This can be regarded as an improvement in the sense that the number of parameters required to ensure optimizability of the network does not increase monotonically with increasing number of datapoints. 3. This work also proposes a way to mitigate the loss incurred by the regularization on the data points used for training the networks."
SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"This paper studies equivariant representations of groups and networks. The main contributions are :   1.    Introducing the concept of full equivariances of group equivariance, which is defined as the equivalence between any group and any set of discrete variables that can be expressed in terms of a set of high - level features   2. ivanov et al. ( 2014 ) provide a theoretical analysis of the equivariants of full and low - level CNNs and show that the latter is equivalent to the former. 3."
SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"This paper proposes a Monte Carlo ( MCMC ) method for generating generative models from large - scale datasets. The proposed MCMC method is based on the following steps :    1. initialize a latent variable model $ \mathcal{D}$ and a distribution $ \text{x}$ $ x_t$ $ y_t$. 2. perform a series of MCMC iterations on $ y_{t } \log(x_t)$ with $ \nabla_{t}$ \log{x_{t})$ and $ \tilde{x_0}$. 3. evaluate the performance of the proposed method on the image generation task.   * * Contributions * * : The authors propose a MCMC - based generative model generation method, which they call FID - based method. The authors show that the proposed FID method converges faster than previous methods on a variety of datasets, and is also able to tolerate intractable distributions."
SP:5631097031c7e599bdeae64366ffa6e4558837c6,"This paper presents a theoretical and empirical study of the problem of hyper - relational reasoning on large - scale relational graphs. The paper provides a theoretical analysis of the adequateness of different relational quantification operations ( e.g., tensors, hypergraphs ) and shows that they are suboptimal when applied to real - world knowledge graphs. To address this problem, the paper proposes two approaches : ( 1 ) learn - and - classify hypergraph reasoning on tensors and ( 2 ) train hyper - reasoning on relationships.   The theoretical analysis is based on the fact that the existing approaches do n’t provide sufficient information about the quality of the datasets used for training and inference ( i.e., they fail to distinguish between different representations of the same data ). To overcome this issue, this paper proposes to train and classify hyper - graph reasoning on a large set of datasets, and show that the resulting approaches are more data - efficient than existing approaches."
SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"This paper proposes a new algorithm for improving the top - k and top - 5 accuracy of the sorting and ranking algorithms of the world - wide web classification systems ( wSLs ). The proposed method is based on the concept of "" scale - invariant partitioning "", which is defined as partitioning the world into subsets of subsets based on their similarity in terms of the number of dimensions. The paper shows that the proposed method can be used to improve the accuracy of both top-1 and top-5 accuracies.    The paper also proposes a "" fine - tuning "" method for improving top-k accuracy. The authors show that this method is more scalable than the baseline - based sorting method of ResNeXt-101 WSL."
SP:cb3188f435c54a365890e20e4d582c250d919833,"This paper studies the following problems : given a set of $ \mathcal{T}$ points $ \ell_t$ and a $ \text{text{T } \infty}$, what is the fastest way to get $ \tilde{T}\infty$ to $ t$?    This paper answers this question by using the following two techniques :   1. Douglas - Rachford splitting technique   2. Sinkhorn method   3. dual stopping technique.   The main contributions of this paper are as follows :"
SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"This paper presents a theoretical analysis of the gap between distributions of federated data and natural data. The paper shows that the distribution of natural data can be partitioned into equal parts, and that there exists a gap between the natural and federated distributions. To address this problem, the paper proposes a gap - narrowing strategy, which is based on minimizing the difference between the distributions of natural and Federated data. Experiments are conducted to validate the theoretical results."
SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"This paper proposes a zero - shot prompt - based learning framework for language understanding tasks. The framework consists of two components : prompt - free and prompt - guided learning. The prompts - free setting is used to train a language model ( LMs ), while the prompts - guided one is used for fine - tuning the capacity of the model ( PLMs ).    The paper presents a series of experiments to evaluate the effectiveness of the two approaches. First, the authors show that PLMs trained in the zero - shots setting outperform PLMs that are trained on the fine - tuned language model. Next, the paper presents results on a set of 5 different language models trained on a dataset from the IMDB dataset, and shows that the PLMs outperforms PLMs in terms of accuracy on all 5 of them."
SP:9817dccb1a121058b23a2ef825ed339cf8b53674,"In this paper, the attention and sharpener modules are proposed for real - world scene text recognition. The attention module is designed for soft attention and the sharpener module for hard attention."
SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"This paper studies the given - number - of - available - vehicles ( VRP ) problem in the context of deep learning. The authors show that given any fixed number of vehicles, there exists a solution to the VRP problem that minimizes the length of the shortest route between the origin and the destination. This length minimization problem has been studied extensively in the literature, and has been shown to be equivalent to the problem of minimizing the number of available vehicles in the federated setting.    This paper focuses on the special case where there are more than two routes, and shows that there exist solutions to this problem that are equivalent to each other in terms of size. The paper also shows that the fixed - size solution to this special case is equivalent to a solution that minimises the total amount of vehicles that can be transported by the given route."
SP:594a813c0d0baa66738b9c8331370f861ad3c416,"This paper presents a theoretical analysis of the relationship between the structure of the world graph and the knowledge of the link between two entities ( e.g., a link and a subgraph ). The paper shows that the existence of such a relationship can be used to improve the performance of the cross - link prediction and counterfactual link learning methods.    The paper also proposes a data augmentation strategy to improve both the quality of the results obtained by the two methods."
SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"This paper presents a two - stage approach to solve the feature selection problem in supervised learning. The first stage is to select a subset of features from a set of data such that the selected features are not too different from the original ones. The second stage selects the features based on the covariance matrix between the features and the data. The authors provide guidance on how to select the features in the first stage, and how to choose the second stage by looking at the relation between the first and second stage data."
SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,This paper proposes a variational autoencoders ( VAE ) framework for data augmentation based on a distributional autoencoder ( DAE ). The main contribution of the paper is the introduction of the concept of supervision and complete observation schemes for VAEs.
SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"This paper proposes a reinforcement learning paradigm where an agent is provided with an extrinsic reward that motivates the agent to explore options in a bounded set of games. The goal is to maximize the sum of the agent ’s cumulative sum of rewards over all possible actions in the set. The paper proposes two approaches to this problem : ( 1 ) a buffer - selection algorithm where the agent is allowed to choose between a set of easy and hard games, and ( 2 ) a reward - based approach where the reward is given to the agent when it reaches a state that satisfies a certain set of criteria.   The paper shows that both approaches are equivalent to each other in the sense that the buffer selection algorithm selects a subset of the options that maximizes the sum over all actions in a given set. However, the paper also shows that buffer selection algorithms that do n’t provide intrinsic rewards do not generalize well to a larger set of options. To address this issue, the authors propose two approaches that combine buffer selection with intrinsic reward learning. The first one is based on the idea that the agent should be able to distinguish between different classes of games by looking at their similarities and differences in their visual representations. The second one uses the notion of easy / hard games as a proxy for intrinsic rewards and aims to train an agent that can distinguish between hard and easy games."
SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,"This paper proposes a neural network - based method to adaptively adapt to different types of physical systems. The proposed approach is based on the notion of “ Stiffness - aware neural network ”, which is defined as neural network that is able to distinguish between a given physical system and a set of non - physical systems with the same set of parameters.    The main contributions of this paper are :   1. A neural network based approach to adapt to non - Hamiltonian physical systems, which can be regarded as a special case of the Stiff neural network ( SANN ) proposed in [ 1 ]. 2. A new integration strategy, called “ Spatial - of - the - art ” ( SOTA ), which adapts a given system to a different set of different physical systems by sampling from a given set of data points. 3. A cross - entropy based approach, which extends the previous SOTA to the case of a more complex physical system."
SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"This paper proposes a multi - step learning framework for generating computer programs that can be evaluated in a single step. The framework is based on the concept of “ window size ”, which is defined as the number of tokens that need to be learned in order for a program to be valid in a given window. The authors show that this framework can be used to evaluate the quality of a given program in terms of its timestep and the amount of tokens required to learn it.    The main contributions of the paper are :   1. A theoretical analysis of window size and its relationship with timesteps. 2. A proof - of - concept example that shows that the window size of a program can be controlled by tuning the length of the tokens required for learning it. 3. Experimental results that demonstrate the effectiveness of the proposed framework."
SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,This paper presents a set of techniques for generating feature - level adversarial perturbations that can be used to improve the quality of the misclassification of natural images produced by deep image generators. The proposed techniques are based on the idea that a good image generator should be able to produce images that are not too different from those produced by a malicious actor who wishes to misclassify the input images.    The paper provides a number of theoretical results showing that the proposed techniques can be combined with a variety of existing techniques for improving the quality / misclassifiability of images generated by deep networks.
SP:873618263dc4246a39c44d0abfecfb5f688817e3,"This paper proposes a new method to solve the timestepacking problem in the form of an augmented version of the Travelling Salesperson problem. The proposed method is based on the fact that the original timesteps of the original problem can be decomposed into a set of components, each of which can be approximated by a $ \ell_t$-dimensional continuous variable problem.    The authors provide a theoretical analysis of the different components of the problem, and show that the proposed method can be used to solve both discrete and continuous versions of it. The authors also provide theoretical guarantees for the quality of the solutions."
SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"This paper proposes a multi - agent descent policy learning framework that addresses the trust - region decomposition and divergence issues that arise when training policies in environments with different levels of granularity. The paper proposes two approaches to this problem. The first approach is to train a policy gradient based agent that learns a set of policies over a large set of data points, and the second one is to learn a policy that is scalable to a given number of agents. The authors prove that both approaches converge to the same policy in a finite number of iterations.   The paper also provides a theoretical analysis of the policy factorization problem that arises when training the policy gradient with different granularity levels, and shows that policy divergence is a factorizable problem that can not be solved analytically. To alleviate this problem, the authors propose a policy - gradient - based gradient descent method that is empirically shown to converge to a policy with a factorization error that scales linearly in the number of agent dimensions."
SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"This paper presents an approach to improve the visual and audio - only speech recognition and text - only visual representation representations of video and audio data. The proposed approach is based on cross - entropy reduction ( WER ) approach, where the model is trained on a multi - stream video data set ( HuBERT ) and a dataset of audio and text data ( VHT3 ). The paper shows that the proposed approach achieves better performance than previous works on both the audio and the text representation representations. Moreover, the paper also shows that it is able to achieve better performance on the downstream task of automatic speech recognition."
SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,"This paper proposes a new algorithm to solve the cut problem in the context of neural networks. The key idea is to divide the problem into two halves, each half having two halves of the same length, and solve the second half using a neural network. The goal is to find a solution that minimises the difference between the two halves.    This paper proposes to do this by iteratively adding a new layer on top of top of the existing layer, and a new level of complexity is added to the top layer. The authors prove that the new layer is better than the previous layer in the sense that it is less complex and more scalable."
SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"This paper proposes a zero - shot learning approach to train neural networks with a variational approach to ensure that the learned state is close to the target state in the training phase. The authors argue that this approach is more suitable for training large neural networks than for training small ones. To this end, the authors propose two approaches : ( 1 ) gradient ascent and ( 2 ) variational variational training.    The first approach is based on gradient descent, and the second one is variational. Both approaches are shown to be equivalent to each other in the sense that the gradient descent algorithm converges faster than the variational one."
SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"This paper proposes a new method, CEN, to measure the “ controllableness ” of the actions learned by a policy - learning agent in an agent - driven environment. CEN consists of two components : ( 1 ) a control network ( CEN ) that aims to identify and quantify the effects of the agent ’s actions, and ( 2 ) a learning agent ( LE ) that tries to learn a policy that is “ safe ” in the presence of the control network. The control network is built on top of the concept of “ criticality ”, which is defined as the amount of time it takes for an agent to reach a state that is not in conflict with its own initial state, and is defined in terms of the number of actions that the agent has to make in order to avoid a catastrophic outcome.    The authors propose two approaches to this problem. The first one is to train the LE agent on a set of control data, and then to train a policy based on the control data. The second approach is to use the learned policy as a proxy for the control model, and to train an LE agent based on that proxy."
SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"This paper presents a series of compression techniques to improve the performance of structured and unstructured neural networks ( SR networks ). In particular, this paper focuses on regularized structure - regularized pruning ( SRP ) and structured distillation ( SRD ). The authors show that the proposed techniques can be combined with the existing compression techniques such as distillation and regularization to obtain more powerful networks. Moreover, the authors also propose a new compression technique to regularize the structure of SR networks."
SP:0dee45001ae9600f485614dfe6874a516ac01db5,"This paper proposes a few - shot learning framework for meta - learning based on few - shots learning. The framework consists of a few components : 1 ) a generic representation of the target domain, 2 ) a set of meta - learners and 3 ) a feature selection mechanism. The feature selection is done by selecting a subset of relevant and irrelevant features for each of the three components, which are then used to train a few meta - learner and feature selection system.   The main contributions are as follows :    1 ) A generic representation for the source domain, 3 ) A feature selection framework that is able to select relevant features and features that are relevant to each domain."
SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"This paper studies the generalisation of PAC - Bayes bounds to networks. The main contributions are :    - Defines a new metric, $ \ell_0 $, that quantifies the expected number of samples required to generalise a given network to a given set of data points. This metric can be used as a metric to quantify the expected amount of samples needed for generalisation. - Establishes a lower bound on the expected sample size required for generalised networks to generalize to given data points of $ n$, and shows that this bound is bounded by $ n^{-1/2 } \ell_{\ell_\infty}$, where $ n_1 \infty$ is the number of classes in the dataset and $ N_2 $ is the average error of the classifier on the data points sampled from the dataset. - Extensive experiments are conducted to demonstrate the theoretical results."
SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"This paper proposes a pre - trained multilingual manifold mixup and lingual manifold transfer method for cross - lingual data understanding tasks. The proposed method is based on the previous work Mixup - Mixup, which pre - trains multilingual representations on the same datapoints and aims to bridge the gap between the source - target and target - target representations.    The main contribution of this work is the pre - training of the source and target representations on a set of data points, each of which is pre - annotated with the same set of sources and target symbols. The source representation is then used to train the target representations and the target representation is used to pre - train the source representations."
SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"This paper presents a theoretical analysis of the robustness of distributed and federated deep learning algorithms under non - iid / non - realistic assumptions. The main contributions are the following :    1. This paper shows that a distributed / federated learning model can be viewed as a federated version of a standard deep learning model.   2. The authors prove the convergence of a distributed version of the proposed model under the following assumptions : ( 1 ) the data distribution is distributed uniformly over the world, ( 2 ) the model is robust to attacks, and ( 3 ) the attack vector is bounded by a constant factor."
SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"This paper proposes a multi - task learning setting where a set of supervised tasks is generated for each task, and the goal is to train a neural network that learns representations of the tasks in this setting. To this end, the paper proposes to share the representation of each task among all the other tasks in the training set. The paper also proposes a parameter sharing scheme that allows each task to be shared across all the tasks.    Experiments are conducted on a variety of datasets, including real - world and synthetic datasets."
SP:9851adb72e2918780f661f83f7da06eb866787be,"This paper proposes a new robustness framework based on smoothing. The key idea is to train a policy that is robust to adversarial attacks, and then provide a cumulative reward that can be used as a metric to evaluate the robustness of the policy.   The main contributions of the paper are as follows :    1. Introducing a new smoothing framework, named CROP. 2. Providing bounds on the cumulative rewards of the smoothing policy. 3. Demonstrating the effectiveness of the CROP framework."
SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"This paper presents a theoretical analysis of the failure - to - cover ( NF ) guarantees for the classification tasks of natural language processing ( NLP ) and chemistry ( CCLR ) tasks. The paper shows that NF and cCLR fail to satisfy the following guarantees :    ( 1 ) false positive rate / sets ; ( 2 ) suboptimal coverage / coverage ; ( 3 ) uncertainty in the classifiers ; ( 4 ) failure to detect the counterpart of a given set ; and ( 5 ) failure of the classifier to detect a set that is not a subset of the target set under the null hypothesis.   To address these issues, the paper proposes a new framework, NF - NF, where the goal is to learn a classifier that is able to detect and classify a set of non - false positive examples that satisfy the above guarantees. To achieve this goal, the authors propose two approaches. First, they assume that the set of examples to be classified is not too large. Then, they propose a new metric, $ \tilde{x}$ where $ x_t$ is the number of examples in the set, and $ t_\text{x_t } is the ratio of the false positive instances to the true ones. The authors show that under the proposed metric, there exist instances for which $ x_{t_t}$ is greater than $ y_{t-1}$, where $ y_t(x_{t+1 } \leq \in \mathbb{x_{0,0 } \log n/\log n(y_t)$ and $ n_t+\log t_t$.   The paper also shows that the proposed framework can not only detect the true counterparts of the proposed non - null hypothesis, but also provides a lower bound on the failure rate / coverage of the corresponding set."
SP:b126d2f3c397633745c8833e22ace93a2470e963,"This paper studies the problem of minimizing the length / volume distortion of higher dimensional data distributions in the context of networked learning. In particular, it focuses on the setting where the goal is to learn a network whose outputs are linear in the dimension of the input data distribution. The authors prove upper bounds on the length and volume distortion in the case of convolutional and recurrent networks, as well as lower bounds for ReLU networks. In addition, it is shown that the same bounds hold for residual networks as well."
SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"This paper studies the problem of policy learning in the context of safety - critical robotic grasping tasks, where the goal is to learn a policy that minimizes the risk of failure while satisfying a set of safety constraints. To this end, the paper proposes two approaches : ( 1 ) safe and unsafe data ( 2 ) successful data ( 3 ) policy learning ( 4 ) prior learning ( 5 ) data reduction ( 6 ) skill learning ( 7 ).    The first approach is based on minimizing the risk in both the safe and the unsafe data. The second approach uses the data reduction approach in the first step and the prior in the second step. In both approaches, the authors provide two types of data sets : safe data and data reduction. The safe data sets are used to train the policy in the safe data stage, while the data used in the successful data stage is used to learn the skills in the RL stage. The authors provide empirical results showing that both approaches outperform the other in terms of both safety and success rate. The paper also provides theoretical results showing the importance of the different choices of data ( i.e., safe vs. unsafe vs. data reduction )."
SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"This paper proposes a new multi - path RGC framework for image restoration tasks. The proposed framework is based on the idea of blur - free multi - branch architecture. Different from previous works, the proposed framework does not require the user to specify a specific model ( e.g. $ \mathcal{R}$ or $ \cal{D}$ ) for each of the three types of restoration tasks : $ \text{R } \infty$, $ \theta$ and $ \gamma$.    The paper provides a theoretical analysis of the different types of blur protection tasks and shows that each of them is equivalent to a different type of cross - entropy loss. The paper also provides a proof - of - concept example to show that the cross entropy loss can be approximated by $ \sqrt{\theta}$ where $ \alpha$ is the number of iterations of the model ( $ \log(\theta)$."
SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"This paper proposes a federated learning ( FL ) framework with data heterogeneity ( PFL ) and server - side heterogeneity ( HN ). The authors claim that the data heterogeneity of PFL and PFL - HN is caused by the fact that the server uses a different hyper - network representation for each domain. To address this issue, the authors propose two approaches : ( 1 ) PFLHN and ( 2 ) data heterogeneity - based PFL.    The authors prove that both the PFL framework and HN framework fail to provide a good representation of the target domain under the same data heterogeneity. Moreover, they show that data heterogeneity can be harmful for the performance of both PFL methods. To resolve this problem, they propose to federate the two models separately and provide the clients with different representations of the source and target domains."
SP:960d0a63a82593f6e72275b65f0501f0469d1924,"This paper proposes a diffusion based generative model ( RCDM ) that aims to generate representations that are faithful to the source data and that can be used for supervised learning.    The main contribution of this paper is to provide a representation that is not too different from the standard generative models that have been used in the literature. The main idea is to use a data - augmented version of the R CDM that has the property that it does not require the source and target data to be identical, and that it can be trained on a set of supervised data."
SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"This paper proposes a sketch - based streaming algorithm that provides a privacy - preserving upper bound on the number of iterations required to decode a given set of data points in a compressed version of the original compressed version. The proposed algorithm is based on the following assumptions :    1. Each data point in the compressed version can be decomposed into a set of datapoints of length at most $ t$ and dimension $ m$. 2. For each data point, there exists a vector $ z$ such that $ z \in \mathbb{R}$ is the sum of $ z$. 3. For a given data point $ y$, the algorithm first computes a sketch of the data points $ z $, and then computes the fraction of iterations needed to decode it.   The authors provide a theoretical analysis of the proposed algorithm and empirically prove that the upper bound is inversely proportional to $ \ell_sqrt{\sqrt{R } \log n}$, where $ n$ is a factor of $ t$."
SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"This paper proposes RSPO, a policy optimization framework for multi - agent stag - hunt games. The framework is based on the continuous control framework of MuJoCo continuous control games, where agents are given a set of objectives and agents are required to solve these objectives in a continuous control game. The paper proposes a learning algorithm that learns a policy that maximizes the total number of agents ’ objectives and the total amount of actions they need to execute in order to achieve a given goal. In addition, the paper proposes to provide a diversity reward that encourages agents to explore strategies that are different from their default policies in a variety of environments.   The paper provides theoretical analysis of the novelty of the policy learning and intrinsic rewards. Experiments are conducted in both single agent and multi agent settings."
SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"This paper proposes a new sampler search method, DDSS, based on gradient - based optimization. The proposed method is based on a generalization of the Diffusion Probabilistic Model ( GGDM ). The main contribution of the paper is the introduction of the GGDM family of samplers. The authors show that the proposed method can be used to improve the quality of the samples produced by the DDPM. The paper also shows that the method is provably more stable than the previous sampler search method DSS."
SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,"This paper presents a text - to - text ( text - only ) model for natural language prompts. The model is built on top of the large language model ( LLM ) framework. The main contributions are :    1 )   the introduction of the concept of “ prompts ” in the context of natural language models ( prompts ). This allows the model to be used in a more general context, e.g., without the need of referring to the source code of the language model.   2 ) the development of a set of prompts that can be used to annotate the source data of the model. This is done by adding a prefix to the input of the prompt, such that the source can be seen as an annotation of the source ( or target ). 3 ) the training of the prompts that are used to generate the source and target data for the source. The source data is then used to train the model in two stages. The first stage is to provide the source to the prompt embedding layer. The second stage consists of training the model on the source as well as the target data, and the source is fed to the feed - forward layer to produce the source for the data in the target layer."
SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"This paper proposes a multi - distribution learning task with the following components : ( 1 ) learn the concept of time series ( CCTS ), and ( 2 ) learn a policy to adapt to changes in the distribution of the world data over time ( CTS ). To this end, the paper proposes two approaches : ( a ) data size - based diversification and ( b ) data diversification. In the first approach, the data size is kept constant over time, while in the second one, the distributions are diversified over time. The paper shows that both approaches fail to converge to the same distribution under certain assumptions, and proposes a policy that adapts to the changes in distribution over time by varying the size of the diversified distributions."
SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"This paper proposes a new approach to provide access to a large corpus of documents for the purpose of language modeling. The corpus is divided into two parts : ( 1 ) a dataset of documents and ( 2 ) a set of works that can be used as input to a model. The datasets are used to train the model and the works are used for checking the accuracy of the model. To this end, the authors provide a number of contributions :    1.   a new attention mechanism that allows the model to be retrieved from the source corpus without the need to re - write the source code. This is done by storing the source and works in the same place. The source code is stored in a different place for each of the two parts of the corpus. The authors also provide an attention mechanism to ensure that the model is not over - represented. The attention mechanism is also used to provide a retrieval mechanism for the works that are used in the model training. The model is used to compare the performance of the models on different types of data sets. The results show that the attention mechanism outperforms the other methods in terms of model accuracy and retrieval speed."
SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"This paper proposes a new sampling scheme based on the Hastings Monte Carlo algorithm for energy - based language modeling ( MLM ). The main contribution of this paper is to provide a sampling scheme that is more tractable than the previous one ( Xie et al., 2020 ).    The main contributions are as follows :   1. Introducing the Hastings - based sampling scheme, which has been shown to be tractable by Xie et. al. ( 2020 ) ; 2. Providing a sampling strategy that is provably safe and provably energy - compatible ; 3. Demonstrating that the proposed scheme is more data - efficient than Xie et.al. ( 2014 )."
SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"This paper proposes two approaches to improve the quality of data augmentation for policy learning on unsupervised text classification tasks. The first approach is based on re - weighting the samples generated by the policy and the second one is a combination of two approaches, i.e., data - augmentation - based and data - based. Experiments are conducted on a set of four tasks, where the goal is to find a policy that is more similar to the target text than to the source text. The experiments are conducted in two different settings : low - data and class - imbalanced settings."
SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"This paper proposes a new attention mechanism for task representation learning. The attention mechanism is based on the contrastive learning framework proposed in [ 1 ] and [ 2 ]. The novelty of the attention mechanism lies in the fact that it allows the encoder to focus on a subset of tasks instead of the entire set of tasks. The paper also proposes a context - based encoder that can be used for differentiating between different tasks.    Experiments are conducted on the following benchmarks :   1. CIFAR - FSRL, 2. COMRL, 3. DQN, 4. SOTA."
SP:ed86c60850d5c8302dcf1c2167db303e778fe681,"This paper proposes a sequential generative model improvement framework for improving the quality of public belief state - based value functions. The framework consists of two parts : ( 1 ) fine - tuning and ( 2 ) iterative learning. The first part is based on the idea of publicizing the state - value function parameters of the model. In the first part, the model parameters are assumed to be known to the public. The second part consists in iteratively improving the model by sampling from the public data. The authors prove that both approaches are equivalent to each other in the sense that in the case of the sequential model setting, the iterative training of the public value function is equivalent to the iterate of a sequential model.  "
SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"This paper presents an analysis of the training time - cost - effectiveness tradeoff between sparsity and sparsity - sparsity for the classification and WikiText tasks. The paper shows that sparsity is more expensive than sparsity in terms of the number of classes and the total number of sparsity blocks, and that the sparsity tradeoff is less than $ \ell_0 $ for the language modeling task. To address these issues, the paper proposes to prune the class of MLP - based and Transformer - based classification models, and show that the pruning strategy is more cost - effective than the one proposed in the previous work."
SP:136e31054a55abca840f6478491972023c2296cb,"This paper proposes an image generation method based on the diffusion probabilistic model ST - DDPM. The main idea is to generate an image by sampling from the distribution of a distribution $ p(x, z)$, where $ z$ is a distribution of $ z$. The goal is to obtain an image that is not too different from the one generated by the reverse process.    The main contribution of this paper is to show that the score of the reverse distribution is not worse than the one obtained by the forward distribution."
SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,This paper proposes a new framework for domain generalization ( DG ) based on data shift - invariant sub - spaces ( data shift ) and space - space ( space - domain ) invariant features. This framework is built on top of the existing DG framework [ 1 ]. The proposed framework has two main contributions :    1 ) it provides a data shift invariant hypothesis that can be used to improve the generalization capacity of the current DG framework.   2 ) it also provides a new method to obtain domain invariant data shift that can improve the performance of the DG framework under different data shift settings.
SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"This paper presents a theoretical analysis of the kernel thinning ( KT ) algorithm, which aims to produce kernels that satisfy the following guarantees :    1.   For any smooth convex function $ \mathcal{x}$, there exists a kernel $ \text{x } \infty$ such that for any set $ x$ of points $ z$, the probability distribution $ p(x | z)$ over $ z $ can be represented as a sum of $ \cal p(z|x)$ and $ \sqrt{\frac{x}{\sqrt{x}}$, where $ z}$ is the kernel of the function defined on $ z$.   This paper shows that for $ x = z$ with $ z=1 $ and $ z<2 $, there exist kernels $ \tilde{x}\infty } $ such that $ \log p(\text{xi}$ can be expressed as a product of $ p(\sqrt { x}$ and   $ z(x, z ) $ with $ \nabla_{\infty}$ \text{\frac { x}{\text{z } } \log P(x|x ) } \vert p(xi)$. 2. For any $ z = z$. There exists a smooth function defined $ \lambda$ $ \hat{xi } such that if $ z \log z$ is a kernel, then $ \gamma(z ) \log(x)\log p(y ) $ is also a kernel of $ z.3. This paper gives an upper bound on the probability that $ z\log z$."
SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"This paper presents a set of open - source tree search algorithms for solving hard convex optimization problems with different kernelization assumptions. The authors provide a theoretical analysis of the quality of the solutions obtained by each of them. The paper also provides a benchmark suite for evaluating the solvers in terms of the number of instances in the set, number of iterations in the tree search algorithm, and number of solutions in the closed - source benchmark suite.    * * Contributions * * :   1. This paper provides theoretical analysis on the quality properties of tree search solvers for different kernelsization assumptions ( e.g., $ \ell_1 $, $ z_2 $ ), and shows that they are all at least as good as the state - of - the - art. 2. They provide theoretical guarantees on the amount of iterations required to solve a given set of hard problems. 3. They show that for any given kernelization assumption, there exists a solver that can solve the set of problems in the given set. 4. They compare the performance of the different solvers against each other and with a synthetic solver."
SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"This paper presents a theoretical analysis of the quantization problem of convolutional neural networks ( CNNs ). The authors show that the current best quantization rate of CNNs is $ \sqrt{1 } \log p(n_{\text{quantization}(n+1})$, where $ n$ is the number of convolutions and $ p$ is a constant factor that can be used to estimate the bit - quantization error. They show that this is not enough to solve the problem, and propose a lower bound of $ \text{1}$ that is better than the previous best by a factor of $ n^{-1}$. They also show that $ \log(n^{-2}$ is suboptimal for the image - to - image task."
SP:004865e6affad32403b7965493a53c8a7ffdda0a,"This paper considers the following questions :    1. What is the probability that a fixed point strategy that minimizes the regret minimization of a sequence of games played by a player who starts from an initial state with a fixed value and moves to a state with an unknown value at time t, and ends up in a state that has a different value at timestep t+1?   2. Is there a distribution of games that equilibrates this question? 3. What are the probability distributions of games in which the fixed point strategies that minimise this question are correlated with timesteps t + 1?"
SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,This paper proposes an action - specific control method for deep reinforcement learning ( RL ) based on continuous action spaces. The paper provides a theoretical analysis of the action space and proposes a control method based on the action - value function. Experiments are provided to demonstrate the effectiveness of the proposed method.
SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,This paper proposes a novel approach to improve semantic segmentation in domain generalization approaches. The proposed approach is based on two approaches : ( 1 ) image - level style feature augmentation ( AdvStyle ) and ( 2 ) pseudo - code ( Pseudo - code ). The first approach is to train a model on a set of unseen domains and then train a pseudo - model on top of the unseen domain. The second approach trains the model on the source domain and the pseudo - domain separately. The authors conduct experiments to evaluate the effectiveness of the proposed approach.
SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"This paper presents a novel Gated VAE ( Gesture Recognition Systems ) framework that leverages the recent advances in device - based gesture recognition systems. The main contributions are :    1.   a gesture recognition system that aggregates real - world gesture data aggregated from a variety of different kinds of devices. This is done by using a Gated VQ - VAE encoder - decoder that is based on Gated SGD. The encoder is designed to be able to capture both the spatial and the temporal properties of the input data. This allows the encoder to be trained on different types of devices, and the decoder is trained on top of a set of different encoders. The decoder can then be used on any of the devices that are supported by the GatedVAE framework. 2."
SP:2e66468a6b94177e54b0052b97713ee63902c278,"This paper presents a series of experiments aimed to evaluate the ability of deep learning models to learn sparse representations. The experiments cover a range of different deep learning architectures, with emphasis on PyTorch - based architectures. The paper provides a set of experiments that compare the performance of different models on a variety of metrics, including the number of queries used, the amount of data used in each query, the fraction of samples used for regression, and the number and quality of hyper - parameter choices used for inference.    The paper also presents an analysis of the impact of each of these metrics on the results of the experiments."
SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper studies the problem of action Q - learning ( LAQ ), where the goal is to learn a set of actions that will help the learner achieve a goal that is closer to the goal than the one that would be obtained by taking the actions that would lead to a state that is far from the goal. This paper proposes two approaches to this problem. The first approach is to use a low - level controller that learns the value function $ \ell_t$ and the action function $ t$. The second approach uses a higher level controller $ t$ that is able to predict $ t \ell_{t } \in \mathbb R^{-1/2}$, where $ T$ is the distance between the current state and the goal and $ R^2 $ is the number of actions required to reach the goal in the future.    The main contributions of this paper are as follows :   1. The paper provides a theoretical analysis of the difference between the two approaches. The authors show that the lower - level controllers learn functions that are closer to each other than the upper - level ones. 2. They show that both approaches converge to the same value function when $ t^2$ is large enough. 3. They provide a theoretical justification for the difference."
SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"This paper presents a theoretical analysis of the distributed training problem of parallelizing the pre - trained Transformer language model in the presence of a pre - defined source - to - target ( P2P2P ) pipeline. The paper focuses on the following issues :   ( 1 ) How to ensure that the parallelization of the T4 model with respect to the source and target pipeline is not too different ; ( 2 ) What is the optimal amount of data that needs to be transmitted across the source / target and the target / target to ensure the correctness of the parallel model?   The paper proposes two approaches to address these issues. First, the paper proposes to parallelize the model with the source ( i.e., the source ) and target ( ii ) pipelines in a way that ensures that the source is as close to the target as possible ; and ( 3 ) that the target and target are as close as possible to each other in terms of how much data they need to send across the p2P pipeline to ensure correctness.    To this end, the authors propose two approaches. The first approach is to pre - train the model on a set of sources and target, and the second one is to train the target on a subset of those sources. The main contributions of the paper are as follows : ivanov et al. ( 2020 ) and ( 2021 ), which provide theoretical guarantees on the required data for both approaches."
SP:91d2f094d5481651b554f58aecc2a6207057a47c,"This paper proposes a multi - agent reinforcement learning framework for policy improvement. The framework consists of three components :    ( 1 ) a policy - based learning framework that learns from a dataset of real - world experiences, ( 2 ) an agent policy that is prioritized based on the quality of the experiences provided by the policy, and ( 3 ) a value - based policy that aims to minimize the distance between the value of the policy and the actual value obtained by the agent.   Experiments are provided in the form of offline training and online experiences, where the agent is provided with access to a fixed dataset, and the goal is to learn a policy that minimizes the efficiency of the agent policy in terms of its ability to learn from the real world experiences. The proposed framework is shown to achieve state - of - the - art performance on a variety of policy improvement benchmarks."
SP:d0e650d568214481b07a0452ec606ccbf6d05410,"This paper proposes a quantitatively unbiased quantization strategy for neural network training. The proposed strategy is based on the quantization of the forward and backward trajectories of the neural network. The authors also propose a quantized gradient strategy for the backward trajectory.    The proposed quantized strategy has the following components :   1. The quantized trajectories are divided into four stages : * * forward * *, * * * backward * * and * * quantized * *. For the forward stage, the neural networks are initialized with the same set of parameters as in the case of the unquantized training. * * For the backward stage, a set of different neural networks is trained with different values of the parameters in the previous stage. *   2. The gradients of the backward and quantized stage are compared with each other and with the standard quantized approach. * 3. The performance of the quantized method is evaluated with respect to different choices of parameters."
SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"This paper presents a few - shot classifier approach for the problem of identifying rare diseases in meta - learners. The proposed approach is based on a feature - selection mechanism similar to the one proposed in [ 1 ] and [ 2 ], where the goal is to select features that are relevant to the problem at hand and that are likely to be helpful to the learner. The key difference is that the feature selection is done in two steps : first, features are selected based on the embedding dimension of the classifier, and then the feature is used to score the features that should be selected for the next few rounds of classification.    The paper evaluates the proposed approach on a number of real - world applications."
SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"This paper presents a reinforcement learning framework for the continuous communication channel. The framework consists of three components :    1.   A language environment. This is composed of a pre - trained language model and a teacher model. The teacher model is used to train the student model, which is then used to learn the language model. 2.   A training methodology. This consists of training the teacher model on a set of data points, and training the agent model on these data points. 3.    A validation framework. This framework is also used to validate the model."
SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,This paper proposes a two stage backdoor attack on NLP framework. The first stage is designed to train the NLP model and the second stage is used to perform the backdoor attack. The authors also provide an analysis on the effectiveness of the two stages.
SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"This paper studies the problem of skill discovery in an evolving or expanding world setting. The paper proposes two approaches to this problem, i.e., skill discovery and skill supervision. The first approach is to discover new skills in a static world setting, and the second one is to learn skills in an expanding one. Both approaches are based on the assumption that the world will become increasingly complex as the number of agents increases.    The paper shows that both approaches fail to converge to the same solution under certain assumptions. Specifically, the first approach fails to discover a new set of skills when the world becomes increasingly complex, while the second fails to find new skills when it becomes more complex. This paper argues that the failure of both approaches is due to the fact that the new skills discovered in the static world do n’t have the same quality / timestep with the existing skills in the expanding world, which makes the learning of new skills more difficult."
SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"This paper proposes a new convolution kernel for cosineural neural networks. The proposed kernel is based on log - polar space convolution kernels and can be used for various cosine - centric neural networks such as cosine neural networks, cosine convolutional neural networks and cosine vector networks.    The main contributions of this paper are as follows :   1. This paper shows that the kernel can be shared between different cosine networks such that it is not too different from the previous kernel. 2. The authors show that the proposed kernel can also be used to solve the cosine segmentation and vector network tasks."
SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,This paper studies the problem of learning how to compress the complexity of a given data structure by training a model on a finite set of data points and then training a weight - based learning algorithm on top of the data points.    The authors prove the following results :   1. They show that there exists a closed - form upper bound on the size of the batch size of a data point in terms of the number of labeled data points in the training set. 2. This bound is tight enough to guarantee that the learned model is not over - compressible. 3. They provide bounds on the dimensionality of the entropy function of the compressed data point and show that it is at least as large as the dimension of the original data point.
SP:a733847ade77ffbf38760fc79da17893dea8d53f,"In this paper, the authors present a data poisoning attack framework based on separable separable perturbations that can be used for data poisoning attacks. The authors also provide a theoretical analysis of the data poisoning framework."
SP:7b50be406138ad01db3ee112899f622637896fe9,"This paper studies the problem of offline policy learning from a data - driven continuous - world dataset. The authors propose two approaches to this problem : ( 1 ) a policy - based approach based on sampling from the dataset, and ( 2 ) a behavior - based method based on a policy evaluation. The first approach is based on estimating the value function of the policy, and the second one is a combination of the two approaches. Both approaches are shown to converge to a good value function when the dataset is large enough.    The paper also presents a proof - of - concept proof that both approaches converge to good value functions when the data is small enough."
SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper proposes a zero - shot image embedding model for language - image classification ( CLIP ).    The key idea is to learn a vector - vector embedding that maps an image to a given language ( e.g., c / n ), and then to classify the image using the embeddings of this vector. The authors claim that this model can be used to improve the classification performance of CLIP."
SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"This paper proposes a new object captioning framework, called NOC - VLSTM ( Visual - Linguistic Adequacy and Literacy in Sec. 2 ), which aims to improve the quality of the text - to - image captioning provided by visual - linguistic models. The proposed framework consists of two components : ( 1 ) an encoder - decoder model, ( 2 ) a teacher - supervised model, and ( 3 ) a decoder - evaluator model. The decoder is trained on a corpus of object captions and decoders, and the teacher is provided with annotated examples of the object captioned by the decoder. Experiments are conducted to evaluate the effectiveness of the encoder, decoder, and decoder models, as well as the supervised model."
SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"This paper studies the property of cross - entropy learning of classification networks under the setting of few - shot classification problems. The paper provides a theoretical analysis of the property under the following assumptions : ( 1 ) the number of classes per shot is bounded by a constant constant, ( 2 ) the classifier is a linear regression, and ( 3 ) the data is uniformly distributed over all the classes in the dataset.    The paper shows that the property is invariant under these assumptions, and provides a proof - of - concept solution to each of these assumptions."
SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"This paper presents a point - cloud - voxel fusion framework with a two - stage refinement strategy to improve the quality of the cross - entropy point cloud. The first stage focuses on improving the accuracy of the point cloud and the voxels. The second stage concentrates on improving both the cross entropy and the distance of the points to the source vectors.    * * Contributions * * :   1. This paper presents two stages of point - point cross entropy refinement. In the first stage, cross entropy is improved by iteratively adding new points from the source and the target point clouds, while in the second stage, the distances are improved by adding new point clouds from source and target vectors, while keeping the same cross entropy. The authors claim that cross entropy improves cross entropy but not cross entropy, and that the distances improve cross entropy only marginally. 2. The paper also proposes a refinement strategy that iteratively adds new points and distances from source to target points, but not from source vectors, which is shown to be better than cross entropy in the paper."
SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,This paper provides a theoretical convergence analysis for distributed graph neural network ( GCN ) training with feature gradients. The paper focuses on feature - based GCN and feature - free GCN training. The main contributions are :    1.   a theoretical analysis of the convergence rate of distributed GCN with feature gradient gradients ; 2. ) a convergence analysis of feature and subgraph gradients with respect to the number of datapoints in the training set.
SP:8302d49558ee0f16392d623d4e604e92db10d041,"This paper presents a series of experiments aimed at improving the robustness of the ResNet50 vision transformer model. The experiments cover a range of data augmentation and adaptation strategies, as well as time - adaptation strategies. The paper also includes a discussion on model robustness under different data augmentations.    The main contributions of the paper are as follows : - Improving the model's robustness to changes in the distribution model parameters during training. - Conducting experiments on different models to improve the model s robustness against changes in model parameters."
SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"This paper studies the problem of learning a policy that maximizes the expected return of a given set of data points given a set of approximated data points. The paper proposes two approaches to this problem : ( 1 ) data - driven and ( 2 ) model - free.   The first approach is a data - based approach, where the data points are assumed to come from a fixed source ( e.g., an agent ) and the goal is to learn the policy that minimizes the return of the given data point. The second approach is an RL approach, in which the data point comes from an agent that has to learn a model that satisfies both the objective and the model. In the first approach, the model is assumed to be free of any prior knowledge about the source and the data. The authors show that both approaches converge to a lower bound of $ \ell_0 $ on the return, provided that the model satisfies certain conditions on the source ( i.e., the quality of the data ) and on the model ( ii ). The lower bound can be further improved by fine - tuning the model in such a way that the resulting policy is more data - efficient."
SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"This paper studies the problem of cloning from a single observation to a sequence of similar actions in an autonomous driving and robotics control system. The authors propose two approaches to this problem. The first approach is based on learning a policy that is able to imitate the behavior of an agent that has been observed in a continuous control task ( e.g., a vehicle driven by an autonomous agent that is stopped at a given point in time ). The second approach uses a combination of two approaches, i.e., an imitator policy and an action cloning policy.    The authors prove that both approaches converge to the same solution under certain assumptions on the observation history ( BC - SO ) and the action history ( SO - BC - BC ). They also prove that the combination approach converges to a solution that is close to the optimal solution under some additional assumptions."
SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"This paper proposes a meta - learning method that is based on a model - based approach to predict the physical dynamics of a given set of data points in an unknown domain. To this end, the model is first decomposed into a set of sub - domains, and then the learned model is used to predict sub - domain features ( e.g., flows ). The model is then used to generate supervision signals that are used to guide the learner to make predictions about the underlying dynamics of the data points.    Experiments are conducted on a grid - based learning model, with different choices of supervision signals ( i.e., grid - normalization vs. grid - padding ), and the results show that the proposed approach outperforms the baselines in terms of safety."
SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,This paper proposes a supervised monocular 3D object detection method that aims to improve the accuracy of the 3D cross - entropy loss of the unsupervised LiDAR point clouds.    The proposed method is based on the idea of supervised 3D point clouds that are annotated with 3D box labels. The main contribution of this paper is that it is able to show that the supervised point clouds can be made to be more accurate in terms of the cross entropy loss.
SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,This paper presents a model - free subword tokenization method based on gradient - based subword representation. The proposed method is shown to outperform previous subword - based tokenization methods on both text - level and character - level benchmarks. The paper also presents an open - source model of the proposed method.
SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"This paper studies the problem of detecting backdoor attacks on deep neural networks ( DNNs ) under the extreme value analysis ( AEVA ) framework. Specifically, the authors focus on black - box hard - label scenarios and derive a theoretical analysis of the AEVA framework from the perspective of the singularity of the distribution of the adversarial map generated by the attacker. They show that AEVA is not able to detect backdoor attacks in the considered cases. They also provide a theoretical justification of the failure of AEVA under the assumption that the attacker has access to a large set of data points."
SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"This paper proposes a neural network - based uncertainty measure to estimate the uncertainty in the class - probability simplex - categorical distributions produced by a classifier trained on a set of data. The key idea is to train the classifier such that the uncertainty is less than $ \ell_2$. To this end, the paper proposes to use the Leibler divergence criterion to measure the uncertainty between the classifications produced by the classifiers and the data.    The main contribution of this paper is to provide a theoretical justification of the divergence criterion."
SP:8b4f3916dca4e627931558e14836749bd4a6792f,"This paper studies the distributional properties of the patch space of convolutional neural networks ( CNNs ) under the assumption that the number of patches in the network is low and the dimensionality of the space is low.    The main contributions of the paper are as follows :   1. This paper analyzes the distributionality of patch space under two assumptions : ( 1 ) that the network has a low - dimensional structure, and ( 2 ) that there is a sufficient number of data points in each patch space for the network to cover a certain number of natural data points. The paper shows that the first assumption is sufficient and the second assumption is not sufficient, and proposes a semi - supervised learning algorithm that satisfies both assumptions simultaneously. The proposed algorithm is shown to outperform the state - of - the - art CNNs by a large margin."
SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"This paper proposes a new neighbour discovery method for the state - of - the - art clustering tasks. The proposed method is based on the AE - NETS algorithm. The key idea of the method is to divide the space into a set of features and then to discover a subset of those features that have a certain quality metric ( e.g., clean, rich, etc. ). The main contribution of the paper is to show that the proposed neighbour discovery strategy can be used to solve the clustering problem in a differentiable manner."
SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"This paper proposes a cross - domain datacenter approach to evaluate the robustness of neural networks to changes in the source and target domain representations. The approach is based on the DGWD - ReID framework, which has been used successfully in previous work such as ReID and ReID++. The main contribution of this paper is the introduction of a new metric, $ \ell_sqrt{d}$, to measure the invariance of the source / target representations under changes in domain representations, which is used as a constraint for cross - data evaluation.    The paper also presents a set of experiments to validate the effectiveness of the proposed approach."
SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"This paper presents a new neural property prediction framework, GNNs3D, based on the Open Graph Benchmark ( OGB ) dataset. The proposed framework has three main components :    1.   GNN framework, which is based on GNN-2D and GNN - GNN. The network is designed to be able to predict both spatial and molecular property predictions. The key idea is to regularise the representations of the nodes in the GNN so that they are not too differentiable. The authors claim that this regularisation helps improve the quality of the property prediction. 2."
SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"This paper considers the problem of representation learning from data - driven multi - component mixture models ( EM models ). The authors propose two approaches to this problem. The first approach is to learn a set of representations for each element in the EM model, which is then used to perform a series of EM steps, each of which is conditioned on a subset of the elements in the original EM model. In the second approach, the authors propose to add a cost term to the cost of each EM step to ensure that the resulting representations are not too differentiable.    The authors prove that both approaches converge to the same solution when the number of feature vectors in each EM model is large enough. They also prove that the cost minimization problem is equivalent to minimizing the likelihood maximization problem."
SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"This paper proposes a feature selection framework for biomedical data analysis. The proposed framework consists of two components : ( 1 ) feature selection and ( 2 ) downstream feature selection. In the first component, a synthetic dataset is used for feature selection, while in the second component, the feature selection is done on the background dataset. The main contribution of this paper is to provide a set of downstream features that can be used in both the CA setting and the downstream setting."
SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"This paper analyzes the properties of the stopping time of neural networks trained in different stopping - time settings. Specifically, the authors consider the following settings :    1. linear regression models ; 2. deep neural networks ; 3. data augmentation ; 4. training neural networks."
SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"This paper presents a new policy gradient algorithm, named NPG, which is based on the idea of Newton - type quadratic convergence of the policy gradient ( NPG ). NPG has been proposed in [ 1 ], [ 2 ] and [ 3 ], and has been shown to converge to a state - of - the - art solution [ 4 ]. In this paper, the authors show that NPG can also be used to solve the following problems :   ( 1 ) $ \ell_1 $ regularization regularization problem ; ( 2 ) $ n\ell_2 $ regularisation problem ; and ( 3 ) $ p\ell_{\infty } \infty$."
SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"This paper proposes a reinforcement learning method for agents that is based on the concept of “ safe passage ”, where the agent is provided with a set of examples and is required to solve the given set of problems in a safe passage. The goal is to ensure that the agent “ escapes ” from the given problem ’s “ unsafe passages ” as much as possible. To this end, the paper proposes to train a policy agent that is able to learn the safe passage through a series of safe passages, each of which is conditioned on a different “ reason ” ( e.g., “ failure ” or “ success ” ) of the previous one.    The paper shows that the policy agent can be trained in two stages : ( 1 ) it learns the safe passages based on “ case - based ” reasoning, and ( 2 ) it “ learns a ” safe passage based on distribution - based reasoning. The paper also proposes a “ reinforcement learning ” approach, where agents are provided with examples and are required to “ escape ” the given safe passages ( i.e., to solve a given safe passage ) using a policy that is conditioned to escape from the problem at hand."
SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,"This paper presents a novel method to train a multi - sense encoder - distillation model for contextual word similarity induction tasks. The proposed method is based on an attention - based topic model ( ETM ), which is trained with a pre - trained contextual language model ( BERT ). The paper also proposes a novel way to distil contextual word embeddings that can be used in combination with the ETM. Experiments show that the proposed method can achieve better performance than the baselines."
SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"This paper presents a point - cloud - based image and 3D classification and shape representation learning framework that is built on top of a pre - trained generative model ( FIP ). The proposed framework consists of two parts :    1. image - pretrained convolutional filters and   2. Point - cloud generative models. The first part is a generative pre - training framework that consists of three stages : ( 1. ) pretraining of the generative point cloud model, ( 2. ) learning of the 3D point cloud models, ( 3. ) segmentation and ( 4. ) data efficiency.   Experiments show that the proposed framework achieves state - of - the - art performance on a variety of image classification and cloud classification tasks."
SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"This paper studies the problem of generating generative models for autoregressive machine translation ( NMT ). To this end, the authors propose a conditional generative model ( CMC ) approach, which is based on conditional sampling from a set of data points sampled from the dataset of the WMT16 benchmark dataset. The authors show that the CMC approach is able to learn a model with good performance on the dataset.    * * Contributions * * :   1. This paper provides a theoretical analysis of the problem and shows that CMC approaches are not able to solve the cross - entropy - based learning problem. 2. The paper proposes a way to solve this problem by sampling from the data points of the dataset generated by the model. 3. Experimental results on the benchmark dataset show that cMC approach can learn a good model with high performance."
SP:51e748c55bd4134047098559577fa3f37aa7433a,This paper provides a unified framework to analyze the distributional and adversarial robustness of deep neural networks ( DNNs ). The main contributions are as follows :    1. Introducing a family of risk functions that can be used to compare DNN - based and distributional robustness methods.   2. Conducting a series of experiments to show that the proposed framework is able to match the performance of both the PGD - AT and TRADES - AT methods.
SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"This paper proposes a framework for representation learning in time series based on a bilinear temporal - spectral fusion module. The proposed approach is based on two components : ( 1 ) global context - level augmentation ( time slicing ) and ( 2 ) cross - domain interactions ( feature representation learning ). The first component aims to capture long - term dependencies between time series and other domains, while the second one aims to learn the representation of time series without long - range dependencies.    The authors provide a thorough analysis of the proposed framework and show that the proposed approach achieves state - of - the - art results."
SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"This paper proposes a new learning rate metric based on gradient descent. The proposed metric has the following components :    1. $ \ell_tilde{x } \infty$, where $ t_t$ is the first - order gradients of the metric, and $ T_\ell_0 $ is the second - order gradient. The paper shows that $ t_{t_0}$ can be thought of as the gradient descent metric with the following parameters : $ \alpha$, $ \beta$ and $ \gamma$.   The paper also shows that the metric can be viewed as a convex version of $ \mathbb{R}$ with $ \text{text{x}$ being the second order metric.   2. The authors show that the proposed metric can also be seen as an adaptive version of the standard metric with $ n_t$. The paper further shows that this metric also has the property $ \sqrt{\text{delta}$ where $ n_{t } \to n_1}$ is a metric that can be used to learn the metric $ \log n_0$."
SP:263c787361cd6d4443ce516d389c694d0fe44b28,"This paper proposes a meta - reinforcement learning method for policy search and reinforcement learning ( RL ). The method consists of two steps : ( 1 ) meta - search and ( 2 ) continual learning. The first step is a search phase where the policy is trained on a set of control tasks, and the second one is a continual learning phase where a new policy is learned on top of the existing policy. The paper shows that the policy search phase is more effective than the meta - training phase, and that the continual learning stage is more stable than the first step."
SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"This paper proposes a backdoor - based classifier poisoning attack method, which is based on the concept of “ poisoning by default ”, i.e., the use of default classifiers that are not robust to adversarial examples. The authors provide a theoretical analysis of the vulnerability of the proposed method, and show that the default classifier ’s robustness can be regarded as a measure of the “ quality ” of the examples provided by the attack. The paper also presents a dataset - based robustness metric that quantifies the amount of data that needs to be collected in order for the attack to be robust to a given set of examples."
SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"This paper studies distillation strategies for GANs. The main contributions are :    1. This paper proposes a new distillation strategy based on direction - based distillation, which is shown to be more stable than previous distillation methods.   2. A theoretical analysis is provided to show that the proposed strategy is more consistent than prior distillation approaches. 3. Experimental results are provided to support the theoretical analysis."
SP:2c2231743fa33b95828c6615263954ce1c05f95d,"This paper studies the problem of multi - task learning in the context of the financial markets. The authors propose a multi - stage learning model that learns from both real - world data ( e.g., equities ) and a set of online data ( i.e., algorithms ). In particular, the authors consider the setting where the goal is to predict the future value of a given stock. They consider two types of algorithms : online and offline.    The online setting consists of the following steps : ( 1 ) learn a sequence of functions $ \ell_t$ that describes the expected future value $ t$ of a stock $ t$. ( 2 ) predict $ t_{t_{t_t } \in \mathbb R^{-1/2 } \log p}$, where $ t_0 $ is the current value of the stock and $ T_1$ is the next trading day's closing price. In the offline setting, they consider the following sequence of $ p$ : $ p_0$.   For the online setting, there are two different types of data sets :   ( i ) a set consisting of $ n$ of commonly traded equities and ( ii ) an offline setting consisting of a set composed of $ s_t$. For each of these two sets, they propose a training set $ \tilde{x } \leq \infty$ and a learning task $ \text{x}$ that aims to predict $ p_{t-1}$ and $ t-1$. They show that under certain assumptions on the data, they can learn the following a sequence $ \alpha$ where $ \theta$ is a function that represents the expected value of $ t $, and $ z$ a function representing the expected return of the underlying stock."
SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"This paper proposes a new variational approach to approximate variational distributions of Gaussian Processes ( GPs ). The proposed approach is based on variational approximations of GPs that are derived from a neural network ( DNN ) that has been trained on a set of data points.    The main contributions of the paper are as follows :   1 )   a theoretical analysis of the variational properties of the DNN. The authors show that there exist variational GP approaches that can be approximated to a good approximation to the true distribution of the data points, but that these approaches do not generalize well to distributions that are not directly related to the underlying process ( e.g., binary classification problems ). 2 ) A set of experiments is conducted to show that the proposed approach can be trained to approximate the true distributions of both binary classification and GPs. 3 ) Experimental results are provided to show the effectiveness of the proposed approaches."
SP:f20c99b441545047a16ae524cc2e317b2c3787a2,This paper proposes a new training strategy for distributed deep learning with the goal of ensuring the correctness of the proposed method. The proposed approach is based on the following components :    1. Protocols - Secured ( Protocol - Sec - Sec ) server - side communication and datacenter - side data sharing.   2. Sec - Txt - Sec ( Sec - txt ) datastritent - Secentent - tolerant training strategy.
SP:93894f20ab2593e5237b6972fef9fe63e96af89a,This paper presents a theoretical analysis of the structure of the parameter space of turbulence simulators with respect to the number of particles and rotational symmetries of the underlying physical structure.    The main contributions are as follows :   - A theoretical analysis based on the logarithmically invariant space of parameters of SPH based fluid simulators. - A set of theoretical results on the invariance of spH based parameters to different types of rotational and angular momentum. - An empirical study on the sensitivity of the model to different values of the parameters. - Experiments on a set of hyperparameterized SPH simulators to test the theoretical results.
SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,This paper studies the problem of estimating the uncertainty in the likelihood of a given set of distributions over a set of data points. The paper proposes a regularization regularizer that allows one to estimate the probability that a given point in the space of distributions will be close to another point in another space. The main contribution of the paper is to show that this regularizer can be used to train a neural network that is able to provide estimates that are close to each other and that are not too different from each other.    The main contributions of this paper are as follows :   1. Demonstrating the regularizability of the proposed regularizer in terms of its ability to distinguish between different sets of distributions. 2. Showing that it is possible to train neural networks that are confident in their uncertainty estimates. 3. Providing sufficient training data to train the regularizer.
SP:365490b872464f00634dc7a50d024fceaf0a61ee,"This paper proposes a generative auto - encoder - based approach for generating video representations that can be used in conjunction with text - to - video ( TED - talk ) models. The proposed approach is based on the idea that a video can be represented as a combination of linear transformations of a set of tokens, and that the goal is to learn a representation of the space under which the tokens can be embedded. The authors provide a number of qualitative and quantitative evaluations of the proposed approach."
SP:86f9f89f84e117c86478b9afaf087f65524f5472,"This paper proposes a meta - learning framework with adaptive data - adaptive meta - regularization for the classification of medical image classification tasks. The proposed framework has the following components :   ( 1 )    a pre - defined set of meta - training tasks, ( 2 ) a set of data sets, ( 3 ) an adaptive data regularization strategy, ( 4 ) a classification task set, and ( 5 ) a classifier task set.   This paper shows that the proposed framework achieves state - of - the - art performance on the proposed set of tasks."
SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"This paper studies the problem of learning representations of finite - world flows under finite - difference perturbations, and proposes a theoretical framework to provide theoretical guarantees on the representations learned from the perturbed data. Specifically, the paper considers the following problem : given a finite set of data points $ \mathcal{x}$ and a finite - distance vector $ z$, what is the best way to learn a representation of $ z$. The paper shows that if $ z \log p(x)$ is not too different from $ \cal p(z)$, then the learned representations are not sensitive to the perturbation $ \sqrt{z}$.    The paper further proposes a learning strategy based on the notion of downstream predictors, and shows that this approach leads to better representations than the naive approaches based on distance - invariant representations."
SP:404d5643327f60f0f06f820033a56081f9e01900,"This paper proposes a message - passing framework for graph neural networks ( GNNs ) that is based on query - conditioned isomorphism counting. The key idea is to construct a set of graphs that are isomorphic to a fixed set of subgraphs, such that a message passing mechanism can be applied to each subgraph without changing the underlying structure of the underlying subgraph. The paper provides sufficient conditions on the subgraph to ensure that the message passing scheme converges to the given subgraph representation under certain assumptions, e.g., that the number of messages per subgraph is less than some threshold value, and that there exists a subgraph of size at least as large as the size of the input graph."
SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"This paper proposes a federated learning framework that aims to bridge the gap between personalized and federated knowledge. The proposed framework is based on the idea of factorization - factorization ( SimFed ), where the goal is to achieve task - level similarity with respect to a set of datastreams. The paper proposes two approaches to achieve this goal. First, the authors propose to federate the knowledge from two clients ( e.g., a teacher and a student ), and then the teacher uses the data from both clients and the student uses the dataset from the other client. The authors prove that the two approaches converge to each other in terms of the parameter space and the number of data points, respectively. The second approach, called Personalized Federated Learning ( APFL ), is a special case of SimFed where the teacher and student use the same dataset ( i.e., they only need to know the labels from the student ).    The paper also proposes a dataset - level metric that measures the similarity between the student and the teacher. This metric is called parameter space similarity loss ( MLE ), which is defined as the ratio of the student's knowledge loss to the teacher's loss."
SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"This paper presents a video reasoning framework ODDN that learns to predict dynamic representations of objects from video input. The framework is built on top of the Distillation Network ( DQN ) framework, which has been shown to achieve state - of - the - art results on several video reasoning tasks. The main contribution of this paper is the introduction of a scene decomposition ability that is able to predict the quality of a given scene from a sequence of videos, and an object - pair interaction ability that can predict the similarity between two sequences of videos."
SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"This paper presents a theoretical analysis of the epistemic equivariance of cross - layer neural networks ( GNNs ) and proposes a new theoretical framework to study the problem. The theoretical analysis is based on the fact that the features of the network are not invariant to the distance between the input nodes, and proposes to encode the features into a set of vector vectors that can be used to predict the distance to the source nodes. The proposed theoretical framework is then applied to three different learning tasks :    ( 1 ) GNN prediction, ( 2 ) feature encoding, ( 3 ) cross layer neural network analysis.   The paper provides theoretical analysis on the theoretical and empirical properties of the proposed framework. The paper also provides empirical results showing that the proposed approach is able to converge faster than previous approaches."
SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,This paper proposes a supervised text style transfer framework that learns to parallelize a large set of data from two sources ( e.g. text and formality ) without the need of fine - tuning the model size. The main contribution of the paper is the introduction of a learning refinement mechanism that ensures that the model is not too different from the source source in terms of the amount of data used to train the supervised model.    The main contributions are as follows :   1. Introducing a learning framework that allows the model to be trained on a large number of parallel data sets ( i.e. tens of datapoints ) in a single training run. This is done by training the model on a set of source and target datasets that are parallel to the source data and the target data. 2. Conducting a series of experiments to validate the effectiveness of the learning framework and to show that the learned model can be used in a variety of downstream tasks.
SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"This paper tackles the multi - hop reasoning problem of answering queries in hyper - relational context - based knowledge graphs ( KGs ). To this end, the paper proposes a set of embedding techniques based on graph neural networks ( GNNs ), where the goal is to learn a knowledge graph ( KG ) that can be used to answer queries in a logical reasoning framework.    The key contributions of the paper are as follows : 1 )   to solve this problem, the authors propose a new embedding scheme based on three types of relations : ( 1 ) relational, ( 2 ) classical, and ( 3 ) triple - relational. The proposed embedding is based on the embedding of the set of relations in [ 1 ], [ 2 ] and [ 3 ], which is then used to learn the knowledge graph [ 4 ]."
SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"This paper provides a theoretical analysis of the learning curve of the recently proposed BO ( BO ) algorithm in the context of the multi - fidelity hyperparameter optimization setting. The main contributions are as follows :   1. The authors prove that BO converges to the optimal solution in a finite number of iterations, i.e., $ \mathcal{O}(\sqrt{T}^{-1}$, under the assumption that $ \ell_t$ \infty$ is the number of samples in the box $ \text{T } \log n\infty$. 2. They show that the error rate of the algorithm converges as $ \log t$ goes to zero, and provide a theoretical justification for this result. 3. They provide a proof of the convergence of BO in the case of the hyperparametrized version of the problem $ \alpha$, and show that it converges faster than the naive version $ \nabla_t$. 4. Finally, they provide an empirical evaluation of the performance of the BO algorithm in terms of the following metrics."
SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,This paper presents a theoretical and empirical analysis of image compression - based generative models for cross - platform training and fine - tuning. The main contributions are the following :    1. introduction. This paper provides a theoretical analysis of the image compression model and its derivation.   2. A quantitative derivation of the compression model is presented. 3. An empirical analysis is presented showing that the proposed model is able to learn a mixture model in a deterministic manner.
SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"This paper proposes a new unsupervised training approach, NRRN, for the analysis of cellular ultrastructure data sets. The proposed approach is based on the idea of spatial invariance of data sets with respect to the resolution and the number of cells in the data set.    The paper proposes two approaches to this problem. The first approach is to train a set of data points $ \mathcal{T}$ and $ \cal{R}$ with $ \text{T } \infty$ resolution, $ \sqrt{T}\infty$. The second approach uses $ \tilde{T\infty}$ resolution $ \nabla_{t } \log(t)$, $ tilde{R } \leq \log{t}$, and $ p(t ) \log { t}$.   In the experiments, the paper shows that the proposed approach can be applied to a variety of different data sets, and the results show that the approach is more robust to different resolutions and different numbers of cells."
SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"This paper proposes a new label trick - based training framework for graph neural networks ( GNNs ). The proposed approach is based on the Open Graph Benchmark ( OGB ) framework. The key idea is to train a GNN on a set of data points and then train the model on top of the data points. The training objective is to ensure that the model does n’t leak any label information during the training process. To this end, the authors propose two label trick strategies. The first one is to exclude label information from the input data points that are not part of the training set. The second one uses the model - free label trick strategy. Experiments show that the proposed approach outperforms the state - of - the - art GNN methods on the OGB benchmark."
SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"This paper proposes a theory of mind in multi - agent reinforcement learning setting, where agents are assumed to have access to both natural language and machine world views of the world. The goal is to provide a theoretical foundation for deep reinforcement learning in this setting.    The main contribution of this paper is the following :   ( 1 ) Theorem 1.1 states that the knowledge base of a given agent can be divided into two parts : ( 2 ) A datum base that encodes the agent's knowledge about the world, and ( 3 ) a datum that maps the agent ’s knowledge to the knowledge of the data base of the other agent. Theorem 2.1 is proved in the case of natural language, and theorem 3.2 is proven in the setting of machine world."
SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,This paper presents a zero - shot object detection algorithm for scenes produced in a dynamic manufacturing environment. The proposed method is based on the GZSD ( Zero - Shot Secured Secured Structured Visual Representation ) framework. The key idea is to split the video dataset into different categories based on their size and volume. Each category is divided into a set of scenes and then a classification layer is used to classify the objects in each category. The output of each category is used for the next classification layer. The main contribution of this paper is to show that the proposed method can detect objects produced in the different categories.
SP:aa1dcd9217270010f16a00004facede942efea17,"This paper presents a video - based latent video prediction framework, which is based on a transformer - based generative model. The key idea is to predict the future frame of a video with a high - resolution ( 256x256 ) and high - fidelity ( $ \ell_\infty$ ) data augmentation. To achieve this goal, the authors propose a series of approaches :    1.   a latent video generator model, which has been trained on a variety of datasets, including both complex and large - scale datasets ; 2. a video prediction dataset, which consists of a set of videos with a fixed number of frames and a fixed resolution ; 3. a training set of video prediction benchmarks, where the quality of the video is measured by the ratio of the number of video frames sampled from the training set to the total number of videos sampled, and 4. a quality metric, which measures the difference in the quality between the video and the set of data sampled from it. The proposed framework has been shown to outperform the state - of - the - art approaches."
SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,"This paper presents ViT - based generative adversarial networks ( GANs ), a framework for generating high - resolution images of identifiable faces for GAN - based models. The proposed approach is based on the ViT framework, which has been proposed in [ 1 ], [ 2 ] and [ 3 ]. The main contributions of this paper are as follows :    1. Providing a set of benchmarks for improving the quality of images generated by GAN models. This includes : ( 1 ) improving the image recognition capabilities of ViT generative models ; ( 2 ) improving their stability ; and ( 3 ) providing more robustness to changes in the generative model parameters."
SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"This paper proposes a two - stage training procedure to train a generative autoencoder with two types of data distributions. The first stage consists of training the generative model on a set of image data, and the second stage is to train the model on the second set of data. The main idea of the training procedure is to divide the data into two stages, i.e., the first stage samples the data at the beginning of training, and then trains the model at the end of training.    The main contributions of this paper are as follows :   1. Introducing the concept of “ imperceptible information ”, which is defined as the probability that the output of a given model is not too different from the input at the start of training ; 2. Demonstrating that the imperceptibility of a model can be measured by the ratio of the number of samples in the training stage over the total number of training examples ; 3. Demonstituting that the probability of not having too many samples in training is a good indicator of the quality of the model."
SP:bfed56018134ec66cde9a7e958df964d4cca3164,"This paper proposes a new inference framework for score - based probabilistic models ( DPMs ) with training - free inference framework.    The main contributions are as follows :   1. This paper provides a new derivation of the KL divergence between the score function and the covariance matrix in the framework of DPM. The derivation is based on the fact that the score functions can be expressed as functions of the data covariance matrices. 2. This derivation provides a lower bound on the reverse variance of the score, which is then used to derive the upper bounds on the score - likelihood of the model."
SP:3f935ba5784c3e86db72421426bc479061af1a4b,"This paper proposes a new image classification and segmentation framework for medical image classification, which is based on transformer - based models. The proposed framework has three main components : ( 1 ) transformers - based transformer model, ( 2 ) attention - based transformers model, and ( 3 ) self - supervised transformer model.    The proposed model has been trained in both supervised and unsupervised setting. Experiments have been conducted on four image classification tasks : image detection, image segmentation, transformers transformers, and transformers transformer. Results show that the proposed model can achieve state - of - the - art performance on all four tasks."
SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"This paper presents the results of a pre - trained neural language model ( NLM ) for natural language understanding ( NLU ) tasks. The model consists of two stages : pre - training and fine - tuning. Pre - training consists of training a set of sentences that are similar to each other, followed by a series of experiments to fine - tune the model. The experiments are divided into two parts :    1. pre - pretraining, in which the model is trained on a fixed number of examples, and 2. fine - training, where it is fine - tuned on a large set of examples that are not related to the target domain ( e.g., question answering )."
SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,This paper proposes a new optimizer framework based on a symbolic representation and analysis framework. The goal of the framework is to provide an unsupervised learning of the optimizability rules and to train the optimizer in a data - free manner.    The framework consists of two parts :   ( 1 )   a set of $ \ell_t$-normals $ \infty$ and ( 2 ) a set $ \tilde{L}$ of $ tilde{O}$ -normals. The first part aims to provide a theoretical framework for the optimization of $ Tilde{Optimizability}$ under $ \mathbb{R}$. The second part aims at providing a practical framework for learning the optimizers in the data - not - so - safe room of optimization.
SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"This paper proposes a policy smoothing - based defense strategy against adversarial perturbations in the multi - agent supervised learning setting. The main contributions are two - fold :   ( 1 )    a smoothing based policy that is designed to be robust to both smoothing and perturbation - based adversarial attacks, and ( 2 ) a policy that can be used to adaptively adjust the parameters of the smoothing policy in the presence of an adaptive adversarial agent.   The smoothing technique is based on the assumption that the policy is invariant to the agent's actions, and that the perturbed policy is not too different from the original policy in terms of its smoothing parameters. The authors prove that the proposed policy can be made robust to a set of adversarial actions that are bounded in the form of smoothing, and provide a certificate of robustness for the adaptive policy."
SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"This paper studies the binary classification problem with a toy model. The toy model consists of two components : a source and a target distribution. The source data is assumed to be generated by a random distribution, while the target data is given by the distribution of the source data.   The paper proposes two methods to train the toy model : ( 1 ) a softmax - based method and ( 2 ) a hyperparameter - based one. The paper shows that the hyperparametrization error of the softmax method is less than the one of the target method."
SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"This paper studies the problem of point set registration and distribution matching ( PDM ) in the context of the multi - label learning setting. The authors propose a new method to solve the PDM problem by estimating the discrepancy between the label of a point set and the distribution of the point set under the assumption that the distribution is invariant under a set of non - rigid deformations of the original distribution. To this end, they propose a discrepancy discrepancy regularizer and a distribution matching regularizer, and show that both methods converge to the same solution when the number of deformations is large enough."
SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,This paper presents a meta - feature extractor - based transfer learning framework that is based on meta - features extracted from the dataset of the open source hyper - datasets OpenML and OpenAI. The authors provide a set of metrics that quantify the energy consumption of different hyperparameter configurations across the source and target datasets. The paper also provides an analysis of the source - target and target - target hyper - feature space to evaluate the performance of the proposed approach.
SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,This paper proposes a new fake detection and attribution framework based on deep generative models. The proposed framework is based on a bi - level neural network with two layers of deep neural networks. The first layer is used to generate deep fakes and the second layer uses the generated fakes to generate a few bits of information.    The authors provide a detailed analysis of the proposed framework and show that it is able to achieve state - of - the - art results in terms of both detection and attributability.
SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"This paper proposes a local explanation framework for black box similarity learning. The proposed framework is based on a combination of two approaches : ( 1 ) black box model agnostic local explanations and ( 2 ) text - agnostic black box models.    Contributions :   1 ) This paper provides a theoretical analysis of the two approaches and shows that both approaches are equivalent to each other in the sense that they satisfy the same objective function under certain assumptions ( e.g., they are not differentiable ). The paper also shows that black box explanations can be used to improve the performance of black box learning in a number of practical settings, such as domain specific machine learning and machine translation learning."
SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"This paper proposes a new training approach to improve the robustness of single and ensemble ML models. The proposed approach is based on regularization - based training approach, where a single ML model is trained with a DNN, and an ensemble of ML models is trained on top of it. The main contributions are as follows :    1. Introducing a new metric, L2 - robustness metric, which measures the distance between the model - smoothness and the smoothness of the data under the assumption that both the base model and the ensemble model are smooth.   2. Using this metric, the authors propose a new strategy to train ensembles that are more robust than the single base model. 3. Conducting experiments to validate the effectiveness of the proposed approach."
SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"This paper presents a lower bound on the complexity of the message passing graph neural networks ( GNNs ). More precisely, given a graph $ G$ and a subgraph $ z$, it is shown that $ \text{text}$ is convex if and only if $ z \in [ 0, 1]$ is a convex combination of $ \mathbb{Z}$ and $ z\in [ 1,2]$, where $ z $ is a graph of size $ \infty$ such that $ z(\infty)$ is either convex or convex with respect to $ z$.   This is achieved by pooling $ z(x, z)$ by $ \sqrt{x}$, which is a weighted sum of $ z^{-1}$ over all $ z}$ subgraphs $ z_\infty$. In particular, $ z_{-1,2}$ can be expressed as a product of $ p(x,z)$ where $ p$ is the number of vertices of $ x$, $ x_i$ the subgraph of $ y$, and $ g$ the dimension of the vector $ g(x)$ between two vertices $ y_i$."
SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"This paper presents a knowledge - enhanced graph convolutional simulator ( GCS ) based on the K - Adapter framework, which aims to provide users with a more complete set of relational knowledge about the underlying model ( e.g., the source and target entities ), which can be used to improve the performance of the simulator.    The authors present two approaches to this end. The first approach is to train the model on a set of source entities, and the second one is to augment the source entity description with the knowledge of the target entities. The authors demonstrate that both approaches are able to achieve better performance than the baselines, and that the k - Adapter approach is more robust to changes in the source entities."
SP:7e73948421e98307fceb69a316d8a4e7c4926cda,"This paper studies the problem of learning how to adapt to changes in the context of linear regression. Specifically, it considers the case when the regression function is a linear regression, where the data points are assumed to be sampled from a fixed distribution ( e.g., $ \mathbb{R}^2 $ ), and the learning rate is the ratio of the number of samples required to learn the linear regression function ( $ \log n$ ) over the set of data points sampled from the fixed distribution.    The paper considers two versions of this problem, i.e., linear regression with fixed $ \lambda$ and linear regression without fixed $ n$, and shows that both versions of the problem are equivalent to each other in terms of learning rate and risk minimization. Moreover, it is shown that for both linear regression and non - linear regression cases, there exists a learning rate that minimizes the risk of not adapting to the new data points in the given context."
SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"This paper proposes a new dataset - free source - domain shift - based method for improving the data efficiency of feature extractor and feature calibration methods. The proposed method is based on the concept of space - space class separation, where the source domain is the set of features extracted from a given source dataset, and the feature space is a subset of the source dataset. The authors claim that the space - class separation is necessary for better data efficiency. To this end, the authors propose two approaches : ( 1 ) distillation - based source - space separation and ( 2 ) feature - extractor - based distillation.    The authors also propose a new training scheme to improve the quality of the extracted features."
SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"This paper proposes a new robustness metric for data - driven federated learning ( FL ), which is based on the FedRBN framework. The main idea is to train a client - wise robustness framework that ensures that the data produced by the FL framework is not too different from the data generated by the standard federated training framework. To achieve this goal, the paper proposes two approaches : ( 1 ) a data - efficient robustness setting ( AT ) and ( 2 ) a knowledge - distillation - based ( DE ) approach.    The main contributions of the paper are as follows :   1 ) introducing a new metric that quantifies the difference between the knowledge produced by FL and the standard training framework in terms of the cross - entropy between the two. This metric can be used as a metric to evaluate the robustness of the FL and DE approaches. The paper also shows that the DE approach is more efficient than the knowledge distillation approach."
SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"This paper studies the question of how well we know the structure of the world's strategic interactions in dynamic games. Specifically, the paper focuses on the following questions :    1. How well do we know how the world ’s strategic structure affects the strategies we take to play the game?   2. What is the best strategy to take to ensure that we do n’t learn too many strategies that are not too different from each other? 3. What are the best strategies to learn from data that do n ’t rely on other strategies?"
SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"This paper proposes a new graph completion and relation prediction framework based on the concept of “ cross - relationship learning ”, which aims to predict the cross - relation between two sets of data points ( e.g.    a set of graphs $ \mathcal{X}$ and $ \cal{Y}$, where $ y$ is the number of elements in the input graph $ x$, and $ Y$ is a subset of $ z$, $ z_0 $ is the size of the input set $ x_1 $ and $ y_2 $ is a subgraph of $ x_{\cal{x } \log p}$ with $ z_{\text{x}$ = 0, z_1 \log z_2$.   The proposed framework is based on two approaches :   ( 1 )   * Cross - relation learning ( CCL ) *, which is an extension of the previous work “ Cross - Relationship Learning ” ( Xie et al., 2017 ), and ( 2 ) * Graph Embedding Learning ( GraphANGEL ) * which is a more generalization of CCL.   Experiments show that CCL outperforms CCL by a large margin, especially in the setting of quadratic graphs."
SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"This paper presents a series of experiments on the problem of supervised learning with pre - trained data augmentation. In particular, the paper focuses on the image analysis community and the few - shot image augmentation community.    The main contributions are as follows :   1. A set of experiments is conducted to analyze the label - efficient pre - training of the supervised learning dataset. The experiments are divided into three parts : ( 1 ) image analysis, ( 2 ) image classification and ( 3 ) label efficient learning. The image classification community is divided into two groups : image - based and image - free community. The first group analyses images from supervised learning data, and the second group analyses data from the expert - supervised learning community. Experiments are conducted on a set of datasets ( e.g., CIFAR-10, Fashion - MNIST, Cifar-100 ). The results show that the supervised - learning dataset is more data - efficient than the data - free one, and that the expert dataset is better than the expert one."
SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,This paper studies the problem of training neural networks ( RNNs ) to detect hidden state trajectories in time series. The main contributions are two - fold :   ( 1 ) train RNN with mixed - memory - RNN ( mmRNN ) and ( 2 ) train neural networks with time series - based counterparts ( TRTN ).   The first contribution is to show that the data sampled from the two networks can be used to detect trajectories with different timesteps. The second contribution is a proof - of - concept approach to train an RNN that can detect the time - lags of trajectories sampled from both networks.
SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"This paper presents a theoretical analysis of the problem of minimizing the bit - cost of quantization of BERT model in the resource - constrained setting. The main contributions are as follows :    1. The paper analyzes the effect of different bit - centric operations on the performance of binarized BERT quantization methods and proposes a new method, DMD ( Directionally Distillation with Mixing Distillation ), to tackle this problem.   2. A theoretical analysis is presented showing that the proposed DMD outperforms the previous approaches in terms of bit cost and bit - complexity. 3. Experimental results are provided to demonstrate the effectiveness of the proposed method."
SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"This paper presents an attention - based multi - person keypoint detection and association problem with the goal to identify and classify entities that are likely to appear in the context of a particular instance segmentation task. To this end, the paper proposes two approaches : ( 1 ) similarity - based attention and ( 2 ) cross - attention.   The attention approach is based on a CNN - based top - up model, where each instance is represented by a vector field vector, and the attention task is to identify entities that appear in both the vector field and the subset of entities that can be represented by the attention model. The paper provides a theoretical analysis of the attention problem and shows that both approaches are suboptimal when the number of entities per instance is large. To overcome this problem, the authors propose a multi - point detection approach, where they first classify entities based on similarity and then classify them based on the similarity of their representations to the target entities. Experiments show that the proposed approach outperforms the previous approaches."
SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"This paper studies the problem of efficient policy learning in reinforcement learning ( RL ) setting, where the goal is to learn a policy that maximizes the utility of the actions taken by the agent. The paper proposes two approaches to this problem. The first approach is based on minimizing the variance in the agent's actions in order to minimize the cost of making decisions. The second approach uses a differentiable optimization method to estimate the variance of the agent ’s actions. Both approaches are empirically validated on synthetic and real - world datasets."
SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,This paper proposes a new method to adapt a distribution - conditional neural network ( DNN ) model to the mmWave FPGA testbed. The key idea is to train the model on a set of datasets and then train the decoder network on top of these datasets.    The main contributions are as follows :   1. The authors propose a new DNN model to be trained on the data from the testbed and show that it is able to adapt to the test - time domain well. 2. They show that the model can be trained to handle different distributions of the source data. 3. They provide a proof - of - concept error rate for the model.
SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"This paper proposes a new model with Structural Loss ( IMSL ) for natural language inference. The main idea is to replace the softmax $ \ell_t$ with a $ t$-hardmax $ t$.   The key idea of the proposed model is to assume that the natural language model $ T$ is not too different from the one $ T $, where T is the number of natural words in the target language $ T$. In order to make the model more data - efficient, the authors propose a softmax loss of $ t \ell_{t } \in \mathbb R^{-1/2}$, where $ t_{t}$ is the size of the data set and $ R^2 $ is the ratio of the total amount of data in the source and target language.    This paper shows that the model with $ t $ can be seen as a special case of the previous model with structured loss ( i.e., $ t_t$. The main contributions of this paper are as follows : 1 )   i.i.i. show that there exist data sets $ T_{t+1}$ for which the model can be made more data efficient by adding $ t^2$. 2 ) i.iii. give a lower bound on the total number of data required for the model to be able to make $ t+1$. 3 ) iiv. give an upper bound on how much data can be added to the model in order to achieve $ T+1$."
SP:17cd72df5fc19398f582d27516fd742b073f79e3,"This paper proposes a method to obtain OOD - robust classifier / data - free classifiers / distributions for deep neural networks. The main idea is to train a discriminator / classifier that is adversarially robust with respect to a set of data points sampled from a distribution that is uncertain about the true distribution of the data points. The authors propose two ways to do this : ( 1 ) to train the discriminator and the classifier on the same set, and ( 2 ) to select the hyperparameters such that the resulting classifier and data - conditioned distributions are similar.    The paper provides theoretical analysis of the proposed method and empirical results to show that the proposed approach can be applied to a variety of deep neural network architectures."
SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"This paper proposes a new class of transferable adversarial attack methods based on white - box surrogate models.    The main contribution of this paper is to show that the proposed method is transferable under the following assumptions :   1. There exists a black - box model $ \mathcal{x}$ $ \theta$ such that $ \text{x } \in [ 0,1,2,3]$ can be transferred to any dataset $ x$ with probability $ \sqrt{x}{\theta}$. 2. For each $ \tilde{x},$ there exists a $ \infty$ classifier $ \alpha$ $ such that the attack vector $ \eta$ $ can be translated to another vector $ x$. 3. There exist $ \nabla$ classifiers $ \beta$ $ zeta$ and $ \overline$ classesifiers $ x_{\text{z } } \to $ \log(zeta)$.   This paper provides a proof - of - concept for $ \sigma$-based attack method $ \to zeta$."
SP:2e0447c741a3f09be1095633d870200355211260,"This paper investigates the problem of false negative pre - training and post - training of language models. The paper proposes two approaches to address this problem. The first approach is to leak a pre - trained language model ( PrLM ) to the unsupervised learner, and the second one is to improve the quality of the PrLM by providing an update to the language model. To this end, the paper proposes a series of experiments to evaluate the performance of the two approaches. The experiments are divided into two parts : 1 ) to identify the source of the leak and 2 ) to mitigate the issue of criticality. The main contributions of this paper are as follows :   1 ) identify and mitigate the source for the leaky PrLM model ; 2 ) perform an analysis of the impact of the source on the criticality of PrLM ; 3 ) propose an improvement strategy to prevent the source from leaking."
SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"This paper proposes a semi - supervised learning approach to address the problem of class variance in the cross - domain cross - label ( SSL ) setting. To this end, the authors propose two approaches : 1 ) unlabeled single - cell dataset and 2 ) labeled data from ImageNet dataset.    The first approach is based on the idea that the class distributions of the two datasets are similar and can be used to predict the class variance between the two data sets. The second approach uses a differentiable classifier to learn the class of the data from both datasets. The authors provide a theoretical analysis of the proposed approach and empirically verify the performance of their approach."
SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"This paper provides a theoretical analysis of the convergence of the second - order update rule for transformers and ViT models. The authors show that the update rule is unstable with respect to the number of iterations and the size of the training set. The paper also provides theoretical guarantees on the stability of the update and memory overhead in the case of large datasets. Moreover, the authors provide empirical evidence that the second order update method is more stable than the first order update in the presence of large scale datasets."
SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"This paper proposes a locality - sensitive hashing ( LSP ) method for graph neural networks ( GNNs ) that aims to preserve the local properties of the graph while speeding up the processing time of the neural network. The proposed method is based on the locality sensitive hashing method proposed in ( Xie et al., 2020 ). The main contribution of this paper is to provide a theoretical justification of the correctness of the proposed LSP method and to show that it can be applied to a variety of real - world datasets. The paper also proposes a number of variants of the LSP that can be used for different types of graph classification tasks."
SP:c5e024f4e2079586298519ca868630efd7579eca,"This paper proposes a new adversarial augmentation method for self - supervised learning ( SLE ) with data augmentation to improve the performance of VAE.    The main contributions are as follows :   1 ) This paper proposes an objective that augments the VAE with data from a set of benchmarks to improve its performance. The goal is to ensure that the data is not too disentangled and that the representations of the data are not too different from each other. 2 ) To achieve this goal, the paper proposes two types of augmentation strategies, i.e., adversarial space augmentation ( IDAA ) and information distillation ( IDSH ). Experiments show that the proposed approach outperforms the state - of - the - art SLE methods in terms of both the quality of the augmentation and the overall performance."
SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"This paper proposes a new method for checking the timestruckness of data generated by machine learning models trained on a sequence of data points with different timesteps. The key idea is to use a function of interest that measures the ratio of the probability that a given data point is timestocked with respect to another data point, and that the ratio is greater than a certain threshold. The paper proposes to use this function to test the hypothesis that the data distribution of a given model has a given timestock, and to test whether the model is able to produce data that satisfies this hypothesis.    The paper presents two approaches to this problem. The first one is to train a model on a set of data, and the second one uses a model trained on top of top - k data. Both approaches are empirically shown to be able to provide evidence that the model produces data that is statistically indistinguishable from the real data."
SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,This paper proposes a neural ODE - based machine learning approach for generating high - resolution video clips of physical phenomena. The model consists of two components :   ( 1 ) a neural encoder - decoder model that encodes the video sequence into a set of parameters and ( 2 ) a soft - body dynamics model that predicts the velocities and rotations of the objects in the scene.   The encoder consists of a convolutional neural network and a neural decoder that is trained on a large set of videos. The authors show that the encoder is able to generate high - quality images of the physical phenomena in the training videos. They also show that their encoder can be used to improve the accuracy of the long - term term prediction in state space.
SP:51efd1451343f4994d857daa5490e299b812bc2d,"This paper studies the problem of policy distillation and policy learning in the context of continuous variable process ( cvsP ) and discrete continuous variable processes ( DCPs ). The paper proposes two approaches to this problem. The first approach is based on the prior knowledge of the dynamics of a discrete variable process, and the second one is a meta - RL approach.    The main contributions of the paper are as follows :   1. A theoretical analysis of the two approaches is presented, which shows that both approaches are equivalent to each other in the sense that they both lead to the same final policy. 2. A trade - off between model quality and policy quality is provided, which is shown to be optimal for both approaches. 3. Experimental results are provided to demonstrate the effectiveness of the proposed approaches."
SP:ea167b126212b2092bc1190d7f8376bf7c54a888,This paper presents a knowledge - based pre - trained language model for cross - lingual NLP tasks. The model consists of two components :    1 )   a knowledge graph   2 ) a knowledge model   3 ) a set of knowledge tokens   4 ) an entity recognition   5 ) a retrieval mechanism   Experiments are conducted to validate the effectiveness of the model.
SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"This paper proposes a multi - agent learning framework that allows agents to interact with each other in a task - agnostic manner. The proposed framework is based on the concept of supervision learning, where agents are assumed to be experts in a specific task and are supervised by a manager who wishes to ensure that the agent ’s actions are consistent with the goals of the other agents. The paper proposes two approaches to this problem :    ( 1 )   proxy learning agents that are conditioned to behave in a manner similar to that of the supervision agent, and ( 2 ) a learning agent that is conditioned to act in a way similar to the agent in the supervision setting.   The paper presents convergence results for both approaches. Specifically, the paper shows that the proxy learning agent converges faster than the supervised agent when it is conditioned on actions that are similar to supervision agents, and that the learning agent is able to learn policies that align with supervision agents ’ actions in a similar manner."
SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"This paper proposes a new loss function for neural networks that can be used as a replacement for the loss function proposed in [ 1 ]. The new function has the property that it does not depend on the dimension of the input space, and can be applied to any neural network with kernel regression.    The paper provides a theoretical analysis of the new function and shows that it is equivalent to the standard loss function in [ 2 ]."
SP:b485114712055f39a7afb951dbc3db482ff523fd,"This paper studies the problem of improving the trainability of graph neural Tangent Kernel ( GNTK ) models. The authors prove the following results :    1. For a fixed $ \ell_t$, there exists a $ \tilde{x}$ such that $ \text{x } \infty$ can be approximated with $ \mathbb{R}$.   2. For fixed $ t$ and $ tilde{z }, there exist $ \nabla_{x}^2 $ such that for $ t=1,2 $, $ \log n(x, z)$ is approximated by $ \sqrt{\frac{x}{\sqrt{z}$, where $ z}$ is the number of vertices in the graph $ z$, $ y$ the size of the input set $ z_0 $, and $ A$ the dimensionality of the embedding space $ y_0$. 3. For $ t = 1,2$, the optimal solution can be obtained by iteratively adding new vertices or deleting existing vertices.   The authors also prove that $ t\infty^2$ is also an upper bound for $ \sigma^2$."
SP:25a92b3583afdc6892e59f1e769125d52c8011af,This paper proposes a novel method to measure the criticality of the second derivative of the first - order dynamics of a finite - order flow model. The key idea is to train the second - order and higher - order derivatives of the dynamics in a similar way to how one would train the first derivative.    * * Contributions * * * :   1. This paper proposes to train a second derivative - derivative - based criticality measure that is based on the concept of criticality. 2. The paper shows that this metric can be used to improve the quality of the trajectory - based vital sign measurement. 3. It is shown that the proposed metric is more robust than the one proposed in [ 1 ].
SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,This paper presents a communication system for multi - agent language learning. The main idea is to provide a mapping between the agent ’s communication system and a symmetric mapping of the target language. This mapping can then be used to perform a set of interactions with the source language.   The main contributions of this paper are as follows :   1. Providing a mapping from the source to the communication system that can be used as a basis for language learning ; 2. Conducting a series of experiments to demonstrate the effectiveness of this mapping.
SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"This paper presents a policy - based approach to improve the agent - agent interaction experience in the Robotic agent framework. The approach is based on two components : ( 1 ) the object encoding module ( OEM ) and ( 2 ) the policy composition controller ( PCC ). In the OEM, the goal is to provide a policy that allows the agent to escape from a given state without having to re - visit the target state. In contrast, in the PCC, the goals are to provide an agent that can navigate to a new state without re - visiting the original state.   "
SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"This paper proposes a new model of the cross - entropy distribution of the cosmological variable x - ray classification problem. The key idea is to use the concept of cross entropy distribution as a measure of the distance between the source and target x - rays, which can be used as a diagnostic of the nature of the source.    The main contribution of the paper is to show that the cross entropy problem can be formulated as a set of two questions :   ( 1 ) what is the probability that the source - target distribution is cross entropy?   and ( 2 ) what are the distributions of the sources and the targets that are cross entropy differentiable?"
SP:c75998b76f4e0510fc719d25959a10fc07db1c40,This paper proposes a zero - shot image - text distillation method that learns to distill a set of images and text pairs from a dataset that has been pre - trained on a large number of image and text encoders. The key idea is to train the encoder and decoder on the same dataset and then distill the images and the text pairs based on the similarity matrix of the two datasets. The authors provide a theoretical analysis of the optimal distillation strategy and show that it is optimal when the dataset size is large and the number of datasets is large enough.
SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper presents a dataset - free approach to the question answering task in the framework of the Visual Question answering ( VQA ) framework. The goal is to provide a set of tokens that can be used in conjunction with question answering tasks such as object detection, object description and class classification. The proposed approach is based on the following steps : ( 1 ) create a corpus of question answering tokens, ( 2 ) train a classification algorithm on the corpus, ( 3 ) annotate the tokens with the question answer labels and ( 4 ) train the classification algorithms on the annotated tokens and the question question labels.    The experiments show that the proposed approach achieves state - of - the - art performance in terms of terms of the number of tokens per task and the amount of training time."
SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,This paper proposes a policy distillation approach that distills a symbolic policy to a teacher - student model. The key idea is to distill the policy into a set of symbolic rules and then train a policy regression algorithm to find the policy that minimizes the sum of the policy and the teacher policy's sum. The paper also proposes an end - to - end learning pipeline where the policy is distillable to both the teacher and student.
SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"This paper proposes a new training scheme to improve the performance of the VQ neural network ( VQNN ). The proposed training scheme is based on a combination of two components : ( 1 ) image - to - space ( image - space - space ) masking and ( 2 ) object - masking.   The first part of the training scheme consists of two stages : ( i ) image synthesis and ( ii ) object masking, where the first stage is to synthesize a set of images of a given shape and composition. The second stage consists of image - inversion - based masking ( inversion encoder ) and ( iii ) cross - domain ( cross - attribute - based ) supervision. The authors show that the proposed approach outperforms the state - of - the - art methods in terms of both quality and supervision."
SP:e51a7f45493064972585109f203a867e9828eb15,"This paper proposes a multi - layer perceptron ( MLP ) architecture for speech processing tasks that is based on knowledge - driven feature extractor. To this end, the authors propose two approaches : ( 1 ) local temporal dependency and ( 2 ) bridging the gap between different MLP architectures. The first approach builds on the existing GPT - V2 - 35 dataset ( VoiceBank ) and aims to provide a more complete knowledge base for MLP - based speech processing. The second approach is a bridging - based approach that builds on top of GPT-2 dataset and aims at providing a better MLP architecture for the task of speech enhancement.    The authors conduct extensive experiments to validate the effectiveness of the proposed approach."
SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"This paper studies the empirical risk minimization and upper bounds of the transfer learning algorithm with two sets of data sets : source and target data sets.    The source set is a set of image classification and action recognition data sets, and the target set is the set of domain classification and classification data. The authors prove that the upper bound is $ \ell_0 $ \infty$ for source data sets $ \text{data}$ and $ \tilde{text}$ for target data set $ { data}$."
SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"This paper presents a two - stage training approach for generating generative models of 3D continuous geometry. The first stage consists of training a generative autoencoder and a voxel embedding model, and the second stage is to complete the training of the generative model by generating 3D scenes from the generated point cloud.    * * Contributions * * * :   1. This paper provides a thorough analysis of the two stages of training. It shows that the first stage is not complete, and that the voxels embeddings of the generated scenes are not uniformly distributed over the 3D space. This motivates the need for a lower bound on the scale of the space distribution. 2. The paper proposes a new lower bound, which is based on the fact that the number of points in the point cloud is larger than the size of the spaces. This lower bound can be phrased as follows : $ \sqrt{\theta}$ where $ \theta_t$ is the ratio of the total number of point clouds in the training set $ t$ and $ t\theta$ is a measure of how large the space is in terms of scale $ t$.   3. A proof - of - concept experiment is provided to show that the proposed lower bound is better than the previous lower bound."
SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"This paper considers the problem of policy reinforcement learning in the context of continuous control tasks, where the goal is to ensure that the agent does not overfit to a set of data points with different timesteps. To this end, the paper proposes two approaches : ( 1 ) temporal prior conditioned and ( 2 ) domain - conditioned. The temporal prior is assumed to be conditioned on the current state of the agent as well as the horizon of the next data point. The domain gap between the current and the future data points is defined as the ratio of the distance between current state and the horizon.    The paper shows that both approaches fail to ensure the same quality of the data points under different priors. To address this problem, the authors propose to divide the domain into domains with different temporal priors and to train a policy with respect to these domains. The paper also proposes to train the policy in two stages : ( a ) for the initial stage, the policy is conditioned on current state, ( b ) the agent is allowed to explore the domain for a fixed period of time ( i.e., time horizons ), and ( c ) at the end of the policy training phase, the agent has access to data points conditioned on future state and horizon."
SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"This paper proposes a new learning rate scheduler for deep neural networks, which is based on the concept of message passing. The key idea is to pre - train a set of datasets and then schedule the learning of a subset of these datasets to be passed through an optimizer in a given time horizon. The authors argue that this is necessary to ensure that the learning rate of the learned dataset is not too high compared to the number of examples that the network has to learn. To this end, the authors propose to divide the training time horizons into two parts : pre - training and post - training. Pre - training is divided into two stages, i.e., pre - defined classification and classification - based halving steps. During the pre - trained stage, the network is partitioned into a number of sub - networks and each sub - network is learned using a message passing algorithm. In the post - trained phase, the topology of each subnetwork is updated according to the message passing rate schedule.    The authors conduct experiments on CNN and Transformer networks to demonstrate the effectiveness of their approach."
SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,This paper proposes a unsupervised objectcentric learning model for the point cloud - centric object segmentation task. The model consists of two components : a point cloud and a spatial mixture model. The point cloud consists of a pre - trained point cloud model and a relational mixture model that is trained on a set of datasets. The relational model and the relational model are jointly trained on the same dataset.   
SP:3c57e921c1bf23e482551ceb71702931a7f07439,"This paper presents an action - based framework that aims to provide the user with access to knowledge about the natural language models ( LLMs ) for high - level natural - world tasks. The framework consists of three components : ( 1 ) a baseline that provides access to the LLMs for low - level tasks, ( 2 ) a set of actions that allows the user to generate actions that satisfy the LLM baseline, and ( 3 ) an action plan that allows for the agent to perform actions in the environment provided by the baseline.    The baseline that is used for the first component consists of the following components :   1 ) A set of action plans that are given to the user in the form of an action sequence that consists of a sequence of steps, each of which is accompanied by a context description. The goal of the action sequence is to generate an action that satisfies the action plan given in the context description, and the goal is to prevent the agent from performing actions that violate the action - plan. The actions that are generated are then evaluated by the agent using the action sequences provided in the second component."
SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"This paper proposes a generative model - free approach to learn latent space representations of VAEs. The proposed approach is based on a data augmentation scheme, where the generative models are assumed to have a uniform distribution over the space. The authors show that the proposed approach can be applied to a wide range of data settings.    * * Contributions * * :   1. This paper proposes an approach for learning VAEs in a data - free setting. The data is assumed to be generated by a neural network that learns a latent space representation of a given VAE. 2. The paper shows that the approach is able to learn a manifold of distributions over the learned latent space. 3. In order to ensure that the learned manifold is not too different from the original manifold, the authors propose a differentiable sampling scheme. 4. They show that their approach is robust to the number of samples and the amount of data."
SP:b4b8e1727f8617894f10f20365cb68de79f0e650,This paper presents a set of experiments on cross - entropy transformer - MGK / MLK with the goal of improving the performance of the softmax and linear transformers on the Long - range - arena ( LRA ) language modeling benchmark.    The experiments are divided into two parts. The first part is an in - depth analysis of the current state - of - the - art softmax transformers and their performance on the LRA benchmark. The second part is a series of experiments aimed at providing insights into the performance gap between the two approaches.
SP:82731dcce233e748f63382e09b6224a513fe9689,This paper proposes a zero - shot learning framework that learns from an inverse model of the spatial and temporal structure of the environment. The model consists of two components : ( 1 ) a first - person view - based navigation model and ( 2 ) a second - person - based trajectory - based model.   The first component is based on a neural network that maps the space into a set of spatial representations and ( 3 ) a policy learning model that learns how to navigate through this set of representations. The second component is built on top of the first and the third component is a policy - based approach that learns to navigate in the second component. The authors provide empirical results showing that the proposed framework outperforms the state - of - the - art in terms of both the number of failed attempts and the total number of attempts.
SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"This paper provides a theoretical analysis of the query model for neural networks. The main contributions are two - fold :   ( 1 ) This paper proposes a new type of gradient descent descent algorithm, which is based on the similarity between the input distribution of the feature vectors and the query vectors, and ( 2 ) this paper shows that the proposed gradient descent algorithm can be used to solve a set of data - independent feature learning problems."
SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,This paper presents a theoretical analysis of the problem of learning a classification model that is robust to adversarial examples. The main contributions are :    ( 1 )   a theoretical proof that the learned model is not vulnerable to adversaries ; ( 2 ) an empirical proof of the impossibility of learning an extractor that can not find examples that are not adversarial ; and ( 3 ) a practical algorithm to train such a classifier that satisfies the above two constraints.
SP:874b5fa51924cbcceed490d98a0ea80f74586b32,"This paper proposes a value - based reinforcement learning ( V - learning ) framework for learning from data - driven benchmarks. The framework consists of three components : ( 1 ) value - value cloning ( EVL ), ( 2 ) policy learning ( PLE ), and ( 3 ) learning - to - failure ( RL ). The first component is an iterative learning procedure that computes a set of rewards for each task in the dataset. The second component is a policy learning step that learns a policy that maximizes the sum of the rewards across all the tasks in the data set. The third component consists of a planning step that iteratively selects the best policy among all the existing policies and the set of tasks for learning.    The paper provides theoretical analysis on the importance of the different components of EVL and PLE, and empirical results show that the PLE and RL algorithms outperform the other two in terms of value - quality and error - quality."
SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"This paper proposes a new adversarial training framework based on classifier - conditioned margin - conditioned generative adversarial attacks. The proposed framework has two main contributions : ( 1 ) it provides a theoretical analysis of the proposed framework, and ( 2 ) it proposes a sample - based method to train the classifier on a multi - class dataset. The experiments are conducted on MNIST, CIFAR-10, and Fashion - MNIST."
SP:3ad36be6b6900aabe43da043461cf178ce977082,This paper studies the problem of designing non - linear message and update functions for graph neural networks. The main contributions are :    1.   provide a unified framework for both message and information passing over MLPs. This framework is based on the notion of feature fields. 2. provide sufficient conditions for a message to be non - steerable and invariant. 3. provide a sufficient condition for the information to be steerable.
SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,This paper proposes a differentiable physics model for differentiable interactions between differentiable materials. The main idea is to train a simulator that simulates the interaction between two differentiable physical systems by simulating a set of differentiable velocities and forces.    The main contributions are as follows :   1. Introducing the concept of common forces. 2. Demonstrating that common forces can be learned from a simple model. 3. Providing a data - efficient learning strategy to train the simulators. 4. Showing that the learned simulators can be used to solve differentiable dynamics problems.
SP:2c8358c095b10981d3015b9f6c75765419a9480d,"This paper proposes a new learning framework for the transfer learning of new skills in the space - based learning approach. The new framework is based on the notion of space - specific skill - space generalisation. The key idea is to divide the world into a set of tasks and assign a value function to each task, and then to learn a policy that transfers the learned skill to the new task in the given space.    The paper proposes two learning approaches : ( 1 ) a policy - based approach where the goal is to learn the new skill in the new space, and ( 2 ) an approach that learns the value in the learned space, which is to transfer the learned skills to a new task. The paper shows that both approaches yield similar results."
SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"This paper presents a theoretical analysis of the time series classification problem ( MTSC ) and proposes a solution - based method to solve it. The theoretical analysis is based on the time - series - based neural network ( TensorNet ) framework, which has been shown to be able to solve the original problem faster than previous methods. Experiments are conducted to validate the theoretical analysis.    * * Contributions * * :   1. This paper provides theoretical analysis and experiments to show the speed - speed trade - off between the number of samples used in the training and the total number of iterations used to obtain the solution. 2. A set of experiments is presented to show that the proposed method is able to find the solution faster than prior methods. 3. Experimental results are presented to verify the theoretical results."
SP:db43614ca016280a79448f44a97c81c8ff5ba981,"This paper proposes a novel approach to improve the performance of pre - trained text encoders in adversarial learning. The proposed approach consists of two steps :   ( 1 )   a pre - training framework that consists of multiple training signal generators, and ( 2 ) a set of tokens that are replaced by masked language models ( MLMs ). The authors show that the proposed approach leads to better performance compared to the baselines ( i.e., Gumbel - Softmax and SQuAD ).    The contributions are as follows : - A pre - train framework that is data - centric and pre - fine - tuned ; - A set of MLMs that are trained to produce tokens that can be replaced by the masked language model."
SP:db3825633ab5d0671340390b23ab655838cc38b2,"This paper presents a fact extraction framework that attempts to improve the fine - tuning performance of a pre - trained language model for the task of fact extraction from wikipedia abstracts. The framework is based on the BER triple task, where the goal is to extract a subset of facts from a set of facts that are not available in the source language. The authors propose two approaches to this task : ( 1 ) fine - tune the language model to ensure that the extracted facts are not too different from the source, and ( 2 ) provide a prompting mechanism to encourage the model to improve its performance on the target task.   The paper provides empirical results showing that the proposed framework improves the performance of the model on both the source - target and target - fact extraction tasks."
SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"This paper proposes a learning framework for embeddings of knowledge bases in hyperbolic spaces. The key idea is to train a knowledge base embedding $ \mathcal{Knowledge}$ on a set of data points $ x_0 $ and $ y_1 $ such that $ \text{x_0 } \infty$ can be translated to $ \cal{x_{\text{knowledge}$ and $ \log{y_1}\infty$.    The main contribution of this paper is to show that the embedding space $ \sqrt{Know}$ is invariant under $ \theta_{\theta}$, $ \tau_{\tau}$ being a metric space, and $ z_1$ being an embedding embedding of the knowledge base.  "
SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"This paper presents a one - shot learning framework that aims to predict new events in the context of a given set of entities in a real - world knowledge graph. The framework is built on top of the attention mechanism proposed in [ 1 ], where each entity is connected to a subset of other entities in the knowledge graph via a set of events. The goal is to learn relations between entities that can be used to predict the new events that will be discovered in the future.    The framework consists of three components :   1 )   one - attention mechanism that predicts new events for each entity in the set. This is based on the observation that entities that are connected to the same entity in a set will discover new events if they have similar attention mechanisms. The two components are called attention mechanism 1 and attention mechanism 2. The other component is a relation mechanism that learns relations between two entities that have different attention mechanisms but similar relations in terms of the number of events they will learn. This mechanism is called relation mechanism 3. The attention mechanism 4 ) learns relations among entities that will learn new events from a given entity if it has a similar attention mechanism to that of the entity that is not connected to it in the given set."
SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"This paper presents a set of multi - task learning modules for humanoid robots. The modules are based on visual reasoning tasks, where the goal is to build a humanoid robot that is able to solve a visual reasoning task in a multi - level manner. The paper proposes two approaches to this task learning problem : ( 1 ) learn the task by visualizing the input as a sequence of images, and ( 2 ) train the robot to solve the task using a combination of visual reasoning and reasoning modules. The experiments show that the proposed approach is more effective than prior approaches in terms of test accuracy and generalizability.   * * Contributions * * :   1. Introducing a new task learning framework, called Progressive Module Networks ( PMN ). 2. Conducting a series of experiments to validate the performance of the proposed framework."
SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"This paper studies the problem of improving the compressibility of convolutional neural networks with respect to the number of channels and the amount of features per channel. To this end, the authors propose to divide the network into three groups :    1. Convolutional unit ( SCU ), which is composed of two groups of networks, each of which has its own channel - selectivity and channel - to - feature ratio.   2. Architectural unit ( AiU ) which consists of two networks, one of which is based on the large - scale features ( e.g., CNN ) and the other one on the small - scale ones ( i.e., channels ). 3. Container - based unit ( CUB ) which is built on top of the above two groups."
SP:2d80fa4bc440061be2234b5070503d3fa056baed,"This paper presents a learning method based on conditional classifier with positive and negative data conditioned on real - world data. The goal is to learn a classifier that is able to distinguish between two sets of data that are identifiably different and that are conditioned on the same set of data ( i.e., the two sets are not identical ).    The main contribution of this paper is to show that the optimal classifier can be obtained by training the conditioned classifier on a set of positive and negatively - conditioned data, where the positive data comes from a distribution that is conditional on the real world data, and the negative data is from the conditional distribution of the data. To do so, the paper proposes two learning strategies : ( 1 ) learn - and - label ( 2 ) learn + label ( 3 ) learn and label ( 4 ) learn / label."
SP:5f312626b0613d2e07c59214c5f00db208a98717,"This paper studies the problem of supervised learning in the setting of multi - task supervised learning, where the goal is to predict the performance of a set of tasks on the basis of the similarity between the learned metrics of the tasks and the corresponding metric ( e.g., the number of steps in the learning process ).   The paper shows that there are two types of metrics that can be used to measure the quality of the learned metric. The first metric is the similarity metric, and the second metric measures the difference in the performance between the learnt metric and the metric used to train the metric. In particular, the metric metric metric can be thought of as the ratio of the average loss of the metric over the set of metrics over which the metric is learned compared to the metric that is used to learn the metric ( i.e. the ratio between metric and metric metric used in training the metric )."
SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"This paper proposes a geometric - based adversarial training framework, which is based on the geometric geometry of the data manifold, to provide more robustness to black - box attacks. The proposed framework has two main contributions : ( 1 ) it provides a more robust classifier than previous works, and ( 2 ) it introduces a new classifier that is more robust than the previous works.   [ 1 ] This paper provides a geometric framework that provides more robust classesifier than prior works. [ 2 ] It also provides a new neighbor classifier, which can be used to improve the robustness of the existing classifiers."
SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"This paper proposes a generative modeling framework for learning representations of time series and static data from real world medical time series. The proposed framework is based on a gradient - based variant of the organizing map algorithm proposed in ( Xie et al., 2020 ). The key contributions are :    1. Differentiability of differentiability of the gradients - based representation learning framework.   2. Representation space structure of the Markov model."
SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"This paper considers the following question : given a probability distribution $ \mathbb{R}$ and a generative model $ G$ over the space $ R$, what is the probability that there exists a vector $ \text{x}$ such that $ \tilde{x } \in R}$ is a linear interpolation of $ \phi$ with respect to $ \theta$?   This question was posed in [ 1 ] and [ 2 ].    The main contribution of this paper is to show that there exist distributions $ \gamma_{\theta}$ that satisfy the following property :   ( 1 ) for any $ \alpha$ and $ tilde$, there exist $ \lambda$-like vectors $ \sqrt{\theta } $ such that the probability $ \log(tilde|x)$ for $ \eta$ is linear interpolated over $ \Phi$, and ( 2 ) there exists $ \leq \log(-tilde^n)$ that is a nonlinear interpolation over $\gamma_\alpha$.   [ 3 ] This property is further generalized to the case $ \delta^n$, which is assumed to be a convex function over $ R$. In particular, this property is shown to be equivalent to the property [ 4 ]."
SP:19b63ca635712f1509ca6e0141303c192f2709e0,"This paper presents a new attention mechanism for the real - world graph question answering tasks. The attention mechanism is based on the attention operation on hyperbolic geometry, which can be viewed as an embedding of the embedding space of the object representations of the question answering dataset.    This attention operation is used to improve the cross - entropy prediction of the shortest path length between two vertices in the question - answering dataset ( CLEVR ). This paper also improves the cross entropy prediction for the machine translation task ( WMT-14 )."
SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"This paper proposes a new vulnerability analysis framework for neural networks based on the Flush+reload side - channel attacks. The main idea is to train a model $ \ell_0 $ on top of top - k layer of a neural network $ k$ with $ k=\ell_1$. Then, the attacker can perform a series of attacks on the top layer of $ k$. The authors claim that the proposed framework is more robust than previous work on the same topic.    The main contributions of this paper are as follows :   1. Defending against the previous works on the flush + reload attacks on neural networks. The proposed framework provides a better analysis of the model's vulnerability. 2. Introducing a new side channel technique that allows the attacker to identify which layer of the network is more vulnerable to the attacks. 3. Conducting experiments on a variety of neural networks to show the effectiveness of the proposed approach."
SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"This paper presents a neural network that learns to predict future video sequences based on the current state of the previous video sequences. The network consists of three components : ( 1 ) Hierarchical prediction network ( HPNet ), ( 2 ) feedforward - stream / analysis - by - synthesis ( FCT ) and ( 3 ) recurrent circuit / feedback network ( LSTM ). The main contributions of the paper are as follows :    1 )   * The authors show that the proposed model is able to predict video sequences with high accuracy. The model is trained on a large dataset of video sequences, and it is shown to be able to learn to predict sequences from a large set of unseen videos.   2 ) The authors demonstrate that the model can be trained to discriminate between different representations of the same video sequence. They show that this is possible because the model learns to distinguish between representations that are similar to each other and those that are different from each other."
SP:fb74e57f35666742caf651e6da33b5defcf259a8,"This paper presents a pipeline - based approach for the study of the similarity of the sequence similarity between two sets of data ( e.g., a set of rna sequences and the set of mutational sequences ). The idea is to use the similarity between the two sequences as a measure of information about the similarity in the space of possible mutations of the rna sequence.    The main contribution of this paper is the following :   1 ) to develop a pipeline that can be used to study the relationship between sequence similarity and space of mutations of a given set of data. 2 ) to train such a pipeline, the authors propose two approaches : ( 1 ) similarity - space - based pipeline and ( 2 ) vector - space based pipeline."
SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,This paper presents a series of experiments to evaluate the learning properties of compressors for visual recognition tasks. The experiments are divided into two parts. The first part aims to measure the number of iterations required to learn a visual recognition task in a compressed space. The second part is to measure how quickly the learned task can be transferred to the next one.    The paper provides empirical evidence to support the claim that compressors are not able to learn tasks that are not trivial to compress. The paper also provides a theoretical justification of the impossibility of learning tasks that can not be compressible.
SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"This paper proposes a value function learning framework based on an internal dynamics model. The proposed framework consists of two phases : online and offline. In the online phase, the agent is provided with a set of trajectories and is allowed to explore the world in a supervised manner. The offline phase is used to provide the value function to the agent in an offline phase. The paper provides theoretical guarantees on the uncertainty of the learned value function and the error of the uncertainty function."
SP:771494fda4702cd8c7efbf225b19028f91b449b9,"This paper proposes a zero - shot dual learning approach for unsupervised and semi - supervised machine translation ( MT ) learning. The main contribution of the paper is to jointly evaluate the performance of two approaches :    1.   Zero - shot learning approach, which aims to learn a baseline model of the source language and the target machine translation task. The source language is assumed to be the source domain and the machine translation is the target domain. The goal is to learn the capacity of both approaches to generalize the source and target language simultaneously. The two approaches are evaluated in the following way : ( 1 )   data - parallelization and ( 2 ) data - cross - domain evaluation. The proposed approach is shown to outperform the baseline - free approaches in both cases."
SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"This paper proposes a framework for learning correct adversarial frameworks from scratch. The key idea is to train a generative adversarial framework on a set of data points, and then provide a training set of questions and answers to the question posed by the given data point. The question answers are given in the form of a sequence of documents, each of which contains a probability distribution over the data points in the domain, and the question answer is given in terms of the probability that the answer is correct given the data point in question.    Experiments are conducted on the following domains :   [ 1 ] CIFAR-10 - E ; [ 2 ] DAGGER ; [ 3 ] NLR ; [ 4 ] MNIST ; [ 5 ] ADVERSAR ; [ 6 ] RMS ; [ 7 ] CQL ; [ 8 ] SGD."
SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"This paper proposes a variational inference framework for learning representations from sparse probabilistic sparse coding models. The main contribution is a lower bound on the posterior likelihood of the posterior distribution of the representations learned from the data synthesized from the model. The lower bound is based on the assumption that the prior distribution over the space of representations of the model is a mixture of two components, i.e., the distribution over representations of a given generative model and the distribution of representations over the set of generative models.    The main contributions of the paper are as follows :   1. A variational framework for inferring representations from data synthesis. The prior distribution is assumed to be a convex combination of a linear model and a non - linear one. 2. A posterior inference framework is proposed for learning the representations from this convex prior distribution. 3. A set of benchmarks is provided to evaluate the quality of the learned representations."
SP:06a22143186fa2948fbe324ccae96a62ff12064e,"This paper presents a non - adversarial feature matching - based approach to improve the quality of feature extractors for generative models. The proposed approach is based on the GFMN framework, which has been shown to outperform the state - of - the - art generative features extractors in several adversarial benchmarks."
SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,This paper presents a theoretical framework for the learning of neural networks ( NNs ) under the assumption that the underlying vector representation of a graph is invariant to all possible variants of the underlying graph. The authors provide a theoretical analysis of the capacity of different variants of GNNs under different assumptions on the underlying representation and the number of variants. They also provide theoretical guarantees on the learning rate of different NNs in terms of the expected number of instances of a given graph under the given assumptions. They provide empirical results showing that the proposed framework is able to learn NNs with a capacity that is at least as good as the current state - of - the - art.
SP:51126f2dd37ce57d2614c9044ede1e43627f0829,"This paper studies the problem of continual learning ( CL ) in the context of variational continual learning, and proposes a framework for CL that is able to cope with catastrophic forgetting, i.e., catastrophic forgetting when learning from a large number of examples. To this end, the paper proposes a variational CL framework that is based on the attention mechanism, where each example is divided into a number of categories, and each category is assigned a classification score based on its similarity to the previous example. The paper shows that the proposed CL framework can handle catastrophic forgetting in two ways : ( 1 ) catastrophic forgetting where the classification scores of all examples are close to each other, and ( 2 ) categorical forgetting where only a small fraction of examples are correctly classified."
SP:27a565b3e5442b93d208652784051e640b0c1bfe,"This paper proposes a framework for the evaluation of the robustness under adversarial attacks. The proposed framework is based on the idea of “ preserving adversarial perturbations ”, which has been introduced in prior works. The main contribution of this paper is the introduction of a metric that quantifies the violation of the invariance of the model under attacks.    [ 1 ] This paper proposes to use this metric in the context of a two - stage adversarial attack framework. The first stage is a standard adversarial example attack. The second stage is an attack that is more sophisticated and more data - efficient. [ 2 ] The authors show that the metric can be used to evaluate the quality of the attacks."
SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper proposes a two - stage reinforcement learning framework with two stages : policy - critic and policy - actor - critic. The first stage consists of learning a policy gradient and critic algorithm, and the second stage is learning an actor critic algorithm.    The first phase consists of fine - tuning the policy and the critic algorithms. The policy is fine - tuned on top of the policy gradient, while the critic is trained on the policy. In the second phase, the policy is trained with the actor critic, which is trained in the same way as in the first stage. The main contributions of the paper are as follows :   1. Providing a theoretical justification for the two stages. The authors show that the two phases are equivalent. 2. Demonstrating that the policy gradients converge to the critic gradients in both phases."
SP:89a732b57934d08b937c93560f391b7758e54f8a,"This paper proposes a new method to learn structural object representations from a disentangled set of data. The proposed approach is based on the idea of segmentation - based object representations, where a set of objects is first segmented into parts, and then the model is used to predict the parts from the segmentation data.    Experiments are conducted on the following datasets : ( 1 ) synthetic dataset, ( 2 ) real dataset, and ( 3 ) synthetic datasets. Experiments show that the proposed method is able to recover some of the features of the original dataset ( e.g., segmentation and dynamics )."
SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,"This paper presents a series of experiments aimed at assessing the quality of the training data used to train deep generative classifiers on the CIFAR10 dataset. The experiments cover a range of different feature space settings, from feature - free to feature - constrained settings, with the goal of assessing the classifier's ability to distinguish between generative and non - generative representations of the data.   The paper presents three main results :   ( 1 ) the classifiers trained on the cifar10 dataset are shown to be capable of classifying the data in a manner that is qualitatively indistinguishable from that of a generative model trained on a similar set of data ; ( 2 ) the model ’s performance on the different feature spaces is shown to improve as a function of the number of features used ; and ( 3 ) the robustness of the model to perturbations in the feature space used for training is demonstrated."
SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"This paper proposes a new motivation learning mechanism based on delayed feedback feedback. The motivation learning is based on the 2600 game, where the goal is to find a solution to a set of 2600 problems, each of which has a different number of solutions and a different scale of complexity. The paper shows that there are two types of delayed feedback mechanisms : ( 1 ) reward - based and ( 2 ) goal - based.    The first method is a linear combination of reward signal and goal discovery. The second one is an unsupervised learning method that selects an action to solve a given problem based on its complexity. In this paper, the authors show that both methods fail to converge to the same solution when the number of problems is large enough. To address this problem, the paper proposes to divide the problem space into 2600 games and 2600 sub - problems, and to use a combination of the two approaches. In the experiments, it is shown that the combination of both methods fails to discover the solution to the 2600 problem, but fails to solve the sub - problem."
SP:e5861538bc8bb9165cb33299bbf12dd875abf976,"This paper proposes a differentiable training procedure for the classifier of the SAT problem. The main idea is to train the encoder and decoder in the same way as in [ 1 ] and [ 2 ]. However, the difference is that instead of training the decoder $ \mathcal{T}$ on $ \theta$ variables $ \alpha$ and $ \text{T } $ variables $ g$, this paper trains $ \tilde{T}\alpha$ to $ \log(t\theta)$, where $ t$ is the number of variables and $ G$ is a binary variable.    The main contributions of this paper are as follows :   1. Introducing a new differentiable encoder - decoder framework for the classification of SAT problem   2. Providing an end - to - end differentiable learning procedure for this problem. 3. Demonstrating that this framework can be used to solve the satisfiability problem in differentiable manner."
SP:ff3e5d44619df3825632b0b1a943add081364861,"This paper proposes a combination of two reinforcement learning ( RL ) algorithms, namely DDPG and CEM, to solve the problem of policy search in deep reinforcement learning. The first one is an off - policy RL algorithm, while the second one is a policy search algorithm. The authors propose a cross - entropy method ( CEM ) to combine the two algorithms. CEM is shown to be more efficient than the other algorithm in terms of exploration efficiency, and the authors also show that CEM can be used to improve the performance of the policy - search algorithm in the V2 benchmark."
SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"This paper proposes a temporal and variable attention mechanism for extracting knowledge about the state of the universe in the era of the big bang. The attention mechanism is based on the concept of time series and variable importance. In particular, the temporal importance is defined as the ratio of the time interval between the current state and the previous state, and the variable importance is the ratio between the time difference between the present and the past state.    The key idea is to first extract the hidden state matrix and update process, and then to extract the time series of the update process. In order to do so, the authors propose two approaches : ( 1 ) to extract time series, and ( 2 ) to obtain variable importance, they define a new metric $ \tilde{x}$, $ z}$ and $ z$, which can be used to extract information about the past and future state, respectively. In the experiments, the proposed attention scheme is shown to outperform the state - of - the - art methods."
SP:1c26660569b579f060f7b4a31e321c6d2356b928,"This paper proposes a new smoothing and data augmentation method to improve the quality of the cross - entropy estimation of the decision boundary under the assumption that the data points are not too different from each other.    The proposed method is based on the up - smoothing method proposed in [ 1 ] and [ 2 ]. The key idea is to mix up $ \ell_t$ and $ t$ smoothing with $ t \in \mathbb R^{-1/2}$, $ t\infty$ data points, and $ p \infty$.   Experiments show that the proposed method outperforms the other two methods in terms of both quality and timestep."
SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"This paper provides a theoretical framework for the derivation of gradient descent rules for nonlinear neural networks with locally connected representations under the assumption that the input vectors are locally connected and that the representations are not convex. In particular, the authors consider the case of a convolutional neural network ( DCNN ) and a locally connected nonlinear network ( NLL ). The authors show that under certain assumptions on the inputs and the representations of the NLL and the locally connected NLL, it is possible to prove that the gradients descent rules are invariant under the assumptions of the nonlinearity of the inputs."
SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,This paper proposes a 0 - shot transfer strategy to train a neural network agent on a new set of tasks. The proposed approach is based on two components : ( 1 ) a behavior module ( BM ) and ( 2 ) a network architecture module ( NN ). The BM consists of two stages : ( i ) a training phase where the agent is provided with a set of new tasks and ( ii ) a reinforcement learning phase where it is asked to solve the new tasks in a supervised manner. The authors claim that the BM is more effective than the NN in terms of transfer quality ( PFC ) and the network expandability ( EWC ).
SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,This paper proposes a meta - meta - learning framework for plasticity reinforcement learning ( RL ) with a weight - modifying algorithm. The meta - training framework consists of two stages. The first stage is a pre - training phase where the model is initialized with a set of pre - trained tasks and the second stage is an iterative phase where a new task is added to the training set. The authors compare the performance of the two stages with respect to a baseline RL method ( A2C ) and a gradient descent method ( GPT ). The results show that the meta - RL method outperforms the GPT and the gradient descent methods in most cases.
SP:1ab5d94d31e99351433436c026799c8aa597bf73,"This paper presents a quantization - based approach to improve the precision accuracy of the WikiText2 and WikiText-1 models. The approach is based on the idea of quantizability of the model produced during the training phase. The paper provides a theoretical analysis of the quantization error and the memory consumption during training and shows that the approach can be used to improve both the model accuracy and the quality of the training data. Experiments are performed on two datasets, WikiText1 $ \text{text}$ and $ { text}$, to show the effectiveness of the approach."
SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"This paper proposes a new neural network - based method for learning representations of the softmax domain from scratch. The proposed method is based on cross - entropy gradient descent ( caldi et al., 2020 ).    The main contribution of the paper is the proposed method to train the neural network to be able to transfer domain knowledge from scratch to the target domain without the need to re - train the encoder / decoder layer. The paper also proposes a data augmentation strategy to improve the accuracy of the learned representations."
SP:d37e15cde7765fca87595a242f0a4511b3346d46,"This paper proposes a reinforcement learning algorithm for the problem of learning to navigate in discrete and continuous discrete action spaces. The main idea is to train the algorithm on a set of problems and to provide reinforcement feedback on the state - action permissibility ( SAP ) and action - per - problem ( PAP ) properties of the domain.    The main contribution of this paper is the following :   1. Introducing the concept of “ continuous and discrete action space ”, which is defined as the set of functions that can be used to solve a discrete action problem in a continuous space. 2. Providing a theoretical analysis of the continuous / discrete space property of the action space, and showing that it is equivalent to the property of a function that can solve a continuous action problem. 3. Demonstrating that this property can be leveraged to train an RL algorithm to solve discrete / continuous action problems."
SP:20015d8b60e13300586b67c281858cbe28825c48,"This paper proposes a training approach for deep autoencoders based on a multilayer vanilla autoencoder model with a weight - tie between the weights of the input tokens and the output tokens. The authors propose to train the model on top of a pre - trained batch of tokens with different dimensions, and to test the model by comparing the activation of the model with that of its vanilla vanilla counterpart."
SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"This paper proposes a new approach to improving the quality of the feedback provided by black - box adversarial attacks. The key idea is to use the cosine transform ( DCT ) principle, which states that the number of queries in an attack is equal to $ \sqrt{\sqrt{delta}$, where $ d$ is the total number of tokens in the attack and $ \tilde{D}$ is a fixed constant that can be used as a feedback metric. The paper shows that the DCT principle can be applied to the following types of attacks :    * black box attacks *   [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] [ 9 ] [ 10 ] [ 11 ] [ 12 ] [ 13 ] [ 14 ] [ 15 ] [ 16 ] [ 17 ] [ 18 ] [ 19 ] [ 20 ] [ 21 ] [ 22 ] [ 23 ] [ 24 ]"
SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"This paper proposes a new framework for the reinforcement learning setting where the goal is to discover a set of options that can be used to solve a given set of tasks in a high dimensional environment. The framework is built on top of top - k options framework proposed in [ 1 ] and [ 2 ]. The main contributions are as follows :    1. Introducing a new metric that quantifies the amount of information that needs to be extracted from the data in order to obtain a good representation of the data ; 2. Using this metric, the authors propose a probabilistic approach to estimate the value of each option in terms of the information contained in the data, and 3. Using the metric, they propose a new set of policies that are able to extract information about the dimensionality of the world and the number of actions required to solve the given task."
SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper proposes a new domain division algorithm, named K - S / S / G - ZSL, which is based on the concept of “ unknown and uncertain domains ”. Different from previous approaches, the proposed approach is to divide the domain into unknown and unknown domains by a set of threshold steps, and to learn the boundary between them. The authors provide a theoretical analysis of the different approaches, and empirical results are provided to show that the proposed approaches are more scalable than previous approaches. Experiments are performed on three different domains : unknown domain, unknown domain and unknown source domain."
SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper studies the problem of constructing a prototype network that minimizes the cross - entropy of a given distribution over a set of data points in a given dimension. This problem is formulated as a convex optimization problem, where the goal is to find a solution that satisfies the lower and upper bounds on the squared error of the distribution over the data points.    The main contributions of this paper are as follows :   -   1. provide a theoretical analysis of the problem, and show that there exists a lower bound of $ \ell_2 $ on the distance between any two data points if and only if they are located in the same dimension. 2. give a proof that this lower bound is upper than the upper bound given in ( Xie et al., 2017 ). 3. give an upper bound on the dimension of the space of possible solutions that can be obtained by minimizing the lower bound given the given lower bound."
SP:d1034342785d133cf8372b8624897963cc2ee83a,"This paper studies the problem of learning to tolerate different types of environments in an agent that may differ in terms of the amount of knowledge they contain about the underlying dynamics. The paper proposes two approaches to this problem. The first is a policy - conditioned learning approach, where the agent is provided with a set of tasks and is allowed to explore a large number of different environments ( e.g., environments in which the agent has never been before ). The second is a more data - driven approach, in which a subset of the environments is provided to the agent, and the goal is to learn a policy that minimizes the difference between the expected value of the actions taken by the agent in each of these environments.    The paper shows that both approaches are equivalent to each other in the sense that the difference in expected value between actions taken in the two environments is quantified by a metric that measures the ratio of expected value in each environment over the total number of actions taken, and that this metric can be used as a measure of uncertainty in the learned policy."
SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"This paper proposes a variational autoencoder ( VAE ) framework that learns a dependency structure between latent variables and the network parameters. The VAE is motivated by the observation that existing VAEs do not learn dependency structures that are flexible enough to handle a large number of variables. To this end, the paper proposes to learn a structure learning framework that allows the VAE to learn both topological and discrete dependency structures. The paper also proposes an objective and discrete differentiation framework to learn the structure of the latent variables. Experiments show that the proposed framework outperforms previous VAEs in terms of test accuracy and sample efficiency."
SP:976dedab53e69610692a563382ada1dbb82c1e9d,"This paper studies the problem of minimizing the total number of lexicographic dictionaries required to train a neural network with parallel representations. The authors prove that this problem can be formulated as a convex optimization problem where the number of data points required to solve the problem is bounded by a constant factor $ \mathcal{P}(\sqrt{T } \log p}$, where $ \text{T}$ is a factor of $ \nabla_{t}$ and $ \log(t+\log p)$ is the average number of datapoints required to learn a given representation.    The paper also proposes a learning strategy where each data point $ t$ is divided into subsets $ t_t$ such that each subset has the same number of dictionaries ( $ t_{t_t } = t_0, t_1 $ ). The paper claims that this approach is more data - efficient and more scalable than previous approaches that do not take into account the fact that the data points in each subsets may have different meanings ( e.g., different representations )."
SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,This paper proposes a new image segmentation - based auto - detection framework for neural networks. The proposed network consists of multiple encoder - decoders and a single network - based lane detection module. The main idea of the proposed network is to localize the information about the source images and the target images in the network.    The main contributions of this paper are as follows :   1. This paper proposes to localise the information of the source image and the source lanes in the same way. 2. The paper shows that the localization of the information can be used to improve the performance of the network in terms of visual appearance and level of accuracy.
SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"This paper studies the problem of policy optimization on large - scale contextual and interactive data sets. To this end, the paper proposes two approaches. The first approach is to train a surrogate policy on the contextual dataset and evaluate it on the interactive data set. The second approach uses the surrogate policy to evaluate and optimize the policy in the context dataset. The paper provides empirical results showing that both approaches outperform each other in terms of squared error reduction."
SP:8e0ed65c5dded23b34798499b2436b24422fd729,This paper proposes a new learning framework for mini - image classification tasks. The proposed framework is based on the idea of embedding a set of images into a feature space and then fine - tuning a few - shot classification task based on that feature space.    Experiments are conducted to validate the effectiveness of the proposed framework.
SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"This paper presents a series of experiments on improving the quality of gradient - based reinforcement learning algorithms for solving deep reinforcement learning ( RL ) problems. In particular, the paper focuses on the following :    1. Introducing a new type of reinforcement learning problem, named “ deep GA - powered novelty search ”, which aims to find a solution to a set of 5 deep RL problems. The goal is to find an algorithm that maximizes the difference between the solution to the original problem and the solution of the new RL problem.   2. Developing a new encoding technique for training deep neural networks. 3. Conducting experiments on the novelty search problem."
SP:dfdbe3267a8160f24746884cdf5297993e424231,"This paper proposes a curiosity - based learning framework that uses a first - person view curiosity to motivate the learning of 3D environments. The key idea is to create a set of environments that are both safe and interesting to explore, and then use the curiosity method to learn from these environments. To this end, the paper proposes two modules :    ( 1 ) Curiosity module, which learns a 3D environment dynamics model, and ( 2 ) Curiosity method, which builds on top of this dynamics model and learns a 2D environment model. Experiments are conducted on a variety of environments, and the results show that the proposed Curiosity module is more effective than the other two modules."
SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"This paper proposes a new approach to the problem of learning to navigate in a domain with uncertain timesteps. To this end, the authors propose a two - stage approach. In the first stage, they first construct a domain - specific reference - based set of tokens, and in the second stage they provide a set of actions that can be used to navigate through the domain.    The main contribution of this paper is the introduction of the concept of “ deictic tokens ”, which can be viewed as tokens that are tokens that can not be observed in the original domain, but can only be seen as tokens in the new domain. The authors show that this property is useful for the following reasons :   1. It allows us to use tokens that do not depend on the initial conditions of the domain ( e.g., the initial condition of the world before the start of time ), and thus can be easily incorporated into a more general class of tokens. 2. It enables us to make the transition model more data - efficient, since the number of tokens required to traverse the domain is no longer limited by the size of the data set. 3. In order to allow for the use of more data in the later stage of the process, they introduce a new metric that measures the ratio of the total amount of tokens needed to navigate across the domain to the total number of steps required to get from the beginning to the end of time, and use this metric as a metric called the “ greedy metric ”."
SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"This paper proposes a new feature selection method to select the relevant features in a data - driven neural network setting. The key idea is to select relevant features from a set of data points based on the similarity between the features observed in the data points and the ones predicted by the network. This is done by performing a series of experiments with different data points ( i.e., a static data point and a data point - wise feature selection ).    The main contribution of this paper is to show that the proposed approach can be used to select features that are relevant to a given data point ( e.g., in a network ), and that it is not necessary to know the quality of the feature selection. The paper also shows that the selected features are useful for other data points that are not relevant to the data point."
SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"This paper proposes a new adversarial learning scheme for patch - level domain - level neural networks. The key idea is to align the output space representation of patches with the one - level representation of the source domain. The authors claim that this is necessary for better performance in terms of scalability and scalability. To this end, the authors propose two approaches : ( 1 ) segmentation - level one - layer neural networks, and ( 2 ) patch- level neural network.    The first approach is based on the idea of segmentation, where each patch is divided into a set of patches, and each patch corresponds to a different source domain ( e.g. text or image ). The second approach is to segment the input into patches and then annotate each patch with the corresponding source domain representation. The author claims that this approach is more scalable than the first one, and that the two approaches are complementary to each other."
SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"This paper provides a theoretical analysis of the learning of gradient descent algorithms in the context of stochastic gradient descent. The authors show that the learning process of the ADAGRAD algorithm can be divided into two stages, i.e., a mini - batch and a large - batch of gradients, and provide theoretical guarantees on the speed of convergence of the algorithm in both stages."
SP:52228b48f2776d57dd422edb33b82e247f056b75,"This paper studies the topic of corruption and perturbation robustness in the context of classifiers. Specifically, the paper focuses on the following topics :    ( 1 )   corruption robustness. The paper proposes a new metric that measures the ratio of the ratio between the number of corrupt instances in a dataset with respect to the total number of examples in the dataset. This metric can be used as a metric for comparing the quality of the classifiers under different assumptions ( e.g., whether they are clean or not ). ( 2 ) “ robustness to perturbations. ” This paper proposes to use the notion of “ perturbational robustness ” to measure the difference between the amount of corruption under a given assumption and the total amount of examples under a different assumption.   The paper also proposes a way to improve the robustness of a given classifier by taking into account the fact that it is subject to adversarial attacks. This is done by adding an additional term to the metric “ adversarial defense ” that aims to make the classifier more robust to the presence of adversarial examples. The main contributions of this paper are as follows : ( a ) to show that the proposed metric is more robust than the one proposed in [ 1 ], and ( b ) to prove that the new metric is better than the previous metric in the case of clean and non - clean datasets."
SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"This paper presents a dropout - based approach to improve the quality of the dropout masks used by Dropout - RCMC. The main contribution of this work is a new dropout model, which is based on a combination of two approaches : ( 1 ) a power - mean - based model and ( 2 ) a heavy language modelling approach.    The main contributions of the paper are as follows :   1 ) i. improve the accuracy of dropout by using a more data - efficient dropout training. This is done by training two models, one of which is a softmax and the other is a hardmax. The softmax model is trained on a set of data points sampled from the Dropout dataset. 2. improve the performance of the heavy language model by training it on top of the softmax."
SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"This paper proposes a new pruning strategy to improve the robustness of CNN models. The paper provides a theoretical analysis of the effect of different pruning strategies ( i.e., soft pruning vs. hard pruning ) on the quality of the recovered models. In particular, the paper focuses on the importance of the ratio of the pruning rate to the number of data sets used in the model training and the amount of data used in each filter pruning step.    The paper also provides theoretical analysis on the impact of the different types of pruning. The main contributions are as follows :   1 ) The paper analyzes the effect on the model robustness when the number and quality of data set used for pruning are different. 2 ) This paper shows that the default pruning policy is not enough to ensure that the recovered model is robust to small changes in the set size. 3 ) To address this issue, this paper proposes to prune the set sizes of the filters in each layer of the model."
SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"This paper presents a novel method to train a text - to - text encoder - decoder that can be used in conjunction with a lexicon - based classifier and resource - based encoder. The encoder and decoder are trained in two stages : first, the encoder is trained on a set of text - only documents ( e.g., a few lines of text ), and then the decoder is used to generate text - based embeddings of the corresponding documents. The decoder can then be used to learn a character representation of the documents, which is then used to classify the documents in the context of the resource encoder / decoder."
SP:544e421f9c747640d949f433e3091763508b7237,"This paper studies the problem of supervised action localization. Specifically, it focuses on the following three tasks : ( 1 ) average attentional network ( MAA ) operation, ( 2 ) supervised temporal action localization ( TTA ) operation and ( 3 ) location - of - the - art ( AOA ) operation. Each of these tasks has its own advantages and disadvantages.    The main contributions of this paper are as follows :   1 ) An analysis of the average aggregation problem. The paper shows that the average action localization problem is intractable for some subset features ( e.g. $ \ell_1 $ ), and proposes a new strategy to aggregate features from different subsets of the same feature space to obtain the most salient subset features. The proposed approach is shown to be more efficient than the previous approaches ( i.e., average and average aggregation )."
SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"This paper presents a new representation framework for structured language models. The framework is based on the reduced representation ( HRR ) framework proposed in [ 1 ] and [ 2 ]. The main contributions are :   1.    the introduction of the HRR framework, which provides a unified view of structured and unstructured language models, and   2. a set of natural language processing models that can be used for representing structured and disentangled representations."
SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"This paper proposes a value - based action selection and planning algorithm based on point - based uncertainty reduction. The key idea is to select an action from a set of observations, and then make a decision based on the uncertainty reduction of the selected action. The paper provides a theoretical analysis of the optimal uncertainty reduction and a proof - of - concept value iteration algorithm. Experiments are conducted on a variety of different Robotic navigation scenarios, and it is shown that the proposed method achieves better performance than the state of the art."
SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"This paper proposes a new metric to measure the “ safeness ” of cross - entropy loss for deep neural networks. The key idea is to divide the training set into two parts, i.e., the first part is a set of examples, and the second part is to train a neural network on top of these examples. The main idea is that the neural network ’s performance will be degraded as the number of examples goes to infinity. The paper proposes to measure this metric as a metric of “ SAFE ”, where SAFE is defined as the ratio of the cumulative loss of the two parts over a fixed period of time, and SAFE can be regarded as a measure of the difference in performance between two networks trained on different versions of the same dataset.   The paper shows that SAFE measures the following metrics :    1. SAFE in terms of the total amount of examples in the first half of the training phase, and inversely proportional to the total number of iterations in the second half. This metric is used to measure SAFE for the first time in a deep neural network, and it is shown to be a better metric than the one used in prior work. 2.. SAFE metrics on the amount of data used in each iteration, and how they are distributed across layers. This is also shown to correlate with SAFE."
SP:8b555b9f24044bc68c204169d6a37e262361d706,"This paper studies the problem of "" greedy rollout "", i.e., the case when the goal is to provide a good value function to all customers who arrive at the same time. This problem has been called the "" Travelling Salesman Problem ( TSP ) "" in the literature, and has been studied extensively in the last few years. In this paper, it is shown that the greedy rollout problem ( PCTSP ) is equivalent to the original TSP ( VRP ) in the sense that it is a convex optimization problem with two subproblems : ( 1 ) the value function is not convex, and ( 2 ) there is no solution to both problems.    The main contributions of the paper are as follows :   1. This paper shows that there is a non - convex solution to the TSP, and that the solution is convex. 2. This gives an upper bound on the number of iterations required to solve the problem. 3. This upper bound is tight, and the lower bound is upper bounded by a factor of 2."
SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"This paper proposes a new architecture search framework, DNAS, which aims to find a neural architecture that minimizes the precision quantization cost of a given set of models. DNAS is motivated by the fact that most of the existing works on neural network quantization do not consider the space complexity of the models used to quantize the models. The main contribution of this paper is that it proposes a space - based optimization framework that does so by taking into account both the space and the dimensionality of the data sets used for quantization.    The main contributions of the paper are as follows :   1. Introducing the DNAS framework, which is based on the well - known Sec. 3.2 objective. 2. Providing a space and a dimensionality analysis of each data set used for the quantization of the model. 3. Developing a new bit - wise quantization method that is able to assign a precision to each data point in the data set."
SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"This paper proposes a new attention model, which is based on the multi - headed attention model proposed in [ 1 ] and [ 2 ]. The main idea is to combine two attention models, one of which is a single attention model ( i.e., the attention model $ m$ ) and the other is a set of attention models $ p(x, z ) $ z$, where $ p$ is the number of tokens in $ z$. The authors show that the proposed attention model can be used to solve the following sequence prediction tasks :    1.   \theorem 1.1. \max p(z ) \inftyq(z)$, \sqrt{\theta } \log p(y|z)$.   2. \theta ( z ) \text{max p(\theta})\log p(\sqrt{z}/z}$."
SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"This paper studies the problem of image - to - image cross - sectional translational invariance and self - consistency in medical imaging tasks. Specifically, the paper focuses on the following : given a set of data points $ \mathcal{x}$ and a set $ \text{x } $ of labels $ \theta$, what is the best way to translate $ x$ to $ y$?    The paper proposes two approaches to this problem. First, the authors propose to translate the data point $ z$ to a vector $ z_0 $ and then translate $ z_{\theta } = z_1 $ to another vector $ x_2 $, which is then translated to a pair of vectors $ y_1$ and $ z^{-1}$, which are then translated back to the original $ z$. The second approach is to use a graph - based approach, where $ x_{\text{xi}$ is replaced by a vector - based embedding $ y_{\tau } = y_2$. The paper shows that both approaches are equivalent to each other in terms of the following metrics : $ \tau \infty$, $ \sqrt{\theta}$ ( \log p(z_0, z_2)$, and $ \nabla p(x_2,z_1)$ where $ p(\tau)$ is the number of vectors in the input vector and $ p(y_2 ) is the cardinality of the output vector. The paper also shows that the two approaches are inversely proportional to $ \sigma$, showing that $ \gamma$ is orthogonal to the input vectors.   Experiments are conducted on image classification tasks ( e.g., medical imaging ), and the paper also includes a qualitative study of the proposed approaches."
SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"This paper presents a theoretical analysis of the vanishing gradient problem ( EVGP ) for gradient based neural network training. The authors show that it is np - hard to find a solution that converges to the optimal solution in time $ t$ in terms of the number of iterations $ t$. They show that this is due to the fact that there are two factors : ( 1 ) the size of the set of input vectors and ( 2 ) the amount of iterations required to obtain a solution. The paper proposes a way to minimize these factors by dividing the input vectors into a linear and a non - linear component. The linear component is used for training the neural network, while the nonlinear one is used as a benchmark for evaluating the quality of the solution.   The authors provide theoretical analysis and empirical results showing that the proposed approach converges faster than the state - of - the - art gradient based training algorithms ( e.g., [ 1 ] and [ 2 ] ). They also provide empirical results on the following tasks : ( a ) learning a neural network model from scratch, ( b ) training a set of benchmarks, and ( c ) training neural networks from scratch."
SP:9aaff3777321347d1194884af5690b0b5185eff9,"This paper proposes a policy network architecture for visual recognition tasks based on parameterization of the distribution of binary weights. The proposed approach is based on the deep learning perspective, where the goal is to learn the parameter structure of the parameterized distribution of the binary weights, which is then used for the recognition tasks.    The paper provides a theoretical analysis of the proposed approach, and empirical results show that the proposed policy network achieves state - of - the - art performance on visual recognition benchmarks."
SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"This paper proposes a federated learning with neural networks framework for image classification. The proposed approach is based on the idea of pooling image classification datasets from different federated networks, where each network has its own pool of classification data and weights. The authors show that the proposed approach can be applied to a variety of image classification problems, which can be decomposed into two types of problems : global classification and local classification."
SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"This paper studies the problem of learning to play games that are safe from catastrophic losses. The paper proposes two approaches to this problem. The first approach is to play a set of games in which the goal is to learn a policy that minimizes the total number of losses incurred by the opponent. The second approach is a variant of the stochastic convex optimization ( CO ) algorithm that aims to minimize the number of iterations required to learn the policy.    The paper provides theoretical guarantees for both approaches. In particular, the paper shows that both approaches converge to the same solution under certain assumptions on the initial state and the set of losses. Moreover, it shows that the trajectories of the two approaches converge linearly to each other in time, and that the trajectory of the CO algorithm converges linearly in time as well."
SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"This paper proposes a new classifier for medical segmentation task. The proposed method is based on a linear regression method, where the goal is to find a set of data points $ \ell_t$ such that $ \tilde{x } \infty$ can be approximated by $ \sqrt{x}$, where $ \text{xi}$ is the dimensionality of the feature space $ \mathbb{R}$ and $ \theta$ is a function of $ \alpha$.    The main contributions of this paper are as follows :   1. Introducing a novel classifier $ \sigma$ that can be used for segmentation. 2. Using this classifier, the paper proposes to train a segmentation algorithm $ \nabla$ that learns $ \eta$-\theta-\infty$. 3. Developing an adversarial network $ \beta$ that trains $ \ta$ based on $ \gamma$. 4. Conducting a series of experiments to validate the performance of the proposed classifier."
SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"This paper studies the problem of image decoders and representations in deep neural networks under the assumption that the input image is a linear combination of a set of channels. The authors prove that under certain assumptions on the dimensionality of the input data and on the number of channels, the decoder will be able to provide representations that satisfy a certain metric.   The authors provide a theoretical analysis of this metric and show that it is equivalent to the following metric : $ \ell_0 \in \mathcal{L}$ where $ L$ and $ \alpha$ are the dimension of input and output, respectively. The paper also shows that the metric can be viewed as a special case of the metric introduced in [ 1 ] and [ 2 ]."
SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"This paper proposes a multi - layer neural network for generating short code snippets in natural language ( NL ). The proposed framework is based on the attention mechanism proposed in [ 1 ]. The attention mechanism consists of two components : ( 1 ) an end - to - end embedding mechanism that learns a sequence of tokens from the source code to the target code, and ( 2 ) a gradient descent mechanism that trains a neural network to predict the next token to be added to the input sequence. The authors show that the attention and gradient descent mechanisms can be used to improve the quality of the short code generated by the proposed framework."
SP:d2ec231bb6153a303e5110e671dea14c2721e636,"This paper studies the problem of robustness to small adversarial perturbations in neural networks. To this end, the authors propose two approaches : ( 1 ) a generative adversarial model based on flow - based perturbation, and ( 2 ) a combination of the two approaches. In the first approach, the model is first perturbed with a set of L2 and L1 images, and then the authors show that the proposed approach is more robust than the baselines of the previous works. The authors also show that their approach is also more robust to L1 and L2 attacks. The second approach, on the other hand, trains the model on a larger set of examples, and shows that it is less robust."
SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"This paper studies the problem of controlling the singular value decays of the weight matrices of GANs under the assumption that the number of elements in the set of values is bounded by a constant factor.    The main contribution of this paper is to show that there exists a set of data points such that for any $ \ell_t$-th dimension $ \infty$, there exists an instance $ \tilde{x}$ such that $ x}$ can be expressed as a weighted sum of $ x$ and $ z$, where $ \text{xi}$ is the dimension of the set $ z$.   In particular, given $ \mathcal{x } = \ell_{\infty}$, the paper shows that there exist a set $ x_t $ such that the ratio of $ \alpha$ over $ y$ is greater than $ \sqrt{\alpha}$ for all $ \nabla_{\alpha } \log n(\sqrt{x})$, and that $ \log{xi } \leq \log N \in { \alpha}$."
SP:8115fd9b681198d62100c36794926fb57dc0a4f5,"This paper proposes a new value - based Q - learning method, A2VI, which is based on the idea of “ Accelerated Value Iteration ” ( A1VI ). The main idea is to iteratively update the value of a point $ \ell_0 $ at each time step $ t$ in a way that is “ safe ” in the sense that it does n’t depend on any prior knowledge of the value $ t$. The paper also proposes a “ game - based ” approach to evaluate the quality of the iterated policy. The paper provides a theoretical analysis of the games - based approach and empirically shows that it is able to converge faster than the standard value iteration approach."
SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,"This paper considers the incremental learning scenario where the goal is to learn a vector machine ( SVM ) that can be used as a data regularizer for deep learning ( DL ). The paper proposes two approaches to this problem. The first approach is to train the SVM on a set of data points, and the second one is to use the learned model on top of the data points.    The paper provides a theoretical analysis of the two approaches."
SP:d228d213f79716774043cea253305fecece659ec,"This paper studies the problem of top - class and unit selectivity of representations of neural networks. Specifically, the paper focuses on the question of how to construct representations that are robust to short - term variations in the number of classes and/or lengths of the representations. To this end, the authors propose two measures : ( 1 ) top - conditional mean selectivity score ( TCTS ) and ( 2 ) group - specific metric ( GSM ).    The first measure is based on the notion of _ short term memory _, which is defined as the ability of a neural network to preserve a set of examples for a given set of queries over a finite number of queries. The authors show that the TCTSS and GSM measures can be combined to form a metric that is robust to changes in the length of representations and in the amount of queries for which the representations are preserved. The paper also shows that the GSM metric can be used as a metric to measure the extent to which representations are able to preserve long - term invariance under perturbations."
SP:b9deae0392e0160b400d76c549d382e235196f8c,"This paper studies the problem of community detection of graph neural networks ( GNNs ) from the perspective of edge - wise classification and edge - cut problem from a statistical point of view. Specifically, the paper focuses on the following : given a set of 5 classes of graphs $ \mathcal{G}$ and a set $ \text{x}$ of edge adjacencies $ \lambda_t$, what is the probability that $ \alpha_t(x)$ is a subset of $ \gamma_0 $? This question is phrased as follows : given $ \log(x,\lambda_0)$ where $ x$ is the number of classes of $ G$ and $ y$ the size of the set $ G$.    The paper proposes two approaches to solve this problem :   1.   the first one is based on the well - known GNN - based approach [ 1 ], where $ \beta_t(\lambda_1)$ corresponds to the set of $ g$-th classes of the given graph $ { G}$, and the second one is a more recent approach [ 2 ], based on a differentiable graph - neural - network approach [ 3 ]. Both approaches are shown to converge to the same closed - form solution in the limit of infinitely many classes."
SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"This paper studies the problem of dictionary - based machine learning ( DL ) based on least squares ( least squares ) problem. The authors provide a theoretical analysis of the convergence properties of the proposed approach. They show that the update - based approach converges linearly with respect to the number of factors in the least squares problem, and provide a rate - of - convergence result for the update step. They also provide a proof that the updated - step approach recovers the original data better than the original one. Finally, they provide a neural implementation of their approach."
SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"This paper presents a new approach to the problem of information retrieval from binary hash codes. The proposed approach is based on the following ideas :    1. Each time a binary hash code $ \text{x}$ is sent to a server $ \mathbb{R}$, it is assumed that the server has access to a set of $ \tilde{x } = \alpha$ hash codes $ \gamma$ such that $ \theta$ can be retrieved from the server with probability $ \sqrt{L_{\theta } \log p}$ where $ p$ is the number of tokens in the input. 2. The server is assumed to have access to at most $ t$ bits of information. The goal is to find a way to minimize the cost of retrieving such a bit of information by minimizing the difference between the probability of retrieving the input and the value of $ p(x)$.   The main contributions of this paper are as follows : 1. This paper proposes a new way to estimate the loss of a bit - wise - similar - to - another - bit - similar hash - code by computing the difference in the likelihood between the two bits, and 2. This is done by computing a new loss function $ \sigma_{\text{l}$ that can be used as a metric to distinguish between two bits - differentiable and -differentiable hash - codes."
SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,"This paper presents a theoretical analysis of the tradeoff between the speed and accuracy of human - designed hypernetworks and neural networks designed to predict the future arrival time of the next query. The paper shows that the current state - of - the - art neural networks ( e.g., GNNs, ImageNet, etc. ) are not able to achieve the speed - accuracy tradeoff required by the current world average. To address this issue, the paper proposes to tradeoff the speed / accuracy of the human designed neural network with that of the neural network designed by the user. To achieve this tradeoff, the authors propose to design neural networks that are more robust to changes in the design of the weights used in the neural networks."
SP:65ccf43cd4e033d22239069057f5200d49f33724,"This paper proposes a policy - based adversarial imitation learning ( AiRL ) framework for learning classification functions from expert demonstrations. The baseline consists of two components :   ( 1 )   an expert - based learning baseline, which consists of a set of expert demonstrations, and ( 2 ) a policy that is conditioned on the expert demonstrations to learn a classification function.    Experiments are conducted on the following tasks : ( a ) image - based tasks, ( b ) domain - specific tasks where the expert is provided only a few examples, ( c ) classification tasks where both the expert and the policy have access to a large set of examples. Results show that the AiRL framework is able to learn classification functions that are better than the baseline baseline, and that the policy is more stable than the data - driven baseline."
SP:e8427949a98effbd37ce7604fa11f240e2342196,"This paper considers the following problem : given a set of data points $ \mathcal{x}$, estimate $ \text{x } \infty$ of the volume fraction of the total volume of the data $ \log(x_{\text{volume}/\sqrt{\text{inf } \log{volume})$, what is the best way to estimate the ratio $ \sqrt{inf}$ between the two sets?    This question is formulated as a convex optimization problem where the objective is to find a minimizer $ \tilde{xi}$ that minimizes the difference between the estimates of $ \theta$ and the true value $ \nabla_{\theta}$.   The paper considers two approaches to this problem : ( 1 )   a priori, the authors assume that the data point $ \sigma$ is a subset of the set $ \gamma$ and ( 2 ) they assume that $ \alpha$ is the average of the prior distribution of $ z$ over all $ z$.   For the first approach, the paper proposes to sample $ z $ from the distribution $ \sum_{\mathcal}$ over $ y$ and $ z_i$ from $ z_{\gamma}$ and to estimate $ z(\theta)$ from the mean of the posterior distribution $ z^{\mathbb{v}$. In the second approach, instead of using a prior distribution over $ z}$ as in the case of [ 1 ] and [ 2 ], they use a prior i.e., a prior on $ z(z(z)$, the prior on the $ z \log{\theta$. The paper shows that this approach is equivalent to minimizing the inverse problem of estimating the ratio of the two prior distributions $ z^2$."
SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"This paper proposes a new quantification approach to quantifying the uncertainty in the input distribution of a mixture model ( NNs ) with respect to the distribution of its components. The main contribution of this paper lies in the fact that NNs can be seen as a special case of density networks ( MDNs ), which have been introduced in [ 1 ], [ 2 ] and [ 3 ]. The authors prove that the input - dependent quantification of NNs is equivalent to that of CDNs under certain assumptions ( e.g., $ \ell_0 $ ). The paper also provides a proof of the quantification uncertainty in terms of the number of components of the NNs.   * * Contributions * * :   1. Introducing a novel quantification framework for NNs, which is based on the notion of mixture model. This framework allows the authors to prove the quantifications of the input with uncountably many components. 2. Further, the authors propose a new adversarial attack on NNs that can be used to quantitatively quantify the uncertainty of their input distribution. 3. Conducting experiments on MDNs and CDNs to show the effectiveness of the proposed approach."
SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"This paper presents a theoretical lower bound on the amount of information that can be transmitted in a compressed neural network with a fixed number of bits / sec. The lower bound relies on the following assumptions :    1. There exists a distribution over the weights of the neural network such that for every instance of the network, there exists a datapoint of size at least $ \infty$ such that the total amount of data transmitted in the network can be decreased by at most $ \sqrt{1}$. 2. For each instance of a network, the number of bytes required to compress the network is bounded by a constant factor that depends on the size of the dataset.   The authors prove that if the above assumption is satisfied, then there exist instances of the networks for which the entropy per bit / sec can be reduced by a factor of $ \tilde{1 } \log n\infty$.   In addition, the authors provide a theoretical upper bound on how much information needs to be transmitted per instance in order for a given model to be able to provide insights about the structure of the data."
SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"This paper studies the problem of designing neural networks that can be used for differentiating between differentiable hardware - based and non - differentiable neural network architectures. Specifically, the paper focuses on training neural networks to solve a set of tasks that are differentiable in terms of the number of parameters and the size of the target tasks.   The paper proposes two approaches to this problem. First, the authors propose a hyper - parameterized network architecture search ( NAS ) that aims to find a subset of network architectures that can satisfy a given set of small - scale and large - scale tasks. The second approach is to train a neural network architecture that can handle a large number of differentiable tasks at the same time. The paper also proposes a method to train such a neural architecture that is differentiable with respect to the number and the scale of the tasks."
SP:e5b70d43d301d1980fae02623ea711976b429c14,"This paper studies the problem of minimizing the two - player min - max games objective in the setting of convex, non - convex and large - data settings.    The main contribution of the paper is to provide a theoretical analysis of the problem in terms of the dual - log - metric and the second - order penalties in the case of the convex case. The main contributions are as follows :   1. A theoretical analysis on the dual-log - metric problem is provided, which shows that the objective can be phrased as a convex optimization problem with two objectives : ( 1 ) equilibrating the number of players and ( 2 ) minimizing the second order penalty on the set of games played by the two players. 2. Experimental results are provided showing that the second objective is equivalent to the first one."
SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"This paper proposes a new class of variational gradient estimators for the class of generative models of discrete latent variables. The main contribution of the paper is a new control variate gradient estimator for the generative model of discrete variables.    The main contributions are two - fold :   - Introducing a new variational method for the generation of tensors for discrete latent variable models, and - Developing a new relaxation method for tensors in the case of discrete models."
SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"This paper presents a non - differentiable and non - convex optimization framework for supervised learning of structured prediction energy networks ( SPENs ). The proposed framework consists of two components : ( 1 ) a policy gradient based search and ( 2 ) a supervised learning framework. The policy gradient search is based on a randomized search, where the goal is to find the optimal solution to a given set of tasks in the output space. The supervised learning is done by training the model on a set of data generated by a supervised learner who has access to a subset of the output of the given task and is provided with a training data set generated by the policy gradient.    The main contributions of the paper are as follows :   1 ) An unifying framework that unifies the differentiability of the policy search and the supervised learning. This unifies and unifies all the existing works on supervised learning in a single framework. This unified framework can be viewed as a unified framework for both supervised learning and unsupervised learning."
SP:638c1bc09992029b78bd83f0127594dcccb96c06,"This paper proposes a policy search framework for learning in a multi - task learning setting, where the goal is to find a policy that minimizes the total number of actions required to solve a set of tasks in a given domain. The paper proposes two approaches to this problem. The first approach is based on learning the dynamics model of the domain, and the second approach is to search for policies that minimise the total amount of actions needed to solve the domain.    The paper provides theoretical analysis on the two approaches, and empirical results show that the policy search approach is more efficient than the first approach."
SP:491c239713a6489f0b1790ca26db54a1813c67ae,This paper studies the two - timescale network ( TTN ) framework for value function approximation and policy evaluation. The main contributions are as follows :    1. This paper provides a theoretical analysis of the TTN framework.   2. The paper provides empirical results on the convergence properties of TTN.
SP:327d606cf3813b00a009a7785e08ef9e11f89493,"This paper presents LEAPS, a reinforcement learning agent that learns to navigate in a 3D environment that is designed to provide a rich set of objects to interact with. The goal is to train an agent that is able to navigate through a variety of different environments that may have different types of goals and objectives.    The main contributions of the paper are as follows :   1. Introducing a new class of environments that allows the agent to search over a large variety of objectives that may not have been previously explored by the agent. 2. Providing a set of actions that can be used by a policy that has been trained over a range of objectives and goals that may be different from the agent ’s goals. 3. Conducting a series of experiments where the agent is allowed to explore a large number of environments and is provided with a large set of goals that it is unable to navigate to. The paper presents results that show that the policy can navigate to a wide variety of environments in a way that is consistent with the goals of the agent, and that it can also navigate to environments that are not designed to be easy to navigate."
SP:d7c26f43bc68d160095b1f50447528843d79edbd,"This paper studies the problem of understanding the ability of a vehicle driven by a given control system to explain the current state of the environment. To this end, the paper proposes two approaches : ( 1 ) map - based and ( 2 ) spatial - based knowledge - related knowledge - sharing. The first approach is based on the mapping - based approach, where a set of maps are given to the agent and the goal is to obtain a knowledge - based understanding of the current conditions under which the vehicle can be driven. In the second approach, the authors use a dataset - free approach to obtain the knowledge - level knowledge about the current condition under which a vehicle can drive.    The paper conducts experiments on the following tasks :   1. Exploiting the knowledge about current conditions in the environment   2. Providing the agent with the knowledge of current state - of - the - art vehicle control systems   3. Learning the model of the underlying driving system   4. Conducting experiments on different types of datasets   Experiments are conducted to evaluate the performance of the proposed approaches."
SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"This paper proposes a new classifier based on adversarial perturbation theory. The main idea is to train a classifier that is robust to perturbations of the form $ \ell_t$ \in \mathbb{R}$, where $ t$ is the number of samples, $ \infty$ is a set of data points, and $ z$ is an integer that measures the difference between the data points.    The main contribution of this paper is to show that the proposed classifier can be used to generalize the standard classification."
SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"This paper proposes a gradient - based training of neural networks in reverse - mode automatic differentiation, where the goal is to learn a local learning rule that can be used in tandem with a global learning rule in the federated model.    The paper provides a theoretical analysis of the proposed approach, which is based on the idea of equilinear optimization of neural activations. The paper also proposes a training procedure for the feed - forward network based on this approach, and empirically shows that the proposed training procedure outperforms the state - of - the - art approaches in terms of performance."
SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"This paper studies the problem of generating adversarial examples for black box deep learning algorithms. The authors provide a theoretical analysis of the convergence rate of gradient estimators for black - box adversarial example generation and show that it is non - convex with respect to the number of examples and the dimension of the dataset. The paper also provides a proof that the gradient estimates are not convex under certain assumptions on the size of the set of examples. Finally, the paper provides an empirical study of the speed of the gradient estimator and shows that it does not depend on the dimensionality of the data set."
SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"This paper presents a series of experiments on improving the quality of the training of neural network - based optimizers for edge device applications. The experiments cover a wide range of settings, from deep learning to deep neural networks, and are conducted on the CIFAR-10 dataset.    The experiments are divided into three parts : ( i ) the training phase, ( ii ) the drop phase, and ( iii ) the deployment phase. The drop phase is divided into two stages, i.e., the training and deployment phase, where the model is trained on a fixed set of data points and then fine - tuned on the new data points. The deployment phase is split into two parts : the initialization phase and the pooling phase. In the first stage, the network is trained with a fixed number of layers, and the layer - level weights are updated as the number of new examples increases. The second stage is a mixture of training and fine - tuning, where new examples are added to the training set, and a new layer is added at each iteration. The authors conduct experiments on a variety of different settings, and show that the proposed approach is more stable than previous approaches."
SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"This paper studies the problem of adversarial attacks on audio data. Specifically, the paper focuses on the dependency property property of the data produced by adversarial examples attacks. The paper proposes two types of attacks, i.e., power - attack and power - defense attacks, and shows that both attacks are robust to the same set of data. Moreover, this paper also proposes a retraining strategy to ensure that the data generated by the attacks are not too different from the original data."
SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"This paper proposes a generative model of real - world images that can be used to understand the relationship between object representations and their generative models. To this end, it first synthesizes a set of synthetic datasets of objects and then uses the synthetic data to construct a model of the objects. The model is then used to train a neural network that is able to reproduce the observed data."
SP:fb59990b8da0e95d8202383478a456667de60449,"This paper proposes a supervised learning framework based on variational autoencoder based generative models. The key idea is to train a generative model on a set of examples, and then provide supervision on the representations obtained from the model. To this end, the authors propose two approaches : ( 1 ) image generation and ( 2 ) supervision. The image generation approach is based on a variational auto - encoder - decoder model. The supervision approach is built on the similarity between the representations generated by the two approaches."
SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"This paper presents a reinforcement learning approach for online learning with a model - based reinforcement learning ( MOLe ) approach, where the goal is to learn a policy that adapts well to changes in the environment. To this end, the authors propose two approaches : ( 1 ) to learn the dynamics model, and ( 2 ) to adapt the learned policy to the changes in environment. The first approach is based on learning the model p(s ) of the environment, and the second approach uses the learned dynamics model as a feedback mechanism.    The authors evaluate the performance of the MOLe approach on a set of stationary and online tasks, and show that it is able to cope with changes in both the environment and the learning procedure."
SP:5665e5f006f84927beb0440e145f476e02538077,"This paper presents a series of experiments aimed at providing agents with a better understanding of the state - of - the - art in terms of their ability to adapt to changes in the environment during their training. To this end, the paper presents two sets of experiments :   ( 1 ) a set of DQN - based RL agents trained on a single network with a fixed number of agents, and ( 2 ) a larger set of agents trained in a distributed manner. The experiments are divided into two parts : ( i ) the training of the agents on the single network and ( ii ) the reinforcement learning on the larger network.    The first part focuses on improving the agent ’s knowledge of the environment ( i.e. “ the environment ” ) during the training phase. The second part is devoted to improving the agents “ knowledge ” of the underlying environment ( ii.1 ). To do so, the authors propose to provide a “ feedback loop ” that allows the agent to “ re - play ” the same set of tasks at different timesteps. The authors show that the feedback loop can be used to provide agents with an “ experience replay ” buffer that allows them to keep up to date with the environment over a longer period of time. The paper also shows that this buffer can also be used as a way of “ diversifying ” agents ’ knowledge about the environment, allowing them “ to diversify ” their “ initial state ” during training. The experimental results show that this approach can lead to better performance."
SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"This paper proposes a framework for describing multi - agent trajectories of basketball players in terms of trajectories produced by a generic generative model. The framework is built on top of two components :    ( 1 ) a generic generic trajectory model and ( 2 ) a bi - agent trajectory model.   Experiments are carried out in two different scenarios : ( a ) a static regime, where the agent is assumed to know all the other agents trajectories, and ( b ) a dynamic regime, in which the agent knows only the actions taken by the other agent. Experiments show that the generic trajectories are better suited for describing the dynamics in the static regime than for the dynamic regime."
SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"This paper presents a self - supervised learning framework for real - world sports videos. The framework consists of two components :   ( 1 ) a visual information model and ( 2 ) a game engine. The visual model consists of 3 components : 1 ) an agent that takes as input a set of video clips and generates a video from each point in time, and 2 ) an action model that takes the video as input and generates action sequences from it.    The game engine is trained with the following components : ( a ) a VQA - based agent that learns from the video clips ; ( b ) a video embedding network that takes in as input the video and outputs action sequences ; and ( c ) a player - agent network that is trained on the video embeddings and action sequences."
SP:8392f04b7265f665ba6d44d297bca245d44b4708,"This paper proposes a new method to improve the quality of black - box functionality of deep neural networks. The key idea is to use gradient descent descent to train a model that is differentiable with respect to a set of blackbox functions, and then to deploy the model to the blackbox functionality via a policy gradient descent method. The main contributions are as follows :    1. Introducing a new metric that quantifies the difference between blackbox and white - box functions ; 2. Demonstrating that this metric can be used to quantify the differentiability of the model ; 3. Showing that the model can be trained to be differentiable to blackbox function ; 4. Conducting experiments to demonstrate the effectiveness of the proposed method."
SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,This paper proposes a meta - learning method for the classification of few - shot regression tasks. The proposed method is based on a black - box function approximator that is a mixture of hierarchical Bayesian models. The authors provide a data - driven inductive bias that encourages the model complexity to decrease as the number of tasks increases. They also provide an empirical evaluation of the performance of the proposed method.
SP:a410144dbe19713a06c63da87d9fb58b999a7492,"This paper proposes a meta - supervised learning framework for auxiliary tasks, where the goal is to obtain knowledge about the sub - categories of a given task without the need of human supervision. The proposed framework is based on the meta - classification framework proposed in [ 1 ], where sub - classes are defined by a set of domain - specific labels, and the goal of the task is to learn the classification of the domain of interest to the task at hand.    The authors propose two approaches to this problem. The first one is to use a dataset with sub - class labels for the principal task and a dataset for the auxiliary tasks. The second one uses the same dataset for both the principal and the auxiliary tasks, but with a different set of labels for each of them. The authors show that both approaches outperform the baselines in terms of the number of supervised tasks and the amount of supervised data."
SP:76248e1c914c60ce69de244fe7ec62488d01e161,"In this paper, the authors propose a new open set recognition approach based on neural network based representation and entropy loss."
SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"This paper provides a theoretical analysis of the learning rate annealing of precision baseline networks for deep neural networks. Specifically, the paper considers the following :    1.   apertured fp32 precision baseline network, which has been shown to be a good baseline for improving the energy and area efficiency of deep neural network inference. The paper shows that it is possible to improve the quality of the baseline by minimizing the variance of the gradient estimates of the model parameters. 2. izent the similarity between the learned model parameters and the baseline parameters, which is shown to increase the area efficiency. 3."
SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,"This paper presents a theoretical analysis of the property of post - bounce trajectories obtained from pre - collision 3D trajectories in the context of the VIM ( Visual Inference Module ) framework.    The main contributions are as follows :   - Providing theoretical analysis on the properties of pre - bounce 3D trajectory obtained in the framework of VIM. - Demonstrating that trajectories produced by VIM can be used to predict the location of a given point in the interval $ \mathbb{R}^{-1}$, and that this location is inversely proportional to the distance between the point and the origin. - Proposing a new metric $ \phi$ that quantifies the distance from the origin in terms of the distance to the origin ( $ \pi$ ), and showing that this metric is independent of the value of $ \gamma$ in the range $ \log(\gamma)$."
SP:010bd055310c363d3cb0fbe0e11546de58220e15,"This paper provides a theoretical analysis on the properties of networks that are vulnerable to p - attacks. Specifically, the paper focuses on the following :   1.   Image - size and weight distribution distributions. The paper shows that a network that is not imperceptible to perturbations larger than $ \ell_1 $ is not differentiable in terms of the size of its classifiers and the distribution of their weights. 2. The vulnerability of the network is defined as the ratio of the number of instances of a given classifier in the network with respect to its size. This paper provides theoretical results showing that the network can not be made differentiable by perturbing the classifiers of a certain classifier if it is not able to produce images that are imperceptibly perturbable. 3. In order to make the network differentiable, this paper introduces a new metric that measures the difference between the size distribution of the classifier and the weight distribution of its perturbation. This metric is called the “ 1 - norm ”."
SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"This paper presents a curiosity - driven reinforcement learning framework that aims to minimize the total number of actions required to learn a policy from a set of demonstrations. The framework consists of two components :   ( 1 ) a policy - based learning framework, and ( 2 ) an agent - based framework.    The first component is an agent modeling framework that learns a policy that maximizes the expected value of actions given an observation of the agent model. The second component is a policy network that learns to plan actions based on the observations of the first component. The policy network is designed to be able to learn from a large number of demonstrations in a closed - form, and the agent network is trained on top of this learning framework. The main contributions of the paper are as follows : 1 ) to ensure that the policy network does not learn too many actions in the first part of the learning process, the paper introduces a learning metric that measures how many actions needed to learn the policy in the second part. The paper shows that this metric is a good indicator of the total amount of actions needed in the learning phase, and that it can be used to estimate the number of steps required for learning a good policy. 2 ) to improve the quality of the demonstrations provided by the policy networks, the authors propose a learning strategy that is based on probing the model in both the first and the second half of learning. Experiments show that the proposed approach achieves better results than a naive approach that only considers the first half of the steps in learning."
SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,This paper proposes a new activation function for neural networks. The activation function is based on the idea of short - term memory networks. This paper shows that the activation function can be used to train neural networks with long - tailed activation function.
SP:287a577834fd2820a939a1113b39146a22727491,"This paper presents a framework for supervised learning of speech - to - text representations based on the NANSY framework ( Nansy framework for Secured Speech Transformer with Secured Representation Learning ). The framework consists of three components :    1 ) Secured Transformer framework, which is based on Secured Voice Transformer ( SVT ). 2 ) Aperturbation - based approach to improve the quality of the learned representations. 3 ) A time - scale modification strategy to preserve the pitch - shift and scale - shift invariance of the generated representations.   Experiments are conducted on three datasets : ( 1 ) VQA, ( 2 ) GAT, ( 3 ) and ( 4 ) CIFAR-10. Experiments show that the proposed framework outperforms the state - of - the - art methods in terms of supervised learning performance."
SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"This paper provides a theoretical analysis of the ( gradient - based ) bilevel programming framework. The main contributions are :    1.   A theoretical analysis on the stability properties of gradient based algorithms. The authors show that the data reweighting assumption is not sufficient to ensure the stability of the algorithms. They show that under certain assumptions on the hyperparameters ( e.g., $ \ell_t$ ), the level of the gradient algorithm is not guaranteed to be less than $ \sqrt{\sqrt{x}$, where $ \tilde{x } \in \mathbb{R}$ is the number of iterations of the algorithm and $ \text{data } $ is the set of data points in the program. 2. An empirical study is provided to show that there exist algorithms that do not satisfy these assumptions. 3. This paper also provides a proof that there exists an algorithm that satisfies $ \sigma(x, y)$ such that for any $ y$, there exists a data point $ z_t $ such that $ y_t(x ) \infty$ can be expressed as a convex combination of $ \sum_{t_t\infty}$ with $ \alpha(x,y)$.   4. A set of experiments are provided to verify the theoretical results."
SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"This paper presents an approach to distilling knowledge to student and teacher networks in a student - friendly manner. The approach is based on the idea of distilling the knowledge from a student model to a teacher model that has the same distribution as the student model. The main idea is to distill the knowledge to the teacher model in a manner that is similar to that of distillation to student models, but with the ability to adapt to changes in the distribution of the student models.    The main contribution of this paper is the following :   1. Introducing a new distillation approach that is different from the one used in the previous work ( SFTN ). 2. Demonstrating that the proposed approach can be used to distil knowledge from student models to teacher models that have different distributions. 3. Conducting experiments on a variety of distributions to demonstrate the effectiveness of the approach."
SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,This paper studies the problem of learning the generalization error bounds under the assumption that the features of a given dataset are invariant to the choice of data points. The paper proposes two approaches to this problem. The first approach is to select a subset of the data points such that the error of the learning algorithm is bounded inversely proportionally to the number of features in the data set. The second approach is a variation - based approach that selects data points based on the similarity of the features in each data point.    The paper provides a theoretical analysis of the error bounds and a selection criterion for the two approaches.
SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"This paper addresses the problem of knowledge transfer between dynamic Gaussian mixture model distributions and a set of static Gaussians. The authors propose a variational variational inference method that approximates the distribution of the static Gaussian parameters of the dynamic Gaussian with respect to the set of tasks. This is done in two stages :    ( i ) for stationary distributions, where the distributions of the tasks are assumed to be stationary, and ( ii ) for dynamic distributions that are dynamic and/or non - stationary.   The main contributions of the paper are as follows : - A new variational approximation method for learning the variational distribution of dynamic parameters of a dynamic model that is a mixture of static and dynamic distributions. - A theoretical proof of the approximation method and its convergence guarantees."
SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"This paper proposes a new method for learning probabilistic approximations of the first - order boundary value problems in the context of machine learning. The main contribution of the paper is a theoretical analysis of the convergence rate of the proposed method, which is based on the following assumptions :    1. There exists a set of $ \mathcal{x}$-th order boundary - value problems $ \theta$ such that $ \tilde{x } \in \mathbb{R}(\theta)$ can be approximated with $ \sqrt{\theta } \log p(x)$, where $ \text{xi}$ is the number of steps in the iterative process.   2. The authors prove that the method converges to the right - hand - side solution of the problem with high probability under certain conditions. 3. They provide a proof that the error of the method is bounded by a constant factor $ \nabla_{\theta}(\sqrt{x})$.   Experiments are performed on a number of different machine learning problems."
SP:86aac0c6b75fdc12f84bba342934865616f866d4,"This paper studies the problem of learning a policy that minimizes the mix - mixing metric between two distributions of state - actions in a mixed - product distribution problem. The paper provides a theoretical analysis of this metric and a theoretical proof that the metric is invariant to both the initial state - action distribution and the value distributions of the MDPs. Then, the paper proposes a learning procedure that recovers the original model from the mixed product distribution, and uses it to learn a policy to minimize the mixing metric. The authors also provide an empirical proof that this policy is provably optimal. Finally, the authors provide an online version of the learned policy that is empirically shown to be provably safe."
SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"This paper proposes a two - step procedure to estimate the cumulative effect of two interventions under the assumption that one of them is not too different from the other. The proposed procedure is based on the observation that the average of the two interventions will converge to zero as the number of steps goes to infinity.    The paper provides a theoretical analysis of the proposed procedure and a set of experiments to verify the adequateness of the estimator. The paper also presents results of two experiments : ( 1 ) a semi - synthetic experiment where the dataset is divided into two steps, ( 2 ) a real - world experiment where data points are randomly sampled from a dataset, and ( 3 ) a single - cause intervention where the data point is randomly selected from the dataset. The results show that the proposed approach is more robust than previous approaches."
SP:247bc6675cce89d51558537daf63dadb0c4307f8,This paper proposes a neural operator learning scheme based on the kernel - based approach proposed in [ 1 ] and [ 2 ]. The key idea is to learn a mapping between function spaces of the kernel and the vector space of the vector operator. The authors show that this mapping leads to a data - driven learning scheme that can be used to solve the corresponding flow equation.    [ 1]theorem [ 2]lemma [ 3]proposition [ 4]problem [ 5]claim [ 6]conclusion [ 8]conjecture [ 9]claim
SP:1153785e6a016cfee2644952a772aa08927299b6,"This paper studies the problem of improving the gradient of the original sign function of a high - frequency domain approximation ( FS term ) of a neural network. The authors show that there exist high - order FS terms that can be efficiently approximated with the sign function, provided that they are not too different from the original FS terms.   The authors provide a theoretical analysis of this problem, and show that the resulting gradient is optimal for a class of neural networks."
SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,"This paper proposes a multi - area neural network ( RNN ) framework for solving multi - task knowledge - based tasks. The goal is to provide sufficient representations of task information across a number of different scientific domains. To this end, the authors propose a set of tasks that each domain has its own set of objectives and constraints. Each domain is divided into multiple categories and each category corresponds to a different set of RNNs. Each category is further divided into sub - categories according to the number of tasks in the category. Each sub - category is then divided into a set called “ RNN ” and each RNN is trained on a subset of these categories. The authors show that the proposed RNN can be used to solve different types of tasks."
SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,"This paper presents a study of cross - over between different map - based and counterfactual question answering approaches. The main contributions are :    ( 1 )   a cross - tiered study of different map and question - answering approaches ( SAG, CNNs, GOS ), and ( 2 ) a decision - making algorithm ( GOS - GOS, COS - COS ) to select the best of the three approaches based on the similarity of their representations.   The paper also presents a sample - wise analysis of the quality of the representations obtained by each approach. The results show that the GOS and COS approaches are better than the SAG and CNN approaches, respectively."
SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"This paper presents a theoretical analysis of the cross - entropy - based learning objective for neural network classification tasks. The authors show that the default representations of convolutional neural networks are not invariant to changes in the layer - level structure of the network. To address this issue, the authors propose to fix the feature extractors and separations in the layers of the neural networks and train the networks with different kernel alignments. The theoretical analysis is complemented with empirical results on a set of classification tasks and shows that the proposed model achieves better performance than the baselines."
SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"This paper presents a neural network training strategy based on two - photon calcium imaging data to train a model of latent dynamics in the brain. The training strategy is based on a two - step process : first, the dynamics model is sampled from a set of data points sampled at different timesteps, and then, a second step is used to train the model on these data points. The authors show that the proposed strategy can be applied to a variety of different types of neural networks, and that it is able to capture the temporal structure of the population dynamics across different networks."
SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"This paper presents a grammar - based machine translation and sequence - to - sequence learning framework that is built on top of a pre - trained generative model. The model consists of two components :   ( 1 ) a generative grammar model, and ( 2 ) a sequence model, which consists of a set of syntactic entities and a syntactic sequence model.    Experiments are conducted on the following tasks : ( i ) small - scale machine translation ( SCAN ), ( ii ) sequence learning ( SGLT )."
SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"This paper studies the problem of feature selection in the framework of function - on - function regression. This is a variant of the well - known feature selection problem in the context of data - adaptive regression, where the goal is to select a feature that minimizes the risk of missing data points while satisfying a certain property ( e.g., scalar invariance ).   This paper considers the setting where the data consists of a set of functions and a data set of data points where the function is a scalar and the data point is a vector. The paper provides a theoretical analysis of the problem in terms of the principal components representation of the data points and the scalar feature selection. The main contributions of the paper are :    1. A theoretical analysis showing that the principal component representation is not invariant to the dimensionality of data sets, and hence that feature selection is not guaranteed to be on - scalar ; 2. A proof that this is not the case ; 3. A practical solution to this problem is given in the form of a convex combination of two algorithms."
SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,"This paper presents a point process - based learning framework, based on cross - entropy ( CE ) and principal component analysis ( FPCA ), for the classification of point processes over temporal domains.    The proposed framework is based on a multi - level marked point process, which can be viewed as a mixture of two types of structured point processes, i.e. 1 - dimensional and 2 - dimensional. The key idea is to learn a parametric model of the point process from the data, which is then used to estimate the principal component of the events produced by the process."
SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"This paper provides convergence guarantees for meta - adaptive control ( OMAC ), a multi - task nonlinear control approach for representation learning in the presence of a nonlinear dynamics model. The main contributions are as follows :   1.   A theoretical analysis of the convergence rate of OMAC is provided, which shows that the dynamics model admits a metric that is invariant to the number of iterations, and that the metric is robust to perturbations in the nonlinearity of the control dynamics. 2. A proof is provided that OMAC converges asymptotically to the non - linear dynamics model in a finite time horizon. 3. A numerical experiment is provided to verify the theoretical results."
SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"This paper proposes a new method for certifying the robustness of neural networks. The proposed method is based on the concept of cross - entropy bounding ( IBP ), which is defined as the upper bound on the difference between the probability that a neural network is robust to a given set of perturbations ( e.g., errors ) and the probability of a perturbation that does not affect the network in a way that would prevent the network from being robust. The paper provides a theoretical analysis of the bounds that can be obtained under the assumption that the network is not too differentiable and that there is no significant difference in the network architecture between the two classes of networks. Moreover, the paper also provides bounds on the number of iterations required for the network to be robust and the amount of training iterations required to achieve the bounds."
SP:18ffeb199a670fb2b1f4417b8653479001944dab,"This paper studies the problem of change point detection in the context of the phase transition phenomenon. Specifically, the paper considers the following scenario : given a set of data $ \mathcal{x}$ and a distribution $ \alpha$ over $ \theta$, the goal is to detect a point $ x$ in $ \text{x } $ such that $ x}$ is a positive integer greater than $ \sqrt{\theta}$, where $ \tau$ is the number of $ z$ points in $ x$.    The paper proposes two approaches to this problem :   1.   a localisation approach based on sampling $ z$. The first approach is based on the assumption that $ z = \alpha$. The second approach uses a differentiable estimator $ \beta$. The authors prove a lower bound on the probability of finding a point in $ z $ that is not contaminated by outliers under the second approach."
SP:d03617b5fc446768809cf015c9234b0c9386a690,"This paper proposes a sample - based learning algorithm based on learning from a small number of examples. The main idea is to learn a set of problems from a large set of examples, and then solve the problems on the subset of those problems that are differentiable with respect to the sample size.    The authors prove the following results :   ( 1 ) The sample size of a given problem is not a good indicator of the quality of the learned solution ; ( 2 ) the number of problems in a given batch is not enough to ensure that the solution is differentiable ; ( 3 ) there is no optimal sample size for learning a certain class of problems ; ( 4 ) there exist problems for which a sample size greater than a certain threshold is sufficient to learn the solution."
SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"This paper presents a theoretical analysis of the probability distribution of the distance between two sets of data points $ \mathcal{x}$ and $ \cal{y}$ with respect to $ \sqrt{x } \log n_{\textrm{x}}$, and proposes a new metric $ \tilde{x}\infty$ that can be used to estimate the critical point $ \infty$.   The paper shows that $ \text{x\infty}$ can be approximated with $ \frac{x}{\log n}$, where $ n$ is the number of points in the interval $ \nabla$ $ \ell_0 $ and $ z$ is a probability distribution over $ \theta$, $ z_\theta$. The paper also shows that the metric can be improved by adding a term $ \sigma$ in the distribution $ \log{x},$ that penalizes the difference between the probability of the two sets $ \gamma$ and the value of $ \eta$ ( $ \sum_{\gamma}$ ).   Experiments are performed on a variety of machine learning problems, and it is shown that the proposed metric can solve some of them better than the previous ones."
SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"This paper studies the problem of self - attention and attention - centric learning in videos. Specifically, it focuses on the question of how to train a video encoder / decoder that is able to understand the motion dynamics in videos, while also being able to capture the relational structure of the video representations. To this end, the paper proposes two approaches. First, it first learns the relational representation of the videos, which is then used to train the encoder and decoder. Next, it uses the learned representation to learn the dynamic representations, which are then used as benchmarks in the second step of the learning process. The experiments show that the proposed approach outperforms the existing approaches in terms of terms of attention and learning speed."
SP:2c2530069d5cab485629090243da464d107feadd,This paper provides a theoretical and empirical proof of the finite - width limit of the mean - field limit of networks in the setting where the number of layers is large and the network size is small compared to the dimension of the domain. The paper also provides theoretical guarantees for the network depth in this setting. The main contributions of the paper are as follows :    1. A theoretical derivation of the limit in the case of finite - dimensionality of the network. 2. An empirical proof that the network will converge to the mean field limit in a finite number of dimensions. 3. Experimental verification of the theoretical results.
SP:a3d927854d9d7fd39b8d05a79666810d585d5062,This paper studies the problem of learning structure - preserving dynamical systems from data - driven simulations of dissipative systems. The main contribution of the paper is a theoretical proof of the equivalence between entropy - preserving and entropy - decreasing dissipative dynamics. The proof is based on the fact that the entropy of a dissipative system is invariant under the assumption that the dynamics of the system is reversible.    The proof relies on the following assumptions :   ( 1 ) that the underlying dynamics of a system is isotropic ; ( 2 ) that there exists a finite set of variables such that the total number of variables in the system increases linearly with time ; and ( 3 ) that entropy of the dissipative process decreases linearly in time with the length of the time series of variables.
SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"This paper studies the problem of ensuring the robustness of machine learning algorithms trained on data corrupted by adversarial perturbations. To this end, the paper proposes a sampling - based approach to ensure the quality of the samples used for training the algorithm. The paper provides a theoretical analysis of the sampling strategy and empirically shows that it is necessary to select a set of samples that is unbiased and robust enough to ensure that the algorithm is not corrupted by the perturbation introduced by the adversarial data. Moreover, it is shown that the selection of samples with high - quality data is necessary for the algorithm to be robust. To further improve the sample selection strategy, the authors propose a technique to select samples that are clean and robust to adversarial noise. The authors conduct extensive experiments on both synthetic and real datasets to demonstrate the effectiveness of their approach."
SP:991127729bf067fe27fdd7ed360aab39e4df5921,"This paper considers the problem of out - of - domain detection of neural network activations functions in the context of neural neural network models with different covariance structure. The authors provide a theoretical analysis of the problem under the following assumptions : ( 1 ) inputs are stationary on a cosine - invariant ( sinusoidal ) basis, and ( 2 ) the limiting stationary process $ \mathbb{R}^n$ is a Gaussian process over the space of all possible activations of the input.    The authors show that under these assumptions there exist activations that are invariant under the above assumptions and that are locally stationary. They also show that the corresponding activations can be used to infer the weights of a neural network."
SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,"This paper proposes a feedback - based approach to improve the quality of the student feedback provided by dynamic systems. The approach is based on the idea of “ MDP ”, which is an extension of the MDP framework proposed in [ 1 ]. The main contribution of this paper is to provide a feedback system that is able to provide feedback to a student who has not completed a given set of programs.    The main contributions of the paper are as follows :   1. Introducing the concept of MDP and a feedback framework that provides a student with access to a feedback model that provides feedback to programs that fail to satisfy a certain set of criteria. 2. Using this feedback model, the paper shows that a student can be provided with feedback that can help improve the performance of programs that failed to satisfy the given criteria. 3. Conducting a series of experiments with different types of feedback systems that are based on MDPs, and show that the proposed approach leads to better performance than previous approaches."
SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"This paper proposes a new model - based approach for generating high - level latent object representations for deep reinforcement learning ( DRL ) systems. The proposed approach is based on the MCRTS algorithm. The key idea is to train a model $ \mathcal{T}$ with a set of $ \cal{L}$ features $ \infty$ $ \theta$ and $ \text{text{R}$ action values $ \tilde{x}$ $ x_t$ $ y_t$.    The main contribution of this paper is the following :   1.   i.   propose a new objective $ \sqrt{x } \log n_{\theta}$ that quantifies the difference between the expected value of a given set of actions under the given set $ \alpha$ and the set of values under $ \nabla$ $ t$. This objective is phrased as follows : $ \sigma^n_{\text{x_t}$ where $ n_t(x_0)$ is the number of actions in the set $ z$, $ z_0 $ is the dimensionality of the set, $ \eta$ is a dimension of the action set $ y_{\infty}$, and $ t_0$ is an integer factor that measures the similarity between the set - valued set $ x_{\mathcal}$ and its set of un - valued counterparts $ z_{\cal{X_t } $ ( $ \ell_0,\ell_1}$ ( $ z^{-1},$ z_t)$. The paper also proposes a $ \log{text}$ objective that quantitatively measures the difference in expected value between sets $ \gamma$ and their set of latent representations $ \sum_{\alpha}$ under $ nabla$. The paper shows that this objective is equivalent to $ \frac{x}{\sqrt{\theta } } $ where $ \kappa^n_t\log n^{-2}$   $ is a factor that factors in $ \delta$ of the total number of action sets $ z^n$ in $ z$. This paper also shows that $ \rho$ can be thought of as a factor in $ n$ of variation in the total value of an action set, and that $ n\log(x_{\tilde}$ can also be seen as an infinitesimal factor in the amount of variation of a set $ Z$ under the same set of input features $ n(\tilde0)$. Theorem   ii.1. provides a proof of this fact by showing that the proposed approach can be viewed as an improvement over the previous approach [ 1 ]. Theorem iii.2. provides an empirical evidence that this approach is better than the previous one in terms of expected value, and shows that it is also better than a naive approach that doesn't consider this factor."
SP:84560de78af979354fff83d1370d8675c1e9191f,"This paper studies the question of how well we can infer posterior probability path distributions of future binary outcomes from a dataset of real - world weather - contingent events. Specifically, the paper focuses on the following questions : ( 1 ) how well do we know the posterior probability distributions of the future binary outcome distributions for a given set of events? ( 2 ) how good are we at inferring posterior probability paths from the current data?    The main contribution of this paper is to provide a framework to answer these questions in a data - free manner. To do so, it first constructs a dataset that consists of a series of events, each of which is represented as a probability distribution over a set of future events, and then makes predictions about the probability that each of these events will lead to a future outcome. This dataset is then used to train a probabilistic model that is able to predict the future outcome of each of the events in the series.   Experiments are conducted on a variety of datasets, and it is shown that the proposed framework can be used to infer posterior distributions that are qualitatively different from those obtained by baselines that do not take into account the dynamic structure of the data."
SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"This paper considers the following pure exploration problem : given a set of points $ \mathbb{R}$ and an action $ \alpha$, what is the best way to estimate the probability that $ \beta$ is a positive integer?   This question is formulated as a convex optimization problem, and the authors propose to solve it by minimizing a minimization problem over a set $ \tilde{W}$ of points drawn from the set $ w$, where $ W$ is the set of elements drawn from $ w$.   The authors prove that the convex problem is equivalent to the following :    1. There exists a smooth function $ \text{W } such that for any $ w \infty$, there exist $ w^{-1}$ such that $ w_{\alpha}$ for all $ w^\infty$. 2. For each $ w }, there exists a $ p(w|x)$ that divides $ w|x$ into an arm draw of size $ x$. 3. For any $ \gamma_0 $, for any set $ x$ of $ w_i$ and $ y_{\gamma}$, for some fixed $ \lambda_0$, the following is the optimal value for $ w(\gamma_{\lambda_i}$ : $ \sqrt{\frac{W}{\sqrt{W}}(W|x ) + \log(W^{-2})$ where $ w(W^\alpha)$ is an upper bound on the sum of the upper and lower bounds on the expected value of the action under the action.   Experiments are divided into two parts : first, an iterative approach is used to identify the optimal assignment of draws to the next iteration of the algorithm, and this is repeated until convergence. In the second part, the algorithm is iteratively sampled from a fixed set $ W^\pi$ with the goal of finding a solution that minimizes the objective under the given policy. The authors show that this approach leads to a lower bound on $ \eta$ that is inversely proportional to the number of iterations needed to find a solution in the first stage, and that is smaller than the upper bound in the second stage."
SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"This paper proposes a new latent space method based on black - box function evaluation. The main idea is to learn a generative model based on the space representation of a black box function, which is then used as a surrogate model to evaluate the performance of the proposed method.    * * Contributions * * :   - This paper presents a new method to evaluate black box functions based on space representation. - This method is based on a new surrogate model which is a convex combination of two types of blackbox function evaluations. - The surrogate model is trained using a stochastic process based surrogate model. - Experiments are conducted on several real - world benchmarks to validate the effectiveness of the method."
SP:37adabdc6615c5199a481553c8ccc06d57363614,"This paper considers the following question : given a fixed $ \ell_0 $, what is the optimal value function $ \alpha$ that minimizes the expected Bellman error of a learning algorithm $ \mathcal{L}$ under the following assumptions?    1.   * * Bellman closure assumption * * : $ \text{max}^n \infty$, where $ n$ is a linear combination of $ \beta$ and $ \theta$ is the sum of the Bellman errors $ \sqrt{\theta } \log n\infty$.   2. * * Sec. 3 : * * regret condition * *  , which states that $ \tau \log N\log N$ is not too large compared to $ \nabla_{\theta}$, under the assumption that $ n\log n(\theta)$ is bounded from below by a constant factor $ \lambda_t$, and that the error is bounded inversely proportional to the number of iterations of the algorithm $ n(\tau)$. This question is phrased as follows :   $ \gamma_{\text{\text{L } } \leq n\sqrt{l}$ where $ N\leq \log \lambda_{\tau}$ is an upper bound on the error incurred by the learning algorithm under $ \sum_{\alpha_t}$. The paper shows that the upper bound is tight under the assumptions $ \eta_t$."
SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"This paper proposes a data - driven learning framework for improving the quality of gradient - based simulation and control of physical systems. The presented framework consists of three main components :    1.   a dynamic model - based learning framework. This is built on top of the previous work   ( Zhang et al., 2020 ). The main contribution of this paper is a new approach to learn a contact model and a physical system from data. The contact model consists of two components. The first component is a static dynamics model. The second one is a non - static mechanical system. The physical system consists of a 3D physical system and a 2D rotator. The goal of the paper is to learn safe control policies for both the contact and the 3D mechanical systems. In order to do so, the authors propose two methods : ( 1 ) to learn the contact model from the data, and ( 2 ) to train the physical system by learning the 2D mechanical system by solving the differential equation of motion. The authors also propose a new method to generate data for the physical systems, which is based on the time - stepping method. The experimental results show that the proposed approach is more efficient than the previous one."
SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"This paper presents a theoretical and empirical analysis of the problem of training a neural network to learn to tolerate different levels of complexity. The paper shows that there exist two types of trajectories for training the neural network, i.e. constant and datadent trajectories. The constant trajectory is defined as the one that is invariant to the number of layers in the network, and the datadanent trajectory is a special case of the constant trajectory.    The paper provides theoretical analysis of both types of trajectory, and empirically shows that both trajectories are equivalent to each other in terms of their complexity."
SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"This paper studies the following questions :    1. What is the probability that a linear classifier trained on a halfspace $ \mathbb{R}$ with $ \ell_1 $ samples will fail to classify a given set of examples? This question is phrased as follows : given a set $ \text{x}$ of examples $ \infty$ and a classifier $ \alpha$ on $ \theta$, the goal is to learn a $ \tilde{x } \log p(\theta)$ such that $ \sqrt{\theta}$ is not too different from $ \gamma_1$.   2. How do we know if this classifier will fail?   The paper answers this question by analyzing the following two questions : ( 1 ) what is the expected probability that the classifier fails to classify the given set $ x$, and ( 2 ) how does the sample complexity of the learned classifier compare to the expected risk of not classifying the data? The paper shows that the first question is equivalent to answering the second question.   3. This paper also provides a running time / sample complexity analysis of the proposed classifier. The running time is shown to be independent of PAC learning of $ x$."
SP:e5229305af00067ae2dbabd903e585964aec8928,This paper proposes a new attack method for improving the robustness of graph - based classification tasks. The proposed method is based on two approaches : ( 1 ) augmenting the existing classification tasks with a new set of examples and ( 2 ) adding a new classifier to each of the existing tasks.   The main contributions of the paper are as follows :   1 ) Introducing a new classification classifier and a new data augmentation function for each new classification task. The new classification model is trained on top of a set of existing classification models and the new classification is added to each existing classification model by adding new examples. The improved classification models are then used to perform new classification tasks on existing and new data sets. 2 ) Conducting a series of experiments to validate the effectiveness of the proposed method.
SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"This paper studies the problem of label shift adaptation under the assumption that the label distribution of a given set of data points changes over time. The paper proposes a new learning strategy based on the concept of expected test loss ( ETL ). The authors prove that under certain assumptions ( e.g., convexity assumption and boundedness assumption ), the ETL algorithm converges to a set of labeled data points with 0 - 1 expected loss under a given label shift, and show that the eTL algorithm can be extended to the case where the data points come from different regions of the world."
SP:806515ae07fb1c9d02773592005d53d4158ef102,"This paper proposes a new method to detect and localize gradual changes in the temperature of the world. The proposed approach is based on cross - entropy - distillation ( CPD ) approach. CPD is used to generate a dataset of time - ordered observations of temperature changes across the world, which is then used as a data generating model for a temperature - aware machine learning task ( e.g., logistic regression ). The authors claim that CPD approach is more robust than previous approaches in terms of its ability to detect changes in time and to localize them.    The authors also claim that the proposed approach can be used for other machine learning tasks such as deep learning and deep learning with knowledge distillation."
SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"This paper proposes a new neural network called Neural Network ( NN ) to address the source separation ( BSS ) and cross - entropy ( CE ) problems in neural networks. The main idea of the NN is to train a neural network such that it is able to handle a wide variety of source - to - target ( source - vs. source ) separation scenarios. To this end, the authors propose two approaches : ( 1 ) learn a set of $ \ell_0 $ variables $ \infty$ such that $ \tilde{x}$ can be approximated by $ \nabla_{\ell_{\infty}$, and ( 2 ) learn $ \text{x}\infty } $ such that if $ \mathbb{R}$ is a function $ \theta_{\theta}$ with $ \overline \alpha_{\overline } $, then $ \eta_{\text{\theta } \to 0 $ can be represented by a $ \sqrt{\text{\overline}$ function with the property $ \frac{x}{\sqrt{x } \times \text{\underline}$.    The main contributions of the paper are as follows : -   1 ) This paper provides a theoretical analysis of the BSS and CE problems and shows that the proposed NN can handle them. - This paper gives a theoretical solution to each of the two types of BSS problems in a closed form. - Theoretically, this solution is shown to be more plausible than the closed form solution to the original BSS problem."
SP:22f8b517a3df65144412938f5891c463d7bae0ab,"This paper proposes a two - neuron neural network ( RNN ) that learns to reproduce the dynamics of a set of discrete dynamical systems under two conditions. The first condition is that the system has a fixed number of solutions, and the second condition states that the network has access to a fixed set of data points that can be used as input to a second task.    The authors show that the proposed RNN is able to reproduce a variety of different types of dynamics under the first condition, and that it is also able to learn to reproduce diverse types of data under a second condition."
SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"This paper studies the problem of learning one - dimensional conditional likelihood estimations of covariates of distributions of energy functions. The main contributions are two - fold :    1.   Theorem 1. provides sufficient conditions on the covariates to ensure that the learned distribution is not too different from the one obtained by minimizing the conditional likelihood of the corresponding energy function, and   ii. provides a sufficient condition on the dimensionality of the covariate space to guarantee that it is not more than one dimensional. Theorem 2. gives sufficient conditions for the condition to be equivalent to the one derived in ( Xie et al., 2020 )."
SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"This paper proposes a new loss function that aims to adaptively measure the uncertainty in the network topology of a given set of data points. The key idea is to estimate the uncertainty of the topology in terms of the quality of the images and the edge pixels of the data points in the set. This uncertainty is modeled as a weighted convex combination of two factors : ( 1 ) the similarity of the source and target images and ( 2 ) the variance in the number of images and edge pixels.    The main contributions of this paper are as follows :   1. The proposed loss function is a weighted version of the one proposed in ( Xie et al., 2020 ). The difference between the two is that the former is metric - based and the latter is not. 2. This loss function can be used to estimate both the quality and the variance of the sources and the edges of a data point. 3. This paper shows that it is possible to obtain a network with high - resolution ( SISR ) images and low - level ( L1 ) edge pixels without the need of a metric."
SP:9997583f40fa648adf57bb4fc34228f357be0cf1,This paper presents a theoretical analysis of the PAC - Bayesian framework for robustness to attacks in the setting of binary classification. The main contributions are as follows :   1. A theoretical proof that the PAC framework is not vulnerable to adversarial attacks in terms of votes alone. 2. A multiview learning strategy that is based on multivariate classification and multi - view learning. 3. A set of experiments that demonstrate the effectiveness of the proposed approach.
SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"This paper presents a method for reasoning about the relationship between position and covariance metrics in the context of the Knowledge Graph ( KG ) dataset. To this end, the paper proposes two approaches : ( 1 ) to visualize the representations of the KG dataset and ( 2 ) to provide a closed form solution to the logical reasoning problem of constructing a closed set of metrics that can be used to evaluate the adequateness of the decision boundary and the end - to - end metrics.    The first approach is based on the fact that the representation of the kG dataset can be represented in terms of a set of closed - form queries. The second approach uses a differentiable query processing pipeline that allows to obtain closed form solutions to the question of whether or not a given set of data points lie in the same space as the set of entities that are specified in the closed form answer. The paper provides a number of experiments that demonstrate the effectiveness of both approaches."
SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"This paper provides a theoretical analysis of the gradient - based hyper - parameterized few - shot meta - learning ( HPO ) method with the aim to provide a better understanding of its scaling properties. The paper provides the following main contributions :    1 ) The paper analyzes the scaling properties of the proposed method and shows that it is not able to achieve the state - of - the - art performance in terms of the number of queries / queries / iterations. 2 ) This paper shows that there are no alternatives to the proposed HPO method that can achieve the same performance as the proposed one. 3 ) In order to alleviate the issue of gradient degradation, the paper proposes to use black - box methods."
SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"This paper presents a sequence - based logical reasoning model that can be used to generate sequences of logical reasoning tasks that are consistent with human - level knowledge.   The model consists of two components :    ( 1 ) sequence model 1 - like logical reasoning, and ( 2 ) instruction model 2 - like sequence model, which is based on symbolic world model. The first component is an extension of [ 1 ], and the second one is a combination of [ 2 ] and [ 3 ]. The reasoning model is used to train a neural network that is able to produce sequences that satisfy logical reasoning constraints."
SP:d77d046095e4c8336c0c76ac48cb046923230753,"This paper proposes a new learning method, OPE, which is based on a combination of two approaches : ( 1 ) a personalized dose - finding rule and ( 2 ) a change - point detection and decision rule. The key idea of the approach is to identify the conditions under which a given data point can be safely classified as a “ safe ” or “ unsafe ” data point, and then use a policy to classify the data points that are safe and those that are not safe.    The main contributions of the paper are as follows :   1 ) This paper proposes an approach to identify safe and unsafe data points for each of the two approaches. The idea is to first identify the condition under which the data point is safe and then classify the safe data points based on the difference between the safe points and the unsafe ones. 2 ) This approach is then used to train a policy that is both safe and safe."
SP:4d085e57286fdd36143108a002d16914222c239a,"This paper proposes a variational inference framework for learning dynamical systems from time - series data. The key idea is to use the variational approximations of the latent state of a given process to estimate the posterior distribution of the parameters of the process. The derivation of this variational approximation is based on two assumptions : ( 1 ) that the process diverges as a function of time, and ( 2 ) that there exists a smooth distribution over the time interval $ t$ over which the posterior distributions of the processes diverge.    The main contributions of the paper are as follows :   1. A variational framework for inferring the posterior of a diffusion process from time series data that is invariant to the choice of time $ t$. This framework is shown to be robust to various perturbations in the data distribution, and can be applied to a variety of natural and synthetic biological systems. 2. A proof - of - concept inference algorithm is presented for learning such a framework, which can be used in a number of real - world biological and natural - sciences applications."
SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"This paper studies the problem of recovering the spectrum of the sensing matrices of a nonlinear mapping function with nonlinear inverse problem. The authors prove the existence of an upper bound on the number of iterations needed to recover the spectrum. The lower bound is then used to prove the convergence of an EP algorithm to the upper bound.    * * Key words * * : * * compressed sensing, non - linear mapping function, nonlinear processing function, phase retrieval, phase - retrieval"
SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"This paper studies the problem of attribute localization in the context of domain shift. Specifically, this paper focuses on the question of cross - domain transferability of visual representations in the domain shift setting. To this end, the paper proposes two approaches : ( 1 ) Prototypic Prototypical Classification ( PCT ) and ( 2 ) Domain - shift Learning ( DZSL ). PCT is an approach that aims to design a set of prototypes that can be used by a learner to discriminate between different types of attributes ( e.g., categories, domains ). The key idea is to design prototypes that are not too different from each other in terms of their similarity to the target domain, and ( 3 ) domain - shift learning is a way to train such prototypes so that the learner can transfer the learned prototypes to a target domain without having to re - define the source and target domains.    The main contributions of this paper are as follows : 1 ) PCT provides a solution to the attribute localization problem in the framework of the zero - shot learning framework. The proposed prototypes are shown to be more diverse than the existing state - of - the - art approaches, and are able to distinguish between different kinds of attributes."
SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"This paper proposes a new model of defocus blurring, based on a recurrent attention module, to tackle image deblurring problems in deep neural networks. The main contributions are :    1. Introducing a new metric to measure the amount of blur / blurring that needs to be dealt with in each iteration of the attention module ; 2. Estimating the number of iterations required for each iteration to converge to a fixed point ; 3. Demonstrating that $ \ell_0 $ iterations are necessary for a fixed - point blur - to - in - focus image to converge ; 4. Providing empirical evidence that the proposed approach is more efficient than previous approaches in terms of blur and blurring cross - entropy."
SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This paper presents a video self - supervised learning algorithm for video classification. The main idea is to provide the student with a series of videos with different frames, each of which is given a different amount of information ( e.g., the number of frames, the length of the video ), and the goal is to train the student to predict the next frame's visual content.    The main contribution of this paper is the contrastive learning algorithm, which is based on the idea of cross - reference between the visual content of the videos and the information provided by the teacher. The paper shows that this can be used to improve the student ’s knowledge of the future frames."
SP:8c7b1d976d9758cd534c565ec31a23f97892e503,This paper considers the following questions :   1.   What is the best way to train a neural network to learn a non - parametric / kernel model?   2. what is the worst way to learn such a model? 3. how do we know if we can trust it? 4. what are the assumptions that need to be satisfied in order for it to be a good model?
SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper provides bounds on the sample complexity of the worst - arm - identification algorithms under the following assumptions :    ( 1 ) the number of samples is at least as large as the total number of queries, and ( 2 ) the distribution of queries is not too complex.   The authors provide bounds on both of these quantities using a collection mechanism that is based on minimizing the variance of the distribution over queries sampled from a set of potentially complex functions with respect to a fixed set of queries."
SP:471361588bfc6c6033631509d1e43e77fd9721ce,"This paper provides a theoretical analysis of the convergence rate of the SGD - compressed gradient descent gradient descent algorithms in terms of the number of iterations and the amount of data used to train the algorithms. The authors show that the rate of convergence of the gradient descent algorithm does not scale linearly with the total amount of training data used, but rather scales linearly in the fraction of iterations used. This is in contrast to the naive assumption that all iterations of the algorithm are equal in size, which is assumed in most existing works. Moreover, the authors provide theoretical analysis on the effect of different amounts of training ( i.e., how much data used for each iteration, how many iterations, and how much training data is used ) on the cumulative rate of gradient descent. Finally, they provide empirical evidence to support their theoretical analysis."
SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"This paper presents a series of explanations - aware training and fine - tuning ideas for GNN, a probabilistic model for graph classification. In particular, the paper focuses on GNN's ability to be robust to changes in the model's parameters during pre - training and post - training. The paper also presents a set of examples that illustrate the importance of these ideas."
SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"This paper proposes a new linear neural network ( PLNN ) model for protecting against adversarial attacks on neural networks. The proposed PLNN has two main components :    1 )   a linear network with linear layers, 2 ) a neural network with nonlinear layers, 3 ) a nonlinear network with topological layers, 4 ) a linear layer with hyperparameters.   Experiments are conducted on several real - life benchmark datasets and PLNNs are shown to outperform the state - of - the - art."
SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"This paper proposes a novel approach to tackle the problem of information bottleneck in text - to - speech ( TTS ) conversion. The main contributions are :    1 )   fake audio detection model that is designed to mislead the user into thinking that the source audio is a real audio. This is done by exploiting the fact that the teacher does not have access to the original source audio, and that the student has access only to the teacher's transcriptions.   2 ) “ whitewashing ” the teacher ’s representations to ensure that they are not “ fake ”, i.e., that they do n’t leak any private information about the teacher to the adversary. The teacher has access to only the original audio, but not the transcriptions of the teacher, and thus can not be used to create fake audio clips. The authors use this fact to devise a policy that prevents the teacher from having access to any audio clips that violate the privacy of the student. The policy is based on the idea of “ teacher - supervised representation learning ” where the teacher is provided with a large amount of training data, and the student is provided only with a small fraction of the training data. The goal is to train a teacher that is able to “ decode ” real audio clips without leaking any private data, while “ supervising ” a teacher to not leak any of the private data. Experiments are conducted on a variety of TTS and speech - related systems, showing that the proposed policy works better than state - of - the art."
SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,This paper proposes a target localization network that learns 3D shape - aware feature learning from point clouds. The proposed network is built on top of the BEV feature map embedding and complete target tracking network. Experiments are conducted on a variety of different environments.
SP:8b788c78680a54c453a04f4551436763ee57585e,This paper proposes a new encoding method for multi - dimensional spatial structure modeling. The proposed method is based on the Fourier feature representation. The authors show that the proposed encoding method can be used for spatial classification tasks.
SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"This paper proposes a new method for testing the quality of the structure learned from a data point of view. The key idea is to use a constraint - based approach, where the goal is to find a set of data points such that the resulting structure is at least as good as the one learned from the data point without the data points.    The main contribution of this paper is the introduction of the concept of boundary information, which is defined as the difference between the set of structures obtained from a given data point and the one obtained from another data point if the two data points lie on the same line of the same dimension. This concept is then used to motivate a new bounding - ball method, which can be used to test whether the given data points are at or away from the boundary of a given structure. The authors also propose a new way to test the invariance of the obtained structure under different choices of constraints."
SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,"This paper considers multi - armed contextual bandit attacks, where each arm is equipped with a different contextual data point ( e.g., a < s, a > < a >, < b > ), and the goal is to maximize the exploitation of the contextual data provided by each arm. The paper considers the following multi - arm bandit attack scenarios :    1. For each arm, there is a data point < s >, and a batch of < a>samples are provided to the arm.   2. For the rest of the arms, the batch is allocated according to the data points < s<a >, where $ a \in \mathcal{S}$ is the contextual datapoint of the arm and $ b \in { S}$ the batch size of the data point. 3. The arm is assumed to have access to at most one data point in each batch, and to be able to exploit this data point to some other data point with a probability proportional to $ \sqrt{\sqrt{s}$.   The paper shows that there exist instances where this batch allocation is optimal, and instances where it is suboptimal."
SP:653a519e3c799c25e0d0b4240322642040b121a3,"This paper studies the problem of domain - invariant representation of the MNIST dataset. The main contributions are two - fold :    1. This paper provides a theoretical analysis of the representation discrepancy between the original datastream ( DI ) and the domain - specific domain ( DG ) representations under the following assumptions : ( 1 ) the original dataset is invariant to all domains, and ( 2 ) there exists a source - domain correspondence ( DA ) such that the representations of the two sources are identical under the assumption that both the source and the target domains are domain invariant.   The main result is that under the above assumptions, the bounds on the DA representation discrepancy can be phrased as follows : ( i ) for any source DA representation $ \theta$, there exist source DA representations $ \tilde{i } $ such that $ tilde{j } \in \mathbb{R}(\theta)$ can be expressed as $ \text{max}$ where $ j\theta = \text{\theta}$ is the ratio of the difference between the two DA representations and $ k$ is an upper bound on the distance between them. ( ii ) this result is extended to the case where the source domain is a subset of the target domain $ d$, and the source representation $ j$ is a set of domains $ d$. The bounds are phrased in terms of $ k$. The result is shown to hold for both DA and DG representations."
SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"This paper proposes a new structured sparsity learning ( SR ) framework, named ASSLN, which aims to provide a better knowledge distillation between the structured structure of a network and the amount of data that needs to be transmitted across the network in order to make the network more scalable. The proposed framework is based on two main components : ( 1 ) regularization regularization layer and ( 2 ) location penalty term term term. The paper provides a theoretical analysis of the proposed framework and empirically shows that the proposed method is more scalable than the previous Sec. 1 method ( Sec. 2 ) and achieves state - of - the - art performance in terms of the number of data required and the total number of bits required."
SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"This paper presents a multi - agent reinforcement learning framework that aims to provide informative experiences for agents to explore in order to better understand their ability to perform well in different environments. The framework consists of three components : ( 1 ) informative experience training, ( 2 ) informative reward training, and ( 3 ) Q - value driven exploration. The first component provides agents with a set of local action - based examples, which is used to train agents to select actions that are informative to them in terms of their utility function. The second component provides a collection of actions that can be used by the agent to learn a policy that is informative to other agents. The third component provides rewards that are used to reward the agent for performing well in a different environment.   Experiments are conducted on the following benchmarks :    [ 1 ] Multi - agent Reinforcement Learning ( MARL ), [ 2 ] Robust Experiments with Multi - Agent Learning ( RLE )."
SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"This paper presents a lower bound on the linear regression problem for vector - decodable linear regression. The problem is formulated as follows : given a linear regression model $ p(x, y)$ and a set $ y$ of covariates $ \theta$, define a vector $ z$ such that $ z \in \mathcal{O}(\theta)$ is the product of $ z\infty$ and $ y \infty$. The goal is to find $ z(x,y)$ that minimizes the ratio of the expected value of the vector $ y^t$ over the set $ z$.    The main contribution of this paper is the following :   -   1.   This paper shows that there exists a $ \tilde{xi}$-th order linear regression with $ z=\theta$. This is the first lower bound for linear regression on $ z^t$. - 2. This paper gives an upper bound for $ z^{-1}$ under the following assumptions : $ z = \alpha(x)$, $ y=\alpha(z)$. - 3. This lower bound is tight enough to be applicable to all linear regression models with $ \text{text{xi } > \mathbb{Z}$."
SP:7b258252a9063514348f5fa8d9c85afd85748747,"This paper proposes a method to train neural ODEs that can be used in combination with machine learning ( ML ) approaches. The main idea is to train a set of ML models on a small set of data points, each of which is a subset of a larger data set. The goal is to learn an ODE that is able to capture a given system's temporal behaviour. To this end, the authors propose a hybridisation model ( LHM ) that is designed to capture both the temporal and the dynamic aspects of the data.    The main contributions of this paper are as follows :   1. Developing an ML model that captures both the dynamic and the data aspects of a system ; 2. Conducting a series of experiments to test the model's ability to capture different types of data ; 3. Providing a framework to train ML models that can handle different kinds of data."
SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"This paper proposes a new framework, MAML - like, for the task - specific representation learning. The framework is built on top of GPT-2, which has been shown to achieve state - of - the - art performance in the meta - learning setting. The main contribution of the proposed framework is that it is able to adapt the learned representation to new tasks without compromising on the quality of the original representation."
SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"This paper presents a program - synthesis - based approach for unsupervised learning of the compositional and grounded meaning representation of words. The approach is based on the following components :   1.    * Concretely, the authors define a compositional space $ g$ of words $ x_0 $ and $ y_1 $ tokens $ z_0$ such that $ z$ can be represented as a program of length $ y$ with $ z \in \mathcal{L}$ being the number of tokens in the program and $ \text{x_0 } $ being the length of the program. This space is then used to train a program synthesis algorithm $ g2L$ that learns a program $ y^t$ from $ z_{0,1}$ by parsing $ y_{1,2}$ into $ z^{-1 } $ tokens and then marginalizing $ z}$ to get $ z^t$.   2. * Inversely, they define a hyper - space $ space $ G$ of $ n$ tokens $ x_{0, 1 } $ that can be used to learn programs $ y^{-2 } $ from a set of tokens $ n_0$. This space can then be used as input to a program generation algorithm $ G2L$. This is done by using a lexicon - based token embedding model $ g^t $ that consists of $ g(x_{0}$ and $ g_1$, where $ g_{0 } = g_2 $ is a program with $ n(x_1,1)$ tokens and $ G_2$ is the program representation of the token $ z$. In this way, the program is not restricted to a single language, but rather can be learned over a large set of languages. This allows the program generation process to be more flexible and more data - efficient. 3. * This paper evaluates the approach in two ways. First, it evaluates the effectiveness of the embedding by comparing the learned program - representation space with the set of data generated by the program - synthesizer. The second way is to evaluate the learned programs by comparing them to the set generated by a program synthesizer trained on top of this embedding space. The results show that the proposed approach is more data efficient than the previous approaches ( e.g., [ 1 ] and [ 2 ] )."
SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"This paper considers the following convex optimization problem : given a convex function $ \mathbb{Z}$ and a vector $ z$, find a distribution $ z_t$ such that $ z_{\text{x } \log z}$ satisfies the following objectives : $ \text{xi } \leq \sqrt{\sqrt{x}$ where $ \tilde{xi}$ is the convex hull of $ z$.    The paper proposes two methods to achieve these objectives :   1.   stochastic gradient - based gradient descent ( SGD ). The authors prove that SGD converges to the objective with high probability under certain assumptions ( e.g., $ \ell_t\log z$ ), and give a lower bound on the number of iterations required to achieve the objective. 2. )   Stochastic convex communication ( STC ). This is a variant of SGD where $ z \log t$ is assumed to be a vector of length $ \infty$. The authors show that STC is guaranteed to converge to the target with high confidence under some assumptions."
SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"This paper studies the problem of training a metric metric to measure the distance between two points in a dense domain. This is a very interesting and important problem and has been studied extensively in the literature.    The main contribution of this paper is the following :   1. The authors provide a theoretical analysis of the problem. They show that if we assume that the two points are located in the same domain ( e.g. $ \ell_2 $ ), then the metric can be approximated by a convex function $ \sqrt{\sqrt{T}$, where $ t$ is the distance and $ T$ is a hyperparameter ( $ \epsilon \in \mathbb R^{-1}$ ). 2. They prove that the metric is convex if and only if $ t \infty$ and $ t\infty$.   3. They provide an empirical evaluation of the metric in terms of $ \nabla_{\ell_\text{t}$ and show that $ \tilde{T } \log(T)$ is inversely proportional to $ t$."
SP:e4b302009520770814ff2c096020b779a9fc38fe,"This paper proposes an approach for distillation of small student - generalization networks. The main idea is to first distill a small student network into a teacher model, and then train the teacher on top of the distillation.    The main contributions are as follows :   1. This paper shows that the student - teacher distillation pipeline can be seen as a way of distilling the student model into a generalization pipeline. 2. The paper proposes a way to train the teacher model by first distilling a small dataset into a teacher model and then training the generalization on the top of this teacher model."
SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"This paper considers the problem of learning a metric tensor over a set of data points $ k$ and a set $ t$ of time $ t$.   It is assumed that the data points in the set are real - world data points ( e.g., trees ), and that the metric $ \mathcal{T}$ can be decomposed into $ t \log t$ and $ t\log p(k, t)$, where $ p$ is the distance between the points $ t(k,t)$ and the corresponding metric $ t(\log t)$. The authors assume that $ t^t$ is a convex function of $ t $ and that $ \log p(\log k)$ is not too large.    They show that the following metrics are not optimal :   ( 1 )   $ \text{max}_t^t $ is not convex ; ( 2 ) $ \gamma_{t } \log k$ is too large ; ( 3 ) $ p(\text{t}^t ) is too small ; ( 4 ) $ k^t(k)$ can not be smaller than $ \sqrt{\log t}$ ; ( 5 ) for any $ t_t$, $ t_{t}$ is either too large or too small, and ( 6 ) there is no metric that can be made large enough to make the metric learnable with high probability ; ( 7 ) there exists a metric that will make the model learnable in the worst case, but not in the best case."
SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"This paper studies the top - m identification problem for linear bandit models, where the goal is to find an algorithm that can identify a set of $ \top$ elements $ m$ with probability $ \sqrt{t\top}$ such that $ m = \top\infty$, where $ t$ is the number of elements and $ \text{t}$ is a linear model of $ m$.    The authors propose a new lower bound on the complexity of the problem. They show that $ t\top$ can be expressed as a sum of two terms : $ \tilde{t } \infty$. The first term is an upper bound that is upper bounded by $ \ell_t$, while the second term is a lower bound that depends only on $ \alpha$ and $ t$. They also show that the lower bound is also lower than the upper bound of [ 1 ]."
SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"This paper proposes a contrastive learning method to learn disentangled graph representations from supervised graph representations. The proposed method is based on a multi - channel message - passing layer, where each message is passed through a pre - trained graph encoder followed by a supervised learning layer. The goal of the supervised learning is to learn a representation that satisfies both the information - theoretic and the discriminative objectives. To achieve this goal, the authors propose to first learn a factorized representation of the input graph, and then train a discriminator based on the factorized graph representation. Experiments are conducted on standard graph classification datasets and show that the proposed method achieves state - of - the - art performance on both supervised and unsupervised learning benchmarks."
SP:0a7edbbdabab11273689c40c517001eb46491113,"This paper proposes a hypothesis testing framework for assessing the robustness of simulated systems to adversarial attacks. The proposed framework is based on the notion of kernel - based robustness, which is defined as the ratio of the kernel - size of the network with respect to the number of agents in the system. The paper proposes two types of robustness measures, i.e., adversarial and corruption robustness metrics. The first metric is used to measure the difference between the kernels of the two metrics, and the second metric measures the ratio between the kernel kernels of two metrics.    The paper also proposes a data - driven robustness metric, which measures the difference in the kernels between two sets of agents when they are placed in different environments. This metric is also used to evaluate the quality of the simulated systems."
SP:c1db485ff1ff9573daa421e167225654babb55ac,"This paper presents a generative model - free text - to - speech synthesis and conditional - generation framework. The proposed framework is based on the VQA framework.    The main contributions are as follows :   ( 1 )   a new formulation - specific formulation - based generative modeling framework, which can be used to solve the cross - sectional cross - correlations and auto - order cross - sectionsal problems in the text synthesis framework ; ( 2 ) a new vector - conditional vector - expansion framework that is able to solve both the auto - and cross - intersection problems."
SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"This paper presents a two - sample power - testing approach to measure the tangent kernel of the MMD ( MMD ) statistic. The proposed approach is based on two assumptions : ( 1 ) the test statistic is invariant to both type - i error and type - ii error, and ( 2 ) the statistic is robust to both types of error.   The paper provides a theoretical analysis of the proposed approach, which shows that both assumptions are necessary for validating the proposed test statistic. To validate the theoretical analysis, the paper proposes two types of data sets : synthetic and real - world datasets. The theoretical analysis shows that the two assumptions are equivalent, and the empirical results show that the proposed power - test statistic can be robust to type - I error."
SP:1df2ffbbe56b8018067820980b93af2a8b57f891,This paper presents a new adversarial example - based defense framework based on net - classifier. The proposed framework is built on top of the net - disentanglement framework developed in [ 1 ] and [ 2 ]. The main contribution of the paper is to provide sufficient necessary information for the classifier to be able to defend against adversarial attacks.
SP:2789874561620ba7894c4672f935056bb911e919,This paper studies the problem of ensuring user - level privacy in the federated Thompson sampling ( FTS ) framework. The authors propose two approaches to this problem. The first approach is to use a differentially private ( DP ) framework to learn the hyperparameters of the FTS algorithm. The second approach uses a risk - aversion based approach to choose the parameters of the algorithm to ensure that the privacy of the learned parameters is preserved. The experiments are conducted to evaluate the effectiveness of the proposed approaches and show that the DP - FTS - DE approach outperforms the other two approaches.
SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"This paper proposes a variational inference method to predict the covariance matrix of multi - label data from a mixture of label correlations. The method is based on the GP - B2 M model, which has been proposed in [ 1 ] and [ 2 ]. The main contribution of this paper is to propose a sampling - based covariance mapping between component - label covariance and feature - component label mapping, which can be used to learn a non - exclusive classifier from the mixture of labels. The paper also proposes a learning strategy to sample from the mixed component - component mapping space, which is used in [ 3 ]. Experiments are conducted on both synthetic and real data to show the effectiveness of the method."
SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"This paper proposes a new cosmological coordinate system based on 3D object detection model. The main idea is to add a multi - scale context padding to the 3D convolutional layer to make it more cosmologically safe. The proposed method is based on the idea of stratified convolution and normalization.    The main contributions are as follows :   1. To make the cosmology more safe, the authors propose to add an extra dimension to the dimensionality of the convolution layer. 2. To further improve the cosy, they add a scale - invariant dimensionality to the coordinate layer. 3. They show that the proposed cosy is more stable than the previous cosy based method."
SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"This paper presents a theoretical and practical framework for the learning of function gradient estimators based on SSTs. The main contributions are :    1. Introducing the concept of "" softmax trick "", an invariant variable that can be used as a surrogate for learning the gradient estimator of a given function, 2. providing theoretical results showing that the gradient of a function can be approximated with the help of a vector - based surrogate, 3. providing empirical results showing the equivalence between gradient - based and scalar surrogate estimates of the function."
SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"This paper presents a new method for improving the quality of the training data for the TEM ( Transmission Ensembles for Emission Measurement and Spectroscopy ) ( TEM ) system. The method is based on the idea of training a convolutional neural network ( CNN ) on a large set of data, and then fine - tuning the CNN on top of this data using a series of data points sampled from the training set.    The main contributions of this paper are as follows :   1. A new dataset of data for TEM is generated, which is used to train the CNN. 2. A series of training data is collected from the public domain, and the CNN is fine - tuned on it. 3. Experiments are conducted to evaluate the performance of the proposed method."
SP:90afa1102683b456bc72a54abef466326827546a,"This paper proposes a new method to solve the panoptic quality surrogate loss for semantic and instance segmentation ( COPS ) problems. The proposed method is based on the multi - way cut problem solver proposed in ( Xie et al., 2020 ). The main contribution of this paper is to propose a surrogate loss that can be used in the post - processing stage of COPS. The paper also proposes a differentiable architecture for segmentation and boundary prediction. Experiments are conducted on COCO dataset to validate the effectiveness of the proposed method."
SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"This paper proposes a new method for constructing hierarchical Bayesian networks from discrete and continuous latent variables. The main contribution of the paper is the following :    1.   a hierarchical clustering framework for tensors that can be used to obtain posterior estimates of the likelihood of a set of variables conditioned on a discrete or continuous class of variables. This is done by first constructing a tree structure of the class of possible structures conditioned on these variables, and then using it to obtain a posterior distribution over the tensors conditioned on them. The resulting posterior distribution is then used to construct a classifier for the data conditioned on the class variables. 2. a probabilistic inference framework is developed for the classifier that is trained on this tree structure. 3. A set of experiments are provided to demonstrate the effectiveness of the proposed method."
SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"This paper proposes a quantization - based approach to improve the performance of the backpropagation ( CBP ) algorithm for learning functions in neural networks. The main idea is to train a neural network with a fixed number of parameters $ \ell_2 $ and a fixed amount of iterations $ t$, and then train the algorithm on a set of functions $ t$.   The authors prove that the proposed quantization framework achieves better performance than the baselines on the following metrics : $ t \infty$ and $ t\infty$. Moreover, the authors show that the quantization method is more robust to the number of iterations and the length of the training iterations."
SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"This paper studies the problem of active learning in the context of query synthesis and pool - based active learning. The main contributions are two - fold :    1. This paper provides a theoretical analysis of the trade - off between data / label efficiency ( EER ) and query - synthesis error reduction ( SER ) in the active learning setting. In particular, it shows that the optimal query synthesis strategy is the one that minimizes the difference between the probability that a query is generated from a subset of instances in the instance space and the probability of a query being generated from the subset in the pool.   2. The paper provides an empirical study of the error - reduction tradeoff between EER and SER for two active learning algorithms, namely GPC and EER - pool. The empirical results show that the SER algorithm is more error - efficient than the GPC algorithm in terms of the number of instances generated per query and the fraction of queries generated per instance."
SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"This paper provides a theoretical analysis of the stability of variational autoencoder ( VAE ) based architectures with respect to the number of informative latent dimensions. The main contributions are :    1.   The authors prove that under certain assumptions on the dimensionality of the space, the convex hull of the gradients of VAE models converges to the cosine of the energy function of the underlying convex manifold. This is in sharp contrast to the naive assumption that the space is not convex, which assumes the space to be convex. The authors show that this assumption is equivalent to assuming that the volume of the spaces is bounded by a constant factor, which is not guaranteed by the assumption of convexity. 2. They show that under a certain condition on the volume, the entropy of the model can be approximated by a convex function. 3. They provide an upper bound on the step size of the gradient that can be used to ensure that the model is under - regularized and under - trainable."
SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"This paper studies the following packing problem : given a graph $ G$ and a linear program $ R$, what is the best way to send $ \text{R}$ to $ R}$ without hurting the quality of the message?   This paper answers this question by showing that for any fixed $ \mathcal{R } \in \mathbb R^{-1/2}$, there exists a set $ R^n$ such that the message < r^n > satisfies the following guarantees :    1.   * * For any $ \cal_{R}^n $, there is a fixed $ r^i \in { \mathbf r^{-2 } $ such that $ \log(r^n)$ is a message that can be sent to   any arbitrary $ \lambda$ ( see also [ 1 ] ). * 2. * * for any $ r$ and any fixed "" R^i "" ( see [ 2 ] ), there exist a fixed "" r^d "" such that any message > r^t + 1 $ ( where R^t is the number of elements in the message ), and any message < i \in [ 0, 1]^t+1 $ ( e.g., < i\in [ 1, 0, 0 ] ) can be transmitted to ------------ < i.e.,   \lambda^n + 1. * 3. * This paper shows that the following problem is equivalent to the following : given $ \ell_2 $ and $ \alpha_1 $, let $ \tilde{r}$ be the convex hull of the set $ G$. Then $ \gamma^n \in G$ is the set of all messages that are sent through $ \theta_1$.   The goal of this paper is to send a message to $ \silence^n\in G$. The goal is to make the message "" indistinguishable "" from the message received by the reader if the message is not too close to the one sent by the speaker.  * [ Sec. 1 ] * * Theorem 1.1 ( Sec. 2 ) shows that there is an upper bound on the amount of information that needs to be sent in order for the message to escape from the problem ( see Sec. 3 ). This bound is tight ( tighter than the upper bound shown in [ 3 ] ) ; moreover, it is shown that if $ \kappa_1$ is greater than $ \sqrt{r^i }, then the problem is non - convex ( and possibly non - dual ). In fact, this upper bound can be phrased as a convex combination of two inequalities ( the first of which is tight, and the second is tight ), where the first inequality is the product of the upper and lower bounds of the lower inequality ( the second one is the lower bound )."
SP:e50dec57af337839cbde4b65fb7b431785fda44d,"This paper studies the question of feature relevance in the context of agnostic agnostic classifiers. Specifically, it considers the following : given a set of data points $ \mathcal{X}$ and a set $ \text{x}$ of $ \theta$, what is the probability that the feature $ x$ is relevant to the given data point $ y$?    This question is formulated in terms of the following two questions :   ( 1 ) is there a distribution over $ \tilde{x } $ such that $ x}$ is not too different from $ z$, and ( 2 ) how well do we know the model's behaviour under different assumptions on $ z$.   To answer these questions, the paper proposes two approaches.   First, it firstly considers the problem of determining whether there is a relation between the feature relevance of a given feature and the behaviour of a generic classifier ( e.g., the one used to model it ). The second approach is to consider the case when $ z = z$ and $ z=\theta$. To do so, it assumes that $ z \to z$ is a cosine of $ y$. The paper shows that there exists a distribution $ \sqrt{\theta}$ such that the model behaviour under $ z $ is similar to the one under $ x$. In other words, the model is not able to distinguish between the two distributions if $ z\to z$. To handle this problem, the authors propose to use a normalised importance sampling approach, i.e., they assume that $ y\toz$ is close to $ z^{-1}$ where $ z is the ratio of the expected value of each feature to the value of the generic feature ( $ z_i$ ), and $ x_j$ is the fraction of the feature absent from the data set $ z.   The main contributions of this paper are as follows : ( a ) a new estimator for the importance of the model model, $ \alpha$, for the cross - model relation between feature relevance and model behaviour ; ( b ) a derivation of the relation between $ z_{i } and $ y_j$."
SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"This paper presents an approach to learn feature representations for reinforcement learning on virtual state - action trajectories that satisfy the constraint of timestep invariance. The approach is based on two components :    ( 1 ) a dynamic model that learns a set of trajectories conditioned on the state of the environment, and ( 2 ) a control model that is trained to ensure that the learned trajectories are consistent with the environment.   Experiments are conducted on a variety of environments, with the goal of learning representations that are representative of the agent's state and action experience. The paper presents results that demonstrate that the approach achieves state - of - the - art performance on a suite of benchmarks."
SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,This paper presents a series of data - driven label - training methods for real - world neural network architectures. The main contributions are :    1. Introducing a new metric that quantifies the amount of data that needs to be transmitted in order to train a neural network to be robust to noise ; 2. Demonstrating that this metric can be used to quantify the extent to which the network is able to learn representations that are robust to perturbations in the data ; 3. Conducting experiments on a variety of datasets to demonstrate the effectiveness of each of these metrics.
SP:903727fe028684623a8ccadec210e641ecffc685,"This paper proposes a two - stage learning algorithm based on the Bellman equation. The first stage consists of two stages :    1.   learning a $ \ell_t$-value function $ \tilde{x}$ and a $ t+\ell_{t}$ - value function $ x_t$.   The second stage consists in learning $ t$-values $ x_{t } \infty$ and $ t + \ell_{0}$ by solving a set of $ T$-dimensional control problems with $ t=1 $ and $ T=2 $ dimensions. The authors provide a theoretical analysis of the proposed approach and show that $ t-\ell_0$-$ value function can be approximated by $ \mathbb{R}^2 $, where $ \text{R } \log(t)$ is a convex combination of $ t$. The authors also give a theoretical proof that the $ t_0 $ - value can be represented as a sum of $ \alpha$-\alpha$, $ \sqrt{\alpha}$, and $ \gamma$."
SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"This paper studies the excess population risk of stochastic convex convex optimization algorithms with non - smooth convex losses. The main contributions are two - fold :    ( 1 ) it is shown that for any linear convex loss $ \ell_1 $, there exists a private algorithm $ \sqrt{x}$ such that $ \tilde{x } \leq \log p(x)$ where $ x \in \mathbb{R}$ is the number of iterations in the algorithm and $ \text{eps}$ the ratio of the expected excess risk of the algorithm with respect to $ \epsilon$ over $ \eps$ is less than $ \sum_{eps}/\ell_2}$. This is a lower bound than the one obtained in [ 1 ], and it is the first one to be obtained for non - convex algorithms with private losses. ( 2 ) the authors show that there exist algorithms $ \sigma^2 $ that satisfy this lower bound.   The proof relies on the following assumptions : $ \infty$ is a convex function $ \alpha$, $ \gamma_0 $ is a smooth function $ z_1$, and $ x_2 $ is the product of $ \log(z_1/\gamma)$ with $ \lambda_0$.   In the first part of the proof, the authors prove that the above assumptions are equivalent to the lower bound of [ 2 ]. In the second part, they show that the upper bound is tight in the convex - linear setting $ \nabla$ \ell_{eps$."
SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"This paper considers the following multi - agent reinforcement learning problem :    1. Each agent has access to a fixed number of messages and a fixed amount of data. The goal is to learn a network topology such that each agent receives at most one message from all other agents in the group.   2. For each agent, there is a delay between the last message received by the sender and the next message sent by the target agent. 3. In each round, the goal of the agent is to decide whether to send the message to the target or not, and whether to share the received messages with the other agents. The problem is formulated as follows : the sender first sends a message to all the agents, which is supposed to be legitimate. The target agent then sends a reply to the sender, which will be legitimate if the received message is legitimate. 4. For every round, there are at most two actions taken by the agent : 1. The network topologies of the agents are assumed to be the same, i.e., the topology of all the messages in the last round is the same for all the actions taken in round 2. The objective is to minimize the sum of the expected losses incurred by each agent for each action taken in the round. The main contribution of this paper is to provide a minimax lower bound for this problem."
SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"This paper proposes a quantization - based approach to improve the quality of the training quantization of transformers for computer vision applications. The proposed approach is based on the attention - centric quantization approach, where each attention layer is divided into a set of tokens and each token is quantized with a small number of tokens. Each token is then converted into a vector and then passed through an attention layer to obtain a vector representation. The authors provide a theoretical analysis of the quantization loss and a strategy to minimize it. Experiments are conducted on ImageNet dataset to demonstrate the effectiveness of the proposed approach."
SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"This paper studies the problem of Q - learning, where the goal is to learn a function that approximates the Bellman operator $ \ell_2$. This problem has been studied extensively in the context of stochastic learning, e.g.   [ 1 ], [ 2 ] and [ 3 ]. In this paper, the authors extend these results to the setting where the function approximates a random variable $ z$. In particular, they show that $ z$ is a convex function that satisfies the following property : $ z_\ell_\infty$ is not convex if and only if it is convex in the sense that $ \sqrt{\ell_z}$ is the product of $ z^{-1}$ and $ z_{-2}$, where $ z}$ can be thought of as the sum of the expectation of the expected value of the function $ z^2 $ and the probability that the function z^3 $ is close to zero.    The main contributions of this paper are two - fold :   1. This paper provides a theoretical analysis of the convergence rate of the naive sampling strategy used in the naive learning setting, and shows that it converges to zero in a finite time interval. 2. The authors provide a practical sampling strategy that achieves the same convergence rate as the naive one."
SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"This paper proposes a new approach to OOD detection based on supervised learning ( SSL ). The main idea is to first generate a set of unlabeled data and then train a supervised learning algorithm to detect OODs from it. To this end, the paper proposes two approaches : ( 1 ) to generate labeled data and ( 2 ) to optimize the labeled data.   The first approach is based on cross - entropy ( CE ) and ( 3 ) on topological structure ( K ). In the first approach, the data is divided into two subsets, labeled and unlabeled, and the SSL algorithm is used to detect the OOD from both subsets. The second approach, on the other hand, is to generate data with different topologies ( K and K ) and to optimize a different SSL algorithm based on the topology of the data. The paper provides a series of experiments to validate the effectiveness of both approaches."
SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"This paper presents a multi - task pre - training framework that consists of two stages : ( 1 ) grounding grounding and ( 2 ) expression comprehension / segmentation. The grounding stage first trains the model on a set of visual representations of the input, and ( 3 ) evaluates the model in terms of how well it is able to understand / segmentate the input. The two stages are separated into two stages, where the first stage is used to train the model, and the second one is used for the segmentation stage.    The authors provide a thorough analysis of the proposed framework. They show that the grounding stage is more efficient than the expression comprehension stage, and that both stages are more effective than the one - step one - stage approach. They also show that both the grounding and expression comprehension stages are faster than one another, and discuss the advantages of each of them."
SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"This paper studies the problem of improving the performance of a generic PAC learner trained on a set of moderately inaccurate data points. The proposed approach is based on the idea of boosting the quality of the data points that are used to train the learner. The main contributions of the paper are as follows :    1. This paper shows that it is possible to improve the performance on a moderately inaccurate set of data points by using a combination of two strategies : ( 1 ) boosting the data point that is more informative than the other data points, and ( 2 ) using a hypothesis - based approach.   The main contribution of this paper is that it shows that the proposed approach can be applied to a variety of PAC problems."
SP:f63b050773871338c48b778c362172e4b72477a4,"This paper presents a unified view of object - centric scene generation and segmentation for both supervised and real - world datasets. The main contributions are as follows :    1 ) A unified view on object segmentation and scene generation is presented, which is based on the following three main ideas : ( 1 ) firstly, each dataset is divided into subsets of objects and ( 2 ) each subset is partitioned into a set of object representations and ( 3 ) the representations are ordered according to the number of objects in the subsets. The authors show that this approach leads to more interpretable and more scalable representations.   2 ) A series of experiments is conducted to evaluate the quality of the representations produced by the proposed approach. The experiments are divided into two parts : ( a ) image segmentation, and ( b ) scene segmentation. The results show that the segmentation procedure is more efficient than the first one, and the scene generation procedure is better than the second one."
SP:408deb9e5577ee7118b836fee77135df641fe545,"This paper studies the problem of conformal and non - conformal inference of the variance of the coverage frequency of the world stock market. The authors propose a non - adaptive version of the black box approach to this problem, which is based on generating distributions that are adaptive to changes in the underlying distribution of the underlying data.    The authors show that this approach can be used to improve the quality of the conformal score of the distribution generated by black box method."
SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,This paper proposes a multi - person pose estimation framework that is based on cue - based cues. The proposed framework consists of three modules :    1. pose - level inference pipeline. The first one is a pre - trained framework that learns to infer the pose of each point in a scene from a set of multi - people poses. The second one is an inference strategy that learns a single actor - critic classifier that is trained to distinguish between actor and critic based on the number of posed objects in the scene. The third one is the cross - sectional inference pipeline that is used to classify objects based on their pose.   Experiments are conducted on CIFAR-10 and ImageNet datasets to validate the effectiveness of the proposed framework.
SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,"This paper presents two new LP solvers for the problem of learning the Bellman operator for rectangular robust decision processes ( RMDPs ). The first one is based on the well - known stochastic continuity method, while the second one relies on the recently proposed linear programming method. Both methods are shown to converge to the same optimal solution in time $ \sqrt{\sqrt{L}$, where $ L$ is the number of variables in the RMDP and $ \tilde{L } \in \mathbb R^2 $ is the norm of the rectangular set of RMDP variables.    The authors also show that both methods converge to an optimal LP solver for the general linear programming problem."
SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,"This paper presents a two - stage online knapsack problem, where each round is divided into two stages :   ( 1 ) a prediction is made for each round, and ( 2 ) a machine - learned algorithm is used to predict the next round. The goal is to maximize the ratio between the expected value of the learned algorithm ( i.e., the upper bound ) and the probability that the machine learned algorithm will deliver the correct answer to the first round.    The paper proposes two approaches to this problem : upper and lower bound. Upper and lower bounds are shown to be equivalent to each other, and upper bound is shown to have lower bound better than lower bound by a factor of 1.5."
SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"This paper proposes a learning model - based approach to address the problem of self - supervision and safe exploration in the context of continuous action space learning. The approach is built on top of two components : ( 1 ) a learned control model and ( 2 ) a learning agent. The control model consists of a set of trajectories sampled from a continuous control model, and the learning agent is a mixture of the learned model and the control agent. Experiments are conducted in both a continuous and a discrete setting, where the agent is allowed to explore a variety of environments ( e.g., environments with different levels of supervision and/or with different amounts of supervision ).    The authors propose a number of strategies to mitigate the risk of forgetting the control and learning agent trajectories. The first one is a multi - step learning strategy that uses a combination of learning agent and control model. The second one is an episodic learning strategy, which uses the learned control agent and the learned agent to plan actions in a way that is both self - supervised and self - driven. Experimentally, the authors show that both approaches outperform the control - based and learning - agent baselines in terms of sample efficiency."
SP:551174c1266b5f4b6aaf5432a4c713386f90898c,This paper presents a data - driven supervised learning approach to improve the accuracy of the SSL method DP - SSL. The main idea is to provide a set of labeled data to train a model that can be used as a data regularizer for learning SSL method. The authors provide a series of experiments to evaluate the quality of the labeled data and the regularizer used to train the model.    The main contributions of this paper are as follows : 1. Introducing a new data programming idea ( DP ) scheme to train SSL method that is more data - efficient and data - safe. 2. Conducting experiments to validate the correctness of the learned model.
SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"This paper presents a multi - view pose transformer ( MvP ) that learns multi - person 3D pose representations for a set of 3D poses from a pre - trained set of 2D pose encoders. The main contributions are :    1 ) a novel attention mechanism for cross - camera generalization of pose embeddings that is based on the attention mechanism proposed in ( Xie et al., 2020 ) ; 2 ) a joint query embedding scheme that learns 3D joint locations for pose embedding and skeleton embedding ; 3 ) an efficient pre - training strategy to recover common objects from the dataset for each pose in the training set."
SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"This paper studies the problem of approximate recovery of sparse vector - based representations of the cosmological constant under the assumption that the underlying vector space is sparse.    The paper proposes an adaptive solution to this problem, which is based on the idea that the vector space may be decomposed into a set of vectors $ \mathcal{x}$ and $ \log(x)\log(z)$, where $ z$ is the cosine of the vector $ z$.   This paper provides a theoretical analysis of the problem, and provides a solution to the recovery problem in terms of the following components :   1. $ \text{text{x } \log{x}\log{z}$, $ z \log { z}$. 2. $ z\log { x}\log z$. 3. $ x\log{xi}$. 4. $ y\log{\text{z }$."
SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"This paper studies the problem of detecting abrupt changes in temporal behavior patterns in real data, with particular emphasis on real - world datasets. The paper proposes a generic sensing scheme that can be used for this problem. The main contribution of the paper is a lower bound on the probability that a query will lead to a changepoint detection error under the following assumptions : ( 1 ) the query will be informative, ( 2 ) the probability of the query being informative is upper bounded, and ( 3 ) there exists a set of informative actions such that the query probability will be less than a certain threshold.  "
SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"This paper proposes a new type of gradient descent algorithm for solving nested optimization problems. The proposed algorithm is based on the gradient descent approach ( ALSET ). The key idea is to update the gradient of the solution of a nested problem according to a smoothness metric ( smoothness is defined as the ratio of the number of iterations needed to solve the original problem with the gradient update ) and a stochastic gradient. The paper provides a theoretical analysis of the rate of convergence of the ALSET algorithm under the following assumptions :    1. There exists a set of $ \ell_0 $ smoothness metrics $ \theta$ such that for every $ \infty$, there exists a $ \tilde{e}$-regularity condition $ \sqrt{e\infty}$ satisfying $ \epsilon_{\theta}$ and $ \text{max}$, where $ \eta$ is the smoothness of the original ( smooth ) problem and $ E$ is a constant factor depending on $ \mathbb{R}$.   2. There exist $ \leq \ell_{\text{e}\infty } $ problems $ \overline{e } \log(\sqrt{\theta)$ where $ e \log{e(\theta})$ is either smooth or has a constant constant factor.   3. For each $ \alpha$, the authors provide an analysis of $ e$-th order of the gradient updates of the proposed algorithm. The authors show that $ e\tilde { e\leq_0}$ is not monotonically decreasing with time, and that the gradient gradient descent converges to zero as time goes on."
SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"This paper presents a video QA framework that aims to provide contextualized knowledge to the learner who is asked to answer a video question within a multi - lingual reasoning paradigm. The framework consists of three components : ( 1 ) video question answering ( videoQA ), ( 2 ) contextualizing knowledge ( SiaSamRea ), and ( 3 ) reasoning ( reasoning ).    VideoQA is divided into two parts : sampling and reasoning. Sampling and reasoning are based on video clips, and reasoning is based on the label guidance provided by the expert who has seen the question in the video. The authors present a series of experiments to evaluate the effectiveness of their approach. They also provide a set of benchmarks to measure the quality of the learned knowledge."
SP:160022e2cd61159da92f92e85520b7062a337a8d,"This paper proposes a probabilistic context - free grammar model ( PCFG ), which is built on top of the recent work on structured state spaces. The main contributions are :    1.   A theoretical analysis of the PCFG model. The authors show that the parameterization of the state space can be expressed in terms of low - rank factorization factorization, and show that this is equivalent to a lower - rank constraint on the dimension of the space. This gives a lower bound on the number of states that can be produced in a PCFG. 2. A proof of the lower bound is given. 3. A theorem is given showing that the probability of producing a nonterminal state is at least as large as the probability that it is a terminal state."
SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"This paper studies the exploration - exploitation trade - off problem in the context of Bayesian bandit algorithms, where the goal is to find a good action that minimizes the cost of exploration and exploitation while satisfying a bounded set of assumptions on the internal model parameters. The paper proposes two approaches to this problem. The first approach is based on sampling from a deep neural network and estimating the posterior of the model parameters, which is then used to select the best action to exploit in a greedy manner. The second approach, based on a hyperparameter estimator, is used to estimate the uncertainty in the value function of the chosen action. Both approaches are empirically shown to outperform the baselines in a variety of settings, including large scale and online settings."
SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"This paper presents a video - based approach for learning from video representations of human behavior. The approach is based on a multi - context embedding strategy, where a video is divided into a series of videos with different levels of supervision. Each video is decomposed into a sequence of sub - contexts, where each sub - context corresponds to a different set of behavioral motifs, and the goal is to learn a dynamic model that captures the interplay between motifs and the overall dynamics of the video. The paper also proposes a dynamic embedding mechanism that is able to capture the temporal structure of the videos.    The main contributions of the paper are as follows :   1. A video embedding framework that learns a dynamic dynamic model from a set of video representations. This model is then used to segment the video into context embeddings, which are then used for analysis. 2. A training strategy that is used to train a dynamic encoder and decoder that learns to capture both the pose and the dynamics of a given video. 3. A dataset that is designed to allow the encoder to learn to capture different types of pose and dynamics."
SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"This paper proposes a new 3D conditional generative model for 3D shapes. The main contributions are :    1.   A new signed distance representation is introduced, which is a convex combination of a signed distance function and a metric on the 3D space of 3D vectors. The authors show that this distance function is not convex, and that it is convex if and only if it is isotropic. 2. A new shape synthesis model is proposed, which consists of two parts : a shape synthesis layer and a distance representation layer. The distances between the two layers are obtained by taking the distance between two vectors of the same dimension, and then taking the distances between vectors of different dimension. 3. A set of losses is given for each distance."
SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"This paper studies the problem of extracting high - dimensional information quantitatively from a set of random projections of a feature extraction dataset. The authors provide theoretical guarantees on the convergence rate of SMI - based feature extractors ( SMI ) with respect to the dimensionality of the random projections. The theoretical guarantees are based on the assumption that the extracted features are not too different from each other. The empirical results show that the SMI converges linearly in dimensionality with a factor of sqrt(sqrt(log(t ) ) ), where t is the number of samples in the dataset.    The paper also provides examples of linear and nonlinear SMI based feature extraction examples."
SP:e220b348901b476c2afd95f97630fb5400582f40,"This paper proposes a new Bayesian optimization method, called C - OPT - C, for improving the convergence rate of the standard BO method in sequential and batch settings. The main contributions are as follows :   1.    Introducing a new metric that quantifies the time required to evaluate a given approximation to a given function in a sequential setting. This metric can be used to estimate the cost of evaluating the approximation to the function in the sequential setting, and is then used as a metric to compare the performance of different BO methods in the batch setting. Under this metric, the proposed method is shown to converge to the optimal solution in terms of the number of iterations and the quality of the approximation used to evaluate the function. 2."
SP:51fbd861422647912f275b48861ea3c4812afdc8,"This paper proposes a new distributional RL method called DQN ( MD3QN ). The main idea is to model the return distribution of a set of points as a weighted sum of two functions : $ \ell_0 $, where $ x_0$ and $ y_1$ are the points in the set and $ z_1 $ is the value of the set in the control setting. The authors show that under certain assumptions on $ x_{0}$, $ y_{1 } \in $ z_{2 } $, and $ p_1$. Then, they show that the corresponding value function $ p_{2}$ can be expressed as a sum of $ p(x_{1}$ and p(y_{2})$ where $ p(\ell_{1})$ is the difference between the two functions. The paper also shows that the value function can be represented as a product of two function functions $ p\ell_{0 } $ and \ell_{\ell_1}$. Finally, the authors provide some theoretical guarantees on the distributional QN under these assumptions."
SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"This paper studies the problem of reconstructing a 3 - dimensional vector flow surface from a set of data points. The authors propose to use the Ordinary Differential Equation ( ODE ) framework to prove the topological properties of the reconstructed surface. To this end, they first derive the condition under which the surface is topologically plausible. Then, they propose a neural network - based solution to the ODE problem. Finally, they show that the proposed method can be used to generate a surface that satisfies the condition of topological invariance."
SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"This paper presents a new deletion algorithm, called adaptive deletion, that can be used to protect the privacy of data generated by a federated data model ( e.g., cifar-10 ). The key idea is to provide a sequence of data points such that each data point can be deleted with a probability proportional to the number of deleted data points in the previous data point.    The paper provides a theoretical analysis of the property of adaptive deletion. Then, it provides a practical deletion algorithm based on this property."
SP:7150006590e268ab732c9be6c9048f67a377f956,"This paper presents a Bayesian optimisation algorithm for value - at - risk ( CVaR ), which is an adaptive version of the well - known MCTS algorithm proposed in [ 1 ]. The main idea is to learn a policy that minimizes the uncertainty in the distribution of the value of a given point in the space of actions. The authors prove that this policy is optimal under stochasticity assumptions on the uncertainty of the actions.    The paper also presents a proof - of - concept approach to ensure that the policy is not too close to the optimal point in terms of uncertainty."
SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,This paper studies the problem of learning a conditional model from unlabeled data on networks. The authors propose two approaches to this problem. The first approach is a gradient descent - based approach. The second approach is an extension of the gradient descent approach proposed in [ 1 ].    The main contributions of the paper are as follows :   1. A theoretical analysis of the error properties of the two approaches. The main result shows that both approaches fail to satisfy the property of being convex in the sense that they do not converge to the same set of values in the limit of infinitely many samples. 2. A proof of the failure of the first approach relies on the fact that the second approach does not provide a sufficient number of samples to train the conditional model. 3. An empirical analysis is provided to show that the proposed approach fails to satisfy both the convexity property of the learned conditional model as well as the failure to converge to a set of valid samples.
SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"This paper presents an approach to mitigate the problem of misinformation in the social media. The approach is based on the following three components : ( 1 ) knowledge guided neural point process, ( 2 ) group detection and ( 3 ) data - driven neural coordination detector. To this end, the authors use the dataset of 19 vaccine related tweets collected by the Wikitext collaboration.    The first contribution of the paper is a knowledge - guided point process that learns to classify groups based on their similarity to real - world entities. The second contribution is a data - based coordination detector that is able to identify and disentangle potentially suspicious accounts. The third contribution is an integration module that is built on top of the learned point process."
SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"This paper considers the following classification problem : given a set of data points $ \mathcal{x}$ and a fully - connected neural network $ \text{network}$ $ p(x, y)$, what is the best way to classify them?   The paper first shows that $ \cal{network } \infty$ can be expressed as a linear combination of $ \sqrt{\sqrt{x } \log p(y|x)$ and $ \tilde{n\log p}$, where $ y$ is the dimension of the data set and $ p$ is a fixed constant.    Next, the paper shows that under certain assumptions, $ \nabla_{\text{dynamic}_delta } $ can be represented as a polynomial function $ \log n\log n$ with $ p(\sqrt { x}^{-1/2}$ regularity condition, and shows that the problem is equivalent to the following : $ \sum_{delta}_p(y | x)$ where $ \sigma^2 $ is the number of points in the set $ y$."
SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"This paper presents a theoretical analysis of cross - entropy loss ( CEGAN ) for GANs with classifier and projection - based datasets. The paper shows that cross entropy loss is a convex function of the number of data points in the training set, and that the convexity of the dataset is a good indicator of the quality of the classifier. Moreover, the paper provides a proof that the CEGAN can be regarded as an upper bound on the cardinality of classifier - labeled datasets.    The paper also presents experiments on two datasets ( CUB200 and CUB100 ) to show that the proposed CEGAN is more robust than prior works."
SP:080e80746a87228b156408ff649ab7a17f44e92d,This paper presents a two - player zero - sum game - based reinforcement learning algorithm called Double Oracle ( XDO ) for sequential sequential action games. The main contribution of this paper is the following :   1.    a deep RL method called Deep RL Method XDO ( NXDO ) is proposed to solve two sequential sequential games in a deep manner. The second contribution is a response - based RL method XDO which is shown to converge to Nash equilibria.
SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"This paper studies the problem of unsupervised and supervised learning of graph - level representations of structured data. Specifically, this paper focuses on the un - supervised learning and supervised - level representation learning problems and proposes two approaches.    First, the authors propose a new un - structured representation learning method, which is based on node clustering and edge feature reconstruction. Second, they propose a supervised learning method based on self - attention framework. Experiments show that both approaches outperform the state - of - the - art representation learning methods by a large margin."
SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"This paper presents a theoretical analysis of the expressive properties of GNNs and proposes a new metric, GraphSAGE, to measure the scalability of their representations.    The paper shows that the expressive property of the GNN can be expressed as a function of the size of the input graph and the number of tokens in the input. The paper also shows that GNN scalability can be phrased as a scalability metric that measures the difference between the representation of a given graph and that of a subset of its tokens in terms of the distance between the embeddings in the original graph and those in the subset of tokens. This metric can be used as a metric for comparing the expressivity of different GNN architectures."
SP:4890f251db559a0a572afc66e0c1f899b577d9ff,This paper considers the problem of learning an affine time - conditioned distribution of flows conditioned on the input distribution of variables.    The authors propose two approaches to this problem. The first approach is based on the assumption that the distributions of variables in a dynamical system are convex under the action of a Langevin dynamics. The second approach is a gradient - based approach based on minimizing the likelihood of the distribution of the variables conditioned on variables conditioned by the dynamics. Both approaches are shown to converge to the same distribution under certain assumptions.
SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"This paper considers the problem of learning a policy that generalizes well to the real - world e - commerce market. To this end, the paper proposes a new evaluation method, called BCQ, that learns a policy to generalize well to a real - commerce platform. The main contribution of the paper lies in the fact that BCQ is a reinforcement learning method that does not rely on any prior knowledge about the policy. Moreover, BCQ can be viewed as a generalization of the well - known R - BCQ method."
SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"This paper proposes a new method to adapt a pre - trained domain classifier to a new dataset ( e.g., image or 3D point cloud ). The proposed method is based on two components : ( 1 ) a pretrained model and ( 2 ) a neighborhood model. The pretrained domain model is used to train the neighborhood model, while the target model is trained on the image and point cloud datasets. The neighborhood model is then used to improve the quality of the domain model and the neighbors model."
SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"This paper proposes a generic pooling method for point cloud, graph, and image representations learning. The proposed pooling approach is based on the existing pooling methods for point clouds, graph embedding, and video recognition. The main contributions are :    1 )   a generic embedding method that can be used for both point cloud and graph representation learning.   2 ) a set - invariant feature aggregation method that is able to handle different types of structured data."
SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"This paper presents a theoretical analysis of the stability of the SGD / exploding gradient descent ( SGD ) and SGD - RNN ( SBO ) problems. The paper shows that SGD and RNN converge to the same stable solution under different perturbations ( i.e., perturbation - free and perturbative - free ), and that exploding and vanishing gradient descent converge to stable solutions respectively. The authors also provide empirical evidence that exploding / vanishing gradients are more likely to be unstable than SGD or RNN solutions."
SP:d3a4300e21ca215334f256f0467a428470548fe4,"This paper considers the following problem : given a set of $ \mathcal{x}$ points $ z$ and a $ p(x)$. The goal is to minimize the cost of sending $ p$ to the source $ z$. The source $ x$ is assumed to be a pre - defined function $ \alpha$ that specifies the amount of $ p \infty$ that needs to be sent in order for the source to save $ p$.   The paper shows that there exists a solution to this problem in the form of a learning - augmented online algorithm with $ p\infty$. In particular, it is shown that $ p(\alpha)$ can be expressed as a weighted sum of the following terms : $ \sum_{\alpha } \log p(z)$ where $ \text{xi}$ is the ratio of the expected value of the source for $ p_i$ and $ \sqrt{\alpha}$ the value of $ z_i$. The paper also gives bounds on the number of iterations required to solve the problem in terms of the length of the idle periods $ \pi$ and the amount $ \eta$ required to ensure that the source satisfies the above condition.    Finally, the paper also provides bounds on $ \beta$ for the power - saving problem ( which is a special case of the above problem )."
SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"This paper studies the problem of transferability between source and source neural networks in the setting of multi - source and few - shot classification tasks. The paper provides a theoretical analysis of the transferability problem in terms of two factors : ( 1 ) the similarity of the source - source complexity and ( 2 ) the number of data points required to transfer the knowledge. To this end, the paper proposes a new expression for the cross - entropy complexity of source / source transfer learning problems, which is based on the similarity between the set of source data points and a set of few data points that are used to train the transfer learning algorithm. Experiments are conducted on the standard classification tasks and a few new classification tasks, which show that the proposed approach achieves better transferability than existing approaches."
SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"This paper proposes a visual recognition - based model for visual search tasks. The model is trained on top of a pre - trained human - based visual search network. The network consists of two components : a top - down cue - dependent visual search model and a perceptual model. The visual model is designed to capture the search asymmetry, while the perceptual model is used to model the human behavior. Experiments show that the visual model outperforms the other two models on a variety of visual search datasets."
SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"This paper provides a theoretical analysis of the upper bound of the cross - entropy loss landscape obtained by training a set of models on a finite set of examples. The main contributions of the paper are as follows :    1. Establa un criterio de la quantidad de la traversazione de un probabilit de la proprieur entre un set de models de probabilits de la dimension dellentes examples ;   2. Concretely, given a given class of examples, i.e., a given set of parameters ( e.g., the dimension of the set of actions ) and a given distribution over the examples, the paper proposes a method ( IBP ) to compute the upper and lower bounds of the loss landscape in terms of the number of actions in the training set.   3. Provably, the bounds obtained by IBP are shown to be in a good correspondence with those obtained by other relaxation - based methods."
SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"This paper studies the following problem : given a linear regression problem $ \mathbb R^n$ and a set $ \ell_t$ of observations $ \infty$, what is the best way to estimate the expected value of $ \text{ell}$ under the assumption that $ \tilde{ell } \log n$ is a sub - gaussian noise?   This paper answers this question by showing that the best possible estimate of $ $ \theta$ is bounded by a factor $ \sqrt{\sqrt{ell}}$.   The main contributions of this paper are two - fold :   1.   Proposing a theoretical analysis of this problem, and   ii. �providing a theoretical proof of the upper - boundedness of the estimated value under the above assumption."
SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"This paper studies the following two variants of gradient descent for structured convex - concave and structured nonconvex - nonconcave problems. The first variant is based on line - search. The second variant uses gradient norm averaging. The authors prove that the first variant converges faster than the second variant.    The authors also prove that both variants converge to the same gradient norm under the same assumptions. The main contributions of the paper are as follows :   - The first version of the gradient descent method ( ELTA ) is shown to converge to a gradient norm of $ \sqrt{O(1 / k}$, where $ k$ is the number of subproblems in the problem. - The authors show that the ELTA method converges to $ \sigma^2 $ when $ k = 1 / k$ and $ k=2 / k$. - The second version of ELTA ( EG ) relies on backtracking the gradient of the first iteration of the original problem to obtain a $ \tilde{O}$-norm of the second iteration. This method is proved to converge faster than both the first and the second version."
SP:4e38973033de24fc183c6112e1146f8eef0ddaea,This paper studies the problem of uniformity testing of Bernoulli distributions. The main contributions are :   1. This paper gives a uniform complexity upper bound for the single parameter Mallows class in the case of private data.   2. This is extended to the case where the data comes from a non - private distribution. The upper bound is shown to be upper bounded by a factor of $ \sqrt{\sqrt{d}$ where $ \tilde{D}$ is the dimension of the data set and $ \text{text{data } \in \mathbb{R}$. 3. This extends the result to the setting where data come from a public domain and $ d$ is a set of $ d$.    4. This gives a lower bound on the total variation complexity of the public data in the same way as the previous work [ 1 ] and [ 2 ]. The difference between the two is that the latter works in the private setting and the former in the public setting.
SP:99a835191a3ba8372e391b6d3316e9b68e543295,"This paper studies score - based algorithms for learning graphical models with sparse structure. The main contributions are two - fold :    1. This paper gives a score based score based algorithm for learning DAGs that satisfies the following conditions : $ \ell_t \in \mathcal{O}(\sqrt{T})$, where $ \infty$ is the number of graphs in the set $ T$ and $ \text{T}$ is a constant factor in $ T$.   2. The paper shows that this score based gradient algorithm can be used as a metric for evaluating the complexity of the learned graphs."
SP:b60989706296b963b6671c01f22384978a334be1,This paper proposes a neural network - based adversarial robustness training framework based on backbone CNNs. The proposed framework has the following components :    1. Defines the network as a set of neural networks that are robust to adversarial attacks.   2. Demonstrates that the network is robust to attacks with $ \ell_t$-\ell_0 $ attacks. 3. Introduces a metric that quantifies the difference between the number of iterations required for the network to reach a given state under the attack. 4. Extensive experiments are conducted to demonstrate the effectiveness of the proposed framework.
SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"This paper studies the problem of reward - based reinforcement learning ( RL ), where the goal is to learn a policy that maximizes the transition probability between two MDPs ( i.e., the probability that the policy achieves the goal in the first MDP and the one that achieves it in the second MDP ). It is well - known that this problem is hard to solve. This paper aims to address this problem by providing a lower bound on the sample complexity of a reward - free RL algorithm that achieves the optimal transition probability in each MDP.   The main contributions of this paper are as follows :   1. Provably lower sample complexity bounds on the parameterization of a model - based RL algorithm. 2. A theoretical proof of the lower bound. 3. A practical algorithm based on this framework."
SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"This paper proposes a new data compression scheme based on the time - varying seasonal matrix factorization approach. The key idea is to encode the data stream of events as a series of sequences of events with different timesteps, and then use a factorization scheme similar to the one proposed in the previous work ( Xie et al., 2020 ). The main difference is that instead of using a constant factorization, the paper uses a factorized version of the factorization.    The paper provides a theoretical analysis of the proposed method and shows that it converges faster than the previous one."
SP:e4bb07033001be4d04695ef058f426d49fe440be,"This paper presents an approach to solve the assignment problem of matching two input vectors to a set of non - linear linear assignment problems. The goal is to find a solution that satisfies the following assumptions :    ( 1 )   1.   match the input vectors $ \mathcal{x}$ with $ \text{x } \infty$, ( 2. ) provide a sufficient condition for the assignment to be non - trivial and ( 3. ) ensure that the solution to the problem is not too different from the one obtained by solving the input problem in the original setting   [ 1 ]. In this paper, the authors propose a new approach to this problem based on the knowledge - based 7 - factorization approach. The key idea is to use the fact that the input vector $ x$ can be represented as a linear combination of two vectors $ y$ and $ z$, and then to obtain an upper bound on the number of elements $ z$. The authors show that the proposed approach can solve the problem in two ways : ( 1. ) for the linear case, the upper bound can be phrased as a convex combination of $ \cal \sqrt{\sqrt{z}$, where $ z}$ is a vector with $ z_1 $ and $ y_2 $ is a matrix of size $ x_1$.   The authors also show that this approach can be used to solve a subset of the above problem in a differentiable way."
SP:8a559e21d45661eef427b310e5fe8488d5749137,"This paper proposes a point - cloud - based adversarial robustness framework for point clouds. The proposed framework is based on PointNet - based ( PointNet ) point clouds and DGCNN point clouds, which are two variants of point clouds that have been proposed recently in the literature. The main contribution of this paper is the introduction of a pre - training strategy to ensure that the point clouds are not vulnerable to adversarial attacks. The paper also provides a theoretical analysis to prove the correctness of the proposed framework. Experiments show that the proposed approach outperforms the state - of - the - art point clouds in terms of criticality."
SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"This paper studies the regret bounds of the Frank - Wolfe algorithm, which is a variant of the well - known metric metric descent algorithm. The authors prove that the metric version of this algorithm has near - optimal regret bounds in the continuous and finite - horizon perspectives, and that it converges to the optimal metric in the finite horizon perspective. They also provide a theoretical analysis of the convergence rate of this metric."
SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"This paper studies the problem of estimating the likelihood of the parameters of exponential family distributions with respect to a finite set of samples. It is well - known that this problem is computationally intractable and hence, the authors propose to approximate the parameters by minimizing the minimization of a loss function defined over the set of parameters of the exponential family.    The main contribution of this paper is the following :   - Given a set of $ \mathcal{x}$-normal $ \alpha$-parameters $ \gamma$ and a $ \text{text{x } \log(k|\alpha)$ -parametrization of the log - likelihood of $ x$, they show that the probability that $ \eta$ is greater than $ \sqrt{\alpha}$ can be approximated with high asymptotic efficiency under the following assumptions : - $ \ell_1 \in \mathbb{R}^{n_{\alpha } \leq n_{\text{k } \infty}$, where $ n_i$ is the number of samples in the set $ \lambda$ and $ k$ is a polynomial in $ k$. - $ k \in\mathbb { \alpha}^n \in $ \log{k}$ where $ k\in$ denotes the size of the polynomials of $ k $ and $ 1 $ is a constant random variable $ k. - $ n \in$-\infty$-regularization of the loss function $ \theta$ is defined over $ \tau$- $ k -parameter $ \nabla_{\theta^n\sqrt{text{\text{n}(\tau})$ -$ k -\text{\tau}$ -\tau $ k-\log{n\in\alpha}$.   They show that this minimization problem is equivalent to approximating $ \sum_{n\log n\in}$ with low - rank constraint under $ \beta$. -   This paper extends the analysis to the case when $ k=1 $ -parameters are assumed to belong to the sparse exponential family $ \geq n_\alpha$. - Theorem 1.1 provides a lower bound on the probability of the parameter of $ n$ under the assumption that $ k^n$ is not larger than $ n(\gamma)$. - theorem 2.2 provides an upper bound on $ k_n$-normalization of $ p(k^n)$ under this assumption. - theorem 3.1 gives bounds on the likelihood - normality of the corresponding log - probability distribution $ \sigma(k / n)$."
SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"This paper proposes a differentiable renderer framework for differentiating between different types of lighting and transport in the context of real - world applications. The main contributions are as follows :    1. Introducing a new renderer for differentiable lighting and lighting - based renderers.   2. Developing a simple and expressive shading model for different kinds of lighting, which is shown to be more expressive than previous approaches. 3. Providing a set of empirical results showing that the proposed renderer is more consistent than existing approaches."
SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"This paper proposes a sampling - based localization operation that satisfies the following constraints :    1. Differentiable sampling distribution of distributions of distributions ; 2. Differentizability of the distributions ; and 3. Sec. 4. Qualitative error of the localization error.   The main contributions of the paper are as follows : - Introducing the sampling - argmax operation, which satisfies the above three constraints ; - Providing a distribution distribution that is differentiable and satisfies the Sec. 5. Providing an error metric that quantifies the difference in the quality of the samples sampled by the localization operation."
SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"This paper proposes a new view augmentation framework for graph contrastive learning ( GCL ), which aims to provide easy - to - difficult contrastive views for a set of graph - level learning problems. The proposed framework is based on the idea of directed graph structure information ( DIT ) augmentation, where the goal is to provide the learner with a view that is easy to understand and difficult to understand for a given set of problems. To achieve this goal, the authors propose two views augmentation schemes : easy - easy and hard - difficult views. The easy view scheme first augments the graph structure of a given problem with a simple perturbation, and the difficult view is obtained by passing the perturbed graph to the easy view. The hard view, on the other hand, is constructed by augmenting the graph with a more complex graph representation, which is then passed to the hard view. Experiments show that the proposed approach achieves state - of - the - art performance on a number of benchmark datasets."
SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"This paper presents a multi - environment benchmarking framework that aims to provide agents with the opportunity to interact with rich natural language representations of visual world scenarios. The proposed framework consists of three components :    ( 1 ) grounding - centric attention, ( 2 ) architectural grounding and ( 3 ) state - tracking.   The grounding component consists of two components. The first component is designed to provide the agent with access to a rich set of natural - language representations ( e.g., text, images ), while the second component provides a grounded set of visual - world representations. In the experiments, the grounding component is used to train a set of grounded agents that are able to navigate through the environments provided by the third component. The results show that the grounded component is more effective than the un - grounded component."
SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"This paper proposes a new vector - vision - transformer ( V - MoE ) model that scales according to $ \ell_sqrt{x } \infty$ $ \mathcal{x}$, where $ \alpha$ and $ \tilde$ are the number of tokens and $ z$ is the dimension of the domain, respectively.    The authors claim that this is the first time such a metric has been used in the context of V - Vision - Transformer ( ViT ) models, and claim that it can be used as a metric to compare the performance of different models ( e.g., V - COCO, CO2 ) in terms of their ability to adapt to changes in the scale of the data domain. To this end, the authors propose a new metric that measures the ratio of the total amount of tokens required to adapt a given model to the change in the data scale ( $ \text{xt}$ ), and show that this metric is better than the one used in prior work ( $ tilde$ )."
SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,This paper provides a theoretical analysis of the convergence of the minimizer of hidden - layer neural networks with different number of hidden layers. The main contributions are :    - Theoretical analysis of convergence of different minimizers of hidden layer neural nets with different numbers of layers. - Theorems showing that the number of layers does not matter for the convergence. - A convergence analysis of different types of minimizers.
SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"This paper studies the problem of learning to mitigate risk in multi - armed continuous - action online decision making, where the goal is to minimize the cumulative risk incurred by all actions taken by the agent. The paper provides a theoretical analysis of the covariance structure of the actions taken in the agent's response strategy, and derives lower bounds on the total amount of information that needs to be transmitted to the agent in order for the agent to mitigate the risk. In addition, the paper proposes a practical algorithm for this problem, named CMCB - FB, which is empirically shown to outperform the state - of - the - art baselines."
SP:472a90bb175b0286765c5a47b040e1a58f594a05,"This paper studies the problem of minimizing factorizations of nonnegative matrix rank inequalities under the following assumptions :   ( 1 ) the matrix factorization problem can be reformulated as a convex optimization problem, ( 2 ) the number of variables in the matrix is polynomial in the dimension $ \mathcal{x}$, and ( 3 ) the factorization matrix $ \log(x|x)$ can be represented as a linear combination of $ \sqrt{\sqrt{x } \infty}$ and $ \text{max}$ factors.    The authors propose a new minimization strategy based on minimizing the rank minimization problem ( RMU ). They show that RMU converges faster than the previous RMU method ( MRU ), and they also propose a variant of RMU that uses a quadratic minimization scheme."
SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"This paper proposes two approaches to provide domain - invariant and domain - specific representation of the state - of - the - art techniques learned from a training dataset. The first approach is a generalization - based approach where the goal is to obtain a representation that is invariant to both the source and target domains. The second one is a specific approach that aims to learn a specific representation for each domain. The authors claim that both approaches have their advantages and disadvantages. The main contributions of the paper are as follows :    - Domain - specific representations that are not merely domain invariant, but also domain specific. - Domain invariant representations that do not rely on the source or target domains, but rather only on the domain specific representations."
SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"This paper presents a method to generate datasets with a single step - likelihood - based generative classifier that can be used to sample from a large set of unlabeled datasets in a single pass.    The method is based on the idea that a generative model can be represented as a mixture of two generative models, one generative and one diffusion model, and that the generative part of the model is the one that generates the image. The diffusion part is the part that produces the data that is used to train the classifier. The authors show that the method can be applied to a variety of image synthesis and image editing tasks."
SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"This paper proposes a few - shot learning framework, which is based on the concept of few - shots learning, where a small number of samples are collected per domain, and is used to train a classifier that is able to detect both positive and negative domain representations.    The proposed framework has the following components :   1. Few - shot learner, which consists of two components, one for positive and one for negative domain representation. The first component is designed for positive domain representation, and the second one is designed to train the classifier in the case when the domain representations are positive or negative."
SP:b1f65724926f136979829b7a6c870bc31f38f591,"This paper studies the problem of learning a policy that maximizes the return on a set of samples from a dataset that has been provided to the agent during a training phase. The paper proposes a prioritization strategy that prioritizes the number of samples that the agent should take into account during the training phase in order to ensure that the resulting policy is not too different from the one that is not prioritized during training. To this end, the paper proposes two sampling strategies : ( 1 ) sampling from a distribution that is based on a model - based feedback feedback model, and ( 2 ) sampling based on the distribution of state - action pairs that have been observed in a given environment.   The paper provides empirical results showing that both the sampling strategy and the prioritization strategies outperform the other in terms of return minimization."
SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,"This paper proposes a new entropy - sharing approach to improve the quality of the predictions made by the entropy - projection step - wise gradient - based gradient descent algorithm. The key idea is to share the knowledge about the entropy of a given vector $ z$ with other vectors $ z$.    The main contribution of the paper is to show that if $ z \in \mathcal{T}$ is a constant factor, then $ z\infty$ is guaranteed to be close to $ \log n\log n$, where $ n$ is the number of vectors in the vector space.  "
SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"This paper presents a theoretical analysis of the problem of constructing a convex set of size poly(d log d ) for linear bandits. The main contributions are :    1. This paper shows that there exists a closed - form solution to this problem for any linear bandits $ d$ such that for any $ \ell_0 $ \infty$, there exists an instance $ \tilde{d } such that $ d}$ can be represented as a linear combination of $ d^t$ and $ d_t$, where $ t$ is the dimension of the set $ d$.   2. The authors show that this solution is equivalent to the one obtained by steiner in his famous proof of the convex hull conjecture.   3. They show that the size of a set of linear bandits can be expressed in terms of the ratio of the number of elements in the set of the bandits, and show that it is upper bounded by a factor $ \sqrt{d\log d}$. 4. This gives rise to a new algorithm for constructing linear bandits, which is shown to be asymptotically tight."
SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"This paper presents an AutoML - based validation framework for machine learning ( AutoML ) with the goal of bridging the gap between data - driven and machine learning - driven approaches. The main contribution of this work is a set of experiments to demonstrate that AutoML can be used for a variety of machine learning tasks. The experiments cover a range of data science tasks ( e.g., classification, machine learning ), and are based on the following components :   1.   programming model changes. The authors show that the proposed AutoML framework is able to tolerate a wide range of model changes ( from unsupervised to supervised learning ) without compromising on the quality of the resulting results. This is a major contribution to the community. 2. ) validation of the proposed framework. The author demonstrates that the approach can be applied to a number of existing machine learning approaches, and shows that it can lead to better results than existing approaches."
SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"This paper analyzes the problem - by - problem relationship between the initialization of sparsity vectors and the quality of the learned representations. The paper shows that there exists a correspondence between the number of instances of a given sparsity vector and the fraction of instances for which it is guaranteed to be correct on a given test set. This relationship is used to motivate the paper's claim that there is a "" generalization error metric "" that measures the probability that a given example is correct on the test set for a given set of examples. To support this claim, the paper presents a series of experiments where it is assumed that the training set contains a large number of examples and a small number of training data points. It is shown that this metric can be used as a metric to quantify the error ratio between the amount of instances that are correct and those that are incorrect on a test set, as well as the percentage of examples for which the metric is not correct.    The paper also proposes a way to measure this metric, which is based on the ratio of the total number of tokens used in a training set ( i.e., how many instances are used for each example ) and how many iterations it takes for the metric to converge to a correct answer."
SP:05037e1850003a725a466b64d3e32aa2aed458fb,This paper proposes a multi - view learning method to estimate the components of a Gaussian model from a set of independent data points. The main contribution of the paper is the following :    1. A multi - set canonical correlation analysis method is proposed to identify the independent components of each data point. 2. A maximum likelihood approach is used to evaluate the likelihood of each component. 3. A training algorithm is proposed for the estimation of the components.
SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"This paper presents a reinforcement learning approach for learning agent - human - aware preferences from data. The approach is based on a two - agent collaborative cooking simulator, where the agent and the co - player are assumed to be experts in their own domains. The goal is to train the agent to be able to distinguish between the two domains, and to provide a good value to the other agent. To this end, the paper proposes two approaches. First, the agent learns to be “ aware ” of the other agents ’ preferences. The second approach is to “ generalize ” the agent ’s preferences to both domains. To achieve this goal, the authors propose two approaches : ( 1 ) learn - to - play ( SP ) and ( 2 ) play - play. In SP, the agents learn to be aware of their own preferences, and ( 3 ) learn to imitate the preferences of their co - players.    Experiments are conducted on two domains :   [ 1 ]   a vanilla agent, and [ 2 ] a cheetah - based agent. Experiments were conducted on the standard agent - critic domain ( e.g., the web ), and on two different domains : [ 3 ] and [ 4 ]. Results show that the agent - aware approach performs better than the other approaches."
SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"This paper proposes a multi - agent actor - critic method for solving a set of micromanagement tasks. The method is based on a policy gradient estimator that is a monotonic factorisation of the critic, which is then used as a surrogate for the action - value function. The authors prove convergence of the method under the assumption that the critic is monotonically factored, and that the policy is deterministic in the space of actions and value functions.    The paper also provides a proof that the method converges to the critic in the discrete action space, and a proof of convergence in the continuous action space."
SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"This paper proposes a neural network augmented neural network learning task that aims to learn a key - value mechanism that is robust to changes in the network parameters over time. The proposed approach is based on the notion of plasticity, which is defined as the amount of time it takes for a learner to memorize a set of data points in a data - free environment. The paper shows that the proposed approach can be used to solve a variety of different types of neural networks, and is able to achieve state - of - the - art performance on a number of different tasks."
SP:7ad6da2c63859d64970e9b35326e9ceab48add47,This paper studies the problem of private learning of SGD algorithms for machine learning tasks. The main contributions are as follows :   1. This paper provides a theoretical analysis of the learning error bounds for SGD with private data.   2. It provides a practical gradient descent algorithm for private learning.
SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"This paper presents a video - based implicit function that can be used to reconstruct the dynamics of a 3D object from a set of 3D video data 3DPW and real - world videos.    The proposed framework is based on the following components :   1. shape - invariant 4D implicit function, 2. invariant invariant 3D invariant object dynamics, 3. class - agnostic invariant implicit function."
SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper studies the problem of learning stochastic approximations to the expected value of a given metric ( e.g., $ \mathcal{sqrt{x}$ ) over a distribution of $ \ell_t$-th order $ \infty$, where $ \text{xi}$ and $ \tau$ are the expected values of the metric and $ t$ is a random noise distribution. The paper shows that there are two types of approaches to this problem.    The first approach assumes that the metric $ \alpha$ is smooth, i.e., does not depend on the distribution $ \nabla_{\alpha}$, while the second approach assumes the following : $ \log n\infty$.   For the first approach, the authors assume that $ \lambda$ \alpha is smooth and i.i.d. $ tau$ is not too different from $ \gamma$. The authors show that both approaches converge to the same upper and lower bounds, respectively. The lower bound is tight, and the upper bound is bounded by a factor of \epsilon $ \sqrt{\log n(\log n(t\log n)$ where $ n\to n\theta$ is the number of iterations of the algorithm."
SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"This paper studies the problem of option learning in the setting of multi - level planning problems, where each level of the problem is a set of options that can be discovered by solving a subset of the subproblems of the larger problem. The authors provide sufficient conditions for the existence of an optimal solution for the subproblem in terms of the number of options and the length of the optimal solution in the domain. The paper also provides sufficient conditions under which an option - discovery algorithm can be guaranteed to converge to a solution that is at least as good as a solution to a subproblem that has infinitely many options. Finally, the paper provides a proof that an option discovery algorithm will converge to solutions that are better than solutions that don't exist."
SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"This paper proposes a multi - task learning framework for the Visual Transformer (VT ) framework. The main contribution of the paper is the design of a set of tokens embedding grids for each task in the training set, which is then used to localize the data from each task to the target domain. The authors also propose a way to generate a subset of tokens per task that can be used for supervised learning.    The paper presents a series of experiments to validate the effectiveness of the proposed framework. First, the authors show that the proposed model can localize data from a small set of tasks to a target domain with a high degree of accuracy. Next, the paper shows that the model is able to localise data from different tasks to the same datapoint with a low level of error. The paper also shows that this model is robust to the number of supervised tasks and the amount of data used."
SP:0132ef17585e293b23e9dc45189c0989d829b52a,"This paper proposes a data - free, label - free and batch - free hyperbolic space - based representation of the data in Procrustes analysis ( HPA ) framework. The proposed model is based on the first and second Riemannian moments of a hyperboloid, and is free from labels. The paper also proposes a batch - based data fusion strategy, which is used to train the model. Experiments show that the proposed model can be used to improve the performance of HPA."
SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"This paper proposes a privacy - preserving public - use / private - data - generating algorithm that satisfies the following queries :    * * Sec. * * $ \ell_sqrt{quant } \in \mathbb{R}$ \ell_{\text{quant}$ + Sec.   $ \infty$ $ \text{Quant}$ - Sec. $ \tilde{Quant } $ \nabla n\infty$.   This paper is motivated by the fact that most of the existing publicly - available datasets are noisy and contain some amount of error, which can degrade the quality of the answers to the queries. To mitigate this issue, the paper proposes to use a combination of two approaches : ( 1 ) private - use data - preserving and ( 2 ) concentrated differential privacy preserving. The first approach is based on the assumption that the source of the query is a subset of the source data that is not available to the public. The second approach uses the data that has been produced by the source ( e.g., the source - target ) and the output of the ( target ) algorithm to protect the privacy of the produced data. The paper provides a theoretical analysis of the trade - off between the two approaches and empirically shows that both approaches lead to similar results."
SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"This paper proposes a reinforcement learning framework that learns from a tree - structured set of long - horizon tasks. The framework consists of two components : ( 1 ) a set of sub - tasks, and ( 2 ) a reward / guidance model that provides feedback on the success of each sub - task in the set. The first component is designed to guide the RL agent in selecting the best set of tasks to tackle in the next stage of the RL training. The second component is used to provide feedback to the planner.    The main contributions of the paper are as follows :   1 ) Introducing a curriculum of long horizon tasks that is structured according to the length of the horizon and the number of steps required to complete the task ; 2 ) Learning to plan “ hard ” and “ easy ” to - hard curriculum that allows the planner to plan to tackle new tasks in a safe way ; 3 ) Using the learned curriculum as feedback, the planner learns to minimize the total number of actions needed to solve the given new tasks ; 4 ) Using feedback and guidance to ensure that the planner ’s actions are “ safe ”."
SP:9911693a04a300b5a93634fb0267ef83e5489d77,"This paper aims to provide trust - based explanations for black box classification problems. To this end, it proposes a framework for generating black box explanations that can be used in conjunction with the decision surface of the black box classifier. The main contribution of the paper is to provide sufficient conditions under which the blackbox classifier can be trust - proven to be trustworthy. In particular, this is done by providing sufficient conditions on the hyperparameters of the classifier and the set of samples used to train it."
SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"This paper considers the following classification problem : given a set of distributions $ \text{x}$ and a set $ \tilde{x } \in \mathbb{R}$, what is the probability that $ x}$ is greater than $ y$? This question is posed under the following assumptions : $ y \infty$ is not too differentiable, and $ z\infty$.    The paper shows that under these assumptions, there exists a distribution $ z$ such that $ z \in { \mathbf{R } \times \text{\text{X}$ can be expressed as a sum of $ z^{-1/\text{Y}$ where $ z}$ denotes the distribution over $ y$. The paper also shows that the corresponding distribution is also a convex function $ \sqrt{Z}$ that can be written as a product of $ \alpha$ and $ \gamma$, and shows that there exist distributions $ z_{-1}$ satisfying $ z_i \in\mathbb {R } $ such that for any $ y}$ there exist $ z^2 $ distributions $ y_i$ that are convex ( i.e., $ z+\gamma^2$ with respect to $ y^3 $."
SP:cbccb65457564992d534504c0d060da44cafce8c,"This paper presents a theoretical analysis of the gradient descent phenomenon in the context of neural networks. The authors show that the standard regularization method ( GS ) does not provide a satisfactory explanation of the phenomenon. Instead, the authors propose a new metric based on entropy loss, which is shown to provide a better explanation.    * * Key words * * : gradient descent ; entropy loss ; neural networks ; regularization."
SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"This paper proposes a reinforcement learning - based agent - teaming framework that aims to improve the performance of the agent and the other agent in a cooperative multi - agent multi - task setting. The framework consists of two components : ( 1 ) a teacher - based RL agent that learns to communicate with other agents and ( 2 ) a “ teacher - student ” agent that is trained to learn to play a cooperative game with a teacher and a student agent. The first component is based on the concept of “ safe communication ”, which aims to ensure that the teacher and the student agent have a good understanding of each other and their goals. The second component focuses on “ trust ”. This is defined as the ability of the teacher to communicate to the student in a safe way without compromising on the goals of the student.    The paper presents a series of experiments that demonstrate the effectiveness of the proposed framework. In particular, the paper considers the following scenarios :   1. A single agent that does n’t communicate with any other agent. 2. A group of agents that communicate with each other. 3. A teacher agent that has a good relationship with the student and learns to play the game."
SP:2a05e333fc1a14057515ef3addde9a40152373db,"This paper presents a supervised learning approach to generate visual hints and answer - based question - aware neural questions that can be used in conjunction with a data - driven supervised learning framework. The framework consists of two components : ( 1 ) visual question generation ( VQG ) and ( 2 ) question - answer generation ( GAN ). Visual question generation is based on visual question - based similarity matching, where the goal is to identify salient visual regions of interest that are likely to lead to a good answer to the question. GAN is used to generate both visual and answer hints, which are then used to evaluate the quality of the answers generated by GANs."
SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"This paper proposes a three - stage gradient descent framework for data weighting ( GDW ) with two stages :    ( i )   a two - stage bi - level weighting scheme where the first stage is designed to generate data with high - level weights, ( ii ) a step - wise gradient descent scheme to generate low - level data with low - class weights, and ( iii ) a gradient descent step to generate high - class data with a fixed - level gradient. The paper also proposes a way to handle the label noise and class imbalance in GDW. Experiments are conducted on two datasets ( CIFAR10 and Tiny - ImageNet ) to show the effectiveness of the proposed framework."
SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"This paper studies the problem of learning a language grounding task that allows an agent to infer a set of observations in a temporal context without relying on held - out sentences.    The main contribution of this paper is the introduction of the concept of * * temporal references * *, a notion of temporal relations that can be used as a metric for quantifying the agent ’s ability to infer information about the temporal context ( e.g., whether the agent is familiar with the past, present, and future history of the world ). The paper shows that this concept can be leveraged to train a language - guided embodied agent that learns to infer such a metric. The main contributions of this work are two - fold :   ( 1 )   a theoretical analysis of the problem and ( 2 ) a proof - of - concept experiment that demonstrates the ability of the proposed concept to be used for grounding agents that do n’t rely on hand - crafted examples."
SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"This paper presents a multi - object tracking and segmentation framework for cross - attention attention network ( PCAN ). The framework consists of two components : ( 1 ) a foreground and background - level tracking framework, and ( 2 ) an instance tracking framework. The foreground - level trackers and the instance - level segmentation frameworks are built on top of top - k datasets ( e.g., VIS and BDD100 K datasets ) from the previous works. The proposed framework has the following contributions :    1 ) It provides a two - stage framework for multiple object tracking, segmentation and background tracking. The first stage consists of a single - stage tracker and a two stage segmentation network. The second stage is composed of two stage trackers, one for instance tracking and one for segmentation."
SP:1175ad16382b349ab1a39895150172d266abe571,"This paper studies the initial value problem of gradient flow trajectories in neural networks. The authors show that there exists a solution to this problem in the case of linear neural networks, and provide an upper bound on the number of iterations required to reach the optimal value."
SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"This paper considers the following problem : given a history of actions taken by an actor who has been stopped by a non - armed adversary, the goal is to learn a metric that quantifies the probability that the actor ’s actions “ delivered ” a “ good ” value to the adversary in the future. The paper proposes two approaches to this problem. First, the actor is allowed to take actions that are delivered by the adversary for a fixed period of time ( called “ history ” in the paper ), and then the actor has to decide whether to continue taking actions or not ( “ stopping ” is defined as taking actions that “ deliver a good value ” ). In the second approach, the attacker has access to a finite set of historical actions that can be used to “ DELAY ” the impact of future actions.    The paper provides theoretical results showing that both approaches converge to the same metric when the action history is sufficiently long ( e.g., $ t$)-delayed, and that the metric also converges to the regret lower bound when the actions history is short ( $ n$ )."
SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"This paper presents a solution to the problem of video instance segmentation in the context of space - time attention. To this end, the paper proposes a new video segmentation network ( VIS 2019 ), which is built on top of the existing video - to - frame segmentation pipeline ( IFC ). The main contribution of this work is to provide a near - end solution to infer the knowledge about the video context from the video clips. Specifically, the VIS dataset is divided into two parts, i.e., the first part is a set of clips, and the second part is the context pipeline. The framework is trained with the following components :    1.   A set of video instances. These instances are first segmented into frames, and then they are passed through a pipeline to learn a knowledge model of the context. This pipeline is then used to train the inference network. The second part of the pipeline is to learn the context for the next video instance. This is done by using the learned segmentation model and the knowledge from the previous video instance to frame model. The inference network is trained using the same framework as in IFC, but with the additional assumption that the video is not too different from the first one. The idea is that the context model should be able to capture both the temporal and spatial aspects of the video, which can help improve the video understanding."
SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"This paper proposes a new embedding method for the context - aware machine learning problem, which is based on the concept of   context - biased walks “. ”   This is an extension of the previous work “ contextual - biased walk ” [ 1 ], where the goal is to learn a representation of the world in terms of a set of context nodes.    The main contribution of this work is to provide a sampler for this embedding, which can be used in a variety of machine learning applications."
SP:851eac96135b577a5014166edcb43db6a190cf4b,"This paper proposes a two - step differential privacy framework for estimating non - linear functionals of discrete distributions. The first step consists of two steps :   ( 1 ) constructing a discrete distribution $ \mathcal{L}$ and estimating its risk $ \sqrt{P}$. The second step consists in generating a data point $ p(x, z ) $ \log p(y|x)$, where $ z$ is the probability that $ y$ is greater than $ z$. The authors assume that the data points $ x$ are available in the form $ z_i$ and that $ z_{i}$ is a linear function of $ y$.    The authors provide a theoretical analysis of the two steps of the proposed framework. The main contributions are : ( i ) providing sufficient conditions for the existence of $ \cal p(z_{i } \log z}$, and ( ii ) providing a sufficient condition for the privacy of the data point generated by the two step framework."
SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"This paper considers the following problem : given a set of data points $ \mathcal{x}$ and a classification error $ \alpha$, what is the best way to send them to the server $ \text{text{x }? }?    The paper proposes two approaches : ( 1 ) to send $ \cal \cal {x } \log n_{t } \infty$, and ( 2 ) to distill $ \sqrt{\cal { x}$, which is a convex combination of $ \nabla_{t}$. The first approach is based on minimizing a weighted hinge loss $ \ell_0$. The second approach uses a metric $ \tilde{O}$ that measures the amount of information that needs to be transmitted between the server and the source graph in order for the server to send the data to the source.   In the first approach, the server is assumed to have access to all the data points in the source and target graphs, and the goal is to ensure that the server does not send any data points that are not relevant for the source ( e.g., those that are irrelevant for the target ). The paper shows that this approach is suboptimal in the case where the source / target graphs are not separable, and gives a surrogate regret that bounds the number of such mistakes that can be sent by the server in the separable case. In the second approach, instead of using the same metric as in the first step, a different metric is used for each metric, which allows for a more efficient distillation of the information. The authors show that the surrogate regret is better than the one based on the metric used in the previous approach."
SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"This paper gives a lower bound on the clustering factorization of the cut - threshold objective : given a graph $ G$ and a finite set $ k$, the goal is to find a clustering vector $ z$ such that $ z(k)$ is at least as dense as $ k(z)$. In particular, given $ g$ and $ K$, this is the first lower bound to be given on $ z$.    The main contribution of this paper is the following :   1.   Given $ \ell_1 $ and $ k=\ell_2 $, the paper shows that there is a tree $ \tilde{t } \in \mathbb{R}^{n_{k } } \log n_{k}$ with probability at least $ p(\ell_{k})$, where $ p(k ) \log k$ is the number of vertices of $ \text{t}^n$, and $ t$ is a subspace of $ k$. 2."
SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"This paper presents a universal dependency parsing framework for natural language processing ( NLP ) tasks. The framework is built on top of a pre - trained language model ( PrLM ). The model consists of two components : a structure encoder and a structure parsing encoder. The encoder is trained on a corpus of structured text ( e.g., wikipedia ), and the structure parsing is done on the datacenter of the language model.   The model is evaluated on a set of NLP tasks."
SP:94f4b65214a648cbc84f13beba45a825e2e9901a,This paper proposes a new method to improve the quality of the solutions of vehicle routing problems ( VRPs ). The proposed method is based on the dual - aspect collaborativeative transformer ( DACT ) model. The main contributions are two - fold :   1.    a new positional encoding method ( PE ) method to encode the positional encoding ( CPE ) of solutions of VRPs ; and   an effective learning strategy ( CL ) to optimize the sample efficiency of the learned solutions. The PE method is applied to two specific VRPs : the Travelling salesman problem ( TSP ) and the vehicle routing problem ( CVRP ). Experiments are conducted to evaluate the effectiveness of the PE method and the CL method.
SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"This paper considers the following data - driven classification problem : given a generative model $ \mathcal{T}$ and a set of data $ \text{data}$, what is the best way to train a classifier $ \tilde{x}$ on it?   The paper proposes two approaches to this problem :    1.   \theorem 1. Theorem 2. Theorem 3. A sufficient condition on the data distribution $ x$ under which the classifier D(pkp|x)$ is guaranteed to fail is derived. Theorems 3 and 4. A proof of the theorem is provided in the form of an upper bound on the ratio of the number of data points required to satisfy the above two objectives. The paper also provides an analysis of the trade - off between the quality of the classifiers and the amount of data required for the performance improvement."
SP:2896679f0472522bc3334178cd7574494cf12b7b,This paper proposes a new initialization scheme for neural networks based on gradient - based generative models. The main contribution of this paper is to provide a theoretical analysis of the stability of the proposed framework and to provide theoretical guarantees for the learning rate of the new framework. The paper also provides theoretical guarantees on the number of iterations required for the framework to reach its theoretical stability.
SP:f69731403592fa5bdd4ca327708582d615aa131c,"This paper studies the problem of learning metric - invariant cohort - specific trajectories for continuous - action continuous - time and space models. The main contribution of the paper is a theoretical analysis of the different metrics that can be used to guarantee the timestep and space - invariance of the cohort model.    The main contributions are as follows :   - Introducing a new metric learning framework, based on the kernel Hilbert space, which is shown to be invariant to both metric - metric and metric - space variability. - Developing a theoretical framework that guarantees both metric and space invariance to metric - parameter variations, and that is able to adapt to different cohort models. - Providing theoretical guarantees for the time - space and space variability of cohort models, which are then used to design a new cohort model that is empirically validated on a set of data."
SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"This paper presents a series of experiments on improving the accuracies of the procedures provided by a neural network ( NN ) in terms of the number of iterations and the amount of data used to train the NN. The experiments cover a range of NN architectures, including CNNs, procedural units ( PUs ), and feed - forward networks ( FUs ).   The paper presents results on the following :    1. Providing a set of 100 benchmarks that cover a wide range of nn architectures ( from CNNs to FUs and from PUs to FoNs ), the paper shows that ( i ) NNs provide better accuracies than PUs, and ( ii ) training a PU with fewer iterations leads to more accurate procedures than a PU with more iterations."
SP:d240173080cd3647dbaa5173a6422396f226775b,"This paper proposes a new method to enforce certain symmetries of physical law by means of invariant tensor products and functions. The key idea is to use the notion of "" parity "", which is defined as the difference between the number of elements in a set of functions satisfying a certain set of constraints.    The main contribution of this paper is to show that there is a natural way to enforce this notion of parity under the following assumptions :   ( 1 ) there exists a natural number of functions $ \ell_t$ such that any set of $ t$ elements satisfying $ t \infty$ can be represented as a product $ t$. ( 2 ) there exist natural numbers $ \tilde{t } $ such that $ t\infty$. ( 3 ) for any $ t $ there exist a set $ \mathbb{R}$ $ t such that for any set $ T$, there exist $ t^t$ elements $ t + t$ satisfying the following constraints : $ t_t(t)$ = t + \ell_{t } + t + T + t$.   This is phrased as follows : $ \text{t}$ is the ratio of the ratio $ t+t$ between $ t_{t+1}$ and $ t-1$. ( 4 ) for $ t=0 $, there exists an equivalence between the set $ t(t+0)$ and the set   of functions defined by $ t - t$ under the above above assumption ( 5 )."
SP:72c0f47566904deb27d8157da30807ec1d6b5685,"This paper presents a theoretical analysis of the impact of different distance - based and gradient reweighting terms on the performance of object detection systems. The main contributions are as follows :    1. A theoretical analysis is presented that shows that a given set of data points ( i.e., a set of objects ) can be used to derive a series of loss functions that preserve / reweight certain properties of the data points.   2. A set of experiments is conducted to evaluate the theoretical results. The experiments are divided into two parts. The first part is based on Bounding Box ( B BOX ) regression and the second one is on object detection. Experiments are carried out on different object detection datasets ( e.g., CIFAR-10, Cifar-100 ) and show that the proposed theoretical results are in good agreement with the experimental results."
SP:397125177d7007316d67194ec00d5dc57b44ac79,"This paper proposes a framework for entropy inverse reinforcement learning ( ERL ) based on the concept of entropy in action spaces and state spaces. In particular, the authors consider a low dimensional decision process ( MDP ) setting, where the goal is to learn a policy that maximizes the entropy of the state space and action spaces under the action space assumption. The authors propose two approaches to this problem. The first approach is based on minimizing a convex number of variables, while the second one uses a non - convex set of variables.    The authors provide a theoretical analysis of both approaches and experiments to show the effectiveness of the entropy - based approach."
SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"This paper studies the post - processing smoothing problem in the setting of large - scale neural network ( NLP ) models. The paper provides a theoretical analysis of the problem in terms of the metric metric metric $ \sqrt{\sqrt{R}$, which is defined as $ \tilde{R } \in \mathbb{R}}$ where $ \infty$ is the scale of the NLP model and $ \text{r}$ is a metric that characterizes the amount of information that needs to be processed in order for the model to satisfy the metric.    The paper shows that the metric can be phrased as a convex combination of two terms : $ \sigma \in R$ and   $ \log(R)$.   This paper provides theoretical results showing that $ \nabla_{\sigma\in R}$ can be expressed as a linear combination of the following terms :   ( 1 ) $ \rho_{\text{R}\in R}}$, ( 2 )   \log(\sqrt { R}^n$, where $ n(r ) is the number of data points in the nLP model, ( 3 ) $ N(r)$ is an integer that measures the difference between the average number of bits in the input and the output of the model, and ( 4 ) $ R \in N$ is $ \theta$ a metric metric that measures how much information has to be passed through the model before the model can be sent to the server."
SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"This paper proposes a question - schema - aware dual graph aggregation mechanism for cross - domain text - to - SQL tasks. The proposed method is based on the question - question model and the schema - schema linking mechanism proposed in [ 1 ] and [ 2 ]. The main contribution of this paper is to provide a structure - aware aggregation mechanism that can be used for both datacnostic and query - based tasks. To achieve this goal, the paper proposes two approaches : ( 1 ) global graph linking and ( 2 ) dual - graph clustering. Experiments are conducted on the Text - to SQL benchmark and show that the proposed method can achieve better performance than the previous two approaches."
SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"This paper studies the problem of learning complex discrete - continuous stochastic models from small gradients. To this end, the authors propose two approaches :    1.   discrete - discrete computation graphs   The first approach is based on the notion of discrete components. The second approach uses the concept of continuous computation graphs. The authors prove that the two approaches are equivalent if and only if they satisfy the following conditions : ( 1 ) the discrete components have the same distribution over the set of metric variables, ( 2 ) the metric variables are not too different from each other, and ( 3 ) the metrics do not contain too many softmax tricks. In the experiments, the proposed approaches are shown to outperform existing approaches."
SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,This paper proposes a new approximate inference procedure for Bayesian neural networks ( BNNs ) based on the Hamiltonian Monte Carlo ( MC ) approach. The proposed procedure is based on a two - stage training procedure. The first stage is to train a neural network with a fixed number of parameters and a fixed batch size. The second stage consists of training the network on a fixed set of data and performing an approximate inference on the distribution of the parameters.    The main contributions of this paper are as follows :   1. The derivation of the MC approach is presented and the derivations of the approximate inference procedures are presented. 2. The empirical results show that the proposed MAP procedure outperforms previous approximate inference approaches.
SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,This paper presents a few - shot evaluation of the distribution of meta - learning methods / training sets in the setting of few shot classification ( OOD ) and few shot test tasks ( FSL ). The paper also presents an OOD / FSL evaluation setting where the goal is to select a set of data points from a distribution that maximizes the probability that a given set of examples can be classified as OOD or FSL.    The paper provides a brief summary of the existing evaluation settings and the selection criteria for OOD and FSL benchmarks.
SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"This paper addresses the open rule induction problem in the context of language - free text - based rule generation. To this end, the authors propose two approaches : ( 1 ) train a corpus of LM - based rules from scratch, and ( 2 ) augment the training corpus with data from a KB - based approach.    The first approach is based on the fact that the authors claim that the existing approaches do not share the expressive power of the closed - form rules ( e.g., [ 1 ], [ 2 ] ). The second approach builds on top of this fact by adding a commonality term to the derivation of the open - form rule ( [ 3 ] ), which is claimed to be more expressive than the previous approach [ 4 ]. The authors show empirically that the proposed approach ( [ 5 ] ) outperforms the previous approaches in terms of expressive power ( [ 6 ] ) and generalizability ( [ 8 ] ) when compared to a naive approach that does not take into account the commonality terms."
SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,"This paper proposes a multi - agent reinforcement learning ( RL ) algorithm with differentiating agent and action space. The goal is to learn a policy that maximizes the difference between the expected value of the agent and the value of an action under the assumption that the agent is not too different from the action in both space and time. To this end, the authors propose a differentiable agent - action and policy - space separation that allows the agent to learn both the state and action in the same time.    The authors also propose an offline version of the RL algorithm that is based on extrapolation error. The authors show that the proposed RL algorithm can be made to satisfy the constraint Q - learning ( ICQ ) under two assumptions : ( 1 ) the agent needs to be able to distinguish between the two space spaces and ( 2 ) the policy must not exceed a certain threshold in time."
SP:1939b24b68970c33ca16ce238deed257f76d009e,"This paper studies the problem of robustness to perturbations in the data distribution of social networks. Specifically, this paper focuses on the domain - specific data distribution and the covariance matrix of the data perturbation.    The main contributions of this paper are as follows :   ( 1 ) This paper provides a theoretical analysis of the robustness of data distribution with respect to uniform and non - uniform covariance matrices, and derives bounds on the number of attacks that can be used to break the invariance of the network. ( 2 ) A set of experiments are conducted to validate the theoretical results."
SP:417b30930b245667d777e5d90ee80dd41546760e,"This paper studies the problem of learning Tikhonov regularization schemes for self - concordant convex convex minimization of the convex hull problem.    The main contributions are :   1. This paper provides a theoretical analysis of the convergence rate of the proposed regularization scheme and shows that it converges to the optimal rate as the number of iterations goes to infinity. 2. The authors provide a theoretical proof that the proposed scheme converges with kernels of the form $ \ell_sqrt(t\infty)$, where $ t \infty$ is a convex function and $ t$ is an integer. 3. They provide an upper bound on the rate of convergence of their scheme."
SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,This paper presents a theoretical analysis of the complexity - accuracy tradeoff between the linear transformable Butterfly ( DeBut ) layer and the network compressibility tradeoff ( FC ) layer. The authors show that the deBut layer is more complex than the FC layer in terms of the number of input dimensions and the amount of output dimensions. They also show that there is a trade - off between the total amount of input dimension and the total number of output dimension for the two networks.   
SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"This paper proposes a new approach to improve the knowledge reusability of the MiniImageNet benchmark dataset. The key idea is to create a dataset with different amounts of old and new data points, and then train a neural network on the old data and reusably update the weights on the new data. The authors claim that this approach is more data - efficient than baselines that do not have the same amount of old data ( e.g., ImageNet ).    The main contributions of this paper are as follows :   1. Introducing a new dataset that has different levels of accuracy ( i.e., “ split ” and “ reusable ” ). 2. Demonstrating that this dataset can be used to train neural networks on different kinds of data. 3. Providing a set of masks for the new dataset."
SP:722c52467e384058f8fdffa254d0e8db47440a64,"This paper proposes a data - driven approach to solving the primal integral programming ( MIP ) problem. The main idea is to collect heuristics over a set of data points and then use them to generate a heuristic schedule for solving a given primal integral problem. To this end, the authors propose two approaches. The first one is based on data augmentation, where a subset of the data points are collected at each time step of the learning process. The second one is data - free, i.e. does n’t require any prior knowledge about the underlying primal problem.    The authors show that the proposed approach can be used to solve the primal problem in a number of different settings. In particular, the default settings considered in the paper are the following :   1. MIP solvers that don't require prior knowledge of the underlying problem ( e.g., the set of variables used to generate the data ), 2. A solver that only needs to know the number of variables in the data set and the type of primal problem(s ) for which the data is collected."
SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"This paper presents a series of experiments aimed at improving the quality of the feedback provided to the learner in the context of self - driving and reinforcement learning ( RL ). The experiments cover a range of different scenarios, including :    ( 1 ) learning a parametric model from scratch, ( 2 ) learning from a set of data points, ( 3 ) learning on the fly, ( 4 ) and ( 5 ) learning with the help of label - based reinforcement learning.   Experiments are divided into three stages : ( i ) learning the parametrization of the model, ( ii ) fine - tuning the model and ( iii ) training on the data points. Experiments cover a wide range of scenarios, with a particular emphasis on learning from data points that are representative of the current state - of - the - art in the field."
SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"This paper proposes a new representation learning framework, DHT, for hypergraph classification tasks. The key idea is to learn a hypergraph representation from a set of hypergraphs, and then use message - passing techniques to obtain a structured representation of each hypergraph. The authors also propose a new pooling pooling method for the representation learning. Experiments show that the proposed method outperforms existing representation learning methods."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper proposes a minimization - based approach for learning representations of high - dimensional observations from a game environment. The key idea is to maximise the similarity between the representations of data collected by a policy and the state - of - the - art observations collected by the policy. To this end, the paper proposes two objectives : ( 1 ) maximization of similarity between data and state - level representations of the MDPs and ( 2 ) maximizability of the representations to learn from high dimensional observations.    The key contributions of the paper are as follows :   1 ) a minimisation - based objective that maximizes the similarity of data and representations. The paper shows that this objective can be phrased as a convex combination of two objectives, i.e. information maximization ( MI ) and similarity minimization ( RL ). 2 ) an approach to learnable representations that minimizes both objectives."
SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"This paper proposes a new deep feature extractor called Steerable Convolution ( SS - Conv ), which aims to improve the accuracy of the previous Sec 6D pose estimation and size estimation. The proposed approach is based on the semantic analysis of the 3D object features extracted from a set of pose - tracking data collected from a corpus of 3D objects.    The approach consists of three steps : ( 1 ) pose - level 6D tracking, ( 2 ) semantic analysis, and ( 3 ) convolution refinement. The first step is to identify the categories of objects that are most likely to be pose - safe and size - safe. The semantic analysis is done by analyzing the similarity between the features extracted by the two approaches. The second step is a convolution - refinement step that adds a new feature extraction layer to the classification pipeline. The third step is an iterative refinement step which adds new features to both the classification and the convolution pipeline. Experiments show that the proposed approach achieves better performance than previous approaches."
SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"This paper proposes a new approach to improve the quality of the token representations used in the transformer - based classification task. The proposed approach is based on a combination of two approaches : ( 1 ) masking - based token representations and ( 2 ) sparsification - based tokens representations.    The main contributions of the paper are as follows :   1 ) Improving the quality / accuracy of the tokens representations used for the transformer classification task by using different masking strategies. The authors claim that their approach is more complex than the previous one ( Xie et al., 2020 ), which uses the same masking strategy but sparsifies the tokens instead of sparsifying the tokens. They also claim that this approach is better than the prior one in terms of overall quality, but suffers from a trade - off between the quality and the number of tokens sparsified."
SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"This paper proposes a new type of cross - validation method for cross - entropy estimation. The main idea of the method is to use a data - augmentation technique similar to that used in [ 1 ] and [ 2 ]. The key difference is that instead of using a sample size, the proposed approach uses a distribution over a set of data points, where each data point is a subset of a larger data set.    The main contributions of the paper are as follows :   1. The authors propose a new way to estimate the cross entropy of a given data point, which is then used to obtain a cross entropy upper bound on the size of the data set, which can be used as a confidence interval for cross entropy inference. 2. They show that this upper bound can be phrased as an upper bound for the vanishing width of the distribution over the data sets. 3. They use this upperbound to show that the probability of cross entropy vanishing under certain assumptions is upper bounded by a factor of \sqrt{\sqrt{x } \log p}(\log p+\log p)$, where $ p(x)$ is the ratio of the expected cross entropy with respect to the data point size and $ p(\lambda)$ the probability that the data points have vanishing cross entropy."
SP:123952325765c040c3078fc7dca2b6d370e55590,"This paper proposes a new model - specific classification framework, called RNF framework, which aims to mitigate the trade - off between fairness and performance. The proposed framework is based on the GCE framework. The main contributions are :   1.    a new attribute annotation framework, named RNF - GCE, which is built on top of the previous framework, GCE - RNF. The key idea is to use the same set of attributes as the original framework, but with the added assumption that the source and target attributes of the encoder are not the same. The authors claim that this is a necessary condition for fairness, and hence, the proposed framework can be regarded as an improvement over the previous one. 2."
SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"This paper provides an analysis of the invariance of CNNs under the following assumptions :   1.    the functions $ \ell_t$ are invariant under the assumption that $ t \in \mathbb{R}$ is a convex combination of $ \infty$ and $ t\infty$.   2. the function $ t$ is invariant to $ \nabla_{t}$, $ \tau_{t } = \ell_{t+1}$ and   $ \text{max}$   is not convex in the sense that $ \sqrt{\infty}$ can not be expressed as a product of $ t+1$ with $ t + 1$. 3. there is no invariance property under $ t $ that can be expressed in terms of $ T$, such that $ T+1 $ does not depend on $ t$."
SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,"This paper studies the problem of learning a kernel estimator that minimizes the effective dimensionality of the feature space of the hypothesis space of a linear regression problem. The paper proposes two approaches to this problem : ( 1 ) partitioning the space into different regions of the kernel space and ( 2 ) training the estimator in such a way that it is orthogonal to the space of data points in each region.   The paper provides a theoretical analysis of the trade - off between the size of the space and the number of iterations required to solve the problem. In addition, the paper proposes an algorithmic strategy for learning the estimators and training the kernel. Experiments show that the proposed approaches outperform the baselines in terms of accuracy and time complexity."
SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"This paper proposes a zero - shot learning framework for communication between unsupervised agents. The proposed framework is based on the concept of one - hot vectors, which are tokens that can be used to encode a set of data points in a supervised manner.   The authors provide a theoretical analysis of the proposed framework, and show that it is able to capture a wide range of agent behaviors. Experiments are performed on a variety of agent architectures, showing that the framework can capture most of the agent behaviors observed in the real world."
SP:8630ccc627534f9033bced04e2137a897ffef701,"This paper considers the problem of computing the capacity of a set of networks with a fixed number of users and a fixed amount of data. The goal is to find a network that is able to accommodate all the users in the set without compromising on the quality of the data provided by the network.    The paper proposes two approaches to this problem. The first approach is to divide the data into subsets and assign each subset a different capacity. Then, each subset is partitioned into layers and each layer is assumed to have the same capacity. The paper shows that if we assume that each layer has a capacity of at most $ \ell_1 $, then there exists a subset of data such that the total capacity of the network can be increased by a factor $ \epsilon$ with respect to the previous layer."
SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"This paper considers the following optimization problem : given a distribution $ \mathbb R^2 $ and a set $ R\mathbb S^{-1}$, what is the best way to optimise it? The goal is to minimize the ratio of the expected risk of the distribution $ R$ with respect to $ R$.    The paper first shows that there is a form of the inequality $ ChebyshevCantelli$ that minimizes this problem, and gives bounds on the ratio $ \sqrt{R}$. The paper then shows that if $ R \log n \in R$ is large enough, then there exists a form $ \tilde{C}$ such that $ \log(R^2)$ is optimal in the sense that for any $ \nabla r^2$ and any $ R^3$, there exists an instance $ r^4 $ such that the probability of the distributions $ \text{C } \leq \log ( R^4)$ and $ \theta r^5 $ are equivalent.   Finally, the paper shows that the corresponding inequality is optimal if and only if $ \sigma^6 $ is small enough."
SP:5bac542a6532d43cf100e085398b4a4783719814,"This paper proposes a cross - modality semantic parsing framework for audio - visual video parsing. The proposed framework consists of three modules : ( 1 ) cross modality co - occurrence module, ( 2 ) audio - video co - attention module, and ( 3 ) sound - feature - wise module.   The first module is designed to provide a unified view of audio and visual events. The second module is to provide audio - language - level annotations for events, and the third one is to represent audio and video events jointly. The authors also propose a weakly supervised video parsing framework, which is based on the idea of cross - attention. Experiments show that the proposed framework achieves state - of - the - art results."
SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"This paper proposes a federated learning ( FL ) framework where the goal is to provide clients with personalized information about the dimensions / structures of the global model and distillation loss of the personalization model. To this end, the authors propose two approaches : ( 1 ) distillation - based personalization ( distillation ) loss and ( 2 ) personalized model compression ( personalization compression ) loss.    The authors provide a theoretical analysis of the proposed framework and show that both approaches are equivalent to each other in the sense that the distillation losses of the two approaches are identical only in terms of the dimensionality of the model and the dimension of the data set of the clients. The authors also show that the personalized model compression loss can be viewed as a solution to the proximal gradient update ( proximal update ) problem in the FL framework."
SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"This paper proposes a new deep clustering method, DC - GMM, for deep generative models. The proposed method is based on a constrained clustering approach, where a set of data points is partitioned into domains, and the goal is to find a clustering algorithm that maximizes the similarity between the domain knowledge of the data points and the distribution over the domains. The main contribution of this paper is the introduction of a new constraint on the dimensionality of the distributions over domains, which allows the model to be trained in a self - supervised manner. Experiments on real - world and complex data sets show that the proposed method achieves state - of - the - art performance."
SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"This paper proposes a new regression and classification framework for neural networks, CNTK, which is based on the similarity between the original NTK and the convolutional version. The proposed framework has two main contributions : ( 1 ) it provides a near - sparsity - time approximation guarantee for the kernel of the NTK matrix, and ( 2 ) it proposes a cosine - based gradient descent algorithm for the rank - space representation of the kernel. Experiments are conducted on a small - scale dataset ( $ \ell_1 $ ) and a large - scale ( $ n_2 $ ) dataset. Results show that the proposed framework outperforms the previous works on both the regression and the classification tasks."
SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,This paper presents a multi - person 3D trajectory prediction framework based on the multi - range Transformers model. The main idea is to predict the trajectory of a person moving in the same direction as the rest of the world using the global range encoder and the individual trajectory encoder.    The main contributions of this paper are as follows :   ( 1 ) This framework is shown to be able to predict long - term 3D trajectories of a single person ; ( 2 ) this framework can be used to predict trajectory of multi - people moving in different directions ; ( 3 ) the proposed framework can also be used for cross - entropy detection.
SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"This paper proposes a reinforcement learning framework for solving long - horizon planning problems. The framework is based on a predictive program synthesis ( PPS ) approach, where the goal is to learn a model - free program that can be guided to solve a given set of problems. To this end, the authors propose two approaches : ( 1 ) program - guided approaches and ( 2 ) guided approaches that are built on top of the learned model.    The first approach is an iterative approach, in which a set of programs are generated and guided to a given goal. The second approach is a more structured approach, which is a combination of two approaches, one guided and one un - guided. Experiments show that both approaches perform better than the first approach."
SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"This paper presents a theoretical analysis of the problem of imitation learning from structured data. The main contributions are two - fold :   ( i ) a proof of the invariance of the learned structure under perturbations, and ( ii ) an empirical study of the quality of the resulting trajectories. The theoretical analysis is based on the following three main assumptions : ( 1 ) that the learned trajectories are invariant under the perturbation, ( 2 ) that there exists a metric that measures the difference between the trajectories of the two trajectories, ( 3 ) that trajectories that are close to each other tend to be similar in terms of the number of iterations, ( 4 ) that a trajectory that is close to one another is likely to have similar trajectories in all stages of the learning process.   Experiments are carried out on two different types of structured data sets, i.e.    two - dimensional ( 2D ) and 3D ( 3D ) data sets. The results show that the two types of data sets yield similar results, but that the 3D data sets are more informative."
SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,"This paper studies the problem of assigning an identity to an object under the assumption that it has been traversed through a set of trajectories that are invariant to time and space, and that the trajectories are conditioned on the identity of the object.    To this end, the paper proposes two approaches : ( 1 ) to assign the identity to a structured representation of an object, and ( 2 ) to obtain an invariant classifier that maps the structured representation to the invariant identity, the authors use the notion of invariance under timesteps. The authors show that both approaches are equivalent to each other in the sense that the invariance classifier can be seen as an upper bound on the dimensionality of the space of invariant classes."
SP:f32eddbb5c33a8422c075579ff08aa9833338d44,This paper studies the following questions :    1. What is the best way to learn a policy from data?   2. What are the best ways to sample from a set of functions that will ensure that the policy will learn to cover a given set of classes? 3. How do we know which functions will satisfy the above questions? This paper answers the first question by showing that functions that satisfy the first two questions are smooth functions. The second question answers the second question by constructing a smooth function that satisfies the third question.
SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,"This paper considers the following learning problem : given a set of data points $ \mathcal{x}$ and a regression model $ \sqrt{x } \log p}$, estimate the distance between the set $ \cal p$ and the set of $ \tau$ points $ p$ given $ \text{xi}$. The goal is to minimize the number of samples required for this estimate to be correct.    The paper proposes two approaches to this problem. First, the authors propose a weighted regression strategy that minimizes the set - size of the data points used for the estimate. The second approach is a weighted reweighting strategy that maximises the set size of samples used to estimate the re - weighting. The authors prove that both approaches converge to the same estimate under certain assumptions."
SP:fe12e13602925b9400fd596a987755beb10aa3d1,"This paper studies the problem of estimating categorical variables under the assumption that they are variables that can be represented as distributions over a set of random variables. The main contribution of the paper is the following :    1. Introducing a new estimator of the categorical variable $ \theta$, which is based on the fact that it is a convex combination of two variables $ \alpha$ and $ \beta$, and which can be expressed as a function of $ \mathbb{R}(\theta)$.   2. Extensive experiments are presented showing that this estimator is one - out - of - distribution, i.e., it does not depend on the choice of the data distribution.   3. Further, the paper presents a relaxation of the estimator in terms of the covariance of the variables $\alpha$ with respect to $ \tau$. 4. This relaxation is used to prove a convergence result for the gradient estimator on the set of $\theta$."
SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"This paper presents a theoretical and empirical study of the problem of predicting the performance of neural architectures in the context of neural network ( NAS ) framework. The main contributions are as follows :   ( 1 ) This paper provides a theoretical analysis of the current state - of - the - art NAS frameworks and shows that they are not able to provide a good predictive performance in terms of the number of samples and the quality of the predictors they use ; ( 2 ) A set of empirical results is presented showing that the current NAS frameworks fail to provide good predictive performances ; ( 3 ) An empirical study is conducted to identify the factors that are responsible for this failure ; and ( 4 ) a set of experiments is presented to show that the failure of current NAS models is due to the fact that they do not consider the diversity of the architectures used in their models ; ( 5 ) An approach is proposed to improve the predictive performance of NAS models by selecting a subset of the models that are better than the others, and by performing an iterative search over these models."
SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"This paper proposes a reinforcement learning framework that aims to provide players with a safe and globally consistent experience of the world. The framework consists of two components :    ( 1 )   Robust Exploration Games, which aims to learn a globally consistent metric that quantifies the safety of a set of games. This framework is based on the concept of “ safe games ” introduced in [ 1 ], where a safe game is defined as one that is not too different from a standard safe game in terms of its timestep.   [ 2 ] Robust ConTrol, which is a similar framework but with the goal of learning a metric that measures the difference between the timesteps of safe and unsafe games. In this paper, the authors propose a new metric that takes into account the fact that the safe games can be played in different environments ( e.g., different environments with different reward functions ). This metric is called “ global cosmological constant ” ( EDD ). The paper shows that EDD is able to provide safe games in the sense that it is not only able to learn metric that is globally consistent, but also able to control the metric in a way that is controllable."
SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,This paper proposes a novel approach to the problem of drug design. The proposed approach is based on a fragment - based approach to generate a set of molecules that can be used as building blocks for the next generation drug. The key contribution of this work is the introduction of a scoring function that quantifies the probability that a given molecule can be safely synthesized in the first stage of the design process. The paper also proposes a replay replay mechanism that encourages the designer to improve the quality of the generated molecules during the second stage of design.
SP:b938bca513e7de1231212064caf8877a78d8b612,"This paper proposes a greedy search algorithm for directed acyclic graphical models that satisfies the following assumptions :    1. There exists a set $ p(x, y)$ of directed graphs $ y$ such that $ \mathcal{x } \in \mathbb{R}$ is a set of directed paths $ z$ with $ \alpha = \sum_{t\infty}$ and $ z \infty$ is the set of vertices of $ \text{x}$ that can be traversed by the greedy search procedure.   2. For every $ \log p(y|x)$, there exists a $ \sqrt{\frac{x}{\sqrt{y } } \times p(\log{x})$ that maps $ z}$ to $ \tilde{x}\infty$.   The goal is to find a path $ x$ that minimizes $ \nabla_{\log p}$ with probability p(\sqrt { x}$ under the following assumption : $ p(\text{xi } \leq p(\lambda)$ where $ \lambda$ is either $ \theta$ or $ \gamma$, and $ x(\lambda ) is the distribution over $ z$. The authors assume that $ x \in [ \mathbf{x},\lambda]$ is not too differentiable and that $ z\in { \lambda}$ satisfies some other assumptions such as   ( 1 )   $ \sum_t\frac{\text{z}$, ( 2 ) $ \frac{xi}$ ( \lambda(x\in \lambda)$. The main contribution of this paper is to show that the following two assumptions hold for directed graphical models : ( 1. ) there exist $ \cal_{t}$ paths $ \in { x } $ such that for every $ y}$ there exists $ \sigma(x)\log p(\alpha)$ $ z(x)$. ( 2. ) for any $ \beta$, for any set $ x\in [ 0, y]$ the set $ z $ can be approximated by $ \overline \log(x^n \log n(\lambda)\log(t\log n)$ under some additional assumptions ( e.g., $ \hat{\frac { x}{\text{y}$ ) $ and $ \vert\lambda(z}$. Theorem 1.1 gives a sample complexity upper bound on the number of samples required for this to hold."
SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"This paper studies the problem of "" differential privacy with differential privacy "" ( DP ) learning, i.e., learning an algorithm that is provably safe for a given set of users under the assumption that the source and target classes are unknown to the rest of the system.    The main contributions of the paper are as follows :   1. This paper provides a lower bound on the probability that a "" user - level "" DP algorithm will be "" identifiably safe "" in terms of the dimension of the source / target classes. This is done by showing that if the source class is unknown to all the other users, and the target class is known to only a few percent of the users, then the algorithm will fail to learn a DP class that is globally safe. 2. This work also provides a proof that an "" interactive local model "" that is interactive with the source classes but not the target classes can not be learned by an "" item - level DP algorithm "" that samples from a source class and a target class, and that such a local model can not learn DP classes that are not publically known to all users. 3. The paper shows that this is equivalent to not being able to learn an interactive model that is identifiability - safe for any user level DP learner."
SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"This paper considers the following problem : given a linear function $ \mathcal{x}$, what is the optimal value function $ x$ that maximizes the expected cumulative rewards over a set of $ z$ points? This question is formulated in terms of SGD updates of the parameterized linear weights $ z$.   The paper provides two main results :    1.   the first shows that SGD with $ z = \sum_{delta } \infty$ converges to $ \delta^2 $ as the number of iterations goes to infinity, and   gives a lower bound of $ \text{max}$ on the expected value of the function $ z\infty$. This bound is tighter than the one obtained in [ 1 ], and it is the first one to be obtained in the case of a neural network function - approximation setting ( e.g., in [ 2 ] ). 2."
SP:992aa07d4f815d1c81f967374590eece933833b1,"This paper proposes a question answering framework based on supervised embeddings. The goal is to improve the quality of the data augmentation provided by the supervised embedding framework. To this end, the authors propose two approaches : ( 1 ) based on question answering, and ( 2 ) structured question answering. Based on the question answering approach, the proposed framework has two main components :    1 ) Based on KG framework. The first component is based on the KG embedding. The second one uses the supervised framework.   Experiments are conducted to validate the effectiveness of the proposed approach. The proposed framework is shown to outperform the baselines by a large margin."
SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"This paper proposes a new evaluation framework for improving the quality of the knowledge base ( KB ) of the QAQ dataset. The evaluation framework consists of two components : ( 1 ) a data - based evaluation criterion that quantifies the probability that a given set of data points has been successfully completed by a given model ; ( 2 ) a quality - based criterion that characterizes the likelihood that a set of such data points have been satisfiably described by a model that satisfies certain criteria. The paper shows that the proposed evaluation framework can be applied to a variety of different types of data sets ( e.g., data sets with different number of components, different levels of similarity between the data points and different model types ).    The paper also proposes a data structure for the evaluation framework that can be used for both data sets."
SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"This paper presents a dialog - based approach to generate human - like responses to pre - trained language models for a set of dialog - oriented NLP tasks. The approach is based on the Roles Dialog Model ( ARDM ), which has been shown to be able to generate text - based responses to a variety of different types of tasks. To further improve the performance of the model, the authors propose to add annotation information to the responses generated by the model. The authors conduct a series of experiments to evaluate the effectiveness of the proposed approach.    The main contributions of the paper are as follows :   1. Introducing a new approach to generating humans - like response - based approaches to dialog - related tasks. 2. Conducting an extensive set of experiments with a pre - trainable language model ( GPT-2 ) and two types of datasets ( e.g. wikipedia and wikipedia - doc ). The results show that the proposed approaches can generate responses that are up - to - date with respect to the current state - of - the - art methods."
SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,"This paper considers the following question : given a set of data points $ \mathbb{R}$, what is the best way to measure the value of $ \sqrt{x}$ over the set $ \log p(x)$?   This question is phrased as follows : given $ \text{x } \leq p(\lambda_{t\log p}$ where $ \lambda_t$ is a positive integer and $ x$ is the set of points $ z$, assume that $ \gamma_t\leq \log n \infty$ and $ z\log n\infty$.    The authors answer this question in the following way :   1. They assume that there exists a vector $ \alpha_t $ such that $ x\log t$ can be represented as a sum of $ z^{-1/2}$ with $ z_t(x,\lambda_0)$. 2. For each $ z_{t}\log p$, they assume $ x_t=\alpha_0$. 3. For $ y$ they assume the following : $ y_t \log z$ to be positive, $ x_{t}$ is an injective measure over $ \theta_t$. 4. For all $ y_{t } $, there exists $ \tau$ such that for any $ z \log q_0 $, $ \sum_{t_t } \log \alpha_{t}}$ is at least $ \frac{\sqrt{\log p}{\text{z}$."
SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"This paper provides a theoretical analysis of the convergence rate of neural networks in terms of the number of iterations and the amount of iterations required to generalize a given network to a given depth. The authors show that for a fixed $ \ell_0 $, there exists a kernel $ \tilde{N_{GP } \infty$ such that for any $ \mathbb{R}^{n_{GP}$, there exist $ \nabla_{k } \times N_{GP}}$ iterations such that $ \sqrt{\ell_2}$ is the kernel of a neural network with $ \text{max}$ iterations.   The authors argue that this kernel is a necessary condition for the network to be able to tolerate large amounts of iterations."
SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"This paper proposes a series of metrics to measure the invariance of sequential and 3D representations of a protein's structure to changes in the number of atoms or molecules. The paper also proposes a metric to measure how well a protein can be represented by a 3D representation.    The main contributions of the paper are as follows :   ( 1 ) This paper proposes two metrics, i.e., local and global invariance metrics, which measure the extent to which a given sequence of atoms and molecules can be mapped to 3D structures under certain assumptions ; ( 2 ) A metric that measures how well 3D structure is preserved under changes in number of molecules or molecules ; and ( 3 ) a metric that quantifies how well 2D structure can be modeled by changing the size of a molecule or protein molecule."
SP:5188280131b58a35d3deda126a0754aea8fa6e58,"This paper studies the space of critical points of convex functions satisfying the following assumptions :   1.    given a convex function $ \mathbf{x}$ and a finite set of points $ z$ in the space $ R$, define a set $ \tilde{x } \infty$ such that $ \gamma_t$ is the convex hull of $ z$.   The authors show that if $ x$ is a function satisfying the above assumptions, then there exists a critical point $ z $ such that for any $ \text{xi}$, the set $ z \times \mathbb R^2 $ is composed of $ \sum_{t } \log p(z}$ where $ p(x)$ and $ t(z)$ are functions satisfying these assumptions and $ z(x)=\gamma_{t}$ is an injective function defined on $ \theta_t$. They also show that $ z\times \gamm$ is injective if and only if $ z^2$ is also injective and that for every $ \sqrt{z } \leq p(Z)$, there exists $ z_t(z ) \sim p(\theta_{t})$ such as $ z_{t+1}$ that $ x_{t-1}(z_t)$ has dimension at most $ z^{-1/2}$ with probability $ \frac{1}{\sqrt{\frac{2}{\frac{x}{\text{z}/z}$. They show that for $ z}$ this is also the case for $ \log{\text{i}$ if $ \lambda_t\leq \log(z+1)$ is convex and satisfies the following additional condition : $ \alpha_t \log ( z+1 ) = \frac { z+0}{\mathbbR^2 } + \log n(z,\text { z}/x}$."
SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,This paper proposes a framework to evaluate the similarity between unsupervised and supervised graph learning tasks. The framework consists of three components :    1.   a set of benchmark datasets. The first dataset is a collection of subgraphs and the second one is a dataset of structured graph objects. The third one is an un - structured dataset. The authors provide a loss function for the structured dataset and an error loss for the unstructured dataset. They also provide a sample loss for structured datasets and a loss for unsuperstructured datasets. They show that the proposed framework outperforms the baselines in terms of the number of errors and the amount of structured data used.
SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,"This paper presents a new CFR algorithm for extensive games based on the regret minimization ( CFR ) approach. The algorithm is based on two assumptions :   ( 1 ) Each iteration of the CFR algorithm contains at most $ \ell_t$ games, and ( 2 ) for each iteration $ t$, there exists a $ \tilde{t } \infty$ such that for every $ t}$, the game $ \text{x}$ is contained in $ tilde{x } $ games, where $ T$ is the number of iterations of the algorithm.    The authors prove that for $ t = 1 $, the algorithm converges to a solution $ \sqrt{\sqrt{t}$ with high probability, provided that $ \mathbb{R}$ and $ \log(t)$ are sufficiently large."
SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"This paper provides a hyper - parameter sensitivity analysis of the dataset of domain discrepancy losses of the Prototypical Network ( DMPN ) dataset. The dataset consists of 5 datasets : 1 ) Domain Adaptation ( UDA ) dataset, 2 ) Component Mean Matching ( GCMM ) dataset and 3 ) Feature Distribution Matching Dataset ( PDM dataset.    The paper shows that UDA dataset is more sensitive to source - domain discrepancy loss than the component mean matching dataset. Moreover, the paper also shows that the feature distributions of the two datasets are more similar than those of UDA and PDM datasets."
SP:40be996e8bb86e887077b762b87c7c34a786ac98,"This paper proposes a learning approach for improving the quality of the supervised code generated during small - batch training of a generative model ( CNF ). The approach consists of two steps : ( 1 ) training the model on large - batch data, and ( 2 ) learning the tolerances between the learned model and the ODE solver. In the first step, the model and solver are jointly trained on a set of test data collected during the training phase of the model. The tolerances of the learned models are then used to improve the test error of the solver during the second step.    The paper provides empirical results showing that the proposed approach is better than the baselines used in previous work ( e.g., [ 1 ] and [ 2 ] )."
SP:97764e3393216106ff2ac3f550845acf4636119f,"This paper studies the problem of TD learning in the setting where the goal is to learn a value function that approximates the difference between the value of a set of samples and the true value of the samples. This setting is referred to as the “ lazy ” setting.    The paper proposes a new learning algorithm, which is based on the concept of “ fixed point ”, i.e., the point at which the value function of a sample converges to its true value in a finite time interval. The key idea is to train the learning algorithm in such a way that it is able to cope with the fact that the data points in the future will be different from the ones sampled in the past. The paper provides a theoretical analysis of the convergence of the proposed algorithm in the context of two different settings : ( i ) the case where the data point is sampled from the past and ( ii ) the situation in which the model is assumed to be invariant to the future. The main contributions of the paper are as follows :   ( 1 ) a proof that the fixed point of the TD learning algorithm converges linearly to the true $ \ell_1$-value in the lazy setting, and ( 2 ) an analysis of its convergence rate in the case of the case in which data points are sampled from different time horizons."
SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"This paper studies the problem of verifying whether an action sequence can be represented as a sequence of sequences of sequences with the property that it is invariant under the action sequence of the target sequence. The problem is formulated as a learning problem, where the goal is to find sequences that satisfy the condition that the sequences are invariant with respect to the target sequences.    The paper proposes two approaches to solve this problem. The first approach is based on the assumption that sequences with invariant action sequences are likely to be sequences of the same length. The second approach uses the assumption of invariance under action sequences of length greater than a certain threshold."
SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"This paper presents a proof - of - concept approach to evaluate the adequateness of certain statements in the setting of neural networks. In particular, the paper focuses on the following :    1.   The paper proposes a neural network - based evaluation methodology to evaluate whether a given set of statements can be represented as a set of embeddings in a space of dimension $ \mathbb{R}^n$, where $ $ n$ is the number of elements in the set, $ t$ the dimension of the space $ d$, and $ \text{x}$ the set of steps in the derivation of the formula $ \theta$.   2. The paper shows that this approach can be used to train neural networks that are able to express certain types of statements ( e.g., * * positive * * and * * negative * * ) in terms of the embedding space $ t$. 3. This paper also shows that it is possible to train a rewrite - based neural network that is able to represent such representations."
SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"This paper proposes a new depth estimator based on a local model of the geometry of the world. The key idea is to use a neural network to predict the depth of a set of points in the world, given a data point $ p(x, y ) $ and a reference point $ z$.    The key contribution of this paper is the following :   1. The authors show that this local model can be used to provide a good value for the distance between any two points $ z$ and $ y$, provided that $ z \in \mathbb R^{-1/2}$, where $ z}$ is a positive integer.   2. They show that if $ z = 0, y$ is greater than $ \infty$, then the error of the local model is bounded by $ \sqrt{\sqrt{d\log n } \log n}$ where $ n$ is the dimension of the set $ y$. 3. They prove that the cosine of this cosine is also bounded by a negative integer, and show that the error is bounded inversely proportional to $ n$."
SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"This paper proposes a multi - layer Transformer - based learning framework for learning the vocabularies of natural language models. The proposed framework consists of two stages :    1.   a filter - based model. The input to the model is a set of tokens $ \mathcal{x}$ with $ \text{x } \in [ 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,28,29,30,32,34,39,40,41,50,51,60,62,63,74,87,90,101,103,123,123,...   2. a data - driven model. Each layer of the proposed framework is equipped with a $ \cal(t_{\text{text}^2 $, $ \log{x}\in [ -1,0, -1, -2, -3, -4, -5,8,...,... ], $ { \log { \text{\text{xt } } } $, and $ \lambda_\in [ 1,2,-1,-2,-3,-4,..., -8,14,... } $. The authors show that the proposed model is able to learn the softmax - to - hard ratio $ \sqrt{xt}$ between the two stages."
SP:745dd86d7f7bba79a02d27922003b764b620f83e,"This paper presents a data - driven iterative perceptual grouping pipeline for shape segmentation. The proposed approach is based on the clustering framework proposed in [ 1 ] and [ 2 ], where the goal is to obtain a dataset of 3D shape representations for each object in a given domain. The key idea is to divide the domain into 3D domain into subsets, each of which is divided into 3 parts. Each of these subsets is then partitioned into a set of proposals, and each of them is used to generate a subset of the 3D features that can be used as a basis for the next segmentation proposal.    The paper proposes two approaches to this problem. First, the paper proposes to group the data according to the dimensionality of the domain and the number of proposals. The second approach is to group all the proposals into a single domain, and then use a clustering approach to aggregate the proposals based on their dimensionality. Experiments are conducted on a number of different domain domains, and the results show that the proposed approach performs well."
SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"This paper proposes a GAN - like approach to the problem of transforming data produced by a generative adversarial network ( GAN ) to a set of non - linear transformations. The proposed approach is based on the idea of batch - distillation, i.e., distillation of a batch of data that is fed to a neural network that is trained to produce data that satisfies a certain set of transformations, and then distillation is performed on top of that data to produce a dataset that satisfies the given transformations.    The key idea of the approach is to divide the data into batches of different sizes and to distill the data in each batch according to the amount of batch artifacts that can be generated by the network. The paper also proposes a way of distilling the data according to a distribution that is invariant to the size of the batch and the number of iterations used to generate it."
SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"This paper presents a series of experiments on few - shot and many - shot image classification and neural network learning based on the FSS-1000 dataset. The experiments cover a range of image segmentation and network architectures, with the goal of providing a few metrics that can be used to evaluate the quality of the learned initializations and to compare the performance of different initialization approaches. The paper also presents a set of experiments that aims to provide insights into the learning of neural network representations for image classification."
SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"This paper proposes a meta - training framework for the Random Walk Networks ( RWN ) model, which is based on the well - known FSL model. The main contribution of the paper is a series of mini - mini - supervised loss - based prototypical prototypical networks ( PN ) that are built on top of the semi - supervised meta - learning framework ( FSL ) from ( Xie et al., 2020 ).    The main contributions are as follows :   ( 1 )   a new meta - loss framework, called Prototypical Networks(PN ), based on random walk ( random walk ), is proposed. The authors show that the proposed PN is more robust than FSL, and that it is able to learn well from data. ( 2 ) a set of experiments is conducted to validate the effectiveness of the proposed framework. The experiments are divided into two parts. First, the authors show the performance of the PN in terms of global consistency loss and local consistency loss. The second part is devoted to demonstrating the performance on a prototypical classification task."
SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"This paper proposes a multi - sensor sensing framework for supervised learning with multi - input / multi - output sensing. The proposed framework consists of three components : ( 1 ) a set of low resolution sensors, ( 2 ) a higher resolution sensor, and ( 3 ) an encoder / decoder network. The encoder and decoder networks are designed such that the encoder can be used to capture both the spatial and the temporal aspects of the data, while the decoder can capture only the temporal aspect. The decoder is designed to be able to capture the spatial / temporal aspect of the input data.    Experiments show that the proposed framework achieves state - of - the - art results on a variety of sensing tasks."
SP:4d8e054f07006b4f896721b5c24da805727d2c22,"This paper proposes a new learning rate schedule for unstructured network - agnostic pruning, which is based on the idea of fine - tuning and rewinding the weights of the pruned network. The paper provides a theoretical analysis of the relation between the proposed rate schedule and the default pruning rate schedule, and shows that the default rate schedule has a lower criticality than the one used in the standard pruning algorithm. Moreover, the paper shows that there is a trade - off between the ratio of the re - wornting and re - timestich timescales, which can be interpreted as a measure of the quality of the training provided by the network."
SP:3bb1c79f9482e09828eda45fbb2e654f37219365,This paper presents an empirical study of the robustness of the update - based gradient descent ( AMO ) algorithm. The main contributions are as follows :    1. This paper shows that the update rule used by AMO does not violate the $ \ell_\infty$ norm of the output margin of the gradient descent algorithm. 2. The authors show empirically that this error - free update rule is more robust than the one used by the standard gradient descent method. 3. They show that the gradient update rule of AMO is more stable than the standard one.   4. They provide empirical evidence to support their claim.
SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"This paper presents an approach to train a generative agent to generate ungrounded and grounded dialogues in the out - of - domain setting. The model consists of two components : a decoder and a response decoder. The decoder is trained with a set of small - scale training examples, and the response component is trained using a larger set of training examples. Experiments are conducted on a variety of natural language understanding tasks."
SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"This paper proposes a unified generative model ( MGNMT ) for source and target text - to - text ( T2T ) models, which is based on the shared semantic space of the source - side and target - side language models. Specifically, the authors propose a generative generative adversarial model ( GDA ) that maps source to source data and target to target data, and an adversarial source - target model ( EMT ) that learns from source to target model and target model to source model. The authors show that the proposed GDA can be used in both resource - rich and low - resource settings."
SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"This paper proposes a sampling - based approach to improving the quality of the entropy reinforcement learning ( RRL ) objective for the soft actor - critic ( SAC ) algorithm. The main contributions are :    1. Introducing a new metric that quantifies the amount of time it takes for an algorithm to learn a metric that is valid for a fixed set of actions ; 2. proposing a sampling scheme that allows the algorithm to sample from a larger set of action spaces ; 3. Using this metric, the authors show that the algorithm can be made to learn an RRL objective that is better than the one that was used in the original SAC algorithm."
SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"This paper presents a theoretical analysis of the current state - of - the - art cross - entropy attacks on the audio - text - copyright - identification systems. The paper shows that the existing works do not consider the fact that the authors of the attacks do not have access to the source - code for the attacks, and thus do not provide any theoretical justification for their attacks. Instead, the paper proposes a new theoretical analysis based on the idea that the source code used in the attacks is not the original source code, but rather a modified source code that has been corrupted by the attacker.    The paper also provides a theoretical justification of the proposed attacks and shows that they are not able to break any of the existing systems."
SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"This paper presents a novel activation map - based approach for learning representations of the metric metric learning problem. The proposed approach is based on the concept of similarity in metric learning, which is defined as the similarity between the metric learned by a point - to - point search and the one learned by re - identifying a set of points in the training set. To achieve this goal, the paper proposes two approaches : ( 1 ) re - identification and ( 2 ) reference embedding.    The first approach uses an image / video embedding to identify the points in a training set that are most likely to be re - identified by the search. The second approach uses a map to re - identify points that are likely to have a similar metric to the original search set. The paper evaluates the effectiveness of both approaches by showing that re - identifiably identifying points leads to better learning representations, and that embedding points that have similar metric learnability leads to more informative representations."
SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"This paper proposes a planning framework for lifelong learning based on a model - free learning agent. The learning agent is built on top of a pre - trained policy learning agent that learns from a set of data points collected over a period of time. The goal is to minimize the number of failures that the learning agent experiences as it moves through the learning process.    The paper proposes two learning strategies : ( 1 ) model - based planning ( AOP ) and ( 2 ) policy - free planning ( POP ). The first one is based on the idea that the agent should be able to learn from data points that it has not seen in the past, while the second one is a combination of both. The paper shows that the POP method is more stable than the policy - based one in terms of performance."
SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"This paper proposes a new attention mechanism for visual captioning tasks. The attention mechanism is based on the similarity between the features of the data and the attention mechanisms of the model. The model is trained on a set of COCO and Flickr30k datasets and is trained with the following components :    1.   A human - annotated attention mechanism. This is done by assigning attention weights to each data point and assigning attention to the features that are most relevant to the given data point. 2. A model that is able to capture the relationship between the attention mechanism and the model ’s structure. This model is then used to generate transformations for the data points. 3. A knowledge - based attention mechanism, which is built on top of the knowledge of both the model   and the data point, is used to promote the transformation."
SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,This paper proposes a methodology to predict the dynamic evolution of dynamic graphs in real - world datasets. The main idea is to train a neural network with a dynamic graph encoder and decoder. The encoder is trained on a set of datasets and the decoder takes as input the dynamic graph instance and outputs the dynamic graphs. The authors claim that the proposed approach is more data - efficient than previous work on dynamic graph mining.
SP:ff722957a1765c0568426ed88dd910a6b74054ef,"This paper investigates the problem of missing data imputation for classification datasets for real - world applications of image and tabular classification datasets. The main contributions are as follows :    1. Developing a generative model that is able to identify missing features and fill in the missing features, 2. Conducting a series of experiments to measure the quality of the missing data, 3. conducting ablation studies of the model, 4. conducting an ablation study of the uncertainty in the model parameters, 5. conducting experiments to evaluate the model's performance, 6. performing experiments to compare the model to other model - free and model - agnostic approaches."
SP:c051b0fe779d9e4131016970b7ba469b596f3009,"This paper studies the problem of estimating the distribution of the time horizon of a policy in the presence of a finite set of data points over which the policy is expected to converge to a stationary distribution over the horizon of the data points sampled from the data set.    The paper provides a theoretical analysis of this problem in the context of two types of data - driven policy estimation problems :   ( 1 ) the long horizon setting, where the horizon horizon is assumed to be of length $ t$, and ( 2 ) the sampling - based setting, in which $ t_t$ is the number of points sampled in the policy estimation problem. The paper shows that both of these problems are equivalent to each other in the sense that the corresponding distributions converge to the same distribution over time $ t$.   In the case of the second setting, the paper provides theoretical results showing that the distributions of time horizon and data - horizon distributions of the two cases are equivalent in terms of $ \mathcal{t_t } \infty$ and $ \sqrt{T_t}$, respectively."
SP:065c900843011a71b70ed35357a2f71fe83872a7,"This paper proposes a generative adversarial network ( GAN ) framework for generating high - quality images in a data - free manner. The GAN framework is based on the GMM paradigm, where a linear interpolation of a set of data points is used to generate a vector representation of each point in the data set. The authors propose a likelihood - based approach to estimate the likelihood of the point - wise distribution of the data points in the GAN dataset. They also propose a prior distribution for the vector representation."
SP:2da1608209058d214f8671062cc9eb0833ba4831,This paper presents a theoretical analysis of the computational cost of gating neural networks with a pre - specified prior distribution of features. The main contributions are :    1. The authors show that gating networks with pre - defined prior distributions are computationally more efficient than un - gated networks.   2. They show that a gating network with a prior distribution that prioritizes features that are likely to appear in a particular channel is more computationally efficient than a gated network that does not have such a prior. 3. They propose a method to aggregate features from different channels to obtain a single set of features that can be aggregated into a single dataset and use this aggregated set to compute a neural network.
SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"This paper proposes a score - based approach to improve the quality of the outputs of deep neural networks ( DNNs ). The main idea is to score the outputs according to the importance of a pair of inputs ( i.e., the similarity of the two inputs ), which is then used as a metric to assign importance to the outputs. The authors claim that the proposed approach is more energy efficient than the previous scores - based approaches ( e.g., [ 1 ], [ 2 ] ) and more informative than [ 3 ] and [ 4 ], which are based on the similarity between the input and the output."
SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"This paper proposes a reinforcement learning approach that compresses the action trajectories into a more hierarchical structure that is more suitable for reinforcement learning. The main idea is to train a set of agents that are conditioned on the topology of the world's topology by viewing the world as a collection of hierarchical hierarchies, where each topology corresponds to an action category.    The main contribution of the paper is to show that compressing the action trajectory into a hierarchical topology leads to better reinforcement learning than not compressing trajectories at all."
SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"This paper presents a generative model - based generative decoder for scene generation. The decoder is built on top of a GAN - based multimodal autoencoder ( EBM ) and is trained on a set of conditional and unconditional samples. The model is trained in two stages : first, the decoder generates a set and then the generator generates a sequence of tokens from the set. The generator is trained to reconstruct the original image and set from the conditional samples. Next, the generator is used to generate a sequence from the unconditional samples, which is then used to train a decoder. Experiments show that the model is able to generate more diverse scenes than previous generative models."
SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"This paper provides a theoretical analysis of normalization and cross - policy and off - policy ( TD ) transitions in the context of reinforcement learning ( RL ) algorithms. The paper provides theoretical analysis on the stability of the different normalization techniques under different policy settings, and proposes a set of benchmarks on which the authors show that the stability properties of different normalisation techniques are correlated with the number of policies and the amount of data used in the policy training phase. Moreover, the paper proposes a way to identify the optimal policy settings for each of these metrics, which can be used as a basis for designing more stable policy settings in the future. Experiments are conducted on the MuJoCo benchmark setting, and the paper shows that the proposed approaches are more stable than the state of the art."
SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"This paper studies the problem of training a model that is robust to confounding variables in the training data. Specifically, the paper considers the following :    1.   * Concretely, given a feature embedding space $ \mathbb{R}$ and a set of data points $ x_t$, what is the training strategy to ensure that the model does not learn features that are not correlated with confounding variables?   2. what are the training strategies that do n’t depend on confounding variables such that they are not harmful to the model? 3. how do we know when the model is not sensitive to confounding data? 4. what do we do when confounding variables are present in the data?"
SP:783049ff463edd1283c058c6106a3e1f9a033df4,"This paper studies the problem of learning a group - level language model to represent the group of entities under the action of a linear operator. The authors propose two approaches to this problem : ( 1 ) to learn the group level model $ \text{group}$, and ( 2 ) to use the language model $ { group}$ $ { lSTM}$ to represent a set of entities $ \theta$, $ z_t$ being the group size of the entities $ z$ and $ t_t $ the number of subgroups $ t$ of the group $ z$. The authors prove that both approaches are equivalent to each other in the sense that for the group version of the problem, there exists a subgroup $ \tilde{group }$ such that $ z_{t_t}$ is the group representation of $ z $, and for the subgroup version $ x_{t-1}$ there exist a sub group $ y_{t -1 } $ such that for any $ t-1$ the group - representation of the entity $ x$ is represented by $ x_t$.   The authors also provide a theoretical analysis of the complexity of both approaches, showing that the first approach is at least as hard as the second one."
SP:946c26d371297c88d0ac246257104099b4585edc,"This paper proposes a new approach to the problem of improving the timestep of smooth latent distributions in the context of hierarchical models. The main contribution of the paper is the introduction of the concept of "" hierarchical latent hierarchies "", which is a generalization of the notion of hierarchical variables introduced in [ 1 ] and [ 2 ]. In particular, the authors show that a hierarchical latent hierarchy can be viewed as a convex combination of two types of distributions : smooth and smooth - smooth.    The main contributions of this paper are the following :   - Introducing a new framework for the study of hierarchical latent distributions. - Exploiting the fact that hierarchical models can be seen as convex combinations of smooth distributions, the paper proposes to optimise the metric on the space of smooth variables in the sense that the metric is invariant to the smoothness of the underlying distributions."
SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"This paper proposes a three - dimensional self - attention mechanism for video generation, which is based on the concept of scale - action recognition dataset. The proposed framework has three components :   1.    scale - object recognition dataset, which consists of 4 categories of videos. The dataset is divided into 4 categories, each category corresponds to a different scale action recognition category. The second component consists of categories of natural video. The third component is category - specific video dataset, consisting of category categories of movements and interactions. The first category is categorized as natural video category, while the second category is category category. Experiments are conducted to evaluate the performance of the three components."
SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"This paper proposes a zero - shot generative model for multi - label text classification task. The model is based on a hierarchical structure similar to that used in prior work [ 1 ], [ 2 ] and [ 3 ]. The main difference is that the zero shot classification task is formulated in terms of a set of data points, where each data point is divided into a sequence of labels and each label is assigned a score based on the similarity between the label and the feature of the data point.    The authors propose two generative models, one is a generative zero shot model and the other is a bi - level model. The zero shot models are trained on the MIMIC - III dataset. The authors show that their model achieves state - of - the - art performance."
SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,"This paper presents a goal - conditioned continuous control approach for policy learning in an action - driven environment. The goal is to learn a high quality policy that is able to predict state and action sequences with high accuracy. To achieve this goal, the authors use a combination of two approaches : ( 1 ) state embedding and ( 2 ) action embedding. State embedding is based on a self - supervised representation learning ( RL ) approach, where the agent is provided with a set of state trajectories and actions, and ( 3 ) action is modeled with a model - free approach. The authors show that the combination of these two approaches leads to state embeddings that are better than the state - only embedding, and that the action - based embedding has higher quality.  "
SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"This paper proposes a meta - learning framework that aims to bridge the gap between data - driven and data - free semantic structure - driven learning for a heterogeneous set of meta - tasks. The proposed framework is based on the concept of space - space - knowledge ( space - metric ) and graph - metric ( metric ) knowledge. The paper proposes two approaches to this problem. The first one is to share meta - knowledge across different datapoints and a set of data points, and the second is to learn a knowledge graph that maps data points to a given metric. The authors claim that this approach is more data - efficient than existing approaches that do not share data points.    The paper also proposes a few additional contributions :   1. To overcome the problem of heterogeneity of the data points and metric, the paper introduces a metric that measures the distance between data points that are related to the same meta - metric and that are not related to each other. This metric is then used as a proxy for the metric that is used to classify data points in the data space. 2. To train the knowledge graph, the authors propose a data - sharing framework that shares meta - metrics and metric across data points but does not share the metric. This is done by sharing the metric and metric with a data point that is not connected to the data point. 3. To further improve the performance of the knowledge - graph - knowledge framework, this paper proposes to train a knowledge - metric that connects data points with different metrics, and that is shared with a set."
SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"This paper proposes a new model for attribute - specific language generation, Plug and Play Language Model ( PPLM ), which is built on top of the plug - and - play framework. The main contributions are :   ( 1 ) introducing a new sampling mechanism to control the cross - entropy between the generated text and the attributes of the source language ; ( 2 ) developing a set of large corpora to train the model ; and ( 3 ) conducting a series of experiments to validate the performance of the model."
SP:12d0980bfea2de880905a0b87b40856969bb1c58,"This paper proposes a new representation learning framework for supervised and self - supervised learning based on the laplacian pyramid space. The proposed framework is based on two assumptions : ( 1 ) that the data representation is not corruptible, and ( 2 ) that it is robust to changes in the data structure.    The presented framework is evaluated on a set of 5 supervised learning tasks, where the goal is to learn a representation that is transferable to different domains and that is not corrupted by changes in data structure ( e.g., changes in domain boundaries ). Experiments show that the proposed framework achieves state - of - the - art performance on supervised learning benchmarks."
SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"This paper proposes a new approach to train natural language processing networks with the goal of improving the cross - sensitivity of natural language detection. The proposed approach is based on the attention mechanism, which has been proposed in [ 1 ], [ 2 ] and [ 3 ].    The main contribution of the paper is to propose a new training method, based on a combination of two approaches : ( 1 ) a training approach based on cross - entropy, and ( 2 ) an attention based approach, which relies on the concept of bounding boxes. The paper shows that the proposed approach achieves better performance than the previous approaches ( e.g., the attention - based approach )."
SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"This paper studies the question of Q - learning, i.e., what is the best way to learn from data provided that we don't have access to all the details of the data at hand? This question is phrased as follows : given a set of data points $ \mathcal{T}$ and an action $ \tilde{x}$, how well can we learn from this data?    The paper answers this question by providing a data - driven upper bound on the difference between the expected value of $ \text{T } \log(x)$ and the actual value $ \sqrt{X}$ given the data $ \nabla_{t}$.   This upper bound is then used to prove a lower bound on $ \sigma^2 $, where $ \infty$ is the average expected value between the data and the data, and $ \ell_t$ is a metric that measures how well we can learn from the data provided by the data."
SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,"This paper studies the problem of learning domain invariant representations in multilayer neural networks. The main contribution is a theoretical analysis of the complexity - dependent complexity tradeoff between domain - invariant and unsupervised domain adaptation. The paper shows that there exist sufficient conditions under which the two domains should be invariant to each other in terms of the following factors : ( 1 ) the number of layers in the encoder must be less than a certain threshold, ( 2 ) the dimensionality of the domain should not exceed a certain critical dimension, and ( 3 ) the degree of domain invariance must be bounded above by a constant factor."
SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"This paper studies the generalization error bounds of the data - dependent gradient Langevin dynamics ( SGLD ) method. The main contribution of this paper is to provide a theoretical analysis of the generalizability of this method in terms of its batch - batch and acceleration error bounds. This is done by considering the case when the batch size is of the form $ m$, where $ \mathcal{T}$ is the number of samples in the batch and $ t$ the size of the acceleration step $ t$.    The main contributions are as follows :   1. A theoretical analysis on the generalisation error bounds for the data dependent SGD method.   2. A proof of the finite - batch stability of SGD."
SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"This paper proposes a goal - directed spatial navigation framework for continual learning of spatial variables. The framework is based on the principal component analysis ( PCA ) framework proposed in [ 1 ] and [ 2 ]. Different from the previous work [ 3 ], the proposed framework has two components :    1.   a navigation framework that learns to navigate to a target location from a fixed set of variables. 2. a spatial model that learns a set of task variables that is conditioned on the current state and the state of the task at time t.   Experiments are conducted on a variety of different tasks, with the goal being to learn a strategy that allows the learner to quickly switch between different tasks without having to re - visit all the previously visited variables in the training set."
SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"This paper proposes a new policy gradient based optimization method, MCTS, which is based on tree - search - based optimization. The key idea is to train a policy with a branching factor that depends on the number of traversal steps in the tree search. The paper shows that this policy gradient can be used as a metric to quantify the amount of action that can be taken by the policy in different environments.    The paper also proposes a policy bootstrapping strategy that is able to adapt to different environments, and shows that the policy can be trained in a variety of environments."
SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"This paper proposes a new classification task, winning tickets, to improve the quality of the existing public dataset of tickets. The proposed method is based on the following idea : given a dataset of $ \ell_t$ tokens $ t$, the goal is to find a subset $ \tilde{t_t } $ such that $ t_t \in \mathcal{x}$ is either $ \infty$ or $ \nabla{x_t\infty}$, and $ \text{text } $ is a combination of $ t_{t_0 } = \ell_{t-1}$ and $ y_{t -1}$.    The paper provides a theoretical analysis of the problem of winning tickets and shows that the existing dataset is not sufficient for this task. To solve this problem, the paper proposes to add a new classifier $ \theta$ to the dataset, and show that it is better than the existing classification dataset by a large margin.  "
SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"This paper studies the problem of under - perceived threat of text - based adversarial attacks. The paper proposes a data - augmentation / evaluation - based model - based attack strategy that is built on top of a pre - trained text - only model. The model consists of two components : ( 1 ) a data augmentation model and ( 2 ) an evaluation model.    The first component trains the model on a fixed set of questions, and the second one trains the evaluation model on the subset of questions that are selected from the training set. The authors show that the proposed strategy is more robust than the data - augmented model."
SP:5da870060778de460c1abe91562d6f3e707efef4,"This paper presents a safety - aware agent that learns to navigate in a multi - domain environment with complex dynamics using a combination of reinforcement learning ( RL ) and play - based approaches. The RL approach is based on a model - based approach, in which the agent is provided with a set of simulated data points and is encouraged to explore in each of these data points in a safe manner. The play approach is built on top of the RL approach, where the RL agent is incentivized to explore each data point in the environment in a way that maximizes its own safety.    Experiments are conducted on the following domains :   1 ) Driving car simulator, 2 ) Multi - domain action space, 3 ) Safety - aware environment, 4 ) Robust action space. Experiments show that the proposed approach outperforms the state - of - the - art in terms of safety."
SP:c2796f28fb067138303df8d424d646f4ada31558,"In this paper, the authors provide a theoretical analysis of the error in the knowledge of the cross - entropy difference between two sets of observations of the same physical system ( e.g., a set of temperature and time series of physical systems ).    They show that the error is caused by the fact that the data used to train the system is not geometrically differentiable, and that it is not possible to learn the cross entropy difference from a data set that does not contain any geometrical difference between the data set and the underlying physical system. They also show that there exist data sets that do not contain such a difference, and propose a way to correct for this error."
SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"This paper provides a theoretical analysis of the state - of - the - art stochastic gradient descent ( SGD ) algorithms for convex neural networks ( NN ) under the following assumptions :    1. There exists a $ \ell_t$-norm $ \infty$-regularization function $ \nabla_{\ell_0}$ that ensures that the gradient descent algorithm converges to a point $ t$ in time $ t$.   2. There exist $ \tilde{x}$-constraints that ensure that the $ t}$ - norm $ \theta$ is not too different from $ \alpha_{\theta}$ for all $ t=1,2,3 $, 4.   The paper provides theoretical guarantees for the following quantities   ( 1 ) $ \text{max_t}$, ( 2 )   $ \mathbb{R}$ and ( 3 ) $\text{sqrt{x } \in { \ell_{\infty } } $ for the convex and non - convex NN algorithms under the above assumptions, respectively."
SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"This paper presents a theoretical analysis of the entropy - based entropy coding framework, which has been proposed in the context of the probabilistic text - to - image compression framework ( PLN ). The authors show that this framework is not differentiable in the sense that it does not provide a differentiable lower bound on the entropy of the compressed data, and that the entropy is not guaranteed to be differentiable even if the data is differentiable. Moreover, the authors provide a theoretical justification of the non - differentiability of the encoder - decoder framework, as well as a proof - of - concept algorithm to compute the entropy in this framework."
SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"This paper proposes a new loss function for the super - resolution ( SR ) problem. The proposed loss function is based on the concept of “ quality - of - the - art ” ( C - JPG ) inputs. The main contribution of this paper is to show that the SR problem can be formulated as a series of two stages. The first stage is the standard SR problem. In the second stage, a new sub - problem is introduced. The key idea is to divide the input sequence into two stages, where the first stage corresponds to the SR input sequence, and the second one corresponds to a different SR sub - sequence. The paper shows that the two stages are equivalent in terms of the quality of the input sequences.  "
SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"This paper presents a neural network - based approach for the learning of the probability surface of the expected value of a given goal in a set of multi - ball cross - match scenarios. The goal is to learn a probability map that maps the probability of a ball crossing the line of play to the probability that the ball will be tackled by a team that has not yet seen a ball cross the line in the set of games played by the given team.    The main contribution of the paper is the following :   ( 1 ) construct a probability surface $ p(x, y)$ of expected value for a given ball crossing under the following assumptions : ( x, y )   that every ball crossing has a probability p(y, z)$ greater than 0.5, ( y, z ) that there is no ball crossing probability greater than p(z ) greater than 1, and ( z, i ) that any ball crossing is likely to be missed by the team if the ball is not tackled by the opponent within a certain window ( e.g., when the ball lies in front of the net ). The resulting probability map $ p(\sqrt{x } \log p(Y|z)$ is then used as input for the following learning problem : 1 ) select a ball from a distribution $ y$ such that ( x ) has a p(s, z)=0.5 and ( y ) that ( z ) is greater than -1, where z is a team's expected goal probability at time z.   2 ) learn the probability for the ball crossing in terms of p(a, z.3 ) and p(b ), i.e., learn the expected p(p(y|z ) as a function of z.4 ) learnable neural network $ \sqrt{\text{p(z, z}$, where $ \text{e}$ is the probability probability that a ball will not be tackled within a window of size z."
SP:1ae31baf383fc520687b255d9cac14c3b040e253,"This paper proposes a new method to estimate the completeness of a user's movies based on their latent feature embeddings. The key idea is to train a neural network that is able to predict the completion of a set of movies in terms of their latent features and the length of the user ’s previous viewing experience with each of them. The network is trained with two approaches :    1.   matrix completion model ( GNN ), which is based on embedding the features of each user $ \mathcal{X}$ $ movies into a matrix $ \sqrt{x } \log p(x_{\text{total})$, and   2. a matrix completion method ( MCTS ) based on graph completion model, which takes as input $ \text{complete}$ the features $ \tilde{total}$ of $ \gamma_0 $ movies and outputs $ \log n_t$ the number of views of each $ z_0$.   Experiments are conducted on the following benchmarks : $ m$ movies, $ y$ movies and $ p_0$ movies."
SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"This paper proposes a policy gradient based continuous control method for the discrete control problem. The proposed method is based on a sampling - based objective function minimization, where the goal is to minimize the sum of the objective function and the policy gradient. The sampling is done by iteratively adding a new objective function at each iteration and a new policy gradient at each step. The paper also proposes a zeroth order sampling method for improving the quality of the sampled objective function."
SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"This paper proposes a multi - agent coordination mechanism for the action semantics network ( ASN ) to ensure that the learned agent network is not too different from each other in terms of its architecture and supervision mechanism. The authors argue that this is necessary for better agent - agent and agent - network training and also for more general reinforcement learning ( DRL ) algorithms.   To this end, the authors propose two coordination mechanisms. The first one is based on the fact that the agent network has to be aware of the supervision mechanism of the other agent network. The second one is built on top of the existing supervision mechanism proposed in [ 1 ] and [ 2 ]. The proposed framework is then used to train a set of agents with different supervision mechanisms. Experiments show that the proposed framework can be used to improve the performance of a variety of agent networks."
SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"This paper studies the relation between low - rank structure of Q function and deep reinforcement learning ( RL ) algorithms. Specifically, the paper proposes a set of tasks and games that can be divided into two categories, namely low rank structure and deep rank structure. The paper further proposes two algorithms that are based on value - based and deep RL algorithms. The first one is based on Q function structure, while the second one uses deep RL algorithm. Experiments show that the proposed algorithms outperform the state - of - the - art algorithms in terms of performance."
SP:430336893b247b7bd45687d78b0d0511a7369e87,This paper presents a sample - efficient learning framework for policy and action - based reinforcement learning ( DRL ) in the setting of batch - based action - driven learning ( BAIL ). The framework consists of two components : ( 1 ) a policy network and ( 2 ) an action network. The policy network consists of a set of policies and an action space space space. The action space consists of the set of actions and the policy network is composed of the action space and the Q function.    The paper presents the following main contributions :   1 ) An empirical evaluation of the performance of the policy and the action network in the BAIL setting. The empirical results show that the proposed policy network outperforms the state - of - the - art DRL algorithms in terms of sample efficiency and Q function performance.
SP:94078964876667e8a5d9ae7728d779d5b91a576e,"This paper presents an extreme multi - label learning method based on text embeddings. The method uses a combination of two approaches : ( 1 ) embedding and ( 2 ) sub - sampling. The first approach is based on the embedding - based approach proposed in [ 1 ]. The second approach uses a search engine to generate a set of documents that can be used as labels for the next round of label learning.    Experiments are conducted on the following benchmarks :   1. < title> < title > < description > < head > < relation_id> < concept_id > < title_location > < relationship_proposition > < /concept > < concept > < line_height> < head_width > < width_height > < meta_data> < /data > < pre - defined > < input_text> < preprocessing_key > < token_number> < < preprocessed_text > < key_value > < priors_theorem > < p> < p > < formula_algorithm_key_problem_definition > < g > < proctors_classifier_desc > < classifier_significance > < c_classifiability > < definiteness > < categorical_distinction > < label_number > < generality > < unsupervised > < multilabel_label_example > < document_id_format > < datastream > < schema_text _ encoding _ format > < schemas_text_properties > < epsf > < alex_vector_id _ > < x_vector _ encoding_desc _ < g_identif_desc> < g_{text_model}_model_data_format_id x_vec_id g_classifiers_desc g_supervised_model > < z_vector > < y_vector < x_{max}_id } > < > < s_vector g_vector G_subset g_max > < m_vector E_vector X_vector C_vector A_vector B_vector F_vector H_vector   # # # = # 1 # 1#2#3#4#5#6#7#8#9#10#11#12#1 * # 2 *, # 3 ( # 4 ) # 5 ( # 6 )"
SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"This paper provides a theoretical analysis of the problem of hash - masking - based collaborative filtering, which is the following : given a vector $ z$ and a set of hash codes $ z_t$, the goal is to find a way to minimize the total number of bits required to ensure that the given vector is not too different from the original vector.    The main contributions of this paper are as follows :   ( 1 ) i ) derive bounds on the total amount of bits needed to achieve this goal ; ( 2 ) show that the required amount is polynomial in the size of the set of bits, and ( 3 ) prove that it is at least as large as the sum of the total length of the vectors used to encode the vector and the number of bit required to achieve the goal."
SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"This paper presents a study of the problem of generating generative adversarial networks ( GANs ) that fail to collapse under dense mode collapse. The paper proposes two approaches to this problem : ( 1 ) collapse - collapse and ( 2 ) dense - collapse. In the first approach, the learned distribution is assumed to be identical to that of the un - learned distribution, while in the second approach the learned distributions are assumed to have low - level perceptual quality ( e.g., black box ) and high - level predictive quality ( i.e., white box ).    The paper provides a theoretical analysis of the two approaches and empirically shows that both approaches fail under collapse and general mode collapse respectively."
SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"This paper presents a theoretical analysis of the two - layer neural network under the following assumptions :    1. There exists a $ \ell_t$-th order term $ t$ such that for any $ t \in \mathbb{R}$, there exist $ t\infty$ - order terms $ t $ t$.   2. There exist $ \tilde{x}$-order terms $ x_t $ such that $ t_t \in\mathbb_{r}$ is the sum of $ t+\ell_{t}$ and $ t + t$, where $ \text{xi}$ denotes the second order terms in the $ t}$ terms.   The authors prove the following guarantees under these assumptions : ( 1. For any two layer neural networks with $ t=1 $, there exists $ t_{t } - th order terms$ $ t-\ell_0 $ such as $ t - \infty$."
SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"This paper proposes a score - based neural network evaluation tool, GFD score, to evaluate the quality of a set of graph convolutional filters designed for a specific set of tasks.    The paper provides a thorough background on the existing works in the field of neural networks, and focuses on the different types of filter design used in the literature. The paper also provides a brief summary of the current state - of - the - art works in this field. The main contribution of the paper is the introduction of a score score that is based on the similarity between the properties of the selected set of data and the set of filters used for the given task. This score is then used to motivate the design of a new filter that is more suitable for the task at hand. In addition, the paper presents a series of experiments on both synthetic and real datasets to demonstrate the effectiveness of the proposed score."
SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"This paper presents a series of experiments aimed at improving the quality of the training datasets used for natural language inference ( NL ) tasks. In particular, it focuses on the following :   1.    Celebrities - based background datasets. The authors show that the CelebA dataset has a better quality than the other background datasets used in the literature, and can be used as a baseline for NL tasks. 2. “ waterbirds ” dataset, which has been used for the past few years to train NL models. The quality of this dataset is also shown to be better than that of the previous works. 3. A set of experiments is presented to evaluate the performance of the neural networks used for each of these tasks."
SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"This paper studies the question of black - box classifiers for predicting the cosmological timesteps of the next - to - leading order ( nlo ) perturbations to the standard metric metric metric. The paper proposes two approaches to this problem, i.e.    ( 1 ) blackbox - based metric and ( 2 ) metric - free metric.   The first approach is based on the assumption that the metric is invariant to a set of distributions ( e.g., $ \mathcal{x}$ ), and ( 3 ) that it is not too different from the metric used in the second approach. The authors show that both approaches fail to explain the nlo metric under the same set of assumptions, and propose a new metric metric that is more resilient to these assumptions."
SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"This paper proposes a new image - decomposition - based supervised learning framework, named MNIST, to solve a set of image classification and object detection / localization tasks. The proposed framework is built on top of top - k cross - entropy ( K - K selection ) and top - K cross entropy ( T - K classification ). The key idea is to first generate a dataset of images and classify them according to their similarity to the original dataset, and then train a classifier and an auto - encoder to classify the images. The paper also proposes a supervision mechanism to ensure that the learned network is not biased towards any particular domain. Experiments show that the proposed framework achieves state - of - the - art performance on the proposed classification and detection tasks."
SP:da1c5f6351d531482e90b86c3cceb52850c520de,"This paper presents a reinforcement learning approach for learning policy networks in the presence of change in the source - to - target distribution. The approach is based on the idea of Carlo Tree Search ( CS ), a well - known approach for generating new programs for reinforcement learning. The main contribution of this paper is to propose a learning strategy that samples the source and target distribution at different timesteps and learns a policy network that is robust to changes in source distribution and policy distribution.    Contributions :   1. Introducing a new approach to learn policy networks. 2. Conducting a series of experiments to evaluate the learning strategy."
SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"This paper proposes a new approach to improve the convergence rate of gradient descent in network architectures. The proposed approach is based on the first - order optimization of the matrix factorization of the gradient descent dynamics. The paper provides a theoretical analysis of the proposed approach and empirically shows that the new approach is more efficient than the existing approaches in terms of both the number of iterations and the overall quality of the solutions. Moreover, the paper also provides theoretical guarantees on the speed of convergence and the amount of iterations required to achieve it."
SP:3e3bc8f617df742a395e7d315ec3810a42071294,"This paper proposes a new initialization strategy for improving the complexity of kernel - based initialization of neural networks ( NNs ). In particular, this paper focuses on improving the initialization of ReLU - NNs. The main contribution of this paper is to propose a new $ \ell_t$-based initialization strategy that is guaranteed to be error - invariant with respect to the number of parameters and the dimension of the input set.    The main contributions of the paper are as follows :   - Introducing a new initialization strategy for NNs that is not based on gradient descent. - Providing error bounds on the number and dimension of parameters that need to be interpolated in order to ensure the correctness of the proposed initialization."
SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"This paper presents a depth - based 3D object detection and depth estimation framework for LiDAR systems. The proposed framework is based on a 3D spatial embedding of the source and target vectors and a vector - based depth estimator.    The main contributions of the paper are as follows :   ( 1 ) a new object detection framework based on 3D embeddings and a new depth estimate framework, ( 2 ) a gradient - based approach to obtain depth estimates, ( 3 ) a better embedding and a more accurate depth estimate, and ( 4 ) a more robust object detection network."
SP:983d84502264633f3385d426c1d4601a0744ea9a,"This paper proposes a new adversarial example detection / classification framework for neural networks. The proposed framework is based on the GAT framework, which has been proposed in [ 1 ] and [ 2 ]. The main contribution of this paper is to propose a new classifier framework for generating adversarial examples that can be used to defend against white - box attacks.   [ 3 ] GAT - Generative Adversarial Training ( GAT ), [ 4 ] Generative Classifiers for Defending against White - Box Exploitation ( GCT )."
SP:461e9308d050bc3dc7b35233452668bb31f5d491,"This paper studies the problem of learning a representation of the state space of an agent that can be used as a guide to improve the agent's ability to distinguish between different environments. To this end, the paper proposes a combination of two approaches : ( 1 ) a distance - driven ( inverse ) dynamics loss and ( 2 ) a reward - based approach. The former is based on the observation that the agent has access to a set of environments that are generated by the agent, and ( 3 ) a policy that aims to maximize the distance between the agent and these environments.    The paper shows that both approaches fail to converge to the same representation when the agent is placed in a fixed number of environments ( i.e., the inverse approach fails to learn the state representation ). To address this problem, the authors propose a pseudo - metric that measures the difference between the trajectories of agents that start in a given state and those that end up in a state that is drawn from a distribution that is invariant to both the initial state and the state of the environment. The paper also shows that this metric is a better indicator of the agent ’s inability to differentiate between environments than the distance metric."
SP:c002c20b5e8696588e029c0f65e88860418826c4,"This paper proposes a new pre - training framework for two - level classification tasks ( MLM and ICT ) based on transformer - based retrieval models. The main idea is to divide the training data into two parts, i.e., tokens and documents. The first part is to pre - train the two - layer transformer model $ \mathcal{T}$ and $ \cal{L}$, and the second one is to train the $ \text{X}$-layer attention model $ { \tilde{X } \log(T_{\text{L})}$ on the corpus of documents $ { x}$ for $ { y}$.    The authors propose two different ways to do this :   1 ) to score the tokens $ \log{x}$ based on the size of the documents $ x$ in the corpus, and 2 ) to re - retrieve documents $ y$ from the source corpus $ { z}$ by the similarity between the source - level and the target - level representations $ y$. The authors show that the proposed framework outperforms the baselines by a large margin."
SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"This paper proposes a framework for flexible and adaptable graph convolutional and pooling operations that can be used for data on irregular domains. Specifically, the paper proposes strided and transpose convolution operations to be used in addition to the standard bilinear pooling and expand layers. The paper also provides a flexible and configurable hyperparameterized pooling layer that can handle arbitrary number of parameters."
SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"This paper proposes a few - shot classification framework that generalizes to a larger set of domains than previous works. The proposed framework consists of two components : feature - wise transformation layer and feature embedding layer. The feature embeddings consist of a set of hyperparameters $ \theta$ and $ \tilde{\theta}$, where $ tilde$ is the number of tokens in the token set $ \text{x}$. The key idea of the proposed framework is to generalize to a wider range of domains by using the same set of tokens $ x$, $ y$, and $ z$ in the transformation layer.    The authors show that the proposed model can generalize well to a wide variety of domains."
SP:df46627cb984a56bba36d510bfc52e00751e9107,"This paper proposes a framework for convolutional fluid simulation based on the PlasH framework. The main contribution of this work is to provide a theoretical analysis of the properties of convolutionsal networks under the assumption that they are deformable and invariant to perturbations of the underlying geometries. In particular, the authors show that there are two types of deformable networks that are invariant under perturbation of the geometrical structure of the initial data points.    The paper also provides a proof - of - concept implementation of the proposed framework."
SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"This paper proposes a new batch - ensemble neural network learning method, which is based on the idea of having a small number of workers and a large amount of samples. The authors show that the proposed method can be used for a range of learning tasks, and provide theoretical guarantees on the uncertainty of the resulting neural networks. The paper also proposes a memory - efficient version of the batch ensemble method."
SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,This paper deals with the problem of finding a smooth solution to the second order differential equations of the form $ \mathbb{P}(\sqrt{T})$ where $ \text{T}$ is a smooth convex function with respect to $ \alpha$ and $ \gamma$ is the convex hull of $ \lambda$.    The authors propose a method based on the finite element method ( FEM ) to solve this problem. The key idea of the method is to first obtain a solution to a smooth function $ \log(t)$ that satisfies a certain condition on $ t$ and then to use this function to solve the inverse problem of the first order differential equation ( $ \phi$ ). The authors prove the convergence of the FEM method to the solution of both the forward and inverse problems.
SP:973d0ad0faadcf7298300f2758de9154205e7113,"This paper proposes a learning procedure to train neural networks for reasoning about cosmological constants. The main idea is to train deep neural networks such that they are able to answer the following types of queries :   1.    $ \ell_tilde{x } \in \mathbb{R}$ \ell_{\ell_0}$,   \in { x_t}$ $ x_0$ \in   { y_t } $ \in{x_0 } $, \text{quantization}$   2. $ \equation{max}$ q_t$ and $ \sqrt{\ell_\infty}$ p_t$.   3. \epsilon_{\text{max } } Q_t(x_t|x_1)$, \tilde { \in\mathbb { R}$ ( q_0, q_1 )   4. \in{\text{min } } q_i$,\tilde   q_p(x_{\sqrt{max})$ \nablaq_i(q_t | x_1,q_0 ) \inftyq_j$."
SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"This paper proposes a message - passing framework ( GNNmp ) for graph neural networks. The main idea is to train a graph neural network such that it is capable of passing messages to a machine in a closed form.   The main contributions of the paper are as follows :   1. Introducing a GNN - based framework, which is able to pass messages to any machine in closed form ; 2. Conducting deep learning on this framework ; 3. Demonstrating the effectiveness of the proposed framework on a variety of tasks."
SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"This paper proposes a new generative flow - based method for estimating the likelihood of distributions from continuous bijections of continuous flows. The key idea is to use a bijectivity - based generative model that is locally localised on the target distributions.   The main contribution of this paper is to show that the generative flows can be used to estimate the likelihoods of distributions that can be mixed with differentiable distributions. This is done by introducing a new variational scheme, RealNVP ( 4 ), where the target distribution is a mixture of continuous distributions from differentiable flows, and the goal is to find a model that minimizes the difference between the log - likelihood of the two distributions."
SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"This paper presents the results of a series of experiments designed to provide semantic information about the agent model. The experiments include :    1. re - splitting the original dataset into two parts, 2. semantic views of the original dataset, and 3. replacing the original data with a set of semantic views.   Experiments were conducted in the following steps : 1. pre - training agent model was trained with the same set of data as in [ 1 ] ; 2. the semantic views were re - divided into two sets, one with data from [ 2 ] and the other with the new semantic views from [ 3 ] ; 3. the visual appearance of the replaced data was compared with the one from [ 4 ], and the results show that the replacement data was more informative."
SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"This paper proposes a feedback - based learning framework for RL agents to navigate in Robotic and virtual environments. The proposed framework consists of two components :   ( 1 )   a feedback system that learns to identify and classify actions in a Robotic environment, and ( 2 ) a reward function that encourages the agent to take actions in environments that are “ safe ” in terms of their perceived risk / reward.    Experiments are conducted on grid - based Robotic environments ( e.g., grid world ) and in virtual environments with a variety of physical environments. Experiments show that the proposed feedback system is more effective than prior work on both types of environments."
SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"This paper proposes a minimal - entropy - positive colour - colour reduction and colour - classifier framework for classification. The proposed framework is based on a combination of colour - sensitive colour - based classifiers ( e.g., [ 1 ] ) and classifiers trained on a set of human and machine classifiers.    The main contributions of the paper are as follows :   1. Reducing the colour - sensitivity of colour sensitive classifiers by a factor of 0.5 - 0.6 ; 2. Introducing a new classifier, [ 3 ], which is trained on colour sensitive and classifier - sensitive images ; and 3. Demonstrating the effectiveness of the proposed framework on a variety of image classification benchmarks."
SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,"This paper presents a theoretical analysis on the robustness to perturbations placed in the space under the assumption that the perturbation distribution is deterministic and stochastic with respect to the dimension of the perturbed vector space.    The paper provides theoretical results on the following :   1. Under the assumption of deterministic distribution, the paper shows that there exists a window $ \mathcal{L}$ such that for any $ \lambda$, there exist $ \gamma_t$ and $ \log(t)$.   2. The paper presents theoretical results showing that under certain assumptions on the dimension $ t$, the probability that the vector space is perturbed is bounded by a constant factor $ \sqrt{\sqrt{L } \log p}$, where $ p(t|\log p)$ is the number of tokens in the input vector space, and $ p(\lambda_t ) is the maximum value of the vector that can be recovered from the input. 3. This paper provides a theoretical result showing that the probability of recovering a given vector under the above assumption is bounded inversely proportional to $ \alpha_t$."
SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,This paper presents a supervised learning framework for 3D feature extraction from video streams. The proposed framework consists of three components :    1.   feature extraction. The first component is a generative network that maps 3D features to a 2D spatial feature space. The second component is an object detector that learns to detect 3D moving objects. The 3D object detectors are trained using a combination of supervised learning and unsupervised learning. The third component is supervised learning of 3D dynamic scenes. The dynamic scenes are generated by generating 3D video streams that are static and dynamic. The video streams are used to train the 3D visual recognition and object detection components. The experiments show that the proposed framework achieves state of the art results.
SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"This paper studies the problem of unsupervised natural language translation ( UDT ) under the assumption of timestep invariance. It is well - known that this problem is intractable, and this paper aims to improve upon this by providing a theoretical analysis of the problem in the framework of the Optimal Transport ( OT ) framework. The main contribution of this paper is a theoretical formulation of the OT problem in terms of energy transformations, which is then used to provide theoretical guarantees on the cost of UDT under the OT framework."
SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"This paper proposes a regularization method for the dropout - and - dropout scenarios. The main idea is to regularize the network in two stages, i.e., in the first stage, the network is assumed to be static, and in the second stage, a new layer is added to the network, which is supposed to be adaptive to the new layer.    The main contribution of this paper is to provide a theoretical analysis of the regularization of the two stages. Specifically, the paper shows that there exist two types of regularization methods for dropout and dropout, and that the first one is noise - based, and the second one is regularization - based."
SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"This paper proposes a new vector - based optimization method, UAP, for improving the quality of the data - free activation function of the neural network in the case of adversarial perturbations ( UAP ).   The key idea is to replace the $ \ell_t$-norm with a $ t_t $-norm, which is the norm of the perturbation, by $ t_{t_t } \infty$, where $ T_0$ is a positive integer and $ T_{t-1}$ is the vector - valued vector. The authors show that $ t-1$ can be approximated with $ t+\ell_0$.    They also show that UAP can be used to improve the value of the activation function for the case when $ t=1$."
SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"This paper proposes a transferable Neural Architecture Search ( NAS ) method for neural architectures to be adapted to different tasks. The main idea is to search for architectures that can be transferred to new tasks without changing the architecture of the original task. The authors claim that the proposed NAS method is more transferable than existing methods that do not adapt to the new task.    The main contributions of the paper are as follows :   1. Introducing a new NAS method, named T - NAS ; 2. Demonstrating the transferability of NAS ; 3. Conducting experiments on a variety of tasks to demonstrate the effectiveness of NAS."
SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"This paper presents a theoretical analysis of the VIB ( Variational Information Blahavior Uncertainty - Perceptual Information Bottleneck ) in neural networks with label noise and regularized learning. The paper shows that VIB can be regarded as an information bottleneck that prevents the network from learning robustly under label noise. To alleviate the information bottleneck, the paper proposes a regularization - based activation prior and a compression - based regularization prior. Experiments show that the proposed activation prior is more effective than the regularization regularization and the compression regularization."
SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"This paper presents a series of papers about curiosity algorithms for meta - learning ( e.g., R - learning ) with the goal of providing a better understanding of the properties of the learned algorithms. The paper presents two main contributions : ( 1 ) a proof - of - concept search space for curiosity algorithms, and ( 2 ) a set of experiments to show that curiosity algorithms can be made to behave more “ curiosityally ” with respect to input and output spaces. The experiments are based on the following components :   1. Showing that the learned curiosity algorithms behave more curiosityally than the un - learned ones. This is done by showing that given a fixed set of inputs and a fixed number of outputs, the curiosity algorithms will behave more / less curiosityally as a function of the number of inputs / outputs and the amount of tokens / tokens in the input / output spaces, respectively. This shows that there is a trade - off between having access to higher - level knowledge about the structure of the programs and the ability to “ navigate ” through them.    2. Providing a more complete understanding of how the learned mechanisms behave in terms of their curiosity properties, and showing that they are “ safe ” to navigate through, is also shown to be helpful for improving the performance of un - learnt algorithms. In particular, this paper shows that if we restrict the search space to programs that can be navigated through ( i.e., programs that do n’t contain any inputs or outputs that are not too different from the ones, we can improve the performance by a large margin."
SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,This paper proposes a new approach to the problem of representing data in the syntax of programming languages in terms of a model of the source and target data domains. The main contribution of this work is to provide a model - free approach to represent data in programming languages that can be used in real - life coding applications.    The main contributions are two - fold : ( 1 ) provide a theoretical framework for understanding the relationship between the probability of paths traversed by a language model and the number of lines of code used in the model ; and ( 2 ) provide an approach for generating code in languages that do not have such a model. The approach is based on the fact that the source - target relationship between two languages can be represented as a weighted sum of probabilities of the total length of a line of code and the length of the shortest path between two input sequences.
SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"This paper studies the problem of minimizing the zero - loss / rank condition of convex gradient descent for large scale neural networks ( NNs ). The main contributions are two - fold :    1.   This paper provides a theoretical analysis of the convex - gradient descent problem for NNs and shows that there exists a linear transformation $ p(x, z)$ such that $ z \in \mathbb{Z}$ can be approximated with $ \sqrt{Z } \log p(z)$, where $ z$ is the number of parameters in $ z$.   2. This paper gives a theoretical proof of the existence of $ z\infty \log z$ for $ z=1 $ NNs."
SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"This paper proposes a new supervised learning framework based on multi - label graph - based segmentation. The proposed approach is based on a multi - component set - based classification model, where each component is composed of a set of annotated data points, and the goal is to obtain a supervised learning algorithm that is able to segmentate the data sets in a way that is “ safe ” in terms of the size of the data set and the quality of the labels.    The authors propose two different approaches to this problem. The first one is a “ naive ” approach that does n’t take into account the different types of data sets. The second one is an “ active ” one that takes into account both the number of data points and the types of labels. The experiments show that the proposed approach performs better than the naive approach."
SP:2eb90879ddbc39b6b5c05152784d6044d1940513,This paper proposes a new method to defend against black - box and white - box attacks based on pixel - based saliency models. The proposed method is based on gradient - based adversarial perturbation models. This paper provides a set of examples to demonstrate the effectiveness of the proposed method.
SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"This paper proposes a local robustness / accuracy trade - off between two neural networks ( NNs ) under adversarial attacks. The main contribution of the paper is to provide a sound estimation of the robustness profile of the two NNs, which can then be used for the design of a better network generalisation. The paper also provides a theoretical analysis of the trade off between the two networks, which is used to justify the choice of the global robustness metric. Finally, the paper provides a thorough experimental evaluation of the proposed metric, which shows that the proposed local metric is more robust than the global metric."
SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"This paper studies the optimal Bellman equation in the setting where the goal is to learn a policy that is robust to perturbations in the dynamics of the environment. To this end, the authors propose to solve the problem in two ways. First, they consider a data - driven setting, where each point in time is sampled from a set of $ \mathcal{x}$ points $ z$. Then, they assume that $ z$ is the set of all points in time $ z_t$, where $ z_{t } = z_0 $ is the number of points in the set and $ t_{t}$ is a constant that controls the probability that the set is perturbed.    The authors prove that the optimal policy can be expressed as a convex combination of two terms : $ \alpha$ and $ \beta$. The second term is phrased as an upper bound on the dimensionality of the world $ \sqrt{\sqrt{delta } \log p}$.   They provide a theoretical analysis of the optimal risk - aware policy and show that it satisfies the Bellman inequality $ \ell_t\log p(z_{t})$ where $ \infty$ is bounded by $ \epsilon_{t_t } \leq p(Z_{t-1}$. They also provide a proof that the policy is optimal in the case when $ \eta$ is large. Finally, they provide an empirical study of the robustness of the proposed policy."
SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"This paper proposes a new strategy - based continuous games framework, which aims to reach Nash equilibria in multi - player continuous games. The framework is based on the gradient descent algorithm.    The key idea is to assume that the strategy of each player is in a Nash equilibrium with the rest of the players in the game. The authors prove that this assumption is sufficient to ensure the stability of the strategy in a finite set of games. To achieve this goal, the authors propose two approaches. First, they assume that each player knows the other player's strategy, and the second approach is to learn a strategy that satisfies both assumptions. The paper shows that both approaches converge to the same Nash equilibrium when the number of players is large enough."
SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"This paper presents a semantic parsing - based model learning framework for natural language processing ( NLP ) tasks. The proposed framework is based on the Neural Execution Tree ( NExT ) framework, where natural language ( NL ) explanations are provided in the form of logical forms, and the model is trained on a dataset of natural language forms and question answering examples.    The paper presents two main contributions :   ( 1 ) a data - driven model learning strategy, where the model learns to extract logical forms and questions from the data, and ( 2 ) a training algorithm, where a subset of the data is annotated with NL explanations and the remaining data is used for model learning."
SP:a9b5f7257dedd719cfe341fca275776734af1d98,"This paper proposes a new approach to verify the robustness of RNNs. The key idea is to train a RNN on a set of input features, and then verify the RNN's robustness to perturbations in the input features. The proposed approach is based on the idea of “ verifiable training ”, i.e., training the model on the set of perturbed input features and then verifying the model ’s properties against the perturbation set.    The main contributions of this paper are as follows :   ( 1 ) This paper proposes an approach to verifiably verify the properties of the model trained on perturbed inputs. This is done by introducing a new metric, “ $ \tilde{x } \in \mathcal{x}$, which measures the ratio between the number of tokens used in the training and the number used to validate the model. ” ( 2 ) This metric is then used to train the model to be more robust. The paper shows that the proposed approach can be used in a variety of deep learning models, e.g., neural networks with regressive regressive weights."
SP:3903680e07b676409e3cf6a1044b67291fe38630,"This paper proposes a new approach to the problem of domain randomization in reinforcement learning. The main contribution is a theoretical analysis of the problem and an off - policy approach to solve it. The theoretical analysis shows that the randomization problem is intractable, and proposes two approaches to address it : ( 1 ) a policy - based randomization method and ( 2 ) a randomization - based approach. The proposed approach is empirically shown to outperform the state - of - the - art approaches in a variety of domains."
SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"This paper proposes a new framework for metric learning ( DML ) with the goal of providing data sets that are safe against binary classification loss. The framework is built on top of the existing DML framework that has been proposed in [ 1 ], [ 2 ] and [ 3 ]. The main contribution of this paper is to provide a data set that satisfies the following guarantees :    * * Sec. 3. Sec. 4. Robustness of the data sets * * Under the assumption that the set of data points is not too different from the original data set ( Sec. 5 ), the framework provides a set of metrics that is safe against classification loss under the following assumptions : * * datapoints * * $ \ell_0 $, * * data sets $ \infty$ and * * set size $ \tilde{x}$, where $ x}$ is the number of elements in the data set and $ y$ is a binary variable that characterizes the similarity between two data points.   Experiments are carried out on the following benchmarks : [ 5 ]   [ 6 ] [ 7 ] [ 8 ] [ 9 ] [ 10 ] [ 11 ] [ 12 ] [ 13 ] [ 14 ]"
SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"This paper studies the problem of constructing trust - region approximations for the Hessian - free stochastic gradient estimator ( STR ) and its variants. The main contributions are as follows :    1. This paper provides a theoretical analysis of the complexity of the first - order and second - order Hessian estimators and shows that they are non - convex in the sense that they do not converge to a convex finite sum in the range $ \mathbb{R}^n$, where $ n$ is the number of variables and $ \text{K}$ is an integer.   2. The paper presents a theoretical result showing that the local minimization of $ \tilde{K } \infty$ is not convex, and gives an upper bound on the rate at which it converges to a finite sum. 3. In the process, the paper gives a lower bound on $ \nabla_{k } \log n}$ for the variance of the estimator $ \gamma$.   4. In addition, this paper provides theoretical guarantees for the convergence rate of the trust region estimator and its Hessian variants."
SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"This paper presents a theoretical analysis of the problem of initialization of neural networks with different number of tokens / tokens / layers. The main contribution is a proof - of - concept proof that the number of token / token / layer sizes does not increase with increasing number of layers / layers, but rather increases with increasing layer size.    The paper also presents an analysis of different initialization strategies for different layer sizes, and shows that there is a trade - off between number of layer sizes and initialization rates."
SP:1d325b148e3efe407241c1f1cbe8d17400499741,"This paper proposes a new type of Robustness Certifications ( CRT ) based on differentiable activation functions. The main idea is to obtain a certificate that is differentiable in the sense that it does not depend on the metric used to define the metric ( e.g., metric used in the standard adversarial training ). The paper also proposes a differentiable regularization term that can be used to improve the quality of the certificate.    The main contributions of the paper are as follows :   1. Introducing the concept of differentiable curvature bounds. 2. Proposing a new classifier that is not only differentiable but also convex. 3. Providing a new certificate based on this classifier. 4. Demonstrating the effectiveness of the new certificate."
SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"This paper presents an approach to solve the inverse problem of training deep generative models with a single layer network. The main idea is to train a generative model with a linear inverse problem, where each layer contains a set of data points and the goal is to minimize the error incurred by each data point as a function of the other data points in the network. To this end, the authors propose a new regularization method, DIP, and a compressed sensing recovery method. Experiments show that the proposed approach outperforms the previous methods in terms of performance."
SP:23c0f621e6041003b59bf0532130760694cf6a4a,"This paper studies the problem of learning a temporal abstraction of action sequences in reinforcement learning ( RL ). The authors propose two approaches to this problem : ( 1 ) abstracting the action sequences into a set of variables, and ( 2 ) learning a closed - loop sub - policy based on these variables. In the first approach, the decoder is given access to the set of actions and actions sequences, and the goal is to find a policy that minimizes the difference between the number of actions in the sequence and the total number of steps in the policy.    The authors prove that both approaches converge to the same solution under certain assumptions. In particular, the first assumption is that the sequences of actions are not too different from each other, and that the policy doesn't need to know the distance between the sequences. The second assumption is sufficient to ensure that the sub - policies don't diverge more than a certain threshold."
SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"This paper studies the problem of learning the representation of the first - order and higher - order proximities to a given set of nodes in a graph using a bi - directional diffusion - based approach. To this end, the paper proposes two approaches : ( 1 ) to first order the distance between the first and higher order vertices in the model, and ( 2 ) to higher order the correlation between the topology of the input graph and that of the source graph.    The first approach is based on the following two components : ( a ) first order layer - wise sampling from the source and target graphs, ( b ) higher order layerwise sampling from a set of topologically disjoint subgraphs. The authors prove that both approaches converge to the same representation when the number of samples is large enough."
SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"This paper presents a video - based model - free and model - based control framework for state - space video modeling and planning. The proposed approach is based on a combination of three components :   1.   a state space model. This is a generative model that consists of a set of trajectories, velocities, and positions. 2. an object - aware model that is able to predict the properties of the objects in the domain. 3. a policy and value network that learns the relationship between the model and the state space. The policy network is trained on top of a supervised model that has been trained on a dataset of videos."
SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"This paper studies the problem of training adversarially trained generative adversarial networks ( GANs ) with finite - dimensional distributions. The authors show that it is possible to train a GAN with finite dimensional distributions such that the model will converge to a finite dimensional representation of the world in a finite time. This is done by minimizing the likelihood of the model collapsing to zero as a function of the dimensionality of the distributions.    The authors provide a theoretical analysis of the failure of this approach. They show that the failure is caused by the fact that the distributions of the generative models are not uniformly distributed over the space of distributions, which results in a divergence in the quality of the representations of the learned discriminator. They also show how to overcome this problem by augmenting the distribution of the discriminator with a distribution that is more complex such that it covers a larger set of distributions."
SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"This paper studies the vulnerability of CNN classifiers under the following assumptions : ( 1 ) the data distribution of the classifiers is linear in the dimension $ \ell_0 $, ( 2 ) the number of dimensions is large, and ( 3 ) the classifier is optimal in the sense that it is not sensitive to the dimensionality of the data distributions.    The paper shows that the default default classifier of CNN is not safe under these assumptions, and proposes a new classifier that is more robust to these assumptions. The proposed classifier has the following properties : ( a ) it does not depend on the dimension of the input data $ y$, ( b ) there is no linear extrapolation between the distribution of $ z$ and the set of $ a$ classes, ( c ) there are no data distributions that are not orthogonal to $ z$."
SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,This paper presents the results of a series of experiments aimed at improving the quality of the top - quality images produced by the CIFAR-10 model.    The main contributions are as follows :   ( 1 ) a set of 10 models is used to train the top-1 test set ; ( 2 ) the model capacity of each model is evaluated ; ( 3 ) a model capacity analysis is performed ; and ( 4 ) the results are shown to be in good agreement with previous work.
SP:4b17edaa7ec6201891433320d85f9a415656b763,"This paper presents an action space - based reinforcement learning agent for natural language text - based action generation.    The main contributions are as follows :   1. An action space space of size $ \mathcal{KG - A2C } $ $ is constructed for each action in the action space of a natural language agent $ A$, which is then used to train an action - based agent $ G$. 2. This space is used to learn a contextually aware natural language understanding of the action $ G$.   3. A set of games is provided to train the agent. The agent is provided with a set of actions and a graph of actions. The goal is to learn an agent that is able to simulate actions in the space of natural language actions."
SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"This paper studies the problem of summarization summarization and unsupervised learning of text summarization tasks. The authors propose a data - dependent Gaussian prior - based pre - training for summarization task, which is used in conjunction with a pre - trained generative model ( G2GP ). They show that the model can be fine - tuned in two stages. First, the model is pretrained on a corpus of text - only documents, and the second stage is pre - tuned on a larger corpus of documents. The model is shown to outperform the generative models in terms of the number of tokens and the quality of the model predictions."
SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"This paper presents a theoretical analysis of the scaling of the cross - entropy loss on multi - class classification networks with different number of classes. The paper shows that the scaling can be understood as a combination of two factors : ( 1 ) the number of classifiers and ( 2 ) the dimensionality of the classifier space.    The paper proposes two approaches to this problem. The first approach is to divide the dataset into two parts, one for classification and one for distribution. In the second approach, the network is divided into two subsets, i.e., the classification and distribution part, and the distribution part is the distribution. The authors show that the loss on the classification part scales as a power - law function of dimensionality and dimensionality."
SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,This paper presents a theoretical and empirical proof of the positivity certificates of various neural network optimization algorithms under different assumptions on the network structure and the number of parameters.   The main contributions of the paper are as follows :    1. The authors provide a theoretical analysis of the different types of constraints that can be imposed on the networks in terms of their weights and dimensionality ; 2. They show that there exist networks for which there exists a constant set of parameters that guarantee the correctness of the algorithms under certain assumptions ; 3. They provide theoretical results showing that there exists an upper bound on the dimensionality of the network under which the algorithms are guaranteed to satisfy certain constraints ; 4. They prove that the upper bound is upper bounded by a constant factor that depends on the number and dimension of parameters of the networks.
SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self - supervised learning approach for improving the quality of video classification and segmentation for downstream video tasks. The proposed approach is based on a cross - modal video classification framework, where a video is divided into several segments and the goal is to classify each segment based on the similarity between the features of the two sequences. To this end, the authors propose two approaches :   ( 1 ) VQA - based classification and ( 2 ) ImageNet - based segmentation. The experiments show that the proposed approaches outperform the state - of - the - art approaches in terms of quality of classification, segmentation, and contrastive quantization."
SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"This paper describes a data - transfer strategy to transfer data from the public storage server to the server for data - intensive domains such as sensing and physics domains.    The main contribution of this paper is a new data transfer strategy based on a combination of data - compressibility, data - distillation, and data - filtering strategies. The proposed strategy is based on the following steps : ( 1 ) data is compressed to a few tens of bytes, ( 2 ) the data is transferred to a domain - specific network, ( 3 ) the network is partitioned into domains, and ( 4 ) a data layer is placed on top of each domain to allow the data to propagate through the network."
SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,This paper proposes a new classification method based on distance - based neural networks. The proposed method is based on the concept of out - of - distribution ( OE ) detection. The main contribution of this paper is to provide a theoretical justification of the OE detection function. The paper also provides a theoretical proof that the proposed method can be used to train a neural network with a differentiable loss function. Experiments are conducted to show the effectiveness of the proposed detection method.
SP:89bc528ef801182365ac279e8963803afccb391d,"This paper presents a theoretical analysis of the time - varying structured structure prediction problem in deep learning with constrained programming. The paper shows that the structured structure problem can be phrased as follows : given a set of data points $ \mathcal{T}$, the goal is to predict $ \tilde{T } \infty$ the base - pairing matrix $ \text{text{data}$ of $ t$ and $ tilde{text}$ the $ t}$-value $ t$. The paper proposes two approaches to solve this problem. The first approach is to train a model $ T$ with $ t\infty$. In the second approach, the authors propose to pre - train the model $ t $ with a constraint on $ t_t$ that prevents the model from overfitting $ t_{t}$ to $ t+1$.    The paper provides theoretical analysis showing that both approaches fail to solve the structured prediction problem under the same set of constraints."
SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"This paper presents a policy - based virtual simulation of the real - world environment. The goal of the paper is to create a policy that can be used in a variety of scenarios in the real world. To this end, the paper proposes two approaches : ( 1 ) train a policy in a real world scenario and ( 2 ) simulate a virtual world scenario using a trained policy.    The first approach is based on training a policy with a fixed number of policies and a fixed set of actions. The second approach uses a policy trained with a different number of actions and a different set of policies to simulate a different world scenario. In both approaches, the policy is assumed to be invariant to the initial state and to the actions taken by the other policies. Experiments are carried out in two settings :   1 ) a simulated world scenario in which the agent is allowed to explore a large variety of environments ( e.g., space, time ), and 2 ) a simulation in which there is only one environment ( i.e., time and space )."
SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"This paper studies the problem of generating generic latent space - space - based responses to the one - to - one task of generating domain - level dialogs under the assumption that the target domain is a space - free domain. To this end, the authors propose an autoencoder - based framework that learns a latent space representation of the source domain, conditioned on the source and target domains, and is able to generate a set of responses that satisfy the following conditions : ( 1 ) the source is a domain free domain, ( 2 ) the target domains are at least as expressive as the source domains, ( 3 ) the domain is not too different from the source, and ( 4 ) the goal is to generate responses that are compatible with the source - domain relation."
SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"This paper proposes a new visual explanation method for the classification of black - box and light - shadow images. The key idea is to use a Gaussian mask to explain the visual similarity between the two types of images. To this end, the authors propose a new classification dataset and fine - grained classification method. The proposed framework is based on the large scale visual recognition challenge ( LSVRC ). The main contributions are as follows :    1.   A new framework is proposed for the visual explanation of colour - magnitude - scale visual classification. 2. A new classification method is proposed based on a black box model. 3. This framework is used to provide a visual explanation for the light and shadow visual similarity."
SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,This paper proposes a deconvolution layer for neural network training. The proposed layer consists of two layers. The first layer is used to generate a representation of the input data. The second layer takes as input the original data and converts it into a vector representation. The authors claim that this layer is more efficient than the first layer in training neural networks.
SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"This paper proposes a quantization method for GANs based on multi - precision quantization. The proposed quantization approach is based on minimizing the number of bits required to obtain a given set of representations. The paper provides a theoretical justification of the quantization metric and empirically shows that the proposed approach is more scalable than previous quantization methods such as bit - bit, bit - max and bit - min."
SP:58c4905f59f04a50b30d27c99521126a6455d38a,"This paper studies the problem of training a generative adversarial network to solve convex convex optimization problems with finite - sum structure. The authors prove convergence rates for the convex - convex regularizer and concave min - max settings under the following assumptions : ( 1 ) the problem is convex, ( 2 ) the set of solutions is concave, and ( 3 ) the number of iterations per iteration is bounded by a constant factor."
SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"This paper provides a theoretical analysis of the convergence properties of the normalization layer of the vanilla feedforward network under the assumption that the network is not too differentiable. The analysis is based on the following assumptions :    1.   * the network structure is invariant to the number of layers *   2. * each layer has the same number of parameters * 3. * every layer has a similar number of minima * 4. * for each layer, there exists a threshold value $ \theta$ such that $ \tilde{x } \in \mathbb{R}$.   The paper provides theoretical analysis on the convergence of the above assumptions under these assumptions for the case when the layer size is less than $ \infty$ and the network size is of size $ m$."
SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"This paper presents a theoretical and empirical study of the time - horizons of sparse neural network training. Specifically, the paper focuses on the following :    ( 1 )   how quickly can we learn sparse neural networks from sparse data?   The paper proposes a new metric, $ \tilde{inf } \infty$, that quantifies the time it takes for a sparse model to converge to a sparsity - invariant set of data points given a fixed number of training iterations.   This metric can be used as a metric to measure the amount of time required to learn a sparse network from a sparse dataset. ( 2 ) how quickly we can learn sparse networks from a dataset given a set of sparse data points $ \nabla_{t } \times t } $, and how much we need to update the sparse model in order to ensure that the sparsity of the data points is not too different from the number of iterations required to train the model. The paper presents theoretical results showing that the time required for sparse network training scales linearly in $ \mathcal{inf}$, and empirically shows that this scaling is inversely proportional to the size of the dataset ( $ t$ ), with a maximum value of $ \sqrt{T } \log n(t+1/\log t)$, where $ t+1 $ corresponds to the dataset size, and $ t-1 $ \log t$ corresponds to how many iterations needed to learn the model in terms of $ t."
SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"This paper proposes a generative model - based text - to - image conversion, which can be used to improve the quality of the text - text ( text - visual ) encoding and decoding.    The main contribution of this work is to provide a model - free, generative encoder - decoder that can be trained on top of a pre - trained text - vision encoder and decoder. The decoder is trained on a set of generative models, and the decoder uses the learned decoder to decode the model."
SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"This paper presents a video - based model - predictive control for physical systems, which is based on a vision - based video prediction. The video is divided into two parts : the first part is a static video and the second one is a dynamic video. The goal of the video is to predict the future frame of a given physical system.    The video consists of three parts : ( 1 ) the static video, ( 2 ) the dynamic video, and ( 3 ) the future video, which represents the future state of the system in terms of the current state and velocity of the objects in the scene. The videos are divided into 4 parts :   ( i ) the video segmentation, ( ii ) the object segmentation and ( iii ) the velocity segmentation. The visual segmentation is done by using a linear combination of the actor - critic model and the model - based predictive control. In the video section, the model is used to predict both the current and future frames of the physical system, while in the dynamic section, it is only used for the future frames. The authors also provide a short - term extrapolative video prediction and a long - term predictive control, which are used to improve the performance of the model."
SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"This paper proposes a new classifier based on cross - entropy loss function to distinguish clean and noisy images. The proposed method is based on few - shot classification and cleaning. The clean examples are generated by randomly picking a subset of clean examples from a set of noisy examples, and the classifier is trained to distinguish the clean from the noisy examples by minimizing the probability that the clean example is the same as the noisy example.    The authors show that the proposed method outperforms several previous works on the clean vs. noisy image classification problem."
SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"This paper presents a knowledge graph regression approach for solving unsupervised stochastic learning problems. The goal is to find the optimal set of data points for each iteration of the learning problem. To this end, the paper proposes a set of knowledge graphs $ \mathcal{x}$ and sets of benchmarks $ \text{max}$, $ \log{sqrt{t\log n}$ where $ n$ is the number of iterations of the problem and $ \tilde{x } \log n\log p(n)$ is a metric used to measure the uncertainty in the knowledge representation of the data points.    The paper proposes two approaches to this problem. The first approach is based on the concept of knowledge graph entropy and the second one is on the notion of knowledge entropy. The paper provides a theoretical analysis of both approaches and empirically shows that both approaches lead to similar results."
SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"This paper studies the problem of generating generative models with non - convexity and relaxation properties. Specifically, the paper focuses on the question of how to construct generative networks that satisfy the following properties :    1.   * * Sec. 3. * *   Sec. 4 : * * Relaxation properties * * * of generative network * * in terms of their latent space spaces * * 2. *   Sec. 5 : * Qualitatively * * Qualitative properties   of the latent space transformations   obtained in Sec. 6 : *   Qualitative property of the generative model   under the following assumptions   ( i.e.,   that the network is convex and that it is not over - parameterized )   is presented."
SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"This paper studies the problem of ensuring the continuity of the representations of the GNN ( Graph Residual Network ) framework in the presence of changes in the underlying structure of the underlying network. Specifically, this paper focuses on the following :    1.   The authors prove that the representation of GNN is not invariant under changes in underlying structure ( e.g., changes in topology, topology of subsets of nodes ). 2. The authors show that this is due to the fact that GNNs do not preserve the invariance of the network topology under change in structure. 3. They show that there is no guarantee that the representations preserved by GNN can be made invariant to changes in structure of underlying networks. 4. They provide a theoretical analysis of the problem and show bounds on the number of iterations required to ensure invariance under change of topology and topology."
SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"This paper presents a new method for reconstructing 3D cross - over representations from unlabeled and labeled face images to 3D morphable representations. The key idea is to use a convolutional neural network ( CNN ) with a linear parametric model ( 3DMM ), which is trained on a set of 3D datasets, and a cross - entropy loss that maps the input 3D images to a 3D vector representation.    The main contributions of the paper are as follows :   1. A new method to reconstruct 3D face images in a supervised manner. 2. A set of cross entropy loss functions that are trained on top of the proposed CNN. 3. A series of experiments that demonstrate the effectiveness of the reconstruction method."
SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"This paper studies the problem of learning a model - free alternative to the standard model - based imitation learning approach. The paper proposes a new approach to this problem, which is based on learning the kernel of the interaction between two components of the model. The authors show that the proposed approach is able to avoid some of the timesteps of the standard approach.    * * Update after authors'response * * : I am happy to see the response of the authors positively to my suggestions. However, I am not entirely convinced that this approach is a solution to the problem, as it does not take into account the fact that the two components are not mutually exclusive, and that there is a trade - off between the knowledge of the kernel and the ability to learn an alternative model. * *   Update after author's response : I agree with the authors that the approach does not solve the problem satisfactorily, but that it does provide a better alternative model than the standard one. I am still not convinced that the choice of the alternative model is a good choice, and will have a major impact on the performance of the approach in the future. I will raise my concerns to the authors in future discussions."
SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"This paper presents a reinforcement learning framework for learning information - based robotic manipulation tasks. The goal is to train a policy that learns how to navigate a given set of robotic manipulations. To achieve this goal, the paper proposes two approaches : ( 1 ) a pre - trained policy and ( 2 ) a reward function that is designed to encourage the policy to learn a set of behaviors that are useful to the agent.    The first approach is based on the idea of information discriminator, which aims to provide information about the state of the agent to motivate the goal. The second approach uses a simulator - based reward function, which is trained to simulate the agent ’s behavior in a supervised manner. The experiments show that the proposed approach achieves state - of - the - art performance in terms of knowledge transfer between the policy and the learned behaviors."
SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"This paper proposes a new trojaning attack method, NN - TRT, that is based on dynamic and out - of - scope attacks on a dataset of small - scale domain - level models. The authors claim that the proposed method is more robust than previous works that relied on large - scale data sets."
SP:35ea626ee4dd1a7a368a660eb852192924966b7f,"This paper presents a set of results on the few - shot classification and regression ( FSR ) problems for kernel - based deep learning ( kernel - 8 ) algorithms. The authors claim that the set of FSR problems covers a wide variety of domains and has been used to generate data for a variety of tasks. The paper presents results on drug discovery, classification, and regression problems and shows that kernel 8 algorithms have been shown to achieve state - of - the - art performance in most of these domains."
SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"This paper studies the question answering problem in the setting of neural networks. The authors propose a new paradigm of value difference difference tasks, where the goal is to find a set of vectors $ V$ such that $ \tilde{x}$ can be represented as a sum of $ \mathcal{X}$ and $ \text{x } \log p(x_{\text{value}$, where $ p}$ is the length of the shortest path between two points in the input sequence $ p$, and $ z$ is a vector that maps $ x$ to $ z$.    The authors show that this new paradigm has the following properties :   1.   \theorems 1. \theorem 2. \proposes a lower bound on the number of iterations required to solve the given problem $ \sqrt{\text{v}$ in terms of $ p$. 3.\theorem 4. izenthesis 5. ivesenthesis 6. iventhesis 7. iveenthesis 8. ieventhesis 9. iienthesis 10. iianthesis 11. iiijectives 12. ieveenthesis 13. iventhesis 14. iiiives Presents conjectures and conjectures 15."
SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"This paper proposes a new metric, FJD, that quantifies the diversity in the cross - entropy between two sets of distributions. The paper also proposes a cGAN - based metric, CGAN - DQN, that measures the consistency between two distributions.    The paper provides a synthetic dataset that can be used for benchmarking the proposed metric. Experiments are conducted on the following :   ( 1 ) CGAN dataset, which consists of 5 datasets : 1 ) text captions, 2 ) cGAN dataset that consists of 4 datasets : 2 ) text - textures dataset, 3 ) cross entropy between 2 datasets : 3 ) randomized dataset, 4 ) randomised dataset, and 5 ) unrolled dataset."
SP:fa822e8472efae17c7dfde8258057898383ecbbb,"This paper studies the problem of reaching decision states in a goal - driven sequential exploration framework, where the goal is to find a state that satisfies a set of assumptions ( e.g., that the state is bounded from above by a constant factor that depends on the distance between the initial state and the state that is reached ). In particular, the paper focuses on the following questions :    1. What is the probability that a state exists that satisfies the above assumptions?   2. What are the conditions under which a state can be reached?"
SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"This paper proposes a time series sampling framework for real - world time - series healthcare applications. The proposed framework is based on the idea of sampling time series from a set of data points sampled at different timesteps, and then using the sampled time series as a metric to evaluate the quality of the time series.    The main contributions of the paper are as follows :   1. Introducing the concept of time series sampled and asynchronous time series, and showing that it is not necessary to have a complete knowledge of the metric metric to ensure that the timestep is not too different from the original time series ; 2. Demonstrating that the metric is not a metric, but rather a function of time, and 3. showing that sampling and using time series can be combined to provide a metric that can be used for healthcare evaluation."
SP:4ae89d64460b08749acc192004545c1fa8b7553b,"This paper proposes a neural network - based audio restoration task that aims to achieve the audio - to - text ( audio / text ) separation of the source and the source - sound. The idea is to train neural networks such that they can be used for this task.    The main contribution of this paper is to show that the neural networks can be trained to be able to achieve this separation task. To do so, the authors propose to train a network that has the following structure : ( 1 ) image recognition, ( 2 ) audio processing, ( 3 ) audio source separation, and ( 4 ) network initialization."
SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"This paper presents an analysis of the data - echoing algorithm for neural network training. The paper shows that the average amount of data collected during the training stage is proportional to the total amount of training data collected in the pre - training stage, and that the number of training iterations per iteration increases with increasing number of data samples.   The paper also shows that there is a trade - off between the amount of preprocessing time and the total number of iterations used during training. This paper proposes to divide the training time into two stages, i.e., pre - train and post - train, and show that the former can be reduced to a fraction of the latter by using a pre - trained version of the algorithm."
SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"This paper proposes a new approach to evaluating the quality of the features learned in a pre - trained version of the Atari task suite. The key idea is to divide the task space into a set of possible features and to use a policy to select the features that are most likely to lead to good performance in the given set of features.    The paper proposes two approaches to this problem. The first one is a search - based approach, where the goal is to find a feature that will lead to the best performance in a given subset of the possible features. The second is a policy approach, which is a combination of a search and a feedback approach. The paper evaluates the effectiveness of both approaches in terms of the number of steps taken in the search and the amount of feedback provided to the learner."
SP:83500230586a9134f910ad067b7233dc563dc1ba,"This paper studies the functional smoothness of initializations of deep neural networks. Specifically, this paper focuses on the view of the neural network as a set of functions that are approximated by a finite set of weights. The main contributions of this paper are as follows :    1. Introducing a new view of neural networks that is based on the functional smoothing of their initializations.   2. Demonstrating that this view leads to a new perspective on the initializability of deep networks. 3. Providing sufficient conditions for a neural network to be functional smooth."
SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"This paper studies the problem of image - to - image translation in generative adversarial networks ( GANs ). The main contribution of this paper is to provide a theoretical analysis of the problem in the context of the GAN framework. Specifically, the paper shows that there exist two types of approaches to the image to image translation problem : supervised and unsupervised. In the supervised setting, the problem is shown to be intractable, and the authors propose a new approach to solve it. Moreover, the authors also propose a way to improve the quality of the results obtained by the previous approaches."
SP:41c089ba65393174dae1dc136f79030a0a4fc532,"This paper studies the problem of constructing multiplicative LSTMs that can be used to represent different types of information in different contexts ( e.g. contextual or task - conditional information ). In particular, this paper focuses on the question of how to construct a LSTM that can represent both contextual and task conditional information in a single layer. To this end, the paper proposes two approaches : ( 1 ) to use factorization layers, and ( 2 ) to combine factors in layers. The paper provides sufficient conditions for this to happen."
SP:5144391584e6d3825e12684b7c053e4e282cff2b,"This paper studies the problem of active learning, i.e., how to train a deep neural network with a set of hyperparameters that can tolerate different types of uncertainty in the input data. The paper proposes two approaches to this problem. The first approach is to divide the data set into subsets of different sizes, and then train a neural network on each subsets with different values of the parameters. The second approach is a gradient descent approach, where the neural network is trained on top of the different subsets.    The paper provides a theoretical analysis of the proposed approach, and empirical results are provided to support the theoretical analysis."
SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"This paper proposes a new level - level feature extraction framework for deep neural networks ( DNNs ) based on linear models ( GLMs ). The proposed framework is based on a linear model reasoning process, which consists of two stages : ( 1 ) layer - level features extraction and ( 2 ) feature - level reasoning.    The main contribution of the paper is to propose a new feature level extraction framework based on the linear model ( GLM ) reasoning process."
SP:b70ceead1bf6c7dc684c74501716e7012b891022,"This paper proposes a new sampling mechanism to approximate the $ \ell_sqrt{\sqrt{softmax } \in \mathcal{n } \log n}$, where $ n$ is the number of samples in the training set, and $ \text{n}$ is a positive integer that can be used as a measure of the quality of the samples.    The main contribution of this paper is the following :   ( 1 ) This paper proposes to use negative sampling to approximate $ n\log n$ as a metric to measure $ n \sqrt { softmax } $, and shows that it is possible to do so in a non - uniform manner by sampling from a distribution that is uniformly negative with respect to $ n$. ( 2 ) This is shown to be a good approximation to the true data distribution, and it is also shown to provide a loss function that is not too different from the one proposed in [ 1 ]."
SP:29b52fee83309268d9864f3b1fc3617948577d41,"This paper studies the problem of minimizing the distance between nearest neighbors of a set of points in a datacube with a given set of objectives. The goal is to minimize the cost of finding the nearest point to the source given the set of goals.   The paper proposes two approaches to this problem. The first approach is based on a model - based approach. The second approach is a sample - efficient approach based on sampling from the source set. The main contributions of the paper are as follows :   1. Providing a theoretical analysis of the two approaches, and showing that both approaches converge to the same solution ; 2. Showing that the first approach has a lower cost than the second, and that the latter approach is more efficient."
SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"This paper presents a set of few - shot classification datasets for the OODO and OOE tasks, with the goal of out - of - distribution detection. The dataset is divided into two categories : supervised and unsupervised. In the supervised setting, the dataset is assumed to consist of only a few shots, and the goal is to detect which one of the two classes is more likely to be correct given the other. The paper presents two sets of benchmarks, one for the supervised dataset, and one for a few shot dataset.    The paper also presents a score - checking score for each dataset."
SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"This paper presents a framework for generating non - autoregressive neural sequence models that can be used for natural language understanding tasks such as cross - lingual ( CE ) and time - time ( txt ) translation. The framework includes both directed and undirected models. In particular, the authors propose a new model of sequence generation and a new decoding strategy for CE tasks. The authors also propose a strategy for time - txt and txt - time translation."
SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"This paper presents a two - stage approach to identify two sequences of mathematical expressions in two stages. The first stage is to identify the two sequences $ \mathcal{X}$ and $ \cal{Y}$ that correspond to the first and second stage respectively. The second stage is the analysis of the first sequence and the second one is the recognition of the second sequence.    This paper presents two approaches to this problem. First, the paper proposes to use two different approaches : ( 1 ) look at the expressions in the first stage and then look at them in the second stage. In the first approach, the expressions are first looked at in the beginning and then looked at at the end. The idea is to look at each of the two stages separately and see which one is closer to the other in terms of the similarity of the expressions. The paper also shows that the two approaches are equivalent in the sense that the difference between them is in the ratio of the number of symbols used in the expressions and the difference in the ratios of the symbols used to classify them. This paper shows that this ratio can be used as a metric to measure the similarity between two sequences when looking at the two stage approach. In addition, this paper also proposes a detection algorithm based on two stages and shows that it is better than the one based on one stage."
SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"This paper investigates the problem of cross - entropy quantization of the cosmological constant. The paper proposes a new quantization method, which is based on the concept of quantization time, and shows that it can be more accurate than previous quantization methods.    The paper also proposes a way to improve the quality of the data used for quantization."
SP:74850ad70241948f93fed95ba1f0ac11360437c1,"In this paper, the attention mechanism of the TP - Transformer is presented. The attention mechanism is based on the relation structure between two vectors, and it is used to generate attention maps."
SP:d319df820c6630c409fab32097652a083e8f53ea,"This paper studies the problem of cross - entropy training and testing on artificial neural networks under the assumption that the input features are corrupted or perturbed. To this end, the authors propose two approaches, i.e., cross entropy - based training and cross entropy ( CE ) and CE - based test set ( CE - TTS ), where CE is defined as the ratio of the number of samples in the training set with respect to the total number of examples in the test set. The authors show that CE - CE can be used to reduce the amount of data corruptions and test set perturbations while maintaining the quality of the training and test sets."
SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"This paper studies the problem of pooling graph classification tasks. The main contribution is a theoretical analysis of the complexity of the pooling problem. The authors show that the problem is linear in the number of nodes and non - trivial in the total number of elements. They also show that there exist two types of graph convolution layers that can be used for pooling : ( 1 ) convolution layer and ( 2 ) transform layer.   The main contributions of this paper are the following :   1 ) theoretical analysis showing that the complexity is linear for convolution and transform layers, and that the total amount of elements required for the transform layer is at most $ \ell_1 $. 2 ) theoretical results showing that there exists a sufficient number of convolutions for the ( non - convolution ) pooling layer. 3 ) experimental results showing the convergence rate of the proposed pooling method."
SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"This paper proposes a point cloud decoder approach to represent point clouds as a set of samples. The main contribution of this paper is to provide a sample - based representation of the input point clouds that can be used to train point clouds decoders. The proposed approach is based on the idea that point clouds can be represented as a collection of samples, where each point cloud is represented by a distribution of features sampled from the target point clouds.   The main contributions of the paper are as follows :   1. Introducing the concept of point clouds and point cloud representations ; 2. Providing a sample based point cloud representation ; 3. Demonstrating that the proposed approach can be applied to a variety of point cloud architectures ; 4. Conducting experiments on a number of different point clouds to demonstrate the effectiveness of the proposed approaches."
SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,This paper proposes a new method to train deep neural networks ( DNNs ) with synthetic and real - world noise levels. The paper claims that the proposed method is more suitable for deep learning than existing methods such as DNN - RNNs and RNN - GNNs.    The paper proposes to train a DNN on top of a set of synthetic data and a benchmark dataset of real world noisy labels. The main idea is to train the DNN with a stochastic gradient descent method on the synthetic data. The authors claim that this method has better performance than the existing methods on the benchmark dataset.
SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"This paper proposes a new supervision - based training algorithm for supervised machine learning. The main idea is to provide supervision to the learned model to ensure that it is not too close to the human supervision level. To this end, the paper proposes two approaches : ( 1 ) clean supervision and ( 2 ) noisy supervision. The clean supervision is based on ensuring that the model does not violate any of the above two assumptions, while the noisy supervision is built on the assumption that the data satisfies a certain set of assumptions. The paper also proposes a coverage - based supervision that ensures that the supervised model does n’t violate the above three assumptions."
SP:6f2c656dbb7629f652a4291d6971625184d8118b,This paper proposes a GNN - based representation learning framework based on the memory - based GNN ( MemGNN ) framework. The proposed framework is built on top of topological graph kernel and diffusion models. Experiments on classification and regression tasks show that the proposed framework achieves state - of - the - art performance.
SP:81bc52d734c86975d741b6482d65ca71a9d81620,"This paper proposes a new initialization scheme for initialization of neural networks based on gradient - based optimization. The key idea is to minimize the number of parameters in the initialization step of the neural network by minimizing the difference between the initial values of the weights of the network and those of the non - linear network.    The main contributions are as follows :   - Introducing a new metric, $ \tilde{t } \in \mathcal{T}$, that quantifies the difference in the absolute value of the initializations of the two networks. - Demonstrating that this metric can be used as a metric to measure the time - scale of gradient descent for neural networks."
SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"This paper studies the bit - level quantization of neural networks and proposes a new way of quantizing the neural networks with the goal of solving the "" bit - amount - of - knowledge "" problem. The main contribution of this paper is to propose a new quantization framework, which is based on the scalar quantization layer. The authors show that the proposed framework is more scalable than the previous one in terms of the number of iterations, and also shows that it is more robust to the amount of iterations."
SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,This paper proposes a two - player training framework for GANs based on the maximum likelihood principle. The proposed framework is based on GAN - based generative adversarial network ( GAN ). The main contribution of this paper is the two - stage training procedure. The first stage is to train the model on a set of datasets and perform a quality check on the quality of each dataset. The second phase is to perform a test on a larger set of data and perform another quality check.    The main contributions of the paper are as follows :   1. Demonstrating the superiority of the proposed framework in terms of test accuracy. The paper shows that the quality control of the model is better than the baselines used in the previous works. 2. Providing a theoretical justification of the theoretical results. 3. Conducting a series of experiments to validate the theoretical claims.
SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"This paper presents a framework for the analysis of cross - domain ( cross - lingual ) agnostic semantic annotation tasks. The proposed framework is based on the notion of a partially pooled ( partially pooled ) annotation space, where the goal is to construct a set of examples that can be used to solve a given set of tasks. To this end, the authors propose to use a large - scale crowdsourced anaphora dataset, where each task can be represented as a mixture of a number of different environments ( e.g., multi - domain, single domain, mixed domain ).    The paper presents two main contributions :   1. An analysis framework that is able to identify the optimal set of environments for each of the above mentioned tasks. 2. A proof - of - concept framework that allows for the inference of the closed - form solution to each of these tasks."
SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"This paper proposes a reinforcement learning framework that aims to improve the learning performance of a generic agent that is placed in a control domain with extrinsic reward and intrinsic drive. The framework consists of two components. The first component is a policy that learns to maximize the intrinsic drive ( SID ) and the second one is a feature control component that is designed to stabilize the learning process.    The paper presents a set of experiments that compare the performance of the SID and the control components. In particular, the paper shows that SID outperforms control by a large margin in terms of total number of agents and total amount of iterations. The paper also shows that the intrinsic control component outperforms both the policy and the feature control components by a significant margin."
SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"This paper proposes a contextual - aware visual - semantic encoder - decoder for video clips. The encoder is built on top of the WMAN model, which is a pre - trained multi - level model that has been shown to be effective in other contexts ( e.g. text, images ). The decoder is trained on a corpus of video clips, and the encoder learns to localize the video clips based on the similarity between the video sequences and the semantic representation of the video frames.    The authors also propose a contextual annotation mechanism that allows the decoder to generate annotated sequences for each video. Experiments show that the proposed encoder performs better than other encoders and decoders on the same dataset."
SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,This paper proposes a novel image - guided rendering approach to generate high - quality images for virtual and augmented reality applications. The key idea is to first generate a set of images and then re - render the images using a neural network that has been trained on the generated images.   The main contribution of this paper is the image - based re - rendering approach that allows the neural network to generate images that are more realistic than the original images.
SP:257d124367b1da9a595dc11a9df750d6bade298e,"This paper proposes a low - rank approximation to the Kronecker - factored eigenvectors of deep neural networks ( DNNs ), which can be used to improve the quality of the uncertainty representation of the model uncertainty. The proposed approach is based on the factoring of the information matrix in terms of low rank approximation, which is then used to construct a Laplace - Approximation scheme."
SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"This paper studies the problem of Hashing ( Min - hash ), where the goal is to find a metric that minimizes the similarity between two sets of data ( e.g. $ \mathcal{x}$ and $ \cal{y}$ ). The paper proposes a new metric $ \sqrt{x } \log(n_{\text{x})$ that measures the difference between the similarity of two set of data $ \text{xi}$ with respect to a set of $ \nabla{x}\log{y } $ data points $ \log{xi } $ and $ nabla$ with $ \infty$ data point $ \tau$.    The paper provides a theoretical analysis of this metric and a number of related metrics. In particular, the paper shows that $ \sigma^2 $ is not a good metric to measure the similarity, and proposes two strategies to measure it. The first one is to estimate $ \theta$ by minimizing $ \frac{x}{y}$. The second one is based on $ \sum_{\log{z}$, where $ \eta$ is the ratio of $ z^{-1}$ to $ y$, and $ z_i$ is a metric used to estimate the scale of the data set $ z$.   Experiments are conducted on a variety of datasets, showing that the proposed metric achieves a good trade - off between high - dimensional similarity and small - scale learning."
SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,"This paper proposes a new method to train a machine learning model that learns to detect the timesteps in the time evolution of the world's physical system. The proposed method is based on the observation that the world has been in a state of constant rotation since the invention of the wheeled automaton in 1867, and that it will continue to be in this state for the next few billion years.    The paper proposes to train the model on a set of periodic data, each of which can be decomposed into a series of data points, each with a different timestep. Each of these data points can be used as input to the model, and the model is conditioned on the data points in each data point, to learn a metric that measures the ratio between the ratio of the number of tokens in the previous data point and the total number in the next data point. The paper shows that the model can be trained in two stages : ( 1 ) the model learns the metric, and ( 2 ) the metric is used to learn the metric."
SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,This paper proposes a new approach to improving the quality of text - to - data decoders for conditional text generation. The main contribution of the paper is to provide a dataset - based evaluation metric that quantifies the ability of a given decoder to generate text that is more confident in its ability to decode real - world text.    The paper also provides a dataset that measures the confidence of the decoder in the ability to generate data that can be used as input to a text - generating system.
SP:03307deac29173b2968fbd08f95fc77eb1f82410,This paper proposes a new lookahead pruning ( LAP ) scheme for nonlinear networks. The proposed LAP scheme is based on the Frobenius distortion of the activation function of a linear operator. The main contribution of this paper is the proof that the LAP can be applied to any network with high sparsity.
SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"In this paper, the authors propose a decentralized stochastic gradient descent ( SGD ) method. The key idea is to divide the time $ t$ into 4 bits, each of which is $ \ell_0$. The goal is to minimize the total number of bits required for SGD to converge to the optimal solution $ t$.    The main contribution of this paper is to show that SGD can be used to solve linear quantization problems."
SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"This paper proposes a policy learning approach to improve the quality / robustness of the current state - of - the - art policy learning approaches. The proposed approach is based on a combination of two approaches : ( 1 ) learning from the current data, and ( 2 ) modeling future data using a learned policy that is robust to perturbations made by the current model.    The key contributions of the paper are as follows :   1 ) improving the quality of the existing policy by learning from current data and making policy changes based on future data ; 2 ) developing a policy that learns from future data without changing the current policy."
SP:c70479b2096a52584b242de58272ca8d8565feea,"This paper proposes a common representation of two correlated data variables, which can be used for both the joint synthesis and conditional generation tasks. The key idea is to learn the common representation by minimizing the information bottleneck principle. The proposed common representation is shown to be better than the naive representation obtained from the two correlated variables. Experiments are conducted to demonstrate the effectiveness of the proposed representation."
