,paper_id,input,output_best
0,SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"Role-based learning holds the promise of achieving scalable multi-agent learning by decomposing complex tasks using roles. However, it is largely unclear how to efficiently discover such a set of roles. To solve this problem, we propose to first decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Learning a role selector based on action effects makes role discovery much easier because it forms a bi-level learning hierarchy: the role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. We further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. By virtue of these advances, our method (1) outperforms the current state-of-the-art MARL algorithms on 9 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and (2) achieves rapid transfer to new environments with three times the number of agents. Demonstrative videos can be viewed at https:// sites.google.com/view/rode-marl.","This paper proposes a method for role-based multi-agent learning in the context of StarCraft II. The main idea is to learn a role selector based on action effects and a role policy based on the role selector. The role selector searches in a smaller role space and at a lower temporal resolution, while the role policies learn in significantly reduced primitive action-observation spaces. The proposed method is evaluated on 9 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and achieves rapid transfer to new environments with three times the number of agents."
1,SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper studies the behaviour of the stochastic subgradient descent (SSGD) method applied to over-parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of the empirical risk minimization problems, we prove that SSGD converges, respectively, with rates O(1/ ) and O(log(1/ )) for convex and strongly-convex objectives when interpolation holds. These rates coincide with established rates for the stochastic gradient descent (SGD) method applied to smooth problems that also satisfy an interpolation condition. Our analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nonsmooth machine learning models. We also prove that the rate O(1/ ) is optimal for the subgradient method in the convex and interpolation setting.","This paper studies the behavior of stochastic subgradient descent (SSGD) method applied to over-parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of the empirical risk minimization problems, the authors prove that SSGD converges, respectively, with rates O(1/ ) and O(log(1 / ) for convex and strongly-convex objectives when interpolation holds. These rates coincide with established rates for the stochastically gradient descent (SGD) method. The analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nonsmoothed machine learning models."
2,SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and wellestablished ideas in machine learning, we explore a variety of non-linear “reservoir” layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.","This paper proposes a method to improve the performance of Transformer models by replacing some of the layers in the original transformer layers with non-linear ""reservoir"" layers. The authors show that the proposed method is able to reduce the wallclock compute time until convergence, as well as improve the overall performance on various machine translation and (masked) language modelling tasks. The paper is well-written and well-motivated. "
3,SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"Steerable CNN imposes the prior knowledge of transformation invariance or equivariance in the network architecture to enhance the the network robustness on geometry transformation of data and reduce overfitting. Filter transform has been an intuitive and widely used technique to construct steerable CNN in the past decades. Recently, group representation theory is used to analyze steerable CNN and reveals the function space structure of a steerable kernel function. However, it is not yet clear on how this theory is related to the filter transform technique. In this paper, we show that kernel constructed by filter transform can also be interpreted in the group representation theory. Meanwhile, we show that filter transformed kernels can be used to convolve input/output features in different group representation. This interpretation help complete the puzzle of steerable CNN theory and provides a novel and simple approach to implement steerable convolution operators. Experiments are executed on multiple datasets to verify the feasibilty of the proposed approach.",This paper studies steerable convolutional kernels in the context of group representation theory. The authors show that filter transformed kernels can be used to convolve input/output features in different group representations. This interpretation helps complete the puzzle of steerable CNN theory and provides a novel and simple approach to implement steerable operators. Experiments are performed on multiple datasets to verify the feasibility of the proposed approach.
4,SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program’s behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program’s score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user’s constraints. The experimental results on a multimodal synthesis dataset (STRUCTUREDREGEX) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.","This paper proposes an optimal neural synthesis approach for multimodal program synthesis, where the goal is to find a program that satisfies user-provided constraints while maximizing the program’s score with respect to a neural model. Specifically, the paper proposes a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it also allows to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect the user”s constraints. The experimental results on a multimodel synthesis dataset (STRUCTUREDREGEX) show that the proposed method substantially outperforms prior state-of-the-art techniques in terms of accuracy."
5,SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"Specific molecular recognition by proteins, for example, protease enzymes, is critical for maintaining the robustness of key life processes. The substrate specificity landscape of a protease enzyme comprises the set of all sequence motifs that are recognized/cut, or just as importantly, not recognized/cut by the enzyme. Current methods for predicting protease specificity landscapes rely on learning sequence patterns in experimentally derived data with a single enzyme, but are not robust to even small mutational changes. A comprehensive evaluation of specificity requires consideration of the three-dimensional structure and energetics of molecular interactions. In this work, we present a protein graph convolutional neural network (PGCN), which uses a physically intuitive, structure-based molecular interaction graph generated using the Rosetta energy function that describes the topology and energetic features, to determine substrate specificity. We use the PGCN to recapitulate and predict the specificity of the NS3/4 protease from the Hepatitic C virus. We compare our PGCN with previously used machine learning models and show that its performance in classification tasks is equivalent or better. Because PGCN is based on physical interactions, it is inherently more interpretable; determination of feature importance reveals key sub-graph patterns responsible for molecular recognition that are biochemically reasonable. The PGCN model also readily lends itself to the design of novel enzymes with tailored specificity against disease targets.","This paper proposes a protein graph convolutional neural network (PGCN) to predict the substrate specificity landscape of a protease enzyme, which is the set of all sequence motifs that are recognized/cut by the enzyme. The proposed method is based on a structure-based molecular interaction graph generated using the Rosetta energy function that describes the topology and energetic features, to determine substrate specificity. The authors use the PGCN to recapitulate and predict the specificity of the NS3/4 protease from the Hepatitic C virus. They compare the performance of the proposed method with existing machine learning models and show that its performance in classification tasks is equivalent or better."
6,SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"Double Q-learning is a classical method for reducing overestimation bias, which is caused by taking maximum estimated values in the Bellman operation. Its variants in the deep Q-learning paradigm have shown great promise in producing reliable value prediction and improving learning performance. However, as shown by prior work, double Q-learning is not fully unbiased and suffers from underestimation bias. In this paper, we show that such underestimation bias may lead to multiple non-optimal fixed points under an approximate Bellman operator. To address the concerns of converging to non-optimal stationary solutions, we propose a simple but effective approach as a partial fix for the underestimation bias in double Q-learning. This approach leverages an approximate dynamic programming to bound the target value. We extensively evaluate our proposed method in the Atari benchmark tasks and demonstrate its significant improvement over baseline algorithms.","This paper studies the problem of underestimation bias in double Q-learning. Double Q-Learning is a classical method for reducing overestimation bias, which is caused by taking maximum estimated values in the Bellman operation. In this paper, the authors show that such underestimation biases may lead to multiple non-optimal fixed points under an approximate Bellman operator. To address this issue, they propose a simple but effective approach as a partial fix. The proposed method leverages an approximate dynamic programming to bound the target value. The experimental results on Atari benchmark tasks demonstrate the effectiveness of the proposed method."
7,SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present NOT-SO-BIG-GAN (NSB-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64×64). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512×512, our model achieves a Fréchet Inception Distance (FID) of 10.59 – beating the baseline BigGAN model – at half the compute (256 TPU-v3 cores).","This paper proposes a two-step training framework for deep generative models (DGMs) of high-dimensional natural images. The first step is to generate images in low-frequency bands by training a sampler in the wavelet domain and then super-resolve these images back to the pixel-space with a novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64x64). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512x512, the proposed model achieves a Fréchet Inception Distance (FID) of 10.59 – beating the baseline BigGAN model – at half the compute (256 TPU-v3 cores)."
8,SP:b943a73b1ec34867371325748dc3a91ff4011947,"Recently, self-supervised learning (SSL) algorithms have been applied to Fewshot learning(FSL). FSL aims at distilling transferable knowledge on existing classes with large-scale labeled data to cope with novel classes for which only a few labeled data are available. Due to the limited number of novel classes, the initial embedding network becomes an essential component and can largely affect the performance in practice. But almost no one analyzes why a pre-trained embedding network with self-supervised training can provide representation for downstream FSL tasks in theory. In this paper, we first summarized the supervised FSL methods and explained why SSL is suitable for FSL. Then we further analyzed the main difference between supervised training and self-supervised training on FSL and obtained the bound for the gap between self-supervised loss and supervised loss. Finally, we proposed potential ways to improve the test accuracy under the setting of self-supervised FSL.","This paper analyzes why self-supervised learning (SSL) is suitable for few-shot learning (FSL). The authors first analyzed the supervised FSL methods and explained why SSL is suitable to FSL. Then, the authors analyzed the main difference between supervised training and self supervised training on FSL and obtained the bound for the gap between the self supervised loss and the supervised loss. Finally, they proposed potential ways to improve the test accuracy under the setting of self supervised learning."
9,SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for “not-too-wide” neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call Angular Distance (AD) function. Finally, we demonstrate that these properties can be easily verified numerically.","This paper studies the convergence of neural networks with finite width. The authors prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima near the initialization. They extend this result to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that the authors call Angular Distance (AD) function. Finally, they demonstrate that these properties can be easily verified numerically."
10,SP:0f62846913ec10b44ed32845770da0565479dc75,"We introduce Deep Adaptive Semantic Logic (DASL), a novel framework for automating the generation of deep neural networks that incorporates user-provided formal knowledge to improve learning from data. We provide formal semantics that demonstrate that our knowledge representation captures all of first order logic and that finite sampling from infinite domains converges to correct truth values. DASL’s representation improves on prior neuro-symbolic work by avoiding vanishing gradients, allowing deeper logical structure, and enabling richer interactions between the knowledge and learning components. We illustrate DASL through a toy problem in which we add structure to an image classification task and demonstrate that knowledge of that structure reduces data requirements by a factor of 1000. We apply DASL on a visual relationship detection task and demonstrate that the addition of commonsense knowledge improves performance by 10.7% in conditions of data scarcity.","This paper proposes a method for learning deep neural networks that incorporate formal knowledge to improve learning from data. The method is based on a neural network architecture that learns a representation of the input data, which is then used to generate a model that is trained on the learned representation. The authors provide formal semantics that demonstrate that their knowledge representation captures all of first order logic and that finite sampling from infinite domains converges to correct truth values. The paper also demonstrates that their representation improves on prior neuro-symbolic work by avoiding vanishing gradients, allowing deeper logical structure, and enabling richer interactions between the knowledge and learning components."
11,SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"Recent work has suggested that feedforward residual neural networks (ResNets) approximate iterative recurrent computations. Iterative computations are useful in many domains, so they might provide good solutions for neural networks to learn. Here we quantify the degree to which ResNets learn iterative solutions and introduce a regularization approach that encourages learning of iterative solutions. Iterative methods are characterized by two properties: iteration and convergence. To quantify these properties, we define three indices of iterative convergence. Consistent with previous work, we show that, even though ResNets can express iterative solutions, they do not learn them when trained conventionally on computer vision tasks. We then introduce regularizations to encourage iterative convergent computation and test whether this provides a useful inductive bias. To make the networks more iterative, we manipulate the degree of weight sharing across layers using soft gradient coupling. This new method provides a form of recurrence regularization and can interpolate smoothly between an ordinary ResNet and a “recurrent” ResNet (i.e., one that uses identical weights across layers and thus could be physically implemented with a recurrent network computing the successive stages iteratively across time). To make the networks more convergent we impose a Lipschitz constraint on the residual functions using spectral normalization. The three indices of iterative convergence reveal that the gradient coupling and the Lipschitz constraint succeed at making the networks iterative and convergent, respectively. However, neither recurrence regularization nor spectral normalization improve classification accuracy on standard visual recognition tasks (MNIST, CIFAR-10, CIFAR-100) or on challenging recognition tasks with partial occlusions (Digitclutter). Iterative convergent computation, in these tasks, does not provide a useful inductive bias for ResNets.","This paper studies the problem of learning iterative solutions in feedforward residual neural networks (ResNets). The authors define three indices of iterative convergence and show that, even though ResNets can express iterative solution, they do not learn them when trained conventionally on computer vision tasks. They then introduce regularizations to encourage iterative convergent computation and test whether this provides a useful inductive bias. To make the networks more iterative, they manipulate the degree of weight sharing across layers using soft gradient coupling. They also impose a Lipschitz constraint on the residual functions using spectral normalization to make them more convergent."
12,SP:6c14506b8b2b06043409d912e6bf877651aaa665,"Normalization techniques are crucial in stabilizing and accelerating the training of deep neural networks. However, they are mainly designed for the independent and identically distributed (IID) data, not satisfying many real-world out-ofdistribution (OOD) situations. Unlike most previous works, this paper presents two normalization methods, SelfNorm and CrossNorm, to promote OOD generalization. SelfNorm uses attention to recalibrate statistics (channel-wise mean and variance), while CrossNorm exchanges the statistics between feature maps. SelfNorm and CrossNorm can complement each other in OOD generalization, though exploring different directions in statistics usage. Extensive experiments on different domains (vision and language), tasks (classification and segmentation), and settings (supervised and semi-supervised) show their effectiveness.","This paper proposes two new normalization methods for out-of-distribution (OOD) training. The first method, SelfNorm, uses attention to recalibrate statistics (channel-wise mean and variance), while the second method, CrossNorm, exchanges statistics between feature maps. Experiments on different domains (vision and language), tasks (classification and segmentation), and settings (supervised and semi-supervised) show the effectiveness of both methods."
13,SP:2774abdc11917321dd4994af0f0da1ff824bea03,"Attention mechanisms are generic inductive biases that have played a critical role in improving the state-of-the-art in supervised learning, unsupervised pre-training and generative modeling for multiple domains including vision, language and speech. However, they remain relatively under-explored for neural network architectures typically used in reinforcement learning (RL) from high dimensional inputs such as pixels. In this paper, we propose and study the effectiveness of augmenting a simple attention module in the convolutional encoder of an RL agent. Through experiments on the widely benchmarked DeepMind Control Suite environments, we demonstrate that our proposed module can (i) extract interpretable task-relevant information such as agent locations and movements without the need for data augmentations or contrastive losses; (ii) significantly improve the sampleefficiency and final performance of the agents. We hope our simple and effective approach will serve as a strong baseline for future research incorporating attention mechanisms in reinforcement learning and control.",This paper proposes to augment the attention module in the convolutional encoder of an RL agent with a simple attention module. The proposed module is a simple extension of the attention mechanism in the attention layer of the CNN. The authors show that the proposed module can extract interpretable task-relevant information such as agent locations and movements without the need for data augmentations or contrastive losses. Experiments on the DeepMind Control Suite environments demonstrate that their proposed module extracts interpretable information without the use of contrastive loss. The experiments also show that their module can significantly improve the sampleefficiency and final performance of the agents.
14,SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"GradNorm (Chen et al., 2018) is a broadly used gradient-based approach for training multitask networks, where different tasks share, and thus compete during learning, for the network parameters. GradNorm eases the fitting of all individual tasks by dynamically equalizing the contribution of each task to the overall gradient magnitude. However, it does not prevent the individual tasks’ gradients from conflicting, i.e., pointing towards opposite directions, and thus resulting in a poor multitask performance. In this work we propose Rotograd, an extension to GradNorm that addresses this problem by dynamically homogenizing not only the gradient magnitudes but also their directions across tasks. For this purpose, Rotograd adds a layer of task-specific rotation matrices that aligns all the task gradients. Importantly, we then analyze Rotograd (and its predecessor) through the lens of game theory, providing theoretical guarantees on the algorithm stability and convergence. Finally, our experiments on several real-world datasets and network architectures show that Rotograd outperforms previous approaches for multitask learning.","This paper proposes an extension of the GradNorm algorithm for multitask learning. The main idea is to add a rotation matrix to the gradient of the gradient for each task, which is added to the original GradNorm layer. The paper provides theoretical analysis on the stability and convergence of the proposed method. The experiments on several real-world datasets and network architectures show that the proposed Rotograd outperforms previous approaches."
15,SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"Unsupervised image-to-image (I2I) translation, which aims to learn a domain mapping function without paired data, is very challenging because the function is highly under-constrained. Despite the significant progress in constraining the mapping function, current methods suffer from the geometry distortion problem: the geometry structure of the translated image is inconsistent with the input source image, which may cause the undesired distortions in the translated images. To remedy this issue, we propose a novel I2I translation constraint, called Minimal Geometry-Distortion Constraint (MGC), which promotes the consistency of geometry structures and reduce the unwanted distortions in translation by reducing the randomness of color transformation in the translation process. To facilitate estimation and maximization of MGC, we propose an approximate representation of mutual information called relative Squared-loss Mutual Information (rSMI) that can be efficiently estimated analytically. We demonstrate the effectiveness of our MGC by providing quantitative and qualitative comparisons with the state-of-the-art methods on several benchmark datasets.","This paper proposes a novel I2I translation constraint, called Minimal Geometry-Distortion Constraint (MGC), which promotes the consistency of geometry structures and reduce the unwanted distortions in translation by reducing the randomness of color transformation in the translation process. To facilitate estimation and maximization of MGC, the authors propose an approximate representation of mutual information called relative Squared-loss Mutual Information (rSMI) that can be efficiently estimated analytically. The experimental results demonstrate the effectiveness of the proposed MGC on several benchmark datasets."
16,SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"In this paper, we examine the long-neglected yet important effects of point sampling patterns in point cloud GANs. Through extensive experiments, we show that sampling-insensitive discriminators (e.g. PointNet-Max) produce shape point clouds with point clustering artifacts while sampling-oversensitive discriminators (e.g. PointNet++, DGCNN, PointConv, KPConv) fail to guide valid shape generation. We propose the concept of sampling spectrum to depict the different sampling sensitivities of discriminators. We further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics forming a sampling spectrum of metrics. Guided by the proposed sampling spectrum, we discover a middle-point sampling-aware baseline discriminator, PointNet-Mix, which improves all existing point cloud generators by a large margin on sampling-related metrics. We point out that, given that recent research has been focused on the generator design, the discriminator design needs more attention. Our work provides both suggestions and tools for building future discriminators. We will release the code to facilitate future research.","This paper studies the effect of point sampling patterns in point cloud GANs. The authors propose a sampling spectrum to depict the different sampling sensitivities of discriminators. They further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics. They discover a middle-point sampling-aware baseline discriminator, PointNet-Mix, which improves all existing point cloud generators by a large margin on sampling related metrics."
17,SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"Standard Convolutional Neural Networks (CNNs) can be easily fooled by images with small quasi-imperceptible artificial perturbations. As alternatives to CNNs, the recently proposed Capsule Networks (CapsNets) are shown to be more robust to white-box attacks than CNNs under popular attack protocols. Besides, the class-conditional reconstruction part of CapsNets is also used to detect adversarial examples. In this work, we investigate the adversarial robustness of CapsNets, especially how the inner workings of CapsNets change when the output capsules are attacked. The first observation is that adversarial examples misled CapsNets by manipulating the votes from primary capsules. Another observation is the high computational cost, when we directly apply multi-step attack methods designed for CNNs to attack CapsNets, due to the computationally expensive routing mechanism. Motivated by these two observations, we propose a novel vote attack where we attack votes of CapsNets directly. Our vote attack is not only effective but also efficient by circumventing the routing process. Furthermore, we integrate our vote attack into the detection-aware attack paradigm, which can successfully bypass the class-conditional reconstruction based detection method. Extensive experiments demonstrate the superior attack performance of our vote attack on CapsNets.",This paper investigates the adversarial robustness of Capsule neural networks (CapsNets) and proposes a novel vote attack to attack the votes of CapsNets directly instead of using multi-step attack methods designed for CNNs. The proposed method is based on the observation that adversarial examples can manipulate the votes from primary capsules. The authors propose a vote attack that bypasses the expensive routing mechanism of the traditional CNN attack method. The experimental results show that the proposed vote attack is effective and efficient by circumventing the routing process.
18,SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"Meta-reinforcement learning aims at finding a policy able to generalize to new environments. When facing a new environment, this policy must explore to identify its particular characteristics and then exploit this information for collecting reward. We consider the online adaptation setting where the agent needs to trade-off between the two types of behaviour within the same episode. Even though policies based on recurrent neural networks can be used in this setting by training them on multiple environments, they often fail to model this trade-off, or solve it at a very high computational cost. In this paper, we propose a new algorithm that uses privileged information in the form of a task descriptor at train time to improve the learning of recurrent policies. Our method learns an informed policy (i.e., a policy receiving as input the description of the current task) that is used to both construct task embeddings from the descriptors, and to regularize the training of the recurrent policy through parameters sharing and an auxiliary objective. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the task, and by exploiting them efficiently. We evaluate our algorithm in a variety of environments that require sophisticated exploration/exploitation strategies and show that it outperforms vanilla RNNs, Thompson sampling and the task-inference approaches to meta-reinforcement learning.","This paper proposes a method for meta-reinforcement learning in the online adaptation setting, where the agent must explore to identify its particular characteristics and then exploit this information for collecting reward. The authors propose a new algorithm that uses privileged information in the form of a task descriptor at train time to improve the learning of recurrent policies. The proposed method learns an informed policy (i.e., a policy receiving as input the description of the current task) that is used to both construct task embeddings from the descriptors, and to regularize the training of the recurrent policy through parameters sharing and an auxiliary objective. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the task, and by exploiting them efficiently. The experimental results show that the proposed method outperforms vanilla RNN, Thompson sampling and the task-inference approaches to meta reinforcement learning."
19,SP:bd89d254fbf31db61db237d08ab42981e27c52df,"The training process of RL requires many trial-and-errors that are costly in realworld applications. To avoid the cost, a promising solution is to learn the policy from an offline dataset, e.g., to learn a simulator from the dataset, and train optimal policies in the simulator. By this approach, the quality of policies highly relies on the fidelity of the simulator. Unfortunately, due to the stochasticity and unsteadiness of the real-world and the unavailability of online sampling, the distortion of the simulator is inevitable. In this paper, based on the model learning technique, we propose a new paradigm to learn an RL policy from offline data in the real-world sequential recommendation system (SRS). Instead of increasing the fidelity of models for policy learning, we handle the distortion issue via learning to adapt to diverse simulators generated by the offline dataset. The adaptive policy is suitable to real-world environments where dynamics are changing and have stochasticity in the offline setting. Experiments are conducted in synthetic environments and a real-world ride-hailing platform. The results show that the method overcomes the distortion problem and produces robust recommendations in the unseen real-world.","This paper proposes a method to learn an RL policy from offline data in the real-world sequential recommendation system (SRS). Instead of increasing the fidelity of models for policy learning, this paper learns to adapt to diverse simulators generated by the offline dataset. Experiments are conducted in synthetic environments and a real world ride-hailing platform. The results show that the method overcomes the distortion problem and produces robust recommendations in the unseen real world."
20,SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a simple and stable alternative, it requires access to demonstrations from a human supervisor. In this paper, we study RL algorithms that use imitation learning to acquire goal reaching policies from scratch, without the need for expert demonstrations or a value function. In lieu of demonstrations, we leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. We propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal-reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. We formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal-reaching performance and robustness over current RL algorithms in several benchmark tasks.","This paper proposes an imitation learning algorithm for goal-reaching reinforcement learning. The main idea is to leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. The authors propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. "
21,SP:c306530164d677e670554eeba8203c66bb3d9f7a,"Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.","This paper proposes a new method for non-autoregressive text to speech (TTS) models, FastSpeech 2, which addresses the one-to-many mapping problem in TTS by directly training the model with ground-truth target instead of the simplified output from teacher, and introducing more variation information of speech (e.g., pitch, pitch, energy and more accurate duration) as conditional inputs in training and use predicted values in inference. Experimental results show that the proposed method can achieve a 3x training speed-up over the original fastSpeech model."
22,SP:79e9fb20d383816f54738ce70d137131ebc10290,"We reformulate unsupervised dimension reduction problem (UDR) in the language of tempered distributions, i.e. as a problem of approximating an empirical probability density function pemp(x) by another tempered distribution q(x) whose support is in a k-dimensional subspace. Thus, our problem is reduced to the minimization of the distance between q and pemp, D(q, pemp), over a pertinent set of generalized functions. This infinite-dimensional formulation allows to establish a connection with another classical problem of data science — the sufficient dimension reduction problem (SDR). Thus, an algorithm for the first problem induces an algorithm for the second and vice versa. In order to reduce an optimization problem over distributions to an optimization problem over ordinary functions we introduce a nonnegative penalty function R(f) that “forces” the support of f to be k-dimensional. Then we present an algorithm for minimization of I(f) + λR(f), based on the idea of two-step iterative computation, briefly described as a) an adaptation to real data and to fake data sampled around a k-dimensional subspace found at a previous iteration, b) calculation of a new k-dimensional subspace. We demonstrate the method on 4 examples (3 UDR and 1 SDR) using synthetic data and standard datasets.","This paper studies the unsupervised dimension reduction problem (UDR) in the language of tempered distributions. The authors reformulate the problem of approximating an empirical probability density function pemp(x) by another tempered distribution q(x), whose support is in a k-dimensional subspace, as the minimization of the distance between q and pemp, D(q, pemp) over a set of generalized functions. This formulation allows to establish a connection with another classical problem of data science — the sufficient dimension reduction (SDR) problem. In order to reduce an optimization problem over distributions to an optimization problems over ordinary functions, the authors introduce a nonnegative penalty function R(f) that “forces” the support of f to be k-diminishing. Then they present an algorithm for minimization I(f + \lambda) + \rhoR(f), based on the idea of two-step iterative computation, which is briefly described as an adaptation to real data and to fake data sampled around a k dimensional subspace found at a previous iteration, and b) calculation of a new k-dimension subspace."
23,SP:93e54522e6c2b805905d21fc968fc40866f2898b,"It is generally believed that robust training of extremely large networks is critical to their success in real-world applications. However, when taken to the extreme, methods that promote robustness can hurt the model’s sensitivity to rare or underrepresented patterns. In this paper, we discuss this trade-off between robustness and sensitivity by introducing two notions: contextual feature utility and contextual feature sensitivity. We propose Feature Contrastive Learning (FCL) that encourages the model to be more sensitive to the features that have higher contextual utility. Empirical results demonstrate that models trained with FCL achieve a better balance of robustness and sensitivity, leading to improved generalization in the presence of noise.","This paper proposes Feature Contrastive Learning (FCL) to improve the robustness of deep neural networks. The authors propose two notions: contextual feature utility and contextual feature sensitivity, and propose a method that encourages the model to be more sensitive to features that have higher contextual utility. Experiments on MNIST and CIFAR-10 show that the proposed method achieves a better balance of robustness and sensitivity."
24,SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent’s actuators and from the agent’s point of view. In this paper, we introduce a new algorithm – called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) – with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert’s and the agent’s domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.","This paper proposes a new method for imitation learning based on adversarial imitation learning. The main idea is to learn a latent representation of a task using a discriminator network, which is trained with adversarial examples. The discriminator is regularized with mutual information constraints to encourage the discriminator to learn only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, the proposed method is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks while being robust to various domain differences in terms of both environment appearance and agent embodiment."
25,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"The lottery ticket hypothesis (LTH) [20] states that learning on a properly pruned network (the winning ticket) improves test accuracy over the original unpruned network. Although LTH has been justified empirically in a broad range of deep neural network (DNN) involved applications like computer vision and natural language processing, the theoretical validation of the improved generalization of a winning ticket remains elusive. To the best of our knowledge, our work, for the first time, characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. We show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned neural network is specified as an (accelerated) stochastic gradient descent algorithm, we theoretically show that the number of samples required for achieving zero generalization error is proportional to the number of the non-pruned weights in the hidden layer. With a fixed number of samples, training a pruned neural network enjoys a faster convergence rate to the desired model than training the original unpruned one, providing a formal justification of the improved generalization of the winning ticket. Our theoretical results are acquired from learning a pruned neural network of one hidden layer, while experimental results are further provided to justify the implications in pruning multi-layer neural networks.","This paper studies the lottery ticket hypothesis (LTH) which states that learning on a properly pruned network (the winning ticket) improves test accuracy over the original unpruned network. The paper provides a theoretical analysis of the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. It shows that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. The number of samples required for achieving zero generalisation error is proportional to the number of non-pruned weights in the hidden layer. The theoretical results are acquired from learning a pruning neural network of one hidden layer, while experimental results are further provided to justify the implications in pruning multi-layer neural networks."
26,SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"A wide breadth of research has devised data augmentation approaches that can improve both accuracy and generalization performance for neural networks. However, augmented data can end up being far from the clean data and what is the appropriate label is less clear. Despite this, most existing work simply reuses the original label from the clean data, and the choice of label accompanying the augmented data is relatively less explored. In this paper, we propose AutoLabel to automatically learn the labels for augmented data, based on the distance between the clean distribution and augmented distribution. AutoLabel is built on label smoothing and is guided by the calibration-performance over a hold-out validation set. We show that AutoLabel is a generic framework that can be easily applied to existing data augmentation methods, including AugMix, mixup, and adversarial training. Experiments on CIFAR-10, CIFAR-100 and ImageNet show that AutoLabel can improve models’ accuracy and calibration performance, especially under distributional shift. Additionally, we demonstrate that AutoLabel can help adversarial training by bridging the gap between clean accuracy and adversarial robustness.","This paper proposes AutoLabel to automatically learn the labels for augmented data, based on the distance between the clean distribution and augmented distribution. AutoLabel is built on label smoothing and is guided by the calibration-performance over a hold-out validation set. The authors show that AutoLabel can be easily applied to existing data augmentation methods, including AugMix, mixup, and adversarial training. Experiments on Cifar-10, CIFAR-100 and ImageNet show that the proposed method can improve models’ accuracy and calibration performance, especially under distributional shift."
27,SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signals by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel selfsupervised objective, Representation Learning via Invariant Causal Mechanisms (RELIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, RELIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games.","This paper proposes a novel self-supervised representation learning method based on a causal framework. The authors propose a novel objective, Representation Learning via Invariant Causal Mechanisms (RELIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, the authors generalize contrastive learning and provide an alternative theoretical explanation for the success of these methods. Empirically, RELIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games."
28,SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"Object goal navigation aims to steer an agent towards a target object based on observations of the agent. It is of pivotal importance to design effective visual representations of the observed scene in determining navigation actions. In this paper, we introduce a Visual Transformer Network (VTNet) for learning informative visual representation in navigation. VTNet is a highly effective structure that embodies two key properties for visual representations: First, the relationships among all the object instances in a scene are exploited; Second, the spatial locations of objects and image regions are emphasized so that directional navigation signals can be learned. Furthermore, we also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. In a nutshell, VTNet embeds object and region features with their location cues as spatial-aware descriptors and then incorporates all the encoded descriptors through attention operations to achieve informative representation for navigation. Given such visual representations, agents are able to explore the correlations between visual observations and navigation actions. For example, an agent would prioritize “turning right” over “turning left” when the visual representation emphasizes on the right side of activation map. Experiments in the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments.","This paper proposes a visual transformer network (VTNet) for object goal navigation. The main idea is to learn a spatial-aware representation of the scene, which can be used to guide the agent towards a target object based on observations of the agent. In particular, the authors propose a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. Experiments on the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments."
29,SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,"Federated learning has been spotlighted as a way to train neural network models using data distributed over multiple clients without a need to share private data. Unfortunately, however, it has been shown that data privacy could not be fully guaranteed as adversaries may be able to extract certain information on local data from the model parameters transmitted during federated learning. A recent solution based on the secure aggregation primitive enables privacy-preserving federated learning, but at the expense of significant extra communication/computational resources. In this paper, we propose communication-computation efficient secure aggregation which reduces the amount of communication/computational resources at least by a factor of √ n/ log n relative to the existing secure solution without sacrificing data privacy, where n is the number of clients. The key idea behind the suggested scheme is to design the topology of the secret-sharing nodes (denoted by the assignment graph G) as sparse random graphs instead of the complete graph corresponding to the existing solution. We first obtain a sufficient condition on G to guarantee reliable and private federated learning. Afterwards, we suggest using the Erdős-Rényi graph as G, and provide theoretical guarantees on the reliability/privacy of the proposed scheme. Through extensive real-world experiments, we demonstrate that our scheme, using only 50% of the resources required in the conventional scheme, maintains virtually the same levels of reliability and data privacy in practical federated learning systems.","This paper proposes a communication-computation efficient secure aggregation method for federated learning. The key idea is to design the topology of the secret-sharing nodes (denoted by the assignment graph G) as sparse random graphs instead of the complete graph corresponding to the existing secure aggregation solution. Theoretical guarantees on the reliability/privacy of the proposed scheme are provided, and extensive real-world experiments are conducted to demonstrate that the proposed method can maintain the same levels of reliability and data privacy while using only 50% of the resources."
30,SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"Designing an incentive compatible auction that maximizes expected revenue is a central problem in Auction Design. While theoretical approaches to the problem have hit some limits, a recent research direction initiated by Duetting et al. (2019) consists in building neural network architectures to find optimal auctions. We propose two conceptual deviations from their approach which result in enhanced performance. First, we use recent results in theoretical auction design to introduce a time-independent Lagrangian. This not only circumvents the need for an expensive hyper-parameter search (as in prior work), but also provides a single metric to compare the performance of two auctions (absent from prior work). Second,the optimization procedure in previous work uses an inner maximization loop to compute optimal misreports. We amortize this process through the introduction of an additional neural network. We demonstrate the effectiveness of our approach by learning competitive or strictly improved auctions compared to prior work. Both results together further imply a novel formulation of Auction Design as a two-player game with stationary utility functions.",This paper proposes a novel approach to the problem of designing an incentive-compatible auction that maximizes expected revenue. The main contribution of this paper is the introduction of a time-independent Lagrangian that allows for a single metric to compare the performance of two auctions. The paper also proposes to amortize the inner maximization loop by adding an additional neural network to the optimization procedure. The experimental results show that the proposed method outperforms the existing methods.
31,SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"It is common within the deep learning community to first pre-train a deep neural network from a large-scale dataset and then fine-tune the pre-trained model to a specific downstream task. Recently, both supervised and unsupervised pre-training approaches to learning representations have achieved remarkable advances, which exploit the discriminative knowledge of labels and the intrinsic structure of data, respectively. It follows natural intuition that both discriminative knowledge and intrinsic structure of the downstream task can be useful for fine-tuning, however, existing fine-tuning methods mainly leverage the former and discard the latter. A question arises: How to fully explore the intrinsic structure of data for boosting fine-tuning? In this paper, we propose Bi-tuning, a general learning approach to finetuning both supervised and unsupervised pre-trained representations to downstream tasks. Bi-tuning generalizes the vanilla fine-tuning by integrating two heads upon the backbone of pre-trained representations: a classifier head with an improved contrastive cross-entropy loss to better leverage the label information in an instancecontrast way, and a projector head with a newly-designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category-consistent way. Comprehensive experiments confirm that Bi-tuning achieves state-of-the-art results for fine-tuning tasks of both supervised and unsupervised pre-trained models by large margins (e.g. 10.7% absolute rise in accuracy on CUB in low-data regime).","This paper proposes Bi-tuning, a general learning approach to finetune both supervised and unsupervised pre-trained representations to downstream tasks. The authors propose two heads: a classifier head with an improved contrastive cross-entropy loss to better leverage the label information in an instance-contrastive way, and a projector head with a newly-designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category-consistent way. The experiments show that the proposed method achieves state-of-the-art results for fine-tuneing tasks on CUB in low-data regime."
32,SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper analyzes the problems of adversarial accuracy and adversarial training. We argue that standard adversarial accuracy fails to properly measure the robustness of classifiers. Its definition has a tradeoff with standard accuracy even when we neglect generalization. In order to handle the problems of the standard adversarial accuracy, we introduce a new measure for the robustness of classifiers called genuine adversarial accuracy. It can measure the adversarial robustness of classifiers without trading off accuracy on clean data and accuracy on the adversarially perturbed samples. In addition, it does not favor a model with invariance-based adversarial examples, samples whose predicted classes are unchanged even if the perceptual classes are changed. We prove that a single nearest neighbor (1-NN) classifier is the most robust classifier according to genuine adversarial accuracy for given data and a norm-based distance metric when the class for each data point is unique. Based on this result, we suggest that using poor distance metrics might be one factor for the tradeoff between test accuracy and lp norm-based test adversarial robustness.",This paper studies the problem of adversarial robustness of classifiers. The authors argue that standard adversarial accuracy fails to properly measure the robustness and propose a new measure called genuine adversarial error (GAE) which measures the adversarially perturbed samples without trading off accuracy on clean data and accuracy on the adversarial samples. They prove that a single nearest neighbor (1-NN) classifier is the most robust classifier according to GAE for given data and a norm-based distance metric.
33,SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"Disparate impact has raised serious concerns in machine learning applications and its societal impacts. In response to the need of mitigating discrimination, fairness has been regarded as a crucial property in algorithmic designs. In this work, we study the problem of disparate impact on graph-structured data. Specifically, we focus on dyadic fairness, which articulates a fairness concept that a predictive relationship between two instances should be independent of the sensitive attributes. Based on this, we theoretically relate the graph connections to dyadic fairness on link predictive scores in learning graph neural networks, and reveal that regulating weights on existing edges in a graph contributes to dyadic fairness conditionally. Subsequently, we propose our algorithm, FairAdj, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that our method delivers effective dyadic fairness in terms of various statistics, and at the same time enjoys a favorable fairness-utility tradeoff.","This paper studies the problem of dyadic fairness in the context of link prediction in graph neural networks. The authors propose a new algorithm, FairAdj, to learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that the proposed method delivers effective dyadic fairness in terms of various statistics and at the same time enjoys a favorable fairness-utility tradeoff."
34,SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"Autoencoders are a powerful information compression framework, but it lacks generative ability. We explore whether an autoencoder can gain generative ability without invoking GAN-based adversarial training, which is notoriously hard to train and control. VAE provides one solution by adding Gaussian prior knowledge that can be sampled for synthesis but faces a low-quality problem. One naive way is directly through exploration in latent space such as interpolation; however, randomly interpolating the latent code may wander out of the acceptable distribution of the decoder, which leads to inferior output. Here we propose a new method: Disentangled Exploration Autoencoder (DEAE), which uses disentangled representation and regularization to guarantee the validity of exploration in latent space and achieve controllable synthesis. The encoder of DEAE first turns the input sample into a disentangled latent code, then explores the latent code space through directed interpolation. To aid the interpolated latent code in successfully outputting a meaningful sample, after the decoder, we regularize the output by ’reusing’ the encoder to force the obtained latent representation to maintain perfect disentanglement, which implicitly improves the quality of the interpolated sample. The disentanglement and exploration can boost each other and form a positive loop that empowers DEAE’s generative ability. Experiments demonstrate that DEAE can improve the performance of downstream tasks by synthesizing attribute-controllable augmented samples. We also demonstrate that DEAE can help to eliminate dataset bias, which provides a solution for fairness problems.","This paper proposes a disentangled exploration autoencoder (DEAE) that uses disentanglement and regularization to improve the quality of exploration in latent space and achieve controllable synthesis. The encoder first turns the input sample into a disenangled latent code, then explores the latent code space through directed interpolation. To aid the interpolated latent code in successfully outputting a meaningful sample, after the decoder, DEAE regularizes the output by ’reusing’ the encoder to force the obtained latent representation to maintain perfect disentangling. Experiments demonstrate that DEAE can improve the performance of downstream tasks by synthesizing attribute-controllable augmented samples."
35,SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"Episodic and semantic memory are critical components of the human memory model. The theory of complementary learning systems (McClelland et al., 1995) suggests that the compressed representation produced by a serial event (episodic memory) is later restructured to build a more generalized form of reusable knowledge (semantic memory). In this work we develop a new principled Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. We take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. In contrast to the Kanerva Machine, we simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. We demonstrate that this allocation scheme improves performance in memory conditional image generation, resulting in new state-of-the-art conditional likelihood values on binarized MNIST (≤41.58 nats/image), binarized Omniglot (≤66.24 nats/image), as well as presenting competitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32×32.","This paper proposes a new Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. The authors take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. They simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. The experimental results demonstrate that this allocation scheme improves performance in memory conditional image generation."
36,SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,"Attention mechanisms have advanced state-of-the-art deep learning models for many machine learning tasks. Despite significant empirical gains, there is a lack of theoretical analyses on their effectiveness. In this paper, we address this problem by studying the sample complexity and loss landscape of attention-based neural networks. Our results show that, under mild assumptions, every local minimum of the attention model has low prediction error, and attention models require lower sample complexity than models without attention. Besides revealing why popular self-attention works, our theoretical results also provide guidelines for designing future attention models. Experiments on various datasets validate our theoretical findings.","This paper studies the sample complexity and loss landscape of attention-based neural networks. Theoretical results show that, under mild assumptions, every local minimum of the attention model has low prediction error, and attention models require lower sample complexity than models without attention. Experiments on various datasets validate the theoretical findings."
37,SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"Active inference may be defined as Bayesian modeling of a brain with a biologically plausible model of the agent. Its primary idea relies on the free energy principle and the prior preference of the agent. An agent will choose an action that leads to its prior preference for a future observation. In this paper, we claim that active inference can be interpreted using reinforcement learning (RL) algorithms and find a theoretical connection between them. We extend the concept of expected free energy (EFE), which is a core quantity in active inference, and claim that EFE can be treated as a negative value function. Motivated by the concept of prior preference and a theoretical connection, we propose a simple but novel method for learning a prior preference from experts. This illustrates that the problem with inverse RL can be approached with a new perspective of active inference. Experimental results of prior preference learning show the possibility of active inference with EFE-based rewards and its application to an inverse RL problem.","This paper studies the problem of active inference in the context of reinforcement learning. The authors extend the concept of expected free energy (EFE), which is a core quantity in active inference, and claim that EFE can be treated as a negative value function. Motivated by the prior preference and a theoretical connection, the authors propose a simple but novel method for learning a prior preference from experts. Experimental results of prior preference learning show the possibility of using EFE-based rewards and its application to an inverse RL problem."
38,SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"Several data augmentation methods deploy unlabeled-in-distribution (UID) data to bridge the gap between the training and inference of neural networks. However, these methods have clear limitations in terms of availability of UID data and dependence of algorithms on pseudo-labels. Herein, we propose a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. We show how to improve generalization theoretically using OOD data in each learning scenario and complement our theoretical analysis with experiments on CIFAR-10, CIFAR-100, and a subset of ImageNet. The results indicate that undesirable features are shared even among image data that seem to have little correlation from a human point of view. We also present the advantages of the proposed method through comparison with other data augmentation methods, which can be used in the absence of UID data. Furthermore, we demonstrate that the proposed method can further improve the existing state-of-the-art adversarial training.","This paper proposes a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. Theoretical analysis is provided to show how to improve the generalization theoretically using OOD data in each learning scenario and complement the theoretical analysis with experiments on CIFAR-10, Cifar-100, and a subset of ImageNet. The authors also present the advantages of the proposed method through comparison with other data augmentations methods, which can be used in the absence of unlabeled-in- distribution data."
39,SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"This paper introduces Fast Linearized Adaptive Policy (FLAP), a new metareinforcement learning (meta-RL) method that is able to extrapolate well to outof-distribution tasks without the need to reuse data from training, and adapt almost instantaneously with the need of only a few samples during testing. FLAP builds upon the idea of learning a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights. A separate adapter network is trained simultaneously with the policy such that during adaptation, we can directly use the adapter network to predict these linear weights instead of updating a meta-policy via gradient descent, such as in prior meta-RL methods like MAML, to obtain the new policy. The application of the separate feed-forward network not only speeds up the adaptation run-time significantly, but also generalizes extremely well to very different tasks that prior MetaRL methods fail to generalize to. Experiments on standard continuous-control meta-RL benchmarks show FLAP presenting significantly stronger performance on out-of-distribution tasks with up to double the average return and up to 8X faster adaptation run-time speeds when compared to prior methods.","This paper proposes a new meta-RL method called Fast Linearized Adaptive Policy (FLAP), which is based on the idea of learning a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights. A separate adapter network is trained simultaneously with the policy such that during adaptation, FLAP can directly use the adapter network to predict these linear weights instead of updating a meta-policy via gradient descent. The application of the separate feed-forward network not only speeds up the adaptation run-time significantly, but also generalizes extremely well to very different tasks that prior MetaRL methods fail to generalize to. Experiments on standard continuous-control metaRL benchmarks show FLAP presenting significantly stronger performance on out-of-distribution tasks."
40,SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"A federated kernel k-means algorithm is developed in this paper. This algorithm resolves two challenging issues: 1) how to distributedly so lve the optimization problem of kernel k-means under federated settings; 2) how to maintain communication efficiency in the algorithm. To tackle the first chal lenge, a distributed stochastic proximal gradient descent (DSPGD) algorithm is developed to determine an approximate solution to the optimization problem of kernelk-means. To tackle the second challenge, a communication efficient mech anism (CEM) is designed to reduce the communication cost. Besides, the feder ated kernelk-means provides two levels of privacy preservation : 1) users’ local data are not exposed to the cloud server; 2) the cloud server cannot recover users ’ local data from the local computational results via matrix operations. Theoretical analysis shows: 1) DSPGD with CEM converges with an O(1/T ) rate, whereT is the number of iterations; 2) the communication cost of DSPGD with CEM is un related to the number of data samples; 3) the clustering quality of the federated kernel k-means approaches that of the standard kernel k-means, with a(1 + ǫ) approximate ratio. The experimental results show that the federated kerne l k-means achieves the highest clustering quality with the communication cost red uced by more than 60% in most cases.","This paper proposes a federated kernel k-means algorithm to solve the optimization problem of kernel k means under federated settings. The authors propose a distributed stochastic proximal gradient descent (DSPGD) algorithm to determine an approximate solution to the optimization problems of kernelk means, and a communication efficient mech anism (CEM) is designed to reduce the communication cost. Theoretical analysis shows that DSPGD with CEM converges with an O(1/T) rate, where T is the number of iterations, and communication cost is not related to number of data samples. The clustering quality of the federated k means algorithm approaches that of the standard kernel k mean algorithm with a(1 +    ) approximate ratio. The experimental results show that the proposed federated kerne l k means achieves the highest clustering performance with the communication costs reduced by more than 60% in most cases."
41,SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"The emergence of CNNs in mainstream deployment has necessitated methods to design and train efficient architectures tailored to maximize the accuracy under diverse hardware & latency constraints. To scale these resource-intensive tasks with an increasing number of deployment targets, Once-For-All (OFA) proposed an approach to jointly train several models at once with a constant training cost. However, this cost remains as high as 40-50 GPU days and also suffers from a combinatorial explosion of sub-optimal model configurations. We seek to reduce this search space – and hence the training budget – by constraining search to models close to the accuracy-latency Pareto frontier. We incorporate insights of compound relationships between model dimensions to build CompOFA, a design space smaller by several orders of magnitude. Through experiments on ImageNet, we demonstrate that even with simple heuristics we can achieve a 2x reduction in training time1 and 216x speedup in model search/extraction time compared to the state of the art, without loss of Pareto optimality! We also show that this smaller design space is dense enough to support equally accurate models for a similar diversity of hardware and latency targets, while also reducing the complexity of the training and subsequent extraction algorithms.2","This paper proposes CompOFA, a method to reduce the search space of Once-For-All (OFA) by constraining search to models close to the accuracy-latency Pareto frontier. The authors propose to use compound relationships between model dimensions to build a design space smaller by several orders of magnitude. They demonstrate that even with simple heuristics they can achieve a 2x reduction in training time and 216x speedup in model search/extraction time compared to the state-of-the-art."
42,SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"Meta-learning enables a model to learn from very limited data to undertake a new task. In this paper, we study the general meta-learning with adversarial samples. We present a meta-learning algorithm, ADML (ADversarial Meta-Learner), which leverages clean and adversarial samples to optimize the initialization of a learning model in an adversarial manner. ADML leads to the following desirable properties: 1) it turns out to be very effective even in the cases with only clean samples; 2) it is robust to adversarial samples, i.e., unlike other meta-learning algorithms, it only leads to a minor performance degradation when there are adversarial samples; 3) it sheds light on tackling the cases with limited and even contaminated samples. It has been shown by extensive experimental results that ADML outperforms several representative meta-learning algorithms in the cases involving adversarial samples generated by different attack mechanisms, on two widely-used image datasets, MiniImageNet and CIFAR100, in terms of both accuracy and robustness.","This paper proposes a meta-learning algorithm, ADML (Adversarial Meta-Learner), which leverages clean and adversarial samples to optimize the initialization of a learning model in an adversarial manner. The authors claim that the proposed algorithm is robust to adversarial attacks, and that it sheds light on tackling the cases with limited and even contaminated samples. The experimental results on MiniImageNet and CIFAR100 show that ADML outperforms several representative meta learning algorithms in the cases involving adversarial attack mechanisms."
43,SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"Error correction codes are an integral part of communication applications and boost the reliability of transmission. The optimal decoding of transmitted codewords is the maximum likelihood rule, which is NP-hard. For practical realizations, suboptimal decoding algorithms are employed; however, the lack of theoretical insights currently impedes the exploitation of the full potential of these algorithms. One key insight is the choice of permutation in permutation decoding. We present a data-driven framework for permutation selection combining domain knowledge with machine learning concepts such as node embedding and self-attention. Significant and consistent improvements in the bit error rate are shown for the simulated Bose Chaudhuri Hocquenghem (BCH) code as compared to the baseline decoders. To the best of our knowledge, this work is the first to leverage the benefits of self-attention networks in physical layer communication systems.",This paper proposes a method to improve the performance of decoding error correction codes for Bose-Chaudhuri-Hocquenghem (BCH) codes. The authors propose a data-driven framework for permutation selection that combines domain knowledge with machine learning concepts such as node embedding and self-attention. The results show that the proposed method can improve the bit error rate of BCH codes compared to the existing methods.
44,SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"In cases where labeled data is scarce, the common practice of fine-tuning BERT for a target text classification task is prone to producing poor performance. In such scenarios, we suggest performing an unsupervised classification task prior to finetuning on the target task. Specifically, as such an intermediate task, we perform unsupervised clustering, training BERT on predicting the cluster labels. We test this hypothesis on various data sets, and show that this additional classification step can significantly reduce the demand for labeled examples mainly for topical classification tasks. We further discuss under which conditions this task is helpful and why.","This paper proposes to perform an unsupervised classification task prior to fine-tuning BERT for a target text classification task. Specifically, as such an intermediate task, the authors perform an additional clustering, training BERT on predicting the cluster labels. The authors test this hypothesis on various data sets, and show that this additional classification step can significantly reduce the demand for labeled examples mainly for topical classification tasks."
45,SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"We contribute to micro-data model-based reinforcement learning (MBRL) by rigorously comparing popular generative models using a fixed (random shooting) control agent. We find that on an environment that requires multimodal posterior predictives, mixture density nets outperform all other models by a large margin. When multimodality is not required, our surprising finding is that we do not need probabilistic posterior predictives: deterministic models are on par, in fact they consistently (although non-significantly) outperform their probabilistic counterparts. We also found that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. At the methodological side, we design metrics and an experimental protocol which can be used to evaluate the various models, predicting their asymptotic performance when using them on the control problem. Using this framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot by two to four folds, using an aggressive training schedule which is outside of the hyperparameter interval usually considered.","This paper studies the problem of micro-data model-based reinforcement learning (MBRL) in the context of a random shooting environment. The authors compare a variety of generative models on the Acrobot environment, including a mixture density model, a deterministic model, and a probabilistic model. They find that the mixture density models outperform the deterministic models by a large margin. They also find that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. The paper also proposes metrics and an experimental protocol to evaluate the various models."
46,SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner. The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias. The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum likelihood estimation. Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew, horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.","This paper proposes an affine disentangled GAN that can explicitly disentangle affine transformations in a self-supervised and rigorous manner. The affine regularizer is derived by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum likelihood estimation. The features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, and skew can be explicitly selected and learned. Experiments are conducted on MNIST, CelebA, and dSprites."
47,SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"Representation learning has been greatly improved with the advance of contrastive learning methods with the performance being closer to their supervised learning counterparts. Those methods have greatly benefited from various data augmentations that are carefully designated to maintain their identities so that the images transformed from the same instance can still be retrieved. Although stronger augmentations could expose novel patterns of representations to improve their generalizability, directly using stronger augmentations in instance discrimination-based contrastive learning may even deteriorate the performance, because the distortions induced from the stronger augmentations could ridiculously change the image structures and thus the transformed images can not be viewed as the same as the original ones any more. Additional efforts are needed for us to explore the role of the stronger augmentations in further pushing the performance of unsupervised learning to the fully supervised upper bound. Instead of applying the stronger augmentations directly to minimize the contrastive loss, we propose to minimize the distribution divergence between the weakly and strongly augmented images over the representation bank to supervise the retrieval of strongly augmented queries from a pool of candidates. This avoids an overoptimistic assumption that could overfit the strongly augmented queries containing distorted visual structures into the positive targets in the representation bank, while still being able to distinguish them from the negative samples by leveraging the distributions of weakly augmented counterparts. The proposed method achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned. This is almost the same as 76.5% of top-1 accuracy with a fully supervised ResNet-50. Moreover, it outperforms the previous self-supervised and supervised methods on both the transfer learning and object detection tasks.","This paper proposes a method for instance discrimination-based contrastive learning based on representation learning. The main idea is to minimize the distribution divergence between the weakly and strongly augmented images over the representation bank to supervise the retrieval of strongly augmented queries from a pool of candidates. The proposed method achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned. Moreover, the proposed method outperforms the previous self-supervised and supervised methods on both the transfer learning and object detection tasks."
48,SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"De-identification of magnetic resonance imagery (MRI) is intrinsically difficult since, even with all metadata removed, a person’s face can easily be rendered and matched against a database. Existing de-identification methods tackle this task by obfuscating or removing parts of the face, but they either fail to reliably hide the patient’s identity or they remove so much information that they adversely affect further analyses. In this work, we describe a new class of MRI de-identification techniques that remodel privacy-sensitive facial features as opposed to removing them. To accomplish this, we propose a conditional, multi-scale, 3D GAN architecture that takes a patient’s MRI scan as input and generates a 3D volume in which the brain is not modified but the face has been de-identified. Compared to the classical removal-based techniques, our deep learning framework preserves privacy more reliably without adversely affecting downstream medical analyses on the brain, including segmentation and age prediction.","This paper proposes a method for de-identifying the face of a patient using MRI scans. The proposed method is based on a conditional GAN architecture that takes a patient’s MRI scan as input and generates a 3D volume in which the brain is not modified but the face has been de-identified. Compared to the classical removal-based techniques, the proposed method preserves privacy more reliably without adversely affecting downstream medical analyses on the brain, including segmentation and age prediction."
49,SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"Graph neural networks have been widely used on modeling graph data, achieving impressive results on node classification and link prediction tasks. Yet, obtaining an accurate representation for a graph further requires a pooling function that maps a set of node representations into a compact form. A simple sum or average over all node representations considers all node features equally without consideration of their task relevance, and any structural dependencies among them. Recently proposed hierarchical graph pooling methods, on the other hand, may yield the same representation for two different graphs that are distinguished by the Weisfeiler-Lehman test, as they suboptimally preserve information from the node features. To tackle these limitations of existing graph pooling methods, we first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head attention based global pooling layer that captures the interaction between nodes according to their structural dependencies. We show that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods can be easily extended to the previous node clustering approaches for hierarchical graph pooling. Our experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks.1","This paper proposes a multi-head attention based global pooling layer for hierarchical graph pooling. The proposed method is based on a multiset encoding problem with auxiliary information about the graph structure, and proposes a Graph Multiset Transformer (GMT) which captures the interaction between nodes according to their structural dependencies. The paper shows that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. The experimental results show that GMT significantly outperforms state-of-the-art graph pooler methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks."
50,SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffer from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/.","This paper studies the problem of over-squashing in graph neural networks (GNNs) and proposes a new explanation for this problem. The paper argues that GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-Squashing of exponentially growing information into fixed-size vectors, which causes the GNN to fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, the authors demonstrate that the bottleneck hinders popular GNN models such as GCN, GGNN, GCN-GIN, and GGNN-GAT. The authors also show that prior work, which extensively tuned GNN model of long range problems, suffer from oversquashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights."
51,SP:90d8fa381446923902e42b259392e5e975e6caa1,"Sentiment analysis is a costly yet necessary task for enterprises to study the opinions of their customers to improve their products and services and to determine optimal marketing strategies. Due to the existence of a wide range of domains across different products and services, cross-domain sentiment analysis methods have received significant attention in recent years. These methods mitigate the domain gap between different applications by training cross-domain generalizable classifiers which help to relax the need for individual data annotation for each domain. Most existing methods focus on learning domain-agnostic representations that are invariant with respect to both the source and the target domains. As a result, a classifier that is trained using annotated data in a source domain, would generalize well in a related target domain. In this work, we introduce a new domain adaptation method which induces large margins between different classes in an embedding space based on the notion of prototypical distribution. This embedding space is trained to be domain-agnostic by matching the data distributions across the domains. Large margins in the source domain help to reduce the effect of “domain shift” on the performance of a trained classifier in the target domain. Theoretical and empirical analysis are provided to demonstrate that the method is effective.","This paper proposes a new method for cross-domain sentiment analysis based on prototypical distribution. The method is based on the idea of prototypical embedding space, which is trained to be domain-agnostic by matching the data distributions across the domains. The authors show that the proposed method can reduce the effect of “domain shift” on the performance of a trained classifier in the target domain. Theoretical and empirical analysis are provided to demonstrate that the method is effective."
52,SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"Gender-bias stereotypes have recently raised significant ethical concerns in natural language processing. However, progress in detection and evaluation of genderbias in natural language understanding through inference is limited and requires further investigation. In this work, we propose an evaluation methodology to measure these biases by constructing a challenge task which involves pairing gender neutral premise against gender-specific hypothesis. We use our challenge task to investigate state-of-the-art NLI models on the presence of gender stereotypes using occupations. Our findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI data-sets are significantly prone to genderinduced prediction errors. We also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases.","This paper proposes an evaluation methodology to measure the gender bias in NLI models. The authors propose a challenge task to evaluate the presence of gender stereotypes using occupations. They evaluate three models (BERT, RoBERTa, BART) trained on MNLI and SNLI data-sets and show that three models are significantly prone to gender induced prediction errors. They also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases."
53,SP:a32ab755bd249c393b70938036ce8e810c0c439f,"In this paper, we revisit variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. We show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior and achieve the maximal empowerment, we propose two methods respectively based on the transitional probability model and Gaussian mixture model. We substantiate our claims through rigorous mathematical derivations and experimental analyses.","This paper studies variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. The authors show that the intrinsic reward used in the original work by Gregor et al. (2016) is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior and achieve the maximal empowerment, the authors propose two methods based on the transitional probability model and Gaussian mixture model. The experimental results demonstrate the effectiveness of the proposed methods."
54,SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,"Deep neural networks represent the gold standard for image classification. However, they usually need large amounts of data to reach superior performance. In this work, we focus on image classification problems with a few labeled examples per class and improve sample efficiency in the low data regime by using an ensemble of relatively small deep networks. For the first time, our work broadly studies the existing concept of neural ensembling in small data domains, through an extensive validation using popular datasets and architectures. We show that deep ensembling is a simple yet effective technique that outperforms current state-of-the-art approaches for learning from small datasets. We compare different ensemble configurations to their deeper and wider competitors given a total fixed computational budget and provide empirical evidence of their advantage. Furthermore, we investigate the effectiveness of different losses and show that their choice should be made considering different factors.","This paper studies the problem of image classification in the low-data regime with few labeled examples per class. The authors propose to use an ensemble of relatively small deep neural networks to improve sample efficiency. The proposed method is based on the idea of neural ensembling, which is a simple yet effective technique that outperforms current state-of-the-art approaches for learning from small datasets. The experimental results show that the proposed method can outperform current state of the art approaches. "
55,SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,"Quantized neural networks are gaining popularity thanks to their ability to solve complex tasks with comparable accuracy as full-precision Deep Neural Networks (DNNs), while also reducing computational power and storage requirements and increasing the processing speed. These properties make them an attractive alternative for the development and deployment of DNN-based applications in InternetOf-Things (IoT) devices. Among quantized networks, Binary Neural Networks (BNNs) have reported the largest speed-up. However, they suffer from a fixed and limited compression factor that may result insufficient for certain devices with very limited resources. In this work, we propose Sparse Binary Neural Networks, a novel model and training scheme that allows to introduce sparsity in BNNs by using positive 0/1 binary weights, instead of the -1/+1 weights used by state-ofthe-art binary networks. As a result, our method is able to achieve a high compression factor and reduces the number of operations and parameters at inference time. We study the properties of our method through experiments on linear and convolutional networks over MNIST and CIFAR-10 datasets. Experiments confirm that SBNNs can achieve high compression rates and good generalization, while further reducing the operations of BNNs, making it a viable option for deploying DNNs in very cheap and low-cost IoT devices and sensors.","This paper proposes Sparse Binary Neural Networks (SBNN), a novel method for reducing the number of operations in quantized neural networks (BNNs) by using positive 0/1 binary weights instead of the -1/1 weights used in state-of-the-art BNNs. The authors claim that the proposed SBNN is able to achieve a high compression factor and reduce the operations and parameters at inference time. The proposed method is evaluated on both linear and convolutional networks over MNIST and CIFAR-10 datasets. The experiments confirm that SBNNs can achieve high compression rates and good generalization, while further reducing the operations of BNN."
56,SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"In addition to achieving high accuracy, in many applications, it is important to estimate the probability that a model prediction is correct. Predictive uncertainty is particularly important on out-of-distribution (OOD) data where accuracy degrades. However, models are typically overconfident, and model calibration on OOD data remains a challenge. In this paper we propose a simple post hoc calibration method that significantly improves on benchmark results (Ovadia et al., 2019) on a wide range of corrupted data. Our method uses outlier exposure to properly calibrate the model probabilities. 1 PREDICTIVE UNCERTAINTY When a machine learning model makes a prediction, we want to know how confident (or uncertain) we should be about the result. Uncertainty estimates are useful for both in distribution and outof-distribution (OOD) data. Predictive uncertainty addresses this challenge by endowing model predictions with estimates of class membership probabilities. The baseline method for predictive uncertainty is to simply use the softmax probabilities of the model, p(x) = softmax(f(x)), as a surrogate for class membership probabilities (Hendrycks & Gimpel, 2017). Here f(x) denotes the model outputs. Other approaches include temperature scaling (Guo et al., 2017), dropout (Gal & Ghahramani, 2016; Srivastava et al., 2014), and model ensembles (Lakshminarayanan et al., 2017), as well as Stochastic Variational Bayesian Inference (SVBI) for deep learning (Blundell et al., 2015; Graves, 2011; Louizos & Welling, 2016; 2017; Wen et al., 2018), among others. All methods suffer from some degree of calibration error, which is the difference between predicted error rates and actual error rates, as measured by collecting data into bins based on pmax = maxi p softmax i bins. The standard measurement of calibration error is the expected calibration error (ECE) Guo et al. (2017), although other measures have been used (see Nguyen et al. (2015), Hendrycks & Gimpel (2017)), including the Brier score (DeGroot & Fienberg, 1983), which is also used in Ovadia et al. (2019).","This paper proposes a post hoc calibration method for predicting predictive uncertainty for deep learning models on corrupted data. The proposed method is based on the Brier score, which is a measure of uncertainty that measures the difference between the predicted error rate and the actual error rate of the model. The authors propose to use this measure as a surrogate for the class membership probabilities of the classifier. The method is evaluated on corrupted MNIST, CIFAR-10, and MNIST-100 datasets."
57,SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"Graph neural networks and graph kernels have achieved great success in solving machine learning problems on graphs. Recently, there has been considerable interest in determining the expressive power mainly of graph neural networks and of graph kernels, to a lesser extent. Most studies have focused on the ability of these approaches to distinguish non-isomorphic graphs or to identify specific graph properties. However, there is often a need for algorithms whose produced graph representations can accurately capture similarity/distance of graphs. This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, we compare the graph representations and similarities produced by these algorithms against those generated by a wellaccepted, but intractable graph similarity function. We also investigate the impact of node attributes on the performance of the different models and kernels. Our results reveal interesting findings. For instance, we find that theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with graph neural networks.","This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, the authors compare the graph representations and similarities produced by these algorithms against those generated by a wellaccepted, but intractable graph similarity function. The authors also investigate the impact of node attributes on the performance of the different models and kernels. The results reveal interesting findings."
58,SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"Image animation generates a video of a source image following the motion of a driving video. Self-supervised image animation approaches do not require explicit pose references as inputs, thus offering large flexibility in learning. State-of-the-art self-supervised image animation approaches mostly warp the source image according to the motion of the driving video, and recover the warping artifacts by inpainting. When the source and the driving images have large pose differences, heavy inpainting is necessary. Without guidance, heavily inpainted regions usually suffer from loss of details. While previous data augmentation techniques such as CutMix are effective in regularizing non-warp-based image generation, directly applying them to image animation ignores the difficulty of inpainting on the warped image. We propose PriorityCut, a novel augmentation approach that uses the top-k percent occluded pixels of the foreground to regularize image animation. By taking into account the difficulty of inpainting, PriorityCut preserves better identity than vanilla CutMix and outperforms state-of-the-art image animation models in terms of the pixel-wise difference, low-level similarity, keypoint distance, and feature embedding distance. Source Image Driving Image Generated Image Occlusion Mask PriorityCut Mask CutMix Image Figure 1: Warp-based image animation warps the source image based on the motion of the driving image and recovers the warping artifacts by inpainting. PriorityCut utilizes the occlusion information in image animation indicating the locations of warping artifacts to regularize discriminator predictions on inpainting. The augmented image has smooth transitions without loss or mixture of context.","This paper proposes a data augmentation method for self-supervised image animation. The proposed method, named PriorityCut, is based on the idea of using top-k percent occlusion masks to regularize the inpainting of the source image and the driving video. The authors claim that the proposed method is more effective than CutMix, which is an existing method that inpaints the inpainted regions of the driving image to improve the quality of the original source image. The main contribution of this paper is to propose a method that takes into account the difficulty of the hard-to-inpainting regions. "
59,SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"Current approaches for learning disentangled representations assume that independent latent variables generate the data through a single data generation process. In contrast, this manuscript considers independent causal mechanisms (ICM), which, unlike disentangled representations, directly model multiple data generation processes (mechanisms) in a coarse granularity. In this work, we aim to learn a model that disentangles each mechanism and approximates the groundtruth mechanisms from observational data. We outline sufficient conditions under which the mechanisms can be learned using a single self-supervised generative model with an unconventional mixture prior, simplifying previous methods. Moreover, we prove the identifiability of our model w.r.t. the mechanisms in the self-supervised scenario. We compare our approach to disentangled representations on various downstream tasks, showing that our approach is more robust to intervention, covariant shift, and noise due to the disentanglement between the data generation processes.","This paper proposes a method for learning disentangled representations of independent causal mechanisms (ICM) by learning a model that disentangles each mechanism and approximates the groundtruth mechanisms from observational data. The authors provide sufficient conditions under which the mechanisms can be learned using a single self-supervised generative model with an unconventional mixture prior, simplifying previous methods. Moreover, they prove the identifiability of their model w.r.t. the mechanisms in the self supervised scenario. They compare their approach to disentangling representations on various downstream tasks, showing that their approach is more robust to intervention, covariant shift, and noise due to the disentanglement between the data generation processes."
60,SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"To be able to predict a molecular graph structure (W ) given a 2D image of a chemical compound (U ) is a challenging problem in machine learning. We are interested to learn f : U →W where we have a fully mediating representation V such that f factors into U → V → W. However, observing V requires detailed and expensive labels. We propose graph aligning approach that generates rich or detailed labels given normal labels W. In this paper we investigate the scenario of domain adaptation from the source domain where we have access to the expensive labels V to the target domain where only normal labels W are available. Focusing on the problem of predicting chemical compound graphs from 2D images the fully mediating layer is represented using the planar embedding of the chemical graph structure we are predicting. The use of a fully mediating layer implies some assumptions on the mechanism of the underlying process. However if the assumptions are correct it should allow the machine learning model to be more interpretable, generalize better and be more data efficient at training time. The empirical results show that, using only 4000 data points, we obtain up to 4x improvement of performance after domain adaptation to target domain compared to pretrained model only on the source domain. After domain adaptation, the model is even able to detect atom types that were never seen in the original source domain. Finally, on the Maybridge data set the proposed self-labeling approach reached higher performance than the current state of the art.",This paper proposes a self-labeling approach for predicting molecular graph structures from 2D images of a chemical compound. The authors propose a graph aligning approach that generates rich or detailed labels given normal labels W in the target domain. The proposed method is evaluated on the Maybridge dataset and compared to the state-of-the-art on the original source domain. 
61,SP:ad906dd9a176cffd283593321ff6b9ad19595528,"In this paper, we are interested in building a domain knowledge based deep learning framework to solve the chiller plants energy optimization problems. Compared to the hotspot applications of deep learning (e.g. image classification and NLP), it is difficult to collect enormous data for deep network training in realworld physical systems. Most existing methods reduce the complex systems into linear model to facilitate the training on small samples. To tackle the small sample size problem, this paper considers domain knowledge in the structure and loss design of deep network to build a nonlinear model with lower redundancy function space. Specifically, the energy consumption estimation of most chillers can be physically viewed as an input-output monotonic problem. Thus, we can design a Neural Network with monotonic constraints to mimic the physical behavior of the system. We verify the proposed method in a cooling system of a data center, experimental results show the superiority of our framework in energy optimization compared to the existing ones.",This paper proposes a method to train a neural network to estimate the energy consumption of a chiller plant. The method is based on the observation that the input to the neural network is monotonic and the output of the network can be viewed as an input-output monotonicity problem. The authors propose to train the network on the input of the chiller system and then train the model on the outputs of the model. The proposed method is evaluated on a cooling system of a data center and compared to the existing methods.
62,SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"Achieving accurate spatio-temporal predictions in large-scale systems is extremely valuable in many real-world applications, such as weather forecasts, retail forecasting, and urban traffic forecasting. So far, most existing methods for multi-horizon, multi-task and multitarget predictions select important predicting variables via their correlations with responses of interest, and thus it is highly possible that many forecasting models generated from those methods are not causal, leading to poor interpretability. The aim of this paper is to develop a collaborative causal spatio-temporal fusion transformer, named CausalTrans, to establish the collaborative causal effects of predictors on multiple forecasting targets, such as supply and demand in ride-sharing platforms. Specifically, we integrate the causal attention with the Conditional Average Treatment Effect (CATE) estimation method in causal inference. Moreover, we propose a novel and fast multi-head attention evolved from Taylor’s expansion instead of softmax, reducing time complexity from O(V) to O(V), where V is the number of nodes in a graph. We further design a spatial graph fusion mechanism to significantly reduce the parameters’ scale. We conduct a wide range of experiments to demonstrate the interpretability of causal attention, the effectiveness of various model components, and the time efficiency of our CausalTrans. As shown in these experiments, our CausalTrans framework can achieve up to 15% error reduction compared with various baseline methods.","This paper proposes a new method for multi-horizon, multi-task and multi-target forecasting based on causal attention. The authors propose to use the Conditional Average Treatment Effect (CATE) estimation method in causal inference. They also propose a novel and fast multi-head attention evolved from Taylor’s expansion instead of softmax, reducing the time complexity from O(V) to $O(V), where V is the number of nodes in a graph. They conduct a wide range of experiments to demonstrate the interpretability of causal attention, the effectiveness of various model components, and the time efficiency of their CausalTrans framework."
63,SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.","This paper proposes an unsupervised method for jointly identifying a mixture of discrete and continuous factors of variability that can help unravel complex phenomena. The proposed method is based on a multi-agent VAE framework that utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. The authors provide theoretical justification to motivate the use of a multi agent framework, and formulate it as a variational inference problem. The experimental results on MNIST and dSprites show that the proposed method achieves state-of-the-art categorical assignment while preserving interpretability of continuous factors."
64,SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"Group equivariant convolutional networks (GCNNs) endow classical convolutional networks with additional symmetry priors, which can lead to a considerably improved performance. Recent advances in the theoretical description of GCNNs revealed that such models can generally be understood as performing convolutions with G-steerable kernels, that is, kernels that satisfy an equivariance constraint themselves. While the G-steerability constraint has been derived, it has to date only been solved for specific use cases – a general characterization of Gsteerable kernel spaces is still missing. This work provides such a characterization for the practically relevant case of G being any compact group. Our investigation is motivated by a striking analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. By generalizing the famous Wigner-Eckart theorem for spherical tensor operators, we prove that steerable kernel spaces are fully understood and parameterized in terms of 1) generalized reduced matrix elements, 2) ClebschGordan coefficients, and 3) harmonic basis functions on homogeneous spaces.","This paper provides a general characterization of steerable kernel spaces for group equivariant convolutional neural networks (GCNNs). The paper is motivated by the analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. In particular, the paper generalizes the famous Wigner-Eckart theorem for spherical tensors to the case of group-equivariant kernels. The paper also provides a characterization of generalized reduced matrix elements, ClebschGordan coefficients, and harmonic basis functions on homogeneous spaces."
65,SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"Selective classification, in which models can abstain on uncertain predictions, is a natural approach to improving accuracy in settings where errors are costly but abstentions are manageable. In this paper, we find that while selective classification can improve average accuracies, it can simultaneously magnify existing accuracy disparities between various groups within a population, especially in the presence of spurious correlations. We observe this behavior consistently across five vision and NLP datasets. Surprisingly, increasing abstentions can even decrease accuracies on some groups. To better understand this phenomenon, we study the margin distribution, which captures the model’s confidences over all predictions. For symmetric margin distributions, we prove that whether selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage (i.e., without any abstentions) and whether the distribution satisfies a property we call left-log-concavity. Our analysis also shows that selective classification tends to magnify full-coverage accuracy disparities. Motivated by our analysis, we train distributionally-robust models that achieve similar full-coverage accuracies across groups and show that selective classification uniformly improves each group on these models. Altogether, our results suggest that selective classification should be used with care and underscore the importance of training models to perform equally well across groups at full coverage.","This paper studies the effect of selective classification on accuracy disparities between different groups in the presence of spurious correlations. The authors study the margin distribution, which captures the model’s confidences over all predictions, for symmetric margin distributions. They prove that selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage (i.e., without any abstentions) and whether the distribution satisfies a property we call left-log-concavity. Motivated by their analysis, the authors train distributionally-robust models that achieve similar full-coverage accuracies across groups and show that they uniformly improve each group on these models."
66,SP:f1d57ee27e901daf7e4e2b84139019e945818911,"There is a significant demand for topic modeling on large-scale data with complex multi-modal structure in applications such as multi-layer network analysis, temporal document classification, and video data analysis; frequently this multi-modal data has latent hierarchical structure. We propose a new hierarchical nonnegative CANDECOMP/PARAFAC (CP) decomposition (hierarchical NCPD) model and a training method, Neural NCPD, for performing hierarchical topic modeling on multi-modal tensor data. Neural NCPD utilizes a neural network architecture and backpropagation to mitigate error propagation through hierarchical NCPD.","This paper proposes a hierarchical nonnegative CANDECOMP/PARAFAC (CP) decomposition (hierarchical NCPD) model and a training method, Neural NCPD, for performing hierarchical topic modeling on multi-modal tensor data. The proposed method utilizes a neural network architecture and backpropagation to mitigate error propagation through hierarchical NCPD. "
67,SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"In tasks like node classification, image segmentation, and named-entity recognition we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respectively. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first collective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural Networks and leverage their locality property – perturbations only affect the predictions in a close neighborhood – to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from 7 to 351.",This paper proposes a new adversarial robustness certificate for graph neural networks (GNNs) that is based on the notion of collective robustness. The key idea is to fuse multiple single-node certificates into a stronger collective certificate that is guaranteed to remain stable under perturbation. The authors propose to use the locality property of GNNs to leverage their locality property. The proposed method is evaluated on the Citeseer dataset and shows that the proposed method can increase the number of certifiable feature perturbations from 7 to 351.
68,SP:cc93dd2f68e415e2457166e78627865dc1b44697,"Learning high-dimensional probability distributions by competitively training generative and discriminative neural networks is a prominent approach of Generative Adversarial Networks (GANs) among generative models to model complex real-world data. Nevertheless, training GANs likely suffer from non-convergence problem, mode collapse and gradient explosion or vanishing. Least Squares GAN (LSGANs) and Wasserstein GANs (WGAN) are of representative variants of GANs in literature that diminish the inherent problems of GANs by proposing the modification methodology of loss functions. However, LSGANs often fall into local minima and cause mode collapse. While WGANs unexpectedly encounter with inefficient computation and slow training due to its constraints in Wasserstein distance approximation. In this paper, we propose Quantile Regression GAN (QRGAN) in which quantile regression is adopted to minimize 1-Wasserstein distance between real and generated data distribution as a novel approach in modification of loss functions for improvement of GANs. To study the culprits of mode collapse problem, the output space of discriminator and gradients of fake samples are analyzed to see if the discriminator guides the generator well. And we found that the discriminator should not be bounded to specific numbers. Our proposed QRGAN exposes high robustness against mode collapse problem. Furthermore, QRGAN obtains an apparent improvement in the evaluation and comparison of Frechet Inception Distance (FID) for generation performance assessment compared to existing variants of GANs.","This paper proposes Quantile Regression GAN (QRGAN), a novel method to improve the performance of GANs. The main idea is to use quantile regression to minimize the 1-Wasserstein distance between the real and generated data distribution. The authors also analyze the output space of discriminator and gradients of fake samples to see if the discriminator guides the generator well. QRGAN is shown to be more robust against mode collapse problem than LSGAN, WGAN, and WGAN."
69,SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,"Explaining the predictions made by complex machine learning models helps users to understand and accept the predicted outputs with confidence. One promising way is to use similarity-based explanation that provides similar instances as evidence to support model predictions. Several relevance metrics are used for this purpose. In this study, we investigated relevance metrics that can provide reasonable explanations to users. Specifically, we adopted three tests to evaluate whether the relevance metrics satisfy the minimal requirements for similarity-based explanation. Our experiments revealed that the cosine similarity of the gradients of the loss performs best, which would be a recommended choice in practice. In addition, we showed that some metrics perform poorly in our tests and analyzed the reasons of their failure. We expect our insights to help practitioners in selecting appropriate relevance metrics and also aid further researches for designing better relevance metrics for explanations.","This paper investigates relevance metrics for explaining the predictions made by machine learning models. The authors evaluate relevance metrics on three different tasks: (1) comparing the cosine similarity of the gradients of the loss, (2) comparing cosine similarities of the gradient of the log-likelihood of the model, and (3) comparing similarity metrics of the classifier. They find that cosine-similarity of gradients is the best metric for explaining model predictions. They also find that some metrics perform poorly in their tests and analyze the reasons of their failure. "
70,SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"This paper advocates incorporating a Low-Rank Global Attention (LRGA) module, a computation and memory efficient variant of the dot-product attention (Vaswani et al., 2017), to Graph Neural Networks (GNNs) for improving their generalization power. To theoretically quantify the generalization properties granted by adding the LRGA module to GNNs, we focus on a specific family of expressive GNNs and show that augmenting it with LRGA provides algorithmic alignment to a powerful graph isomorphism test, namely the 2-Folklore Weisfeiler-Lehman (2-FWL) algorithm. In more detail we: (i) consider the recent Random Graph Neural Network (RGNN) (Sato et al., 2020) framework and prove that it is universal in probability; (ii) show that RGNN augmented with LRGA aligns with 2-FWL update step via polynomial kernels; and (iii) bound the sample complexity of the kernel’s feature map when learned with a randomly initialized two-layer MLP. From a practical point of view, augmenting existing GNN layers with LRGA produces state of the art results in current GNN benchmarks. Lastly, we observe that augmenting various GNN architectures with LRGA often closes the performance gap between different models.","This paper proposes adding a low-rank global attention (LRGA) module to Graph Neural Networks (GNNs) for improving their generalization power. The authors show that adding the LRGA module to a specific family of expressive GNNs provides algorithmic alignment to a powerful graph isomorphism test, namely the 2-Folklore Weisfeiler-Lehman (2-FWL) algorithm. In more detail, the authors consider the recent Random Graph Neural Network (Sato et al., 2020) framework and prove that it is universal in probability; (ii) show that RGNN augmented with LRGA aligns with polynomial kernels; and (iii) bound the sample complexity of kernel’s feature map when learned with a randomly initialized two-layer MLP."
71,SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"This paper concerns the use of objectness measures to improve the calibration performance of Convolutional Neural Networks (CNNs). CNNs have proven to be very good classifiers and generally localize objects well; however, the loss functions typically used to train classification CNNs do not penalize inability to localize an object, nor do they take into account an object’s relative size in the given image. During training on ImageNet-1K almost all approaches use random crops on the images and this transformation sometimes provides the CNN with background only samples. This causes the classifiers to depend on context. Context dependence is harmful for safety-critical applications. We present a novel approach to classification that combines the ideas of objectness and label smoothing during training. Unlike previous methods, we compute a smoothing factor that is adaptive based on relative object size within an image. This causes our approach to produce confidences that are grounded in the size of the object being classified instead of relying on context to make the correct predictions. We present extensive results using ImageNet to demonstrate that CNNs trained using adaptive label smoothing are much less likely to be overconfident in their predictions. We show qualitative results using class activation maps and quantitative results using classification and transfer learning tasks. Our approach is able to produce an order of magnitude reduction in confidence when predicting on context only images when compared to baselines. Using transfer learning, we gain 0.021AP on MS COCO compared to the hard label approach.","This paper proposes an adaptive label smoothing method to improve the calibration performance of convolutional neural networks (CNNs) on ImageNet-1K. The proposed method is based on the idea of objectness, which is a measure of the relative size of an object in an image. The authors propose to compute a smoothing factor that is adaptive based on relative object size within an image, which allows the model to produce confidences that are grounded in the size of the object being classified instead of relying on context to make the correct predictions. Experiments are conducted on classification and transfer learning tasks to demonstrate the effectiveness of the proposed method."
72,SP:5254658923e594294b69d124a8d004166852822a,"Neural networks have shown tremendous potential for reconstructing highresolution images in inverse problems. The non-convex and opaque nature of neural networks, however, hinders their utility in sensitive applications such as medical imaging. To cope with this challenge, this paper advocates a convex duality framework that makes a two-layer fully-convolutional ReLU denoising network amenable to convex optimization. The convex dual network not only offers the optimum training with convex solvers, but also facilitates interpreting training and prediction. In particular, it implies training neural networks with weight decay regularization induces path sparsity while the prediction is piecewise linear filtering. A range of experiments with MNIST and fastMRI datasets confirm the efficacy of the dual network optimization problem.","This paper proposes a dual dual network for inverse image reconstruction. The dual network is a two-layer fully-convolutional ReLU denoising network that is amenable to convex optimization. In particular, the authors propose to train the dual network with weight decay regularization and piecewise linear filtering. Experiments on MNIST and fastMRI datasets confirm the efficacy of the proposed dual network."
73,SP:085cad6bc143c8713580bddfaa71f06496dac314,"Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.1","This paper presents an end-to-end speech synthesis model that learns to synthesise speech from normalised text or phonemes. The authors propose a differentiable alignment scheme based on token length prediction and adversarial feedback to produce high fidelity audio. The proposed model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to state-of-the-art models relying on multi-stage training and additional supervision."
74,SP:01148cea55db606aa78d27e900818684a8bce9ab,"Many real-world graphs are attributed graphs where nodes are associated with non-topological features. While attributes can be missing anywhere in an attributed graph, most of existing node representation learning approaches do not consider such incomplete information. In this paper, we propose a general non-parametric framework to mitigate this problem. Starting from a decomposition of the attribute matrix, we transform node features into discrete distributions in a lower-dimensional space equipped with the Wasserstein metric. On this Wasserstein space, we propose Wasserstein graph diffusion to smooth the distribution representations of nodes with information from their local neighborhoods. This allows us to reduce the distortion caused by missing attributes and obtain integrated representations expressing information of both topology structure and attributes. We then pull the nodes back to the original space and produce corresponding point representations to facilitate various downstream tasks. To show the power of our representation method, we designed two algorithms based on it for node classification (with missing attributes) and matrix completion respectively, and demonstrate their effectiveness in experiments.",This paper proposes a method for node representation learning for attributed graphs with missing attributes. The proposed method is based on a decomposition of the attribute matrix into discrete distributions in a lower-dimensional space equipped with the Wasserstein metric. The authors propose to smooth the distribution representations of nodes with information from their local neighborhoods. This allows them to reduce the distortion caused by missing attributes and obtain integrated representations expressing information of both topology structure and attributes.
75,SP:aeeb5909f7123ef631f569b469af9715205c881f,"A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGO, a novel agent incorporating—as form of meta-learning—a goal-generating teacher that proposes Adversarially Motivated Intrinsic GOals to train a goal-conditioned “student” policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective “constructively adversarial” objective, the teacher learns to propose increasingly challenging—yet achievable—goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.","This paper proposes a meta-learning method for goal-conditioned reinforcement learning (RL) in the absence of extrinsic rewards. In particular, the authors propose to use adversarial training as an intrinsic motivation for the teacher to generate a curriculum of self-proposed goals for the student policy. The teacher learns to propose increasingly challenging goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. The authors show that their method generates a natural curriculum of goals that ultimately allows the agent to solve challenging procedurally-generated tasks where other intrinsic motivation and state-of-the-art RL methods fail."
76,SP:3d05bc7dca97681cb582298e318b9b973841eed3,"We consider the problem of information retrieval from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. Specifically, a user requesting a file from the dataset should be able to reconstruct the requested file with a prescribed distortion, and in addition, the identity of the requested file should be kept private from the server with a prescribed privacy level. The proposed model can be seen as an extension of the well-known concept of private information retrieval by allowing for distortion in the retrieval process and relaxing the perfect privacy requirement. We initiate the study of the tradeoff between download rate, distortion, and user privacy leakage, and show that the optimal rate-distortion-leakage tradeoff is convex and that in the limit of large file sizes this allows for a concise information-theoretical formulation in terms of mutual information. Moreover, we propose a new data-driven framework by leveraging recent advancements in generative adversarial models which allows a user to learn efficient schemes in terms of download rate from the data itself. Learning the scheme is formulated as a constrained minimax game between a user which desires to keep the identity of the requested file private and an adversary that tries to infer which file the user is interested in under a distortion constraint. In general, guaranteeing a certain privacy level leads to a higher rate-distortion tradeoff curve, and hence a sacrifice in either download rate or distortion. We evaluate the performance of the scheme on a synthetic Gaussian dataset as well as on the MNIST and CIFAR-10 datasets. For the MNIST dataset, the data-driven approach significantly outperforms a proposed general achievable scheme combining source coding with the download of multiple files, while for CIFAR-10 the performances are comparable.","This paper studies the problem of information retrieval from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. Specifically, a user requesting a file from the dataset should be able to reconstruct the requested file with a prescribed distortion, and in addition, the identity of requested file should be kept private from the server with a specified privacy level. The paper proposes a data-driven framework by leveraging recent advancements in generative adversarial models which allows a user to learn efficient schemes in terms of download rate from the data itself. The proposed model can be seen as an extension of the well-known concept of private information retrieval by allowing for distortion in the retrieval process and relaxing the perfect privacy requirement. In general, guaranteeing a certain privacy level leads to a higher rate-distortion tradeoff curve, and hence a sacrifice in either download rate or distortion."
77,SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"Graph Neural Networks (GNNs) become very popular for graph-related applications due to their superior performance. However, they have been shown to be computationally expensive in large scale settings, because their produced node embeddings have to be computed recursively, which scales exponentially with the number of layers. To address this issue, several sampling-based methods have recently been proposed to perform training on a subset of nodes while maintaining the fidelity of the trained model. In this work, we introduce a decoupled greedy learning method for GNNs (DGL-GNN) that, instead of sampling the input graph, decouples the GNN into smaller modules and associates each module with greedy auxiliary objectives. Our approach allows GNN layers to be updated during the training process without waiting for feedback from successor layers, thus making parallel GNN training possible. Our method achieves improved efficiency without significantly compromising model performances, which would be important for time or memory limited applications. Further, we propose a lazy-update scheme during training to further improve its efficiency. We empirically analyse our proposed DGL-GNN model, and demonstrate its effectiveness and superior efficiency through a range of experiments. Compared to the sampling-based acceleration, our model is more stable, and we do not have to trade-off between efficiency and accuracy. Finally, we note that while here we focus on comparing the decoupled approach as an alternative to other methods, it can also be regarded as complementary, for example, to sampling and other scalability-enhancing improvements of GNN training.","This paper proposes a decoupled greedy learning method for graph neural networks (GNNs) that decouples the GNN into smaller modules and associates each module with greedy auxiliary objectives. This allows GNN layers to be updated during the training process without waiting for feedback from successor layers, thus making parallel GNN training possible. The paper also proposes a lazy-update scheme during training to further improve its efficiency. Empirical results show that the proposed method achieves improved efficiency without significantly compromising model performances, which is important for time or memory limited applications."
78,SP:5ecb1b288f7fc02aead4493f81640867bc349290,"Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions (∧), disjunctions (∨) and existential quantifiers (∃), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods — black-box neural models trained on millions of generated queries — without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8% up to 40% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online 1.","This paper proposes a method for answering complex queries on incomplete Knowledge Graphs. The proposed method is based on a pre-trained neural link predictor, which is trained to predict the truth value of each query atom. The method is evaluated on a variety of knowledge graph datasets, and the proposed method outperforms the state-of-the-art models trained on millions of generated queries. "
79,SP:f04a522fd04c503754fdb8c52da68646d31271a4,"Local robustness ensures that a model classifies all inputs within an `p-ball consistently, which precludes various forms of adversarial inputs. In this paper, we present a fast procedure for checking local robustness in feed-forward neural networks with piecewise-linear activation functions. Such networks partition the input space into a set of convex polyhedral regions in which the network’s behavior is linear; hence, a systematic search for decision boundaries within the regions around a given input is sufficient for assessing robustness. Crucially, we show how the regions around a point can be analyzed using simple geometric projections, thus admitting an efficient, highly-parallel GPU implementation that excels particularly for the `2 norm, where previous work has been less effective. Empirically we find this approach to be far more precise than many approximate verification approaches, while at the same time performing multiple orders of magnitude faster than complete verifiers, and scaling to much deeper networks. An implementation of our proposed algorithm is available on GitHub1.","This paper proposes a method for checking the local robustness of neural networks with piecewise-linear activation functions. The method is based on the observation that the decision boundaries within a set of convex polyhedral regions around a given input can be analyzed using simple geometric projections. The authors show that the proposed method can be implemented in parallel on the GPU. The proposed method is shown to be far more precise than approximate verification approaches, while at the same time performing multiple orders of magnitude faster than complete verifiers, and scaling to much deeper networks."
80,SP:5297651ff873f97c07b9c47ed3eff52251661844,"In order to interact with objects in our environment, we rely on an understanding of the actions that can be performed on them, and the extent to which they rely or have an effect on the properties of the object. This knowledge is called the object “affordance”. We propose an approach for creating an embedding of objects in an affordance space, in which each dimension corresponds to an aspect of meaning shared by many actions, using text corpora. This embedding makes it possible to predict which verbs will be applicable to a given object, as captured in human judgments of affordance, better than a variety of alternative approaches. Furthermore, we show that the dimensions learned are interpretable, and that they correspond to typical patterns of interaction with objects. Finally, we show that the dimensions can be used to predict a state-of-the-art mental representation of objects, derived purely from human judgements of object similarity.","This paper proposes an approach for embedding objects in an affordance space, in which each dimension corresponds to an aspect of meaning shared by many actions, using text corpora. This embedding makes it possible to predict which verbs will be applicable to a given object, as captured in human judgments of affordance, better than a variety of alternative approaches. Furthermore, the dimensions learned are interpretable, and that they correspond to typical patterns of interaction with objects. The dimensions can be used to predict a state-of-the-art mental representation of objects, derived purely from human judgements of object similarity."
81,SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"Individuality is essential in human society. It induces the division of labor and thus improves the efficiency and productivity. Similarly, it should also be a key to multi-agent cooperation. Inspired by that individuality is of being an individual separate from others, we propose a simple yet efficient method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). EOI learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, and learning the classifier by such observations makes the intrinsic reward signals stronger and in turn makes the agents more identifiable. To further enhance the intrinsic reward and promote the emergence of individuality, two regularizers are proposed to increase the discriminability of the classifier. We implement EOI on top of popular MARL algorithms. Empirically, we show that EOI outperforms existing methods in a variety of multi-agent cooperative scenarios.","This paper proposes a method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL) that learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, which makes the intrinsic reward signals stronger and in turn makes the agents more identifiable. Two regularizers are proposed to increase the discriminability of the classifiers. Empirical results show that EOI outperforms existing methods in a variety of MARL scenarios."
82,SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"Randomized smoothing has achieved state-of-the-art certified robustness against l2-norm adversarial attacks. However, it is not wholly resolved on how to find the optimal base classifier for randomized smoothing. In this work, we employ a Smoothed WEighted ENsembling (SWEEN) scheme to improve the performance of randomized smoothed classifiers. We show the ensembling generality that SWEEN can help achieve optimal certified robustness. Furthermore, theoretical analysis proves that the optimal SWEEN model can be obtained from training under mild assumptions. We also develop an adaptive prediction algorithm to reduce the prediction and certification cost of SWEEN models. Extensive experiments show that SWEEN models outperform the upper envelope of their corresponding candidate models by a large margin. Moreover, SWEEN models constructed using a few small models can achieve comparable performance to a single large model with a notable reduction in training time.","This paper proposes a new method to improve the certified robustness of randomized smoothed classifiers. The proposed method, Smoothed WEighted Ensembling (SWEEN), is based on the ensembling generality of smoothed smoothing. Theoretical analysis shows that the optimal SWEEN model can be obtained from training under mild assumptions. The authors also develop an adaptive prediction algorithm to reduce the prediction and certification cost of the proposed model. Extensive experiments show that the proposed method outperforms the state-of-the-art candidate models by a large margin."
83,SP:ea892e3d199ed6121279b20061a87f43afae8796,"Many complex real-world tasks are composed of several levels of sub-tasks. Humans leverage these hierarchical structures to accelerate the learning process and achieve better generalization. In this work, we study the inductive bias and propose Ordered Memory Policy Network (OMPN) to discover subtask hierarchy by learning from demonstration. The discovered subtask hierarchy could be used to perform task decomposition, recovering the subtask boundaries in an unstructured demonstration. Experiments on Craft and Dial demonstrate that our model can achieve higher task decomposition performance under both unsupervised and weakly supervised settings, comparing with strong baselines. OMPN can also be directly applied to partially observable environments and still achieve higher task decomposition performance. Our visualization further confirms that the subtask hierarchy can emerge in our model 1.",This paper proposes a method to discover subtask hierarchy by learning from demonstration. The method is based on the inductive bias of learning from demonstrations. The proposed method is evaluated on two tasks: Craft and Dial. The experiments show that the proposed method can achieve higher task decomposition performance under both unsupervised and weakly supervised settings.
84,SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"Conventional supervised learning methods, especially deep ones, are found to be sensitive to out-of-distribution (OOD) examples, largely because the learned representation mixes the semantic factor with the variation factor due to their domain-specific correlation, while only the semantic factor causes the output. To address the problem, we propose a Causal Semantic Generative model (CSG) based on a causal reasoning so that the two factors are modeled separately, and develop methods for OOD prediction from a single training domain, which is common and challenging. The methods are based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction. Theoretically, we prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic-identification guarantees the boundedness of OOD generalization error and the success of adaptation. Empirical study shows improved OOD performance over prevailing baselines.","This paper proposes a causal semantic generative model (CSG) for out-of-distribution (OOD) prediction. The proposed model is based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction. In particular, the authors prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic-identification guarantees the boundedness of OOD generalization error and the success of adaptation. Empirical study shows improved OOD performance over prevailing baselines."
85,SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"We study the design of efficient online learning algorithms tolerant to adversarially corrupted rewards. In particular, we study settings where an online algorithm makes a prediction at each time step, and receives a stochastic reward from the environment that can be arbitrarily corrupted with probability ∈ [0, 12 ). Here is the noise rate the characterizes the strength of the adversary. As is standard in online learning, we study the design of algorithms with small regret over a period of time steps. However, while the algorithm observes corrupted rewards, we require its regret to be small with respect to the true uncorrupted reward distribution. We build upon recent advances in robust estimation for unsupervised learning problems to design robust online algorithms with near optimal regret in three different scenarios: stochastic multi-armed bandits, linear contextual bandits, and Markov Decision Processes (MDPs) with stochastic rewards and transitions. Finally, we provide empirical evidence regarding the robustness of our proposed algorithms on synthetic and real datasets.","This paper studies the problem of online learning with adversarially corrupted rewards. In particular, the authors consider the case where the adversarial reward can be arbitrarily corrupted with probability $\mathcal{P}^2$. The authors propose to design algorithms with small regret over a period of time steps, while the algorithm observes corrupted rewards, and require its regret to be small with respect to the true uncorrupted reward distribution. They build upon recent advances in robust estimation for unsupervised learning problems to design robust online algorithms with near optimal regret in three different scenarios: stochastic multi-armed bandits, linear contextual bandits, and Markov Decision Processes (MDPs). They provide empirical evidence regarding the robustness of their proposed algorithms."
86,SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"Encoder-decoder architecture has been widely used in neural machine translation (NMT). A few methods have been proposed to improve it with multiple passes of decoding. However, their full potential is limited by a lack of appropriate termination policy. To address this issue, we present a novel framework, RewriterEvaluator. It consists of a rewriter and an evaluator. Translating a source sentence involves multiple passes. At every pass, the rewriter produces a new translation to improve the past translation and the evaluator estimates the translation quality to decide whether to terminate the rewriting process. We also propose a prioritized gradient descent (PGD) method that facilitates training the rewriter and the evaluator jointly. Though incurring multiple passes of decoding, Rewriter-Evaluator with the proposed PGD method can be trained with similar time to that of training encoder-decoder models. We apply the proposed framework to improve the general NMT models (e.g., Transformer). We conduct extensive experiments on two translation tasks, Chinese-English and English-German, and show that the proposed framework notably improves the performances of NMT models and significantly outperforms previous baselines.","This paper proposes Rewriter-Evaluator, a method for improving the performance of neural machine translation (NMT) models. It consists of a rewriter and an evaluator. At every pass, the rewriter produces a new translation to improve the past translation and the evaluators estimates the translation quality to decide whether to terminate the rewriting process. The authors also propose a prioritized gradient descent (PGD) method to train the evaluation and rewriter jointly. The experiments on two translation tasks, Chinese-English and English-German, show that the proposed framework significantly improves the performances of NMT models."
87,SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"Ambiguities in images or unsystematic annotation can lead to multiple valid solutions in semantic segmentation. To learn a distribution over predictions, recent work has explored the use of probabilistic networks. However, these do not necessarily capture the empirical distribution accurately. In this work, we aim to learn a multimodal predictive distribution, where the empirical frequency of the sampled predictions closely reflects that of the corresponding labels in the training set. To this end, we propose a novel two-stage, cascaded strategy for calibrated adversarial refinement. In the first stage, we explicitly model the data with a categorical likelihood. In the second, we train an adversarial network to sample from it an arbitrary number of coherent predictions. The model can be used independently or integrated into any black-box segmentation framework to facilitate learning of calibrated stochastic mappings. We demonstrate the utility and versatility of the approach by attaining state-of-the-art results on the multigrader LIDC dataset and a modified Cityscapes dataset. In addition, we use a toy regression dataset to show that our framework is not confined to semantic segmentation, and the core design can be adapted to other tasks requiring learning a calibrated predictive distribution.","This paper proposes a method for learning a calibrated adversarial model for semantic segmentation. The proposed method is based on a two-stage approach, where the first stage models the data with a categorical likelihood, and the second stage uses an adversarial network to sample from it an arbitrary number of coherent predictions. The model can be used independently or integrated into any black-box segmentation framework to facilitate learning of calibrated stochastic mappings. The authors demonstrate the utility and versatility of the approach on the multigrader LIDC dataset and a modified Cityscapes dataset."
88,SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information (e.g., stochastic gradients) across the workers. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-K or PowerSGD. In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.","This paper proposes a new method for dealing with contractive compressors. The main idea is to transform any contractive compressor into an induced unbiased compressor. Theoretical analysis and experimental results are provided to show that the proposed method is better than the existing compressed communication with error feedback (EF) method, which is the only known technique that can deal with the error induced by such compressors such as Top-K or PowerSGD. Experimental results are also provided for federated learning with partial participation."
89,SP:4fd702490293e481c79614852ba27dd3ce9215a4,"After developer adjustments to a machine learning (ML) algorithm, how can the results of an old hyperparameter optimization (HPO) automatically be used to speedup a new HPO? This question poses a challenging problem, as developer adjustments can change which hyperparameter settings perform well, or even the hyperparameter search space itself. While many approaches exist that leverage knowledge obtained on previous tasks, so far, knowledge from previous development steps remains entirely untapped. In this work, we remedy this situation and propose a new research framework: hyperparameter transfer across adjustments (HT-AA). To lay a solid foundation for this research framework, we provide four simple HT-AA baseline algorithms and eight benchmarks changing various aspects of ML algorithms, their hyperparameter search spaces, and the neural architectures used. The best baseline, on average and depending on the budgets for the old and new HPO, reaches a given performance 1.2–3.6x faster than a prominent HPO algorithm without transfer. As HPO is a crucial step in ML development but requires extensive computational resources, this speedup would lead to faster development cycles, lower costs, and reduced environmental impacts. To make these benefits available to ML developers off-the-shelf and to facilitate future research on HT-AA, we provide python packages for our baselines and benchmarks.","This paper proposes a new research framework for hyperparameter transfer across adjustments (HT-AA) to speed up the development of machine learning (ML) algorithms. The authors provide four simple baseline algorithms and eight benchmarks changing various aspects of ML algorithms, their hyperparameters search spaces, and the neural architectures used. The best baseline, on average and depending on the budgets for the old and new HPO, reaches a given performance 1.2-3.6x faster than a prominent HPO algorithm without transfer."
90,SP:e8f99bae5853de525450fcb8facd23cf973fc161,"We find that the way we choose to represent data labels can have a profound effect on the quality of trained models. For example, training an image classifier to regress audio labels rather than traditional categorical probabilities produces a more reliable classification. This result is surprising, considering that audio labels are more complex than simpler numerical probabilities or text. We hypothesize that high dimensional, high entropy label representations are generally more useful because they provide a stronger error signal. We support this hypothesis with evidence from various label representations including constant matrices, spectrograms, shuffled spectrograms, Gaussian mixtures, and uniform random matrices of various dimensionalities. Our experiments reveal that high dimensional, high entropy labels achieve comparable accuracy to text (categorical) labels on the standard image classification task, but features learned through our label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data. These results suggest that label representation may play a more important role than previously thought.","This paper studies the effect of label representations on the performance of image classification models. The authors propose to use high dimensional, high entropy label representations, which they argue are more useful because they provide a stronger error signal. In particular, the authors propose constant matrices, spectrograms, shuffled spectrogram, Gaussian mixtures, and uniform random matrices of various dimensionalities. The experiments are conducted on the standard image classification task, where they show that high dimensional labels achieve comparable accuracy to text (categorical) labels, but features learned through our label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data."
91,SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant computational cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved ‘for free’ under a single model’s forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model’s capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.","This paper proposes a method for ensemble neural networks (ENsembling) that uses a single model to train multiple subnetworks that independently learn the task at hand. The authors show that the benefits of using multiple predictions can be achieved ‘for free’ under the single model’s forward pass. In particular, they show that, using a multi-input multi-output (MIMO) configuration, one can utilize the capacity of a single network to train several subnetwork to improve model robustness without increasing compute. They observe a significant improvement in negative log-likelihood, accuracy, and calibration error compared to previous methods."
92,SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"Knowledge Distillation refers to a class of methods that transfers the knowledge from a teacher network to a student network. In this paper, we propose Sparse Representation Matching (SRM), a method to transfer intermediate knowledge obtained from one Convolutional Neural Network (CNN) to another by utilizing sparse representation learning. SRM first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixellevel and image-level labels for training intermediate feature maps of the student network. We formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plugand-play manner. Our experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets.","This paper proposes a method to transfer intermediate knowledge obtained from one Convolutional Neural Network (CNN) to another by utilizing sparse representation learning. The proposed method first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixellevel and image-level labels for training intermediate feature maps of the student network. The authors formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plug-and-play manner. The experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets."
93,SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,"Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs1). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite.","This paper proposes a novel representation learning method for reinforcement learning that leverages the sequential structure in reinforcement learning to improve generalization. Specifically, the authors introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. The authors also present a contrastive representation learning procedure to embed any state similarity metric, which they instantiate with PSM to obtain policy similarity embeddings (PSEs1). The authors demonstrate that PSEs can improve the generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite."
94,SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"A core challenge in Machine Learning is to disentangle natural factors of variation in data (e.g. object shape vs pose). A popular approach to disentanglement consists in learning to map each of these factors to distinct subspaces of a model’s latent representation. However, this approach has shown limited empirical success to date. Here, we show that this approach to disentanglement introduces topological defects (i.e. discontinuities in the encoder) for a broad family of transformations acting on images —encompassing simple affine transformations such as rotations and translations. Moreover, motivated by classical results from group representation theory, we propose an alternative, more flexible approach to disentanglement which relies on distributed equivariant operators, potentially acting on the entire latent space. We theoretically and empirically demonstrate the effectiveness of our approach to disentangle affine transformations. Our work lays a theoretical foundation for the recent success of a new generation of models using distributed operators for disentanglement ::: (see :::::::::: discussion). All code is available at https://anonymous.4open. science/r/5b7e2cbb-54dc-4fde-bc2c-8f75d29fc15a/.","This paper studies the problem of disentangling affine transformations in deep neural networks. The authors show that disentanglement can lead to discontinuities in the encoder for a broad family of transformations such as rotations and translations. To address this problem, the authors propose an alternative, more flexible approach to disentangle these transformations which relies on distributed equivariant operators, potentially acting on the entire latent space. Theoretical and empirical results are provided to demonstrate the effectiveness of the proposed disentangled affine transformation."
95,SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"Hawkes process provides an effective statistical framework for analyzing the timedependent interaction of neuronal spiking activities. Although utilized in many real applications, the classic Hawkes process is incapable of modelling inhibitory interactions among neurons. Instead, the nonlinear Hawkes process allows for a more flexible influence pattern with excitatory or inhibitory interactions. In this paper, three sets of auxiliary latent variables (Pólya-Gamma variables, latent marked Poisson processes and sparsity variables) are augmented to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. As a result, an efficient expectationmaximization (EM) algorithm is derived to obtain the maximum a posteriori (MAP) estimate. We demonstrate the accuracy and efficiency performance of our algorithm on synthetic and real data. For real neural recordings, we show our algorithm can estimate the temporal dynamics of interaction and reveal the interpretable functional connectivity underlying neural spike trains.","This paper proposes a method to estimate the temporal dynamics of neural spike trains using the Hawkes process. The method is based on the Pólya-Gamma variables, latent marked Poisson processes, and sparsity variables, which are augmented to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. The authors demonstrate the accuracy and efficiency performance of their algorithm on synthetic and real data. In the experiments, the authors show that their algorithm can estimate temporal dynamics and reveal the interpretable functional connectivity underlying neural spike train."
96,SP:1156d3deac022829bda930ffcb081947609d972b,"A numerical and phenomenological study of the gradient descent (GD) algorithm for training two-layer neural network models is carried out for different parameter regimes. It is found that there are two distinctive phases in the GD dynamics in the under-parameterized regime: An early phase in which the GD dynamics follows closely that of the corresponding random feature model, followed by a late phase in which the neurons are divided into two groups: a group of a few (maybe none) “activated” neurons that dominate the dynamics and a group of “quenched” neurons that support the continued activation and deactivation process. In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions. This neural network-like behavior is continued into the mildly over-parameterized regime, in which it undergoes a transition to a random featurelike behavior where the inner-layer parameters are effectively frozen during the training process. The quenching process seems to provide a clear mechanism for “implicit regularization”. This is qualitatively different from the GD dynamics associated with the “mean-field” scaling where all neurons participate equally.","This paper studies the gradient descent (GD) algorithm for training two-layer neural network models. It shows that there are two phases in the dynamics in the under-parameterized regime: an early phase in which the GD dynamics follows closely that of the corresponding random feature model, followed by a late phase where the neurons are divided into two groups: a group of a few (maybe none) “activated” neurons that dominate the dynamics and another group of quenched neurons that support the continued activation and deactivation process. In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions. This neural network-like behavior is continued into the mildly over-parametrized regime, in which it undergoes a transition to a random featurelike behavior where the inner-layer parameters are effectively frozen during the training process. This seems to provide a clear mechanism for ""implicit regularization"""
97,SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"Practical reinforcement learning problems are often formulated as constrained Markov decision process (CMDP) problems, in which the agent has to maximize the expected return while satisfying a set of prescribed safety constraints. In this study, we consider a situation in which the agent has access to the generative model which provides us with a next state sample for any given state-action pair, and propose a model to solve a CMDP problem by decomposing the CMDP into a pair of MDPs; reconnaissance MDP (R-MDP) and planning MDP (P-MDP). In R-MDP, we train threat function, the Q-function analogue of danger that can determine whether a given state-action pair is safe or not. In P-MDP, we train a reward-seeking policy while using a fixed threat function to determine the safeness of each action. With the help of generative model, we can efficiently train the threat function by preferentially sampling rare dangerous events. Once the threat function for a baseline policy is computed, we can solve other CMDP problems with different reward and different danger-constraint without the need to re-train the model. We also present an efficient approximation method for the threat function that can greatly reduce the difficulty of solving R-MDP. We will demonstrate the efficacy of our method over classical approaches in benchmark dataset and complex collision-free navigation tasks.","This paper proposes a method to solve constrained Markov decision process (CMDP) problems by decomposing the CMDPs into a pair of MDPs; reconnaissance MDP and planning MDP. In R-MDP, the authors train threat function, the Q-function analogue of danger that can determine whether a given state-action pair is safe or not. In P-PDP, they train a reward-seeking policy while using a fixed threat function to determine the safeness of each action. With the help of generative model, they can efficiently train the threat function by preferentially sampling rare dangerous events. In addition, they also present an efficient approximation method for threat function that can greatly reduce the difficulty of solving R-RDP."
98,SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,"Modern neural architectures for classification tasks are trained using the crossentropy loss, which is widely believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be wellfounded. We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources. Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks. We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy.","This paper argues that the cross-entropy loss is not as good as the square loss for classification tasks. The authors argue that there is little compelling evidence indicating a clear-cut advantage to the cross entropy loss. They argue that the performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. "
99,SP:915f1f0fc4850507c28c1d609239b41775863ebe,"While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent’s parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent’s representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. We’ve made the code associated with this work available at https://github.com/mila-iqia/spr.","This paper proposes a method for self-supervised reinforcement learning in the context of Atari games. The method is based on the idea of self-predictive representations (SPR), which is an exponential moving average of the agent’s parameters and a learned transition model for future state prediction. The authors also propose to add data augmentation to the future prediction loss, which forces the representations to be consistent across multiple views of an observation. The proposed method achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the state-of-the-art."
100,SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"In this paper, we introduce InstantEmbedding, an efficient method for generating single-node representations using local PageRank computations. We theoretically prove that our approach produces globally consistent representations in sublinear time. We demonstrate this empirically by conducting extensive experiments on real-world datasets with over a billion edges. Our experiments confirm that InstantEmbedding requires drastically less computation time (over 9,000 times faster) and less memory (by over 8,000 times) to produce a single node’s embedding than traditional methods including DeepWalk, node2vec, VERSE, and FastRP. We also show that our method produces high quality representations, demonstrating results that meet or exceed the state of the art for unsupervised representation learning on tasks like node classification and link prediction. 1 I N T R O D U C T I O N Graphs are widely used to represent data when are objects connected to each other, such as social networks, chemical molecules, and knowledge graphs. A widely used approach in dealing with graphs is learning compact representations of graphs (Perozzi et al., 2014; Grover & Leskovec, 2016; Abu-El-Haija et al., 2018), which learns a d-dimensional embedding vector for each node in a given graph. Unsupervised embeddings in particular have shown improvements in many downstream machine learning tasks, such as visualization (Maaten & Hinton, 2008), node classification (Perozzi et al., 2014) and link prediction (Abu-El-Haija et al., 2018). Importantly, since such embeddings are learned solely from the structure of the graph, they can be used across multiple tasks and applications. Typically, graph embedding models often assume that graph data fits in memory (Perozzi et al., 2014) and require representations for all nodes to be generated. However, in many real-world applications, it is often the case that graph data is large but also scarcely annotated. For example, the Friendster social graph (Yang & Leskovec, 2015) has only 30% nodes assigned to a community, from its total 65M entries. At the same time, many applications of graph embeddings such as classifying a data item only require one current representation for the item itself, and eventually representations of labeled nodes. Therefore, computing a full graph embedding is at worst infeasible and at best inefficient. These observations motivate the problem which we study in this paper – the Local","This paper proposes InstantEmbedding, an unsupervised representation learning method for graph embeddings. The main idea is to use local PageRank computation to compute the embedding vector for each node in a given graph. The paper theoretically proves that the proposed method can produce representations that are globally consistent in sublinear time. Empirical results on real-world datasets with over a billion edges demonstrate the effectiveness of the proposed algorithm."
101,SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"As large-scale graphs become increasingly more prevalent, it poses significant computational challenges to process, extract and analyze large graph data. Graph coarsening is one popular technique to reduce the size of a graph while maintaining essential properties. Despite rich graph coarsening literature, there is only limited exploration of data-driven methods in the field. In this work, we leverage the recent progress of deep learning on graphs for graph coarsening. We first propose a framework for measuring the quality of coarsening algorithm and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight for the coarse graph may be suboptimal, we parametrize the weight assignment map with graph neural networks and train it to improve the coarsening quality in an unsupervised way. Through extensive experiments on both synthetic and real networks, we demonstrate that our method significantly improves common graph coarsening methods under various metrics, reduction ratios, graph sizes, and graph types. It generalizes to graphs of larger size (25× of training graphs), is adaptive to different losses (differentiable and non-differentiable), and scales to much larger graphs than previous work.","This paper proposes an unsupervised method for graph coarsening based on graph neural networks (GNNs) to improve the quality of the coarsened graph. The authors propose a framework for measuring the quality and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. The paper also proposes to parametrize the edge weight assignment map with GNNs and train it to improve coarsens quality. The experiments on both synthetic and real networks demonstrate that the proposed method significantly improves the quality compared to existing methods. The proposed method generalizes to graphs of larger size (25× of training graphs) and is adaptive to different losses (differentiable and non-differentiable)."
102,SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,"Acoustic properties of objects corresponding to scattering characteristics are frequently used for 3D audio content creation, environmental acoustic effects, localization and acoustic scene analysis, etc. The numeric solvers used to compute these acoustic properties are too slow for interactive applications. We present a novel geometric deep learning algorithm based on discrete-laplacian and implicit encoders to compute these characteristics for general 3D objects at interactive rates. We use a point cloud approximation of each object, and each point is encoded in a high-dimensional latent space. Our multi-layer network can accurately estimate these acoustic properties for arbitrary topologies and takes less than 1ms per object on a NVIDIA GeForce RTX 2080 Ti GPU. We also prove that our learning method is permutation and rotation invariant and demonstrate high accuracy on objects that are quite different from the training data. We highlight its application to generating environmental acoustic effects in dynamic environments.","This paper proposes a method to estimate the acoustic properties of 3D objects based on discrete-laplacian and implicit encoders. The authors use a point cloud approximation of each object, and each point is encoded in a high-dimensional latent space. The proposed method can accurately estimate these acoustic properties for arbitrary topologies and takes less than 1ms per object on a NVIDIA GeForce RTX 2080 Ti GPU. The learning method is permutation and rotation invariant and demonstrate high accuracy on objects that are quite different from the training data."
103,SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model’s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MMREx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (“covariate shift”). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.","This paper proposes Risk Extrapolation (REx) as a form of robust optimization over a perturbation set of extrapolated domains (MMREx), and propose a penalty on the variance of training risks (V-REx). The authors prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (“covariate shift”). The proposed method is able to outperform Invariant Risk Minimization in situations where these types of shift co-occur."
104,SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers’ equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.","This paper proposes a neural operator for solving partial differential equations (PDEs) in Fourier space. The proposed method is based on the Fourier neural operator (Fourier operator) which learns the mapping from any functional parametric dependence to the solution of the PDE. The authors propose to parameterize the integral kernel of the integral of the solution as a Fourier operator, which allows for an expressive and efficient architecture. Experiments on Burgers’ equation, Darcy flow, and Navier-Stokes equation demonstrate the effectiveness of the proposed method. It is up to three orders of magnitude faster compared to traditional PDE solvers."
105,SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"We study the implicit bias of gradient flow (i.e., gradient descent with infinitesimal step size) on linear neural network training. We propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. With this formulation, we can characterize the convergence direction of the network parameters as singular vectors of a tensor defined by the network. For L-layer linear tensor networks that are orthogonally decomposable, we show that gradient flow on separable classification finds a stationary point of the `2/L max-margin problem in a “transformed” input space defined by the network. For underdetermined regression, we prove that gradient flow finds a global minimum which minimizes a norm-like function that interpolates between weighted `1 and `2 norms in the transformed input space. Our theorems subsume existing results in the literature while removing standard convergence assumptions. We also provide experiments that corroborate our analysis.","This paper studies the implicit bias of gradient flow on linear neural network training. The authors propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. With this formulation, the convergence direction of the network parameters can be characterized as singular vectors of the tensor defined by the network. For separable classification, the authors show that gradient flow finds a stationary point of the `2/L max-margin problem in a “transformed” input space. For underdetermined regression, they prove a global minimum which minimizes a norm-like function that interpolates between weighted `1 and `2 norms in the transformed input space, and provide experiments that corroborate their analysis."
106,SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"Slimmable neural networks provide a flexible trade-off front between prediction error and computational cost (such as the number of floating-point operations or FLOPs) with the same storage cost as a single model. They have been proposed recently for resource-constrained settings such as mobile devices. However, current slimmable neural networks use a single width-multiplier for all the layers to arrive at sub-networks with different performance profiles, which neglects that different layers affect the network’s prediction accuracy differently and have different FLOP requirements. Hence, developing a principled approach for deciding widthmultipliers across different layers could potentially improve the performance of slimmable networks. To allow for heterogeneous width-multipliers across different layers, we formulate the problem of optimizing slimmable networks from a multiobjective optimization lens, which leads to a novel algorithm for optimizing both the shared weights and the width-multipliers for the sub-networks. We perform extensive empirical analysis with 15 network and dataset combinations and two types of cost objectives, i.e., FLOPs and memory footprint, to demonstrate the effectiveness of the proposed method compared to existing alternatives. Quantitatively, improvements up to 1.7% and 8% in top-1 accuracy on the ImageNet dataset can be attained for MobileNetV2 considering FLOPs and memory footprint, respectively. Our results highlight the potential of optimizing the channel counts for different layers jointly with the weights for slimmable networks.","This paper proposes a method for optimizing slimmable neural networks. The main idea is to optimize the width multipliers for different layers of the network. The authors propose to use a multi-objective optimization lens to optimize both the shared weights and the width-multipliers for the sub-networks. The proposed method is evaluated on 15 network and dataset combinations and two types of cost objectives, i.e., FLOPs and memory footprint, to demonstrate the effectiveness of the proposed method."
107,SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"While existing federated learning approaches mostly require that clients have fullylabeled data to train on, in realistic settings, data obtained at the client-side often comes without any accompanying labels. Such deficiency of labels may result from either high labeling cost, or difficulty of annotation due to the requirement of expert knowledge. Thus the private data at each client may be either partly labeled, or completely unlabeled with labeled data being available only at the server, which leads us to a new practical federated learning problem, namely Federated SemiSupervised Learning (FSSL). In this work, we study two essential scenarios of FSSL based on the location of the labeled data. The first scenario considers a conventional case where clients have both labeled and unlabeled data (labels-at-client), and the second scenario considers a more challenging case, where the labeled data is only available at the server (labels-at-server). We then propose a novel method to tackle the problems, which we refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of federated learning and semi-supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabeled data. Through extensive experimental validation of our method in the two different scenarios, we show that our method outperforms both local semi-supervised learning and baselines which naively combine federated learning with semi-supervised learning. The code is available at https://github.com/wyjeong/FedMatch.","This paper studies the problem of federated semi-supervised learning (FSSL) in the setting where the data obtained at the client-side often comes without any accompanying labels. The authors study two scenarios of FSSL based on the location of the labeled data. The first scenario considers a conventional case where clients have both labeled and unlabeled data (labels-at-client), and the second scenario considers the more challenging case, where the labelled data is only available at the server. They propose a novel method to tackle the problems, which they refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of local federated learning and semi supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning. The experimental validation of FedMatch in the two different scenarios is extensive and the results show that FedMatch outperforms both local local semi supervised learning and baselines."
108,SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,"We address the problem of self-supervised learning on discrete event sequences generated by real-world users. Self-supervised learning incorporates complex information from the raw data in low-dimensional fixed-length vector representations that could be easily applied in various downstream machine learning tasks. In this paper, we propose a new method CoLES, which adopts contrastive learning, previously used for audio and computer vision domains, to the discrete event sequences domain in a self-supervised setting. Unlike most previous studies, we theoretically justify under mild conditions that the augmentation method underlying CoLES provides representative samples of discrete event sequences. We evaluated CoLES on several public datasets and showed that CoLES representations consistently outperform other methods on different downstream tasks.","This paper proposes a self-supervised learning method for discrete event sequences. The method is based on contrastive learning, which is an extension of contrastive methods for audio and computer vision. The authors show that the proposed method can be applied to the discrete event sequence domain in a self supervised setting. The proposed method is evaluated on several public datasets and showed that it can outperform other methods on different downstream tasks."
109,SP:385942a5bcee7384bb722a1669b541f2fac0cd36,"There are two major classes of natural language grammars — the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words. While previous unsupervised parsing methods mostly focus on only inducing one class of grammars, we introduce a novel model, StructFormer, that can induce dependency and constituency structure at the same time. To achieve this, we propose a new parsing framework that can jointly generates constituency tree and dependency graph. Then we integrate the induced dependency relations into transformer, in a differentiable manner, through a novel dependency-constrained self-attention mechanism. Experimental results show that our model can achieve strong results on unsupervised constituency parsing, unsupervised dependency parsing and masked language modeling at the same time.","This paper proposes a method for unsupervised parsing of natural language grammars. The main idea is to combine the dependency grammar and constituency grammar in the same model. The authors propose a new parsing framework that can jointly generate constituency tree and dependency graph, and integrate the induced dependency relations into transformer, in a differentiable manner, through a dependency-constrained self-attention mechanism. Experimental results show that the proposed method can achieve strong results on unsupervisory constituency parsing, unsuper supervised dependency parsing and masked language modeling at the same time."
110,SP:078966ff62775bba6031e47d374bda95f4a7dde3,"Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method.",This paper proposes a method for learning the mapping between scene graph nodes and visual objects under weak supervision. The proposed method learns a metric among visual objects and scene graph node by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that the proposed method post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graphs parsing task verify the grounding found by the proposed model.
111,SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the latent space. A recent attempt to reduce the inner discrepancy between the prior and aggregated posterior distributions is to incorporate sliced fused GromovWasserstein (SFG) between these distributions. That approach has a weakness since it treats every slicing direction similarly, meanwhile several directions are not useful for the discriminative task. To improve the discrepancy and consequently the relational regularization, we propose a new relational discrepancy, named spherical sliced fused Gromov Wasserstein (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, we introduce two variants of SSFG to improve its performance. The first variant, named mixture spherical sliced fused Gromov Wasserstein (MSSFG), replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fused Gromov Wasserstein (PSSFG), replaces the vMF distribution by a power spherical distribution to improve the sampling time in high dimension settings. We then apply the new discrepancies to the RAE framework to achieve its new variants. Finally, we conduct extensive experiments to show that the new proposed autoencoders have favorable performance in learning latent manifold structure, image generation, and reconstruction.","This paper proposes two variants of sliced fused Gromov Wasserstein (SFG) to improve the performance of relational regularized autoencoder (RAE) framework. The first variant, called mixture spherical sliced fused gromov wasserstein, replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fg, is a power spherical distribution that improves the sampling time in high-dimensional settings. The experimental results on latent manifold structure, image generation, and reconstruction demonstrate the effectiveness of the proposed SSFG variants."
112,SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"It has been widely observed that increasing deep learning model sizes often leads to significant performance improvements on a variety of natural language processing and computer vision tasks. In the meantime, however, computational costs and training time would dramatically increase when models get larger. In this paper, we propose a simple approach to speed up training for a particular kind of deep networks which contain repeated structures, such as the transformer module. In our method, we first train such a deep network with the weights shared across all the repeated layers till some point. We then stop weight sharing and continue training until convergence. The untying point is automatically determined by monitoring gradient statistics. Our adaptive untying criterion is obtained from a theoretic analysis over deep linear networks. Empirical results show that our method is able to reduce the training time of BERT by 50%.","This paper proposes an adaptive untying criterion to speed up training for deep networks with repeated structures, such as the transformer module. The authors first train such a deep network with the weights shared across all the repeated layers till some point, then stop weight sharing and continue training until convergence. The untying point is automatically determined by monitoring gradient statistics. Empirical results show that the proposed method is able to reduce the training time of BERT by 50%."
113,SP:a51710551142316b67e2fccd969fea1ece35ba39,"In this paper, we use the interaction inside adversarial perturbations to explain and boost the adversarial transferability. We discover and prove the negative correlation between the adversarial transferability and the interaction inside adversarial perturbations. The negative correlation is further verified through different DNNs with various inputs. Moreover, this negative correlation can be regarded as a unified perspective to understand current transferability-boosting methods. To this end, we prove that some classic methods of enhancing the transferability essentially decease interactions inside adversarial perturbations. Based on this, we propose to directly penalize interactions during the attacking process, which significantly improves the adversarial transferability. Our code is available online1.","This paper studies the adversarial transferability and the interaction inside adversarial perturbations. The authors show that the transferability of adversarial attacks is negatively correlated with the interaction between adversarial and non-adversarial components of the perturbation. To this end, the authors propose to penalize interactions during the attacking process, which significantly improves the adversarially transferable attack."
114,SP:f1565319075c1442c2cb52d96443facb492c06c2,"Catastrophic forgetting is a recurring challenge to developing versatile deep learning models. Despite its ubiquity, there is limited understanding of its connections to neural network (hidden) representations and task semantics. In this paper, we address this important knowledge gap. Through quantitative analysis of neural representations, we find that deeper layers are disproportionately responsible for forgetting, with sequential training resulting in an erasure of earlier task representational subspaces. Methods to mitigate forgetting stabilize these deeper layers, but show diversity on precise effects, with some increasing feature reuse while others store task representations orthogonally, preventing interference. These insights also enable the development of an analytic argument and empirical picture relating forgetting to task semantic similarity, where we find that maximal forgetting occurs for task sequences with intermediate similarity.","This paper studies the problem of forgetting in deep neural networks (DNNs) in the context of task semantic similarity. The authors show that deeper layers of DNNs are disproportionately responsible for forgetting, with sequential training resulting in an erasure of earlier task representational subspaces. To mitigate this forgetting, the authors propose several methods to stabilize these deeper layers, but show diversity on precise effects, with some increasing feature reuse while others store task representations orthogonally, preventing interference. They also provide an analytic argument and empirical picture relating forgetting to task semantic similarities, where they find that maximal forgetting occurs for task sequences with intermediate similarity."
115,SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"Deep, heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many natural language processing (NLP) tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring expensive training process. Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands. In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training. We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks. Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35∼45% less training time.","This paper proposes a method for pre-training and fine-tuning BERT, which is inspired by the early-Bird Lottery Tickets (EBT) algorithm for computer vision tasks. The authors propose to reduce the self-attention and fully-connected sub-layers inside a transformer in BERT by using structured winning tickets in the early stage of BERT training. The proposed method is evaluated on GLUE and SQuAD downstream tasks and achieves comparable performance to standard BERT."
116,SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"We show when maximizing a properly defined f -divergence measure with respect to a classifier’s predictions and the supervised labels is robust with label noise. Leveraging its variational form, we derive a nice decoupling property for a family of f -divergence measures when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. The above derivation helps us analyze the robustness of different f -divergence functions. With established robustness, this family of f -divergence functions arises as useful metrics for the problem of learning with noisy labels, which do not require the specification of the labels’ noise rate. When they are possibly not robust, we propose fixes to make them so. In addition to the analytical results, we present thorough experimental evidence. Our code is available at https: //github.com/UCSC-REAL/Robust-f-divergence-measures.","This paper studies the robustness of f-divergence measures in the context of label noise. The authors derive a new family of f divergence measures that are robust to label noise and show that they can be used as metrics for the problem of learning with noisy labels. In particular, the authors show that the proposed measure is a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. In addition to the theoretical results, the paper also provides experimental results on MNIST and CIFAR-10."
117,SP:841888179dcdac901889c8d62cb5234311fe28f1,"Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from low signal and even instability in Q-learning because target values are derived from current Q-estimates, which are often noisy. To mitigate the issue, we propose ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble. We empirically observe that the proposed method stabilizes and improves learning on both continuous and discrete control benchmarks. We also specifically investigate the signal-to-noise aspect by studying environments with noisy rewards, and find that weighted Bellman backups significantly outperform standard Bellman backups. Furthermore, since our weighted Bellman backups rely on maintaining an ensemble, we investigate how weighted Bellman backups interact with UCB Exploration. By enforcing the diversity between agents using Bootstrap, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both lowdimensional and high-dimensional environments.",This paper proposes an ensemble-based weighted Bellman backup method for off-policy deep reinforcement learning. The authors propose to re-weight the target Q-values based on uncertainty estimates from a Q-ensemble. The proposed method is evaluated on both continuous and discrete control tasks on both low-dimensional and high-dimensional environments. The experiments show that the proposed method stabilizes and improves the performance of existing algorithms such as Soft Actor-Critic and Rainbow DQN.
118,SP:afc08f203562b841180811aef943bfb63a1659ea,"As numerous meta-learning algorithms improve performance when solving fewshot classification problems for practical applications, accurate prediction of uncertainty, though challenging, has been considered essential. In this study, we contemplate modeling uncertainty in a few-shot classification framework and propose a straightforward method that appropriately predicts task uncertainty. We suppose that the random sampling of tasks can generate those in which it may be hard for the model to infer the queries from the support examples. Specifically, measuring the distributional mismatch between support and query sets via class-wise similarities, we propose novel meta-training that lets the model predict with careful confidence. Moreover, our method is algorithm-agnostic and readily expanded to include a range of meta-learning models. Through extensive experiments including dataset shift, we present that our training strategy helps the model avoid being indiscriminately confident, and thereby, produce calibrated classification results without the loss of accuracy.","This paper proposes a meta-training method for few-shot classification. The authors propose to train a class-wise similarity measure between support and query sets in the meta-learning setting. The proposed method is algorithm-agnostic and can be easily extended to a range of meta learning models. The experiments show that the proposed method can help the model avoid being indiscriminately confident, and thereby produce calibrated classification results without the loss of accuracy."
119,SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"The dominant paradigm for learning video-text representations – noise contrastive learning – increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semanticallyrelated – for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample’s caption must be reconstructed as a weighted combination of other support samples’ visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD for video-to-text and text-to-video retrieval.","This paper proposes a method for video-text representation learning based on a generative model. The authors argue that noise contrastive learning (NCL) encourages dissimilar representations even for samples that are semantically related. To address this issue, the authors propose to use a weighted combination of other support samples’ visual representations. The proposed method is evaluated on MSR-VTT, VATEX, ActivityNet, and MSVD for video to text and text to video retrieval."
120,SP:8a71d8fad25a126aff01431cacf348c05de75667,"Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert Devlin et al. (2018), which is based on Chinese characters. Second, the masked language model pre-training is based on a single vocabulary, which limits its downstream task performances. In this work, we first propose a novel method, seg tok, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness. Experiments show that: (a) compared with char based vocabulary, seg tok does not only improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs’ downstream performance, especially it can improve seg tok’s performances on sequence labeling tasks.","This paper proposes a novel method, seg tok, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. The authors also propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness. Experiments show that seg-tok improves the performance of Chinese PLMs on sentence level tasks, and MVP improves the downstream performance on sequence labeling tasks."
121,SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art model for graph-based learning tasks. However, it is still challenging to train GCNs at scale, limiting their applications to real-world large graphs and hindering the exploration of deeper and more sophisticated GCN architectures. While it can be natural to leverage graph partition and distributed training for tackling this challenge, this direction has only been slightly touched on previously due to the unique challenge posed by the GCN structures, especially the excessive amount of boundary nodes in each partitioned subgraph, which can easily explode the required memory and communications for distributed training of GCNs. To this end, we propose BDS-GCN, a method that adopts unbiased boundary sampling strategy to enable efficient and scalable distributed GCN training while maintaining the full-graph accuracy. Empirical evaluations and ablation studies validate the effectiveness of the proposed BDS-GCN, e.g., boosting the throughput by up-to 500% and reducing the memory usage by up-to 58% for distributed GCN training, while achieving the same accuracy, as compared with the state-of-the-art methods. We believe our BDS-GCN would open up a new paradigm for enabling GCN training at scale. All code will be released publicly upon acceptance.",This paper proposes an unbiased boundary sampling strategy for distributed training of graph convolutional networks (GCN) in order to reduce the memory and communication cost of distributed GCN training while maintaining the full-graph accuracy. The proposed method is based on the idea of partitioning the graph into subgraphs and then sampling the boundary nodes of each subgraph based on an unbiased sampling strategy. The authors evaluate the proposed method on a variety of benchmark datasets and show that it can significantly reduce the communication cost and memory usage while keeping the same accuracy. 
122,SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"Machine Learning (ML) has a potential to dramatically accelerate large-scale physics-based simulations. However, practical models for real large-scale and complex problems remain out of reach. Here we present ForceNet, a model for accurate and fast quantum chemistry simulations to accelerate catalyst discovery for renewable energy applications. ForceNet is a graph neural network that uses surrounding 3D molecular structure to estimate per-atom forces—a central capability for performing atomic simulations. The key challenge is to accurately capture highly complex and non-linear quantum interactions of atoms in 3D space, on which forces are dependent. To this end, ForceNet adopts (1) expressive message passing architecture, (2) appropriate choice of basis and non-linear activation functions, and (3) model scaling in terms of network depth and width. We show ForceNet reduces the estimation error of atomic forces by 30% compared to existing ML models, and generalizes well to out-of-distribution structures. Finally, we apply ForceNet to the large-scale catalyst dataset, OC20. We use ForceNet to perform quantum chemistry simulations, where ForceNet is able to achieve 4× higher success rate than existing ML models. Overall, we demonstrate the potential for ML-based simulations to achieve practical usefulness while being orders of magnitude faster than physics-based simulations.","This paper presents a graph neural network (GNN) based model for quantum chemistry simulations. The model is based on graph neural networks (GANs) that use 3D molecular structure to estimate per-atom forces, which is a central capability for performing atomic simulations. In particular, the authors propose a message-passing architecture, expressive message passing architecture, non-linear activation functions, and model scaling in terms of network depth and width. The authors show that the proposed model reduces the estimation error of atomic forces by 30% compared to existing ML models, and generalizes well to out-of-distribution structures. The experiments are conducted on the large-scale catalyst dataset, OC20."
123,SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,"We investigate approaches to regularisation during fine-tuning of deep neural networks. First we provide a neural network generalisation bound based on Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Our bound is highly relevant for fine-tuning, because providing a network with a good initialisation based on transfer learning means that learning can modify the weights less, and hence achieve tighter generalisation. Inspired by this, we develop a simple yet effective fine-tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre-trained weights, thus obtaining provably better generalisation performance than conventional transfer learning. Empirical evaluation shows that our algorithm works well, corroborating our theoretical results. It outperforms both state of the art fine-tuning competitors, and penalty-based alternatives that we show do not directly constrain the radius of the search space.","This paper proposes a new generalization bound for neural networks based on Rademacher complexity for convolutional networks, which is based on the distance between the weights of the network and the original weights. The authors show that this bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to CNNs. This bound is highly relevant for fine-tuning, because providing a network with a good initialisation based on transfer learning means that learning can modify the weights less, and hence achieve tighter generalization. Inspired by this, the authors develop a simple yet effective finetuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre-trained weights, thus obtaining provably better generalisation performance than conventional transfer learning. Empirical evaluation shows that their algorithm works well, corroborating their theoretical results."
124,SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"A common paradigm in model pruning is to train a model, prune, and then either fine-tune or, in the lottery ticket framework, reinitialize and retrain. Prior work has implicitly assumed that the best training configuration for model evaluation is also the best configuration for mask discovery. However, what if a training configuration which yields worse performance actually yields a mask which trains to higher performance? To test this, we decoupled the hyperparameters for mask discovery (Hfind) and mask evaluation (Heval). Using unstructured magnitude pruning on vision classification tasks, we discovered the “decoupled find-eval phenomenon,” in which certain Hfind values lead to models which have lower performance, but generate masks with substantially higher eventual performance compared to using the same hyperparameters for both stages. We show that this phenomenon holds across a number of models, datasets, configurations, and also for one-shot structured pruning. Finally, we demonstrate that different Hfind values yield masks with materially different layerwise pruning ratios and that the decoupled find-eval phenomenon is causally mediated by these ratios. Our results demonstrate the practical utility of decoupling hyperparameters and provide clear insights into the mechanisms underlying this counterintuitive effect.","This paper studies the phenomenon of ""decoupled find-eval phenomenon"" in unstructured magnitude pruning on vision classification tasks. The authors propose to decouple the hyperparameters for mask discovery (Hfind) and mask evaluation (Heval) and show that the decoupled Hfind values lead to models which have lower performance, but generate masks with substantially higher eventual performance compared to using the same hyperparameter for both stages. They show that this phenomenon holds across a number of models, datasets, configurations, and also for one-shot structured pruning. Finally, they demonstrate that different layerwise pruning ratios are causally mediated by these ratios."
125,SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"We propose a new metric (m-coherence) to experimentally study the alignment of per-example gradients during training. Intuitively, given a sample of size m, m-coherence is the number of examples in the sample that benefit from a small step along the gradient of any one example on average. We show that compared to other commonly used metrics, m-coherence is more interpretable, cheaper to compute (O(m) instead ofO(m)) and mathematically cleaner. (We note thatm-coherence is closely connected to gradient diversity, a quantity previously used in some theoretical bounds.) Using m-coherence, we study the evolution of alignment of per-example gradients in ResNet and EfficientNet models on ImageNet and several variants with label noise, particularly from the perspective of the recently proposed Coherent Gradients (CG) theory that provides a simple, unified explanation for memorization and generalization [Chatterjee, ICLR 20]. Although we have several interesting takeaways, our most surprising result concerns memorization. Naïvely, one might expect that when training with completely random labels, each example is fitted independently, and so m-coherence should be close to 1. However, this is not the case: m-coherence reaches moderately high values during training (though still much smaller than real labels), indicating that over-parameterized neural networks find common patterns even in scenarios where generalization is not possible. A detailed analysis of this phenomenon provides both a deeper confirmation of CG, but at the same point puts into sharp relief what is missing from the theory in order to provide a complete explanation of generalization in neural networks.","This paper proposes a new metric, called m-coherence, to study the alignment of per-example gradients during training. The metric is based on gradient diversity, which is a quantity previously used in some theoretical bounds. The authors show that m-Coherence is more interpretable, cheaper to compute (O(m) instead of O(m)) and mathematically cleaner than gradient diversity. They also show that the most surprising result of the paper is that memorization is not as strong as expected when training with random labels."
126,SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"We consider the fundamental problem of how to automatically construct summary statistics for implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. We apply our approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks.","This paper considers the problem of how to automatically construct statistics for implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. The authors apply their approach to both approximate approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks."
127,SP:c5997bf2348e94949684f45fbd418661e85220c1,"Every recent image-to-image translation model uses either image-level (i.e. inputoutput pairs) or set-level (i.e. domain labels) supervision at a minimum. However, even the set-level supervision can be a serious bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose a truly unsupervised image-to-image translation model (TUNIT) that simultaneously learns to separate image domains and translate input images into the estimated domains. Experimental results show that our model achieves comparable or even better performance than the set-level supervised model trained with full labels, generalizes well on various datasets, and is robust against the choice of hyperparameters (e.g. the preset number of pseudo domains). In addition, TUNIT extends well to the semi-supervised scenario with various amount of labels provided.",This paper proposes a new unsupervised image-to-image translation model (TUNIT) that simultaneously learns to separate image domains and translate input images into the estimated domains. TUNIT is trained with a set-level supervised model trained with full labels. Experimental results show that the proposed model achieves comparable or better performance than the set level supervised model. The proposed model generalizes well on various datasets and is robust against the choice of hyperparameters.
128,SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"We investigate gradient descent training of wide neural networks and the corresponding implicit bias in function space. For 1D regression, we show that the solution of training a width-n shallow ReLU network is within n−1/2 of the function which fits the training data and whose difference from initialization has smallest 2-norm of the weighted second derivative with respect to the input. The curvature penalty function 1/ζ is expressed in terms of the probability distribution that is utilized to initialize the network parameters, and we compute it explicitly for various common initialization procedures. For instance, asymmetric initialization with a uniform distribution yields a constant curvature penalty, and thence the solution function is the natural cubic spline interpolation of the training data. While similar results have been obtained in previous works, our analysis clarifies important details and allows us to obtain significant generalizations. In particular, the result generalizes to multivariate regression and different activation functions. Moreover, we show that the training trajectories are captured by trajectories of spatially adaptive smoothing splines with decreasing regularization strength.","This paper studies the implicit bias of gradient descent training of wide neural networks and the corresponding implicit bias in function space. The authors show that the solution of training a width-n shallow ReLU network is within n-1/2 of the function which fits the training data and whose difference from initialization has smallest 2-norm of the weighted second derivative with respect to the input. The curvature penalty function 1/ζ is expressed in terms of the probability distribution that is utilized to initialize the network parameters, and they compute it explicitly for various common initialization procedures such as asymmetric initialization with a uniform distribution and cubic spline interpolation. The result generalizes to multivariate regression and different activation functions and the training trajectories are captured by trajectories of spatially adaptive smoothing splines with decreasing regularization strength."
129,SP:8b885142facbb3b8db41ec9d83822cee81324694,"Weight decay is a popular regularization technique for training of deep neural networks. Modern deep learning libraries mainly use L2 regularization as the default implementation of weight decay. Loshchilov & Hutter (2018) demonstrated that L2 regularization is not identical to weight decay for adaptive gradient methods, such as Adaptive Momentum Estimation (Adam), and proposed Adam with Decoupled Weight Decay (AdamW). However, we found that the popular implementations of weight decay, including L2 regularization and decoupled weight decay, in modern deep learning libraries usually damage performance. First, the L2 regularization is unstable weight decay for all optimizers that use Momentum, such as stochastic gradient descent (SGD). Second, decoupled weight decay is highly unstable for all adaptive gradient methods. We further propose the Stable Weight Decay (SWD) method to fix the unstable weight decay problem from a dynamical perspective. The proposed SWD method makes significant improvements over L2 regularization and decoupled weight decay in our experiments. Simply fixing weight decay in Adam by SWD, with no extra hyperparameter, can outperform complex Adam variants, which have more hyperparameters.",This paper studies the problem of weight decay in deep neural networks. The authors argue that the L2 regularization and decoupled weight decay are not identical to weight decay for adaptive gradient methods such as Adaptive Momentum Estimation (Adam) and proposed Adam with Decoupled Weight Decay (AdamW). They further propose the stable weight decay (SWD) method to fix the unstable weight decay problem from a dynamical perspective. The proposed SWD method makes significant improvements over L2-regularization and AdamW in the experiments.
130,SP:a3206dc71e32ba1830895bf442d3840f3331a532,"Many studies have proven that Translation Memory (TM) can help improve the translation quality of neural machine translation (NMT). Existing ways either employ extra encoder to encode information from TM or concatenate source sentence and TM sentences as encoder’s input. These previous methods don’t model the semantic relationship between the source sentence and TM sentences. Meanwhile, the training corpus related to TM is limited, and the sentence level retrieval approach further limits its scale. In this paper, we propose a novel method to combine the strengths of both TM and NMT. We treat the matched sentence pair of TM as the additional signal and apply one encoder enhanced by the pre-trained language model (PLM) to encode the TM information and source sentence together. Additionally, we extend the sentence level retrieval method to the n-gram retrieval method that we don’t need to calculate the similarity score. Further, we explore new methods to manipulate the information flow from TM to the NMT decoder. We validate our proposed methods on a mixed test set of multiple domains. Experiment results demonstrate that the proposed methods can significantly improve the translation quality and show strong adaptation for an unknown or new domain.",This paper proposes a novel method to combine the strengths of both Translation Memory (TM) and Neural Machine Translation (NMT). The authors treat the matched sentence pair of TM as the additional signal and apply one encoder enhanced by the pre-trained language model (PLM) to encode the TM information and source sentence together. The authors extend the sentence level retrieval method to the n-gram retrieval method and explore new methods to manipulate the information flow from TM to the NMT decoder. The experimental results demonstrate that the proposed methods can significantly improve the translation quality and show strong adaptation for an unknown or new domain.
131,SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,"ABSTRACT This work attempts to interpret modern deep (convolutional) networks from the principles of rate reduction and (shift) invariant classification. We show that the basic iterative gradient ascent scheme for maximizing the rate reduction of learned features naturally leads to a deep network, one iteration per layer. The architectures, operators (linear or nonlinear), and parameters of the network are all explicitly constructed layer-by-layer in a forward propagation fashion. All components of this “white box” network have precise optimization, statistical, and geometric interpretation. Our preliminary experiments indicate that such a network can already learn a good discriminative deep representation without any back propagation training. Moreover, all linear operators of the so-derived network naturally become multi-channel convolutions when we enforce classification to be rigorously shift-invariant. The derivation also indicates that such a convolutional network is significantly more efficient to learn and construct in the spectral domain.","This paper presents a theoretical analysis of convolutional neural networks based on rate reduction and shift-invariant classification. In particular, the authors show that the basic iterative gradient ascent scheme for maximizing the rate reduction of learned features naturally leads to a deep network, one iteration per layer, and that the architectures, operators (linear or nonlinear), and parameters of the network are all explicitly constructed layer-by-layer in a forward propagation fashion. The authors also show that such a network can already learn a good discriminative deep representation without any back propagation training. Moreover, all linear operators of the so-derived network naturally become multi-channel convolutions when we enforce classification to be rigorously shift invariant. Finally, the derivation also indicates that the derived network is significantly more efficient to learn in the spectral domain."
132,SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"We study the implicit acceleration of gradient flow in over-parameterized two-layer linear models. We show that implicit acceleration emerges from a conservation law that constrains the dynamics to follow certain trajectories. More precisely, gradient flow preserves the difference of the Gramian matrices of the input and output weights and we show that the amount of acceleration depends on both the magnitude of that difference (which is fixed at initialization) and the spectrum of the data. In addition, and generalizing prior work, we prove our results without assuming small, balanced or spectral initialization for the weights, and establish interesting connections between the matrix factorization problem and Riccati type differential equations.","This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models. The authors show that implicit acceleration emerges from a conservation law that constrains the dynamics to follow certain trajectories. More precisely, gradient flow preserves the difference of the Gramian matrices of the input and output weights and the amount of acceleration depends on both the magnitude of that difference (which is fixed at initialization) and the spectrum of the data. In addition, and generalizing prior work, the authors prove their results without assuming small, balanced or spectral initialization for the weights."
133,SP:e5f086c806be88d50e461a782b5b00124f4656fb,"Modern machine learning techniques have enjoyed widespread success, but are plagued by lack of transparency in their decision making, which has led to the emergence of the field of explainable AI. One popular approach called LIME, seeks to explain an opaque model’s behavior, by training a surrogate interpretable model to be locally faithful on perturbed instances. Despite being model-agnostic and easyto-use, it is known that LIME’s explanations can be unstable and are susceptible to adversarial attacks as a result of Out-Of-Distribution (OOD) sampling. The quality of explanations is also calculated heuristically, and lacks a strong theoretical foundation. In spite of numerous attempts to remedy some of these issues, making the LIME framework more trustworthy and reliable remains an open problem. In this work, we demonstrate that the OOD sampling problem stems from rigidity of the perturbation procedure. To resolve this issue, we propose a theoretically sound framework based on uniform sampling of user-defined subspaces. Through logical constraints, we afford the end-user the flexibility to delineate the precise subspace of the input domain to be explained. This not only helps mitigate the problem of OOD sampling, but also allow experts to drill down and uncover bugs and biases hidden deep inside the model. For testing the quality of generated explanations, we develop an efficient estimation algorithm that is able to certifiably measure the true value of metrics such as fidelity up to any desired degree of accuracy, which can help in building trust in the generated explanations. Our framework called CLIME can be applied to any ML model, and extensive experiments demonstrate its versatility on real-world problems.","This paper proposes a new approach to explainable AI, CLIME, which is based on uniform sampling of user-defined subspaces of the input domain. The authors argue that LIME’s explanations can be unstable and are susceptible to adversarial attacks as a result of out-of-distribution (OOD) sampling. To resolve this issue, the authors propose a theoretically sound framework based on logical constraints, which allows the end-user the flexibility to delineate the precise subspace to be explained. For testing the quality of generated explanations, they develop an efficient estimation algorithm that is able to certifiably measure the true value of metrics such as fidelity up to any desired degree of accuracy, which can help build trust in the generated explanations."
134,SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,"Pre-trained language models such as BERT have exhibited remarkable performances in many tasks in natural language understanding (NLU). The tokens in the models are usually fine-grained in the sense that for languages like English they are words or sub-words and for languages like Chinese they are characters. In English, for example, there are multi-word expressions which form natural lexical units and thus the use of coarse-grained tokenization also appears to be reasonable. In fact, both fine-grained and coarse-grained tokenizations have advantages and disadvantages for learning of pre-trained language models. In this paper, we propose a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained and coarse-grained tokenizations. For English, AMBERT takes both the sequence of words (finegrained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization, employs one encoder for processing the sequence of words and the other encoder for processing the sequence of the phrases, utilizes shared parameters between the two encoders, and finally creates a sequence of contextualized representations of the words and a sequence of contextualized representations of the phrases. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE. The results show that AMBERT outperforms the existing best performing models in almost all cases, particularly the improvements are significant for Chinese. We also develop a version of AMBERT which performs equally well as AMBERT but uses about half of its inference time.","This paper proposes a novel pre-trained language model, AMBERT (A Multi-grained BERT), which takes both the sequence of words (finegrained tokens) and sequence of phrases as input after tokenization, and employs one encoder for processing the sequences of words and the other encoder (coarse) for processing phrases. The authors propose a sequence of contextualized representations of the words and contextualized representation of the phrases. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE, and the results show that the proposed model outperforms the existing best performing models in almost all cases."
135,SP:fd1cfe80343d3789227d99d836a5674374a234f5,"Semantic parsing is a challenging task whose purpose is to convert a natural language utterance to machine-understandable information representation. Recently, solutions using Neural Machine Translation have achieved many promising results, especially Transformer because of the ability to learn long-range word dependencies. However, the one drawback of adapting the original Transformer to the semantic parsing is the lack of detail in expressing the information of sentences. Therefore, this work proposes a PhraseTransformer architecture that is capable of a more detailed meaning representation by learning the phrase dependencies in the sentence. The main idea is to incorporate Long Short-Term Memory (LSTM) into the Self-Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong competitive performance on Geo, MSParS datasets, and leads to SOTA performance on Atis dataset in methods using Neural Network."," Transformer is a Transformer-based model that can be used for semantic parsing. This paper proposes a new Transformer architecture that is capable of learning the phrase dependencies in the sentence. The main idea is to incorporate Long Short-Term Memory (LSTM) into the Self-Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong competitive performance on Geo, MSParS datasets and leads to SOTA performance on Atis dataset in methods using Neural Network."
136,SP:2056a65a7500d79465685af883083cd706277c1f,"One intriguing property of deep neural networks (DNNs) is their vulnerability to adversarial perturbations. Despite the plethora of work on defending against individual perturbation models, improving DNN robustness against the combinations of multiple perturbations is still fairly under-studied. In this paper, we propose composite adversarial training (CAT), a novel training method that flexibly integrates and optimizes multiple adversarial losses, leading to significant robustness improvement with respect to individual perturbations as well as their “compositions”. Through empirical evaluation on benchmark datasets and models, we show that CAT outperforms existing adversarial training methods by large margins in defending against the compositions of pixel perturbations and spatial transformations, two major classes of adversarial perturbation models, while incurring limited impact on clean inputs.","This paper proposes a method for improving the robustness of deep neural networks (DNNs) against adversarial perturbations. Specifically, the authors propose a new adversarial training method, CAT, which combines and optimizes multiple adversarial losses to improve robustness to individual perturbation as well as their compositions. The proposed method is evaluated on a variety of datasets and models, and it is shown to outperform existing adversarial defense methods by large margins."
137,SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"A key aspect of human intelligence is the ability to infer abstract rules directly from high-dimensional sensory data, and to do so given only a limited amount of training experience. Deep neural network algorithms have proven to be a powerful tool for learning directly from high-dimensional data, but currently lack this capacity for data-efficient induction of abstract rules, leading some to argue that symbol-processing mechanisms will be necessary to account for this capacity. In this work, we take a step toward bridging this gap by introducing the Emergent Symbol Binding Network (ESBN), a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, we show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples, and outperforms a number of other competitive neural network architectures.","This paper proposes a method for learning abstract rules from high-dimensional data. The method is based on a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. The authors show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples."
138,SP:4171ce45966ac499f51450a19fb233934c0847f0,"We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.","This paper proposes a new framework, Translation between Augmented Natural Language (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, the authors frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. The proposed TANL can match or outperform task specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic roles labeling (Co-NLL-2005 and CoNLL2012)."
139,SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"In many scenarios, named entity recognition (NER) models severely suffer from unlabeled entity problem, where the entities of a sentence may not be fully annotated. Through empirical studies performed on synthetic datasets, we find two causes of performance degradation. One is the reduction of annotated entities and the other is treating unlabeled entities as negative instances. The first cause has less impact than the second one and can be mitigated by adopting pretraining language models. The second cause seriously misguides a model in training and greatly affects its performances. Based on the above observations, we propose a general approach, which can almost eliminate the misguidance brought by unlabeled entities. The key idea is to use negative sampling that, to a large extent, avoids training NER models with unlabeled entities. Experiments on synthetic datasets and real-world datasets show that our model is robust to unlabeled entity problem and surpasses prior baselines. On well-annotated datasets, our model is competitive with the state-of-the-art method1.","This paper studies the problem of unlabeled entity recognition in NER models. The authors propose a general approach to address this problem, which can almost eliminate the misguidance brought by unlabelled entities. The key idea is to use negative sampling that, to a large extent, avoids training NER model with unlabeling entities. Experiments on synthetic datasets and real-world datasets show that the proposed model is robust to unlabelED entity problem and surpasses prior baselines."
140,SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper proposes a novel acoustic word embedding called Acoustic Neighbor Embeddings where speech or text of arbitrary length are mapped to a vector space of fixed, reduced dimensions by adapting stochastic neighbor embedding (SNE) to sequential inputs. The Euclidean distance between coordinates in the embedding space reflects the phonetic confusability between their corresponding sequences. Two encoder neural networks are trained: an acoustic encoder that accepts speech signals in the form of frame-wise subword posterior probabilities obtained from an acoustic model and a text encoder that accepts text in the form of subword transcriptions. Compared to a triplet loss criterion, the proposed method is shown to have more effective gradients for neural network training. Experimentally, it also gives more accurate results with low-dimensional embeddings when the two encoder networks are used in tandem in a word (name) recognition task, and when the text encoder network is used standalone in an approximate phonetic matching task. In particular, in an isolated name recognition task depending solely on Euclidean nearest-neighbor search between the proposed embedding vectors, the recognition accuracy is identical to that of conventional finite state transducer(FST)-based decoding using test data with up to 1 million names in the vocabulary and 40 dimensions in the embeddings.","This paper proposes a novel acoustic word embedding called Acoustic Neighbor Embeddings where speech or text of arbitrary length are mapped to a vector space of fixed, reduced dimensions by adapting stochastic neighbor embedding (SNE) to sequential inputs. The Euclidean distance between coordinates in the embedding space reflects the phonetic confusability between their corresponding sequences. Two encoder neural networks are trained: an acoustic encoder that accepts speech signals in the form of frame-wise subword posterior probabilities obtained from an acoustic model and a text encoder network that accepts text in the forms of subword transcriptions. Compared to a triplet loss criterion, the proposed method is shown to have more effective gradients for neural network training. Experimentally, it gives more accurate results with low-dimensional embeddings when the two encoder networks are used in tandem in a word (name) recognition task, and standalone in an approximate phonetic matching task."
141,SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,"We propose a reinforcement learning algorithm for stationary mean-field games, where the goal is to learn a pair of mean-field state and stationary policy that constitutes the Nash equilibrium. When viewing the mean-field state and the policy as two players, we propose a fictitious play algorithm which alternatively updates the mean-field state and the policy via gradient-descent and proximal policy optimization, respectively. Our algorithm is in stark contrast with previous literature which solves each single-agent reinforcement learning problem induced by the iterates mean-field states to the optimum. Furthermore, we prove that our fictitious play algorithm converges to the Nash equilibrium at a sublinear rate. To the best of our knowledge, this seems the first provably convergent reinforcement learning algorithm for mean-field games based on iterative updates of both mean-field state and policy.","This paper proposes a reinforcement learning algorithm for stationary mean-field games, where the goal is to learn a pair of state and stationary policy that constitutes the Nash equilibrium. The authors propose a fictitious play algorithm that alternatively updates the mean field state and the policy via gradient descent and proximal policy optimization, respectively. The algorithm is in stark contrast with previous literature which solves each single-agent reinforcement learning problem induced by the iterates mean- field states to the optimum. Furthermore, the authors prove that their algorithm converges at a sublinear rate."
142,SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"We study the problem of probabilistic inference on the joint distribution defined by a normalizing flow model. Given a pre-trained flow model p(x), we wish to estimate p(x2 | x1) for some arbitrary partitioning of the variables x = (x1,x2). We first show that this task is computationally hard for a large class of flow models. Motivated by this hardness result, we propose a framework for approximate probabilistic inference. Specifically, our method trains a new generative model with the property that its composition with the given model approximates the target conditional distribution. By parametrizing this new distribution as another flow model, we can efficiently train it using variational inference and also handle conditioning under arbitrary differentiable transformations. Since the resulting approximate posterior remains a flow, it offers exact likelihood evaluation, inversion, and efficient sampling. We provide an extensive empirical evidence showcasing the flexibility of our method on a variety of inference tasks with applications to inverse problems. We also experimentally demonstrate that our approach is comparable to simple MCMC baselines in terms of sample quality. Further, we explain the failure of naively applying variational inference and show that our method does not suffer from the same issue.","This paper studies the problem of probabilistic inference on the joint distribution defined by a normalizing flow model. Given a pre-trained flow model p(x), we wish to estimate p( x2 | x1) for some arbitrary partitioning of the variables x = (x1,x2). This task is computationally hard for a large class of flow models. Motivated by this hardness result, this paper proposes a framework for approximate probablistic inference. Specifically, it trains a new generative model with the property that its composition with the given model approximates the target conditional distribution. By parametrizing this new distribution as another flow model, it can efficiently train it using variational inference and handle conditioning under arbitrary differentiable transformations. Since the resulting approximate posterior remains a flow, it offers exact likelihood evaluation, inversion, and efficient sampling. Empirical evidence demonstrates the flexibility of the proposed method on a variety of inference tasks with applications to inverse problems."
143,SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"Computer vision technology is widely used in biological and medical data analysis and understanding. However, there are still two major bottlenecks in the field of cell membrane segmentation, which seriously hinder further research: lack of sufficient high-quality data and lack of suitable evaluation criteria. In order to solve these two problems, this paper first introduces an Ultra-high Resolution Image Segmentation dataset for the Cell membrane, called U-RISC, the largest annotated Electron Microscopy (EM) dataset for the Cell membrane with multiple iterative annotations and uncompressed high-resolution raw data. During the analysis process of the U-RISC, we found that the current popular segmentation evaluation criteria are inconsistent with human perception. This interesting phenomenon is confirmed by a subjective experiment involving twenty people. Furthermore, to resolve this inconsistency, we propose a new evaluation criterion called Perceptual Hausdorff Distance (PHD) to measure the quality of cell membrane segmentation results. Detailed performance comparison and discussion of classic segmentation methods along with two iterative manual annotation results under existing evaluation criteria and PHD is given.","This paper proposes a new evaluation criteria for cell membrane segmentation based on perception distance. The paper presents a new dataset called U-RISC, which is the largest annotated Electron Microscopy (EM) dataset for the Cell membrane with multiple iterative annotations and uncompressed high-resolution raw data. The authors also propose a new criterion called Perceptual Hausdorff Distance (PHD) to measure the quality of the cell segmentation results. Experiments are conducted to show that PHD is consistent with human perception."
144,SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work. The Benchmark is publicly available at https://github.com/facebookresearch/CTrLBenchmark.",This paper proposes a modular architecture for continual learning (CL) and a learning algorithm for long-term learning. The authors propose a new suite of benchmarks to evaluate the effectiveness of the proposed method. The proposed method is based on the modular architecture and a task-driven prior over the exponential search space of all possible ways to combine modules. The experiments show that the proposed algorithm can outperform the state-of-the-art on several benchmarks. 
145,SP:cc819c61f408e88f247eb87946187ccec3dad32e,"Several recently proposed unsupervised meta-learning approaches rely on synthetic meta-tasks created using techniques such as random selection, clustering and/or augmentation. In this work, we describe a novel approach that generates metatasks using generative models. The proposed family of algorithms generate pairs of in-class and out-of-class samples from the latent space in a principled way, allowing us to create synthetic classes forming the training and validation data of a meta-task. We find that the proposed approach, LAtent Space Interpolation Unsupervised Meta-learning (LASIUM), outperforms or is competitive with current unsupervised learning baselines on few-shot classification tasks on the most widely used benchmark datasets.","This paper proposes a method for generating synthetic meta-tasks for unsupervised meta-learning using generative models. The authors propose a generative model that generates pairs of in-class and out-of-class samples from the latent space in a principled way, allowing them to create synthetic classes forming the training and validation data of a meta-task. The proposed method, LASIUM, is evaluated on few-shot classification tasks on the most widely used benchmark datasets."
146,SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"Injectivity plays an important role in generative models where it enables inference; in inverse problems and compressed sensing with generative priors it is a precursor to well posedness. We establish sharp characterizations of injectivity of fullyconnected and convolutional ReLU layers and networks. First, through a layerwise analysis, we show that an expansivity factor of two is necessary and sufficient for injectivity by constructing appropriate weight matrices. We show that global injectivity with iid Gaussian matrices, a commonly used tractable model, requires larger expansivity between 3.4 and 10.5. We also characterize the stability of inverting an injective network via worst-case Lipschitz constants of the inverse. We then use arguments from differential topology to study injectivity of deep networks and prove that any Lipschitz map can be approximated by an injective ReLU network. Finally, using an argument based on random projections, we show that an end-to-end—rather than layerwise—doubling of the dimension suffices for injectivity. Our results establish a theoretical basis for the study of nonlinear inverse and inference problems using neural networks.","This paper studies the injectivity of fully connected and convolutional ReLU layers and networks. The authors show that an expansivity factor of two is necessary and sufficient for injectivity by constructing appropriate weight matrices. They show that global injectivity with iid Gaussian matrices, a commonly used tractable model, requires larger expansivity between 3.4 and 10.5. They also characterize the stability of inverting an injective network via worst-case Lipschitz constants of the inverse. "
147,SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (e.g., class labels); conditioning on regression labels is mathematically distinct and raises two fundamental problems: (P1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (a.k.a. empirical cGAN losses) often fails in practice; (P2) Since regression labels are scalar and infinitely many, conventional label input methods (e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a novel method to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. The error bounds of a discriminator trained with HVDL and SVDL are derived under mild assumptions in this work. A new benchmark dataset, RC-49, is also proposed for generative image modeling conditional on regression labels. Our experiments on the Circular 2-D Gaussians, RC-49, and UTKFace datasets show that CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label. Moreover, in these experiments, CcGAN substantially outperforms cGAN both visually and quantitatively.","This paper proposes a continuous conditional generative adversarial network (CcGAN) for image generation conditional on continuous, scalar conditions (termed regression labels). The authors propose to reformulate existing empirical cGAN losses to be appropriate for the continuous scenario, and propose a novel method to incorporate regression labels into the generator and the discriminator. The proposed CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label. The experiments on the Circular 2-D Gaussians, RC-49, and UTKFace datasets show that the proposed model substantially outperforms cGAN."
148,SP:10dd09ab315870631d1451d200f2c87a023f8226,"Reducing the sample complexity associated with deep learning (DL) remains one of the most important problems in both theory and practice since its advent. Semisupervised learning (SSL) tackles this task by leveraging unlabeled instances which are usually more accessible than their labeled counterparts. Active learning (AL) directly seeks to reduce the sample complexity by training a classification network and querying unlabeled instances to be annotated by a human-in-the-loop. Under relatively strict settings, it has been shown that both SSL and AL can theoretically achieve the same performance of fully-supervised learning (SL) using far less labeled samples. While empirical works have shown that SSL can attain this benefit in practice, DL-based AL algorithms have yet to show their success to the extent achieved by SSL. Given the accessible pool of unlabeled instances in pool-based AL, we argue that the annotation efficiency brought by AL algorithms that seek diversity on labeled samples can be improved upon when using SSL as the training scheme. Equipped with a few theoretical insights, we designed an AL algorithm that rather focuses on controlling the convergence rate of a classification network by actively querying instances to improve the rate of convergence upon inclusion to the labeled set. We name this AL scheme convergence rate control (CRC), and our experiments show that a deep neural network trained using a combination of CRC and a recently proposed SSL algorithm can quickly achieve high performance using far less labeled samples than SL. In contrast to a few works combining independently developed AL and SSL (ASSL) algorithms, our method is a natural fit to ASSL, and we hope our work can catalyze research combining AL and SSL as opposed to an exclusion of either.","This paper proposes an active learning (AL) algorithm for semi-supervised learning (SSL) that aims to reduce the sample complexity of deep learning by querying unlabeled instances to be annotated by a human-in-the-loop. The authors argue that the annotation efficiency brought by AL algorithms that seek diversity on labeled samples can be improved upon when using SSL as the training scheme. They propose an AL algorithm that instead focuses on controlling the convergence rate of a classification network by actively querying instances to improve the rate of convergence upon inclusion to the labeled set. They name this AL scheme convergence rate control (CRC), and their experiments show that a combination of AL and SSL can quickly achieve high performance using far less labeled samples than SL."
149,SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data.",This paper proposes a federated learning method for distributed training of neural network models. The authors propose a dynamic regularizer for each device at each round of training to ensure that the global and device solutions are aligned. The proposed method is evaluated on both convex and non-convex data and compared with a variety of existing methods. The results show that the proposed method can achieve better performance than the existing methods on both real and synthetic data.
150,SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"Recently, representations learned by self-supervised approaches have significantly reduced the gap with their supervised counterparts in many different computer vision tasks. However, these self-supervised methods are computationally challenging. In this work, we focus on accelerating contrastive learning algorithms with little or even no loss of accuracy. Our insight is that, contrastive learning concentrates on optimizing similarity (dissimilarity) between pairs of inputs, and the similarity on the intermediate layers is a good surrogate of the final similarity. We exploit our observation by introducing additional intermediate contrastive losses. In this way, we can truncate the back-propagation and updates only a part of the parameters for each gradient descent update. Additionally, we do selection based on the intermediate losses to filter easy regions for each image, which further reduces the computational cost. We apply our method to recently-proposed MOCO (He et al., 2020), SimCLR (Chen et al., 2020a), SwAV (Caron et al., 2020) and notice that we can reduce the computational cost with little loss on the performance of ImageNet linear classification and other downstream tasks.","This paper proposes a new algorithm for self-supervised representation learning based on contrastive learning. The main idea is to truncate the back-propagation and update only a part of the parameters for each gradient descent update. The authors also propose to use the intermediate contrastive losses to filter easy regions for each image, which further reduces the computational cost. The proposed algorithm is evaluated on ImageNet linear classification and other downstream tasks."
151,SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"We study the global convergence and global optimality of actor-critic, one of the most popular families of reinforcement learning algorithms. While most existing works on actor-critic employ bi-level or two-timescale updates, we focus on the more practical single-timescale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. Moreover, we consider two function approximation settings where both the actor and critic are represented by linear or deep neural networks. For both cases, we prove that the actor sequence converges to a globally optimal policy at a sublinear O(K−1/2) rate, where K is the number of iterations. To the best of our knowledge, we establish the rate of convergence and global optimality of single-timescale actor-critic with linear function approximation for the first time. Moreover, under the broader scope of policy optimization with nonlinear function approximation, we prove that actorcritic with deep neural network finds the globally optimal policy at a sublinear rate for the first time.","This paper studies the global convergence and global optimality of actor-critic, one of the most popular families of reinforcement learning algorithms. The authors focus on the more practical single-timescale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. Moreover, the authors consider two function approximation settings: linear or deep neural networks. For both cases, they prove that the actor sequence converges to a globally optimal policy at a sublinear O(K−1/2) rate, where K is the number of iterations."
152,SP:26705a4dc305cce336f657c5937d1f5b4209548a,"Log files from computer systems are ubiquitous and record events, messages, or transactions. Logs are rich containers of data because they can store a sequence of structured textual and numerical data. Many sequential forms of data including natural languages and temporal signals can be represented as logs. We propose to represent logs at a few levels of abstraction including field level, log level, and log sequence level. The representation for each level can be computed from the previous level. These representations are in vector format and serve as interfaces to downstream applications. We use a version of Transformer Networks (TNs) to encode numerical and textual information that is suitable for log embeddings. We show how a number of log processing applications can be readily solved with our representation.","This paper proposes to represent log files at three levels of abstraction: field level, log level, and log sequence level. The representation for each level can be computed from the previous level. These representations are in vector format and serve as interfaces to downstream applications. The authors use a version of Transformer Networks (TNs) to encode numerical and textual information that is suitable for log embeddings. They show how a number of log processing applications can be solved with their representation."
153,SP:165c51a16f17fb8726e968f8b34742b62011d60e,"In this paper, we target an important issue of deep convolutional neural networks (CNNs) – the lack of a mathematical understanding of their properties. We present an explicit formalism that is motivated by the similarities between trained CNN kernels and oriented Gabor filters for addressing this problem. The core idea is to constrain the behavior of convolutional layers by splitting them into a succession of wavelet packet decompositions, which are modulated by freely-trained mixture weights. We evaluate our approach with three variants of wavelet decompositions with the AlexNet architecture for image classification as an example. The first variant relies on the separable wavelet packet transform while the other two implement the 2D dual-tree real and complex wavelet packet transforms, taking advantage of their feature extraction properties such as directional selectivity and shift invariance. Our experiments show that we achieve the accuracy rate of standard AlexNet, but with a significantly lower number of parameters, and an interpretation of the network that is grounded in mathematical theory.","This paper presents a theoretical analysis of wavelet decompositions of convolutional layers in deep neural networks (CNNs). The main idea is to decompose the convolution layers into a sequence of wavelets, which are modulated by a mixture of mixture weights. The authors show that the wavelets can be decomposed into 2D dual-tree wavelets and 2D wavelets. They evaluate the proposed wavelets on the AlexNet architecture for image classification, and show that they can achieve the accuracy rate of standard AlexNet with a significantly lower number of parameters."
154,SP:d0a284da462584724ba6a3a48c9e986d391233f6,"In real-world multi-agent teams, agents with different capabilities may join or leave “on the fly” without altering the team’s overarching goals. Coordinating teams with such dynamic composition remains a challenging problem: the optimal team strategy may vary with its composition. Inspired by real-world team sports, we propose a coach-player framework to tackle this problem. We assume that the players only have a partial view of the environment, while the coach has a complete view. The coach coordinates the players by distributing individual strategies. Specifically, we 1) propose an attention mechanism for both the players and the coach; 2) incorporate a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with different players. Our attention mechanism on the players and the coach allows for a varying number of heterogeneous agents, and can thus tackle the dynamic team composition. We validate our methods on resource collection tasks in multi-agent particle environment. We demonstrate zero-shot generalization to new team compositions with varying numbers of heterogeneous agents. The performance of our method is comparable or even better than the setting where all players have a full view of the environment, but no coach. Moreover, we see that the performance stays nearly the same even when the coach communicates as little as 13% of the time using our adaptive communication strategy. These results demonstrate the significance of a coach to coordinate players in dynamic teams.","This paper proposes a coach-player framework to tackle the problem of coordination in multi-agent teams, where agents with different capabilities may join or leave “on the fly” without altering the team’s overarching goals. Inspired by real-world team sports, the authors assume that the players only have a partial view of the environment, while the coach has a complete view. Specifically, they propose an attention mechanism for both the players and the coach; 2) incorporate a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with different players. They validate their methods on resource collection tasks in the Multi-agent particle environment, and demonstrate zero-shot generalization to new team compositions with varying numbers of heterogeneous agents."
155,SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A commonly-used (first-order) influence function can be implemented efficiently as a post-hoc method requiring access only to the gradients and Hessian of the model. For linear models, influence functions are well-defined due to the convexity of the underlying loss function and are generally accurate even across difficult settings where model changes are fairly large such as estimating group influences. Influence functions, however, are not wellunderstood in the context of deep learning with non-convex loss functions. In this paper, we provide a comprehensive and large-scale empirical study of successes and failures of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments, we show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of influence functions. In particular, we find that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy of influence estimates can vary significantly depending on the examined test points. These results suggest that in general influence functions in deep learning are fragile and call for developing improved influence estimation methods to mitigate these issues in non-convex setups.","This paper studies the influence function of neural networks in the context of deep learning with non-convex loss functions. In particular, the authors study the effect of network architecture, depth and width, as well as the extent of model parameterization and regularization techniques have strong effects on the accuracy of influence functions. They find that influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous. The authors also show that for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates."
156,SP:5fea74a2031d097a99dacf613bedcb054b0c3831,"Autoregressive language models, pretrained using large text corpora to do well on next word prediction, have been successful at solving many downstream tasks, even with zero-shot usage. However, there is little theoretical understanding of this success. This paper initiates a mathematical study of this phenomenon for the downstream task of text classification by considering the following questions: (1) What is the intuitive connection between the pretraining task of next word prediction and text classification? (2) How can we mathematically formalize this connection and quantify the benefit of language modeling? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling a meaningful pretraining task. With a mathematical formalization of this hypothesis, we make progress towards (2) and show that language models that are -optimal in crossentropy (log-perplexity) learn features that can linearly solve such classification tasks withO( √ ) error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. We experimentally verify various assumptions and theoretical findings, and also use insights from the analysis to design a new objective function that performs well on some classification tasks.","This paper studies the connection between the pretraining task of next word prediction and text classification tasks. The authors propose to reformulate the classification tasks of interest as sentence completion tasks, and show that language models that are optimal in cross-entropy (log-perplexity) learn features that can linearly solve such classification tasks with O(sqrt(\sqrt{O}) error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. They experimentally verify various assumptions and theoretical findings, and also use insights from the analysis to design a new objective function that performs well on some classification tasks, which can be used to design new language models."
157,SP:a67da438e9821010284416170c3699ae7ff96c99,"Membership inference attacks (MIA) try to detect if data samples were used to train a neural network model. As training data is very valuable in machine learning, MIA can be used to detect the use of unauthorized data. Unlike the traditional MIA approaches, addressing classification models, we address conditional image generation models (e.g. image translation). Due to overfitting, reconstruction errors are typically lower for images used in training. A simple but effective approach for membership attacks can therefore use the reconstruction error. However, we observe that some images are ”universally” easy, and others are difficult. Reconstruction error alone is less effective at discriminating between difficult images used in training and easy images that were never seen before. To overcome this, we propose to use a novel difficulty score that can be computed for each image, and its computation does not require a training set. Our membership error, obtained by subtracting the difficulty score from the reconstruction error, is shown to achieve high MIA accuracy on an extensive number of benchmarks.","This paper proposes a new algorithm for membership inference attacks (MIA) that is based on the reconstruction error of an image generation model. The reconstruction error is used to detect if data samples were used to train a neural network model. However, the authors observe that reconstruction error alone is less effective at discriminating between difficult images used in training and easy images that were never seen before. To overcome this, they propose to use a novel difficulty score that can be computed for each image, and its computation does not require a training set. The proposed algorithm is shown to achieve high MIA accuracy on an extensive number of benchmarks."
158,SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46% for CIFAR-10, 23.7% for ImageNet under the mobile setting. On NASBench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms.","This paper proposes a differentiable architecture search method by formulating it into a distribution learning problem. The authors treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. To alleviate the large memory consumption of differentiable NAS, the authors propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of the proposed method."
159,SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"Although deep networks are typically used to approximate functions over high dimensional inputs, recent work has increased interest in neural networks as function approximators for low-dimensional-but-complex functions, such as representing images as a function of pixel coordinates, solving differential equations, or representing signed distance functions or neural radiance fields. Key to these recent successes has been the use of new elements such as sinusoidal nonlinearities or Fourier features in positional encodings, which vastly outperform simple ReLU networks. In this paper, we propose and empirically demonstrate that an arguably simpler class of function approximators can work just as well for such problems: multiplicative filter networks. In these networks, we avoid traditional compositional depth altogether, and simply multiply together (linear functions of) sinusoidal or Gabor wavelet functions applied to the input. This representation has the notable advantage that the entire function can simply be viewed as a linear function approximator over an exponential number of Fourier or Gabor basis functions, respectively. Despite this simplicity, when compared to recent approaches that use Fourier features with ReLU networks or sinusoidal activation networks, we show that these multiplicative filter networks largely outperform or match the performance of these approaches on the domains highlighted in these past works.","This paper proposes a new neural network architecture for function approximators for low-dimensional-but-complex functions. In particular, the authors propose a multiplicative filter network that combines sinusoidal and Gabor wavelet functions. The authors show that the proposed network can be viewed as a linear function approximation over an exponential number of Fourier or Gabor basis functions, and that it can be used to represent complex functions such as signed distance functions or neural radiance fields. Empirical results are provided to demonstrate the effectiveness of the proposed method."
160,SP:f5be855300f63c185a006834302bd4b033b56258,"Gradient-based meta-learning relates task-specific models to a meta-model by gradients. By this design, an algorithm first optimizes the task-specific models by an inner loop and then backpropagates meta-gradients through the loop to update the meta-model. The number of inner-loop optimization steps has to be small (e.g., one step) to avoid high-order derivatives, big memory footprints, and the risk of vanishing or exploding meta-gradients. We propose an intuitive teacherstudent scheme to enable the gradient-based meta-learning algorithms to explore long horizons by the inner loop. The key idea is to employ a student network to adequately explore the search space of task-specific models (e.g., by more than ten steps), and a teacher then takes a “leap” toward the regions probed by the student. The teacher not only arrives at a high-quality model but also defines a lightweight computation graph for meta-gradients. Our approach is generic; it performs well when applied to four meta-learning algorithms over three tasks: few-shot learning, long-tailed classification, and meta-attack.","This paper proposes a new meta-learning algorithm for few-shot learning, long-tailed classification, and meta-attack. The main idea is to use a student network to explore the search space of task-specific models (e.g., by more than ten steps), and a teacher network to take a “leap” toward the regions probed by the student network. The teacher network is trained to arrive at a high-quality model but also defines a lightweight computation graph for meta-gradients. The proposed algorithm is generic and can be applied to four different meta learning algorithms over three tasks."
161,SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"Online interactions with the environment to collect data samples for training a Reinforcement Learning agent is not always feasible due to economic and safety concerns. The goal of Offline Reinforcement Learning (RL) is to address this problem by learning effective policies using previously collected datasets. Standard off-policy RL algorithms are prone to overestimations of the values of outof-distribution (less explored) actions and are hence unsuitable for Offline RL. Behavior regularization, which constraints the learned policy within the support set of the dataset, has been proposed to tackle the limitations of standard off-policy algorithms. In this paper, we improve the behavior regularized offline reinforcement learning and propose BRAC+. We use an analytical upper bound on KL divergence as the behavior regularizor to reduce variance associated with sample based estimations. Additionally, we employ state-dependent Lagrange multipliers for the regularization term to avoid distributing KL divergence penalty across all states of the sampled batch. The proposed Lagrange multipliers allow more freedom of deviation to high probability (more explored) states leading to better rewards while simultaneously restricting low probability (less explored) states to prevent out-of-distribution actions. To prevent catastrophic performance degradation due to rare out-of-distribution actions, we add a gradient penalty term to the policy evaluation objective to penalize the gradient of the Q value w.r.t the out-of-distribution actions. By doing so, the Q values evaluated at the out-ofdistribution actions are bounded. On challenging offline RL benchmarks, BRAC+ outperforms the state-of-the-art model-free and model-based approaches.","This paper proposes a behavior regularization method for offline reinforcement learning. The main idea is to use an analytical upper bound on KL divergence as the behavior regularizor to reduce variance associated with sample based estimations. The authors also employ state-dependent Lagrange multipliers for the regularization term to avoid distributing KL divergence penalty across all states of the sampled batch. To prevent catastrophic performance degradation due to rare out-of-distribution actions, the authors also add a gradient penalty term to the policy evaluation objective to penalize the gradient of the Q value w.r.t the out of distribution actions. The experimental results show that the proposed method BRAC+ outperforms the existing offline RL algorithms."
162,SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,"Recent research on compressing deep neural networks has focused on reducing the number of parameters. Smaller networks are easier to export and deploy on edge-devices. We introduce Adjoined networks as a training approach that can regularize and compress any CNN-based neural architecture. Our one-shot learning paradigm trains both the original and the smaller networks together. The parameters of the smaller network are shared across both the architectures. We prove strong theoretical guarantees on the regularization behavior of the adjoint training paradigm. We complement our theoretical analysis by an extensive empirical evaluation of both the compression and regularization behavior of adjoint networks. For resnet-50 trained adjointly on Imagenet, we are able to achieve a 13.7x reduction in the number of parameters1 and a 3x improvement in inference time without any significant drop in accuracy. For the same architecture on CIFAR-100, we are able to achieve a 99.7x reduction in the number of parameters and a 5x improvement in inference time. On both these datasets, the original network trained in the adjoint fashion gains about 3% in top-1 accuracy as compared to the same network trained in the standard fashion.",This paper proposes Adjoined networks as a training approach that can regularize and compress any CNN-based neural architecture. The proposed method trains both the original and the smaller networks together. The parameters of the smaller network are shared across both the architectures. The authors prove strong theoretical guarantees on the regularization behavior of the adjoint training paradigm. The theoretical analysis is complemented by an extensive empirical evaluation of both the compression and regularisation behavior of adjoint networks. 
163,SP:dba40073f79143e5355d194aa16db9eee0267a5d,"Recent work on exploration in reinforcement learning (RL) has led to a series of increasingly complex solutions to the problem. This increase in complexity often comes at the expense of generality. Recent empirical studies suggest that, when applied to a broader set of domains, some sophisticated exploration methods are outperformed by simpler counterparts, such as -greedy. In this paper we propose an exploration algorithm that retains the simplicity of -greedy while reducing dithering. We build on a simple hypothesis: the main limitation of greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. We propose a temporally extended form of -greedy that simply repeats the sampled action for a random duration. It turns out that, for many duration distributions, this suffices to improve exploration on a large set of domains. Interestingly, a class of distributions inspired by ecological models of animal foraging behaviour yields particularly strong performance.","This paper proposes a temporal extension of the greedy exploration algorithm in reinforcement learning (RL). The authors argue that the main limitation of greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. To address this limitation, the authors propose a temporally extended form of greedy that simply repeats the sampled action for a random duration. Experiments show that the proposed temporal extension can improve exploration on a large set of domains."
164,SP:5efb581a368ace3bd085d48801a899559d6a43ef,"Matrix factorization is a simple and natural test-bed to investigate the implicit regularization of gradient descent. Gunasekar et al. (2017) conjectured that Gradient Flow with infinitesimal initialization converges to the solution that minimizes the nuclear norm, but a series of recent papers argued that the language of norm minimization is not sufficient to give a full characterization for the implicit regularization. In this work, we provide theoretical and empirical evidence that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to a simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under some reasonable assumptions. This generalizes the rank minimization view from previous works to a much broader setting and enables us to construct counter-examples to refute the conjecture from Gunasekar et al. (2017). We also extend the results to the case where depth ≥ 3, and we show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude so that this rank minimization is more likely to take effect for initialization with practical scale.","This paper studies the implicit regularization of gradient flow with infinitesimal initialization in the case of depth-2 matrix factorization. In particular, the authors show that for the case where the depth is deeper than 2, gradient flow is equivalent to a heuristic rank minimization algorithm, Greedy Low-Rank Learning. The authors also extend the results to the case when the depth goes to 3, and show that the convergence has a much weaker dependence over initialization magnitude."
165,SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. Model patching first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate subgroup features. We instantiate model patching with CAMEL, which (1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and (2) balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. We demonstrate CAMEL’s effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real-world skin cancer dataset.","This paper proposes a method for improving the robustness of classifiers against spurious spurious bandages. The method is based on the idea of model patching, which first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate sub group features. The proposed method, CAMEL, uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. Experiments on 3 benchmark datasets demonstrate the effectiveness of the proposed method."
166,SP:de6cea1e35a0555175e17546a93422e9a96a511e,"Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to tackle these issues, but they sacrifice the model interpretability. In this paper, we propose a new classifier, named Rulebased Representation Learner (RRL), that automatically learns interpretable nonfuzzy rules for data representation. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on 9 small and 4 large data sets show that RRL outperforms the competitive approaches, has low complexity close to the simple decision trees, and is rational for its main technical contributions.","This paper proposes a method for learning interpretable nonfuzzy rules for data representation in rule-based models. The proposed method is based on the idea of learning a non-differentiable classifier that can project the discrete model to a continuous space and train it using gradient descent. The authors propose a novel training method, Gradient Grafting, that can directly optimize the discrete classifier. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Experiments on 9 small and 4 large data sets show that RRL outperforms the state-of-the-art methods."
167,SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"Many biochemical applications such as molecular property prediction require models to generalize beyond their training domains (environments). Moreover, natural environments in these tasks are structured, defined by complex descriptors such as molecular scaffolds or protein families. Therefore, most environments are either never seen during training, or contain only a single training example. To address these challenges, we propose a new regret minimization (RGM) algorithm and its extension for structured environments. RGM builds from invariant risk minimization (IRM) by recasting simultaneous optimality condition in terms of predictive regret, finding a representation that enables the predictor to compete against an oracle with hindsight access to held-out environments. The structured extension adaptively highlights variation due to complex environments via specialized domain perturbations. We evaluate our method on multiple applications: molecular property prediction, protein homology and stability prediction and show that RGM significantly outperforms previous state-of-the-art baselines.","This paper proposes a new regret minimization algorithm for molecular property prediction. The proposed algorithm is based on invariant risk minimization (IRM) and extends it to structured environments. The main idea is to replace the simultaneous optimality condition in IRM with predictive regret, which is a representation that enables the predictor to compete against an oracle with hindsight access to held-out environments. Experiments show that the proposed algorithm outperforms previous state-of-the-art baselines."
168,SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"Inspired by the great success of BERT in NLP tasks, many text-vision BERT models emerged recently. Benefited from cross-modal attentions, text-vision BERT models have achieved excellent performance in many language-vision tasks including text-image retrieval. Nevertheless, cross-modal attentions used in textvision BERT models require too expensive computation cost when solving textvision retrieval, which is impractical for large-scale search. In this work, we develop a novel architecture, cross-probe BERT. It relies on devised text and vision probes, and the cross-modal attentions are conducted on text and vision probes. It takes lightweight computation cost, and meanwhile effectively exploits crossmodal attention. Systematic experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency of the proposed method.","This paper proposes a novel architecture for cross-probe BERT, which is based on the idea of cross-modal attention. The authors propose to use a combination of text and vision probes to perform the attention. This is a very interesting idea, and the experimental results show that the proposed method outperforms the state-of-the-art."
169,SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"In this paper, we propose a new type of Actor, named forward-looking Actor or FORK for short, for Actor-Critic algorithms. FORK can be easily integrated into a model-free ActorCritic algorithm. Our experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement FORK can bring to the state-of-the-art algorithms. A variation of FORK can further solve BipedalWalkerHardcore in as few as four hours using a single GPU.",This paper proposes a new Actor-Critic algorithm named forward-looking Actor (FORK) for actor-critic algorithms. The main idea of FORK is to use a forward-looker actor which can be integrated into a model-free ActorCritic. Experimental results on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate that FORK can bring significant performance improvement to the state-of-the-art algorithms.
170,SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"Federated learning aims to collaboratively train a strong global model by accessing users’ locally trained models but not their own data. A crucial step is therefore to aggregate local models into a global model, which has been shown challenging when users have non-i.i.d. data. In this paper, we propose a novel aggregation algorithm named FEDBE, which takes a Bayesian inference perspective by sampling higher-quality global models and combining them via Bayesian model Ensemble, leading to much robust aggregation. We show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. Our empirical studies validate FEDBE’s superior performance, especially when users’ data are not i.i.d. and when the neural networks go deeper. Moreover, FEDBE is compatible with recent efforts in regularizing users’ model training, making it an easily applicable module: you only need to replace the aggregation method but leave other parts of your federated learning algorithm intact.","This paper proposes a Bayesian aggregation algorithm for federated learning. The main idea is to aggregate local models into a global model, which has been shown challenging when users have non-i.i.d. data. The authors propose to combine higher-quality global models and combine them via Bayesian model Ensemble, leading to much robust aggregation. They show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models."
171,SP:3ac5f437fc349a33810d0645664d1c448528af74,ING INFLUENCE PATHS FOR EXPLAINING (CONTEXTUALIZATION OF) BERT MODELS Anonymous authors Paper under double-blind review,This paper presents a new method for explaining the performance of binary classification models in the context of double-blind review. The method is based on the idea that the model should be able to explain the results of a single blind review of a binary classification model. The paper is well-written and well-motivated. It is well organized and easy to follow. The authors have done a good job of explaining their method to the reader.
172,SP:efa2343ead47263a0d09e1c17f9aa044605b9650,"In this paper, we perform Lyapunov based analysis of the loss function to derive an a priori upper bound on the settling time of deep neural networks. While previous studies have attempted to understand deep learning using control theory framework, there is limited work on a priori finite time convergence analysis. Drawing from the advances in analysis of finite-time control of non-linear systems, we provide a priori guarantees of finite-time convergence in a deterministic control theoretic setting. We formulate the supervised learning framework as a control problem where weights of the network are control inputs and learning translates into a tracking problem. An analytical formula for finite-time upper bound on settling time is provided a priori under the assumptions of boundedness of input. Finally, we prove that our loss function is robust against input perturbations.",This paper provides a priori upper bound on the settling time of deep neural networks based on the Lyapunov analysis of the loss function. The authors derive the upper bound using the analysis of finite-time control of non-linear systems. The main contribution of this paper is to provide an analytical formula for finite- time upper bound under the assumptions of boundedness of input and robustness against input perturbations. The paper is well-written.
173,SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"How to learn a good representation of data is one of the most important topics of machine learning. Disentanglement of representations, though believed to be the core feature of good representations, has caused a lot of debates and discussions in recent. Sorrenson et al. (2020), using the techniques developed in nonlinear independent component analysis theory, show that general incompressible-flow networks (GIN) can recover the underlying latent variables that generate the data, and thus can provide a compact and disentangled representation. However, in this paper, we point out that the method taken by GIN for informative latent variables selection is not theoretically supported and can be disproved by experiments. We propose to use the mutual information between each learned latent variables and the auxiliary variable to correctly identify informative latent variables. We directly verify the improvement brought by our method in experiments on synthetic data. We further show the advantage of our method on various downstream tasks including classification, outlier detection and adversarial attack defence on both synthetic and real data.","This paper proposes a method for learning disentangled representations of latent variables. The authors argue that the GIN method for disentanglement is not theoretically supported and can be disproved by experiments. Instead, the authors propose to use the mutual information between each learned latent variables and the auxiliary variable to correctly identify informative latent variables, and show the improvement of the proposed method in experiments on synthetic data and real data."
174,SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"Pooling is a critical operation in convolutional neural networks for increasing receptive fields and improving robustness to input variations. Most existing pooling operations downsample the feature maps, which is a lossy process. Moreover, they are not invertible: upsampling a downscaled feature map can not recover the lost information in the downsampling. By adopting the philosophy of the classical Lifting Scheme from signal processing, we propose LiftPool for bidirectional pooling layers, including LiftDownPool and LiftUpPool. LiftDownPool decomposes a feature map into various downsized sub-bands, each of which contains information with different frequencies. As the pooling function in LiftDownPool is perfectly invertible, by performing LiftDownPool backward, a corresponding up-pooling layer LiftUpPool is able to generate a refined upsampled feature map using the detail sub-bands, which is useful for image-to-image translation challenges. Experiments show the proposed methods achieve better results on image classification and semantic segmentation, using various backbones. Moreover, LiftDownPool offers better robustness to input corruptions and perturbations.","This paper proposes a new pooling operation for convolutional neural networks. The proposed method is based on the classical Lifting scheme from signal processing. The key idea is to decompose a feature map into various downsized sub-bands, each of which contains information with different frequencies. The authors propose two variants of the proposed method, namely, LiftDownPool and LiftUpPool. The first one is a backward-up-pooling operation, while the second one is an up-up pooling method. Experiments are conducted on image classification and semantic segmentation tasks. "
175,SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"We propose a fast, distance-preserving, binary embedding algorithm to transform a high-dimensional dataset T ⊆ R into binary sequences in the cube {±1}. When T consists of well-spread (i.e., non-sparse) vectors, our embedding method applies a stable noise-shaping quantization scheme to Ax where A ∈ Rm×n is a sparse Gaussian random matrix. This contrasts with most binary embedding methods, which usually use x 7→ sign(Ax) for the embedding. Moreover, we show that Euclidean distances among the elements of T are approximated by the `1 norm on the images of {±1} under a fast linear transformation. This again contrasts with standard methods, where the Hamming distance is used instead. Our method is both fast and memory efficient, with time complexity O(m) and space complexity O(m) on well-spread data. When the data is not well-spread, we show that the approach still works provided that data is transformed via a Walsh-Hadamard matrix, but now the cost is O(n log n) per data point. Further, we prove that the method is accurate and its associated error is comparable to that of a continuous valued Johnson-Lindenstrauss embedding plus a quantization error that admits a polynomial decay as the embedding dimension m increases. Thus the length of the binary codes required to achieve a desired accuracy is quite small, and we show it can even be compressed further without compromising the accuracy. To illustrate our results, we test the proposed method on natural images and show that it achieves strong performance.","This paper proposes a binary embedding method to transform a high-dimensional dataset T into binary sequences in the cube {±1}. The key idea is to use a stable noise-shaping quantization scheme to quantize a sparse Gaussian random matrix A into a binary sequence. The authors show that Euclidean distances among the elements of T are approximated by the `1 norm on the images of the cube. The proposed method is both fast and memory efficient, with time complexity O(m) and space complexity $m$ on well-spread data, and $O(n log n)$ on data with Walsh-Hadamard matrix."
176,SP:f65e229bca3904095743e7a501b1083cc60f1e22,"Brains learn robustly, and generalize effortlessly between different learning tasks; in contrast, robustness and generalization across tasks are well known weaknesses of artificial neural nets (ANNs). How can we use our accelerating understanding of the brain to improve these and other aspects of ANNs? Here we hypothesize that (a) Brains employ synaptic plasticity rules that serve as proxies for Gradient Descent (GD); (b) These rules themselves can be learned by GD on the rule parameters; and (c) This process may be a missing ingredient for the development of ANNs that generalize well and are robust to adversarial perturbations. We provide both empirical and theoretical evidence for this hypothesis. In our experiments, plasticity rules for the synaptic weights of recurrent neural nets (RNNs) are learned through GD and are found to perform reasonably well (with no backpropagation). We find that plasticity rules learned by this process generalize from one type of data/classifier to others (e.g., rules learned on synthetic data work well on MNIST/Fashion MNIST) and converge with fewer updates. Moreover, the classifiers learned using plasticity rules exhibit surprising levels of tolerance to adversarial perturbations. In the special case of the last layer of a classification network, we show analytically that GD on the plasticity rule recovers (and improves upon) the perceptron algorithm and the multiplicative weights method. Finally, we argue that applying GD to learning rules is biologically plausible, in the sense that it can be learned over evolutionary time: we describe a genetic setting where natural selection of a numerical parameter over a sequence of generations provably simulates a simple variant of GD.","This paper proposes to use plasticity rules as a proxy for Gradient Descent (GD) to improve the generalization and robustness of artificial neural networks (ANNs). In particular, the authors argue that the plasticity rule can be learned by GD on the rule parameters of RNNs. The authors provide both empirical and theoretical evidence for this hypothesis. In their experiments, they show that plasticity-rule-based plasticity networks can generalize well and are robust to adversarial perturbations. They also show that GD can be used to recover the perceptron algorithm and multiplicative weights method for the last layer of a classification network."
177,SP:f435530146fa975cb27cd375a857df9bcbd87682,"The task of visual question generation (VQG) aims to generate human-like questions from an image and potentially other side information (e.g. answer type or the answer itself). Despite promising results have been achieved, previous works on VQG either i) suffer from one image to many questions mapping problem rendering the failure of generating referential and meaningful questions from an image, or ii) ignore rich correlations among the visual objects in an image and potential interactions between the side information and image. To address these limitations, we first propose a novel learning paradigm to generate visual questions with answer-awareness and region-reference. In particular, we aim to ask the right visual questions with Double Hints textual answers and visual regions of interests, effectively mitigating the existing one-to-many mapping issue. To this end, we develop a simple methodology to self-learn the visual hints without introducing any additional human annotations. Furthermore, to capture these sophisticated relationships, we propose a new double-hints guided Graph-to-Sequence learning framework that first models them as a dynamic graph and learns the implicit topology end-to-end, and then utilize a graph-to-sequence model to generate the questions with double hints. Our experiments on VQA2.0 and COCO-QA datasets demonstrate that our proposed model on this new setting can significantly outperform existing state-of-the-art baselines by a large margin.","This paper proposes a method for visual question generation (VQG) that aims to generate human-like questions from an image and potentially other side information (e.g. answer type or the answer itself). To address these limitations, the authors propose a novel learning paradigm to generate visual questions with answer-awareness and region-reference. In particular, they aim to ask the right visual question with Double Hints textual answers and visual regions of interest, effectively mitigating the existing one-to-many mapping issue. To this end, they develop a simple methodology to self-learn the visual hints without introducing any additional human annotations. Furthermore, to capture these sophisticated relationships, they propose a new double-hints guided Graph-To-Sequence learning framework that first models them as a dynamic graph and learns the implicit topology end to end, and then utilize a graph to sequence model to generate the questions with double hints. The experiments on VQA2.0 and COCO-QA datasets demonstrate that the proposed model can significantly outperform existing state-of-the-art baselines by a large margin."
178,SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"Recent empirical and theoretical studies have shown that many learning algorithms – from linear regression to neural networks – can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as “double descent”, has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned `2 regularization achieves monotonic test performance as we grow either the sample size or the model size. We also demonstrate empirically that optimally-tuned `2 regularization can mitigate double descent for more general models, including neural networks. Our results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.","This paper studies the double descent phenomenon, which is the phenomenon of test performance that is non-monotonic in quantities such as the sample size and model size. The authors prove that optimally-tuned `2 regularization can achieve monotonic test performance as we grow either the sample or the model size in linear regression models with isotropic data distribution. Theoretical results are provided for linear regression and neural networks. Empirical studies are also provided to demonstrate the effectiveness of the proposed regularization."
179,SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"How to improve generative modeling by better exploiting spatial regularities and coherence in images? We introduce a novel neural network for building image generators (decoders) and apply it to variational autoencoders (VAEs). In our spatial dependency networks (SDNs), feature maps at each level of a deep neural net are computed in a spatially coherent way, using a sequential gating-based mechanism that distributes contextual information across 2-D space. We show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. Furthermore, we demonstrate that SDN can be applied to large images by synthesizing samples of high quality and coherence. In a vanilla VAE setting, we find that a powerful SDN decoder also improves learning disentangled representations, indicating that neural architectures play an important role in this task. Our results suggest favoring spatial dependency over convolutional layers in various VAE settings. The accompanying source code is given at: https://github.com/djordjemila/sdn.","This paper proposes a spatial dependency network (SDN) to improve generative modeling by better exploiting spatial regularities and coherence in images. The authors propose a novel neural network for building image generators (decoders) and apply it to variational autoencoders (VAEs) by augmenting the decoder of a hierarchical VAE by spatial dependency layers. They show that SDN can improve density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. In a vanilla VAE setting, they find that a powerful SDN decoder also improves learning disentangled representations, indicating that neural architectures play an important role in this task."
180,SP:db91512a90e75675af03c2f197751c8526d6f5e9,"Off-policy reinforcement learning (RL) holds the promise of sample-efficient learning of decision-making policies by leveraging past experience. However, in the offline RL setting – where a fixed collection of interactions are provided and no further interactions are allowed – it has been shown that standard off-policy RL methods can significantly underperform. Recently proposed methods often aim to address this shortcoming by constraining learned policies to remain close to the given dataset of interactions. In this work, we closely investigate an important simplification of BCQ (Fujimoto et al., 2018a) – a prior approach for offline RL – which removes a heuristic design choice and naturally restrict extracted policies to remain exactly within the support of a given behavior policy. Importantly, in contrast to their original theoretical considerations, we derive this simplified algorithm through the introduction of a novel backup operator, Expected-Max Q-Learning (EMaQ), which is more closely related to the resulting practical algorithm. Specifically, in addition to the distribution support, EMaQ explicitly considers the number of samples and the proposal distribution, allowing us to derive new sub-optimality bounds which can serve as a novel measure of complexity for offline RL problems. In the offline RL setting – the main focus of this work – EMaQ matches and outperforms prior state-of-the-art in the D4RL benchmarks (Fu et al., 2020a). In the online RL setting, we demonstrate that EMaQ is competitive with Soft Actor Critic (SAC). The key contributions of our empirical findings are demonstrating the importance of careful generative model design for estimating behavior policies, and an intuitive notion of complexity for offline RL problems. With its simple interpretation and fewer moving parts, such as no explicit function approximator representing the policy, EMaQ serves as a strong yet easy to implement baseline for future work.","This paper proposes EMaQ (Expected-Max Q-Learning) for offline reinforcement learning, which is a simplified version of BCQ (Fujimoto et al., 2018a). The main idea is to use the expected-max Q-learning operator (EMaQ) to constrain learned policies to remain close to the given dataset of interactions. The authors also derive a new sub-optimality bound for EMaq that explicitly considers the number of samples and the proposal distribution, which can serve as a novel measure of complexity for offline RL problems. Empirical results on the D4RL benchmark show that EMAQ outperforms prior state-of-the-art in the offline RL setting. In the online RL setting, the authors demonstrate that EMa Q is competitive with Soft Actor Critic."
181,SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit – it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts. Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes.","This paper proposes a batch selection algorithm for improving model fairness in machine learning models. The proposed algorithm is based on batch optimization with an outer optimizer that selects minibatch sizes to improve model fairness. The inner optimizer is the standard training algorithm, while the outer optimiser is a variant of batch selection. The authors propose to use batch selection as an outer optimization problem, and propose a new batch selection method called FairBatch. The main contribution of the paper is that the proposed algorithm does not require any modification to data preprocessing or model training. The experiments conducted both on synthetic and benchmark real data demonstrate that the algorithm can achieve comparable performance while achieving comparable (or even greater) performance against the state of the art."
182,SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"Several methods have been proposed in recent years to provide bounds on the Lipschitz constants of deep networks, which can be used to provide robustness guarantees, generalization bounds, and characterize the smoothness of decision boundaries. However, existing bounds get substantially weaker with increasing depth of the network, which makes it unclear how to apply such bounds to recently proposed models such as the deep equilibrium (DEQ) model, which can be viewed as representing an infinitely-deep network. In this paper, we show that monotone DEQs, a recently-proposed subclass of DEQs, have Lipschitz constants that can be bounded as a simple function of the strong monotonicity parameter of the network. We derive simple-yet-tight bounds on both the input-output mapping and the weight-output mapping defined by these networks, and demonstrate that they are small relative to those for comparable standard DNNs. We show that one can use these bounds to design monotone DEQ models, even with e.g. multiscale convolutional structure, that still have constraints on the Lipschitz constant. We also highlight how to use these bounds to develop PAC-Bayes generalization bounds that do not depend on any depth of the network, and which avoid the exponential depth-dependence of comparable DNN bounds.","This paper studies the bounds on the Lipschitz constant of monotone deep equilibrium (DEQ) networks. The authors show that the monotonicity of the network can be expressed as a function of the strong monotonity of the input-output mapping and the weight output mapping, and derive simple-yet-tight bounds on both of them. They also show how to use these bounds to develop PAC-Bayes generalization bounds that do not depend on the depth of the networks and avoid the exponential depth dependence of comparable DNN bounds."
183,SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This work considers two distinct settings: imitation learning and goal-conditioned reinforcement learning. In either case, effective solutions require the agent to reliably reach a specified state (a goal), or set of states (a demonstration). Drawing a connection between probabilistic long-term dynamics and the desired value function, this work introduces an approach that utilizes recent advances in density estimation to effectively learn to reach a given state. We develop a unified view on the two settings and show that the approach can be applied to both. In goalconditioned reinforcement learning, we show it to circumvent the problem of sparse rewards while addressing hindsight bias in stochastic domains. In imitation learning, we show that the approach can learn from extremely sparse amounts of expert data and achieves state-of-the-art results on a common benchmark.","This paper proposes a method for imitation learning and goal-conditioned reinforcement learning. The main idea is to use density estimation to estimate the probability of reaching a given state, and then use this estimate to train an agent to reach a goal state. The paper is well-written and well-motivated, and the experimental results show that the proposed method can be applied to both imitation learning as well as goal conditioned reinforcement learning, where it can circumvent the problem of sparse rewards while addressing hindsight bias in stochastic domains."
184,SP:d57550b2f323b356d7e609acc35ee33039f376b4,"Multi-task learning aims to improve the overall performance of a set of tasks by leveraging their relatedness. When training data is limited using priors is pivotal, but currently this is done in ad-hoc ways. In this paper, we develop variational multi-task learning VMTL, a general probabilistic inference framework for simultaneously learning multiple related tasks. We cast multi-task learning as a variational Bayesian inference problem, which enables task relatedness to be explored in a principled way by specifying priors. We introduce Gumbel-softmax priors to condition the prior of each task on related tasks. Each prior is represented as a mixture of variational posteriors of other related tasks and the mixing weights are learned in a data-driven manner for each individual task. The posteriors over representations and classifiers are inferred jointly for all tasks and individual tasks are able to improve their performance by using the shared inductive bias. Experimental results demonstrate that VMTL is able to tackle challenging multi-task learning with limited training data well, and it achieves state-of-the-art performance on four benchmark datasets consistently surpassing previous methods.","This paper proposes a variational multi-task learning (VMTL) framework for multi-tasks learning. The main idea is to use Gumbel-softmax priors to condition the prior of each task on related tasks. Each prior is represented as a mixture of variational posteriors of other related tasks and the mixing weights are learned in a data-driven manner for each individual task. The posteriors over representations and classifiers are inferred jointly for all tasks and individual tasks are able to improve their performance by using the shared inductive bias. The experimental results demonstrate that VMTL is able to tackle challenging multi- task learning with limited training data well, and it achieves state-of-the-art performance on four benchmark datasets consistently surpassing previous methods."
185,SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, Long-Range Arena, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. Long-Range Arena paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle.","This paper proposes a systematic and unified benchmark, Long-Range Arena, specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. The paper systematically evaluate ten well-established long-range Transformer models on the newly proposed benchmark suite. The proposed benchmark paves the way towards better understanding this class of efficient Transformer model, facilitates more research in this direction, and presents new challenging tasks to tackle."
186,SP:e12e410c3335b76133ceda4c865b244fbbab8580,"Source code (Context) and its parsed abstract syntax tree (AST; Structure) are two complementary representations of the same computer program. Traditionally, designers of machine learning models have relied predominantly either on Structure or Context. We propose a new model, which jointly learns on Context and Structure of source code. In contrast to previous approaches, our model uses only language-agnostic features, i.e., source code and features that can be computed directly from the AST. Besides obtaining state-of-the-art on monolingual code summarization on all five programming languages considered in this work, we propose the first multilingual code summarization model. We show that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages. Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code.",This paper proposes a multi-language code summarization model that combines source code (context) and abstract syntax tree (AST) representation learning. The main idea is to combine source code and AST representation learning in the same model. The model is trained on monolingual code and multi-lingual code. The authors show that the proposed model outperforms the state-of-the-art on all five programming languages considered in this paper.
187,SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation. Project: http://vision.cs.utexas.edu/ projects/audio_visual_waypoints.","This paper proposes a reinforcement learning method for audio-visual navigation in 3D scenes. The proposed method is based on the idea of waypoints that are dynamically set and learned end-to-end within the navigation policy, and an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Experiments are conducted on Replica and Matterport3D datasets and show that the proposed method outperforms the state-of-the-art."
188,SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"Efforts to improve the learning abilities of neural networks have focused mostly on the role of optimization methods rather than on weight initializations. Recent findings, however, suggest that neural networks rely on lucky random initial weights of subnetworks called “lottery tickets” that converge quickly to a solution (Frankle & Carbin, 2018). To investigate how weight initializations affect performance, we examine small convolutional networks that are trained to predict n steps of the two-dimensional cellular automaton Conway’s Game of Life, the update rules of which can be implemented efficiently in a small CNN. We find that networks of this architecture trained on this task rarely converge. Rather, networks require substantially more parameters to consistently converge. Furthermore, we find that the initialization parameters that gradient descent converges to a solution are sensitive to small perturbations, such as a single sign change. Finally, we observe a critical value d0 such that training minimal networks with examples in which cells are alive with probability d0 dramatically increases the chance of convergence to a solution. Our results are consistent with the lottery ticket hypothesis (Frankle & Carbin, 2018).","This paper investigates how weight initializations affect the performance of small convolutional networks trained to predict n steps of the two-dimensional cellular automaton Conway’s Game of Life. The authors show that networks of this architecture trained on this task rarely converge. They find that networks require substantially more parameters to consistently converge. Furthermore, they find that the initialization parameters that gradient descent converges to a solution are sensitive to small perturbations, such as a single sign change. Finally, they observe a critical value d0 such that training minimal networks with examples in which cells are alive with probability d0 dramatically increases the chance of convergence."
189,SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"Semi-supervised learning (SSL) has played an important role in leveraging unlabeled data when labeled data is limited. One of the most successful SSL approaches is based on consistency regularization, which encourages the model to produce unchanged with perturbed input. However, there has been less attention spent on inputs that have the same label. Motivated by the observation that the inputs having the same label should have the similar model outputs, we propose a novel method, RankingMatch, that considers not only the perturbed inputs but also the similarity among the inputs having the same label. We especially introduce a new objective function, dubbed BatchMean Triplet loss, which has the advantage of computational efficiency while taking into account all input samples. Our RankingMatch achieves state-of-the-art performance across many standard SSL benchmarks with a variety of labeled data amounts, including 95.13% accuracy on CIFAR-10 with 250 labels, 77.65% accuracy on CIFAR-100 with 10000 labels, 97.76% accuracy on SVHN with 250 labels, and 97.77% accuracy on SVHN with 1000 labels. We also perform an ablation study to prove the efficacy of the proposed BatchMean Triplet loss against existing versions of Triplet loss.","This paper proposes a new semi-supervised learning method that considers not only the perturbed inputs but also the similarity among the inputs having the same label. The authors propose a new objective function, dubbed BatchMean Triplet loss, which has the advantage of computational efficiency while taking into account all input samples. The proposed method achieves state-of-the-art performance across many standard SSL benchmarks with a variety of labeled data amounts."
190,SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"Few-shot meta-learning methods consider the problem of learning new tasks from a small, fixed number of examples, by meta-learning across static data from a set of previous tasks. However, in many real world settings, it is more natural to view the problem as one of minimizing the total amount of supervision — both the number of examples needed to learn a new task and the amount of data needed for metalearning. Such a formulation can be studied in a sequential learning setting, where tasks are presented in sequence. When studying meta-learning in this online setting, a critical question arises: can meta-learning improve over the sample complexity and regret of standard empirical risk minimization methods, when considering both meta-training and adaptation together? The answer is particularly non-obvious for meta-learning algorithms with complex bi-level optimizations that may demand large amounts of meta-training data. To answer this question, we extend previous meta-learning algorithms to handle the variable-shot settings that naturally arise in sequential learning: from many-shot learning at the start, to zero-shot learning towards the end. On sequential learning problems, we find that meta-learning solves the full task set with fewer overall labels and achieves greater cumulative performance, compared to standard supervised methods. These results suggest that meta-learning is an important ingredient for building learning systems that continuously learn and improve over a sequence of problems.","This paper studies the problem of meta-learning in the context of few-shot meta-training, where the goal is to learn new tasks from a small, fixed number of examples from a set of previous tasks. The authors consider the problem in a sequential learning setting, where tasks are presented in sequence. They extend previous meta-Learning algorithms to handle the variable-shot settings that naturally arise in sequential learning: from many-shot learning at the start, to zero-shot training towards the end. On sequential learning problems, they find that meta- learning solves the full task set with fewer overall labels and achieves greater cumulative performance, compared to standard supervised methods."
191,SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"While contextual representations from pretrained Transformer models have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from Transformers networks pretrained with self-supervision. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of Transformer representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. We also connect our probe results to the Transformer architecture by relating the attention mechanism to syntactic distance between two words. Results from the three probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. In particular, sensitivity to local phrase structure increases along deeper layers. Based on our analysis of attention, we show that this is at least partly explained by generally larger attention weights between syntactically distant words.1","This paper presents a series of probes designed to test the sensitivity of Transformer representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. The authors experiment with three different perturbations: (1) random permutations of n-grams of varying width, (2) swapping of two spans which do or do not form a syntactic phrase, and (3) swapping two adjacent words which do not break apart a phrase, to test sensitivity to local phrase structure. Results from the three probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process."
192,SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024 × 1024 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains 1, we show our model’s superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited.","This paper proposes a method for few-shot image synthesis task for GAN with minimum computing cost. The authors propose a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. The model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples."
193,SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"Tight and efficient neural network bounding is of critical importance for the scaling of neural network verification systems. A number of efficient specialised dual solvers for neural network bounds have been presented recently, but they are often too loose to verify more challenging properties. This lack of tightness is linked to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise linear activations exists, it comes at the cost of exponentially many constraints and thus currently lacks an efficient customised solver. We alleviate this deficiency via a novel dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. Our method recovers the strengths of the new relaxation in the dual space: tightness and a linear separation oracle. At the same time, it shares the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time. As a consequence, we obtain better bounds than off-the-shelf solvers in only a fraction of their running time and recover the speed-accuracy trade-offs of looser dual solvers if the computational budget is small. We demonstrate that this results in significant formal verification speed-ups.","This paper proposes a dual dual algorithm for neural network bounding. The main idea is to use a linear program of size linear in the number of neurons instead of a piecewise linear program, which is usually the case. The authors argue that this choice of linear program is a weakness of the employed relaxation, which comes at the cost of exponentially many constraints and thus currently lacks an efficient customised solver. To address this issue, the authors propose a dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. The proposed dual algorithm shares the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time."
194,SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM)1, can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, “plug-and-play” method for improving the commonsense reasoning ability of a PTLM.","This paper proposes a method for augmenting pre-trained language models with concept-centric commonsense knowledge. Specifically, the authors propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream tasks). The authors also develop a joint pretraining framework to unify generative-contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that the proposed method, concept-aware language model (CALM), can pack more commonsens knowledge into the parameters of a pre- trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks."
195,SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"We study the problem of unsupervised physical object discovery. While existing frameworks aim to decompose scenes into 2D segments based off each object’s appearance, we explore how physics, especially object interactions, facilitates disentangling of 3D geometry and position of objects from video, in an unsupervised manner. Drawing inspiration from developmental psychology, our Physical Object Discovery Network (POD-Net) uses both multi-scale pixel cues and physical motion cues to accurately segment observable and partially occluded objects of varying sizes, and infer properties of those objects. Our model reliably segments objects on both synthetic and real scenes. The discovered object properties can also be used to reason about physical events.","This paper proposes a method for unsupervised physical object discovery. The method is based on the idea that physics, especially object interactions, facilitates disentangling of 3D geometry and position of objects from video. The proposed method uses both multi-scale pixel cues and physical motion cues to accurately segment observable and partially occluded objects of varying sizes and infer properties of those objects. The model reliably segments objects on both synthetic and real scenes. The discovered object properties can be used to reason about physical events."
196,SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"Deep neural networks (DNNs), including convolutional neural networks, are known to be vulnerable to adversarial attacks, which may lead to disastrous consequences in life-critical applications. Adversarial samples are usually generated by attack algorithms and can also be induced by white noises, and therefore the threats are real. In this study, we propose a novel training method, named Increasing Margin Adversarial (IMA) Training, to improve DNN robustness against adversarial noises. During training, the IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The IMA method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attacks, and the results show that the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data. We hope our approach may facilitate the development of robust DNN applications, especially for COVID-19 diagnosis using CT images.","This paper proposes a method for improving the robustness of deep neural networks (DNNs) against adversarial attacks. Specifically, the authors propose to increase the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. The proposed method is evaluated on six publicly available datasets (including a COVID-19 CT image dataset) under strong 100-PGD white-box adversarial attack and the proposed method significantly improved classification accuracy on noisy data while keeping a relatively high accuracy on clean data."
197,SP:276ffd59fbf49e3ee02756da8920218102214917,"Deep neural networks often have huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. However, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima. In this paper, we propose ProKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student’s parameter space. Such projection is implemented by decomposing the training objective into local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that our proposed ProKT consistently achieves the state-of-the-art performance comparing to all existing knowledge distillation methods.","This paper proposes ProKT, a knowledge distillation method for deep neural networks. The main idea is to project the supervision signals of a teacher model into the student’s parameter space by decomposing the training objective into local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that the proposed ProKT consistently achieves the state-of-the-art performance."
198,SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"In this paper, we propose a novel channel pruning method to solve the problem of compression and acceleration of Convolutional Neural Networks (CNNs). Previous channel pruning methods usually ignore the relationships between channels and layers. Many of them parameterize each channel independently by using gates or similar concepts. To fill this gap, a hyper-structure network is proposed to generate the architecture of the main network. Like the existing hypernet, our hyperstructure network can be optimized by regular backpropagation. Moreover, we use a regularization term to specify the computational resource of the compact network. Usually, FLOPs is used as the criterion of computational resource. However, if FLOPs is used in the regularization, it may over penalize early layers. To address this issue, we further introduce learnable layer-wise scaling factors to balance the gradients from different terms, and they can be optimized by hyper-gradient descent. Extensive experimental results on CIFAR-10 and ImageNet show that our method is competitive with state-of-the-art methods.","This paper proposes a channel pruning method to solve the problem of compression and acceleration of Convolutional Neural Networks (CNNs). The proposed method uses a hyper-structure network to generate the architecture of the main network, which can be optimized by regular backpropagation. The authors also use a regularization term to specify the computational resource of the compact network. Extensive experimental results on CIFAR-10 and ImageNet show that the proposed method is competitive with state-of-the-art methods."
199,SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"In this paper, we demonstrate how to do automated higher-order logic theorem proving in the presence of a large knowledge base of potential premises without learning from human proofs. We augment the exploration of premises based on a simple tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. Our experiments show that our theorem prover trained with this exploration mechanism but no human proofs, dubbed DeepHOL Zero, outperforms provers that are trained only on human proofs. It approaches the performance of a prover trained by a combination of imitation and reinforcement learning. We perform multiple experiments to understand the importance of the underlying assumptions that make our exploration approach work, thus explaining our design choices.","This paper presents a method for learning a theorem prover that can be used to prove a higher-order logic theorem in the presence of a large knowledge base of potential premises without learning from human proofs. The method is based on the exploration of premises based on a simple tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. The experiments show that the algorithm trained with this exploration mechanism but no human proofs, dubbed DeepHOL Zero, outperforms provers that are trained only on human proofs and approaches the performance of a prover trained by a combination of imitation and reinforcement learning."
200,SP:88209417a8ad07e6103084e41709be900303ce5f,"Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization capabilities of models for various machine learning tasks, the underlying augmentation methods are usually manually designed and carefully evaluated for each data modality separately. These include image processing functions for image data and word-replacing rules for text data. In this work, we propose an automated data augmentation approach called MODALS (Modalityagnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentation to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Through comprehensive experiments, we demonstrate the effectiveness of MODALS on multiple datasets for text, tabular, time-series and image modalities.1","This paper proposes MODALS (Modalityagnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentation to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Experiments on multiple datasets for text, tabular, time-series and image modalities demonstrate the effectiveness of MODALS."
201,SP:6d84670d321b0d584b097c630574bd748e85c9a2,"In the mean field regime, neural networks are appropriately scaled so that as the width tends to infinity, the learning dynamics tends to a nonlinear and nontrivial dynamical limit, known as the mean field limit. This lends a way to study large-width neural networks via analyzing the mean field limit. Recent works have successfully applied such analysis to two-layer networks and provided global convergence guarantees. The extension to multilayer ones however has been a highly challenging puzzle, and little is known about the optimization efficiency in the mean field regime when there are more than two layers. In this work, we prove a global convergence result for unregularized feedforward three-layer networks in the mean field regime. We first develop a rigorous framework to establish the mean field limit of three-layer networks under stochastic gradient descent training. To that end, we propose the idea of a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove a global convergence guarantee under suitable regularity and convergence mode assumptions, which – unlike previous works on two-layer networks – does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural networks, which importantly is shown to hold at any finite training time (not necessarily at convergence) via an algebraic topology argument.","This paper studies the global convergence of three-layer neural networks in the mean-field regime. The authors propose a neuronal embedding, which consists of a fixed probability space that encapsulates neural networks of arbitrary sizes. They prove a global convergence guarantee under suitable regularity and convergence mode assumptions, which does not rely critically on convexity. The result is shown to hold at any finite training time (not necessarily at convergence) via an algebraic topology argument."
202,SP:b90f893f927db9c439595fd119a565cf43c971f4,"Building interpretable parameterizations of real-world decision-making on the basis of demonstrated behavior—i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function—is essential for introspecting and auditing policies in different institutions. In this paper, we propose learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to “what if” outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these costbenefit tradeoffs associated with the expert’s actions, we integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making—where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting, and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states. Through illustrative experiments in both real and simulated medical environments, we highlight the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of behavior.","This paper proposes a method for learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to “what if” outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these costbenefit tradeoffs associated with the expert’s actions, the authors integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, Counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states."
203,SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"Multitask Reinforcement Learning is a promising way to obtain models with better performance, generalisation, data efficiency, and robustness. Most existing work is limited to compatible settings, where the state and action space dimensions are the same across tasks. Graph Neural Networks (GNN) are one way to address incompatible environments, because they can process graphs of arbitrary size. They also allow practitioners to inject biases encoded in the structure of the input graph. Existing work in graph-based continuous control uses the physical morphology of the agent to construct the input graph, i.e., encoding limb features as node labels and using edges to connect the nodes if their corresponded limbs are physically connected. In this work, we present a series of ablations on existing methods that show that morphological information encoded in the graph does not improve their performance. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, we also propose AMORPHEUS, a transformer-based approach. Further results show that, while AMORPHEUS ignores the morphological information that GNNs encode, it nonetheless substantially outperforms GNN-based methods that use the morphological information to define the message-passing scheme.","This paper presents a series of ablations on existing methods that show that morphological information encoded in the graph does not improve their performance. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, the authors propose AMORPHEUS, a transformer-based approach. The experiments show that the proposed method is able to outperform GNN-based methods that use the morphologically information to define the message passing scheme."
204,SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for ‘number’ related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting.","This paper proposes a method for visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, the authors propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, they call their method MoVie, short for Modulated ConVolutional bottlenecks. The authors demonstrate strong performance for counting: 1) advancing the state of the art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped to secure the first place of 2020VQA challenge when integrated as a module for ‘number-related questions’ in generic models."
205,SP:c64e77507e562f236cb69361b22fb1a7951ffb22,"In a poisoning attack, an adversary with control over a small fraction of the training data attempts to select that data in a way that induces a model that misbehaves in a particular way desired by the adversary, such as misclassifying certain inputs. We propose an efficient poisoning attack that can target a desired model based on online convex optimization. Unlike previous model-targeted poisoning attacks, our attack comes with provable convergence to any achievable target classifier. The distance from the induced classifier to the target classifier is inversely proportional to the square root of the number of poisoning points. We also provide a lower bound on the minimum number of poisoning points needed to achieve a given target classifier. Our attack is the first model-targeted poisoning attack that provides provable convergence, and in our experiments it either exceeds or matches the best state-of-the-art attacks in terms of attack success rate and distance to the target model. In addition, as an online attack our attack can incrementally determine nearly optimal poisoning points.","This paper proposes a model-targeted poisoning attack that can target a desired model based on online convex optimization. The authors prove that the distance from the induced classifier to the target classifier is inversely proportional to the square root of the number of poisoning points. They also provide a lower bound on the minimum number of poisoned points needed to achieve a given target model. In experiments, the authors show that the proposed attack can either match or outperform the best state-of-the-art attacks in terms of attack success rate and distance to target."
206,SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"To alleviate the resource constraint for real-time point cloud applications that run on edge devices, in this paper we present BiPointNet, the first model binarization approach for efficient deep learning on point clouds. We discover that the immense performance drop of binarized models for point clouds mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. With theoretical justifications and in-depth analysis, our BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy, and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that BiPointNet outperforms existing binarization methods by convincing margins, at the level even comparable with the full precision counterpart. We highlight that our techniques are generic, guaranteeing significant improvements on various fundamental tasks and mainstream backbones. Moreover, BiPointNet gives an impressive 14.7× speedup and 18.9× storage saving on real-world resource-constrained devices.","This paper proposes a method to improve the performance of binarized models for real-time point cloud applications that run on edge devices. The authors argue that aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. They propose Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that BiPointNet outperforms existing binarization methods by convincing margins, at the level even comparable with the full precision counterpart."
207,SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"Transformer-based models have achieved state-of-the-art results in many natural language processing tasks. The self-attention architecture allows transformer to combine information from all elements of a sequence into context-aware representations. However, information about the context is stored mostly in the same element-wise representations. This might limit the processing of properties related to the sequence as a whole more difficult. Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model. Memory-augmented neural networks (MANNs) extend traditional neural architectures with general-purpose memory for representations. MANNs have demonstrated the capability to learn simple algorithms like Copy or Reverse and can be successfully trained via backpropagation on diverse tasks from question answering to language modeling outperforming RNNs and LSTMs of comparable complexity. In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer. We evaluate these memory augmented Transformers and demonstrate that presence of memory positively correlates with the model performance for machine translation and language modelling tasks. Augmentation of pre-trained masked language model with memory tokens shows mixed results for tasks from GLUE benchmark. Visualization of attention patterns over the memory suggest that it improves the model’s ability to process a global context.",This paper proposes a few modifications to the Transformer architecture to improve the performance of the model. The main idea is to add a memory module to the self-attention layer of the transformer architecture to store non-local information about the context of the input sequence. The authors show that adding the memory module can improve the model's ability to process a global context. Experiments are conducted on a variety of language modeling tasks and machine translation tasks. 
208,SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it encodes semantic structures discovered by clustering into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.","This paper proposes Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. Specifically, PCL introduces prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. PCL iteratively performs E-step as finding the distribution of prototypes via clustering and M-step is optimizing the network via contrastive learn. The paper proposes ProtoNCE loss, a generalized version of the InfoNCE losses for contrastive loss, which encourages representations to be closer to their assigned prototypes."
209,SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,"It is now widely known that by adversarial attacks, clean images with invisible perturbations can fool deep neural networks. To defend adversarial attacks, we design a block containing multiple paths to learn robust features and the parameters of these paths are required to be orthogonal with each other. The so-called Orthogonal Multi-Path (OMP) block could be posed in any layer of a neural network. Via forward learning and backward correction, one OMP block makes the neural networks learn features that are appropriate for all the paths and hence are expected to be robust. With careful design and thorough experiments on e.g., the positions of imposing orthogonality constraint, and the trade-off between the variety and accuracy, the robustness of the neural networks is significantly improved. For example, under white-box PGD attack with l∞ bound 8/255 (this is a fierce attack that can make the accuracy of many vanilla neural networks drop to nearly 10% on CIFAR10), VGG16 with the proposed OMP block could keep over 50% accuracy. For black-box attacks, neural networks equipped with an OMP block have accuracy over 80%. The performance under both white-box and black-box attacks is much better than the existing state-of-the-art adversarial defenders.","This paper proposes an orthogonal multi-path (OMP) block to improve the robustness of deep neural networks against adversarial attacks. The proposed OMP block is based on the idea of orthogonality constraint, where the parameters of the paths are orthogonic with each other. The authors propose to use forward learning and backward correction to learn features that are appropriate for all the paths and hence are expected to be robust. Experiments are conducted on CIFAR-10 and ImageNet and show that the proposed method can improve the performance of neural networks in both white-box and black-box attacks."
210,SP:776df66274ed12449fde8dcef873a593980f397c,"Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.","This paper proposes a self-supervised graph attention network (SuperGAT) for noisy graphs. Specifically, the authors exploit two attention forms compatible with a self supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. The experiment on 17 real-world datasets demonstrates that the recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines."
211,SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"Dialogue system for medical automatic diagnosis (DSMAD) aims to learn an agent that mimics the behavior of a human doctor, i.e. inquiring symptoms and informing diseases. Since DSMAD has been formulated as a Markov decisionmaking process, many studies apply reinforcement learning methods to solve it. Unfortunately, existing works solely rely on simple diagnostic accuracy to justify the effectiveness of their DSMAD agents while ignoring the medical rationality of the inquiring process. From the perspective of medical application, it’s critical to develop an agent that is able to produce reliable and convincing diagnosing processes and also is robust in making diagnosis facing noisy interaction with patients. To this end, we propose a novel DSMAD agent, INS-DS (Introspective Diagnosis System) comprising of two separate yet cooperative modules, i.e., an inquiry module for proposing symptom-inquiries and an introspective module for deciding when to inform a disease. INS-DS is inspired by the introspective decision-making process of human, where the inquiry module first proposes the most valuable symptom inquiry, then the introspective module intervenes the potential responses of this inquiry and decides to inquire only if the diagnoses of these interventions vary. We also propose two evaluation metrics to validate the reliability and robustness of DSMAD methods. Extensive experimental results demonstrate that INS-DS achieves the new state-of-the-art under various experimental settings and possesses the advantages of reliability and robustness compared to other methods.","This paper proposes an agent for DSMAD, which aims to learn an agent that mimics the behavior of a human doctor, i.e. inquiring symptoms and informing diseases. The proposed agent consists of two modules: an inquiry module for proposing symptom-inquiries and an introspective module for deciding when to inform a disease. The authors propose two evaluation metrics to validate the reliability and robustness of DSMAD methods. Extensive experimental results demonstrate that INS-DS achieves the new state-of-the-art under various experimental settings."
212,SP:10ae09d90d465125433a9b4f15b1405ab017920d,"We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018.","This paper proposes an adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to address the natural world distribution of fine-grained and long-tailed properties of visual classification. The BCN term can alleviate possible overfitting due to exploring image features of fine details. More importantly, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. The experimental results show the effectiveness of the proposed method on several benchmark datasets."
213,SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms.",This paper proposes a Bayesian imitation learning method for inverse reinforcement learning. The main idea is to jointly learn an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to the latent reward. The proposed method is evaluated on real medical data alongside classic control simulations and compared with focused offline imitation learning algorithms. The experimental results show that the proposed method can achieve state-of-the-art performance on a variety of tasks.
214,SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"Search is an important tool for computing effective policies in singleand multiagent environments, and has been crucial for achieving superhuman performance in several benchmark fully and partially observable games. However, one major limitation of prior search approaches for partially observable environments is that the computational cost scales poorly with the amount of hidden information. In this paper we present Learned Belief Search (LBS), a computationally efficient search procedure for partially observable environments. Rather than maintaining an exact belief distribution, LBS uses an approximate auto-regressive counterfactual belief that is learned as a supervised task. In multi-agent settings, LBS uses a novel public-private model architecture for underlying policies in order to efficiently evaluate these policies during rollouts. In the benchmark domain of Hanabi, LBS obtains more than 60% of the benefit of exact search while reducing compute requirements by 35×, allowing it to scale to larger settings that were inaccessible to previous search methods.","This paper proposes a method for learning counterfactual belief distributions for partially observable environments. The method is based on an auto-regressive belief distribution that is learned as a supervised task. The proposed method is evaluated on Hanabi, where it is shown to outperform the state-of-the-art in terms of performance on the Hanabi benchmark. "
215,SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"Planning in large state spaces inevitably needs to balance depth and breadth of the search. It has a crucial impact on planners performance and most manage this interplay implicitly. We present a novel method Shoot Tree Search (STS), which makes it possible to control this trade-off more explicitly. Our algorithm can be understood as an interpolation between two celebrated search mechanisms: MCTS and random shooting. It also lets the user control the bias-variance trade-off, akin to TD(n), but in the tree search context. In experiments on challenging domains, we show that STS can get the best of both worlds consistently achieving higher scores.","This paper proposes a new algorithm for tree search that aims to balance depth and breadth of the search. The proposed algorithm is based on a combination of MCTS and random shooting. The authors show that the proposed algorithm can be seen as an interpolation between two celebrated search mechanisms: MCTs and Random Shooting. It also lets the user control the bias-variance trade-off, akin to TD(n), but in the tree search context. The experimental results show that STS can get the best of both worlds consistently achieving higher scores."
216,SP:5efc271ccc555fd9aa542548838170bd4c98e957,"While designing inductive bias in neural architectures has been widely studied, we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks. Here, we replace architecture engineering by encoding inductive bias in the form of datasets. Inspired by Peirce’s view that deduction, induction, and abduction form an irreducible set of reasoning primitives, we design three synthetic tasks that are intended to require the model to have these three abilities. We specifically design these synthetic tasks in a way that they are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre-training methodology called “LIME” (Learning Inductive bias for Mathematical rEasoning). Models trained with LIME significantly outperform vanilla transformers on three very different large mathematical reasoning benchmarks. Unlike dominating the computation cost as traditional pre-training approaches, LIME requires only a small fraction of the computation cost of the typical downstream task.","This paper proposes a new pre-training method for inductive bias in neural networks. The authors propose to train a transformer network on a set of synthetic tasks that are designed to require the model to have deduction, induction, and abduction abilities. These synthetic tasks are designed in a way that they are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new method called LIME (Learning Inductive bias for Mathematical rEasoning), which is based on the idea that inductive biases are encoded in the form of datasets. Experiments show that LIME significantly outperforms vanilla transformers on three different mathematical reasoning benchmarks."
217,SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"We analyze the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets, when trained on exponential or cross-entropy loss. Our analysis focuses on exponential weight normalization (EWN), which encourages weight updates along the radial direction. This paper shows that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. These results can be extended to hold for gradient descent via an appropriate adaptive learning rate. The asymptotic convergence rate of the loss in this setting is given by Θ( 1 t(log t)2 ), and is independent of the depth of the network. We contrast these results with the inductive bias of standard weight normalization (SWN) and unnormalized architectures, and demonstrate their implications on synthetic data sets. Experimental results on simple data sets and architectures support our claim on sparse EWN solutions, even with SGD. This demonstrates its potential applications in learning prunable neural networks.","This paper studies the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets, when trained on exponential or cross-entropy loss. The authors show that the gradient flow path with exponential weight normalization (EWN) is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. These results can be extended to hold for gradient descent via an appropriate adaptive learning rates. Experimental results on simple data sets and architectures support the claim on sparse EWN solutions, even with SGD."
218,SP:c71f9d2a602516865a0b103028186e83b52e5f00,"Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator. Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator’s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that dynamically spawns additional discriminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.","This paper studies the mode collapse problem in generative adversarial networks (GANs) and proposes a novel training procedure that dynamically spawns additional discriminators to remember previous modes of generation. The authors claim that mode collapse is caused by the discriminator’s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, the authors introduce a new training procedure to dynamically generate additional discriminator to remember the previous generation. Experiments on several datasets show that the proposed method can mitigate mode collapse and improve standard metrics."
219,SP:52c48198c95826e042f9e5a512ef3265daaff882,"How can we effectively regularize BERT? Although BERT proves its effectiveness in various downstream natural language processing tasks, it often overfits when there are only a small number of training instances. A promising direction to regularize BERT is based on pruning its attention heads based on a proxy score for head importance. However, heuristic-based methods are usually suboptimal since they predetermine the order by which attention heads are pruned. In order to overcome such a limitation, we propose AUBER, an effective regularization method that leverages reinforcement learning to automatically prune attention heads from BERT. Instead of depending on heuristics or rule-based policies, AUBER learns a pruning policy that determines which attention heads should or should not be pruned for regularization. Experimental results show that AUBER outperforms existing pruning methods by achieving up to 9.39% better accuracy. In addition, our ablation study empirically demonstrates the effectiveness of our design choices for AUBER.","This paper proposes a method to regularize BERT by pruning attention heads based on a proxy score for head importance. The paper proposes to use reinforcement learning to automatically prune attention heads from BERT. Instead of relying on heuristics or rule-based policies, AUBER learns a pruning policy that determines which attention heads should or should not be pruned for regularization. The experimental results show that the proposed method outperforms existing pruning methods by achieving up to 9.39% better accuracy."
220,SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"At the heart of many robotics problems is the challenge of learning correspondences across domains. For instance, imitation learning requires obtaining correspondence between humans and robots; sim-to-real requires correspondence between physics simulators and the real world; transfer learning requires correspondences between different robotics environments. This paper aims to learn correspondence across domains differing in representation (vision vs. internal state), physics parameters (mass and friction), and morphology (number of limbs). Importantly, correspondences are learned using unpaired and randomly collected data from the two domains. We propose dynamics cycles that align dynamic robot behavior across two domains using a cycle-consistency constraint. Once this correspondence is found, we can directly transfer the policy trained on one domain to the other, without needing any additional fine-tuning on the second domain. We perform experiments across a variety of problem domains, both in simulation and on real robot. Our framework is able to align uncalibrated monocular video of a real robot arm to dynamic state-action trajectories of a simulated arm without paired data. Video demonstrations of our results are available at: https://sjtuzq.github.io/cycle_dynamics.html.","This paper proposes a method for learning correspondence between two different domains, i.e., sim-to-real and real-world. The method is based on a cycle-consistency constraint on the dynamics of the two domains, which is used to train a policy on one domain and then transfer it to the other domain. The paper is well-written and well-motivated, and the experiments show that the proposed method is able to achieve state-of-the-art results on both simulated and real robotic tasks."
221,SP:006434d56992836ab9420d7d4215bc70664de304,"Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model’s predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model’s features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While “off-manifold” Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.","This paper studies the problem of using Shapley explainability in the context of machine learning. In particular, the authors argue that the Shapley framework for explainability attributes a model’s predictions to its input features in a mathematically principled and model-agnostic way. The authors demonstrate the drawbacks of this assumption and develop two solutions that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns Shapley value-function, providing performance and stability."
222,SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,"Modelling the behaviours of other agents (opponents) is essential for understanding how agents interact and making effective decisions. Existing methods for opponent modelling commonly assume knowledge of the local observations and chosen actions of the modelled opponents, which can significantly limit their applicability. We propose a new modelling technique based on variational autoencoders, which are trained to reconstruct the local actions and observations of the opponent based on embeddings which depend only on the local observations of the modelling agent (its observed world state, chosen actions, and received rewards). The embeddings are used to augment the modelling agent’s decision policy which is trained via deep reinforcement learning; thus the policy does not require access to opponent observations. We provide a comprehensive evaluation and ablation study in diverse multi-agent tasks, showing that our method achieves comparable performance to an ideal baseline which has full access to opponent’s information, and significantly higher returns than a baseline method which does not use the learned embeddings.","This paper proposes a new method for opponent modelling based on variational autoencoders. The proposed method is based on learning an embedding of the agent’s actions and actions of the opponent based on the observed world state, chosen actions, and received rewards. The embeddings are used to augment the decision policy which is trained via deep reinforcement learning. Experiments are conducted on a variety of multi-agent tasks and show that the proposed method achieves comparable performance to an ideal baseline which has full access to opponent’S information."
223,SP:c239bc531bcf7293032748af29a1b786e9d893dd,"Contrastive learning has been adopted as a core method for unsupervised visual representation learning. Without human annotation, the common practice is to perform an instance discrimination task: Given a query image crop, this task labels crops from the same image as positives, and crops from other randomly sampled images as negatives. An important limitation of this label assignment strategy is that it can not reflect the heterogeneous similarity between the query crop and each crop from other images, taking them as equally negative, while some of them may even belong to the same semantic class as the query. To address this issue, inspired by consistency regularization in semi-supervised learning on unlabeled data, we propose Consistent Contrast (CO2), which introduces a consistency regularization term into the current contrastive learning framework. Regarding the similarity of the query crop to each crop from other images as “unlabeled”, the consistency term takes the corresponding similarity of a positive crop as a pseudo label, and encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC. This shows that CO2 learns better visual representations for these downstream tasks.","This paper proposes Consistent Contrastive Learning (CO2), a method for unsupervised contrastive learning on unlabeled images. The authors propose to regularize the contrastive loss to encourage consistency between crops from the same image and crops from other images. This is inspired by consistency regularization in semi-supervised learning. The method is evaluated on image classification, object detection, and semantic segmentation tasks, and it is shown to outperform MoCo."
224,SP:d18bab21790713e2facb053c47298fc9079ab783,"Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization have received growing attention due to their favorable last-iterate convergence. However, their behaviors for simple bilinear games over the probability simplex are still not fully understood — previous analysis lacks explicit convergence rates, only applies to an exponentially small learning rate, or requires additional assumptions such as the uniqueness of the optimal solution. In this work, we significantly expand the understanding of last-iterate convergence for OGDA and OMWU in the constrained setting. Specifically, for OMWU in bilinear games over the simplex, we show that when the equilibrium is unique, linear last-iterate convergence is achieved with a learning rate whose value is set to a universal constant, improving the result of (Daskalakis & Panageas, 2019b) under the same assumption. We then significantly extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last-iterate convergence rates with a constant learning rate whose value only depends on the smoothness of the objective function. We show that bilinear games over any polytope satisfy this condition and OGDA converges exponentially fast even without the unique equilibrium assumption. Our condition also holds for strongly-convex-stronglyconcave functions, recovering the result of (Hsieh et al., 2019). Finally, we provide experimental results to further support our theory.","This paper studies the last-iterate convergence of Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for bilinear games over the probability simplex. The authors show that when the equilibrium is unique, OGDA converges exponentially fast with a learning rate whose value is set to a universal constant, improving the result of (Daskalakis & Panageas, 2019b) under the same assumption. They also extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last iterate convergence rates with a constant learning rate. Finally, they provide experimental results to further support their theory."
225,SP:bbc7f77308b298c332a39747f693bc396f00a89f,"We consider the problem of training User Verification (UV) models in federated setup, where the conventional loss functions are not applicable due to the constraints that each user has access to the data of only one class and user embeddings cannot be shared with the server or other users. To address this problem, we propose Federated User Verification (FedUV), a framework for private and secure training of UV models. In FedUV, users jointly learn a set of vectors and maximize the correlation of their instance embeddings with a secret user-defined linear combination of those vectors. We show that choosing the linear combinations from the codewords of an error-correcting code allows users to collaboratively train the model without revealing their embedding vectors. We present the experimental results for user verification with voice, face, and handwriting data and show that FedUV is on par with existing approaches, while not sharing the embeddings with other users or the server.","This paper proposes a method for training user verification models in federated setup, where each user has access to the data of only one class and user embeddings cannot be shared with the server or other users. The authors propose to jointly learn a set of vectors and maximize the correlation of their instance embedding with a secret user-defined linear combination of those vectors. They show that choosing the linear combinations from the codewords of an error-correcting code allows users to collaboratively train the model without revealing their embedding vectors. The experimental results for user verification with voice, face, and handwriting data are presented."
226,SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"Deep neural network classifiers naturally partition input space into regions belonging to different classes. The geometry of these class manifolds (CMs) is widely studied and is intimately related to model performance; for example, the margin is defined via boundaries between these CMs. We present a simple technique to estimate the effective dimension of CMs as well as boundaries between multiple CMs, by computing their intersection with random affine subspaces of varying dimension. We provide a theory for the technique and verify that our theoretical predictions agree with measurements on real neural networks. Through extensive experiments, we leverage this method to show deep connections between the geometry of CMs, generalization, and robustness. In particular we investigate how CM dimension depends on 1) the dataset, 2) architecture, 3) random initialization, 4) stage of training, 5) class, 6) ensemble size, 7) label randomization, 8) training set size, and 9) model robustness to data corruption. Together a picture emerges that well-performing, robust models have higher dimensional CMs than worse performing models. Moreover, we offer a unique perspective on ensembling via intersections of CMs. Our core code is available on Github.","This paper proposes a method to estimate the effective dimension of class manifolds (CMs) by computing their intersection with random affine subspaces of varying dimension. The authors provide a theory for the technique and verify that their theoretical predictions agree with measurements on real neural networks. Through extensive experiments, the authors leverage this method to show deep connections between the geometry of CMs, generalization, and robustness."
227,SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"The trade-off between exploration and exploitation has long been a crucial issue in reinforcement learning (RL). Most of the existing RL methods handle this problem by adding action noise to the policies, such as the Soft Actor-Critic (SAC) (Haarnoja et al., 2018a;b) that introduces an entropy temperature for maximizing both the external value and the entropy of the policy. However, this temperature is applied indiscriminately to all different environment states, undermining the potential of exploration. In this paper, we argue that the agent should explore more in an unfamiliar state, while less in a familiar state, so as to understand the environment more efficiently. To this purpose, we propose Curiosity-Aware entropy Temperature for SAC (CAT-SAC), which utilizes the curiosity mechanism in developing an instance-level entropy temperature. CAT-SAC uses the state prediction error to model curiosity because an unfamiliar state generally has a large prediction error. The curiosity is added to the target entropy to increase the entropy temperature for unfamiliar states and decrease the target entropy for familiar states. By tuning the entropy specifically and adaptively, CAT-SAC is encouraged to explore when its curiosity is large, otherwise, it is encouraged to exploit. Experimental results on the difficult MuJoCo benchmark testify that the proposed CAT-SAC significantly improves the sample efficiency, outperforming the advanced model-based / model-free RL baselines.","This paper proposes a novel approach to explore more in an unfamiliar state, while less in a familiar state, in order to better understand the environment more efficiently. Specifically, the authors propose to use the state prediction error to model curiosity, which is added to the target entropy to increase the entropy temperature for unfamiliar states and decrease the entropy for familiar states. The experiments on the MuJoCo benchmark show that the proposed CAT-SAC significantly improves the sample efficiency, outperforming the advanced model-based / model-free RL baselines."
228,SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"Reinforcement learning algorithms can acquire policies for complex tasks autonomously. However, the number of samples required to learn a diverse set of skills can be prohibitively large. While meta-reinforcement learning methods have enabled agents to leverage prior experience to adapt quickly to new tasks, their performance depends crucially on how close the new task is to the previously experienced tasks. Current approaches are either not able to extrapolate well, or can do so at the expense of requiring extremely large amounts of data for on-policy meta-training. In this work, we present model identification and experience relabeling (MIER), a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. Our method is based on a simple insight: we recognize that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. These dynamics models can then be used to continue training policies and value functions for out-of-distribution tasks without using meta-reinforcement learning at all, by generating synthetic experience for the new task.","This paper proposes a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. The proposed method is based on the observation that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. The dynamics models are used to generate synthetic experience for the new task, which can then be used to continue training policies and values for out of distribution tasks. "
229,SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"Recent years have seen a surge of interest in meta-learning techniques for tackling the few-shot learning (FSL) problem. However, the meta-learner’s initial model is prone to meta-overfit, as there are only a few available samples with sampling noise. Besides, when handling the data sampled with label noise for FSL, meta-learner could be extremely sensitive to label noise. To address these two challenges that FSL with sampling and label noise. In particular, we first cast the meta-overfitting problem (overfitting on sampling and label noise) as a gradient noise problem since few available samples cause meta-learner to overfit on existing examples (clean or corrupted) of an individual task at every gradient step. We present Eigen-Reptile (ER) that updates the meta-parameters with the main direction of historical taskspecific parameters to alleviate gradient noise. Specifically, the main direction is computed by a special mechanism for the parameter’s large size. Furthermore, to obtain a more accurate main direction for Eigen-Reptile in the presence of label noise, we propose Introspective Self-paced Learning (ISPL) that constructs a plurality of prior models to determine which sample should be abandoned. We have proved the effectiveness of Eigen-Reptile and ISPL, respectively, theoretically and experimentally. Moreover, our experiments on different tasks demonstrate that the proposed methods outperform or achieve highly competitive performance compared with the state-of-the-art methods with or without noisy labels.","This paper studies the problem of meta-learning for few-shot learning (FSL) in the context of sampling noise and label noise. The paper proposes two methods to address these two challenges. First, the paper proposes Eigen-Reptile (ER) that updates the meta-parameters with the main direction of historical taskspecific parameters to alleviate gradient noise. Second, Introspective Self-paced Learning (ISPL) that constructs a plurality of prior models to determine which sample should be abandoned. Experiments on different tasks demonstrate that the proposed methods outperform or achieve highly competitive performance compared with the state-of-the-art methods."
230,SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"Adversarial training is the industry standard for producing models that are robust to small adversarial perturbations. However, machine learning practitioners need models that are robust to other kinds of changes that occur naturally, such as changes in the style or illumination of input images. Such changes in input distribution have been effectively modeled as shifts in the mean and variance of deep image features. We adapt adversarial training by adversarially perturbing these feature statistics, rather than image pixels, to produce models that are robust to distributional shifts. We also visualize images from adversarially crafted distributions. Our method, Adversarial Batch Normalization (AdvBN), significantly improves the performance of ResNet-50 on ImageNet-C (+8.1%), Stylized-ImageNet (+6.7%), and ImageNetInstagram (+3.9%) over standard training practices. In addition, we demonstrate that AdvBN can also improve generalization on semantic segmentation. ImageNet ImageNet-C Stylized-ImageNet ImageNet-AdvBN ImageNet-Instagram 89.6% goldfinch 7.4% sulphur butterfly 0.5% hummingbird 99.9% goldfinch 0.05% bulbul 0.02% house finch 57.4% goldfinch 11.8% brambling 8.8% guillotine 16.2% gong 8.8% bolete 4.5% fox squirrel 10.3% hen-of-the-woods 5.1% Ibizan hound 4.0% flamingo Figure 1: Images from ImageNet variants along with classification scores by a pre-trained ResNet-50 model. The right-most image is generated by our Adversarial Batch Normalization module. Details on how we generate this image can be found in Section 3.","This paper proposes a new adversarial training method for image classification models that is robust to distributional shifts. The proposed method is based on adversarial batch normalization (Adversarial Batch Normalization (AdvBN), which is a variant of batch normalisation (BNN) that is used to train deep neural networks. The authors show that AdvBN improves the performance of ResNet-50 on ImageNet-C, Stylized-ImageNet, and ImageNetInstagram over standard training practices. In addition, AdvBN can also improve generalization on semantic segmentation."
231,SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"In machine learning, a question of great interest is understanding what examples are challenging for a model to classify. Identifying atypical examples helps inform safe deployment of models, isolates examples that require further human inspection, and provides interpretability into model behavior. In this work, we propose Variance of Gradients (VoG) as a valuable and efficient proxy metric for detecting outliers in the data distribution. We provide quantitative and qualitative support that VoG is a meaningful way to rank data by difficulty and to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. Data points with high VoG scores are far more difficult for the model to learn and over-index on corrupted or memorized examples.",This paper proposes Variance of Gradients (VoG) as a proxy metric for detecting outliers in the data distribution. The authors provide quantitative and qualitative support that VoG is a meaningful way to rank data by difficulty and to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. Data points with high VoG scores are far more difficult for the model to learn and over-index on corrupted or memorized examples.
232,SP:074bfacc75837bb19049be8a2890e10de073dd8e,"Deep generative modeling has seen impressive advances in recent years, to the point where it is now commonplace to see simulated samples (e.g., images) that closely resemble real-world data. However, generation quality is generally inconsistent for any given model and can vary dramatically between samples. We introduce Discriminator Gradient f low (DGf low), a new technique that improves generated samples via the gradient flow of entropy-regularized f -divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, our technique avoids wasteful sample rejection used by previous methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN variants, we show our refinement approach can be applied to GANs with vector-valued critics and even other deep generative models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate that DGf low leads to significant improvement in the quality of generated samples for a variety of generative models, outperforming the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling (DDLS) methods.","This paper proposes a new method for improving the quality of generated samples in deep generative models. The proposed method is based on the gradient flow of entropy-regularized f-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. Compared to existing works that focus on specific GAN variants, the proposed method can be applied to GANs with vector-valued critics and even other deep models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate the effectiveness of the method."
233,SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"Recent studies about learning multilingual representations have achieved significant performance gains across a wide range of downstream cross-lingual tasks. They train either an encoder-only Transformer mainly for understanding tasks, or an encoder-decoder Transformer specifically for generation tasks, ignoring the correlation between the two tasks and frameworks. In contrast, this paper presents a variable encoder-decoder (VECO) pre-training approach to unify the two mainstreams in both model architectures and pre-training tasks. VECO splits the standard Transformer block into several sub-modules trained with both innersequence and cross-sequence masked language modeling, and correspondingly reorganizes certain sub-modules for understanding and generation tasks during inference. Such a workflow not only ensures to train the most streamlined parameters necessary for two kinds of tasks, but also enables them to boost each other via sharing common sub-modules. As a result, VECO delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark covering text classification, sequence labeling, question answering, and sentence retrieval. For generation tasks, VECO also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1∼2 BLEU.","This paper proposes a new pre-training method for cross-lingual understanding and generation tasks. The main idea is to split the standard Transformer block into several sub-modules trained with both innersequence and cross-sequence masked language modeling, and reorganize certain sub- modules for understanding and generating tasks during inference. The proposed method achieves state-of-the-art results on the XTREME benchmark covering text classification, sequence labeling, question answering, and sentence retrieval tasks."
234,SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"Humans integrate multiple sensory modalities (e.g., visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. We first conduct an in-depth analysis of our module using a set of Atari games. We then apply our model to audio-visual exploration using the Habitat simulator and active learning using the ThreeDWorld (TDW) simulator. Experimental results demonstrate the advantages of using audio signals over vision-based models as intrinsic rewards to guide RL explorations.","This paper proposes a novel intrinsic reward mechanism for reinforcement learning based on auditory event prediction. The authors propose to use K-means to discover underlying auditory event clusters and train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. The method is evaluated on Atari games, Habitat simulator, and TDW."
235,SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper studies the problem of novel category discovery on singleand multimodal data with labels from different but relevant categories. We present a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, we take inspiration from self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabelled data. In particular, we proposed using category discrimination on labelled data and cross-modal discrimination on multi-modal data to augment instance discrimination used in conventional contrastive learning approaches. We further employ Winner-Take-All (WTA) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabelled data to better predict cluster assignments. We thoroughly evaluate our framework on large-scale multi-modal video benchmarks Kinetics-400 and VGG-Sound, and image benchmarks CIFAR10, CIFAR100 and ImageNet, obtaining state-of-the-art results.","This paper proposes an end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, the authors take inspiration from self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabeled data. The proposed method is evaluated on large-scale multi-modal video benchmarks Kinetics-400 and VGG-Sound, and image benchmarks CIFAR10, Cifar100 and ImageNet."
236,SP:4df640f502e88ddba2d7e183625231d70b083e82,"Weakly supervised segmentation requires assigning a label to every pixel based on training instances with partial annotations such as image-level tags, object bounding boxes, labeled points and scribbles. This task is challenging, as coarse annotations (tags, boxes) lack precise pixel localization whereas sparse annotations (points, scribbles) lack broad region coverage. Existing methods tackle these two types of weak supervision differently: Class activation maps are used to localize coarse labels and iteratively refine the segmentation model, whereas conditional random fields are used to propagate sparse labels to the entire image. We formulate weakly supervised segmentation as a semi-supervised metric learning problem, where pixels of the same (different) semantics need to be mapped to the same (distinctive) features. We propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity. They act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images. We deliver a universal weakly supervised segmenter with significant gains on Pascal VOC and DensePose. Our code is publicly available at https://github.com/twke18/SPML.","This paper proposes a method for weakly supervised segmentation of images with partial annotations such as image-level tags, object bounding boxes, labeled points and scribbles. The authors propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity. The pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. Experiments on Pascal VOC and DensePose show that the proposed method outperforms existing methods."
237,SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"Recent advances in self-supervised learning have experienced remarkable progress, especially for contrastive learning based methods, which regard each image as well as its augmentations as an individual class and try to distinguish them from all other images. However, due to the large quantity of exemplars, this kind of pretext task intrinsically suffers from slow convergence and is hard for optimization. This is especially true for small scale models, which we find the performance drops dramatically comparing with its supervised counterpart. In this paper, we propose a simple but effective distillation strategy for unsupervised learning. The highlight is that the relationship among similar samples counts and can be seamlessly transferred to the student to boost the performance. Our method, termed as BINGO, which is short for Bag of InstaNces aGgregatiOn, targets at transferring the relationship learned by the teacher to the student. Here bag of instances indicates a set of similar samples constructed by the teacher and are grouped within a bag, and the goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. Notably, BINGO achieves new state-of-the-art performance on small scale models, i.e., 65.5% and 68.9% top-1 accuracies with linear evaluation on ImageNet, using ResNet-18 and ResNet-34 as backbone, respectively, surpassing baselines (52.5% and 57.4% top-1 accuracies) by a significant margin.",This paper proposes a distillation strategy for unsupervised self-supervised learning based on bag of instances (BINGO) which is short for Bag of InstaNces aGgregatiOn. BINGO aims at transferring the relationship learned by the teacher to the student. The goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. The proposed method achieves new state-of-the-art performance on small scale models with linear evaluation on ImageNet.
238,SP:328866aad6544c81ded8980934df31dc4472435f,"Simulation-based inference (SBI) refers to statistical inference on stochastic models for which we can generate samples, but not compute likelihoods. Like SBI algorithms, generative adversarial networks (GANs) do not require explicit likelihoods. We study the relationship between SBI and GANs, and introduce GATSBI, an adversarial approach to SBI. GATSBI reformulates the variational objective in an adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, works in high-dimensional posterior spaces and supports implicit priors. We evaluate GATSBI on two SBI benchmark problems and on two high-dimensional simulators. On a model for wave propagation on the surface of a shallow water body, we show that GATSBI can return well-calibrated posterior estimates even in high dimensions. On a model of camera optics, it infers a high-dimensional posterior given an implicit prior, and performs better than a state-of-the-art SBI approach. We also show how GATSBI can be extended to perform sequential posterior estimation to focus on individual observations. Overall, GATSBI opens up opportunities for leveraging advances in GANs to perform Bayesian inference on high-dimensional simulation-based models.","This paper proposes GATSBI, a generative adversarial approach to simulation-based inference (SBI) for high-dimensional simulators. The authors reformulate the variational objective in an adversarial setting to learn implicit posterior distributions, which is amortised across observations, and works in high dimensional posterior spaces and supports implicit priors. The proposed method is evaluated on two SBI benchmark problems and on two high dimensional simulators and shows that it can return well-calibrated posterior estimates even in high dimensions. On a model for wave propagation on the surface of a shallow water body, the authors show that the proposed method performs better than a state-of-the-art SBI approach."
239,SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"As an important problem in causal inference, we discuss the identification and estimation of treatment effects (TEs) under limited overlap; that is, when subjects with certain features belong to a single treatment group. We use a latent variable to model a prognostic score which is widely used in biostatistics and sufficient for TEs; i.e., we build a generative prognostic model. We prove that the latent variable recovers a prognostic score, and the model identifies individualized treatment effects. The model is then learned as β-Intact-VAE––a new type of variational autoencoder (VAE). We derive the TE error bounds that enable representations balanced for treatment groups conditioned on individualized features. The proposed method is compared with recent methods using (semi-)synthetic datasets.",This paper proposes a generative prognostic model for the identification and estimation of treatment effects (TEs) under limited overlap. The model uses a latent variable to model a prognostic score which is widely used in biostatistics and sufficient for TEs. The authors prove that the latent variable recovers a prognostic score and the model identifies individualized treatment effects. The proposed model is then learned as a new type of variational autoencoder (VAE) and the TE error bounds are derived that enable representations balanced for treatment groups conditioned on individualized features.
240,SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"Reinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual, non-episodic world, whereas common benchmark tasks in RL are episodic, with the environment resetting between trials to provide the agent with multiple attempts. This discrepancy presents a major challenge when attempting to take RL algorithms developed for episodic simulated environments and run them on real-world platforms, such as robots. In this paper, we aim to address this discrepancy by laying out a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. We introduce a simulated benchmark EARL1 around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy.","This paper proposes a framework for autonomous reinforcement learning (ARL) where the agent is not only learning through its own experience, but also contends with lack of human supervision to reset between trials. The authors introduce a simulated benchmark EARL1 around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. They show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy."
241,SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies have been conducted to attempt to equip QA systems with human-level reasoning capability. To approximate the complicated human reasoning process, state-of-the-art QA systems commonly use pre-trained language models (LMs) to access knowledge encoded in LMs together with elaborately designed modules based on Graph Neural Networks (GNNs) to perform reasoning over knowledge graphs (KGs). However, many problems remain open regarding the reasoning functionality of these GNN-based modules. Can these GNN-based modules really perform a complex reasoning process? Are they underor overcomplicated for QA? To open the black box of GNN and investigate these problems, we dissect state-of-the-art GNN modules for QA and analyze their reasoning capability. We discover that even a very simple graph neural counter can outperform all the existing GNN modules on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge-aware reasoning. Our work reveals that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting. It remains a challenging open problem to build comprehensive reasoning modules for knowledge-powered QA.",This paper investigates the reasoning capability of Graph Neural Networks (GNNs) in question answering systems. The authors propose a simple graph neural counter that can be used to perform reasoning over knowledge graphs. The paper shows that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting. It remains a challenging open problem to build comprehensive reasoning modules for knowledge-powered QA systems.
242,SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"Recent advances in Deep Neural Networks (DNN) compression (e.g. pruning, quantization and etc.) significantly reduces the amount of space consumption for storage, making them easier to deploy in low-cost devices. However, those techniques do not keep the compressed representation during inference runtime, which incurs significant overheads in terms of both performance and space consumption. We introduce “Succinct Compression”, a three-stage framework to enable DNN inference with near-optimal compression and much better performance during inference runtime. The key insight of our method leverages the concept of Succinct Data Structures, which supports fast queries directly on compressed representation without decompression. Our method first transforms DNN models as our proposed formulations in either Element-wise or Block-wise manner, so that Succinct Data Structures can take advantage of. Then, our method compresses transformed DNN models using Succinct Data Structures. Finally, our method exploits our specialized execution pipelines for different model formulations, to retrieve relevant data for DNN inference. Our experimental results show that, our method keeps near-optimal compression, and achieves at least 8.7X/11.5X speedup on AlexNet/VGG-16 inference, compared with Huffman Coding. We also experimentally show that our method is quite synergistic with Pruning and Quantization.","This paper proposes a three-stage compression method for Deep Neural Network (DNN) models. The first step is to transform DNN models in Element-wise or Block-wise manner, and then compresses transformed models using Succinct Data Structures. The second stage is to retrieve relevant data for DNN inference from the compressed representation without decompression. Finally, the third stage is the execution pipelines for different model formulations. The experimental results show that, the proposed method keeps near-optimal compression and achieves at least 8.7X/11.5X speedup on AlexNet/VGG-16 inference, compared with Huffman Coding."
243,SP:94c395afc794a9cc163e362078769ff83f3d20d0,"We introduce Network Augmentation (NetAug), a new training method for improving the performance of tiny neural networks. Existing regularization techniques (e.g., data augmentation, dropout) have shown much success on large neural networks (e.g., ResNet50) by adding noise to overcome over-fitting. However, we found these techniques hurt the performance of tiny neural networks. We argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model. At test time, only the tiny model is used for inference, incurring zero inference overhead. We demonstrate the effectiveness of NetAug on image classification and object detection. NetAug consistently improves the performance of tiny models, achieving up to 2.3% accuracy improvement on ImageNet, and 4.3% on Cars. On Pascal VOC, NetAug provides 2.96% mAP improvement with the same computational cost.","This paper proposes a new training method for improving the performance of tiny neural networks. The authors argue that training tiny models is different from training large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model. The experimental results demonstrate the effectiveness of NetAug on image classification and object detection."
244,SP:9c24549b980e415616f818acbf4cf680ef8edb52,"Point cloud sequence is an important data representation that provides flexible shape and motion information. Prior work demonstrates that incorporating scene flow information into loss can make model learn temporally coherent feature spaces. However, it is prohibitively expensive to acquire point correspondence information across frames in real-world environments. In this work, we propose a super-resolution generative adversarial network (GAN) for dynamic point cloud sequences without requiring point correspondence annotation. Our model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, we propose a learnable masking module to adapt upsampling ratio according to the point distribution. We conduct extensive experiments on point cloud sequences from two different domains: particles in the fluid dynamical system and human action scanned data. The quantitative and qualitative evaluation demonstrates the effectiveness of our method on upsampling task as well as learning temporal coherence from irregular point cloud sequences.","This paper proposes a generative adversarial network (GAN) for dynamic point cloud sequences without requiring point correspondence annotation. The proposed model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, a learnable masking module is proposed to adapt upsampling ratio according to the point distribution. Experiments on two different domains: particles in the fluid dynamical system and human action scanned data demonstrate the effectiveness of the proposed method."
245,SP:67efe60ad37807505369b7852bc0abed29ffdda8,"Large-scale pre-training has proven to be effective for visual representation learning on downstream tasks, especially for improving robustness and generalization. However, the recently developed detection transformers only employ pre-training on its backbone while leaving the key component, i.e., a 12-layer transformer, being trained from scratch, which prevents the model from above benefits. This separated training paradigm is mainly caused by the discrepancy between the upstream and downstream tasks. To mitigate the issue, we propose FP-DETR, a new method that Fully Pre-Trains an encoder-only transformer and smoothly finetunes it for object detection via a task adapter. Inspired by the success of textual prompts in NLP, we treat query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. To this end, we propose the task adapter which leverages self-attention to model the contextual relation between object query embedding. Experiments on the challenging COCO dataset demonstrate that our PT-DETR achieves competitive performance. Moreover, it enjoys better robustness to common corruptions and generalization to small-size datasets than state-of-the-art detection transformers. The source code will be made publicly available.","This paper proposes a method for fully pre-training an encoder-only transformer for object detection. The proposed method is inspired by the success of textual prompts in NLP, which treat query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. The task adapter leverages self-attention to model the contextual relation between object query embedding. Experiments on the challenging COCO dataset demonstrate that the proposed PT-DETR achieves competitive performance and generalization to small-size datasets."
246,SP:a1f9897496303984fc7ad469222106b14b4a6233,"Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017) is a classical federated learning algorithm in which clients run multiple local SGD steps before communicating their update to an orchestrating server. We propose a new federated learning algorithm, FedPAGE, able to further reduce the communication complexity by utilizing the recent optimal PAGE method (Li et al., 2021). We show that FedPAGE uses much fewer communication rounds than previous local methods for both federated convex and nonconvex optimization. Concretely, 1) in the convex setting, the number of communication rounds of FedPAGE is O( 3/4 S ), improving the best-known result O( N S ) of SCAFFOLD (Karimireddy et al., 2020) by a factor of N, where N is the total number of clients (usually is very large in federated learning), S is the sampled subset of clients in each communication round, and is the target error; 2) in the nonconvex setting, the number of communication rounds of FedPAGE isO( √ N+S S 2 ), improving the best-known result O( N 2/3 S2/3 2 ) of SCAFFOLD (Karimireddy et al., 2020) by a factor of NS, if the sampled clients S ≤ √ N. Note that in both settings, the communication cost for each round is the same for both FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new theoretical state-of-the-art results in terms of communication complexity for both federated convex and nonconvex optimization.","This paper proposes FedPAGE, a new local SGD algorithm for federated learning. The main idea is to use the optimal PAGE algorithm to reduce the communication cost of the local gradient descent algorithm. The authors show that the communication costs are much lower than SCAFFOLD in both convex and nonconvex settings. In the convex setting, the number of communication rounds is O(3/4 S), which is a factor of 3/4 of the best known result of Karimireddy et al. (2020). The authors also show that in the non-conveX case, the communication rounds are O(N^3/3 S^2/3) instead of O(NS/3/S^2) which is the state-of-the-art result."
247,SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"Artificial neural networks (ANNs) are constructed using well-understood mathematical operations, and yet their high-dimensional, non-linear, and compositional nature has hindered our ability to provide an intuitive description of how and why they produce any particular output. A striking example of this lack of understanding is our inability to design networks that are robust to adversarial input perturbations, which are often imperceptible to a human observer but cause significant undesirable changes in the network’s response. The primary contribution of this work is to further our understanding of the decision boundary geometry of ANN classifiers by utilizing such adversarial perturbations. For this purpose, we define adversarial subspaces, which are spanned by orthogonal directions of minimal perturbation to the decision boundary from any given input sample. We find that the decision boundary lies close to input samples in a large subspace, where the distance to the boundary grows smoothly and sub-linearly as one increases the dimensionality of the subspace. We undertake analysis to characterize the geometry of the boundary, which is more curved within the adversarial subspace than within a random subspace of equal dimensionality. To date, the most widely used defense against test-time adversarial attacks is adversarial training, where one incorporates adversarial attacks into the training procedure. Using our analysis, we provide new insight into the consequences of adversarial training by quantifying the increase in boundary distance within adversarial subspaces, the redistribution of proximal class labels, and the decrease in boundary curvature.","This paper studies the decision boundary geometry of ANN classifiers in the presence of adversarial attacks. The authors define adversarial subspaces, which are spanned by orthogonal directions of minimal perturbation to the decision boundaries from any given input sample. The decision boundary lies close to input samples in a large subspace, where the distance to the boundary grows smoothly and sub-linearly as one increases the dimensionality of the subspace. The geometry of the boundary is more curved within the adversarial space than within a random subspace of equal dimensionality."
248,SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"We argue that a form of the valuable information provided by the auxiliary information is its implied data clustering information. For instance, considering hashtags as auxiliary information, we can hypothesize that an Instagram image will be semantically more similar with the same hashtags. With this intuition, we present a two-stage weakly-supervised contrastive learning approach. The first stage is to cluster data according to its auxiliary information. The second stage is to learn similar representations within the same cluster and dissimilar representations for data from different clusters. Our empirical experiments suggest the following three contributions. First, compared to conventional self-supervised representations, the auxiliary-information-infused representations bring the performance closer to the supervised representations, which use direct downstream labels as supervision signals. Second, our approach performs the best in most cases, when comparing our approach with other baseline representation learning methods that also leverage auxiliary data information. Third, we show that our approach also works well with unsupervised constructed clusters (e.g., no auxiliary information), resulting in a strong unsupervised representation learning approach.","This paper proposes a two-stage weakly-supervised contrastive learning approach for representation learning. The first stage is to cluster data according to its auxiliary information, and the second stage aims to learn similar representations within the same cluster and dissimilar representations for data from different clusters. The proposed method is evaluated on synthetic and real-world datasets, and compared with several baseline representation learning methods that do not leverage auxiliary data information."
249,SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"Recovering sparse parameters from observational data is a fundamental problem in machine learning with wide applications. Many classic algorithms can solve this problem with theoretical guarantees, but the theoretical performances depend on choosing the correct hyperparameters. Besides, they do not fully exploit the particular problem distribution of interest. In this work, we propose PLISA (Provable Learning-based Iterative Sparse recovery Algorithm) to learn algorithms automatically from data. PLISA is designed by unrolling a classic path-following algorithm, with some components being more flexible and learnable. With this structure, we theoretically show the improved recovery accuracy achievable by PLISA. Furthermore, we analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set. This paper contains novel theoretical contributions to the area of learning-based algorithms in the sense that (i) PLISA is generically applicable to a broad class of sparse estimation problems, (ii) generalization analysis has received less attention so far, and (iii) our analysis makes novel connections between the generalization ability and algorithmic properties such as stability and convergence of the unrolled algorithm, which leads to a tighter bound that can explain the empirical observations. The techniques could potentially be applied to analyze other learning-based algorithms in the literature.",This paper proposes a new algorithm for sparse estimation based on unrolling a path-following algorithm. The authors provide a theoretical analysis of the performance of the proposed algorithm and show that it is able to recover sparse parameters from observational data. They also provide a generalization bound for the algorithm based on the Rademacher complexity analysis. 
250,SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL is to convert the hybrid action space into a unified homogeneous action space by discretization or continualization, so that conventional RL algorithms can be applied. However, this ignores the underlying structure of hybrid action space and also induces the scalability issue and additional approximation difficulties, thus leading to degenerated results. In this paper, we propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variational Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. We evaluate HyAR in a variety of environments with discrete-continuous action space. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces.","This paper proposes a method to learn a compact and decodable latent representation space for the discrete-continuous hybrid action space. The method is based on a VAE-based approach to embedding the discrete action and continuous parameter into an embedding table and conditional Variational Auto-Encoder (VAE). The action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space and using it to learn the policy. The results demonstrate the superiority of HyAR when compared with previous baselines."
251,SP:5128bf712f6b197de113c7a371b4bec36f978eca,"In this paper, we propose SGEM, Stochastic Gradient with Energy and Momentum to solve a large class of general non-convex stochastic optimization problems, based on the AEGD method that originated in the work [AEGD: Adaptive Gradient Descent with Energy. arXiv: 2010.05109]. SGEM incorporates both energy and momentum at the same time so as to inherit their dual advantages. We show that SGEM features an unconditional energy stability property, and derive energydependent convergence rates in the general nonconvex stochastic setting, as well as a regret bound in the online convex setting. A lower threshold for the energy variable is also provided. Our experimental results show that SGEM converges faster than AEGD and generalizes better or at least as well as SGDM in training some deep neural networks.","This paper proposes SGEM, Stochastic Gradient Descent with Energy and Momentum (SGEM) to solve a large class of general non-convex stochastic optimization problems. SGEM incorporates both energy and momentum at the same time so as to inherit their dual advantages. The authors show that SGEM features an unconditional energy stability property, and derive energy-dependent convergence rates in the general nonconveX setting and a regret bound in the online convex setting. The experimental results show that the proposed SGEM converges faster than AEGD and generalizes better than SGDM in training some deep neural networks."
252,SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"Transformer-based autoregressive (AR) machine translation models have achieved significant performance improvements, nearing human-level accuracy on some languages. The AR framework translates one token at a time which can be time consuming, especially for long sequences. To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper we investigate possible reasons behind this performance gap, namely, the indistinguishability of tokens, and mismatch between training and inference. We then propose the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Empirically, we show that CMLMC achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets, a first for NAR models. Full code will be released at the time of publication.","This paper proposes a Conditional Masked Language Model with Correction (CMLMC) for non-autoregressive (NAR) machine translation models. NAR models have achieved state-of-the-art results on several machine translation datasets, but they still lag behind their autoregressive counterparts. This paper investigates possible reasons behind this performance gap, namely, the indistinguishability of tokens, and mismatch between training and inference. To address these problems, CMLMC proposes to use a conditional masked language model with correction. Empirically, the paper shows that the proposed method can outperform existing NAR-based models when trained on raw data without distillation."
253,SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"Ultra-low power local signal processing is a crucial aspect for edge applications on always-on devices. Neuromorphic processors emulating spiking neural networks show great computational power while fulfilling the limited power budget as needed in this domain. In this work we propose spiking neural dynamics as a natural alternative to dilated temporal convolutions. We extend this idea to WaveSense, a spiking neural network inspired by the WaveNet architecture. WaveSense uses simple neural dynamics, fixed time-constants and a simple feed-forward architecture and hence is particularly well suited for a neuromorphic implementation. We test the capabilities of this model on several datasets for keyword-spotting. The results show that the proposed network beats the state of the art of other spiking neural networks and reaches near state-of-the-art performance of artificial neural networks such as CNNs and LSTMs.","This paper proposes WaveSense, a spiking neural network inspired by the WaveNet architecture. WaveSense uses simple neural dynamics, fixed time-constants and a simple feed-forward architecture and hence is particularly well suited for a neuromorphic implementation. The authors test the capabilities of this model on several datasets for keyword-spotting. The results show that the proposed network beats the state of the art of other neural networks and reaches near state-of-the-art performance."
254,SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"Recent studies found that using machine learning for social applications can lead to injustice in the form of racist, sexist, and otherwise unfair and discriminatory outcomes. To address this challenge, recent machine learning algorithms have been designed to limit the likelihood such unfair behavior occurs. However, these approaches typically assume the data used for training is representative of what will be encountered in deployment, which is often untrue. In particular, if certain subgroups of the population become more or less probable in deployment (a phenomenon we call demographic shift), prior work’s fairness assurances are often invalid. In this paper, we consider the impact of demographic shift and present a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift. Shifty, the first technique of its kind, demonstrates an effective strategy for designing algorithms to overcome demographic shift’s challenges. We evaluate Shifty using a real-world dataset of university entrance exams and subsequent student success. We show that the learned models avoid bias under demographic shift, unlike existing methods. Our experiments demonstrate that our algorithm’s high-confidence fairness guarantees are valid in practice and that our algorithm is an effective tool for training models that are fair when demographic shift occurs.","This paper proposes a new algorithm, called Shifty, that is designed to address the problem of demographic shift in social applications. The algorithm is based on the idea that the data used for training is not representative of what will be encountered in deployment, which is known as demographic shift. The authors show that their algorithm is able to provide high-confidence behavioral guarantees that hold under demographic shift, and they evaluate their algorithm on a real-world dataset of university entrance exams and show that the learned models avoid bias."
255,SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"Stochastic dual dynamic programming (SDDP) is a state-of-the-art method for solving multi-stage stochastic optimization, widely used for modeling real-world process optimization tasks. Unfortunately, SDDP has a worst-case complexity that scales exponentially in the number of decision variables, which severely limits applicability to only low dimensional problems. To overcome this limitation, we extend SDDP by introducing a trainable neural model that learns to map problem instances to a piece-wise linear value function within intrinsic low-dimension space, which is architected specifically to interact with a base SDDP solver, so that can accelerate optimization performance on new instances. The proposed Neural Stochastic Dual Dynamic Programming (ν-SDDP) continually self-improves by solving successive problems. An empirical investigation demonstrates that ν-SDDP can significantly reduce problem solving cost without sacrificing solution quality over competitors such as SDDP and reinforcement learning algorithms, across a range of synthetic and real-world process optimization problems.","This paper proposes a neural model for solving multi-stage stochastic dual dynamic programming (SDDP) problems. The main idea is to train a neural network to map problem instances to a piece-wise linear value function within intrinsic low-dimension space, which is architected specifically to interact with a base SDDP solver, so that it can accelerate optimization performance on new instances. The proposed Neural Stochastic Dual Dynamic Programming (ν-SDDP), continually self-improves by solving successive problems. Experiments are conducted on synthetic and real-world process optimization problems."
256,SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"Recent data-extraction attacks have exposed that language models can memorize some training samples verbatim. This is a vulnerability that can compromise the privacy of the model’s training data. In this work, we introduce SUBMIX: a practical protocol for private next-token prediction designed to prevent privacy violations by language models that were fine-tuned on a private corpus after pre-training on a public corpus. We show that SUBMIX limits the leakage of information that is unique to any individual user in the private corpus via a relaxation of group differentially private prediction. Importantly, SUBMIX admits a tight, data-dependent privacy accounting mechanism, which allows it to thwart existing data-extraction attacks while maintaining the utility of the language model. SUBMIX is the first protocol that maintains privacy even when publicly releasing tens of thousands of next-token predictions made by large transformer-based models such as GPT-2.","This paper proposes a privacy-preserving next-token prediction protocol for language models that were fine-tuned on a private corpus after pre-training on a public corpus. The authors show that the proposed method, called SUBMIX, limits the leakage of information that is unique to any individual user in the private corpus via a relaxation of group differentially private prediction. SubMIX admits a tight, data-dependent privacy accounting mechanism, which allows it to thwart existing data-extraction attacks while maintaining the utility of the language model."
257,SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,"Detecting out-of-distribution (OOD) examples is critical in many applications. We propose an unsupervised method to detect OOD samples using a k-NN density estimate with respect to a classification model’s intermediate activations on indistribution samples. We leverage a recent insight about label smoothing, which we call the Label Smoothed Embedding Hypothesis, and show that one of the implications is that the k-NN density estimator performs better as an OOD detection method both theoretically and empirically when the model is trained with label smoothing. Finally, we show that our proposal outperforms many OOD baselines and we also provide new finite-sample high-probability statistical results for k-NN density estimation’s ability to detect OOD examples.","This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using a k-NN density estimate with respect to a classification model’s intermediate activations on indistribution samples. The authors leverage a recent insight about label smoothing, which they call the Label Smoothed Embedding Hypothesis, and show that one of the implications is that the k-nn density estimator performs better as an OOD detection method both theoretically and empirically when the model is trained with label smoothed. Finally, the authors show that their proposal outperforms many OOD baselines and provide new finite-sample high-probability statistical results."
258,SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"Diffusion-based methods represented as stochastic differential equations on a continuous time domain have recently proven successful as a non-adversarial generative model. Training such models relies on denoising score matching, which can be seen as multi-scale denoising autoencoders. Here, we augment the denoising score matching framework to enable representation learning without any supervised signal. GANs and VAEs learn representations by directly transforming latent codes to data samples. In contrast, the introduced diffusion-based representation learning relies on a new formulation of the denoising score matching objective and thus encodes information needed for denoising. We illustrate how this difference allows for manual control of the level of details encoded in the representation. Using the same approach, we propose to learn an infinite-dimensional latent code which achieves improvements of state-of-the-art models on semi-supervised image classification. As a side contribution, we show how adversarial training in diffusionbased models can improve sample quality and improve sampling speed using a new approximation of the prior at smaller noise scales.","This paper proposes a new representation learning method for semi-supervised image classification. The proposed method is based on the idea of denoising autoencoders, which can be seen as a multi-scale autoencoder. In contrast, the proposed method uses a new formulation of the denoizing score matching objective and thus encodes information needed for denoisation. The authors demonstrate how this difference allows for manual control of the level of details encoded in the representation. In addition, the authors show how adversarial training in diffusion-based models can improve sample quality and improve sampling speed using a new approximation of the prior."
259,SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"Goal-conditioned reinforcement learning (RL) can solve tasks in a wide range of domains, including navigation and manipulation, but learning to reach distant goals remains a central challenge to the field. Learning to reach such goals is particularly hard without any offline data, expert demonstrations, and reward shaping. In this paper, we propose an algorithm to solve the distant goal-reaching task by using planning at training time to automatically generate a curriculum of intermediate states. Our algorithm, Classifier-Planning (C-Planning), frames the learning of the goal-conditioned policies as expectation maximization: the E-step corresponds to planning a sequence of waypoints using graph planning, while the M-step aims to learn a goal-conditioned policy to reach those waypoints. Unlike prior methods that combine goal-conditioned RL with graph search, ours performs planning only during training and not testing, significantly decreasing the compute costs of deploying the learned policy. Empirically, we demonstrate that our method is more sample efficient that prior methods. Moreover, it is able to solve very long horizons manipulation and navigation tasks, tasks that prior goalconditioned methods and methods based on graph search fail to solve.","This paper proposes a method for goal-conditioned reinforcement learning that learns a curriculum of intermediate states for reaching distant goals. The method is based on the idea of planning a sequence of waypoints using graph planning, while the M-step aims to learn a policy to reach those waypoints. Unlike prior methods that combine goal-conditional RL with graph search, the proposed method performs planning only during training and not testing, significantly decreasing the compute costs of deploying the learned policy. Empirically, the method is able to solve very long horizons manipulation and navigation tasks, which prior goalconditioned methods such as graph search fail to solve."
260,SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"Mixup is a popular regularization technique for training deep neural networks that can improve generalization and increase adversarial robustness. It perturbs input training data in the direction of other randomly-chosen instances in the training set. To better leverage the structure of the data, we extend mixup to k-mixup by perturbing k-batches of training points in the direction of other k-batches using displacement interpolation, i.e. interpolation under the Wasserstein metric. We demonstrate theoretically and in simulations that k-mixup preserves cluster and manifold structures, and we extend theory studying the efficacy of standard mixup to the k-mixup case. Our empirical results show that training with k-mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities. Standard mixup (Zhang et al., 2018) is a data augmentation approach that trains models on weighted averages of random pairs of training points. Averaging weights are typically drawn from a beta distribution β(α, α), with parameter α such that the generated training set is vicinal, i.e., it does not stray too far from the original dataset. Perturbations generated by mixup may be in the direction of any other datapoint instead of being informed by local distributional structure. As shown in Figure 1, this is a key weakness of mixup that can lead to poor regularization when distributions are clustered or supported on an embedded manifold. With larger α, the procedure can result in averaged training points with incorrect labels in other clusters or in locations that stray far from the data manifold. } {{ } No mixup } {{ } (1-)mixup } {{ } 4-mixup } {{ } 32-mixup Figure 1: We train a fully-connected network on three synthetic datasets for binary classification, with 1-mixup through 32-mixup regularization (α = 1). The plotted functions show the network output and demonstrate that higher k-mixup better captures local structure (visible through less blur, increased contrast) while retaining reasonable, even smoothing between the classes. To address these issues, we present k-mixup, which averages random pairs of sets of k samples from the training dataset. This averaging is done using optimal transport, with displacement interpolation. The sets of k samples are viewed as discrete distributions and are averaged as distributions in a geometric sense. If k = 1, we recover standard mixup regularization. Figures 1","This paper proposes a new regularization method for training deep neural networks called k-mixup, which is a variant of standard mixup (Zhang et al., 2018). Standard mixup is a data augmentation approach that trains models on weighted averages of random pairs of training points. The authors argue that the original mixup can lead to poor regularization when distributions are clustered or supported on an embedded manifold. To address this issue, the authors propose to use displacement interpolation, i.e. interpolation under the Wasserstein metric, to perturb k-batches of training data in the direction of other k-batch instances in the training set. They demonstrate theoretically and in simulations that k-Mixup preserves cluster and manifold structures, and extend theory studying the efficacy of mixup to the k- mixup case. The empirical results show that training with k- Mixup further improves generalization and robustness across several network architectures and benchmark datasets of various modalities."
261,SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,"We investigate the possibility of using the embeddings produced by a lightweight network more effectively with a nonlinear classification layer. Although conventional deep networks use an abundance of nonlinearity for representation (embedding) learning, they almost universally use a linear classifier on the learned embeddings. This could be suboptimal for a network with a limited-capacity backbone since better nonlinear classifiers could exist in the same embedding vector space. We advocate a nonlinear kernelized classification layer for deep networks to tackle this problem. We theoretically show that our classification layer optimizes over all possible radial kernel functions on the space of embeddings to learn an optimal nonlinear classifier. We then demonstrate the usefulness of this layer in learning more model-efficient classifiers in a number of computer vision and natural language processing tasks.",This paper proposes a nonlinear kernelized classification layer for deep networks to tackle the problem of using the embeddings produced by a lightweight network more effectively with a non-linear classification layer. The authors theoretically show that their classification layer optimizes over all possible radial kernel functions on the space of embedding vector space to learn an optimal nonlinear classifier. They then demonstrate the usefulness of this layer in learning more model-efficient classifiers in a number of computer vision and natural language processing tasks.
262,SP:01ee8ec81619784788eb0ce9785098e437d17a7c,"Node representation learning has demonstrated its efficacy for various applications on graphs, which leads to increasing attention towards the area. However, fairness is a largely under-explored territory within the field, which may lead to biased results towards underrepresented groups in ensuing tasks. To this end, this work theoretically explains the sources of bias in node representations obtained via Graph Neural Networks (GNNs). Our analysis reveals that both nodal features and graph structure lead to bias in the obtained representations. Building upon the analysis, fairness-aware data augmentation frameworks on nodal features and graph structure are developed to reduce the intrinsic bias. Our analysis and proposed schemes can be readily employed to enhance the fairness of various GNN-based learning mechanisms. Extensive experiments on node classification and link prediction are carried out over real networks in the context of graph contrastive learning. Comparison with multiple benchmarks demonstrates that the proposed augmentation strategies can improve fairness in terms of statistical parity and equal opportunity, while providing comparable utility to state-of-the-art contrastive methods.","This paper analyzes the sources of bias in node representations obtained via Graph Neural Networks (GNNs) in terms of both nodal features and graph structure. Based on this analysis, two data augmentation methods are proposed to reduce the intrinsic bias in the obtained representations. Extensive experiments on node classification and link prediction are carried out over real networks in the context of graph contrastive learning. The proposed augmentation strategies can improve the statistical parity and equal opportunity, while providing comparable utility to state-of-the-art contrastive methods."
263,SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"This paper considers the challenge of estimating treatment effects from observational data in the presence of unmeasured confounders. A popular way to address this challenge is to utilize an instrumental variable (IV) for two-stage regression, i.e., 2SLS and variants, but they need to assume the additive separability of noise and are limited to the linear setting. Recently, many nonlinear IV regression variants were proposed by regressing the treatment with IVs and confounders in the first stage, leading to confounding bias between the predicted treatment and outcome in the second stage. In this paper, we propose a Confounder Balanced IV Regression (CB-IV) algorithm to jointly remove the bias from the unmeasured confounders with IV regression and achieve better bias-variance trade-off in imbalanced treatment distributions due to the observed confounders by balancing for treatment effect estimation. Specifically, CB-IV algorithm consists of three main modules: (1) treatment regression: regressing the treatment with IVs and confounders like previous nonlinear IV methods for removing the confounding from unmeasured confounders; (2) confounder balancing: learning a balanced representation of confounders to eliminate the bias induced by the observed confounders (3) outcome regression: regressing the outcome with the predicted treatment and the balanced confounders representation for treatment effect estimation. To the best of our knowledge, this is the first work to combine confounder balancing in IV regression for treatment effect estimation. Moreover, we theoretically prove that CB-IV algorithm is also effective under the multiplicative assumption rather than the additive separability assumption. Extensive experiments demonstrate that CB-IV algorithm outperforms the state-of-the-art methods, including IV regression and confounder balancing methods, for treatment effect estimation.","This paper proposes a new method for estimating treatment effects from observational data in the presence of unmeasured confounders. The proposed method consists of three main modules: treatment regression, confounder balancing, and outcome regression. The treatment regression consists of regressing the treatment with IVs and confoundering in the first stage, and then learning a balanced representation of the confounded variables in the second stage. Theoretical analysis is provided to show that the proposed method is also effective under the multiplicative assumption rather than the additive separability assumption. Extensive experiments demonstrate that CB-IV algorithm outperforms the state-of-the-art methods."
264,SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"Model-Agnostic Meta-Learning (MAML) has become increasingly popular for training models that can quickly adapt to new tasks via one or few stochastic gradient descent steps. However, the MAML objective is significantly more difficult to optimize compared to standard non-adaptive learning (NAL), and little is understood about how much MAML improves over NAL in terms of the fast adaptability of their solutions in various scenarios. We analytically address this issue in a linear regression setting consisting of a mixture of easy and hard tasks, where hardness is related to the rate that gradient descent converges on the task. Specifically, we prove that in order for MAML to achieve substantial gain over NAL, (i) there must be some discrepancy in hardness among the tasks, and (ii) the optimal solutions of the hard tasks must be closely packed with the center far from the center of the easy tasks optimal solutions. We also give numerical and analytical results suggesting that these insights apply to two-layer neural networks. Finally, we provide few-shot image classification experiments that support our insights for when MAML should be used and emphasize the importance of training MAML on hard tasks in practice.","This paper analyzes model-agnostic meta-learning (MAML) in a linear regression setting with a mixture of easy and hard tasks, where hardness is related to the rate that gradient descent converges on the task. The authors prove that in order for MAML to achieve substantial gain over NAL, (i) there must be some discrepancy in hardness among the tasks, and (ii) the optimal solutions of the hard tasks must be closely packed with the center far from the center of the easy tasks optimal solutions. They also give numerical and analytical results suggesting that these insights apply to two-layer neural networks. Finally, they provide few-shot image classification experiments that support their insights for when MAMM should be used and emphasize the importance of training MAMl on hard tasks in practice."
265,SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"Sparse Blind Source Separation (BSS) has become a well established tool for a wide range of applications – for instance, in astrophysics and remote sensing. Classical sparse BSS methods, such as the Proximal Alternating Linearized Minimization (PALM) algorithm, nevertheless often suffer from a difficult hyperparameter choice, which undermines their results. To bypass this pitfall, we propose in this work to build on the thriving field of algorithm unfolding/unrolling. Unrolling PALM enables to leverage the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both PALM hyperparameters and variables. In contrast to most existing unrolled algorithms, which assume a fixed known dictionary during the training and testing phases, this article further emphasizes on the ability to deal with variable mixing matrices (a.k.a. dictionaries). The proposed Learned PALM (LPALM) algorithm thus enables to perform semi-blind source separation, which is key to increase the generalization of the learnt model in real-world applications. We illustrate the relevance of LPALM in astrophysical multispectral imaging: the algorithm not only needs up to 10−10 times fewer iterations than PALM, but also improves the separation quality, while avoiding the cumbersome hyperparameter and initialization choice of PALM. We further show that LPALM outperforms other unrolled source separation methods in the semi-blind setting.","This paper proposes an unrolled version of the Proximal Alternating Linearized Minimization (PALM) algorithm for sparse source separation (BSS). The proposed method leverages the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both PALM hyperparameters and variables. The paper further emphasizes on the ability to deal with variable mixing matrices (a.k.a. dictionaries) which enables to perform semi-blind source separation, which is key to increase the generalization of the learnt model in real-world applications. The proposed LPALM outperforms other unrolled source separation methods in the semi blind setting."
266,SP:7716315001949ab88c8a216302fe51bae872fc87,"Recent studies have demonstrated that the performance of transformers on the task of language modeling obeys a power-law relationship with model size over six orders of magnitude. While transformers exhibit impressive scaling, their performance hinges on processing large amounts of data, and their computational and memory requirements grow quadratically with sequence length. Motivated by these considerations, we introduce a novel attention module called implicit self-attention and construct a Legendre Memory Unit based model hat exhibits an O(n) and O(n lnn) (or better) dependency for memory and computation respectively. Over three orders of magnitude, we show that for the same amount of training our model improves the loss over transformers about as much as transformers improve over LSTMs. Additionally, we demonstrate that adding global self-attention complements our architecture and the augmented model improves performance even further.","This paper proposes a novel attention module called implicit self-attention for transformers. The authors argue that transformers exhibit impressive scaling, but their performance hinges on processing large amounts of data, and their computational and memory requirements grow quadratically with sequence length. Motivated by these considerations, the authors propose a Legendre Memory Unit based model that exhibits an O(n) and O(N lnn) dependency for memory and computation respectively. They show that for the same amount of training their model improves the loss over transformers about as much as transformers improve over LSTMs. They also demonstrate that adding global self attention complements their architecture and the augmented model improves performance even further."
267,SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"Generative adversarial networks (GANs) have attained photo-realistic quality in image generation. However, how to best control the image content remains an open challenge. We introduce LatentKeypointGAN, a two-stage GAN which is trained end-to-end on the classical GAN objective with internal conditioning on a set of space keypoints. These keypoints have associated appearance embeddings that respectively control the position and style of the generated objects and their parts. A major difficulty that we address with suitable network architectures and training schemes is disentangling the image into spatial and appearance factors without domain knowledge and supervision signals. We demonstrate that LatentKeypointGAN provides an interpretable latent space that can be used to re-arrange the generated images by re-positioning and exchanging keypoint embeddings, such as generating portraits by combining the eyes, and mouth from different images. In addition, the explicit generation of keypoints and matching images enables a new, GAN-based method for unsupervised keypoint detection.","This paper proposes LatentKeypointGAN, a two-stage GAN that generates keypoint embeddings for image generation. The keypoints are generated using a GAN-based encoder-decoder architecture, where the encoder encodes the image as a set of keypoints, and the decoder decodes the appearance embedding of each keypoint, which is used to control the position and style of the generated objects and their parts. The paper demonstrates that the generated keypoints can be used to re-arrange the generated images by re-positioning and exchanging keypoints embedding, such as generating portraits by combining the eyes, and mouth from different images. In addition, the paper proposes a new method for unsupervised keypoint detection."
268,SP:9206ae6e31077569313838504ef6daa89ad3b59c,"We study deep fully-connected neural networks with layer normalization using the mean field formalism, and carry out a non-perturbative analysis of signal propagation. As a result, we demonstrate that increasing the depth leads to gradient explosion or to another undesirable phenomenon we call representation shrinkage. The appearance of at least one of these problems is not restricted to a specific initialization scheme or a choice of activation function, but rather is an inherent property of the fully-connected architecture itself. Additionally, we show that many popular normalization techniques fail to mitigate these problems. Our method can also be applied to residual networks to guide the choice of initialization variances.","This paper studies the problem of representation shrinkage in fully-connected neural networks with layer normalization using the mean field formalism. The authors show that increasing the depth of the network can lead to gradient explosion or representation shrinkages. They also show that the appearance of these problems is not restricted to a specific initialization scheme or a choice of activation function, but rather is an inherent property of the fully-connections architecture itself. They show that many popular normalization techniques fail to mitigate these problems, and propose a method to guide the choice of initialization variances."
269,SP:2177be818b5843c580c787f1b2d725154846feb6,"A fundamental challenge in Deep Learning is to find optimal step sizes for stochastic gradient descent automatically. In traditional optimization, line searches are a commonly used method to determine step sizes. One problem in Deep Learning is that finding appropriate step sizes on the full-batch loss is unfeasibly expensive. Therefore, classical line search approaches, designed for losses without inherent noise, are usually not applicable. Recent empirical findings suggest that the full-batch loss behaves locally parabolically in the direction of noisy update step directions. Furthermore, the trend of the optimal update step size changes slowly. By exploiting these findings, this work introduces a line-search method that approximates the full-batch loss with a parabola estimated over several mini-batches. Learning rates are derived from such parabolas during training. In the experiments conducted, our approach mostly outperforms SGD tuned with a piece-wise constant learning rate schedule and other line search approaches for Deep Learning across models, datasets, and batch sizes on validation and test accuracy.","This paper proposes a line search method for finding the optimal step size for SGD. The main idea is to approximate the full-batch loss with a parabola estimated over several mini-batches. The learning rates are derived from such parabolas during training. The experiments show that the proposed method outperforms SGD tuned with a piece-wise constant learning rate schedule and other line search approaches for Deep Learning across models, datasets, and batch sizes."
270,SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"Noise-contrastive estimation (NCE) is a statistically consistent method for learning unnormalized probabilistic models. It has been empirically observed that the choice of the noise distribution is crucial for NCE’s performance. However, such observations have never been made formal or quantitative. In fact, it is not even clear whether the difficulties arising from a poorly chosen noise distribution are statistical or algorithmic in nature. In this work, we formally pinpoint reasons for NCE’s poor performance when an inappropriate noise distribution is used. Namely, we prove these challenges arise due to an ill-behaved (more precisely, flat) loss landscape. To address this, we introduce a variant of NCE called eNCE which uses an exponential loss and for which normalized gradient descent addresses the landscape issues provably when the target and noise distributions are in a given exponential family.","This paper studies the problem of noise-contrastive estimation (NCE) in the context of unnormalized probabilistic models. In particular, the authors propose a new method called eNCE, which uses an exponential loss and a normalized gradient descent algorithm to address the landscape issues of the original NCE. The authors prove that the landscape of the loss landscape is flat when the target and noise distributions are in a given exponential family. The paper is well-written and well-motivated."
271,SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"Privacy and Byzantine resilience (BR) are two crucial requirements of modern-day distributed machine learning. The two concepts have been extensively studied individually but the question of how to combine them effectively remains unanswered. This paper contributes to addressing this question by studying the extent to which the distributed SGD algorithm, in the standard parameter-server architecture, can learn an accurate model despite (a) a fraction of the workers being malicious (Byzantine), and (b) the other fraction, whilst being honest, providing noisy information to the server to ensure differential privacy (DP). We first observe that the integration of standard practices in DP and BR is not straightforward. In fact, we show that many existing results on the convergence of distributed SGD under Byzantine faults, especially those relying on (α, f)-Byzantine resilience, are rendered invalid when honest workers enforce DP. To circumvent this shortcoming, we revisit the theory of (α, f)-BR to obtain an approximate convergence guarantee. Our analysis provides key insights on how to improve this guarantee through hyperparameter optimization. Essentially, our theoretical and empirical results show that (1) an imprudent combination of standard approaches to DP and BR might be fruitless, but (2) by carefully re-tuning the learning algorithm, we can obtain reasonable learning accuracy while simultaneously guaranteeing DP and BR.","This paper studies the convergence of distributed SGD under Byzantine fault (DP) and Byzantine resilience (BR) in the context of parameter-server architecture. In particular, the authors consider the case where a fraction of the workers are malicious (Byzantine) and the other fraction are honest (DP). The authors show that the integration of standard practices in DP and BR is not straightforward, and show that many existing results on the convergence under Byzantine faults, especially those relying on Byzantine resilience, are rendered invalid when honest workers enforce DP. To circumvent this shortcoming, the paper revisits the theory of (alpha, f)-BR to obtain an approximate convergence guarantee. The paper also provides insights on how to improve this guarantee through hyperparameter optimization."
272,SP:bc783f0c829f90931535e63687d13172879631b3,"This paper considers the computer source code editing with few exemplars. The editing exemplar, containing the original and modified support code snippets, showcases a certain editorial pattern, and code editing adapts the common pattern derived from few support exemplars to a query code snippet. In this work, we propose a novel deep learning approach to solve this code editing problem automatically. Our learning approach combines edit representations extracted from support exemplars and compositionally generalizes them to the query code snippet editing via multi-extent similarities ensemble. Specifically, we parse the support and query code snippets using language-specific grammar into abstract syntax trees. We apply the similarities measurement in multiple extents from individual nodes to collective tree representations for query and support sample matching, and ensemble the matching results through a similarity-ranking error estimator. We evaluate the proposed method on C# and Python datasets, and show up to 8.6% absolute accuracy improvements compared to non-composition baselines.","This paper proposes a method for code editing with few exemplars. The method is based on the composition of the support and query code snippets into abstract syntax trees, which are then used to compose the query code snippet editing via multi-extent similarities ensemble. The authors evaluate the proposed method on C# and Python datasets and show up to 8.6% absolute accuracy improvements compared to non-composition baselines."
273,SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"There has been significant recent progress designing deep generative models that generate realistic sequence data such as text or music. Nevertheless, it remains difficult to incorporate high-level structure to guide the generative process, and many such models perform well on local coherence, but less so on global coherence. We propose a novel approach for incorporating global structure in the form of relational constraints between different subcomponents of an example (e.g., lines of a poem or measures of music). Our generative model has two parts: (i) one model to generate a realistic set of relational constraints, and (ii) a second model to generate realistic data satisfying these constraints. For model (i), we propose a program synthesis algorithm that infers the relational constraints present in the training data, and then learn a generative model based on the resulting constraint data. In our experiments, we show that our approach significantly improves over state-of-the-art in terms of capturing high-level structure in the data, while performing comparably or better in terms of low-level structure.","This paper proposes a generative model for generating high-level structure in a sequence of sequences. The authors propose to use relational constraints between different subcomponents of an example (e.g., lines of a poem or measures of music) to guide the generative process. The proposed model consists of two parts: (i) one model to generate a realistic set of relational constraints, and (ii) a second model that generates realistic data satisfying these constraints. For model (i), the authors propose a program synthesis algorithm that infers the relational constraints present in the training data, and then learn a model based on the resulting constraint data. The experimental results show that the proposed model significantly improves over state-of-the-art in terms of capturing high level structure in the data, while performing comparably or better in the terms of low level structure."
274,SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper aims for set-to-hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. We address two common scaling problems encountered in set-to-hypergraph tasks that limit the size of the input set: the exponentially growing number of hyperedges and the run-time complexity, both leading to higher memory requirements. We make three contributions. First, we propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. Second, we introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows us to skip iterations in the backward pass for improved efficiency and constant memory usage. Third, we combine both contributions in a single set-to-hypergraph model that enables us to address problems with larger input set sizes. We provide ablations for our main technical contributions and show that our model outperforms prior state-of-the-art, especially for larger sets.","This paper proposes a new method for set-to-hypergraph prediction. The main idea is to train a neural network to predict and supervise the positive edges of the hypergraph. The authors propose to change the asymptotic memory scaling from exponential to linear. They also introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows them to skip iterations in the backward pass for improved efficiency and constant memory usage. Finally, they combine both contributions in a single model that enables them to address problems with larger input set sizes."
275,SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"In spite of the high performance and reliability of deep learning algorithms in broad range everyday applications, many investigations tend to show that a lot of models exhibit biases, discriminating some subgroups of the population. This urges the practitioner to develop fair systems whose performances are uniform among individuals. In this work, we introduce a post-processing method designed to mitigate bias of state-of-the-art models. It consists in learning a shallow neural network, called the Ethical Module, which transforms the deep embeddings of a pre-trained model to give more representation power to the discriminated subgroups. Its training is supervised by the von Mises-Fisher loss, whose hyperparameters allow to control the space allocated to each subgroup in the latent space. Besides being very simple, the resulting methodology is more stable and faster than most current methods of bias mitigation. In order to illustrate our idea in a concrete use case, we focus here on gender bias in facial recognition and conduct extensive numerical experiments on standard datasets.","This paper proposes a post-processing method to mitigate bias of state-of-the-art models. It consists in learning a shallow neural network, called the Ethical Module, which transforms the deep embeddings of a pre-trained model to give more representation power to the discriminated subgroups. Its training is supervised by the von Mises-Fisher loss, whose hyperparameters allow to control the space allocated to each subgroup in the latent space. Besides being very simple, the resulting methodology is more stable and faster than most current methods of bias mitigation."
276,SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"Not forgetting knowledge about previous classes is one of the key challenges in class-incremental learning (CIL). A common technique to address this challenge is knowledge distillation (KD) that penalizes inconsistencies across models of subsequent phases. As old-class data is scarce, the KD loss mainly uses new class data. However, we empirically observe that this both harms learning of new classes and also underperforms to distil old class knowledge from the previous phase model. To address this issue, we propose to compute the KD loss using placebo data chosen from a free image stream (e.g., Google Images), which is both simple and surprisingly effective even when there is no class overlap between the placebos and the old data. When the image stream is available, we use an evaluation function to quickly judge the quality of candidate images (good or bad placebos) and collect good ones. For training this function, we sample pseudo CIL tasks from the data in the 0-th phase and design a reinforcement learning algorithm. Our method does not require any additional supervision or memory budget, and can significantly improve a number of top-performing CIL methods, in particular on higher-resolution benchmarks, e.g., ImageNet-1k and ImageNet-Subset, and with a lower memory budget for old class exemplars, e.g., five exemplars per class.","This paper proposes a new method for class-incremental learning (CIL) based on placebos. Specifically, the authors propose to use a placebos-based evaluation function to evaluate the quality of candidate images from a free image stream. The authors also propose a reinforcement learning algorithm to sample pseudo CIL tasks from the data in the 0-th phase and train the model on these pseudo tasks. The experimental results show that the proposed method can outperform existing KD-based methods on ImageNet-1k and ImageNet Subset. "
277,SP:506e0a888c03a955b708464eed3670c04baf4912,"Energy-based Models (EBMs) offer a powerful approach for modeling discrete structure, but both inference and learning of EBM are hard as it involves sampling from discrete distributions. Recent work shows Markov Chain Monte Carlo (MCMC) with the informed proposal is a powerful tool for such sampling. However, an informed proposal only allows local updates as it requires evaluating all energy changes in the neighborhood. In this work, we present a path auxiliary algorithm that uses a composition of local moves to efficiently explore large neighborhoods. We also give a fast version of our algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function. Empirically, we show that our path auxiliary algorithms considerably outperform other generic samplers on various discrete models for sampling, inference, and learning. Our method can also be used to train deep EBMs for high dimensional discrete data.","This paper proposes a path auxiliary algorithm for sampling from discrete energy-based models (EBMs). The main idea is to use a composition of local moves to efficiently explore large neighborhoods in EBMs. The authors also propose a fast version of their algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function. Empirically, they show that their path auxiliary algorithms considerably outperform other generic samplers on various discrete models for sampling, inference, and learning. "
278,SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"Discovery and learning of an underlying spatiotemporal hierarchy in sequential data is an important topic for machine learning. Despite this, little work has been done to explore hierarchical generative models that can flexibly adapt their layerwise representations in response to datasets with different temporal dynamics. Here, we present Variational Predictive Routing (VPR) – a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process. By employing an event detection mechanism that relies solely on the system’s latent representations (without the need of a separate model), VPR is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model’s latent hierarchy. Using several video datasets, we show that VPR is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate timeagnostic rollouts of the future. Our approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest.","This paper proposes Variational Predictive Routing (VPR), a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy based on their rates of change, thus modeling continuous data as a hierarchical renewal process. VPR is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate timeagnostic rollouts of the future. The proposed approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest."
279,SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,"Image retrieval is to search images similar to the given query image by extracting features. Previously, methods that firstly search by global features then re-rank images using local feature matching were proposed, which has an excellent performance on many datasets. However, their drawbacks are also obvious. For example, the local feature matching consumes time and space greatly, the re-ranking process weakens the influence of global features, and the local feature learning is not accurate enough and semantic enough because of the trivial design. In this work, we proposed a Unifying Global and Attention-based Local Features Retrieval method (referred to as UGALR), which is an end-to-end and single-stage pipeline. Particularly, UGALR benefits from two aspects: 1) it accelerates extraction speed and reduces memory consumption by removing the re-ranking process and learning local feature matching with convolutional neural networks instead of RANSAC algorithm; 2) it learns more accurate and semantic local information through combining spatial and channel attention with the aid of intermediate supervision. Experiments on Revisited Oxford and Paris datasets validate the effectiveness of our approach, and we achieved state-of-the-art performance compared to other popular methods. The codes will be available soon.","This paper proposes a new method for image retrieval that combines global and local features. The proposed method is an end-to-end and single-stage pipeline, which is based on convolutional neural networks (CNNs) and spatial and channel attention. The method is evaluated on the Revisited Oxford and Paris datasets and achieves state-of-the-art performance."
280,SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"Multitask learning is being increasingly adopted in applications domains like computer vision and reinforcement learning. However, optimally exploiting its advantages remains a major challenge due to the effect of negative transfer. Previous works have tracked down this issue to the disparities in gradient magnitudes and directions across tasks when optimizing the shared network parameters. While recent work has acknowledged that negative transfer is a two-fold problem, existing approaches fall short. These methods only focus on either homogenizing the gradient magnitude across tasks; or greedily change the gradient directions, overlooking future conflicts. In this work, we introduce RotoGrad, an algorithm that tackles negative transfer as a whole: it jointly homogenizes gradient magnitudes and directions, while ensuring training convergence. We show that RotoGrad outperforms competing methods in complex problems, including multi-label classification in CelebA and computer vision tasks in the NYUv2 dataset. A Pytorch implementation can be found in https://github.com/adrianjav/rotograd.","This paper proposes RotoGrad, a multi-task learning algorithm that jointly homogenizes gradient magnitudes and directions across tasks while ensuring training convergence. The main contribution of this paper is to address the problem of negative transfer, which is the issue of gradient disparities in gradient magnitude and directions when optimizing the shared network parameters. The authors propose a new algorithm that aims to solve this problem jointly by homogenizing gradient magnitude and directions. The proposed algorithm is evaluated on a variety of multi-label classification and computer vision tasks in the NYUv2 dataset. The experimental results show that the proposed algorithm outperforms competing methods."
281,SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,"Layer-wise model fusion via optimal transport, named OTFusion, applies soft neuron association for unifying different pre-trained networks to save computational resources. While enjoying its success, OTFusion requires the input networks to have the same number of layers. To address this issue, we propose a novel model fusion framework, named CLAFusion, to fuse neural networks with a different number of layers, which we refer to as heterogeneous neural networks, via cross-layer alignment. The cross-layer alignment problem, which is an unbalanced assignment problem, can be solved efficiently using dynamic programming. Based on the cross-layer alignment, our framework balances the number of layers of neural networks before applying layer-wise model fusion. Our synthetic experiments indicate that the fused network from CLAFusion achieves a more favorable performance compared to the individual networks trained on heterogeneous data without the need for any retraining. With an extra finetuning process, it improves the accuracy of residual networks on the CIFAR10 dataset. Finally, we explore its application for model compression and knowledge distillation when applying to the teacher-student setting.","This paper proposes a method to fuse heterogeneous neural networks via cross-layer alignment. The proposed method is based on the idea of model fusion via optimal transport (OTFusion), which uses soft neuron association to unify different pre-trained networks to save computational resources. However, OTFusion requires the input networks to have the same number of layers. To address this issue, the authors propose a novel model fusion framework, named CLAFusion, to fuse neural networks with a different number of hidden layers. The authors propose to use dynamic programming to balance the number of neural networks before applying layer-wise model fusion. Experiments on CIFAR-10 show that the proposed method achieves a more favorable performance compared to the individual networks trained on heterogeneous data without the need for any retraining."
282,SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"Despite overparameterization, deep networks trained via supervised learning are surprisingly easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive “aliasing”, in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images."," in this paper is a theoretical analysis of the effect of implicit regularization in deep reinforcement learning (RL) methods. Specifically, the authors show that the implicit regularizer of SGD is harmful in the offline RL setting, leading to poor generalization and degenerate feature representations. To address this issue, they derive the form of this implicit regularized regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implied regularizer. Experiments show that DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games and D4RL domains."
283,SP:6fd793b27123bf80504e2ad5957455b7ec311612,"Randomized least-square value iteration (RLSVI) is a provably efficient exploration method. However, it is limited to the case where (1) a good feature is known in advance and (2) this feature is fixed during the training. If otherwise, RLSVI suffers an unbearable computational burden to obtain the posterior samples. In this work, we present a practical algorithm named HyperDQN to address the above issues under deep RL. In addition to a non-linear neural network (i.e., base model) that predicts Q-values, our method employs a probabilistic hypermodel (i.e., meta model), which outputs the parameter of the base model. When both models are jointly optimized under a specifically designed objective, three purposes can be achieved. First, the hypermodel can generate approximate posterior samples regarding the parameter of the Q-value function. As a result, diverse Q-value functions are sampled to select exploratory action sequences. This retains the punchline of RLSVI for efficient exploration. Second, a good feature is learned to approximate Q-value functions. This addresses limitation (1). Third, the posterior samples of the Q-value function can be obtained in a more efficient way than the existing methods, and the changing feature does not affect the efficiency. This deals with limitation (2). On the Atari suite, HyperDQN with 20M frames outperforms DQN with 200M frames in terms of the maximum human-normalized score. For SuperMarioBros, HyperDQN outperforms several exploration bonus and randomized exploration methods on 5 out of 9 games.","This paper proposes HyperDQN, an exploration method for deep reinforcement learning (RL) based on Randomized least-square value iteration (RLSVI) with a probabilistic hypermodel (i.e., meta-model) that outputs the parameter of the base model. The hypermodel can generate approximate posterior samples regarding the parameters of the Q-value function, which can be used to select exploration sequences. The proposed method is evaluated on the Atari suite and SuperMarioBros, where it outperforms several exploration bonus methods."
284,SP:b428383660928374c953f659ea1e05852dbdcd6e,"In many real-world scenarios, such as image classification and recommender systems, it is evidence that representation learning can improve model’s performance over multiple downstream tasks. Existing learning approaches rely on establishing the correlation (or its proxy) between features and the downstream task (labels), which typically results in a representation containing cause, effect and spurious correlated variables of the label. Its generalizability may deteriorate because of the unstability of the non-causal parts. In this paper, we propose to learn causal representation from observational data by regularizing the learning procedure with mutual information measures according to our hypothetical causal graph. The optimization involves a counterfactual loss, based on which we deduce a theoretical guarantee that the causality-inspired learning is with reduced sample complexity and better generalization ability. Extensive experiments show that the models trained on causal representations learned by our approach is robust under adversarial attacks and distribution shift.","This paper proposes to learn causal representation from observational data by regularizing the learning procedure with mutual information measures according to a hypothetical causal graph. The authors propose a counterfactual loss, based on which they deduce a theoretical guarantee that the causality-inspired learning is with reduced sample complexity and better generalization ability. Extensive experiments show that the models trained on causal representations learned by the proposed method is robust under adversarial attacks and distribution shift."
285,SP:1258c05a80a17949b50e6dae13deea1d2235f456,"Federated learning is a powerful distributed learning scheme that allows numerous edge devices to collaboratively train a model without sharing their data. However, training is resource-intensive for edge devices, and limited network bandwidth is often the main bottleneck. Prior work often overcomes the constraints by condensing the models or messages into compact formats, e.g., by gradient compression or distillation. In contrast, we propose ProgFed, the first progressive training framework for efficient and effective federated learning. It inherently reduces computation and two-way communication costs while maintaining the strong performance of the final models. We theoretically prove that ProgFed converges at the same asymptotic rate as standard training on full models. Extensive results on a broad range of architectures, including CNNs (VGG, ResNet, ConvNets) and U-nets, and diverse tasks from simple classification to medical image segmentation show that our highly effective training approach saves up to 20% computation and up to 63% communication costs for converged models. As our approach is also complimentary to prior work on compression, we can achieve a wide range of trade-offs, showing reduced communication of up to 50× at only 0.1% loss in utility.","This paper proposes ProgFed, a progressive training framework for federated learning. It is based on gradient compression and distillation. The authors provide theoretical analysis of the convergence rate of the proposed method and show that it converges at the same asymptotic rate as standard training on full models. Experiments are conducted on a variety of architectures, including CNNs, Resnets, ConvNets, and U-nets, and diverse tasks from simple classification to medical image segmentation."
286,SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"Deep neural networks are vulnerable to adversarial attacks. Adversarial training is one of the most effective algorithms to increase the model’s robustness. However, the trained models cannot generalize well to the adversarial examples on the test set. In this paper, we study the generalization of adversarial training through the lens of adversarial Rademacher complexity. Current analysis of adversarial Rademacher complexity is up to two-layer neural networks. In adversarial settings, one major difficulty of generalizing these results to deep neural networks is that we cannot peel off the layer as the classical analysis for standard training. We provide a method to overcome this issue and provide upper bounds of adversarial Rademacher complexity of deep neural networks. Similar to the existing bounds of standard Rademacher complexity of neural nets, our bound also includes the product of weight norms. We provide experiments to show that the adversarially trained weight norms are larger than the standard trained weight norms, thus providing an explanation for the bad generalization performance of adversarial training.",This paper studies the generalization of adversarial training through the lens of the adversarial Rademacher complexity of deep neural networks. The main contribution of this paper is to provide an upper bound on the adversarially trained weight norms of deep networks. This upper bound is based on the standard analysis of the Rademan complexity of two-layer neural networks up to two layers. The upper bound also includes the product of weight norms. Experiments on MNIST and CIFAR-10 show that the proposed upper bound can explain the poor generalization performance of the trained models. 
287,SP:925d6bb051e9b384669fb695085b678c11f7c11a,"Estimation of (differential) entropy and the related mutual information has been pursued with significant efforts by the machine learning community. To address shortcomings in previously proposed estimators for differential entropy, here we introduce KNIFE, a fully parameterized, differentiable kernel-based estimator of differential entropy. The flexibility of our approach also allows us to construct KNIFE-based estimators for conditional (on either discrete or continuous variables) differential entropy, as well as mutual information. We empirically validate our method on high-dimensional synthetic data and further apply it to guide the training of neural networks for real-world tasks. Our experiments on a large variety of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning demonstrate the effectiveness of KNIFE-based estimation.","This paper proposes a differentiable kernel-based estimator for differential entropy. The proposed estimator is based on a kernel based estimator (KNIFE) that is parameterized by a kernel kernel. The authors show that the proposed method can be used to estimate conditional differential entropy and mutual information. Experiments are conducted on a variety of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning. "
288,SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"Soft-greedy operators, namely ε-greedy and softmax, remain a common choice to induce a basic level of exploration for action-value methods in reinforcement learning. These operators, however, have a few critical limitations. In this work, we investigate a simple soft-greedy operator, which we call resmax, that takes actions proportionally to their suboptimality gap: the residual to the estimated maximal value. It is simple to use and ensures coverage of the state-space like ε-greedy, but focuses exploration more on potentially promising actions like softmax. Further, it does not concentrate probability as quickly as softmax, and so better avoids overemphasizing sub-optimal actions that appear high-valued during learning. Additionally, we prove it is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a non-expansion (called mellowmax). We empirically validate that resmax is comparable to or outperforms ε-greedy and softmax across a variety of environments in tabular and deep RL.","This paper proposes a new soft-greedy operator, called resmax, that takes actions proportionally to their suboptimality gap: the residual to the estimated maximal value. It is simple to use and ensures coverage of the state-space, but focuses exploration more on potentially promising actions like softmax. It does not concentrate probability as quickly as softmax, and so better avoids overemphasizing sub-optimal actions that appear high-valued during learning. The authors prove that resmax is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a nonexpansion (called mellowmax)."
289,SP:792ae8808aa6902758146aef1548c975492b833c,"Owing much to the revolution of information technology, the recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats. However, in certain scenarios, people may not want their data being used for training commercial models and thus studied how to attack the learnability of deep learning models. Previous works on learnability attack only consider the goal of preventing unauthorized exploitation on the specific dataset but not the process of restoring the learnability for authorized cases. To tackle this issue, this paper introduces and investigates a new concept called “learnability lock” for controlling the model’s learnability on a specific dataset with a special key. In particular, we propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to slightly modify data samples so that they become “unlearnable” by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. We empirically demonstrate the success and practicability of our method on visual classification tasks.","This paper proposes a new method for controlling the model’s learnability on a specific dataset with a special key. In particular, the authors propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to slightly modify data samples so that they become “unlearnable” by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. The experimental results demonstrate the effectiveness of the proposed method on visual classification tasks."
290,SP:9af10703605e620e563241e2602a50b629f3d37a,"While Graph Neural Networks (GNNs) have recently become the de facto standard for modeling relational data, they impose a strong assumption on the availability of the node or edge features of the graph. In many real-world applications, however, features are only partially available; for example, in social networks, age and gender are available only for a small subset of users. We present a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which we call Feature Propagation. We experimentally show that the proposed approach outperforms previous methods on seven common node-classification benchmarks and can withstand surprisingly high rates of missing features: on average we observe only around 4% relative accuracy drop when 99% of the features are missing. Moreover, it takes only 10 seconds to run on a graph with ∼2.5M nodes and ∼123M edges on a single GPU."," for Graph Neural Networks (GNNs) that assume that node or edge features of the graph are available. In many real-world applications, however, features are only partially available. This paper proposes a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which is called Feature Propagation. The proposed method outperforms previous methods on seven common node-classification benchmarks and can withstand surprisingly high rates of missing features."
291,SP:cbaa3f1379fa99159899d79ccb479c0187403aca,Active learning is the process of training a model with limited labeled data by selecting a core subset of an unlabeled data pool to label. The large scale of data sets used in deep learning forces most sample selection strategies to employ efficient heuristics. This paper introduces an integer optimization problem for selecting a core set that minimizes the discrete Wasserstein distance from the unlabeled pool. We demonstrate that this problem can be tractably solved with a Generalized Benders Decomposition algorithm. Our strategy uses high-quality latent features that can be obtained by unsupervised learning on the unlabeled pool. Numerical results on several data sets show that our optimization approach is competitive with baselines and particularly outperforms them in the low budget regime where less than one percent of the data set is labeled.,"This paper studies the problem of active learning, which is the process of training a model with limited labeled data by selecting a core subset of an unlabeled data pool to label. The core set is selected by minimizing the discrete Wasserstein distance between the labeled data set and the unlabelled data set. The paper proposes a Generalized Benders Decomposition algorithm to solve this problem. The proposed method is based on unsupervised learning of latent features that can be obtained from unlabeled data. The experimental results show that the proposed method outperforms baselines in the low budget regime."
292,SP:4c72923f78ca6590dc11e10d1a2403076a583718,"A quest to determine the human DNA sequence from telomere to telomere started three decades ago and was finally finished in 2021. This accomplishment was a result of a tremendous effort of numerous experts with an abundance of data, various tools, and often included manual inspection during genome reconstruction. Therefore, such method could hardly be used as a general approach to assembling genomes, especially when the assembly speed is important. Motivated by this achievement and aspiring to make it more accessible, we investigate a previously untaken path of applying geometric deep learning to the central part of the genome assembly—untangling a large assembly graph from which a genomic sequence needs to be reconstructed. A graph convolutional network is trained on a dataset generated from human genomic data to reconstruct the genome by finding a path through the assembly graph. We show that our model can compute scores from the lengths of the overlaps between the sequences and the graph topology which, when traversed with a greedy search algorithm, outperforms the greedy search over the overlap lengths only. Moreover, our method reconstructs the correct path through the graph in the fraction of time required for the state-of-the-art de novo assemblers. This favourable result paves the way for the development of powerful graph machine learning algorithms that can solve the de novo genome assembly problem much quicker and possibly more accurately than human handcrafted techniques.",This paper proposes a method for de-novo genome assembly based on graph convolutional neural networks. The method is based on the idea of using a graph neural network to reconstruct the genome by finding a path through the assembly graph. The authors show that their method is able to find the correct path in the fraction of time required for the state-of-the-art de novo genome assemblers. The paper is well-written and well-motivated.
293,SP:24de906e4289c9073b6c55c747b0913b8df5e053,"Continual learning often suffers from catastrophic forgetting. Recently, metacontinual learning algorithms use meta-learning to learn how to continually learn. A recent state-of-the-art is online aware meta-learning (OML) Javed & White (2019). This can be further improved by incorporating experience replay (ER) into its meta-testing. However, the use of ER only in meta-testing but not in metatraining suggests that the model may not be optimally meta-trained. In this paper, we remove this inconsistency in the use of ER and improve continual learning representations by integrating ER also into meta-training. We propose to store the samples’ representations, instead of the samples themselves, into the replay buffer. This ensures the batch nature of ER does not conflict with the online-aware nature of OML. Moreover, we introduce a meta-learned Predictive Sample Selection to replace the widely used reservoir sampling to populate the replay buffer. This allows the most significant samples to be stored, rather than relying on randomness. Experimental results on a number of real-world meta-continual learning benchmark data sets demonstrate that the proposed method outperforms the state-of-the-art. Moreover, the learned representations have better clustering structures and are more discriminative.","This paper proposes a meta-learning method for continual learning. The main idea is to use experience replay (ER) in meta-testing and meta-training to improve continual learning representations. The authors propose to store the samples’ representations, instead of the samples themselves, into the replay buffer. This ensures the batch nature of ER does not conflict with the online-aware nature of OML. Moreover, the authors introduce Predictive Sample Selection to replace the widely used reservoir sampling to populate the replay buffers. Experimental results on a number of real-world meta-continual learning benchmark data sets demonstrate that the proposed method outperforms the state-of-the-art."
294,SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"Multi-agent joint Q-learning based on Centralized Training with Decentralized Execution (CTDE) has become an effective technique for multi-agent cooperation. During centralized training, these methods are essentially addressing the multi-agent credit assignment problem. However, most of the existing methods implicitly learn the credit assignment just by ensuring that the joint Q-value satisfies the Bellman optimality equation. In contrast, we formulate an explicit credit assignment problem where each agent gives its suggestion about how to weight individual Q-values to explicitly maximize the joint Q-value, besides guaranteeing the Bellman optimality of the joint Q-value. In this way, we can conduct credit assignment among multiple agents and along the time horizon. Theoretically, we give a gradient ascent solution for this problem. Empirically, we instantiate the core idea with deep neural networks and propose Explicit Credit Assignment joint Q-learning (ECAQ) to facilitate multi-agent cooperation in complex problems. Extensive experiments justify that ECAQ achieves interpretable credit assignment and superior performance compared to several advanced baselines.","This paper proposes a method for multi-agent joint Q-learning based on centralized training with decentralized execution (CTDE). The authors formulate an explicit credit assignment problem where each agent gives its suggestion about how to weight individual Q-values to explicitly maximize the joint Q value. Theoretically, the authors give a gradient ascent solution for this problem. Empirically, they instantiate the core idea with deep neural networks and propose Explicit Credit Assignment Joint Q-Learning (ECAQ) to facilitate multi agent cooperation in complex problems. Extensive experiments justify that ECAQ achieves interpretable credit assignment and superior performance compared to several advanced baselines."
295,SP:0d2b225ac697679d10df25f371b2a718d4949b42,"There has been an emerging interest in using transductive learning for adversarial robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020; Wang et al., ArXiv 2021). Compared to traditional defenses, these defense mechanisms “dynamically learn” the model based on test-time input; and theoretically, attacking these defenses reduces to solving a bilevel optimization problem, which poses difficulty in crafting adaptive attacks. In this paper, we examine these defense mechanisms from a principled threat analysis perspective. We formulate and analyze threat models for transductive-learning based defenses, and point out important subtleties. We propose the principle of attacking model space for solving bilevel attack objectives, and present Greedy Model Space Attack (GMSA), an attack framework that can serve as a new baseline for evaluating transductivelearning based defenses. Through systematic evaluation, we show that GMSA, even with weak instantiations, can break previous transductive-learning based defenses, which were resilient to previous attacks, such as AutoAttack. On the positive side, we report a somewhat surprising empirical result of “transductive adversarial training”: Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks we consider.","This paper studies adversarial robustness of transductive-learning-based defenses. The authors propose a new attack framework called Greedy Model Space Attack (GMSA) that can be used as a baseline to evaluate the effectiveness of adversarial defenses. GMSA is based on the idea of attacking model space for solving bilevel attack objectives, and the authors show that GMSA can break previous defenses, which were resilient to previous attacks, such as AutoAttack. On the other hand, the authors report a somewhat surprising empirical result of “transductive adversarial training”: Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks."
296,SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"While batch normalization has been successful in speeding up the training of neural networks, it is not well understood. We cast batch normalization as an approximation of the limiting case where the entire dataset is normalized jointly, and explore other ways to approximate the gradient from this limiting case. We demonstrate an approximation that removes the need to keep more than one example in memory at any given time, at the cost of a small factor increase in the training step computation, as well as a fully per-example training procedure, which removes the extra computation at the cost of a small drop in the final model accuracy. We further use our insights to improve batch renormalization for very small minibatches. Unlike previously proposed methods, our normalization does not change the function class of the inference model, and performs well in the absence of identity shortcuts.","This paper studies the problem of batch normalization, which is a popular technique for speeding up the training of neural networks. In particular, the authors consider the case where the entire dataset is normalized jointly, and explore other ways to approximate the gradient from this limiting case. The authors propose two methods: (1) an approximation that removes the need to keep more than one example in memory at any given time, at the cost of a small factor increase in the training step computation, and (2) a fully per-example training procedure which removes the extra computation at a small drop in the final model accuracy. They further use their insights to improve batch renormalization."
297,SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and no additional inference latency. We also provide in the appendix an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA.","This paper proposes a method for reducing the number of trainable parameters in language model fine-tuning. The main idea is to freeze the pretrained model weights and inject the trainable rank decomposition matrices into each layer of the Transformer architecture. The authors compare the proposed method with Adam and show that it can reduce the total number of parameters by 10,000 times and the GPU memory requirement by 3 times. They also provide an empirical investigation into rank-deficiency for language model adaptation."
298,SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"A major challenge in structured prediction is to represent the interdependencies within output structures. When outputs are structured as sequences, linear-chain conditional random fields (CRFs) are a widely used model class which can learn local dependencies in the output. However, the CRF’s Markov assumption makes it impossible for CRFs to represent distributions with nonlocal dependencies, and standard CRFs are unable to respect nonlocal constraints of the data (such as global arity constraints on output labels). We present a generalization of CRFs that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language L. The resulting regular-constrained CRF (RegCCRF) has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in L. Notably, RegCCRFs can incorporate their constraints during training, while related models only enforce constraints during decoding. We prove that constrained training is never worse than constrained decoding, and show empirically that it can be substantially better in practice. Finally, we demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF into a deep neural model for semantic role labeling, exceeding state-of-the-art results on a standard dataset.","This paper proposes a regular-constrained linear-chain conditional random field (RegCCRF) that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language L. RegCCRF has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in L. The authors prove that constrained training is never worse than constrained decoding, and show empirically that it can be substantially better in practice. They demonstrate a practical benefit on downstream tasks such as semantic role labeling."
299,SP:74c186a96c12adff178264aa84ace8d04dc7d725,"Camera-based physiological measurement is a growing field with neural models providing state-the-art-performance. Prior research have explored various “endto-end” models; however these methods still require several preprocessing steps. These additional operations are often non-trivial to implement making replication and deployment difficult and can even have a higher computational budget than the “core” network itself. In this paper, we propose two novel and efficient neural models for camera-based physiological measurement called EfficientPhys that remove the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. Using an input of raw video frames, our models achieve state-of-the-art accuracy on three public datasets. We show that this is the case whether using a transformer or convolutional backbone. We further evaluate the latency of the proposed networks and show that our most light weight network also achieves a 33% improvement in efficiency.","This paper proposes two neural models for camera-based physiological measurement called EfficientPhys that remove the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. The proposed models achieve state-of-the-art accuracy on three public datasets. The authors also evaluate the latency of the proposed networks and show that their most light weight network also achieves a 33% improvement in efficiency."
300,SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"Structural pruning can simplify network architecture and improve inference speed. We propose Hardware-Aware Latency Pruning (HALP) that formulates structural pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget. For filter importance ranking, HALP leverages latency lookup table to track latency reduction potential and global saliency score to gauge accuracy drop. Both metrics can be evaluated very efficiently during pruning, allowing us to reformulate global structural pruning under a reward maximization problem given target constraint. This makes the problem solvable via our augmented knapsack solver, enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off. We examine HALP on both classification and detection tasks, over varying networks, on ImageNet and VOC datasets. In particular, for ResNet-50/-101 pruning on ImageNet, HALP improves network throughput by 1.60×/1.90× with +0.3%/−0.2% top-1 accuracy changes, respectively. For SSD pruning on VOC, HALP improves throughput by 1.94× with only a 0.56 mAP drop. HALP consistently outperforms prior art, sometimes by large margins.","This paper proposes Hardware-Aware Latency Pruning (HALP) that formulates structural pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget. The paper leverages latency lookup table to track latency reduction potential and global saliency score to gauge accuracy drop. This makes the problem solvable via an augmented knapsack solver, enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off."
301,SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"Although significant progress has been made in molecular graph generation recently, permutation invariance and multi-objective generation remain to be important but challenging goals to achieve. In this work, we propose GraphEBM, a molecular graph generation method via energy-based models (EBMs), as an exploratory work to perform permutation invariant and multi-objective molecule generation. Particularly, thanks to the flexibility of EBMs and our parameterized permutation-invariant energy function, our GraphEBM can define a permutation invariant distribution over molecular graphs. We learn the energy function by contrastive divergence and generate samples by Langevin dynamics. In addition, to generate molecules with a specific desirable property, we propose a simple yet effective learning strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules. Further, we explore to use our GraphEBM for generating molecules towards multiple objectives via compositional generation, which is practically desired in drug discovery. We conduct comprehensive experiments on random, single-objective, and multi-objective molecule generation tasks. The results demonstrate our method is effective.","This paper proposes GraphEBM, a molecular graph generation method via energy-based models (EBMs) to perform permutation-invariant and multi-objective molecule generation. The authors propose to learn the energy function by contrastive divergence and generate samples by Langevin dynamics. To generate molecules with a specific desirable property, the authors propose a simple yet effective learning strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules. The experimental results on random, single, and multi objective generation tasks demonstrate the effectiveness of the proposed method."
302,SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CROSSBEAM, uses the neural model to choose how to combine previouslyexplored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CROSSBEAM is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CROSSBEAM in two very different domains, string manipulation and logic programming. We observe that CROSSBEAM learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.","This paper proposes a method for bottom-up program synthesis, where the goal is to find programs that satisfy a given specification. The authors propose to use a neural model to guide the search policy, which is trained on a set of previously searched programs. The neural model is trained to learn how to combine previously explored programs into new programs, taking into account the search history and partial program executions. The proposed method is evaluated on string manipulation and logic programming tasks, where it is shown to outperform the state-of-the-art."
303,SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"Much of the recent successes in Deep Reinforcement Learning have been based on minimizing the squared Bellman error. However, training is often unstable due to fast-changing target Q-values, and target networks are employed to stabilize training by using an additional set of lagging parameters. Despite their advantages, target networks can inhibit the propagation of newly-encountered rewards which may ultimately slow down training. In this work, we address this issue by augmenting the squared Bellman error with a functional regularizer. Unlike target networks, the regularization we propose here is explicit and enables us to use up-to-date parameters as well as control the regularization. This leads to a faster yet more stable training method. Across a range of Atari environments, we demonstrate empirical improvements over target-network based methods in terms of both sample efficiency and performance. In summary, our approach provides a fast and stable alternative to replace the standard squared Bellman error.","This paper proposes to replace the standard squared Bellman error in deep reinforcement learning with a functional regularizer. The proposed method is based on the idea of target networks, where the target network is used to stabilize training by using an additional set of lagging parameters. The authors argue that target networks can inhibit the propagation of newly-encountered rewards which may ultimately slow down training. To address this issue, the authors propose to use an explicit regularizer that allows to use up-to-date parameters as well as control the regularization. This leads to a faster yet more stable training method. The experimental results show that the proposed method outperforms target-network based methods in terms of both sample efficiency and performance."
304,SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how message-passing GNNs can be designed to be more expressive than the Weisfeiler Lehman test. To elaborate this characterization, we propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We empirically verify the strength of our model on different graph learning tasks. It is shown that our model consistently improves the state-of-the-art methods on the benchmark tasks without sacrificing computational simplicity and efficiency.","This paper proposes a new approach to designing graph neural networks (GNNs) that is more expressive than the Weisfeiler Lehman test in distinguishing graph structures. Specifically, the authors propose a new hierarchy of local isomorphism on neighborhood subgraphs, which allows for a message-passing aggregation scheme of GNNs to be more expressive. The authors also propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive in distinguishing graphs. The experimental results on different graph learning tasks demonstrate the effectiveness of the proposed method."
305,SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, they usually underestimate uncertainties of out-of-distribution (OOD) samples leading to over-confident PIs. Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Additionally, PI3NN does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples. Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification.","This paper proposes a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, the authors usually underestimate uncertainties of out of distribution (OOD) samples leading to over-confident PI. The proposed PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss."
306,SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"While deep networks can learn complex functions such as classifiers, detectors, and trackers, many applications require models that continually adapt to changing input distributions, changing tasks, and changing environmental conditions. Indeed, this ability to continuously accrue knowledge and use past experience to learn new tasks quickly in continual settings is one of the key properties of an intelligent system. For complex and high-dimensional problems, simply updating the model continually with standard learning algorithms such as gradient descent may result in slow adaptation. Meta-learning can provide a powerful tool to accelerate adaptation yet is conventionally studied in batch settings. In this paper, we study how metalearning can be applied to tackle online problems of this nature, simultaneously adapting to changing tasks and input distributions and meta-training the model in order to adapt more quickly in the future. Extending meta-learning into the online setting presents its own challenges, and although several prior methods have studied related problems, they generally require a discrete notion of tasks, with known ground-truth task boundaries. Such methods typically adapt to each task in sequence, resetting the model between tasks, rather than adapting continuously across tasks. In many real-world settings, such discrete boundaries are unavailable, and may not even exist. To address these settings, we propose a Fully Online MetaLearning (FOML) algorithm, which does not require any ground truth knowledge about the task boundaries and stays fully online without resetting back to pretrained weights. Our experiments show that FOML was able to learn new tasks faster than the state-of-the-art online learning methods on Rainbow-MNIST, and CIFAR100 datasets.","This paper proposes a meta-learning method for online learning of deep neural networks. The main idea is to meta-train the model to adapt to changing tasks and input distributions and meta-training the model in order to adapt more quickly in the future. The authors propose a fully online meta learning (FOML) algorithm, which does not require any ground truth knowledge about the task boundaries and stays fully online without resetting back to pretrained weights. The experiments show that FOML was able to learn new tasks faster than the state-of-the-art online learning methods on Rainbow-MNIST, and CIFAR100 datasets."
307,SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"The structural design of functional molecules, also called molecular optimization, is an essential chemical science and engineering task with important applications, such as drug discovery. Deep generative models and combinatorial optimization methods achieve initial success but still struggle with directly modeling discrete chemical structures and often heavily rely on brute-force enumeration. The challenge comes from the discrete and non-differentiable nature of molecule structures. To address this, we propose differentiable scaffolding tree (DST) that utilizes a learned knowledge network to convert discrete chemical structures to locally differentiable ones. DST enables a gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network (GNN). Our empirical studies show the gradient-based molecular optimizations are both effective and sample efficient. Furthermore, the learned graph parameters can also provide an explanation that helps domain experts understand the model output.","This paper proposes a differentiable scaffolding tree (DST) method for molecular optimization. DST uses a learned knowledge network to convert discrete chemical structures to locally differentiable ones. In particular, DST enables a gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network (GNN). The experiments show that DST is both effective and sample efficient."
308,SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"Personalized medical systems are rapidly gaining traction as opposed to “one size fits all” systems. Predicting patients’ lab test responses and providing justification for the predictions would help clinicians tailor treatment regimes for patients. This requires modeling the complex interactions among different medications, diseases, and lab tests. We need to learn a strong patient representation that captures both the sequential information accumulated over the visits and information from other similar patients. We model drug-lab interactions and diagnosis-lab interactions as graphs and design a knowledge-augmented approach to predict patients’ response for a target lab result. We also take into consideration patients’ past lab responses to personalize the prediction. Experiments on real-world datasets demonstrate the effectiveness of the proposed solution in reducing prediction errors by a significant margin. Case studies show that the identified factors for influencing the predicted results are consistent with clinicians’ understanding.",This paper proposes a knowledge-augmented approach to predict the response of a patient to a target lab test result. The approach is based on the idea of learning a graph representation of the drug-lab interaction and the diagnosis-lab interactions as graphs. The proposed method is evaluated on two real-world datasets and compared with two baselines. The results show that the proposed method outperforms the baselines in terms of accuracy.
309,SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"Single domain generalization (SDG) is a challenging scenario of domain generalization, where only one source domain is available to train the model. Typical SDG methods are based on the adversarial data augmentation strategy, which complements the diversity of source domain to learn a robust model. Existing SDG methods require the source and target domains to have the same label space. However, as target domains may contain novel categories unseen in source label space, this assumption is not practical in many real-world applications. In this paper, we propose a challenging and untouched problem: Open-Set Single Domain Generalization (OS-SDG), where target domains include unseen categories out of source label space. The goal of OS-SDG is to learn a model, with only one source domain, to classify a target sample with correct class if it belongs to source label space, or assign it to unknown classes. We design a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. We also adopt a consistency regularization on generated auxiliary samples between multibinary classifiers and the model trained by SDG methods, to improve the model’s capability on unknown class identification. Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance of SDG methods in the OS-SDG setting.",This paper proposes an open-set single-domain generalization (OS-SDG) method for domain generalization. The authors propose a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. They also adopt a consistency regularization on generated auxiliary samples between multibinary classifiers and the model trained by SDG models. Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance.
310,SP:126f8ffb855aa22eda4d681a499953879ed3679e,"Trust-region methods based on Kullback-Leibler divergence are pervasively used to stabilize policy optimization in reinforcement learning. In this paper, we examine two natural extensions of policy optimization with Wasserstein and Sinkhorn trust regions, namely Wasserstein policy optimization (WPO) and Sinkhorn policy optimization (SPO). Instead of restricting the policy to a parametric distribution class, we directly optimize the policy distribution and derive their close-form policy updates based on the Lagrangian duality. Theoretically, we show that WPO guarantees a monotonic performance improvement, and SPO provably converges to WPO as the entropic regularizer diminishes. Experiments across tabular domains and robotic locomotion tasks further demonstrate the performance improvement of both approaches, more robustness of WPO to sample insufficiency, and faster convergence of SPO, over state-of-art policy gradient methods.","This paper proposes two extensions of trust-region methods based on Kullback-Leibler divergence for policy optimization in reinforcement learning, namely Wasserstein Policy Optimization (WPO) and Sinkhorn Policy Optimisation (SPO). The main idea is to directly optimize the policy distribution and derive their close-form policy updates based on the Lagrangian duality. Theoretically, they show that WPO guarantees a monotonic performance improvement, and SPO provably converges to WPO as the entropic regularizer diminishes. Experiments across tabular domains and robotic locomotion tasks demonstrate the performance improvement of both approaches."
311,SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"Forgetting is often seen as an unwanted characteristic in both human and machine learning. However, we propose that forgetting can in fact be favorable to learning. We introduce forget-and-relearn as a powerful paradigm for shaping the learning trajectories of artificial neural networks. In this process, the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The forget-and-relearn framework unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. We leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations. Insights from our analysis provide a coherent view on the dynamics of iterative training in neural networks and offer a clear path towards performance improvements.","This paper proposes a forget-and-relearn framework for improving the learning trajectory of neural networks. The idea is that the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The forgetting step is a combination of two steps: (1) removing undesirable information, and (2) re-training the model with the relearned features. The authors provide a unified analysis of existing iterative training algorithms in the image classification and language emergence literature, and show that the success of these algorithms in terms of the disproportionate forgetting of undesirable information. They leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations."
312,SP:2789859517b6624730b14a7e010444a72d3dd3ed,"Batch RL has seen a surge in popularity and is applicable in many practical scenarios where past data is available. Unfortunately, the performance of batch RL agents is limited in both theory and practice without strong assumptions on the data-collection process e.g. sufficient coverage or a good policy. To enable better performance, we investigate the offline-online setting: the agent has access to a batch of data to train on but is also allowed to learn during the evaluation phase in an online manner. This is an extension to batch RL, allowing the agent to adapt to new situations without having to precommit to a policy. In our experiments, we find that standard RL agents trained in an offline-online manner can outperform agents trained only offline or online, sometimes by a large margin, highlighting the potential of this new setting.","This paper proposes an offline-online setting where the agent has access to a batch of data to train on but is also allowed to learn during the evaluation phase in an online manner. This is an extension to batch RL, allowing the agent to adapt to new situations without having to precommit to a policy. The authors show that standard RL agents trained in an offline or online manner can outperform agents trained only offline or offline-only."
313,SP:76625a25e770415599a34122110d61cb3b7e614c,"This work attempts to tackle the problem of domain generalization (DG) via learning to reduce domain shift with an episodic training procedure. In particular, we measure the domain shift with Y-discrepancy and learn to optimize Y-discrepancy between the unseen target domain and source domains only using source-domain samples. Theoretically, we give a PAC-style generalization bound for discrepancyoptimal meta-learning and further make comparisons with other DG bounds including ERM and domain-invariant learning. The theoretical analyses show that there is a tradeoff between classification performance and computational complexity for discrepancy-optimal meta-learning. The theoretical results also shed light on a bilevel optimization algorithm for DG. Empirically, we evaluate the algorithm with DomainBed and achieves state-of-the-art results on two DG benchmarks.","This paper studies the problem of domain generalization (DG) by learning to reduce domain shift with an episodic training procedure. In particular, the authors propose to learn to optimize Y-discrepancy between the unseen target domain and source domains only using source-domain samples. Theoretically, they give a PAC-style generalization bound for discrepancy-optimal meta-learning and further make comparisons with other DG bounds including ERM and domain-invariant learning. Empirically, the algorithm with DomainBed achieves state-of-the-art results on two DG benchmarks. The theoretical results also shed light on a bilevel optimization algorithm for DG."
314,SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"Despite the tremendous success of applying traditional combinatorial search methods in various NP-complete domains such as SAT and CSP as well as using deep reinforcement learning to tackle two-player games such as Go, certain classes of PSPACE-hard planning problems have remained out of reach. Even carefully designed domain-specific solvers can fail quickly due to the exponential combinatorial search space on hard instances. Recent work that combines traditional search methods, such as best-first search and Monte Carlo tree search, with Deep Neural Networks’ (DNN) heuristic prediction has shown promising progress. These methods can solve a significant number of hard planning instances beyond specialized solvers. To better understand why these approaches work, we studied the interplay of the policy and value networks in DNN-based best-first search on the Sokoban domain and show the surprising effectiveness of the policy network, further enhanced by the value network, as a guiding heuristic for the search. To further understand the phenomena, we studied the cost distribution of the search algorithms and found that Sokoban planning instances can have heavy-tailed runtime distributions, with tails both on the left and right-hand sides. In particular, for the first time, we show the existence of left heavy tails and propose an abstract tree model that can empirically explain the appearance of these tails. We provide extensive experiment data supporting our model. The experiments show the critical role of the policy network as a powerful heuristic guiding the search, which can lead to left heavy tails with polynomial scaling by avoiding exploring exponentially sized sub-trees. Our results also demonstrate the importance of random restart strategies, as are widely used in traditional combinatorial solvers, for DNN-based search to avoid left and right heavy tails.","This paper studies the role of policy and value networks in best-first search for Sokoban planning problems in the context of deep reinforcement learning (DNN) based search algorithms. The authors propose an abstract tree model to explain the existence of left heavy tails and propose an algorithm to avoid exploring exponentially large sub-trees. The experiments show the importance of the policy network as a guiding heuristic guiding the search, which can lead to left heavy tail with polynomial scaling by avoiding exploring exponentially sized sub-Trees. "
315,SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"Meta-Imitation Learning is a promising technique for the robot to learn a new task from observing one or a few human demonstrations. However, it usually requires a significant number of demonstrations both from humans and robots during the meta-training phase, which is a laborious and hard work for data collection, especially in recording the actions and specifying the correspondence between human and robot. In this work, we present an approach of meta-imitation learning by watching video demonstrations from humans. In comparison to prior works, our approach is able to translate human videos into practical robot demonstrations and train the meta-policy with adaptive loss based on the quality of the translated data. Our approach relies only on human videos and does not require robot demonstration, which facilitates data collection and is more in line with human imitation behavior. Experiments reveal that our method achieves the comparable performance to the baseline on fast learning a set of vision-based tasks through watching a single video demonstration.","This paper proposes a method for meta-imitation learning from video demonstrations from humans. The method is based on the idea of meta-learning from human demonstrations, which is similar to human imitation learning. The main difference is that the method only relies on human videos and does not require robot demonstration, which facilitates data collection and is more in line with human imitation behavior. Experiments show that the proposed method achieves comparable performance to the baseline on fast learning a set of vision-based tasks through watching a single video demonstration."
316,SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"Over-parameterized deep networks trained using gradient-based optimizers is a popular way of solving classification and ranking problems. Without appropriately tuned regularization, such networks have the tendency to make output scores (logits) and network weights large, causing training loss to become too small and the network to lose its adaptivity (ability to move around and escape regions of poor generalization) in the weight space. Adaptive optimizers like Adam, being aggressive at optimizing the train loss, are particularly affected by this. It is well known that, even with weight decay (WD) and normal hyper-parameter tuning, adaptive optimizers lag behind SGD a lot in terms of generalization performance, mainly in the image classification domain.","This paper studies the problem of over-parameterized deep neural networks trained using gradient-based optimizers. The authors argue that, without appropriately tuned regularization, such networks have the tendency to make output scores (logits) and network weights large, causing training loss to become too small and the network to lose its adaptivity (ability to move around and escape regions of poor generalization) in the weight space. Adaptive optimizers like Adam, being aggressive at optimizing the train loss, are particularly affected by this."
317,SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"Group equivariant Convolutional Neural Networks (G-CNNs) constrain features to respect the chosen symmetries, and lead to better generalization when these symmetries appear in the data. However, if the chosen symmetries are not present, group equivariant architectures lead to overly constrained models and worse performance. Frequently, the distribution of the data can be better represented by a subset of a group than by the group as a whole, e.g., rotations in [−90○,90○]. In such cases, a model that respects equivariance partially is better suited to represent the data. Moreover, relevant symmetries may differ for low and high-level features, e.g., edge orientations in a face, and face poses relative to the camera. As a result, the optimal level of equivariance may differ per layer. In this work, we introduce Partial G-CNNs: a family of equivariant networks able to learn partial and full equivariances from data at every layer end-to-end. Partial G-CNNs retain full equivariance whenever beneficial, e.g., for rotated MNIST, but are able to restrict it whenever it becomes harmful, e.g., for 6 / 9 or natural image classification. Partial G-CNNs perform on par with G-CNNs when full equivariance is necessary, and outperform them otherwise. Our method is applicable to discrete groups, continuous groups and combinations thereof.","This paper proposes a method for group equivariant convolutional neural networks (G-CNNs) that is able to learn partial and full equivariances from data at every layer end-to-end. In particular, the authors propose a family of G-CNN models that can learn partial equivariance when necessary, and full-equivariance if not necessary. The method is evaluated on MNIST, 6/9 classification, and natural image classification. "
318,SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"Markov chain Monte Carlo (MCMC), such as Langevin dynamics, is valid for approximating intractable distributions. However, its usage is limited in the context of deep latent variable models since it is not scalable to data size owing to its datapoint-wise iterations and slow convergence. This paper proposes the amortized Langevin dynamics (ALD), wherein datapoint-wise MCMC iterations are entirely replaced with updates of an inference model that maps observations into latent variables. Since it no longer depends on datapoint-wise iterations, ALD enables scalable inference from large-scale datasets. Despite its efficiency, it retains the excellent property of MCMC; we prove that ALD has the target posterior as a stationary distribution under some assumptions. Furthermore, ALD can be extended to sampling from an unconditional distribution such as an energy-based model, enabling more flexible generative modeling by applying it to the prior distribution of the latent variable. Based on ALD, we construct a new deep latent variable model named the Langevin autoencoder (LAE). LAE uses ALD for autoencoder-like posterior inference and sampling from the latent space EBM. Using toy datasets, we empirically validate that ALD can properly obtain samples from target distributions in both conditional and unconditional cases, and ALD converges significantly faster than traditional LD. We also evaluate LAE on the image generation task using three datasets (SVHN, CIFAR-10, and CelebA-HQ). Not only can LAE be trained faster than non-amortized MCMC methods, but LAE can also generate better samples in terms of the Fréchet Inception Distance (FID) compared to AVI-based methods, such as the variational autoencoder1.","This paper proposes an amortized Langevin dynamics (ALD) method for deep latent variable models. ALD replaces the datapoint-wise MCMC iterations with updates of an inference model that maps observations into latent variables. The authors prove that ALD has the target posterior as a stationary distribution under some assumptions, and can be extended to sampling from an unconditional distribution such as an energy-based model, enabling more flexible generative modeling by applying it to the prior distribution of the latent variable. Based on ALD, the authors construct a new deep latent variables model named the Langevin autoencoder (LAE) which uses ALD for autoencoders-like posterior inference and sampling from the latent space EBM."
319,SP:5631097031c7e599bdeae64366ffa6e4558837c6,"We study the problem of hypergraph reasoning in large domains, e.g., predicting the relationship between several entities based on the input facts. We observe that in logical reasoning, logical rules (e.g., my parent’s parent is my grandparent) usually apply locally (e.g., only three people are involved in a grandparent rule), and sparsely (e.g., the grandparent relationship is sparse across all pairs of people in the world). Inspired by these observations, we propose Sparse and Local Neural Logic Machines (SpaLoc), a structured neural network for hypergraph reasoning. To leverage the sparsity in hypergraph neural networks, SpaLoc represents the grounding of relationships such as parent and grandparent as sparse tensors and uses neural networks and finite-domain quantification operations to infer new facts based on the input. We further introduce a sparsification loss to regularize the number of hyperedges in intermediate layers of a SpaLoc model. To enable training on large-scale graphs such as real-world knowledge graphs, SpaLoc makes training and inference-time sub-sampling of the input graphs. To remedy the information loss in sampled sub-graphs, we propose a novel sampling and label calibration paradigm based on an information-theoretic measure information sufficiency. Our SpaLoc shows superior accuracy and efficiency on synthetic datasets compared with prior art and achieves state-of-the-art performance on several real-world knowledge graph reasoning benchmarks.","This paper proposes a novel method for hypergraph reasoning based on neural networks and finite-domain quantification operations. The proposed method is motivated by the observation that in logical reasoning, logical rules usually apply locally (e.g., only three people are involved in a grandparent rule), and sparsely (e,g., the grandparent relationship is sparse across all pairs of people in the world). To leverage the sparsity in hypergraph neural networks, the proposed method represents the grounding of relationships such as parent and grandparent as sparse tensors and uses neural networks to infer new facts based on the input. The authors also introduce a sparsification loss to regularize the number of hyperedges in intermediate layers of a SpaLoc model. To enable training on large-scale graphs such as real-world knowledge graphs, SpaLoc makes training and inference-time sub-sampling of the input graphs. To remedy the information loss in sampled sub-graphs, the authors propose a novel sampling and label calibration paradigm based on an information-theoretic measure information sufficiency."
320,SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"The top-k classification accuracy is one of the core metrics in machine learning. Here, k is conventionally a positive integer, such as 1 or 5. In this work, we relax this assumption and propose to draw k from a probability distribution for training. Combining this with recent advances in differentiable sorting and ranking, we propose a new family of differentiable top-k cross-entropy classification losses. We find that relaxing k does not only produce better top-5 accuracies, but also makes models more robust, which leads to top-1 accuracy improvements. When finetuning publicly available ImageNet models, we achieve a new state-of-the-art on ImageNet for publicly available models with an 88.37% top-1 and a 98.68% top-5 accuracy.","This paper proposes to relax the top-k classification accuracy, where k is conventionally a positive integer, such as 1 or 5. The authors propose to draw k from a probability distribution for training, and propose a new family of differentiable top k cross-entropy classification losses. They find that relaxing k does not only produce better top-5 accuracies, but also makes models more robust, which leads to top-1 accuracy improvements."
321,SP:cb3188f435c54a365890e20e4d582c250d919833,"We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. Built on the celebrated Douglas-Rachford splitting technique, our method tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows us to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. The proposed method enjoys an iteration complexity O(1/ ) compared to the best-known O(1/ ) of the Sinkhorn method. In addition, we establish a linear convergence rate for our formulation of the OT problem. We detail an efficient GPU implementation of the proposed method that maintains a primal-dual stopping criterion at no extra cost. Substantial experiments demonstrate the effectiveness of our method, both in terms of computation times and robustness.","This paper proposes a new method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. The proposed method is based on the Douglas-Rachford splitting technique, which tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. In addition, the authors establish a linear convergence rate for the formulation of the OT problem and provide an efficient GPU implementation."
322,SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"Federated learning data is drawn from a distribution of distributions: clients are drawn from a meta-distribution, and their data are drawn from local data distributions. Thus generalization studies in federated learning should separate performance gaps from unseen client data (out-of-sample gap) from performance gaps from unseen client distributions (participation gap). In this work, we propose a framework for disentangling these performance gaps. Using this framework, we observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in federated learning. We propose a semantic synthesis strategy that enables realistic simulation without naturally-partitioned data. Informed by our findings, we call out community suggestions for future federated learning works.","This paper proposes a framework for disentangling performance gaps in federated learning. The main idea is to separate performance gaps from unseen client data (out-of-sample gap) from performance gap from unseen clients distributions (participation gap). The authors propose a dataset synthesis strategy that enables realistic simulation without naturally-partitioned data. The authors observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis can be important for realistic simulations."
323,SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"Starting from the resurgence of deep learning, language models (LMs) have never been so popular. Through simply increasing model scale and data size, large LMs pre-trained with self-supervision objectives demonstrate awe-inspiring results on both task performance and generalization. At the early stage, supervised finetuning is indispensable in adapting pre-trained language models (PLMs) to downstream tasks. Later on, the sustained growth of model capacity and data size, as well as newly presented pre-training techniques, make the PLMs perform well under the few-shot setting, especially in the recent paradigm of prompt-based learning. After witnessing the success of PLMs for few-shot tasks, we propose to further study the potential and limitations of PLMs for the zero-shot setting. We utilize 3 models from the most popular BERT family to launch the empirical study on 20 different datasets. We are surprised to find that a simple Multi-Null Prompting (without manually/automatically created prompts) strategy can yield very promising results on a few widely-used datasets, e.g., 86.59%(±0.59) accuracy on the IMDB dataset, and 86.22%(±2.71) accuracy on the Amazon dataset, which outperforms manually created prompts without engineering in achieving much better and stable performance with the accuracy of 74.06%(±13.04), 75.54%(±11.77) for comparison. However, we also observe some limitations of PLMs under the zero-shot setting, particularly for the language understanding tasks (e.g., GLUE).","This paper studies the few-shot prompt-based language modeling in the zero-shot setting. The authors propose a simple multi-null prompting strategy to improve the performance of pre-trained language models (PLMs) in the few shot setting. In particular, the authors use 3 models from the most popular BERT family to launch the empirical study on 20 different datasets. They are surprised to find that a simple Multi-Null Prompting (without manually/automatically created prompts) strategy can yield very promising results on a few widely-used datasets, e.g., 86.59%(±0.59) accuracy on the IMDB dataset, and 86.22% (±2.71) accuracy  on the Amazon dataset, which outperforms manually created prompts without engineering in achieving much better and stable performance. However, they also observe some limitations of PLMs under the zero shot setting, particularly for the language understanding tasks (e.g. GLUE)."
324,SP:9817dccb1a121058b23a2ef825ed339cf8b53674,"Attention mechanism has been widely applied to tasks that output some sequence 1 from an input image. Its success comes from the ability to align relevant parts of 2 the encoded image with the target output. However, most of the existing methods 3 fail to build clear alignment because the aligned parts are unable to well represent 4 the target. In this paper we seek clear alignment in attention mechanism through 5 a sharpener module. Since it deliberately locates the target in an image region 6 and refines representation to be target-specific, the alignment and interpretability 7 of attention can be significantly improved. Experiments on synthetic handwritten 8 digit as well as real-world scene text recognition datasets show that our approach 9 outperforms the mainstream ones such as soft and hard attention. 10",This paper proposes a method to improve the performance of the attention mechanism in image recognition tasks. The proposed method is based on a sharpener module that aims to align the relevant parts of the encoded image with the target output. The authors claim that the proposed method can improve the alignment and interpretability of attention mechanism. Experiments on synthetic handwritten digits and real-world scene text recognition datasets show that their approach outperforms the mainstream methods such as soft and hard attention.
325,SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"Learning to solve combinatorial optimization problems, such as the vehicle routing problem, offers great computational advantages over classical operations research solvers and heuristics. The recently developed deep reinforcement learning approaches either improve an initially given solution iteratively or sequentially construct a set of individual tours. However, most of the existing learning-based approaches are not able to work for a fixed number of vehicles and thus bypass the complex assignment problem of the customers onto an apriori given number of available vehicles. On the other hand, this makes them less suitable for real applications, as many logistic service providers rely on solutions provided for a specific bounded fleet size and cannot accommodate short term changes to the number of vehicles. In contrast we propose a powerful supervised deep learning framework that constructs a complete tour plan from scratch while respecting an apriori fixed number of available vehicles. In combination with an efficient post-processing scheme, our supervised approach is not only much faster and easier to train but also achieves competitive results that incorporate the practical aspect of vehicle costs. In thorough controlled experiments we compare our method to multiple state-of-the-art approaches where we demonstrate stable performance, while utilizing less vehicles and shed some light on existent inconsistencies in the experimentation protocols of the related work.","This paper proposes a reinforcement learning-based approach for vehicle routing problems. The proposed method is based on the idea of learning a tour plan for a fixed number of vehicles. The authors propose a supervised learning framework that constructs a complete tour plan from scratch while respecting an apriori given number of available vehicles. In addition, the authors propose an efficient post-processing scheme to improve the performance of the learned tour plan. Experiments are conducted on a real-world vehicle routing problem and show the effectiveness of the proposed method."
326,SP:594a813c0d0baa66738b9c8331370f861ad3c416,"Learning to predict missing links is important for many graph-based applications. Existing methods were designed to learn the association between two sets of variables: (1) the observed graph structure (e.g., clustering effect) and (2) the existence of link between a pair of nodes. However, the causal relationship between these variables was ignored. We visit the possibility of learning it by asking a counterfactual question: “would the link exist or not if the observed graph structure became different?” To answer this question, we leverage causal models considering the information of the node pair (i.e., learned graph representations) as context, global graph structural properties as treatment, and link existence as outcome. In this work, we propose a novel link prediction method that enhances graph learning by counterfactual inference. It creates counterfactual links from the observed ones, and learns representations from both the observed and counterfactual links. Experiments on benchmark datasets show that this novel graph learning method achieves state-of-the-art performance on link prediction.","This paper proposes a novel method for link prediction based on counterfactual inference. The idea is to learn the causal relationship between two sets of variables: the observed graph structure (e.g., clustering effect) and the existence of link between a pair of nodes. The proposed method learns representations from both the observed and counterfactually links. Experiments on benchmark datasets show that the proposed method achieves state-of-the-art performance on link prediction."
327,SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"Unsupervised feature selection aims to select a subset from the original features that are most useful for the downstream tasks without external guidance information. While most unsupervised feature selection methods focus on ranking features based on the intrinsic properties of data, they do not pay much attention to the relationships between features, which often leads to redundancy among the selected features. In this paper, we propose a two-stage Second-Order unsupervised Feature selection via knowledge contrastive disTillation (SOFT) model that incorporates the second-order covariance matrix with the first-order data matrix for unsupervised feature selection. In the first stage, we learn a sparse attention matrix that can represent second-order relations between features. In the second stage, we build a relational graph based on the learned attention matrix and perform graph segmentation for feature selection. Experimental results on 12 public datasets show that SOFT outperforms classical and recent state-of-the-art methods, which demonstrates the effectiveness of our proposed method.","This paper proposes a two-stage unsupervised feature selection method based on knowledge contrastive disTillation (SOFT) model that incorporates the second-order covariance matrix with the first-order data matrix for feature selection. In the first stage, SOFT learns a sparse attention matrix that can represent second order relations between features, and in the second stage, it builds a relational graph based on the learned attention matrix and performs graph segmentation for feature selector. Experimental results on 12 public datasets show that SOFT outperforms classical and recent state-of-the-art methods, which demonstrates the effectiveness of SOFT."
328,SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"Multimodal variational autoencoders (VAEs) seek to model the joint distribution over heterogeneous data (e.g. vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the Mutually supErvised Multimodal VAE (MEME), that avoids such explicit combinations by repurposing semisupervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partiallyobserved data where some modalities can be entirely missing—something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image–image) and CUB (image–text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data.","This paper proposes a semi-supervised multi-modal variational autoencoder (MEME) that combines information between modalities implicitly through mutual supervision. The idea is to use a semisupervised VAE to model the joint distribution over heterogeneous data (e.g. vision, language), while also capturing a shared representation across such modalities. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing. The authors demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image–image) and CUB (image-text) datasets."
329,SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"Although Intrinsic Motivation allows a Reinforcement Learning agent to generate directed behaviors in an environment, even with sparse or noisy rewards, combining intrinsic and extrinsic rewards is non trivial. As an alternative to the widespread method of a weighted sum of rewards, Explore Options let the agent call an intrinsically motivated agent in order to observe and learn from interesting behaviors in the environment. Such options have only been established for simple tabular cases, and are unfit to high dimensional spaces. In this paper, we propose Deep Explore Options, revising Explore Options within the Deep Reinforcement Learning paradigm to tackle complex visual problems. Deep Explore Options can naturally learn from several unrelated intrinsic rewards, ignore harmful intrinsic rewards, learn to balance exploration, but also isolate exploitative or exploratory behaviors. In order to achieve this, we first introduce J-PER, a new transitionselection algorithm based on the interest of multiple agents. Next, we propose to consider intrinsic reward learning as an auxiliary task, with a resulting architecture achieving 50% faster wall-clock speed and building a stronger, shared representation. We test Deep Explore Options on hard and easy exploration games of the Atari Suite, following a benchmarking study to ensure fairness. Our results show that not only can they learn from multiple intrinsic rewards, they are a very strong alternative to a weighted sum of rewards, convincingly beating the baselines in 4 of the 6 tested environments, and with comparable performances in the other 2.","This paper proposes Deep Explore Options, an extension of Explore Options to tackle complex visual problems. Explore Options is an alternative to the widespread method of a weighted sum of rewards, where the agent calls an intrinsically motivated agent in order to observe and learn from interesting behaviors in the environment. The authors propose to use intrinsic reward learning as an auxiliary task, with a resulting architecture achieving 50% faster wall-clock speed and building a stronger, shared representation. The experiments are conducted on Atari Suite, following a benchmarking study to ensure fairness. "
330,SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,"We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identifies and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows us to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector fields. We evaluate SANN on complex physical systems including a three-body problem and billiard model. We show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to significant improvement in accuracy.",This paper proposes a new method for learning Hamiltonian dynamical systems from data. The authors propose a new neural network architecture called stiffness-aware neural network (SANN) that splits the training data into stiff and non-stiff portions based on a stiffness-awareness index. The proposed method is evaluated on a three-body problem and a billiard model and shows that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods.
331,SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"Large pre-trained language models perform remarkably well on tasks that can be done “in one pass”, such as generating realistic text (Brown et al., 2020) or synthesizing computer programs (Chen et al., 2021; Austin et al., 2021). However, they struggle with tasks that require unbounded multi-step computation, such as adding integers (Brown et al., 2020) or executing programs (Austin et al., 2021). Surprisingly, we find that these same models are able to perform complex multistep computations—even in the few-shot regime—when asked to perform the operation “step by step”, showing the results of intermediate computations. In particular, we train Transformers to perform multi-step computations by asking them to emit intermediate computation steps into a “scratchpad”. On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.","This paper presents a method for training Transformer-based language models to perform multi-step computations. In particular, the authors train Transformers to emit intermediate computation steps into a “scratchpad” and show that this improves the performance of language models on long addition and execution of arbitrary programs. The paper is well-written and well-motivated, and the experimental results are interesting."
332,SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"It is well understood that modern deep networks are vulnerable to adversarial attacks. However, conventional methods fail to produce adversarial perturbations that are intelligible to humans, and they pose limited threats in the physical world. To study feature-class associations in networks and better understand the realworld threats they face, we develop feature-level adversarial perturbations using deep image generators and a novel optimization objective. We term these featurefool attacks. We show that they are versatile and use them to generate targeted feature-level attacks at the ImageNet scale that are simultaneously interpretable, universal to any source image, and physically-realizable. These attacks can also reveal spurious, semantically-describable feature/class associations that can be exploited by novel combinations of natural objects. We use them to guide the design of “copy/paste” adversaries in which one natural image is pasted into another to cause a targeted misclassification.","This paper proposes a novel adversarial attack method that uses deep image generators to generate adversarial perturbations that are interpretable, universal to any source image, and physically-realizable. The authors also propose a novel optimization objective for the adversarial attacks, which they call featurefool attacks. They show that the proposed method can be used to generate targeted feature-level attacks at the ImageNet scale that are simultaneously interpretable and universal to all source images, and can also reveal spurious, semantically-describable feature/class associations that can be exploited by novel combinations of natural objects."
333,SP:873618263dc4246a39c44d0abfecfb5f688817e3,"Simulated annealing (SA) is a stochastic global optimisation technique applicable to a wide range of discrete and continuous variable problems. Despite its simplicity, the development of an effective SA optimiser for a given problem hinges on a handful of carefully handpicked components; namely, neighbour proposal distribution and temperature annealing schedule. In this work, we view SA from a reinforcement learning perspective and frame the proposal distribution as a policy, which can be optimised for higher solution quality given a fixed computational budget. We demonstrate that this Neural SA with such a learnt proposal distribution outperforms SA baselines with hand-selected parameters on a number of problems: Rosenbrock’s function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. We also show that Neural SA scales well to large problems while again outperforming popular off-the-shelf solvers in terms of solution quality and wall clock time.","This paper proposes a reinforcement learning-based algorithm for simulated annealing (SA) based on reinforcement learning. Specifically, the authors propose to learn the proposal distribution of the proposed temperature anneal schedule for a given problem. The proposed method is evaluated on a variety of problems including Rosenbrock’s function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. It is shown that the proposed method outperforms the state-of-the-art baselines on these problems."
334,SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are inevitably inconsistent and disparate. In this paper, we introduce a novel notion, the δ-stationarity measurement, to explicitly measure the non-stationarity of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. A straightforward but highly non-trivial way is to control the joint policies’ divergence, which is difficult to estimate accurately by imposing the trust-region constraint on the joint policy. Although it has lower computational complexity to decompose the joint policy and impose trust-region constraints on the factorized policies, simple policy factorization like mean-field approximation will lead to more considerable policy divergence, which can be considered as the trust-region decomposition dilemma. We model the joint policy as a pairwise Markov random field and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust-region of the local policies adaptively in an end-to-end manner. MAMT can approximately constrain the consecutive joint policies’ divergence to satisfy δ-stationarity and alleviate the non-stationarity problem. Our method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity.","This paper studies the problem of non-stationarity in cooperative multi-agent reinforcement learning (MARL). The authors propose a new metric, called the “δ-stability”, which measures the divergence of the joint policies of the agents. The authors show that the divergence is bounded by the KL-divergence of consecutive joint policies, and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The proposed algorithm MAMT can approximately constrain the consecutive joint joint policies’ divergence to satisfy the $\delta-stationary” metric. The experimental results show the effectiveness of the proposed algorithm."
335,SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker’s lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%).1","This paper proposes a self-supervised representation learning framework for audio-visual speech. The proposed method, AV-HuBERT, learns a multi-modal representation of audio and visual speech, which can be used for both lip-reading and speech recognition tasks. The method is evaluated on the LRS3 lip reading dataset, where it achieves 32.5% WER with only 30 hours of labeled data, outperforming the previous state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip reading WER is further reduced to 26.9% when using all the labeled data from LRS2 and combined with self-training. On the audio-only speech recognition dataset, the proposed method achieves a 40% relative WER reduction."
336,SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,"From logistics to the natural sciences, combinatorial optimisation on graphs underpins numerous real-world applications. Reinforcement learning (RL) has shown particular promise in this setting as it can adapt to specific problem structures and does not require pre-solved instances for these, often NP-hard, problems. However, state-of-the-art (SOTA) approaches typically suffer from severe scalability issues, primarily due to their reliance on expensive graph neural networks (GNNs) at each decision step. We introduce ECORD; a novel RL algorithm that alleviates this expense by restricting the GNN to a single pre-processing step, before entering a fast-acting exploratory phase directed by a recurrent unit. Experimentally, we demonstrate that ECORD achieves a new SOTA for RL algorithms on the Maximum Cut problem, whilst also providing orders of magnitude improvement in speed and scalability. Compared to the nearest competitor, ECORD reduces the optimality gap by up to 73 % on 500 vertex graphs with a decreased wall-clock time. Moreover, ECORD retains strong performance when generalising to larger graphs with up to 10 000 vertices.","This paper proposes a novel reinforcement learning algorithm for combinatorial optimisation on graphs. The main idea is to restrict the GNN to a single pre-processing step, before entering a fast-acting exploratory phase directed by a recurrent unit. Experiments on the Maximum Cut problem show that the proposed algorithm ECORD achieves a new SOTA for RL algorithms on the maximum cut problem, while also providing orders of magnitude improvement in speed and scalability."
337,SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"Discrete latent variables are considered important to model the generation process of real world data, which has motivated research on Variational Autoencoders (VAEs) with discrete latents. However, standard VAE training is not possible in this case, which has motivated different strategies to manipulate discrete distributions in order to train discrete VAEs similarly to conventional ones. Here we ask if it is also possible to keep the discrete nature of the latents fully intact by applying a direct discrete optimization for the encoding model. The studied approach is consequently strongly diverting from standard VAE training by altogether sidestepping absolute standard VAE mechanisms such as sampling approximation, reparameterization trick and amortization. Discrete optimization is realized in a variational setting using truncated posteriors in conjunction with evolutionary algorithms (using a recently suggested approach). For VAEs with binary latents, we first show how such a discrete variational method (A) ties into gradient ascent for network weights and (B) uses the decoder network to select latent states for training. More conventional amortized training is, as may be expected, more efficient than direct discrete optimization, and applicable to large neural networks. However, we here find direct optimization to be efficiently scalable to hundreds of latent variables using smaller networks. More importantly, we find the effectiveness of direct optimization to be highly competitive in ‘zero-shot’ learning (where high effectiveness for small networks is required). In contrast to large supervised neural networks, the here investigated VAEs can, e.g., denoise a single image without previous training on clean data and/or training on large image datasets. More generally, the studied approach shows that training of VAEs is indeed possible without sampling-based approximation and reparameterization, which may be interesting for the analysis of VAE-training in general. In the regime of few data, direct optimization, furthermore, makes VAEs competitive for denoising where they have previously been outperformed by non-generative approaches.","This paper proposes to train VAEs with discrete latents by using a variational variational autoencoder (VAE) with truncated posteriors and evolutionary algorithms. In particular, the authors propose to train the VAE with discrete latent variables by optimizing the weights of the encoder and decoder separately. The authors show that the proposed method is more efficient than amortized VAE training, and that it is competitive in zero-shot learning. They also show that VAE can be trained without sampling approximation and reparameterization."
338,SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"Identifying controllable aspects of the environment has proven to be an extraordinary intrinsic motivator to reinforcement learning agents. Despite repeatedly achieving State-of-the-Art results, this approach has only been studied as a proxy to a reward-based task and has not yet been evaluated on its own. Current methods are based on action-prediction. Humans, on the other hand, assign blame to their actions to decide what they controlled. This work proposes Controlled Effect Network (CEN), an unsupervised method based on counterfactual measures of blame to identify effects on the environment controlled by the agent. CEN is evaluated in a wide range of environments showing that it can accurately identify controlled effects. Moreover, we demonstrate CEN’s capabilities as intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models.","This paper proposes a method to identify controllable aspects of the environment using counterfactual measures of blame. The proposed method, Controlled Effect Network (CEN), is an unsupervised method based on Counterfactual Measures of Blame (CMB) to identify effects on the environment controlled by the agent. CEN is evaluated in a wide range of environments showing that it can accurately identify controlled effects. Moreover, the authors demonstrate CEN’s capabilities as intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models."
339,SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"Lightweight image super-resolution (SR) networks have obtained promising results with moderate model size. However, they are impractical or neglected to be extended to larger networks. At the same time, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly, because filter pruning for residual blocks is well-known tricky. To address the above issues, we propose structure-regularized pruning (SRP), which imposes regularization on the pruned structure to make sure the locations of pruned filters are aligned across different layers. Specifically, for the layers connected by the same residual, we select the filters of the same indices as unimportant filters. To transfer the expressive power in the unimportant filters to the rest of the network, we employ L2 regularization to drive the weights towards zero so that eventually their absence will cause minimal performance degradation. We apply SRP to train efficient image SR networks, resulting in a lightweight network SRPN-L and a very deep one SRPN. We conduct extensive comparisons with both lightweight and larger image SR networks. SRPN-L and SRPN achieve superior performance gains over recent methods quantitatively and visually.","This paper proposes a pruning method for lightweight image super-resolution networks (SRNets). The proposed method is based on the idea of structure-regularized pruning (SRP), which imposes regularization on the pruned structure to make sure the locations of pruned filters are aligned across different layers. To transfer the expressive power in the unimportant filters to the rest of the network, the authors employ L2 regularization to drive the weights towards zero so that eventually their absence will cause minimal performance degradation. The authors apply SRP to train efficient image SR networks, resulting in a lightweight network SRPN-L and a very deep one SRPN."
340,SP:0dee45001ae9600f485614dfe6874a516ac01db5,"Most current few-shot learning methods train a model from abundantly labeled base category data and then transfer and adapt the model to sparsely labeled novel category data. These methods mostly generalize well on novel categories from the same domain as the base categories but perform poorly for distant domain categories. In this paper, we propose a framework for few-shot learning coined as ConFeSS (Contrastive Learning and Feature Selection System) that tackles large domain shift between base and novel categories. The first step of our framework trains a feature extracting backbone with the contrastive loss on the base category data. Since the contrastive loss does not use supervision, the features can generalize better to distant target domains. For the second step, we train a masking module to select relevant features that are more suited to target domain classification. Finally, a classifier is fine-tuned along with the backbone such that the backbone produces features similar to the relevant ones. To evaluate our framework, we tested it on a recently introduced cross-domain few-shot learning benchmark. Experimental results demonstrate that our framework outperforms all meta-learning approaches and produces competitive results against recent cross-domain methods. Additional analyses are also performed to better understand our framework.","This paper proposes a novel method for few-shot learning based on contrastive learning and feature selection. The idea is to train a feature extracting backbone with the contrastive loss on the base category data and a masking module to select relevant features that are more suited to target domain classification. Finally, a classifier is fine-tuned along with the backbone such that the backbone produces features similar to the relevant ones. The proposed method is evaluated on a recently introduced cross-domain few shot learning benchmark. The experimental results demonstrate that the proposed method outperforms all meta-learning approaches and produces competitive results against recent cross- domain methods."
341,SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"Do neural networks generalise because of bias in the functions returned by gradient descent, or bias already present in the network architecture? ¿Por qué no los dos? This paper finds that while typical networks that fit the training data already generalise fairly well, gradient descent can further improve generalisation by selecting networks with a large margin. This conclusion is based on a careful study of the behaviour of infinite width networks trained by Bayesian inference and finite width networks trained by gradient descent. To measure the implicit bias of architecture, new technical tools are developed to both analytically bound and consistently estimate the average test error of the neural network–Gaussian process (NNGP) posterior. This error is found to be already better than chance, corroborating the findings of Valle-Pérez et al. (2019) and underscoring the importance of architecture. Going beyond this result, this paper finds that test performance can be substantially improved by selecting a function with much larger margin than is typical under the NNGP posterior. This highlights a curious fact: minimum a posteriori functions can generalise best, and gradient descent can select for those functions. In summary, new technical tools suggest a nuanced portrait of generalisation involving both the implicit biases of architecture and gradient descent.","This paper studies the generalization properties of neural networks trained by Bayesian inference and finite width networks trained with gradient descent. The authors propose a new technical tools to both analytically bound and consistently estimate the average test error of the neural network–Gaussian process (NNGP) posterior. This error is found to be already better than chance, corroborating the findings of Valle-Pérez et al. (2019) and underscoring the importance of architecture. Further, this paper finds that test performance can be substantially improved by selecting a function with much larger margin than is typical under the NNGP posterior."
342,SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"Based on large-scale pre-trained multilingual representations, recent cross-lingual transfer methods have achieved impressive transfer performances. However, the performance of target languages still lags far behind the source language. In this paper, our analyses indicate such a performance gap is strongly associated with the cross-lingual representation discrepancy. To achieve better cross-lingual transfer performance, we propose the cross-lingual manifold mixup (X-Mixup) method, which adaptively calibrates the representation discrepancy and gives compromised representations for target languages. Experiments on the XTREME benchmark show X-Mixup achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and reduces the cross-lingual representation discrepancy significantly.","This paper proposes a method for cross-lingual manifold mixup (X-Mixup) that adaptively calibrates the representation discrepancy and gives compromised representations for target languages. Experiments on the XTREME benchmark show that X-mixup achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and reduces the crosslingual representation discrepancy significantly."
343,SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to significant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the first to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.","This paper studies the problem of Byzantine robust distributed federated learning, where the data distribution is heterogeneous (i.e., not iid). The authors propose a new attack method, bucketing, to circumvent the existing Byzantine robust defenses. The main idea is to adapt existing robust algorithms to the heterogeneous data distribution. The authors prove the convergence of the proposed bucketing algorithm under the assumption that the data is not identical. They also provide theoretical analysis of the convergence rate of bucketing and show that it converges to a state-of-the-art rate. Finally, they show that bucketing can be combined with existing algorithms to defend against Byzantine robust attacks."
344,SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"One of the main arguments behind studying disentangled representations is the assumption that they can be easily reused in different tasks. At the same time finding a joint, adaptable representation of data is one of the key challenges in the multi-task learning setting. In this paper, we take a closer look at the relationship between disentanglement and multi-task learning based on hard parameter sharing. We perform a thorough empirical study of the representations obtained by neural networks trained on automatically generated supervised tasks. Using a set of standard metrics we show that disentanglement appears naturally during the process of multi-task neural network training.","This paper studies the relationship between disentanglement and multi-task learning based on hard parameter sharing. The authors perform a thorough empirical study of the representations obtained by neural networks trained on automatically generated supervised tasks. Using a set of standard metrics, the authors show that disentangled representations appear naturally during the process of multi- task neural network training."
345,SP:9851adb72e2918780f661f83f7da06eb866787be,"We present a framework of Certifying Robust Policies (CROP) for reinforcement learning against adversarial state perturbations, which provides state level robustness certification and the first certification for cumulative rewards. We propose two particular types of robustness certification criteria: robustness of per-state actions and lower bound of cumulative rewards. Specifically, we develop a local smoothing algorithm that uses a policy derived from Q-functions smoothed with Gaussian noise over each encountered state to guarantee the robustness of actions taken along this trajectory. Next, we develop a global smoothing algorithm for certifying the robustness of a finite-horizon cumulative reward under adversarial state perturbations. Finally, we propose a local smoothing approach that makes use of adaptive search in order to obtain tight certification bounds for the reward. We use the proposed RL robustness certification framework to evaluate methods that have previously been shown to yield empirically robust RL, including adversarial training and several forms of regularization, on three representative Atari games. We show that RegPGD, RegCVX, and RadialRL achieve high certified robustness among these. Furthermore, by evaluating these algorithms against adversarial attacks, we demonstrate that our certifications are often tight. All experiment results are available at website https://crop-leaderboard.me.","This paper proposes a framework of certifying robust policies (CROP) for reinforcement learning against adversarial state perturbations, which provides state level robustness certification and the first certification for cumulative rewards. Specifically, the authors propose a local smoothing algorithm that uses a policy derived from Q-functions smoothed with Gaussian noise over each encountered state to guarantee the robustness of actions taken along this trajectory. They also propose a global smoothing method for certifying the cumulative reward under adversarial attacks. Finally, they evaluate methods that have previously been shown to yield empirically robust RL, including adversarial training and several forms of regularization, on three representative Atari games."
346,SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"In this paper, we develop a new approach to conformal prediction in which we aim to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers. Standard conformal prediction provides the ability to adapt to model uncertainty by constructing a calibrated candidate set in place of a single prediction, with guarantees that the set contains the correct answer with high probability. In order to obey this coverage property, however, conformal sets can often become inundated with noisy candidates—which can render them unhelpful in practice. This is particularly relevant to large-scale settings where the cost (monetary or otherwise) of false positives is substantial, such as for in-silico screening for drug discovery, where any positively identified molecular compound is then manufactured and tested. We propose to trade coverage for precision by enforcing that the presence of incorrect candidates in the predicted conformal sets (i.e., the total number of false positives) is bounded according to a user-specified tolerance. Subject to this constraint, our algorithm then optimizes for a generalized notion of set coverage (i.e., the true positive rate) that allows for any number of true answers for a given query (including zero). We demonstrate the effectiveness of this approach across a number of classification tasks in natural language processing, computer vision, and computational chemistry.","This paper proposes a new approach to conformal prediction, which aims to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers. The authors propose to trade coverage for precision by enforcing that the presence of incorrect candidates in the predicted conformal sets (i.e., the total number of false positives) is bounded according to a user-specified tolerance. Subject to this constraint, the proposed algorithm then optimizes for a generalized notion of set coverage that allows for any number of true answers for a given query (including zero). The authors demonstrate the effectiveness of this approach across several classification tasks in natural language processing, computer vision, and computational chemistry."
347,SP:b126d2f3c397633745c8833e22ace93a2470e963,"Assessing the complexity of functions computed by a neural network helps us understand how the network will learn and generalize. One natural measure of complexity is how the network distorts length – if the network takes a unit-length curve as input, what is the length of the resulting curve of outputs? We prove that the expected length distortion does not grow with depth, and indeed shrinks slightly, for ReLU networks with standard random initialization. We also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher-dimensional volumes. These theoretical results are corroborated by our experiments.","This paper studies the effect of depth and depth depth on the length of the output curve of a ReLU network. The authors prove that the expected length distortion does not grow with depth, and indeed shrinks slightly, for ReLU networks with standard random initialization. They also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher-dimensional volumes. The theoretical results are corroborated by experiments."
348,SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"Though many reinforcement learning (RL) problems involve learning policies in settings that are difficult to specify safety constraints and sparse rewards, current methods struggle to rapidly and safely acquire successful policies. Behavioral priors, which extract useful policy primitives for learning from offline datasets, have recently shown considerable promise at accelerating RL in more complex problems. However, we discover that current behavioral priors may not be wellequipped for safe policy learning, and in some settings, may promote unsafe behavior, due to their tendency to ignore data from undesirable behaviors. To overcome these issues, we propose SAFEty skill pRiors (SAFER), a behavioral prior learning algorithm that accelerates policy learning on complex control tasks, under safety constraints. Through principled contrastive training on safe and unsafe data, SAFER learns to extract a safety variable from offline data that encodes safety requirements, as well as the safe primitive skills over abstract actions in different scenarios. In the inference stage, SAFER composes a safe and successful policy from the safety skills according to the inferred safety variable and abstract action. We demonstrate its effectiveness on several complex safety-critical robotic grasping tasks inspired by the game Operation,1 in which SAFER outperforms baseline methods in learning successful policies and enforcing safety.","This paper proposes SAFEty skill pRiors (SAFER), a behavioral prior learning algorithm that accelerates policy learning on complex control tasks, under safety constraints. SAFER learns to extract a safety variable from offline data that encodes safety requirements, as well as the safe primitive skills over abstract actions in different scenarios. In the inference stage, SAFER composes a safe and successful policy from the safety skills according to the inferred safety variable and abstract action. The proposed method is evaluated on several complex safety-critical robotic grasping tasks inspired by the game Operation, in which SAFER outperforms baseline methods."
349,SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"Image restoration is a challenging and ill-posed problem which also has been a long-standing issue. However, most of learning based restoration methods are proposed to target one degradation type which means they are lack of generalization. In this paper, we proposed a multi-branch restoration model inspired from the Human Visual System (i.e., Retinal Ganglion Cells) which can achieve multiple restoration tasks in a general framework. The experiments show that the proposed multi-branch architecture, called CMFNet, has competitive performance results on four datasets, including image dehazing, deraindrop, and deblurring, which are very common applications for autonomous cars. The source code and pretrained models of three restoration tasks are available at https: //github.com/publish_after_accepting/CMFNet.","This paper proposes a multi-branch neural network architecture for image restoration. The proposed architecture is based on the Retinal Ganglion Cells (RGC) model, which is inspired by the human visual system. The authors show that the proposed architecture can achieve state-of-the-art results on four image restoration tasks, including image dehazing, deraindrop, deblurring, and deblurring. The experiments are conducted on four datasets, and the proposed method is evaluated on three restoration tasks."
350,SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"In Federated learning (FL), multiple clients collaborate to learn a model through a central server but keep the data decentralized. Personalized federated learning (PFL) further extends FL to handle data heterogeneity between clients by learning personalized models. In both FL and PFL, all clients participate in the training process and their labeled data is used for training. However, in reality, novel clients may wish to join a prediction service after it has been deployed, obtaining predictions for their own unlabeled data. Here, we defined a new learning setup, Inference-Time PFL (IT-PFL), where a model trained on a set of clients, needs to be later evaluated on novel unlabeled clients at inference time. We propose a novel approach to this problem IT-PFLHN, based on a hypernetwork module and an encoder module. Specifically, we train an encoder network that learns a representation for a client given its unlabeled data. That client representation is fed to a hypernetwork that generates a personalized model for that client. Evaluated on four benchmark datasets, we find that IT-PFL-HN generalizes better than current FL and PFL methods, especially when the novel client has a large domain shift. We also analyzed the generalization error for the novel client, showing how it can be bounded using results from multi-task learning and domain adaptation. Finally, since novel clients do not contribute their data to training, they can potentially have better control over their data privacy; indeed, we showed analytically and experimentally how novel clients can apply differential privacy to their data.","This paper proposes a new federated learning method, called IT-PFL-HN, which is based on a hypernetwork module and an encoder module. The idea is that the encoder network learns a representation for a client given its unlabeled data. The representation is fed to the hypernetwork that generates a personalized model for that client. The proposed method is evaluated on four benchmark datasets and compared with the state-of-the-art."
351,SP:960d0a63a82593f6e72275b65f0501f0469d1924,"Discovering what is learned by neural networks remains a challenge. In selfsupervised learning, classification is the most common task used to evaluate how good a representation is. However, relying only on such downstream task can limit our understanding of how much information is retained in the representation of a given input. In this work, we showcase the use of a conditional diffusion based generative model (RCDM) to visualize representations learned with self-supervised models. We further demonstrate how this model’s generation quality is on par with state-of-the-art generative models while being faithful to the representation used as conditioning. By using this new tool to analyze self-supervised models, we can show visually that i) SSL (backbone) representation are not really invariant to many data augmentation they were trained on. ii) SSL projector embedding appear too invariant for tasks like classifications. iii) SSL representations are more robust to small adversarial perturbation of their inputs iv) there is an inherent structure learned with SSL model that can be used for image manipulation. Earth from... space1 an untrained representation a supervised representation a SSL representation",This paper presents a method for visualizing representations learned by self-supervised models. The method is based on a conditional diffusion based generative model (RCDM). The authors show that the generated images are not always invariant to the data augmentation they were trained on. They also show that SSL representations are more robust to small adversarial perturbations of their inputs.
352,SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"We prove that Fp sketch, a well-celebrated streaming algorithm for frequency moments estimation, is differentially private as is when p ∈ (0, 1]. Fp sketch uses only polylogarithmic space, exponentially better than existing DP baselines and only worse than the optimal non-private baseline by a logarithmic factor. The evaluation shows that Fp sketch can achieve reasonable accuracy with differential privacy guarantee. The evaluation code is included in the supplementary material.","This paper studies the problem of differentially private streaming algorithms for frequency moments estimation. The main contribution of this paper is to prove that Fp sketch, a well-celebrated streaming algorithm for frequency moment estimation, is differentially privacy as is when p \in (0, 1). The authors show that the algorithm is exponentially better than existing DP baselines and only worse than the optimal non-private baseline by a logarithmic factor. The evaluation shows that the proposed algorithm can achieve reasonable accuracy with differential privacy."
353,SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones. To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process. When a sampled trajectory is sufficiently distinct, RSPO performs standard policy optimization with extrinsic rewards. For trajectories with high likelihood under existing policies, RSPO utilizes an intrinsic diversity reward to promote exploration. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCo continuous control to multi-agent stag-hunt games and StarCraftII challenges.","This paper proposes a reward-switching policy optimization (RSPO) method for finding novel policies that are both locally optimal and sufficiently different from existing ones. The authors propose to switch between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process to encourage the learning policy to consistently converge towards a previously undiscovered local optimum. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single agent particle-world tasks and MuJoCo continuous control to multi-agent stag-hunt games and StarCraftII challenges."
354,SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required.",This paper proposes a method for fast sampling for diffusion models. The main idea is to use gradient-based gradient descent to search for the best sample quality of the diffusion model. This is achieved by optimizing the degrees of freedom of diffusion models by maximizing sample quality scores via gradient descent. The method is evaluated on unconditional image generation on the LSUN dataset.
355,SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,"Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (“experts”) and select one to query the LLM. They require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. PAdapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of only using natural language queries. Finally, we investigate what makes a P-Adapter successful and conclude that access to the LLM’s embeddings of the original natural language prompt, particularly the subject of the entity pair being asked about, is a significant factor.","This paper proposes a new model, called P-Adapters, which is a lightweight model that sits between the embedding layer and the first attention layer of Large Language Models (LLMs). The P-adapters take LLM embeddings as input and output continuous prompts that are used to query the LLM. The authors also investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (“experts”) and select one of them as the prompt to query. They show that the PAdapters show between 12-26% absolute improvement in precision and 36-50% improvement in consistency over a baseline of only using natural language queries."
356,SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"More and more real-world applications require to classify time series at every time. For example, critical patients should be detected for vital signs and diagnosed at all times to facilitate timely life-saving. For this demand, we propose a new concept, Continuous Classification of Time Series (CCTS), to achieve the high-accuracy classification at every time. Time series always evolves dynamically, changing features introducing the multi-distribution form. Thus, different from the existing one-shot classification, the key of CCTS is to model multiple distributions simultaneously. However, most models are hard to achieve it due to their independent identically distributed premise. If a model learns a new distribution, it will likely forget old ones. And if a model repeatedly learns similar data, it will likely be overfitted. Thus, two main problems are the catastrophic forgetting and the over fitting. In this work, we define CCTS as a continual learning task with the unclear distribution division. But different divisions differently affect two problems and a fixed division rule may become invalid as time series evolves. In order to overcome two main problems and finally achieve CCTS, we propose a novel Adaptive model training policy ACCTS. Its adaptability represents in two aspects: (1) Adaptive multi-distribution extraction policy. Instead of the fixed rules and the prior knowledge, ACCTS extracts data distributions adaptive to the time series evolution and the model change; (2) Adaptive importance-based replay policy. Instead of reviewing all old distributions, ACCTS only replays the important samples adaptive to the contribution of data to the model. Experiments on four real-world datasets show that our method can classify more accurately than all baselines at every time.","This paper proposes a method for continuous classification of time series (CCTS) based on adaptive multi-distribution extraction policy. The authors define CCTS as a continual learning task with the unclear distribution division and propose a novel Adaptive model training policy ACCTS to overcome two main problems: catastrophic forgetting and overfitting. In addition, the authors also propose an importance-based replay policy that only replays the important samples adaptive to the contribution of data to the model. Experiments on four real-world datasets show that the proposed method can classify more accurately than all baselines."
357,SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. Despite the fact that our implementation of memory is not differentiable, we demonstrate that an approximate kNN lookup into the memory improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 131k tokens. We also find that the model is capable of making use of newly defined functions and theorems during test time.","This paper proposes a memory-based language model that can read and memorize new data at inference time, thus acquiring new knowledge immediately. The authors propose a kNN-based approach to memorize the internal representations of past inputs. They demonstrate that an approximate kNN lookup into the memory improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), and formal theorems (Isabelle). They show that the performance steadily improves when they increase the size of memory up to 131k tokens."
358,SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable and improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, we develop a tractable sampling scheme based on the Metropolis–Hastings Monte Carlo algorithm. In our approach, samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. We validate the effectiveness of the proposed parametrizations by exploring the quality of samples drawn from these energybased models for both open-ended unconditional generation and a conditional generation task of machine translation. We theoretically and empirically justify our sampling algorithm by showing that the masked conditionals on their own do not yield a Markov chain whose stationary distribution is that of our target distribution, and our approach generates higher quality samples than other recently proposed undirected generation approaches (Wang and Cho, 2019; Ghazvininejad et al., 2019).","This paper proposes a sampling scheme based on Metropolis-Hastings Monte Carlo algorithm to generate samples from the same masked language modeling (MLM) objective. The authors interpret MLMs as energy-based sequence models and propose two energy parametrizations derived from the trained MLMs. The proposed sampling scheme is based on the Metropolis–hastings sampling scheme. The paper shows that the proposed sampling algorithm can generate samples with better quality than other recently proposed undirected generation approaches (Wang and Cho, 2019; Ghazvininejad et al., 2019). "
359,SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"The practice of data augmentation has been extensively used to boost the performance of deep neural networks for various NLP tasks. It is more effective when only a limited number of labeled samples is available, e.g., low-data or classimbalanced regimes. Most current augmentation techniques rely on parameter tuning or inherent randomness; hence, their effectiveness largely varies on the tasks. To efficiently find the best augmentation strategy for each task, learning data augmentation policy is a promising solution, but the question of what makes a good augmentation in NLP tasks and how to design the reward function for learning a good policy remains under-explored. To answer this, we hypothesize that good data augmentation should construct more diverse and challenging samples for providing informative training signals, while avoiding the risk of losing the semantics of original samples. Therefore, we design a novel reward function for updating the augmentation policy to construct difficult but not too different samples (DND). Particularly, we jointly optimize a data augmentation policy while training the model, to construct the augmented samples with low confidence but a high semantic similarity with original ones. In addition, we introduce a sample re-weighting scheme to focus on difficult augmented samples after the original ones are learned confidently for more effective learning from the augmented ones. Our learning-based augmentation outperforms the recent state-of-the-art augmentation schemes on various text classification tasks and GLUE benchmark by successfully discovering the effective augmentations for each task. Remarkably, our method is more effective on the challenging low-data and class-imbalanced regimes, and the learned augmentation policy is well-transferable to the different tasks and models.","This paper proposes a novel reward function for learning data augmentation policy for NLP tasks. The authors propose to jointly optimize a data augmentation policy while training the model, to construct the augmented samples with low confidence but a high semantic similarity with original ones. In addition, they introduce a sample re-weighting scheme to focus on difficult augmented samples after the original ones are learned confidently for more effective learning from the augmented ones. The learning-based augmentation outperforms the state-of-the-art augmentation schemes on various text classification tasks and GLUE benchmark by successfully discovering the effective augmentations for each task."
360,SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"Meta-learning for offline reinforcement learning (OMRL) is an understudied problem with tremendous potential impact by enabling RL algorithms in many realworld applications. A popular solution to the problem is to infer task identity as augmented state using a context-based encoder, for which efficient learning of robust task representations remains an open challenge. In this work, we provably improve upon one of the SOTA OMRL algorithms, FOCAL, by incorporating intra-task attention mechanism and inter-task contrastive learning objectives, to robustify task representation learning against sparse reward and distribution shift. Theoretical analysis and experiments are presented to demonstrate the superior performance and robustness of our end-to-end and model-free framework compared to prior algorithms across multiple meta-RL benchmarks. 1",This paper proposes a novel meta-learning algorithm for offline reinforcement learning. The authors propose to use contrastive learning and intra-task attention to improve the robustness of task representation learning against sparse reward and distribution shift. Theoretical analysis and experiments are presented to demonstrate the superior performance of the proposed algorithm compared to model-free methods.
361,SP:ed86c60850d5c8302dcf1c2167db303e778fe681,"We investigate the challenge of modeling the belief state of a partially observable Markov system, given sample-access to its dynamics model. This problem setting is often approached using parametric sequential generative modeling methods. However, these methods do not leverage any additional computation at inference time to increase their accuracy. Moreover, applying these methods to belief state modeling in multi-agent settings would require passing policies into the belief model—at the time of writing, there have been no successful demonstrations of this. Toward addressing these shortcomings, we propose an inference-time improvement framework for parametric sequential generative modeling methods called belief fine-tuning (BFT). BFT leverages approximate dynamic programming in the form of fine-tuning to determine the model parameters at each time step. It can improve the accuracy of the belief model at test time because it specializes the capacity of the model to the space of local observations. Furthermore, because this specialization occurs after the action or policy has already been decided, BFT does not require the belief model to process it as input. As a result of the latter point, BFT enables, for the first time, approximate public belief state search in imperfect-information games where the number of possible information states is too large to track tabularly. We exhibit these findings on large-scale variants of the benchmark game Hanabi.","This paper proposes a method for modeling the belief state of a partially observable Markov system, given sample-access to its dynamics model. The authors propose an inference-time improvement framework for parametric sequential generative modeling methods called belief fine-tuning (BFT). BFT leverages approximate dynamic programming in the form of fine-tuneing to determine the model parameters at each time step. It can improve the accuracy of the belief model at test time because it specializes the capacity of the model to the space of local observations. BFT enables, for the first time, approximate public belief state search in imperfect-information games where the number of possible information states is too large."
362,SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3× faster than butterfly and speeds up training to achieve favorable accuracy–efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5× faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.","This paper proposes a new method for sparsifying neural networks. The proposed method, Pixelated Butterfly, is based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). The paper also proposes two variants of butterfly matrices (block and flat) to take advantage of modern hardware. Experiments on ImageNet classification and WikiText-103 language modeling tasks show that the proposed method can train up to 2.5x faster than dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy."
363,SP:136e31054a55abca840f6478491972023c2296cb,"Score-based generative models involve sequentially corrupting the data distribution with noise and then learns to recover the data distribution based on score matching. In this paper, for the diffusion probabilistic models, we first delve into the changes of data distribution during the forward process of the Markov chain and explore the class clustering phenomenon. Inspired by the class clustering phenomenon, we devise a novel conditional diffusion probabilistic model by explicitly modeling the class center in the forward and reverse process, and make an elegant modification to the original formulation, which enables controllable generation and gets interpretability. We also provide another direction for faster sampling and more analysis of our method. To verify the effectiveness of the formulated framework, we conduct extensive experiments on multiple tasks, and achieve competitive results compared with the state-of-the-art methods (conditional image generation on CIFAR-10 with an inception score of 9.58 and FID score of 3.05).","This paper proposes a novel conditional diffusion probabilistic model for conditional image generation. The proposed method is based on the class clustering phenomenon. The authors propose to explicitly model the class center in the forward and reverse process, which enables controllable generation and gets interpretability. They also provide another direction for faster sampling and more analysis of their method. The experimental results on CIFAR-10 show that the proposed method achieves competitive results compared with the state-of-the-art methods."
364,SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"To achieve a satisfactory generalization performance on prediction tasks in an unseen domain, existing domain generalization (DG) approaches often rely on the assumption of the existence of fixed domain-invariant features and common hypotheses learned from a set of training domains. While it is a natural and important premise to ground generalization capacity on the target domain, we argue that this assumption could be overly strict and sub-optimal. It is particularly evident when source domains share little information or the target domains leverages information from selective source domains in a compositional way instead of relying on a unique invariant hypothesis across all source domains. Unlike most existing approaches, instead of constructing a single hypothesis shared among domains, we propose a LAtent Sub-Space Orientation (LASSO) method that explores diverse latent sub-spaces and learning individual hypotheses on those sub-spaces. Moreover, in LASSO, since the latent sub-spaces are formed by the label-informative features captured in source domains, they allow us to project target examples onto appropriate sub-spaces while preserving crucial label-informative features for the label prediction task. Finally, we empirically evaluate our method on several wellknown DG benchmarks, where it achieves state-of-the-art results.","-based domain generalization (DG) approaches often rely on the assumption of fixed domain-invariant features and common hypotheses learned from a set of training domains. The authors argue that this assumption could be overly strict and sub-optimal when source domains share little information or the target domains leverages information from selective source domains in a compositional way instead of relying on a unique invariant hypothesis across all source domains. Instead, the authors propose a LASSO method that explores diverse latent sub-spaces and learns individual hypotheses on those sub-space. Moreover, the latent subspaces are formed by the label-informative features captured in source domains, which allows us to project target examples onto appropriate sub-Spaces while preserving crucial label-information features for the label prediction task. Empirical evaluation on several well-known DG benchmarks shows that the proposed method achieves state-of-the-art results."
365,SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a probability distribution more effectively than independent sampling by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth squareroot kernel. Here we provide four improvements. First, we show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RKHS. Second, we show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square-root kernel. Third, we prove that KT with a fractional power kernel yields better-thanMonte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matérn, that do not have square-roots. Fourth, we establish that KT applied to a sum of the target and power kernels (a procedure we call KT+) simultaneously inherits the improved MMD guarantees of power KT and the tighter individual function guarantees of target KT. In our experiments with target KT and KT+, we witness significant improvements in integration error even in 100 dimensions and when compressing challenging differential equation posteriors.","This paper studies the kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) which compresses a probability distribution more effectively than independent sampling by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth squareroot kernel. The authors show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RkHS. They show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square root kernel. They also prove that KT with a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matérn, that do not have square-roots. Finally, they establish that KT+ can be applied to a sum of target and power kernels (a procedure called KT+) simultaneously inherits the improved MMD guarantee of power KT and the tighter individual function guarantees of target KT."
366,SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks (GNNs), the deep learning community has been developing solvers that derive solutions to NP-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make three contributions. First, we present an open-source benchmark suite for the NP-hard MAXIMUM INDEPENDENT SET problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. Second, using our benchmark suite, we conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs. By re-implementing their algorithm with a focus on code quality and extensibility, we show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Instead, the tree search relies on algorithmic techniques like graph kernelization to find good solutions. Thus, the results from the original publication are not reproducible. Third, we extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, we analyze a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality.","This paper presents an open-source benchmark suite for the NP-hard MAXIMUM INDEPENDENT SET problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. The authors conduct an in-depth analysis of the popular guided tree search algorithm by Li et al [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs. They extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, the authors analyze a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality."
367,SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"Quantization is one of the most effective techniques for compressing Convolutional Neural Networks (CNNs), which are known for requiring extensive computational resources. However, aggressive quantization may cause severe degradation in the prediction accuracy of such networks, especially in image-to-image tasks such as semantic segmentation and depth prediction. In this paper, we propose Wavelet Compressed Convolution (WCC)—a novel approach for activation maps compression for 1 × 1 convolutions (the workhorse of modern CNNs). WCC achieves compression ratios and computational savings that are equivalent to low bit quantization rates at a relatively minimal loss of accuracy. To this end, we use a hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. WCC can be utilized with any 1× 1 convolution in an existing network architecture. By combining WCC with light quantization, we show that we achieve compression rates equal to 2-bit and 1-bit with minimal degradation in image-to-image tasks.","This paper proposes Wavelet Compressed Convolution (WCC) for 1x1 convolutional neural networks. The main idea is to use a hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. By combining WCC with light quantization, the authors achieve compression rates equal to 2-bit and 1-bit with minimal degradation in image-to-image tasks."
368,SP:004865e6affad32403b7965493a53c8a7ffdda0a,"A recent emerging trend in the literature on learning in games has been concerned with providing accelerated learning dynamics for correlated and coarse correlated equilibria in normal-form games. Much less is known about the significantly more challenging setting of extensive-form games, which can capture sequential and simultaneous moves, as well as imperfect information. In this paper, we develop faster no-regret learning dynamics for extensive-form correlated equilibrium (EFCE) in multiplayer general-sum imperfect-information extensive-form games. When all agents play T repetitions of the game according to the accelerated dynamics, the correlated distribution of play is an O(T 3/4)-approximate EFCE. This significantly improves over the best prior rate of O(T 1/2). One of our conceptual contributions is to connect predictive (that is, optimistic) regret minimization with the framework of -regret. One of our main technical contributions is to characterize the stability of certain fixed point strategies through a refined perturbation analysis of a structured Markov chain, which may be of independent interest. Finally, experiments on standard benchmarks corroborate our findings.","This paper studies the problem of learning dynamics for extensive-form correlated equilibrium (EFCE) in multiplayer general-sum imperfect-information extensive form games. The authors propose an accelerated learning algorithm for EFCE, which is based on the idea of optimistic regret minimization. The proposed algorithm is evaluated on a set of multiplayer games, where the authors show that their algorithm is able to achieve an O(T 3/4)-approximate state-of-the-art performance. This is a significant improvement over the best prior rate of $O(T 1/2)$."
369,SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"In Reinforcement Learning (RL), discrete actions, as opposed to continuous actions, result in less complex exploration problems and the immediate computation of the maximum of the action-value function which is central to dynamic programmingbased methods. In this paper, we propose a novel method: Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces by leveraging the priors of demonstrations. This dramatically reduces the exploration problem, since the actions faced by the agent not only are in a finite number but also are plausible in light of the demonstrator’s behavior. By discretizing the action space we can apply any discrete action deep RL algorithm to the continuous control problem. We evaluate the proposed method on three different setups: RL with demonstrations, RL with play data –demonstrations of a human playing in an environment but not solving any specific task– and Imitation Learning. For all three setups, we only consider human data, which is more challenging than synthetic data. We found that AQuaDem consistently outperforms state-of-the-art continuous control methods, both in terms of performance and sample efficiency on a variety of hard manipulation tasks.","This paper proposes a method to learn a discretization of continuous action spaces by leveraging the priors of demonstrations. The main idea is to learn the discrete action space by discretizing the action-value function in a way that is similar to that of dynamic programming based methods. The method is evaluated on three different settings: (1) simulated demonstrations, (2) simulated play data, (3) imitation learning, and (4) continuous control tasks. The results show that the proposed method outperforms the state-of-the-art continuous control methods."
370,SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,"In this paper, we consider the problem of domain generalization in semantic segmentation, which aims to learn a robust model using only labeled synthetic (source) data. The model is expected to perform well on unseen real (target) domains. Our study finds that the image style variation can largely influence the model’s performance and the style features can be well represented by the channelwise mean and standard deviation of images. Inspired by this, we propose a novel adversarial style augmentation (AdvStyle) approach, which can dynamically generate hard stylized images during training and thus can effectively prevent the model from overfitting on the source domain. Specifically, AdvStyle regards the style feature as a learnable parameter and updates it by adversarial training. The learned adversarial style feature is used to construct an adversarial image for robust model training. AdvStyle is easy to implement and can be readily applied to different models. Experiments on two synthetic-to-real semantic segmentation benchmarks demonstrate that AdvStyle can significantly improve the model performance on unseen real domains and show that we can achieve the state of the art. Moreover, AdvStyle can be employed to domain generalized image classification and produces a clear improvement on the considered datasets.",This paper proposes an adversarial style augmentation (AdvStyle) method for domain generalization in semantic segmentation. The idea is to dynamically generate hard stylized images during training and thus prevent the model from overfitting on the source domain. Experiments on two synthetic-to-real semantic segmentations benchmarks demonstrate that AdvStyle can significantly improve the model performance on unseen real domains and show that it can achieve the state-of-the-art.
371,SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"Commercial mid-air gesture recognition systems have existed for at least a decade, but they have not become a widespread method of interacting with machines. These systems require rigid, dramatic gestures to be performed for accurate recognition that can be fatiguing and unnatural. To address this limitation, we propose a neuromorphic gesture analysis system which encodes event-based gesture data at high temporal resolution. Our novel approach consists of an event-based guided Variational Autoencoder (VAE) which encodes event-based data sensed by a Dynamic Vision Sensor (DVS) into a latent space representation suitable to compute the similarity of mid-air gesture data. We show that the Hybrid GuidedVAE achieves 87% classification accuracy on the DVSGesture dataset and it can encode the sparse, noisy inputs into an interpretable latent space representation, visualized through T-SNE plots. We also implement the encoder component of the model on neuromorphic hardware and discuss the potential for our algorithm to enable real-time, self-supervised learning of natural mid-air gestures.","This paper proposes a novel method for mid-air gesture recognition. The method is based on a hybrid VAE-VAE architecture that encodes event-based gesture data into a latent space representation, which is then used to compute the similarity between the input gesture and the latent space of a Dynamic Vision Sensor (DVS) sensor. The proposed method is evaluated on the DVSGesture dataset and it is shown that it can encode the sparse, noisy inputs into an interpretable latent representation, visualized through T-SNE plots."
372,SP:2e66468a6b94177e54b0052b97713ee63902c278,"Deep learning for tabular data is drawing increasing attention, with recent work attempting to boost the accuracy of neuron-based networks. However, when computational capacity is low as in Internet of Things (IoT), drone, or Natural User Interface (NUI) application, such deep learning methods are deserted. We offer to enable deep learning capabilities using ferns (oblivious decision trees) instead of neurons, by constructing a Sparse Hierarchical Table Ensemble (S-HTE). S-HTE inference is dense at the beginning of the training process and becomes gradually sparse using an annealing mechanism, leading to an efficient final predictor. Unlike previous work with ferns, S-HTE learns useful internal representations, and it earns from increasing depth. Using a standard classification and regression benchmark, we show its accuracy is comparable to alternatives, while having an order of magnitude lower computational complexity. Our PyTorch implementation is available at https://anonymous.4open.science/r/HTE_ CTE-60EB/.","This paper proposes a method for training sparse hierarchical table ensembles for tabular data. The method is based on ferns (oblivious decision trees), which is an extension of fern-based decision trees. The main idea is to use an annealing mechanism to gradually reduce the number of nodes in the fern tree. The paper shows that the proposed method is able to achieve better performance than the state-of-the-art in classification and regression tasks, while having a much lower computational complexity."
373,SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper tackles the problem of learning value functions from undirected stateonly experience (state transitions without action labels i.e. (s, s′, r) tuples). We first theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-learning or LAQ, an offline RL method that can learn effective value functions from state-only experience. Latent Action Q-learning (LAQ) learns value functions using Q-learning on discrete latent actions obtained through a latent-variable future prediction model. We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. Value functions learned using LAQ lead to sample efficient acquisition of goal-directed behavior, can be used with domain-specific low-level controllers, and facilitate transfer across embodiments. Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods.","This paper proposes a method for learning value functions from undirected state-only experience (i.e. state transitions without action labels). The authors first theoretically characterize the applicability of tabular Q-learning in discrete Markov decision processes (MDPs) and show that it can learn the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-Learning (LAQ), an offline RL method that can learn effective value function from state only experience. The authors show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. The experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives."
374,SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"Many deep learning applications benefit from using large models with billions of parameters. These models can only be trained with specialized distributed training algorithms that require low-latency and high-bandwidth interconnect. As a result, large models are typically trained in dedicated GPU clusters that can be extremely costly to deploy and operate. In contrast, there are more affordable distributed training setups, such as using cheap “preemptible” instances or pooling together existing resources from multiple regions. However, both these setups come with unique challenges that make it impractical to train large models using conventional model parallelism. In this work, we carefully analyze these challenges and find configurations where training larger models becomes less communication-intensive. Based on these observations, we propose SWARM Parallelism1 — a model-parallel training algorithm designed for swarms of poorly connected, heterogeneous unreliable devices. SWARM creates temporary randomized pipelines between available nodes that are rebalanced in case of failure. To further reduce the network usage of our approach, we develop several compression-aware architecture modifications and evaluate their tradeoffs. Finally, we combine our insights to train a large Transformer language model with 1.1B shared parameters (≈13B before sharing) on a swarm of preemptible T4 GPUs with less than 400Mb/s network throughput.","This paper proposes SWARM Parallelism, a model parallelism method for training large models in a distributed manner. The main idea of SWARM is to train a model in a swarms of poorly connected, heterogeneous unreliable devices. The authors propose to use randomized pipelines between available nodes that are rebalanced in case of failure. To further reduce the network usage, the authors develop several compression-aware architecture modifications and evaluate their tradeoffs. Experiments are conducted on a large Transformer language model on a swarm of preemptible T4 GPUs with less than 400Mb/s network throughput."
375,SP:91d2f094d5481651b554f58aecc2a6207057a47c,"Offline reinforcement learning could learn effective policies from a fixed dataset, which is promising in real-world applications. However, in offline decentralized multi-agent reinforcement learning, due to the discrepancy between the behavior policy and learned policy, the transition dynamics in offline experiences do not accord with the transition dynamics in online execution, which creates severe errors in value estimates, leading to uncoordinated and suboptimal policies. One way to overcome the transition bias is to bridge offline training and online tuning. However, considering both deployment efficiency and sample efficiency, we could only collect very limited online experiences, making it insufficient to use merely online data for updating the agent policy. To utilize both offline and online experiences to tune the policies of agents, we introduce online transition correction (OTC) to implicitly correct the biased transition dynamics by modifying sampling probabilities. We design two types of distances, i.e., embedding-based and valuebased distance, to measure the similarity between transitions, and further propose an adaptive rank-based prioritization to sample transitions according to the transition similarity. OTC is simple yet effective to increase data efficiency and improve agent policies in online tuning. Empirically, we show that OTC outperforms baselines in a variety of tasks.","This paper proposes an online transition correction (OTC) method for offline decentralized multi-agent reinforcement learning. The authors argue that the transition dynamics in offline experiences do not accord with the transitions dynamics in online execution, which creates severe errors in value estimates, leading to uncoordinated and suboptimal policies. To address this issue, the authors propose two types of distances, i.e., embedding-based and value-based distance, to measure the similarity between transitions, and propose an adaptive rank-based prioritization to sample transitions according to the transition similarity. Empirical results show that OTC outperforms baselines in a variety of tasks."
376,SP:d0e650d568214481b07a0452ec606ccbf6d05410,"Quantization of the weights and activations is one of the main methods to reduce the computational footprint of Deep Neural Networks (DNNs) training. Current methods enable 4-bit quantization of the forward phase. However, this constitutes only a third of the training process. Reducing the computational footprint of the entire training process requires the quantization of the neural gradients, i.e., the loss gradients with respect to the outputs of intermediate neural layers. In this work, we examine the importance of having unbiased quantization in quantized neural network training, where to maintain it, and how. Based on this, we suggest a logarithmic unbiased quantization (LUQ) method to quantize both the forward and backward phase to 4-bit, achieving state-of-the-art results in 4-bit training. For example, in ResNet50 on ImageNet, we achieved a degradation of 1.18%; we further improve this to degradation of only 0.64% after a single epoch of high precision fine-tuning combined with a variance reduction method. Finally, we suggest a method that exploits the low precision format by avoiding multiplications during two-thirds of the training process, thus reducing by 5x the area used by the multiplier. A reference implementation is supplied in the supplementary material.",This paper proposes a method for quantizing the forward and backward phase of neural network training. The authors propose a logarithmic unbiased quantization (LUQ) method to quantize both the forward phase and the backward phase to 4-bit. The proposed method achieves state-of-the-art results on ResNet50 on ImageNet with a degradation of only 0.64% after a single epoch of high precision fine-tuning combined with a variance reduction method. A reference implementation is provided in the supplementary material.
377,SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"Polythetic classifications, based on shared patterns of features that need neither be universal nor constant among members of a class, are common in the natural world and greatly outnumber monothetic classifications over a set of features. We show that threshold meta-learners, such as Prototypical Networks, require an embedding dimension that is exponential in the number of task-relevant features to emulate these functions. In contrast, attentional classifiers, such as Matching Networks, are polythetic by default and able to solve these problems with a linear embedding dimension. However, we find that in the presence of task-irrelevant features, inherent to meta-learning problems, attentional models are susceptible to misclassification. To address this challenge, we propose a selfattention feature-selection mechanism that adaptively dilutes non-discriminative features. We demonstrate the effectiveness of our approach in meta-learning Boolean functions, and synthetic and real-world few-shot learning tasks.","This paper proposes a self-attention feature-selection mechanism for meta-learning in the presence of task-irrelevant features. The authors show that threshold meta-learners, such as Prototypical Networks, require an embedding dimension that is exponential in the number of task relevant features to emulate these functions. In contrast, attentional classifiers such as Matching Networks are polythetic by default and are able to solve these problems with a linear embedding dimensions. To address this challenge, the authors propose to adaptively dilute non-discriminative features. They demonstrate the effectiveness of their approach in meta learning Boolean functions, and synthetic and real-world few-shot learning tasks."
378,SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"While multi-agent reinforcement learning has been used as an effective means to study emergent communication between agents, existing work has focused almost exclusively on communication with discrete symbols. Human communication often takes place (and emerged) over a continuous acoustic channel; human infants acquire language in large part through continuous signalling with their caregivers. We therefore ask: Are we able to observe emergent language between agents with a continuous communication channel trained through reinforcement learning? And if so, what is the impact of channel characteristics on the emerging language? We propose an environment and training methodology to serve as a means to carry out an initial exploration of these questions. We use a simple messaging environment where a “speaker” agent needs to convey a concept to a “listener”. The Speaker is equipped with a vocoder that maps symbols to a continuous waveform, this is passed over a lossy continuous channel, and the Listener needs to map the continuous signal to the concept. Using deep Q-learning, we show that basic compositionality emerges in the learned language representations. We find that noise is essential in the communication channel when conveying unseen concept combinations. And we show that we can ground the emergent communication by introducing a caregiver predisposed to “hearing” or “speaking” English. Finally, we describe how our platform serves as a starting point for future work that uses a combination of deep reinforcement learning and multi-agent systems to study our questions of continuous signalling in language learning and emergence.","This paper proposes a method to study emergent communication between agents in a multi-agent reinforcement learning setting. The authors use a simple messaging environment where a speaker agent needs to convey a concept to a listener agent. The speaker is equipped with a vocoder that maps symbols to a continuous waveform, and the listener needs to map the continuous signal to the concept. The communication channel is a lossy continuous channel, which is used to train a Q-learning agent that maps the speaker’s concept to the listener. The experiments show that the speaker is able to learn compositionality in the learned language representations, and that noise is essential in the communication channel when conveying unseen concept combinations."
379,SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose BadPre, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.",This paper proposes a novel backdoor attack against pre-trained NLP models. The key idea is to use a pre-defined trigger word in the input text that causes model misprediction. The authors also propose a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that their approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.
380,SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"Reward-free, unsupervised discovery of skills is an attractive alternative to the bottleneck of hand-designing rewards in environments where task supervision is scarce or expensive. However, current skill pre-training methods, like many RL techniques, make a fundamental assumption – stationary environments during training. Traditional methods learn all their skills simultaneously, which makes it difficult for them to both quickly adapt to changes in the environment, and to not forget earlier skills after such adaptation. On the other hand, in an evolving or expanding environment, skill learning must be able to adapt fast to new environment situations while not forgetting previously learned skills. These two conditions make it difficult for classic skill discovery to do well in an evolving environment. In this work, we propose a new framework for skill discovery, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn’t forget a learned skill. We demonstrate experimentally that in both evolving and static environments, incremental skills significantly outperform current state-of-the-art skill discovery methods on both skill quality and the ability to solve downstream tasks. Videos for learned skills and code are made public on: https://sites.google.com/view/ discovery-of-incremental-skill.","This paper proposes an incremental skill discovery method for skill discovery, where skills are learned one after another in an incremental fashion. The idea is to learn skills in a way that allows the agent to adapt fast to new environments while not forgetting previously learned skills. The proposed method is evaluated in both evolving and static environments and compared to several state-of-the-art skill discovery methods. The experimental results show that the proposed method outperforms the existing methods on both skill quality and downstream tasks."
381,SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"Convolutional neural networks use regular quadrilateral convolution kernels to extract features. Since the number of parameters increases quadratically with the size of the convolution kernel, many popular models use small convolution kernels, resulting in small local receptive fields in lower layers. This paper proposes a novel log-polar space convolution (LPSC) layer, where the convolution kernel is elliptical and adaptively divides its local receptive field into different regions according to the relative directions and logarithmic distances. The local receptive field grows exponentially with the number of distance levels. Therefore, the proposed LPSC not only naturally encodes local spatial structures, but also greatly increases the single-layer receptive field while maintaining the number of parameters. We show that LPSC can be implemented with conventional convolution via log-polar space pooling and can be applied in any network architecture to replace conventional convolutions. Experiments on different tasks and datasets demonstrate the effectiveness of the proposed LPSC.","This paper proposes a novel log-polar space convolution (LPSC) layer, where the convolution kernel is elliptical and adaptively divides its local receptive field into different regions according to the relative directions and logarithmic distances. The proposed LPSC not only naturally encodes local spatial structures, but also greatly increases the single-layer receptive field while maintaining the number of parameters. Experiments on different tasks and datasets demonstrate the effectiveness of the proposed method."
382,SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem. However, no solution of IIW has ever been provided, which builds a barrier for further investigation of the IIW’s property and its potential in practical deep learning. In this paper, we propose an algorithm for the efficient approximation of IIW. Then, we build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the fitting to compressing phase transition during NNs’ training and the concrete connection between the IIW compression and the generalization. Besides, we verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, overparameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice.","This paper studies the role of the information bottleneck (IIW) in the generalization ability of neural networks (NNs). The authors propose an algorithm for the efficient approximation of IIW based on the PAC-Bayes theorem. The authors also propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of the IIW in enhancing NNs in practice. "
383,SP:a733847ade77ffbf38760fc79da17893dea8d53f,"Indiscriminate data poisoning attacks, which add imperceptible perturbations to training data to maximize the test error1, have become a trendy topic because they are thought to be capable of preventing unauthorized use of data. In this work, we investigate why these attacks work in principle. We find that the perturbations of advanced attacks are almost linear separable when assigned with the target labels of the corresponding samples. This is an important population property for various perturbations that were not unveiled before. Moreover, we further confirm that linear separability is indeed the workhorse for recent attacks. We synthesize linear separable data as perturbations and show such synthetic perturbations are as powerful as the deliberately crafted attacks. Our finding also suggests that the shortcut learning problem is more serious than previously believed as deep models heavily relies on shortcuts even if they are of an imperceptible scale and mixed together with the normal features. It also suggests that pre-trained feature extractors can be a powerful defense.","This paper studies data poisoning attacks, which add imperceptible perturbations to training data to maximize the test error. The authors show that the perturbation of advanced attacks are almost linear separable when assigned with the target labels of the corresponding samples. They further confirm that linear separability is indeed the workhorse for recent attacks. The paper also shows that the shortcut learning problem is more serious than previously believed as deep models heavily relies on shortcuts even if they are of an imperceptibly scale and mixed together with the normal features. It also suggests that feature extractors can be a powerful defense."
384,SP:7b50be406138ad01db3ee112899f622637896fe9,"Offline policy optimization has a critical impact on many real-world decisionmaking problems, as online learning is costly and concerning in many applications. Importance sampling and its variants are a widely used type of estimator in offline policy evaluation, which can be helpful to remove assumptions on the chosen function approximations used to represent value functions and process models. In this paper, we identify an important overfitting phenomenon in optimizing the importance weighted return, and propose an algorithm to avoid this overfitting. We provide a theoretical justification of the proposed algorithm through a better per-state-neighborhood normalization condition and show the limitation of previous attempts to this approach through an illustrative example. We further test our proposed method in a healthcare-inspired simulator and a logged dataset collected from real hospitals. These experiments show the proposed method with less overfitting and better test performance compared with state-of-the-art batch reinforcement learning algorithms.",This paper proposes a new algorithm for offline policy evaluation based on importance sampling. The main idea of the paper is to use a per-state-neighborhood normalization condition to reduce the overfitting of the importance weighted return. The paper provides a theoretical justification of the proposed algorithm through a better per-states-narrowhood condition and shows the limitation of previous attempts to this approach through an illustrative example. The experiments show the proposed method with less overfitting and better test performance compared with state-of-the-art batch reinforcement learning algorithms.
385,SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper presents CoLLIE: a simple, yet effective model for continual learning of how language is grounded in vision. Given a pre-trained multimodal embedding model, where language and images are projected in the same semantic space (in this case CLIP by OpenAI), CoLLIE learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. Unlike traditional few-shot learning, the model does not just learn new classes and labels, but can also generalize to similar language use. We verify the model’s performance on two different tasks of continual learning and show that it can efficiently learn and generalize from only a few examples, with little interference with the model’s original zero-shot performance.","This paper presents CoLLIE, a model for continual learning of how language is grounded in vision. The model learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. Unlike traditional few-shot learning, the model does not just learn new classes and labels, but can also generalize to similar language uses. The authors verify the model’s performance on two different tasks of continual learning and show that it can efficiently learn and generalize from only a few examples."
386,SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"Novel object captioning (NOC) learns image captioning models for describing objects or visual concepts which are unseen (i.e., novel) in the training captions. Such captioning models need to sufficiently describe such visual data with fluent and natural language expression. In other words, we expect the produced captions being linguistically fluent, containing novel objects of interest, and fitting the visual concept of the image. The above three aspects thus correspond to fluency, fidelity, and adequacy, respectively. However, most novel object captioning models are not explicitly designed to address the aforementioned properties due to the absence of caption annotations. In this paper, we start by providing an insight into the relationship between the above properties and existing visual/language models. Then, we present VLAF2, for learning Visual-Linguistic Adequacy, Fidelity, and Fluency, which utilizes linguistics observed from captions for describing visual information of images with novel objects. More specifically, we revisit BERT and CLIP, and explain how we leverage the intrinsic language knowledge from such popular models to reward captions with precise and rich visual content associated with novel images. To validate the effectiveness of our framework, we conduct extensive experiments on the nocaps dataset. Our method not only performs favorably against state-of-the-art novel captioning models in all caption evaluation metrics, but also surpasses the SPICE scores of human baseline. We perform quantitative and qualitative analysis to demonstrate how our model generates novel object captions with improved fluency, fidelity, and adequacy. Implementation details and code are available in the supplementary materials.","This paper proposes VLAF2, a novel method for novel object captioning (NOC) that aims to improve the fluency, fidelity, and adequacy of the captions produced by BERT, CLIP, and other existing NOC models. The key idea is to leverage the intrinsic language knowledge from such popular models to reward captions with precise and rich visual content associated with novel images. Experiments are conducted on the nocaps dataset to validate the effectiveness of the proposed method."
387,SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. In this paper we provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. We demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and – more importantly – to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting.","This paper studies the phenomenon of neural collapse, which is the phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. The authors show that neural collapse generalizes to new samples from the training classes, and – more importantly – to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting. The paper is well-written and well-motivated."
388,SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"Point cloud obtained from 3D scanning is often sparse, noisy, and irregular. To cope with these issues, recent studies have been separately conducted to densify, denoise, and complete inaccurate point cloud. In this paper, we advocate that jointly solving these tasks leads to significant improvement for point cloud reconstruction. To this end, we propose a deep point cloud reconstruction network consisting of two stages: 1) a 3D sparse stacked-hourglass network as for the initial densification and denoising, 2) a refinement via transformers converting the discrete voxels into 3D points. In particular, we further improve the performance of transformer by a newly proposed module called amplified positional encoding. This module has been designed to differently amplify the magnitude of positional encoding vectors based on the points’ distances for adaptive refinements. Extensive experiments demonstrate that our network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets. Moreover, we underline the ability of our network to generalize toward real-world and unmet scenes. Implementation will be released after the review.","This paper proposes a deep point cloud reconstruction network consisting of two stages: 1) a 3D sparse stacked-hourglass network as for the initial densification and denoising, 2) a refinement via transformers converting the discrete voxels into 3D points. In particular, the proposed module called amplified positional encoding is designed to differently amplify the magnitude of positional encoding vectors based on the points’ distances for adaptive refinements. Extensive experiments demonstrate that the proposed network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets."
389,SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data, and training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and feature gradients among partitions for every GCN layer in each training iteration, limiting the achievable training efficiency and model scalability. To this end, we propose PipeGCN, a simple-yet-effective scheme that hides the communication overhead by pipelining inter-partition communication with intra-partition computation. It is non-trivial to pipeline for efficient GCN training, as communicated node features/gradients will become stale and thus can harm the convergence, negating the pipeline benefit. Notably, little is known regarding the convergence rate of GCN training with both stale features and stale feature gradients. This work not only provides a theoretical convergence guarantee but also finds the convergence rate of PipeGCN to be close to that of the vanilla distributed GCN training without staleness. Furthermore, we develop a smoothing method to further improve PipeGCN’s convergence. Extensive experiments show that PipeGCN can largely boost training throughput (up to 2.2×) while achieving the same accuracy as its vanilla counterpart and outperforming existing full-graph training methods. All code will be released publicly upon acceptance.","This paper proposes PipeGCN, a new method for distributed GCN training. The main idea is to use inter-partition communication to hide the communication overhead of communicating node features and feature gradients among partitions for every GCN layer in each training iteration. The paper also provides a theoretical convergence guarantee and shows that the convergence rate of the proposed method is close to that of the vanilla distributed GCNs training without staleness. Furthermore, a smoothing method is proposed to further improve the convergence of the method. Extensive experiments show that the proposed pipeline can largely boost training throughput while achieving the same accuracy as its vanilla counterpart and outperforming existing full-graph training methods."
390,SP:8302d49558ee0f16392d623d4e604e92db10d041,"While deep neural networks can attain good accuracy on in-distribution test points, many applications require robustness even in the face of unexpected perturbations in the input, changes in the domain, or other sources of distribution shift. We study the problem of test time robustification, i.e., using the test input to improve model robustness. Recent prior works have proposed methods for test time adaptation, however, they each introduce additional assumptions, such as access to multiple test points, that prevent widespread adoption. In this work, we aim to study and devise methods that make no assumptions about the model training process and are broadly applicable at test time. We propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model’s average, or marginal, output distribution across the augmentations. Intuitively, this objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentations, while also maintaining confidence in its predictions. In our experiments, we evaluate two baseline ResNet models, two robust ResNet-50 models, and a robust vision transformer model, and we demonstrate that this approach achieves accuracy gains of 1-8% over standard model evaluation and also generally outperforms prior augmentation and adaptation strategies. For the setting in which only one test point is available, we achieve state-of-the-art results on the ImageNet-C, ImageNet-R, and, among ResNet-50 models, ImageNet-A distribution shift benchmarks.","This paper studies the problem of test-time adaptation, i.e., using the test input to improve model robustness. The authors propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model’s average, or marginal, output distribution across the augmentations. Intuitively, this objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentations and maintaining confidence in its predictions. The experimental results show that this approach achieves accuracy gains of 1-8% over standard model evaluation and also generally outperforms prior augmentation and adaptation strategies."
391,SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"Many model-based reinforcement learning (RL) methods follow a similar template: fit a model to previously observed data, and then use data from that model for RL or planning. However, models that achieve better training performance (e.g., lower MSE) are not necessarily better for control: an RL agent may seek out the small fraction of states where an accurate model makes mistakes, or it might act in ways that do not expose the errors of an inaccurate model. As noted in prior work, there is an objective mismatch: models are useful if they yield good policies, but they are trained to maximize their accuracy, rather than the performance of the policies that result from them. In this work, we propose a single objective for jointly training the model and the policy, such that updates to either component increase a lower bound on expected return. This joint optimization mends the objective mismatch in prior work. Our objective is a global lower bound on expected return, and this bound becomes tight under certain assumptions. The resulting algorithm (MnM) is conceptually similar to a GAN: a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policy is updated to avoid states where the model predictions are unrealistic.","This paper proposes a method for jointly training the model and the policy in a model-based reinforcement learning (RL) setting. The main idea is to jointly train both the policy and the model, such that updates to either component increase a lower bound on expected return. This joint optimization mends the objective mismatch in prior work. The proposed algorithm (MnM) is conceptually similar to a GAN, where a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policies are updated to avoid states where model predictions are unrealistic."
392,SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"When operating in partially observed settings, it is important for a control policy to fuse information from a history of observations. However, a naive implementation of this approach has been observed repeatedly to fail for imitation-learned policies, often in surprising ways, and to the point of sometimes performing worse than when using instantaneous observations alone. We observe that behavioral cloning policies acting on single observations and observation histories each have their strengths and drawbacks, and combining them optimally could achieve the best of both worlds. Motivated by this, we propose a simple model combination approach inspired by human decision making: we first compute a coarse action based on the instantaneous observation, and then refine it into a final action using historical information. Our experiments show that this outperforms all baselines on CARLA autonomous driving from images and various MuJoCo continuous control tasks. BC-SO BC-OH Our model Input Image Figure 1: Attention maps of baseline imitation methods and our method, depicted on the CARLA driving task. Behavioral cloning from single observation (BC-SO) attends to the appropriate visual cues in the scene (the traffic light and pedestrian), but performs suboptimally due to lack of information. Behavioral cloning from observation history (BC-OH) has access to all required information, but manifests the “copycat problem”; it relies excessively on extrapolating past actions, and fails to attend to important visual cues. Our “coarse-to-fine” imitator combines the advantages of each method, and outperforms both.","This paper proposes a method for imitation learning in partially-observable environments. The authors propose to combine behavioral cloning from single observations and observation history, and propose a coarse-to-fine imitation model that combines the advantages of both methods. The proposed method is evaluated on CARLA autonomous driving from images and various MuJoCo continuous control tasks. The experimental results show that the proposed method outperforms the baselines."
393,SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"Current deep learning models for dynamics forecasting struggle with generalization. They can only forecast in a specific domain and fail when applied to systems with different parameters, external forces, or boundary conditions. We propose a model-based meta-learning method called DyAd which can generalize across heterogeneous domains by partitioning them into different tasks. DyAd has two parts: an encoder which infers the time-invariant hidden features of the task with weak supervision, and a forecaster which learns the shared dynamics of the entire domain. The encoder adapts and controls the forecaster during inference using adaptive instance normalization and adaptive padding. Theoretically, we prove that the generalization error of such procedure is related to the task relatedness in the source domain, as well as the domain differences between source and target. Experimentally, we demonstrate that our model outperforms state-of-the-art approaches on both turbulent flow and real-world ocean data forecasting tasks.","This paper proposes a model-based meta-learning method called DyAd for dynamics forecasting. The model consists of two parts: an encoder which infers the time-invariant hidden features of the task with weak supervision, and a forecaster which learns the shared dynamics of the entire domain. The encoder adapts and controls the forecaster during inference using adaptive instance normalization and adaptive padding. Theoretically, the authors prove that the generalization error of such procedure is related to the task relatedness in the source domain, as well as the domain differences between source and target. Experiments on both turbulent flow and real-world ocean data forecasting tasks demonstrate that DyAd outperforms state-of-the-art approaches."
394,SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Without using any 3D box label for training, our method obtains 16.47 AP in KITTI, which even outperforms many prior fully supervised methods. Codes will be made publicly available soon.","This paper proposes a method for weakly supervised monocular 3D object detection. The proposed method first generates 2D boxes on the image and then uses them to select corresponding RoI LiDAR points as the weak supervision. Then, a network is trained to predict 3D boxes which can tightly align with associated RoI points. This network is learned by minimizing the 3D alignment loss between the 2D box estimates and the corresponding corresponding LiDar points. Experiments on KITTI show that the proposed method outperforms the state-of-the-art methods."
395,SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce CHARFORMER, a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that CHARFORMER outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, CHARFORMER is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.","This paper proposes a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, the authors introduce a soft gradient-based subword-based module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. The authors also introduce a deep Transformer model that integrates GBST and operates on the byte level. Experiments on English GLUE, multilingual, and noisy text datasets show that the proposed model outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword based models."
396,SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor is often embedded in the target DNNs through injecting a backdoor trigger into training examples, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Existing backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device deployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution; a singularity is often observed in the adversarial map of a backdoorinfected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis (AEVA) to detect backdoors in black-box neural networks. AEVA is based on an extreme value analysis of the adversarial map, computed from the monte-carlo gradient estimation. Evidenced by extensive experiments across multiple popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios.","This paper proposes a method to detect backdoor attacks in black-box hard-label settings where only the final output label of the target network is available. The authors show that the objective of backdoor detection is bounded by an adversarial objective, which leads to a singularity in the adversarial map of a backdoor-infected example, which they call adversarial singularity phenomenon. Based on this observation, the authors propose an approach called adversarial extreme value analysis (AEVA) to detect backdoors in black box neural networks. AEVA is based on the monte-carlo gradient estimation. Experimental results show the effectiveness of AEVA in detecting backdoor attacks under the black box hard label scenarios."
397,SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"Reliable uncertainty estimation is crucial when deploying a classifier in the wild. In this paper, we tackle the challenge of jointly quantifying in-distribution and out-of-distribution (OOD) uncertainties. To this end, we leverage the secondorder uncertainty representation provided by evidential models and we introduce KLoS, a Kullback–Leibler divergence criterion defined on the class-probability simplex. By keeping the full distributional information, KLoS captures class confusion and lack of evidence in a single score. A crucial property of KLoS is to be a class-wise divergence measure built from in-distribution samples and to not require OOD training data, in contrast to current second-order uncertainty measures. We further design an auxiliary neural network, KLoSNet, to learn a refined criterion directly aligned with the evidential training objective. In the realistic context where no OOD data is available during training, our experiments show that KLoSNet outperforms first-order and second-order uncertainty measures to simultaneously detect misclassifications and OOD samples. When training with OOD samples, we also observe that existing measures are brittle to the choice of the OOD dataset, whereas KLoS remains more robust.","This paper proposes a new uncertainty measure, Kullback-leibler divergence (KLoS) for class-probability simplex, which combines in-distribution and OOD uncertainty measures. KLoS is a class-wise divergence measure that does not require OOD training data. The paper also proposes an auxiliary neural network, KloSNet, to learn a refined criterion directly aligned with the evidential training objective. Experiments are conducted on CIFAR-10 and ImageNet, and the proposed method is shown to be more robust to OOD data."
398,SP:8b4f3916dca4e627931558e14836749bd4a6792f,"Convolutional networks (CNN) are computationally hard to learn. In practice, however, CNNs are learned successfully on natural image data. In this work, we study a semi-supervised algorithm, that learns a linear classifier over datadependent features which were obtained from unlabeled data. We show that the algorithm provably learns CNNs, under some natural distributional assumptions. Specifically, it efficiently learns CNNs, assuming the distribution of patches in the input images has low-dimensional structure (e.g., when the patches are sampled from a low-dimensional manifold). We complement our result with a lower bound, showing that the dependence of our algorithm on the dimension of the patch distribution is essentially optimal.","This paper studies a semi-supervised method for learning convolutional neural networks (CNNs) from unlabeled data. The authors show that the algorithm provably learns CNNs, under some natural distributional assumptions. Specifically, they show that if the distribution of patches in the input images has low-dimensional structure (e.g., when the patches are sampled from a low dimensional manifold), then the algorithm can efficiently learn CNNs. They also provide a lower bound that shows that the dependence of the algorithm on the dimension of the patch distribution is essentially optimal."
399,SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"Face clustering has attracted rising research interest recently to take advantage of massive amounts of face images on the web. State-of-the-art performance has been achieved by Graph Convolutional Networks (GCN) due to their powerful representation capacity. However, existing GCN-based methods build face graphs mainly according to kNN relations in the feature space, which may lead to a lot of noise edges connecting two faces of different classes. The face features will be polluted when messages pass along these noise edges, thus degrading the performance of GCNs. In this paper, a novel algorithm named Ada-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In Ada-NETS, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCNs to cluster faces. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods, proving its superiority and generalization.","This paper proposes a novel algorithm for face clustering based on Graph Convolutional Networks (GCN). The main idea of the paper is to train a GCN to cluster faces by constructing clean graphs for GCNs. In particular, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods, proving its superiority and generalization."
400,SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"Generalizable Person Re-Identification (DG ReID) aims to learn ready-to-use crossdomain representations for direct cross-data evaluation. It typically fully exploit demographics information, e.g., the domain information and camera IDs to learn features that are domain-invariant. However, the protected demographic features are not often accessible due to privacy and regulation issues. Under this more realistic setting, distributionally robust optimization (DRO) provides a promising way for learning robust models that are able to perform well on a collection of possible data distributions (the “uncertainty set”) without demographics. However, the convex condition of KL DRO may not hold for overparameterized neural networks and applying KL DRO fails to generalize under distribution shifts in real scenarios. Instead, by applying the change-of-measure technique and the analytical solution of KL DRO, we propose a simple yet efficient approach, Unit DRO. Unit DRO minimizes the loss over a reweighted dataset where important samples (i.e., samples on which models perform poorly) will be upweighted and others will be downweighted. Empirical results show that Unit DRO achieves superior performance on large-scale DG ReID and cross-domain ReID benchmarks compared to standard baselines.","This paper proposes a new method for generalizable person re-identification (DG ReID) based on distributionally robust optimization (DRO). The authors argue that the convex condition of KL DRO may not hold for overparameterized neural networks and apply KL DRo fails to generalize under distribution shifts in real scenarios. Instead, the authors propose a simple yet efficient approach, Unit DRO, which minimizes the loss over a reweighted dataset where important samples (i.e., samples on which models perform poorly) will be upweighted and others will be downweighted. Empirical results show that the proposed method achieves superior performance on large-scale DG ReID and cross-domain ReID benchmarks compared to standard baselines."
401,SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"Graph Neural Networks (GNNs) have been proven effective across a wide range of molecular property prediction and structured learning problems. However, their efficiency is known to be hindered by practical challenges such as oversmoothing. We introduce “Noisy Nodes”, a very simple technique for improved training of GNNs, in which we corrupt the input graph with noise, and add a noise correcting node-level loss. Adding noise helps overfitting, and the noise correction loss helps ameliorate oversmoothing by encouraging diverse node latents. Our regulariser applies well-studied methods in simple, straightforward ways which allows even generic architectures not designed for quantum chemistry to achieve state of the art results. We also demonstrate the effectiveness of Noisy Nodes with non-spatial architectures on Open Graph Benchmark (OGB) datasets. Our results suggest Noisy Nodes can serve as a complementary building block in the GNN toolkit for 3D molecular property prediction and beyond.","This paper proposes a novel noise-based regularizer for GNNs to improve the training of graph neural networks (GNNs). Specifically, the authors propose to add noise to the input graph and add a noise correcting node-level loss. The authors show that adding noise helps overfitting, and the noise correction loss helps ameliorate oversmoothing by encouraging diverse node latents. Experiments are conducted on the Open Graph Benchmark (OGB) dataset to demonstrate the effectiveness of the proposed method."
402,SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"We tackle the set2vec problem, the task of extracting a vector representation from an input set comprised of a variable number of feature vectors. Although recent approaches based on self attention such as (Set)Transformers were very successful due to the capability of capturing complex interaction between set elements, the computational overhead is the well-known downside. The inducing-point attention and the latest optimal transport kernel embedding (OTKE) are promising remedies that attain comparable or better performance with reduced computational cost, by incorporating a fixed number of learnable queries in attention. In this paper we approach the set2vec problem from a completely different perspective. The elements of an input set are considered as i.i.d. samples from a mixture distribution, and we define our set embedding feed-forward network as the maximum-a-posterior (MAP) estimate of the mixture which is approximately attained by a few ExpectationMaximization (EM) steps. The whole MAP-EM steps are differentiable operations with a fixed number of mixture parameters, allowing efficient auto-diff backpropagation for any given downstream task. Furthermore, the proposed mixture set data fitting framework allows unsupervised set representation learning naturally via marginal likelihood maximization aka the empirical Bayes. Interestingly, we also find that OTKE can be seen as a special case of our framework, specifically a single-step EM with extra balanced assignment constraints on the E-step. Compared to OTKE, our approach provides more flexible set embedding as well as prior-induced model regularization. We evaluate our approach on various tasks demonstrating improved performance over the state-of-the-arts.","This paper proposes a new approach to the set2vec problem, which is the task of extracting a vector representation from an input set comprised of a variable number of feature vectors. The authors propose to use the maximum-a-posterior (MAP) estimate of the mixture distribution which is approximately attained by a few ExpectationMaximization (EM) steps. The whole MAP-EM steps are differentiable operations with a fixed number of mixture parameters, allowing efficient auto-diff backpropagation for any given downstream task. Furthermore, the proposed mixture set data fitting framework allows unsupervised set representation learning naturally via marginal likelihood maximization aka the empirical Bayes. Compared to OTKE, this approach provides more flexible set embedding as well as prior-induced model regularization."
403,SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"The goal of unsupervised feature selection is to select a small number of informative features for use in unknown downstream tasks. Here the definition of “informative” is subjective and dependent on the specifics of a given problem domain. In the contrastive analysis (CA) setting, machine learning practitioners are specifically interested in discovering patterns that are enriched in a target dataset as compared to a background dataset generated from sources of variation irrelevant to the task at hand. For example, a biomedical data analyst may wish to find a small set of genes to use as a proxy for variations in genomic data only present among patients with a given disease as opposed to healthy control subjects. However, as of yet the problem of feature selection in the CA setting has received little attention from the machine learning community. In this work we present CFS (Contrastive Feature Selection), a method for performing feature selection in the CA setting. We experiment with multiple variations of our method on a semi-synthetic dataset and four real-world biomedical datasets, and we find that it consistently outperforms previous state-of-the-art methods designed for standard unsupervised feature selection scenarios.","This paper proposes a method for unsupervised feature selection in contrastive analysis (CA) where the goal is to select a small number of informative features for use in unknown downstream tasks. The proposed method, CFS (Contrastive Feature Selection), is based on the idea of contrastive feature selection (CFS), which is an extension of the contrastive contrastive learning (CFL) method. CFS is evaluated on a semi-synthetic dataset and four real-world biomedical datasets, and it consistently outperforms previous state-of-the-art methods designed for standard feature selection scenarios."
404,SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"Early stopping is a simple and widely used method to prevent over-training neural networks. We develop theoretical results to reveal the relationship between optimal early stopping time and model dimension as well as sample size of the dataset for certain linear regression models. Our results demonstrate two very different behaviors when the model dimension exceeds the number of features versus the opposite scenario. While most previous works on linear models focus on the latter setting, we observe that in common deep learning tasks, the dimension of the model often exceeds the number of features arising from data. We demonstrate experimentally that our theoretical results on optimal early stopping time corresponds to the training process of deep neural network. Moreover, we study the effect of early stopping on generalization and demonstrate that optimal early stopping can help mitigate “double descent” in various settings.","This paper studies the relationship between early stopping and model dimension and sample size of the dataset for linear regression models. Theoretical results show that the optimal early stopping time corresponds to the training process of a deep neural network. The paper also shows that the early stopping can help mitigate double descent in various settings. Experiments are conducted on MNIST, CIFAR-10, and ImageNet. "
405,SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"Policy gradient algorithms have been widely applied to reinforcement learning (RL) problems in recent years. Regularization with various entropy functions is often used to encourage exploration and improve stability. In this paper, we propose a quasi-Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient (NPG) algorithm. For other entropy functions, this method results in brand new policy gradient algorithms. We provide a simple proof that all these algorithms enjoy the Newton-type quadratic convergence near the optimal policy. Using synthetic and industrial-scale examples, we demonstrate that the proposed quasi-Newton method typically converges in single-digit iterations, often orders of magnitude faster than other state-of-the-art algorithms.",This paper proposes a quasi-Newton method for policy gradient algorithms with entropy regularization. The main idea is to regularize the entropy of the policy gradient algorithm with Shannon entropy. The authors show that the proposed algorithm can be viewed as an extension of the natural policy gradient (NPG) algorithm. They also provide a simple proof that all the proposed algorithms enjoy the Newton-type quadratic convergence near the optimal policy.
406,SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efficiently, especially under distributional shifts. In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The case-based reasoner collects instances of positive experiences from the agent’s interaction with the world in the past and later reuses the collected experiences to act efficiently. The method can be applied in conjunction with any existing on-policy neural agent in the literature for TBGs. Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state-of-the-art results on widely used environments 1.","This paper proposes a novel method for text-based games (TBG) based on case-based reasoning. The authors propose to train an agent that learns a case-by-case reasoner that takes in the past experience and uses it to guide the agent to act efficiently. The agent is trained using a neural network that is trained on the current state of the game, and then the agent is used to learn a new state-of-the-art policy for the next state. The proposed method is evaluated on a variety of TBG environments, and the authors show that the proposed method outperforms existing methods in terms of generalization and sample efficiency."
407,SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,"Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems. Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multi-sense embedding in a downstream application.","This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. The authors demonstrate an effective approach to training the sense disambiguation mechanism in their model with a distribution over word senses extracted from the output layer embedding of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi- sense embedding on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multisense embedding in downstream application."
408,SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"3D point-clouds and 2D images are different visual representations of the physical world. While human vision can understand both representations, computer vision models designed for 2D image and 3D point-cloud understanding are quite different. Our paper explores the potential of transferring 2D model architectures and weights to understand 3D point-clouds, by empirically investigating the feasibility of the transfer, the benefits of the transfer, and shedding light on why the transfer works. We discover that we can indeed use the same architecture and pretrained weights of a neural net model to understand both images and point-clouds. Specifically, we transfer the image-pretrained model to a point-cloud model by inflating 2D convolutional filters to 3D convolutional filters and finetuning the inflated imagepretrained models (FIP). We find that models with minimal finetuning efforts — only on input, output, and optionally, batch normalization layers — can achieve competitive performance on 3D point-cloud classification, beating a wide range of point-cloud models that adopt task-specific architectures and use a variety of tricks. When finetuning the whole model, the performance gets further improved. Meanwhile, FIP improves data efficiency, reaching up to 10.0 points top-1 accuracy gain on few-shot classification. It also speeds up training of point-cloud models by up to 11.1x for a target accuracy.",This paper presents a method to transfer a pretrained image-pretrained model to a point-cloud model by inflating 2D convolutional filters to 3D convolutionsal filters and finetuning the inflated imagepretrained models (FIP). The authors evaluate the performance of the method on few-shot point cloud classification and show that it can achieve competitive performance on 3D point cloud. The authors also show that the method can speed up the training of point cloud models by up to 11.1x.
409,SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"Autoregressive generative models are commonly used, especially for those tasks involving sequential data. They have, however, been plagued by a slew of inherent flaws due to the intrinsic characteristics of chain-style conditional modeling (e.g., exposure bias or lack of long-range coherence), severely limiting their ability to model distributions properly. In this paper, we propose a unique method for training the autoregressive generative model that takes advantage of a well-designed energy-based learning objective. We show that our method is capable of alleviating the exposure bias problem and increase temporal coherence by imposing a constraint which fits joint distributions at each time step. Besides, unlike former energy-based models, we estimate energy scores based on the underlying autoregressive network itself, which does not require any extra network. Finally, thanks to importance sampling, we can train the entire model efficiently without requiring an MCMC process. Extensive empirical results, covering benchmarks like language modeling, neural machine translation, and image generation, demonstrate the effectiveness of the proposed approach.","This paper proposes a method for training autoregressive generative models that takes advantage of the energy-based learning objective. The proposed method is motivated by the observation that chain-style conditional models suffer from exposure bias and lack of long-range coherence. To address this issue, the authors propose a constraint which fits joint distributions at each time step. The authors also propose importance sampling to train the entire model efficiently without requiring an MCMC process. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method."
410,SP:51e748c55bd4134047098559577fa3f37aa7433a,"It is well-known that deep neural networks (DNNs) are susceptible to adversarial attacks, exposing a severe fragility of deep learning systems. As the result, adversarial training (AT) method, by incorporating adversarial examples during training, represents a natural and effective approach to strengthen the robustness of a DNN-based classifier. However, most AT-based methods, notably PGD-AT and TRADES, typically seek a pointwise adversary that generates the worst-case adversarial example by independently perturbing each data sample, as a way to “probe” the vulnerability of the classifier. Arguably, there are unexplored benefits in considering such adversarial effects from an entire distribution. To this end, this paper presents a unified framework that connects Wasserstein distributional robustness with current state-of-the-art AT methods. We introduce a new Wasserstein cost function and a new series of risk functions, with which we show that standard AT methods are special cases of their counterparts in our framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distributional robustness AT-based algorithms. Extensive experiments show that our distributional robustness AT algorithms robustify further their standard AT counterparts in various settings.1","This paper presents a unified framework that connects Wasserstein distributional robustness with current state-of-the-art adversarial training (AT) methods, including PGD-AT and TRADES. In particular, the authors introduce a new Wasserstein cost function and a new series of risk functions, with which they show that standard AT methods are special cases of their counterparts in their framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distributional AT-based algorithms. Extensive experiments show that the distributional distributional adversarial robustness AT algorithms robustify further their standard AT counterparts in various settings."
411,SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"Unsupervised representation learning for multivariate time series has practical significances, but it is also a challenging problem because of its complex dynamics and sparse annotations. Existing works mainly adopt the framework of contrastive learning and involve the data augmentation techniques to sample positives and negatives for contrastive training. However, their designs of representation learning framework have two drawbacks. First, we revisit the augmentation methods for time series of existing works and note that they mostly use segmentlevel augmentation derived from time slicing, which may bring about sampling bias and incorrect optimization with false negatives due to the loss of global context. Second, they all pay no attention to incorporate the spectral information and temporal-spectral relations in feature representation. To address these problems, we propose a novel framework, namely Bilinear Temporal-Spectral Fusion (BTSF). In contrast to segment-level augmentation, we utilize the instance-level augmentation by simply applying dropout on the entire time series for better preserving global context and capturing long-term dependencies. Also, an iterative bilinear temporal-spectral fusion module is devised to explicitly encode the affinities of abundant time-frequency pairs and iteratively refine representations of time series through cross-domain interactions with Spectrum-to-Time (S2T) and Timeto-Spectrum (T2S) Aggregation modules. Finally, we make sufficient assessments including alignment and uniformity to prove the effectiveness of our bilinear feature representations produced by BTSF. Extensive experiments are conducted on three major practical tasks for time series such as classification, forecasting and anomaly detection, which is the first to evaluate on all three tasks. Results shows that our BTSF achieves the superiority over the state-of-the-art methods and surpasses them by a large margin across downstream tasks. Code will be released.","This paper proposes a novel method for unsupervised representation learning for multivariate time series. The proposed method is based on the idea of bilinear temporal-spectral fusion (BTSF), which is an iterative method to incorporate temporal and spectral information into the representation learning. The method is evaluated on three tasks: classification, forecasting and anomaly detection. "
412,SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"Selecting an appropriate learning rate for efficiently training deep neural networks is a difficult process that can be affected by numerous parameters, such as the dataset, the model architecture or even the batch size. In this work, we propose an algorithm for automatically adjusting the learning rate during gradient descent. The rationale behind our approach is to train the learning rate along with the model weights, akin to line-search. Learning rate is optimized via a simple extra gradient descent step, justified by an analysis that exploits the structure of neural networks. We formulate first and second-order gradients with respect to learning rate as functions of consecutive weight gradients, leading to a cost-effective implementation. We also show that the scheme can be extended to accommodate for different learning rates per layer. Extensive experimental evaluation is conducted, validating the effectiveness of the proposed method for a plethora of different settings. The proposed method has proven to be robust to both the initial learning rate and the batch size, making it ideal for an off-the-shelf optimizing scheme.","This paper proposes an algorithm for automatically adjusting the learning rate during gradient descent. The learning rate is optimized via an extra extra gradient descent step, justified by an analysis that exploits the structure of neural networks. The authors formulate first and second-order gradients with respect to learning rate as functions of consecutive weight gradients, leading to a cost-effective implementation. Extensive experimental evaluation is conducted, validating the effectiveness of the proposed method for a plethora of different settings."
413,SP:263c787361cd6d4443ce516d389c694d0fe44b28,"We develop a new continual meta-learning method to address challenges in sequential multi-task learning. In this setting, the agent’s goal is to achieve high reward over any sequence of tasks quickly. Prior meta-reinforcement learning algorithms have demonstrated promising results in accelerating the acquisition of new tasks. However, they require access to all tasks during training. Beyond simply transferring past experience to new tasks, our goal is to devise continual reinforcement learning algorithms that learn to learn, using their experience on previous tasks to learn new tasks more quickly. We introduce a new method, continual meta-policy search (CoMPS), that removes this limitation by meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta-learning to prepare for subsequent task learning. We find that CoMPS outperforms prior continual learning and off-policy meta-reinforcement methods on several sequences of challenging continuous control tasks.",This paper proposes a meta-learning method for continual multi-task reinforcement learning. The main idea is to train a policy over a sequence of tasks without revisiting any of the previous tasks during the meta-training phase. This is achieved by using a two-step approach: 1) learning a new task using RL and 2) using the experience from RL to perform offline meta learning to prepare for the next task. The method is evaluated on a variety of continuous control tasks and compared to several baselines.
414,SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"Under a commonly-studied “backdoor” poisoning attack against classification models, an attacker adds a small “trigger” to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is incorrect. We describe a new threat model for poisoned classifier, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, we propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of smoothed adversarial images with human interaction. We demonstrate the effectiveness of our attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI. We also compare our approach to previous work on modeling trigger distributions and find that our method are more scalable and efficient in generating effective triggers. Last, we include a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Thus, we argue that there is no such thing as a secret backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier.","This paper proposes a new threat model for poisoned classifiers, where one without knowledge of the original trigger would want to control the poisoned classifier. Under this threat model, they propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. They construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images with human interaction. They demonstrate the effectiveness of their attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI."
415,SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"Generative adversarial networks (GANs) have achieved impressive results on various content generation tasks. Yet, their high demand on storage and computation impedes their deployment on resource-constrained devices. Though several GAN compression methods have been proposed to address the problem, most of them focus on conditional GANs. In this paper, we provide a comprehensive overhaul of distilling unconditional GAN, especially for the popular StyleGAN2 architecture. Our key insight is that the main challenge of unconditional GAN distillation lies in the output discrepancy issue, where the teacher and student model yield different outputs given the same input latent code. Standard knowledge distillation losses typically fail under this heterogeneous distillation scenario. We conduct thorough analysis about the reasons and effects of this discrepancy issue, and identify that the style module plays a vital role in determining semantic information of generated images. Based on this finding, we propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent. To further enhance the semantic consistency between the teacher and student model, we present a latent-direction-based distillation loss that preserves the semantic relations in latent space. Extensive experiments demonstrate the effectiveness of our approach in distilling StyleGAN2, outperforming existing GAN distillation methods by a large margin. Code and models will be released.","This paper studies the problem of unconditional GAN distillation. The authors propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent. They also propose a latent-direction-based distillation loss to further enhance the semantic consistency between the teacher and student model. Extensive experiments demonstrate the effectiveness of their approach in distilling StyleGAN2."
416,SP:2c2231743fa33b95828c6615263954ce1c05f95d,"In this work, we introduce a general methodology for approximating offline algorithms in online settings. By encoding the behavior of offline algorithms in graphs, we train a multi-task learning model to simultaneously detect behavioral structures which have already occurred and predict those that may come next. We demonstrate the methodology on both synthetic data and historical stock market data, where the contrast between explanation and prediction is particularly stark. Taken together, our work represents the first general and end-to-end differentiable approach for generating online approximations of offline algorithms.1","This paper proposes a method for generating online approximations of offline algorithms based on a multi-task learning model. The model is trained to simultaneously detect behavioral structures which have already occurred and predict those that may come next. The method is evaluated on both synthetic data and historical stock market data, where the contrast between explanation and prediction is particularly stark."
417,SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"Gaussian Processes (GPs) are Bayesian models that provide uncertainty estimates associated to the predictions made. They are also very flexible due to their nonparametric nature. Nevertheless, GPs suffer from poor scalability as the number of training instances N increases since their cost is cubic in N. To overcome this problem, sparse GP approximations are often used, where a set of M N inducing points is introduced during training. The location of the inducing points is learned by considering them as parameters of an approximate posterior distribution q. Sparse GPs, combined with variational inference for inferring q, reduce the cost of GPs per iteration to O(M). Critically, the inducing points determine the flexibility of the model and they are often located in regions of the input space where the latent function changes. A limitation is, however, that for some learning tasks a large number of inducing points may be required to obtain a good prediction performance. To address this limitation, we propose here to amortize the computation of the inducing points locations, as well as the parameters of the variational posterior approximation q. For this, we use a neural network that receives the observed data as an input and outputs the inducing points locations and the parameters of q. We evaluate our method in several experiments, showing that it performs similar or better than other state-of-the-art sparse variational GP approaches. However, with our method the number of inducing points is reduced drastically due to their dependency on the input data. This makes our method scale to larger datasets and have faster training and prediction times.","This paper proposes a method to amortize the computation of the inducing points locations and the parameters of the variational posterior approximation of Gaussian Processes (GP). The inducing points are learned by considering them as parameters of an approximate posterior distribution q. The authors propose to use a neural network that receives the observed data as an input and outputs the inducing point locations and parameters of q. They evaluate their method in several experiments, showing that it performs similar or better than other state-of-the-art sparse variational GP approaches."
418,SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"Some of the hardest problems in deep learning can be solved via pooling together computational resources of many independent parties, as is the case for scientific collaborations and volunteer computing. Unfortunately, any single participant in such systems can jeopardize the entire training run by sending incorrect updates, whether deliberately or by mistake. Training in presence of such peers requires specialized distributed training algorithms with Byzantine tolerance. These algorithms often sacrifice efficiency by introducing redundant communication or passing all updates through a trusted server. As a result, it can be infeasible to apply such algorithms to large-scale distributed deep learning, where models can have billions of parameters. In this work, we propose a novel protocol for secure (Byzantinetolerant) decentralized training that emphasizes communication efficiency. We rigorously analyze this protocol: in particular, we provide theoretical bounds for its resistance against Byzantine and Sybil attacks and show that it has a marginal communication overhead. To demonstrate its practical effectiveness, we conduct large-scale experiments on image classification and language modeling in presence of Byzantine attackers.","This paper proposes a protocol for decentralized training of deep learning models in the presence of Byzantine and Sybil attacks. The main idea is to train a neural network in a decentralized manner, where each network member is responsible for sending updates to the other network members. The authors propose a new protocol for secure decentralized training that emphasizes communication efficiency. Theoretical analysis is provided to show that the proposed protocol is robust to Byzantine attacks. Experiments on image classification and language modeling are conducted to show the effectiveness of the proposed method."
419,SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"Smoothed particle hydrodynamics (SPH) is a mesh-free Lagrangian method for obtaining approximate numerical solutions of the equations of fluid dynamics, which has been widely applied to weaklyand strongly compressible turbulence in astrophysics and engineering applications. We present a learn-able hierarchy of parameterized and ”physics-explainable” SPH informed fluid simulators using both physics based parameters and Neural Networks as universal function approximators. Our learning algorithm develops a mixed mode approach, mixing forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization. We show that our physics informed learning method is capable of: (a) solving inverse problems over the physically interpretable parameter space, as well as over the space of Neural Network parameters; (b) learning Lagrangian statistics of turbulence; (c) combining Lagrangian trajectory based, probabilistic, and Eulerian field based loss functions; and (d) extrapolating beyond training sets into more complex regimes of interest. Furthermore, our hierarchy of models gradually introduces more physical structure, which we show improves interpretability, generalizability (over larger ranges of time scales and Reynolds numbers), preservation of physical symmetries, and requires less training data.","This paper presents a method for learning a physics-based model for smoothed particle hydrodynamics (SPH) simulation. The method is based on the idea of learning a hierarchy of parameterized and “physics-explainable” SPH-based models, which are parameterized by physics based parameters and Neural Networks as universal function approximators. In particular, the authors propose a mixed mode approach, mixing forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization. They show that their physics-informed learning method is capable of solving inverse problems over the physically interpretable parameter space, as well as over the space of Neural Network parameters; learning Lagrangian statistics of turbulence; combining Lagrangians trajectory based, probabilistic, and Eulerian field based loss functions; and extrapolating beyond training sets into more complex regimes of interest."
420,SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"We propose an extremely simple approach to regularize a single deterministic neural network to obtain improved accuracy and reliable uncertainty estimates. Our approach, on top of the cross-entropy loss, simply puts an entropy maximization regularizer corresponding to the predictive distribution in the regions of the embedding space between the class clusters. This is achieved by synthetically generating between-cluster samples via the convex combination of two images from different classes and maximizing the entropy on these samples. Such a data-dependent regularization guides the maximum likelihood estimation to prefer a solution that (1) maps out-of-distribution samples to high entropy regions (creating an entropy barrier); and (2) is more robust to the superficial input perturbations. Via extensive experiments on real-world datasets (CIFAR-10 and CIFAR-100) using ResNet and Wide-ResNet architectures, we demonstrate that Mix-MaxEnt consistently provides much improved classification accuracy, better calibrated probabilities for in-distribution data, and reliable uncertainty estimates when exposed to situations involving domain-shift and out-of-distribution samples.","This paper proposes Mix-MaxEnt, a method to regularize a single deterministic neural network to obtain improved accuracy and reliable uncertainty estimates. The idea is simple: instead of adding a cross-entropy loss, the authors propose to add an entropy maximization regularizer corresponding to the predictive distribution in the regions of the embedding space between the class clusters. This is achieved by synthetically generating between-cluster samples via the convex combination of two images from different classes and maximizing the entropy on these samples. Experiments on real-world datasets (CIFAR-10 and Cifar-100) using ResNet and Wide-ResNet architectures demonstrate that the proposed method consistently provides much improved classification accuracy, better calibrated probabilities for in-distribution data, and reliable uncertainties estimates."
421,SP:365490b872464f00634dc7a50d024fceaf0a61ee,"Due to the remarkable progress of Generative Adversarial Networks (GANs) and auto-encoder animating images has become increasingly efficient, whereas associated results have become increasingly realistic. Current animation-approaches commonly exploit structure representation extracted from driving videos. Such structure representation (e.g., keypoints or regions) is instrumental in transferring motion from driving videos to still images. However, such approaches fail in case that a source image and driving video encompass large appearance variation. Moreover, the extraction of structure information requires additional modules that endow the animation-model with increased complexity. Deviating from such models, we here introduce the Latent Image Animator (LIA), a self-supervised autoencoder that evades need for structure representation. LIA is streamlined to animate images by linear navigation in the latent space. Specifically, motion in generated video is constructed by linear displacement of codes in the latent space. Towards this, we learn a set of orthogonal motion directions simultaneously, and use their linear combination, in order to represent any displacement in the latent space. Extensive quantitative and qualitative analysis suggests that our model systematically and significantly outperforms state-of-art methods on VoxCeleb, Taichi and TED-talk datasets w.r.t. generated quality. Figure 1: LIA animation examples. Two example images animated by LIA, which transfers motion of a driving video (smaller images on the top) from VoxCeleb dataset (Chung et al., 2018) onto still images of two celebrities, Marilyn Monroe and Emmanuel Macron. LIA is able to successfully animate these two images without relying on any explicit structure representations, such as landmarks and region representations","This paper proposes a self-supervised auto-encoder-based method for animating still images. The proposed method is based on the idea of linear displacement of codes in the latent space. The authors propose to learn a set of orthogonal motion directions simultaneously, and use their linear combination, in order to represent any displacement of the latent codes. The method is evaluated on three datasets: VoxCeleb, Taichi, and TED-talk, and compared with several state-of-the-art methods."
422,SP:86f9f89f84e117c86478b9afaf087f65524f5472,"Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies.","This paper proposes a meta-learning method called task interpolation (MLTI) to generate additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. The authors provide theoretical analysis of the proposed method under both gradient-based and metric-based meta-training settings. Theoretical analysis shows that MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification show that the proposed general MLTI framework is compatible with representative meta learning algorithms and consistently outperforms other state-of-the-art strategies."
423,SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets.","This paper proposes a new method for fair representation learning based on normalizing flows. In particular, the authors propose to model the encoder as a normalizing flow which is trained to minimize the statistical distance between the latent representations of different groups. The main advantage of this approach is that exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. The authors evaluate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning on a variety of challenging real-world datasets."
424,SP:404d5643327f60f0f06f820033a56081f9e01900,"The prevalence of graph structures has attracted a surge of research interest in graph data. As many graph-based tasks exploit recurring subgraph patterns on graphs, subgraph isomorphism counting becomes an important problem. Classical methods usually boil down to a backtracking framework that needs to navigate a huge search space with prohibitive computational cost due to the #P-completeness of the problem. Some recent studies resort to graph neural networks (GNNs) to learn a low-dimensional representation for both the query and the input graph, in order to predict the number of subgraph isomorphisms on the input graph. However, typical GNNs employ a node-centric message passing mechanism that receives and aggregates messages on nodes. While effective on node-oriented tasks, they become inadequate in complex structure matching for isomorphism counting. Moreover, given an input graph, the space of possible query graphs is enormous, and different parts of the input graph will be triggered to match different queries. Thus, expecting a fixed representation of the input graph to match diversely structured query graphs is unrealistic. In this paper, we propose a novel GNN called COUNT-GNN for subgraph isomorphism counting, to deal with the above challenges. At the edge level, we resort to an edge-centric message passing scheme, where messages on edges are propagated and aggregated based on the edge adjacency. By treating edges as first-class citizens, COUNT-GNN is able to preserve fine-grained structural information, given that an edge is an atomic unit of encoding graph structures. At the graph level, we modulate the input graph representation conditioned on the query, so that the input graph can be adapted to each query individually to improve their matching. To demonstrate the effectiveness and efficiency of COUNT-GNN, we conduct extensive experiments on a number of benchmark datasets. Results show that COUNT-GNN achieves superior performance in comparison to the state-of-the-art baselines.","This paper proposes a graph neural network (GNN) for subgraph isomorphism counting. The main idea is to learn a low-dimensional representation for both the query and the input graph in order to predict the number of isomorphisms on the query graph. The proposed method is based on edge-centric message passing scheme, where messages on edges are propagated and aggregated based on the edge adjacency. At the graph level, the proposed method modulates the graph representation conditioned on each query individually to improve their matching. The experimental results show that COUNT-GNN achieves superior performance in comparison to the state-of-the-art baselines."
425,SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"Considering the futuristic scenarios of federated learning at a worldwide scale, it is highly probable that local participants can have their own personalized labels, which might not be compatible with others (even for the same class), and can be also possibly from a variety of multiple domains. Nevertheless, they should be benefited from others while selectively taking helpful knowledge. Toward such extreme scenarios of federated learning, however, most existing approaches are limited in that they often assume: (1) labeling schemes are all synchronized amongst clients; (2) the local data is from the same single dataset (domain). In this sense, we introduce loosely constrained federated learning, namely Agnostic Personalized Federated Learning (APFL), where any clients, regardless of what they have learned with their personalized labels, can collaboratively learn while benefiting each other. We then study two essential challenges of the agnostic personalized federated learning, which are (1) Label Heterogeneity where local clients learn from the same single domain but labeling schemes are not synchronized with others and (2) Domain Heterogeneity where the clients learn from the different datasets which can be semantically similar or dissimilar to each other. To tackle these problems, we propose our novel method, namely Similarity Matching and Kernel Factorization (SimFed). Our method measures task-level similarity based on locally learned knowledge and matches the relevant ones for personalized knowledge reflection. Furthermore, we factorize our model parameters into two basis vectors and the highly sparse masks to significantly reduce the dimensionlaity of parameter space for alleviating knowledge collapse and information loss when reflecting the heterogeneous knowledge. We extensively validate our method on both singleand multi-domain datasets, showing that our method outperforms the current state-of-the-art federated learning methods.","This paper proposes a method for federated learning where each client can have their own personalized labels, which might not be compatible with others (even for the same class), and can be also possibly from a variety of multiple domains. The authors study two essential challenges of the agnostic personalized personalized learning, which are (1) Label Heterogeneity where local clients learn from the same single domain but labeling schemes are not synchronized with others and (2) Domain Heterogeneous where the clients learns from the different datasets which can be semantically similar or dissimilar to each other. To tackle these problems, the authors propose a novel method, namely Similarity Matching and Kernel Factorization (SimFed), which measures task-level similarity based on locally learned knowledge and matches the relevant ones for personalized knowledge reflection. SimFed factorizes the model parameters into two basis vectors and the highly sparse masks to significantly reduce the dimensionlaity of parameter space for alleviating knowledge collapse and information loss when reflecting the heterogeneous knowledge. Empirical results on both single and multi-domain datasets show that SimFed outperforms the current state-of-the-art."
426,SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"The ability to perceive scenes in terms of abstract entities is crucial for us to achieve higher-level intelligence. Recently, several methods have been proposed to learn object-centric representations of scenes with multiple objects, but most of them focus on static images. In this paper, we work on object dynamics and propose Object Dynamics Distillation Network (ODDN) which distills explicit object dynamic representations (e.g., velocity) from raw video input. Furthermore build a relation module that calculates object-pair interactions and applies it to the corresponding dynamic representations of objects. We verify our approach on tasks of video events reasoning and video prediction, which are two important evaluations for video understanding. The results show that visual representations of ODDN perform better in answering reasoning questions around physical events in a video compared to representaions of the previous scene representation methods. And, ODDN could generate reasonable future frames given two input frames, considering occlusion and objects collision. In addition, using the object dynamic clues allows the model to obtain better scene decomposition quality in segmentation and reconstruction.","This paper proposes a method to learn object-centric representations of scenes with multiple objects by distilling explicit object dynamic representations (e.g., velocity) from raw video input. The method is based on object Dynamics Distillation Network (ODDN) which distills explicit object dynamics representations (i.e., velocity, velocity, etc.) from raw videos. The relation module calculates object-pair interactions and applies it to the corresponding dynamic representations of objects. Experiments are conducted on video understanding and video prediction. The results show that visual representations of ODDN perform better in answering reasoning questions around physical events in a video compared to representaions of the previous scene representation methods."
427,SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task based on sets of nodes such as link/motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence, inaccurate prediction or high complexity. In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc.. GNNs with PE often get criticized because they are not generalizable to unseen graphs (inductive) or stable. Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original node features and rotation equivariance w.r.t. the positional features simultaneously. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability.1","This paper proposes a new graph neural network (GNN) architecture based on positional encoding (PE) techniques. In particular, the authors propose a new GNN layer called positional encoding layer (PEG) that allows using positional features of nodes given by positional encoding techniques such as Laplacian Eigenmap, Deepwalk, etc. The proposed PEG layer uses separate channels to update the original node features and positional features simultaneously. PEG imposes permutation equivariance w.r.t. the original nodes features and rotation equivariant w.rt. the positional features. Extensive experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability."
428,SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,"The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style. In this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models.","This paper proposes a novel method for text style transfer based on large-scale language models. The main idea is to leverage the intrinsic parallelism within the data. The model first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement. The proposed method is evaluated on two benchmark tasks (sentiment & formality transfer and political stance transfer) and a newly proposed challenging task (political stance transfer), and achieves qualitative advances in transfer accuracy, content preservation, and fluency."
429,SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs). It subsumes both one-hop link prediction as well as other more complex types of logical queries. However, existing algorithms operate only on classical, triple-based graphs, whereas modern KGs often employ a hyper-relational modeling paradigm. In this paradigm, typed edges may have several key-value pairs known as qualifiers that provide fine-grained context for facts. In queries, this context modifies the meaning of relations, and usually reduces the answer set. Hyper-relational queries are often observed in real-world KG applications, and existing approaches for approximate query answering (QA) cannot make use of qualifier pairs. In this work, we bridge this gap and extend the multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new type of complex queries. Building upon recent advancements in Graph Neural Networks and query embedding techniques, we study how to embed and answer hyper-relational conjunctive queries. Besides that, we propose a method to answer such queries and demonstrate in our experiments that qualifiers improve QA on a diverse set of query patterns.","This paper studies the problem of multi-hop logical reasoning on hyper-relational knowledge graphs (KGs). The authors propose a method to embed and answer such queries and demonstrate in their experiments that qualifiers improve QA on a diverse set of query patterns. The proposed method is based on recent advancements in Graph Neural Networks and query embedding techniques. Besides that, the authors also propose a new method to answer queries and show that the proposed method can be used in conjunction with existing approaches for approximate query answering."
430,SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"Gray-box hyperparameter optimization techniques have recently emerged as a promising direction for tuning Deep Learning methods. However, the multibudget search mechanisms of existing prior works can suffer from the poor correlation among the performances of hyperparameter configurations at different budgets. As a remedy, we introduce DYHPO, a method that learns to dynamically decide which configuration to try next, and for what budget. Our technique is a modification to the classical Bayesian optimization for a gray-box setup. Concretely, we propose a new surrogate for Gaussian Processes that embeds the learning curve dynamics and a new acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DYHPO against stateof-the-art hyperparameter optimization baselines through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse neural networks (MLP, CNN/NAS, RNN).","This paper proposes a method for hyperparameter optimization in the gray-box setting. The method is based on Bayesian optimization with a multi-budget search mechanism. The main idea is to learn to dynamically decide which configuration to try next, and for what budget. The authors propose a new surrogate for Gaussian Processes that embeds the learning curve dynamics and a new acquisition function that incorporates multi- budget information. The experimental results show that the proposed method outperforms the state-of-the-art methods on 50 datasets."
431,SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"It has been witnessed that learned image compression has outperformed conventional image coding techniques and tends to be practical in industrial applications. One of the most critical issues that need to be considered is the non-deterministic calculation, which makes the probability prediction cross-platform inconsistent and frustrates successful decoding. We propose to solve this problem by introducing well-developed post-training quantization and making the model inference integer-arithmetic-only, which is much simpler than presently existing training and fine-tuning based approaches yet still keeps the superior rate-distortion performance of learned image compression. Based on that, we further improve the discretization of the entropy parameters and extend the deterministic inference to fit Gaussian mixture models. With our proposed methods, the current state-ofthe-art image compression models can infer in a cross-platform consistent manner, which makes the further development and practice of learned image compression more promising.","This paper proposes a post-training quantization method to improve the performance of learned image compression. The authors propose to use integer arithmetic-only quantization to make the model inference integer-arithmetic-only, which is much simpler than the existing training and fine-tuning based approaches. They further improve the discretization of the entropy parameters and extend the deterministic inference to fit Gaussian mixture models. The proposed method can infer in a cross-platform consistent manner."
432,SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"Recent advances in Focused Ion Beam-Scanning Electron Microscopy (FIB-SEM) allow the imaging and analysis of cellular ultrastructure at nanoscale resolution, but the collection of labels and/or noise-free data sets has several challenges, often immutable. Reasons range from time consuming manual annotations, requiring highly trained specialists, to introducing imaging artifacts from the prolonged scanning during acquisition. We propose a fully unsupervised Noise Reconstruction and Removal Network for denoising scanning electron microscopy images. The architecture, inspired by gated recurrent units, reconstructs and removes the noise by synthesizing the sequential data. At the same time, the fully unsupervised training guides the network in distinguishing true signal from noise and gives comparable/even better results than supervised approaches on 3D electron microscopy data sets. We provide detailed performance analysis using numerical as well as empirical metrics.","This paper proposes an unsupervised noise reconstruction and removal network for denoising scanning electron microscopy images. The network is based on gated recurrent units, which reconstructs and removes the noise by synthesizing the sequential data. The proposed method is evaluated on 3D scanning electron microscope data sets and shows comparable performance compared to supervised methods."
433,SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"Graph neural networks (GNNs) and label propagation represent two interrelated modeling strategies designed to exploit graph structure in tasks such as node property prediction. The former is typically based on stacked message-passing layers that share neighborhood information to transform node features into predictive embeddings. In contrast, the latter involves spreading label information to unlabeled nodes via a parameter-free diffusion process, but operates independently of the node features. Given then that the material difference is merely whether features or labels are smoothed across the graph, it is natural to consider combinations of the two for improving performance. In this regard, it has recently been proposed to use a randomly-selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels. This so-called label trick accommodates the parallel use of features and labels, and is foundational to many of the top-ranking submissions on the Open Graph Benchmark (OGB) leaderboard. And yet despite its wide-spread adoption, thus far there has been little attempt to carefully unpack exactly what statistical properties the label trick introduces into the training pipeline, intended or otherwise. To this end, we prove that under certain simplifying assumptions, the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. Later, we leverage this perspective to motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions.","This paper studies the label trick in graph neural networks (GNNs). The authors show that label trick can be viewed as an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. The authors then leverage this perspective to motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions."
434,SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"Theory of mind (ToM), the ability to understand others’ thoughts and desires, is a cornerstone of human intelligence. Because of this, a number of previous works have attempted to measure the ability of machines to develop a theory of mind, with one agent attempting to understand anothers’ internal “mental state”. However, ToM agents are often tested as passive observers or in tasks with specific predefined roles, such as speaker-listener scenarios. In this work, we propose to model machine theory of mind in a more flexible and symmetric scenario; a multiagent environment SymmToM where all agents can speak, listen, see other agents, and move freely through a grid world. An effective strategy to solve SymmToM requires developing theory of mind to maximize each agent’s rewards. We show that multi-agent deep reinforcement learning models that model the mental states of other agents achieve significant performance improvements over agents with no such ToM model. At the same time, our best agents fail to achieve performance comparable to agents with access to the gold-standard mental state of other agents, demonstrating that the modeling of theory of mind in multi-agent scenarios is very much an open challenge.","This paper proposes to model machine theory of mind (ToM) in a more flexible and symmetric multi-agent environment where all agents can speak, listen, see other agents, and move freely through a grid world. The authors propose to use a deep reinforcement learning model that models the mental states of other agents. They show that the proposed model can improve the performance of agents with access to other agents’ mental states, while the best agents fail to achieve performance comparable to agents without access to the gold-standard mental state."
435,SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"To let robots be able to manipulate objects, they have to sense the location of objects. With the development of visual data collecting and processing technology, robots are gradually evolving to localize objects in a greater field of view rather than being limited to a small space where the object could appear. To train such a robot vision system, pictures of all the objects need to be taken under various orientations and illumination. In the traditional manufacturing environment, this is applicable since objects involved in the production process does not change frequently. However, in the vision of smart manufacturing and high-mix-low-volume production, parts and products for robots to handle may change frequently. Thus, it is unrealistic to re-training the vision system for new products and tasks. Under this situation, we discovered the necessity to introduce a hot concept which is zero-shot object detection. Zero-shot object detection is a subset of unsupervised learning, and it aims to detect novel objects in the image with the knowledge learned from and only from seen objects. With zero-shot object detection algorithm, time can be greatly saved from collecting training data and training the vision system. Previous works focus on detecting objects in outdoor scenes, such as bikes, car, people, and dogs. The detection of daily objects is actually more challenging since the knowledge can be learned from each object is very limited. In this work, we explore the zero-shot detection of daily objects in indoor scenes since the objects’ size and environment are closely related to the manufacturing setup. The YCB Video Dataset is used in this work, which contains 21 objects in various categories. To the best of our knowledge, no previous work has explored zero-shot detection in this object size level and on this dataset.","This paper proposes a novel method for zero-shot object detection, which aims to detect novel objects in the image with the knowledge learned from and only from seen objects. The proposed method is based on the idea of unsupervised object detection. The authors propose to use the YCB Video Dataset, which contains 21 objects in various categories, to train the network. The method is evaluated on two tasks: (1) detecting daily objects in indoor scenes, and (2) detecting objects in outdoor scenes. "
436,SP:aa1dcd9217270010f16a00004facede942efea17,"Video prediction is an important yet challenging problem; burdened with the tasks of generating future frames and learning environment dynamics. Recently, autoregressive latent video models have proved to be a powerful video prediction tool, by separating the video prediction into two sub-problems: pre-training an image generator model, followed by learning an autoregressive prediction model in the latent space of the image generator. However, successfully generating highfidelity and high-resolution videos has yet to be seen. In this work, we investigate how to train an autoregressive latent video prediction model capable of predicting high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, we scale up prior models by employing a high-fidelity image generator (VQ-GAN) with a causal transformer model, and introduce additional techniques of top-k sampling and data augmentation to further improve video prediction quality. Despite the simplicity, the proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables highresolution video prediction on complex and large-scale datasets. Videos are available at the anonymized website https://sites.google.com/view/harp-anonymous.",This paper proposes an autoregressive latent video model for video prediction. The proposed method is based on an image generator model and a causal transformer model. The model is trained with a top-k sampling and data augmentation method. The experimental results show that the proposed method achieves competitive performance to state-of-the-art models with fewer parameters.
437,SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,"Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to faciliate convergence. Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNNbased GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom.","This paper proposes to use Vision Transformers (ViTs) to train generative adversarial networks (GANs) for image generation. The authors propose several regularization techniques for training GANs with ViTs. Specifically, the authors propose two regularization methods for ViT discriminators and ViT generators. The first regularization method is based on the self-attention regularization. The second regularization technique is based off of the latent and pixel mapping layers. Experiments on CIFAR-10, CelebA, and LSUN bedroom show that the proposed method can achieve comparable performance to the leading CNN-based GAN models."
438,SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"Good likelihoods do not imply great sample quality. However, the precise manner in which models trained to achieve good likelihoods fail at sample quality remains poorly understood. In this work, we consider the task of image generative modeling with variational autoencoders and posit that the nature of high-dimensional image data distributions poses an intrinsic challenge. In particular, much of the entropy in these natural image distributions is attributable to visually imperceptible information. This signal dominates the training objective, giving models an easy way to achieve competitive likelihoods without successful modeling of the visually perceptible bits. Based on this hypothesis, we decompose the task of generative modeling explicitly into two steps: we first prioritize the modeling of visually perceptible information to achieve good sample quality, and then subsequently model the imperceptible information—the bulk of the likelihood signal— to achieve good likelihoods. Our work highlights the well-known adage that “not all bits are created equal” and demonstrates that this property can and should be exploited in the design of variational autoencoders.","This paper proposes to decompose the task of image generative modeling explicitly into two steps: 1) modeling the visually perceptible information to achieve good sample quality, and 2) model the imperceptible information, which is the bulk of the likelihood signal, to achieve a good likelihood signal. The authors propose a variational autoencoder model that first models the visual perceptible bits, and then models the other bits to achieve the good likelihoods. The paper is well-written and well-motivated. The experimental results show that the proposed approach can be applied to VAEs."
439,SP:bfed56018134ec66cde9a7e958df964d4cca3164,"Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20× to 80× speed up.","This paper proposes a training-free method for estimating the reverse variance and KL divergence of a DPM using Monte Carlo methods and a score-based model. The authors show that both the reverse and KL divergences have analytic forms w.r.t. its score function. To correct the potential bias caused by the score based model, the authors derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, the proposed method improves the log-likelihood of various DPMs, produces high-quality samples, and enjoys a 20-80x speed up."
440,SP:3f935ba5784c3e86db72421426bc479061af1a4b,"Convolutional Neural Networks (CNNs) have reigned for a decade as the de facto approach to automated medical image diagnosis, pushing the state-of-the-art in classification, detection and segmentation tasks. Recently, vision transformers (ViTs) have appeared as a competitive alternative to CNNs, yielding impressive levels of performance in the natural image domain, while possessing several interesting properties that could prove beneficial for medical imaging tasks. In this work, we explore whether it is feasible to switch to transformer-based models for medical image classification as well, or if we should keep working with CNNs – can we trivially replace CNNs with transformers? We consider this question in a series of experiments on several standard medical image benchmark datasets and tasks. Our findings show that, while CNNs perform better if trained from scratch, off-the-shelf vision transformers can perform on par with CNNs when pretrained on ImageNet, both in a supervised and self-supervised setting.","This paper proposes to use vision transformers (ViTs) as an alternative to CNNs for medical image classification tasks. The authors evaluate the performance of ViTs on several standard medical image benchmark datasets and tasks. They show that ViTs can perform on par with CNNs when pretrained on ImageNet, both in a supervised and self-supervised setting."
441,SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities. This theoretically motivated degree of freedom for “pretraining example design"" indicates new training schemes for self-improving representations.","This paper studies the problem of pretraining neural language models (NLMs) over a large corpus of text. The authors show that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segment in different training examples. This theoretically motivated degree of freedom for “pretraining example design” indicates new training schemes for self-improving representations. "
442,SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural networks create extra memory overhead for applying L2O models, and limits their applicability to optimizing larger tasks; (2) interpretability: it is unclear what each L2O model has learned in its black-box optimization rule, nor is it straightforward to compare different L2O models in an explainable way. To avoid both pitfalls, this paper proves the concept that we can “kill two birds by one stone”, by introducing the powerful tool of symbolic regression to L2O. In this paper, we establish a holistic symbolic representation and analysis framework for L2O, which yields a series of insights for learnable optimizers. Leveraging our findings, we further propose a lightweight L2O model that can be meta-trained on large-scale problems and outperformed human-designed and tuned optimizers. Our work is set to supply a brand-new perspective to L2O research, and our codes will be fully released upon acceptance.","This paper proposes a symbolic regression-based method for learning to optimize (L2O) models. The main idea of the method is to use symbolic regression to improve the interpretability and scalability of L2O models. In particular, the authors propose a holistic symbolic representation and analysis framework for L2o, which yields a series of insights for learnable optimizers. The authors also propose a lightweight model that can be meta-trained on large-scale problems and outperform human-designed and tuned optimizers, and their codes will be released upon acceptance."
443,SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on static supervised learning tasks such as image classification. However, DNNs have been used extensively in real-world adaptive tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in provable robustness in RL seek to certify the behaviour of the victim policy at every time-step against a non-adaptive adversary using methods developed for the static setting. But in the real world, an RL adversary can infer the defense strategy used by the victim agent by observing the states, actions, etc. from previous time-steps and adapt itself to produce stronger attacks in future steps (e.g., by focusing more on states critical to the agent’s performance). We present an efficient procedure, designed specifically to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step. Focusing on randomized smoothing based defenses, our main theoretical contribution is to prove an adaptive version of the Neyman-Pearson Lemma – a key lemma for smoothingbased certificates – where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Building on this result, we propose policy smoothing where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Our robustness certificates guarantee that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack. We show that our certificates are tight by constructing a worst-case scenario that achieves the bounds derived in our analysis. Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice.","This paper studies the problem of adversarial robustness in the context of reinforcement learning. The authors propose a policy smoothing method to defend against an adaptive RL adversary. The main contribution of the paper is to prove an adaptive version of the Neyman-Pearson Lemma for smoothing-based certificates, where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Based on this result, the authors propose to add a Gaussian noise to its observation at each time-step before passing it through the policy function. The proposed method is evaluated on Cartpole, Pong, Freeway and Mountain Car environments."
444,SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model’s confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In our experiments, ATC estimates target performance 2–4ˆ more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works.","This paper proposes a method for predicting the target accuracy of a model trained on labeled source data and unlabeled target data. The method is based on the idea of average thresholded confidence (ATC), which is a method that learns a threshold on the model’s confidence and predicts the accuracy as the fraction of unlabelled examples for which the model confidence exceeds that threshold. The authors evaluate the proposed method on a variety of datasets and model architectures, and show that it outperforms previous methods across several model architectures and types of distribution shifts."
445,SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"Given two point sets, the problem of registration is to recover a transformation that matches one set to the other. This task is challenging due to the presence of a large number of outliers, the unknown non-rigid deformations and the large sizes of point sets. To obtain strong robustness against outliers, we formulate the registration problem as a partial distribution matching (PDM) problem, where point sets are regarded as discrete distributions, and the goal is to partially match them. To handle large point sets, we propose a method for large scale PDM problem by utilizing the partial Wasserstein-1 (PW) discrepancy, which we show can be efficiently optimized. Specifically, we theoretically derive the Kantorovich–Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these theoretical results, we propose a partial Wasserstein adversarial network (PWAN), which approximates the PW discrepancy by a neural network, and learns the transformation adversarially with the network. It also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations. We evaluate PWAN on practical point set registration tasks, and show that the proposed PWAN is robust, scalable and performs more favorably than the state-of-the-art methods.","This paper studies the problem of partial distribution matching (PDM) registration, where the goal is to recover a transformation that matches one set to the other. The authors propose a method for large scale PDM problem by utilizing the partial Wasserstein-1 (PW) discrepancy, which they show can be efficiently optimized. Specifically, they theoretically derive the Kantorovich-Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these theoretical results, they propose a novel method, which approximates the partial PW discrepancy by a neural network, and learns the transformation adversarially with the network. It also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations. Experiments on practical point set registration tasks show that the proposed method is robust, scalable and performs better than the state-of-the-art methods."
446,SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"Hyperparameter optimization (HPO) is a crucial component of deploying machine learning models, however, it remains an open problem due to the resourceconstrained number of possible hyperparameter evaluations. As a result, prior work focuses on exploring the direction of transfer learning for tackling the sample inefficiency of HPO. In contrast to existing approaches, we propose a novel Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. We design DKLM to capture the similarity between hyperparameter configurations with an end-to-end meta-feature network that embeds the set of evaluated configurations and their respective performance. As a result, our novel DKLM can learn contextualized dataset-specific similarity representations for hyperparameter configurations. We experimentally validate the performance of DKLM in a wide range of HPO meta-datasets from OpenML and demonstrate the empirical superiority of our method against a series of stateof-the-art baselines.",This paper proposes a meta-learning method for hyperparameter optimization (HPO) in the context of transfer learning. The proposed method is based on Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. The DKLM is an end-to-end meta-feature network that embeds the set of evaluated configurations and their respective performance and learns contextualized dataset-specific similarity representations for hyper-parameter configurations. Experiments are conducted on a wide range of HPO meta-datasets and demonstrate the empirical superiority of the proposed method.
447,SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"Over the past seven years, deep generative models have achieved a qualitatively new level of performance. Generated data has become difficult, if not impossible, to be distinguished from real data. While there are plenty of use cases that benefit from this technology, there are also strong concerns on how this new technology can be misused to spoof sensors, generate deep fakes, and enable misinformation at scale. Unfortunately, current deep fake detection methods are not sustainable, as the gap between real and fake continues to close. In contrast, our work enables a responsible disclosure of such state-of-the-art generative models, that allows model inventors to fingerprint their models, so that the generated samples containing a fingerprint can be accurately detected and attributed to a source. Our technique achieves this by an efficient and scalable ad-hoc generation of a large population of models with distinct fingerprints. Our recommended operation point uses a 128-bit fingerprint which in principle results in more than 10 identifiable models. Experiments show that our method fulfills key properties of a fingerprinting mechanism and achieves effectiveness in deep fake detection and attribution.","This paper proposes a new method for generating fakes for deep generative models that can be detected and attributed to a source. The key idea is to generate a large population of models with distinct fingerprints, which can then be used to identify the source of the fakes. The authors propose to use a 128-bit fingerprinting operation to generate more than 10 identifiable models. The proposed method is evaluated on a variety of datasets and compared to the state of the art methods."
448,SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"Post-hoc explanations for black box models have been studied extensively in classification and regression settings. However, explanations for models that output similarity between two inputs have received comparatively lesser attention. In this paper, we provide model agnostic local explanations for similarity learners applicable to tabular and text data. We first propose a method that provides feature attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. We then propose analogies as a new form of explanation in machine learning. Here the goal is to identify diverse analogous pairs of examples that share the same level of similarity as the input pair and provide insight into (latent) factors underlying the model’s prediction. The selection of analogies can optionally leverage feature attributions, thus connecting the two forms of explanation while still maintaining complementarity. We prove that our analogy objective function is submodular, making the search for good-quality analogies efficient. We apply the proposed approaches to explain similarities between sentences as predicted by a state-of-the-art sentence encoder, and between patients in a healthcare utilization application. Efficacy is measured through quantitative evaluations, a careful user study, and examples of explanations.","This paper proposes a method to provide model-agnostic local explanations for black box models that output similarity between two inputs. The authors first provide a method that provides feature attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. Then, the authors propose analogies as a new form of explanation in machine learning. The goal is to identify diverse analogous pairs of examples that share the same level of similarity as the input pair and provide insight into (latent) factors underlying the model’s prediction. The selection of analogies can leverage feature attribution, thus connecting the two forms of explanation while still maintaining complementarity. They prove that their analogy objective function is submodular, making the search for good-quality analogies efficient."
449,SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"Recent studies show that deep neural networks (DNN) are vulnerable to adversarial examples, which aim to mislead DNNs by adding perturbations with small magnitude. To defend against such attacks, both empirical and theoretical defense approaches have been extensively studied for a single ML model. In this work, we aim to analyze and provide the certified robustness for ensemble ML models, together with the sufficient and necessary conditions of robustness for different ensemble protocols. Although ensemble models are shown more robust than a single model empirically; surprisingly, we find that in terms of the certified robustness the standard ensemble models only achieve marginal improvement compared to a single model. Thus, to explore the conditions that guarantee to provide certifiably robust ensemble ML models, we first prove that diversified gradient and large confidence margin are sufficient and necessary conditions for certifiably robust ensemble models under the model-smoothness assumption. We then provide the bounded model-smoothness analysis based on the proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble model can always achieve higher certified robustness than a single base model under mild conditions. Inspired by the theoretical findings, we propose the lightweight Diversity Regularized Training (DRT) to train certifiably robust ensemble ML models. Extensive experiments show that our DRT enhanced ensembles can consistently achieve higher certified robustness than existing single and ensemble ML models, demonstrating the state-of-the-art certified L2-robustness on MNIST, CIFAR-10, and ImageNet datasets.","This paper analyzes the certified robustness of ensemble ML models, and provides sufficient and necessary conditions of robustness for different ensemble protocols. The authors prove that diversified gradient and large confidence margin are sufficient conditions for certifiably robust ensemble models under the model-smoothness assumption. They also provide the bounded model smoothness analysis based on the proposed Ensemble-before-smoothing strategy. Inspired by the theoretical findings, the authors propose the lightweight Diversity Regularized Training (DRT) to train certified-robust ensemble models. Extensive experiments show that DRT enhanced ensembles can consistently achieve higher certified L2 robustness than existing single-model and ensemble models, demonstrating the state-of-the-art certified L1 robustness."
450,SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"While message passing Graph Neural Networks (GNNs) have become increasingly popular architectures for learning with graphs, recent works have revealed important shortcomings in their expressive power. In response, several higherorder GNNs have been proposed that substantially increase the expressive power, albeit at a large computational cost. Motivated by this gap, we explore alternative strategies and lower bounds. In particular, we analyze a new recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. First, we prove that this model can count subgraphs of size k, and thereby overcomes a known limitation of low-order GNNs. Second, we show how recursive pooling can exploit sparsity to reduce the computational complexity compared to the existing higher-order GNNs. More generally, we provide a (near) matching information-theoretic lower bound for counting subgraphs with graph representations that pool over representations of derived (sub-)graphs. We also discuss lower bounds on time complexity.","This paper studies the expressive power of graph neural networks (GNNs) in the context of message passing. In particular, the authors propose a new pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. First, they prove that this model can count subgraphs of size k, and thereby overcomes a known limitation of low-order GNNs. Second, they show how recursive pooling can exploit sparsity to reduce the computational complexity compared to the existing higher-order Graph Neural Networks (GNNs). More generally, they provide a (near) matching information-theoretic lower bound for counting subGraphs with graph representations that pool over representations of derived (sub-)graphs."
451,SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"Pretrained language models (LMs) are not very good at robustly capturing factual knowledge. This has led to the development of a number of knowledge integration (KI) methods which aim to incorporate external knowledge into pretrained LMs. Even though KI methods show some performance gains over vanilla LMs, the efficacy and limitations of these methods are not well-understood. For instance, it is unclear how and what kind of knowledge is effectively integrated into LMs and if such integration may lead to catastrophic forgetting of already learned knowledge. In this paper, we revisit the KI process in an information-theoretic view and show that KI could be interpreted using a graph convolution operation. We propose a simple probe model called Graph Convolution Simulator (GCS) for interpreting knowledge-enhanced LMs and exposing what kind of knowledge is integrated into these models. We conduct experiments to verify that our GCS model can indeed be used to correctly interpret the KI process, and we use it to analyze two typical knowledge-enhanced LMs: K-Adapter and ERNIE. We find that only a small amount of factual knowledge is captured in these models during integration. While K-Adapter is better at integrating simple relational knowledge, complex relational knowledge is integrated better in ERNIE. We further find that while K-Adapter struggles to integrate time-related knowledge, it successfully integrates knowledge of unpopular entities and relations. Our analysis also show some challenges in KI. In particular, we find simply increasing the size of the KI corpus may not lead to better KI and more fundamental advances may be needed.","This paper presents an information-theoretic view of the knowledge integration (KI) process from an information theoretic point of view. The authors propose a simple probe model called Graph Convolution Simulator (GCS) for interpreting knowledge-enhanced LMs and exposing what kind of knowledge is integrated into these models. The proposed GCS model can be used to interpret the KI process and analyze two typical KI methods: K-Adapter and ERNIE. They find that only a small amount of factual knowledge is captured in these models during integration. While K- adapter is better at integrating simple relational knowledge, complex relational knowledge is better integrated better in ERNie. They further find that while K-Adapters struggles to integrate time-related knowledge, it successfully integrates knowledge of unpopular entities and relations."
452,SP:7e73948421e98307fceb69a316d8a4e7c4926cda,"Model-Agnostic Meta-Learning (MAML) aims to find initial weights that allow fast adaptation to new tasks. The adaptation (inner loop) learning rate in MAML plays a central role in enabling such fast adaptation. However, how to choose this value in practice and how this choice affects the adaptation error remains less explored. In this paper, we study the effect of the adaptation learning rate in meta-learning with mixed linear regression. First, we present a principled way to estimate optimal adaptation learning rates that minimize the population risk of MAML. Second, we interpret the underlying dependence between the optimal adaptation learning rate and the input data. Finally, we prove that compared with empirical risk minimization (ERM), MAML produces an initialization with a smaller average distance to the task optima, consistent with previous practical findings. These results are corroborated with numerical experiments.","This paper studies the effect of the adaptation learning rate in meta-learning with mixed linear regression. The authors propose a principled way to estimate optimal adaptation learning rates that minimize the population risk of MAML. They also interpret the underlying dependence between the optimal learning rate and the input data. Finally, they prove that compared with empirical risk minimization (ERM), MAMl produces an initialization with a smaller average distance to the task optima, consistent with previous practical findings."
453,SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift— characterized by a change in measurement system—which can be resolved by restoring the source features. In the source domain, we store a lightweight and flexible approximation of the feature distribution under the source data. In the target domain, we adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We call this method Feature Restoration (FR) as it seeks to extract features with the same semantics from the target domain as were previously extracted from the source, rather than extracting new ones. We additionally propose Bottom-Up Feature Restoration (BUFR)—a bottom-up training scheme for FR which boosts performance by preserving learnt structure in the later layers of a network. We demonstrate that BUFR outperforms existing SFDA methods on real and synthetic data in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.","This paper proposes a method for source-free domain adaptation (SFDA) that aims to adapt a model trained on labelled data in a source domain to unlabelled data in the target domain without access to the source-domain data during adaptation. The authors propose Feature Restoration (FR) which aims to extract features with the same semantics from the target data as were previously extracted from the source, rather than extracting new ones. They also propose a bottom-up training scheme for FR which boosts performance by preserving learnt structure in the later layers of a network. They demonstrate that BUFR outperforms existing SFDA methods on real and synthetic data."
454,SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"Federated learning (FL) emerges as a popular distributed learning schema that learns a model from a set of participating users without requiring raw data to be shared. One major challenge of FL comes from heterogeneity in users, which may have distributionally different (or non-iid) data and varying computation resources. Just like in centralized learning, FL users also desire model robustness against malicious attackers at test time. Whereas adversarial training (AT) provides a sound solution for centralized learning, extending its usage for FL users has imposed significant challenges, as many users may have very limited training data as well as tight computational budgets, to afford the data-hungry and costly AT. In this paper, we study a novel learning setting that propagates adversarial robustness from highresource users that can afford AT, to those low-resource users that cannot afford it, during the FL process. We show that existing FL techniques cannot effectively propagate adversarial robustness among non-iid users, and propose a simple yet effective propagation approach that transfers robustness through carefully designed batch-normalization statistics. We demonstrate the rationality and effectiveness of our method through extensive experiments. Especially, the proposed method is shown to grant FL remarkable robustness even when only a small portion of users afford AT during learning.",This paper proposes a method to propagate adversarial robustness in federated learning (FL) from high-resource users that can afford adversarial training (AT) to low resource users that cannot afford it during the FL process. The authors propose a simple yet effective propagation approach that transfers robustness through carefully designed batch-normalization statistics. The experiments show that the proposed method is effective in improving FL robustness even when only a small portion of users afford AT during learning.
455,SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"Strategic interactions between a group of individuals or organisations can be modelled as games played on networks, where a player’s payoff depends not only on their actions but also on those of their neighbors. Inferring the network structure from observed game outcomes (equilibrium actions) is an important problem with numerous potential applications in economics and social sciences. Existing methods mostly require the knowledge of the utility function associated with the game, which is often unrealistic to obtain in real-world scenarios. To address this limitation, we propose a novel transformer-like architecture which correctly accounts for the symmetries of the problem and learns a mapping from the equilibrium actions to the network structure of the game without explicit knowledge of the utility function. We test our method on three different types of network games using both synthetic and real-world data, and demonstrate its effectiveness in network structure inference and superior performance over existing methods.","This paper proposes a transformer-based method for network games, which learns a mapping from the equilibrium actions to the network structure of the game without explicit knowledge of the utility function. The proposed method is evaluated on three different types of network games using both synthetic and real-world data, and demonstrate its effectiveness in network structure inference and superior performance over existing methods."
456,SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"Prevailing methods for relation prediction in heterogeneous graphs including knowledge graphs aim at learning the latent representations (i.e., embeddings) of observed nodes and relations, and are thus limited to the transductive setting where the relation types must be known during training. In this paper, we propose ANalogy SubGraph Embedding Learning (GraphANGEL), a novel relation prediction framework that predicts relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraphs containing the considered relation. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relation types and leads to more explainable predictive models. Our model consistently outperforms existing models in terms of heterogeneous graph based recommendation as well as knowledge graph completion. We also empirically demonstrate the capability of our model in generalizing to new relation types while producing explainable heat maps of attention scores across the discovered logics.","This paper proposes GraphANGEL, a method for relation prediction in heterogeneous graphs. The main idea is to learn a latent representation for each node in a graph, which is then used to predict the relation between a given node and a subgraph based on the similarity of the two subgraphs. The method is evaluated on heterogeneous graph based recommendation and knowledge graph completion tasks, and the proposed method is compared with several baselines."
457,SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures (Chen & Li, 2020). Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available at https://github.com/TencentAILabHealthcare/Few-shot-WSI.","This paper studies few-shot learning in histology images. The authors propose to use contrastive learning (CL) with latent augmentation (LA) to build a few shot learning system. The main idea is to learn representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. The experiments show that the proposed method generalize better than supervised learning for histology image in unseen classes, and that LA brings consistent gains over baselines."
458,SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"Recurrent neural networks (RNNs) with continuous-time hidden states are a natural fit for modeling irregularly-sampled time series. These models, however, face difficulties when the input data possess long-term dependencies. We prove that similar to standard RNNs, the underlying reason for this issue is the vanishing or exploding of the gradient during training. This phenomenon is expressed by the ordinary differential equation (ODE) representation of the hidden state, regardless of the ODE solver’s choice. We provide a solution by equipping arbitrary continuous-time networks with a memory compartment separated from its timecontinuous state. This way, we encode a continuous-time dynamical flow within the RNN, allowing it to respond to inputs arriving at arbitrary time-lags while ensuring a constant error propagation through the memory path. We call these models Mixed-Memory-RNNs (mmRNNs). We experimentally show that Mixed-MemoryRNNs outperform recently proposed RNN-based counterparts on non-uniformly sampled data with long-term dependencies.","This paper proposes a novel RNN-based model for irregularly sampled time series. The authors show that the gradient of the RNN is vanishing or exploding during training, which can be explained by the ordinary differential equation (ODE) representation of the hidden state. To solve this problem, the authors propose to add a memory compartment to the R-NN to separate the time-continuous state from the memory of the memory. This allows the model to respond to inputs arriving at arbitrary time-lags while ensuring a constant error propagation through the memory path. Experiments are conducted on time series with long-term dependencies."
459,SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but is also computation and memory expensive. As one of the powerful compression approaches, binarization extremely reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit weight, embedding, and activation) usually suffer a significant performance drop, and there is rare study addressing this problem. In this paper, with the theoretical justification and empirical analysis, we identify that the severe performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks. Specifically, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a DirectionMatching Distillation (DMD) scheme to optimize the full binarized BERT accurately. Extensive experiments show that BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. As the first fully binarized BERT, our method yields impressive 56.3× and 31.2× saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios.","This paper proposes BiBERT, a new method for fully binarized BERT. The main idea is to use a Bi-Attention structure for maximizing representation information statistically and a DirectionMatching Distillation (DMD) scheme to optimize the full binarization of BERT accurately. The experiments show that the proposed method outperforms both the straightforward baseline and existing state-of-the-art quantized bERTs with ultra-low bit activations."
460,SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"This paper presents a new method to solve keypoint detection and instance association by using Transformer. For bottom-up multi-person pose estimation models, they need to detect keypoints and learn associative information between keypoints. We argue that these problems can be entirely solved by Transformer. Specifically, the self-attention in Transformer measures dependencies between any pair of locations, which can provide association information for keypoints grouping. However, the naive attention patterns are still not subjectively controlled, so there is no guarantee that the keypoints will always attend to the instances to which they belong. To address it we propose a novel approach of supervising self-attention for multi-person keypoint detection and instance association. By using instance masks to supervise self-attention to be instance-aware, we can assign the detected keypoints to their instances based on the pairwise attention scores, without using pre-defined offset vector fields or embedding like CNN-based bottom-up models. An additional benefit of our method is that the instance segmentation results of any number of people can be directly obtained from the supervised attention matrix, thereby simplifying the pixel assignment pipeline. The experiments on the COCO multi-person keypoint detection challenge and person instance segmentation task demonstrate the effectiveness and simplicity of the proposed method, and show a promising way to control self-attention behavior for specific purposes.","This paper presents a new method to solve keypoint detection and instance association by using Transformer. Specifically, the self-attention in Transformer measures dependencies between any pair of locations, which can provide association information for keypoints grouping. However, the naive attention patterns are still not subjectively controlled, so there is no guarantee that the keypoints will always attend to the instances to which they belong. To address this issue, the authors propose a novel approach of supervising self attention by using instance masks to supervise self attention to be instance-aware. The experiments on the COCO multi-person keypoints detection challenge and person instance segmentation task demonstrate the effectiveness and simplicity of the proposed method."
461,SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"In reinforcement learning (RL) for sequential decision making under uncertainty, existing methods proposed for considering mean-variance (MV) trade-off suffer from computational difficulties in computation of the gradient of the variance term. In this paper, we aim to obtain MV-efficient policies that achieve Pareto efficiency regarding MV trade-off. To achieve this purpose, we train an agent to maximize the expected quadratic utility function, in which the maximizer corresponds to the Pareto efficient policy. Our approach does not suffer from the computational difficulties because it does not include gradient estimation of the variance. In experiments, we confirm the effectiveness of our proposed methods.","This paper proposes a method to train an agent to maximize the expected quadratic utility function, which corresponds to the Pareto efficient policy. The method is based on gradient estimation of the variance term of the policy, which does not suffer from the computational difficulties of gradient estimation. The authors evaluate the proposed method on a variety of tasks and show that it outperforms the state-of-the-art."
462,SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"The problem of end-to-end learning of a communication system using an autoencoder has recently been shown to be a promising approach. We focus on the problem of test-time domain adaptation for such an autoencoder system whose channel is generatively-modeled using a mixture density network (MDN). Different from the setting of conventional training-time (unsupervised or semi-supervised) domain adaptation, here we have a fully-trained channel model and autoencoder from a source domain, that we would like to adapt to a target domain using only a small labeled dataset (and no unlabeled data). Moreover, since the distribution of the channel is expected to change frequently (e.g., a wireless link), the error rate of the autoencoder can degrade quickly, making it challenging to collect sufficient data for frequent retraining of the autoencoder. To address this, we propose a fast and sample-efficient method for adapting the autoencoder without modifying the encoder and decoder neural networks, and adapting only the MDN channel model. The method utilizes feature transformations at the decoder to compensate for changes in the channel distribution, and effectively present to the decoder samples close to the source distribution. Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed method can adapt the MDN channel using very limited number of samples, and improve or maintain the error rate of the autoencoder under changing channel conditions.","This paper studies the problem of test-time domain adaptation of an autoencoder system that is trained using a mixture density network (MDN). The authors propose a fast and sample-efficient method for adapting the channel model without modifying the encoder and decoder neural networks, and adapting only the MDN channel model. The proposed method utilizes feature transformations at the decoder to compensate for changes in the channel distribution, and effectively present to the decoders samples close to the source distribution. Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed method can adapt the channel using very limited number of samples, and improve or maintain the error rate of the auto-encoder under changing channel conditions."
463,SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"The abductive natural language inference task (αNLI) is proposed to infer the most plausible explanation between the cause and the event. In the αNLI task, two observations are given, and the most plausible hypothesis is asked to pick out from the candidates. Existing methods model the relation between each candidate hypothesis separately and penalize the inference network uniformly. In this paper, we argue that it is unnecessary to distinguish the reasoning abilities among correct hypotheses; and similarly, all wrong hypotheses contribute the same when explaining the reasons of the observations. Therefore, we propose to group instead of ranking the hypotheses and design a structural loss called “joint softmax focal loss” in this paper. Based on the observation that the hypotheses are generally semantically related, we have designed a novel interactive language model aiming at exploiting the rich interaction among competing hypotheses. We name this new model for αNLI: Interactive Model with Structural Loss (IMSL). The experimental results show that our IMSL has achieved the highest performance on the RoBERTa-large pretrained model, with ACC and AUC results increased by about 1% and 5% respectively.","This paper proposes a new model for the abductive natural language inference task (αNLI) where the goal is to infer the most plausible explanation between the cause and the event. The authors argue that it is unnecessary to distinguish the reasoning abilities among correct hypotheses and similarly, all wrong hypotheses contribute the same when explaining the reasons of the observations. Instead, they propose to group instead of ranking the hypotheses and design a structural loss called “joint softmax focal loss” in this paper. The experimental results show that their IMSL has achieved the highest performance on the RoBERTa-large pretrained model, with ACC and AUC results increased by about 1% and 5% respectively."
464,SP:17cd72df5fc19398f582d27516fd742b073f79e3,"The application of machine learning in safety-critical systems requires a reliable assessment of uncertainy. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data. Even if trained to be non-confident on OOD data one can still adversarially manipulate OOD data so that the classifier again assigns high confidence to the manipulated samples. In this paper we propose a novel method that combines a certifiable OOD detector with a standard classifier from first principles into an OOD aware classifier. This way we achieve the best of two worlds: certifiably adversarially robust OOD detection, even for OOD samples close to the in-distribution, without loss in either prediction accuracy or detection performance for non-manipulated OOD data. Moreover, due to the particular construction our classifier provably avoids the asymptotic overconfidence problem of standard neural networks.","This paper proposes a method for certifiably adversarially robust out-of-distribution (OOD) detection and classification. The method is based on a combination of a classifier with an OOD detector and a standard classifier from first principles. The classifier is trained to be OOD aware, and the detector is trained with a standard OOD classifier. The authors show that the proposed method can detect OOD samples even if they are close to the in-district distribution, without loss in either prediction accuracy or detection performance for non-manipulated OOD data."
465,SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"It has been observed that Deep Neural Networks (DNNs) are vulnerable to transfer attacks in the query-free black-box setting. However, the previous studies on transfer attack commonly assume that the white-box surrogate models possessed by the attacker and the black-box victim models are trained on the same dataset, which means the attacker implicitly knows the label set and the input size of the victim model. However, this assumption is usually unrealistic as the attacker may not know the dataset used by the victim model, and further, the attacker needs to attack any randomly encountered images that may not come from the same dataset. Therefore, in this paper we define a new Generalized Transferable Attack (GTA) problem where we assume the attacker has a set of surrogate models trained on different datasets (with different label sets and image sizes), and none of them is equal to the dataset used by the victim model. We then propose a novel method called Image Classification Eraser (ICE) to erase classification information for any encountered images from arbitrary dataset. Extensive experiments on Cifar-10, Cifar-100, and TieredImageNet demonstrate the effectiveness of the proposed ICE on the GTA problem. Furthermore, we show that existing transfer attack methods can be modified to tackle the GTA problem, but with significantly worse performance compared with ICE.","This paper proposes a new generalized transferable attack (GTA) problem where the attacker has a set of surrogate models trained on different datasets (with different label sets and image sizes), and none of them is equal to the dataset used by the victim model. The paper then proposes a novel method called Image Classification Eraser (ICE) to erase classification information for any encountered images from arbitrary dataset. Extensive experiments on Cifar-10, CIFAR-100, and TieredImageNet demonstrate the effectiveness of the proposed ICE on the GTA problem. Furthermore, the paper shows that existing transfer attack methods can be modified to tackle the GTA, but with significantly worse performance compared with ICE."
466,SP:2e0447c741a3f09be1095633d870200355211260,"Discriminative pre-trained language models (PrLMs) learn to predict original texts from intentionally corrupted ones. Taking the former text as positive and the latter as negative samples, the discriminative PrLM can be trained effectively for contextualized representation. However, though the training of such a type of PrLMs highly relies on the quality of the automatically constructed samples, existing PrLMs simply treat all corrupted texts as equal negative without any examination, which actually lets the resulting model inevitably suffer from the false negative issue where training is carried out on wrong data and leads to less efficiency and less robustness in the resulting PrLMs. Thus in this work, on the basis of defining the false negative issue in discriminative PrLMs that has been ignored for a long time, we design enhanced pre-training methods to counteract false negative predictions and encourage pre-training language models on true negatives, by correcting the harmful gradient updates subject to false negative predictions. Experimental results on GLUE and SQuAD benchmarks show that our counter-false-negative pre-training methods indeed bring about better performance together with stronger robustness.","This paper proposes a counter-false-negative pre-training method for discriminative pre-trained language models (PrLMs) that aims to prevent false negative predictions. The authors propose to train the model on negative samples instead of positive ones, and train the discriminator on the true negative samples. The proposed method is evaluated on GLUE and SQuAD benchmarks, and compared with the existing methods."
467,SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"A fundamental limitation of applying semi-supervised learning in real-world settings is the assumption that unlabeled test data contains only classes previously encountered in the labeled training data. However, this assumption rarely holds for data in-the-wild, where instances belonging to novel classes may appear at testing time. Here, we introduce a novel open-world semi-supervised learning setting that formalizes the notion that novel classes may appear in the unlabeled test data. In this novel setting, the goal is to solve the class distribution mismatch problem between labeled and unlabeled data, where at the test time every input instance either needs to be classified into one of the existing classes or a new unseen class needs to be initialized and the instance assigned to it. To tackle this challenging problem, we propose ORCA, an end-to-end approach that assigns instances to previously seen classes or forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. In this way, ORCA gradually increases the discriminability of the model during the training and reduces the gap between intra-class variance of seen with respect to novel classes. Extensive experiments on image classification datasets and a single-cell dataset demonstrate that ORCA consistently outperforms alternative baselines, achieving 25% improvement on seen and 96% improvement on novel classes of the ImageNet dataset.",This paper proposes an open-world semi-supervised learning method for the problem of class distribution mismatch between labeled and unlabeled data. The authors propose an end-to-end approach that assigns instances to previously seen classes or forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. Experiments on image classification datasets and a single-cell dataset demonstrate that ORCA consistently outperforms alternative baselines.
468,SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"We propose SLIM-QN, a light stochastic quasi-Newton optimizer for training large-scale deep neural networks (DNNs). SLIM-QN addresses two key barriers in existing second-order methods for large-scale DNNs: 1) the high computational cost of obtaining the Hessian matrix and its inverse in every iteration (e.g. KFAC); 2) convergence instability due to stochastic training (e.g. L-BFGS). To tackle the first challenge, SLIM-QN uses the BFGS update rule that directly approximates the Hessian inverse using past parameters and gradients, without explicitly constructing the Hessian matrix and then computing its inverse. To achieve stable convergence, SLIM-QN introduces momentum in Hessian updates together with an adaptive damping mechanism. We provide rigorous theoretical results on the convergence of SLIM-QN in a stochastic setting. We also demonstrate that SLIM-QN has much less compute and memory overhead compared to existing second-order methods. To better understand the limitations and benefits of SLIM-QN, we evaluate its performance on various datasets and network architectures. For instance on large datasets such as ImageNet, we show that SLIM-QN achieves near optimal accuracy 1.5× faster when compared with SGD (1.36× faster in wall-clock time) using the same compute resources. We also show that SLIM-QN can readily be applied to other contemporary non-convolutional architectures such as Transformers.","This paper proposes a second-order method for training large-scale deep neural networks (DNNs). The main idea is to use the BFGS update rule that directly approximates the Hessian inverse using past parameters and gradients, without explicitly constructing the Hessians matrix and then computing its inverse. To achieve stable convergence, the authors introduce momentum in Hessian updates together with an adaptive damping mechanism. The authors provide rigorous theoretical results on the convergence of SLIM-QN in a stochastic setting and demonstrate that it has much less compute and memory overhead compared to existing second order methods."
469,SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"Graph Neural Networks (GNNs) have emerged as highly successful tools for graph-related tasks. However, real-world problems involve very large graphs, and the compute resources needed to fit GNNs to those problems grow rapidly. Moreover, the noisy nature and size of real-world graphs cause GNNs to over-fit if not regularized properly. Surprisingly, recent works show that large graphs often involve many redundant components that can be removed without compromising the performance too much. This includes node or edge removals during inference through GNNs layers or as a pre-processing step that sparsifies the input graph. This intriguing phenomenon enables the development of state-of-the-art GNNs that are both efficient and accurate. In this paper, we take a further step towards demystifying this phenomenon and propose a systematic method called LocalitySensitive Pruning (LSP) for graph pruning based on Locality-Sensitive Hashing. We aim to sparsify a graph so that similar local environments of the original graph result in similar environments in the resulting sparsified graph, which is an essential feature for graph-related tasks. To justify the application of pruning based on local graph properties, we exemplify the advantage of applying pruning based on locality properties over other pruning strategies in various scenarios. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of LSP, which removes a significant amount of edges from large graphs without compromising the performance, accompanied by a considerable acceleration.","This paper proposes a systematic method called Locality-Sensitive Pruning (LSP) for graph pruning based on Locality Sensitive Hashing. LSP aims to sparsify a graph so that similar local environments of the original graph result in similar environments in the resulting sparsified graph, which is an essential feature for graph-related tasks. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of LSP."
470,SP:c5e024f4e2079586298519ca868630efd7579eca,"Data augmentation is critical to contrastive self-supervised learning, whose goal is to distinguish a sample’s augmentations (positives) from other samples (negatives). However, strong augmentations may change the sample-identity of the positives, while weak augmentation produces easy positives/negatives leading to nearlyzero loss and ineffective learning. In this paper, we study a simple adversarial augmentation method that can modify training data to be hard positives/negatives without distorting the key information about their original identities. In particular, we decompose a sample x to be its variational auto-encoder (VAE) reconstruction G(x) plus the residual R(x) = x − G(x), where R(x) retains most identitydistinctive information due to an information-theoretic interpretation of the VAE objective. We then adversarially perturb G(x) in the VAE’s bottleneck space and adds it back to the original R(x) as an augmentation, which is therefore sufficiently challenging for contrastive learning and meanwhile preserves the sample identity intact. We apply this “identity-disentangled adversarial augmentation (IDAA)” to different self-supervised learning methods. On multiple benchmark datasets, IDAA consistently improves both their efficiency and generalization performance. We further show that IDAA learned on a dataset can be transferred to other datasets.","This paper proposes an adversarial data augmentation method for contrastive self-supervised learning. The authors propose to decompose a sample x to be its variational auto-encoder (VAE) reconstruction plus the residual R(x) = x + G(x), where the residual retains most identity-distinctive information due to an information-theoretic interpretation of the VAE objective. They then adversarially perturb the residual and add it back to the original VAE as an augmentation. They apply this “identity-disentangled adversarial augmentation (IDAA)” to different self supervised learning methods and show that IDAA consistently improves both the efficiency and generalization performance on multiple benchmark datasets."
471,SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"When deployed in the real world, machine learning models inevitably encounter changes in the data distribution, and certain—but not all—distribution shifts could result in significant performance degradation. In practice, it may make sense to ignore benign shifts, under which the performance of a deployed model does not degrade substantially, making interventions by a human expert (or model retraining) unnecessary. While several works have developed tests for distribution shifts, these typically either use non-sequential methods, or detect arbitrary shifts (benign or harmful), or both. We argue that a sensible method for firing off a warning has to both (a) detect harmful shifts while ignoring benign ones, and (b) allow continuous monitoring of model performance without increasing the false alarm rate. In this work, we design simple sequential tools for testing if the difference between source (training) and target (test) distributions leads to a significant increase in a risk function of interest, like accuracy or calibration. Recent advances in constructing time-uniform confidence sequences allow efficient aggregation of statistical evidence accumulated during the tracking process. The designed framework is applicable in settings where (some) true labels are revealed after the prediction is performed, or when batches of labels become available in a delayed fashion. We demonstrate the efficacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets.","This paper proposes a method for detecting distribution shifts in machine learning models. The authors argue that a sensible method for firing off a warning has to both (a) detect harmful shifts while ignoring benign ones, and (b) allow continuous monitoring of model performance without increasing the false alarm rate. To this end, they design simple sequential tools for testing if the difference between source (training) and target (test) distributions leads to a significant increase in a risk function of interest, like accuracy or calibration. They demonstrate the efficacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets."
472,SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,"Neural networks have recently been used to model the dynamics of diverse physical systems. While existing methods achieve impressive results, they are limited by their strong demand for training data and their weak generalization abilities. To overcome these limitations, in this work we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) in order to obtain interpretable physical models directly from visual observations. Our proposed model combines several unique advantages: (i) Contrary to existing approaches that require large training datasets, we are able to identify physical parameters from only a single video (ii) The use of neural implicit representations enables the processing of high-resolution videos and the synthesis of photo-realistic imagery. (iii) The embedded neural ODE has a known parametric form that allows for the identification of interpretable physical parameters, and (iv) long-term prediction in state space. (v) Furthermore, the photo-realistic rendering of novel scenes with modified physical parameters becomes possible. synth | real synth | real synth | real synth | real synth | real synth | real pendulum length Figure 1: Our method infers physical parameters directly from real-world videos, like the shown pendulum motion. Separated by the red line, the right half of each image shows the input frame, and the left half shows our reconstruction based on physical parameters that we estimate from the input. We show 6 out of 10 frames that were used for training. The proposed model can precisely recover the metric length of the pendulum from the monocular video (relative error to true length is less than 2.5%). Best viewed on screen with magnification. Please also consider the supplementary video.",This paper proposes a method to model the dynamics of physical systems from a single video. The method is based on neural implicit representations for appearance modeling and neural ordinary differential equations (ODEs) for long-term prediction in state space. The authors show that their method can recover the metric length of the pendulum from the monocular video (relative error to true length is less than 2.5%). The authors also show that the proposed method is able to render novel scenes with modified physical parameters.
473,SP:51efd1451343f4994d857daa5490e299b812bc2d,"We consider a context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. We argue that this challenging case is often met in applications and we tackle it using a Bayesian approach and variational inference. We adapt a sticky Hierarchical Dirichlet Process (HDP) prior for model learning, which is arguably best-suited for Markov process modeling. We then derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. We argue that the combination of these two components allows to infer the number of contexts from data thus dealing with the context cardinality assumption. We then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, we demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that our approach succeeds where state-of-the-art methods of other frameworks fail and elaborate on the reasons for such failures.","This paper proposes a method for context-dependent reinforcement learning in the context-independent setting, which is characterized by an unknown finite number of not directly observable contexts, abrupt (discontinuous) context changes occurring during an episode, and Markovian context evolution. The authors propose to use a Hierarchical Dirichlet Process (HDP) prior for model learning, and derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. They then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, they demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that their approach succeeds where other methods of other frameworks fail."
474,SP:ea167b126212b2092bc1190d7f8376bf7c54a888,"Knowledge enriched language representation learning has shown promising performance across various knowledge-intensive NLP tasks. However, existing knowledge based language models are all trained with monolingual knowledge graph data, which limits their application to more languages. In this work, we present a novel framework to pretrain knowledge based multilingual language models (KMLMs). We first generate a large amount of code-switched synthetic sentences and reasoning-based multilingual training data using the Wikidata knowledge graphs. Then based on the intraand inter-sentence structures of the generated data, we design pretraining tasks to facilitate knowledge learning, which allows the language models to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task designed by us, namely, logic reasoning. Our code and pretrained language models will be made publicly available.","This paper proposes a pretraining framework for knowledge based multilingual language models (KMLMs). The pretraining consists of generating synthetic sentences and reasoning-based multilingual training data using the Wikidata knowledge graphs. The pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task designed by the authors, namely, logic reasoning."
475,SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents’ goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents’ goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully, may be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and can learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and thereby allowing them to better achieve their goals. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach on three different multi-agent environments where another agent’s success depends on the altruistic agent’s behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them.","This paper proposes a method for learning to act altruistically towards other agents by giving them more choice and thereby allowing them to better achieve their goals. The authors propose to learn to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. They evaluate their approach on three different multi-agent environments where another agent’s success depends on the altruistic agent. They show that their unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them."
476,SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"‘Double descent’ delineates the generalization behaviour of models depending on the regime they belong to: underor over-parameterized. The current theoretical understanding behind the occurrence of this phenomenon is primarily based on linear and kernel regression models — with informal parallels to neural networks via the Neural Tangent Kernel. Therefore such analyses do not adequately capture the mechanisms behind double descent in finite-width neural networks, as well as, disregard crucial components — such as the choice of the loss function. We address these shortcomings by leveraging influence functions in order to derive suitable expressions of the population loss and its lower bound, while imposing minimal assumptions on the form of the parametric model. Our derived bounds bear an intimate connection with the spectrum of the Hessian at the optimum, and importantly, exhibit a double descent behaviour at the interpolation threshold. Building on our analysis, we further investigate how the loss function affects double descent — and thus uncover interesting properties of neural networks and their Hessian spectra near the interpolation threshold.","This paper studies the phenomenon of double descent in neural networks. The authors derive a lower bound for the population loss of a neural network based on the influence function, and show that the lower bound is related to the spectrum of the Hessian at the optimum of the network. They further investigate how the loss function affects double descent, and uncover interesting properties of neural networks and their Hessian spectrum near the interpolation threshold."
477,SP:b485114712055f39a7afb951dbc3db482ff523fd,"Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. However, it is well known that deep GCNs suffer from the over-smoothing problem, where node representations tend to be indistinguishable as more layers are stacked up. The theoretical research to date on deep GCNs has focused primarily on expressive power rather than trainability, an optimization perspective. Compared to expressivity, trainability attempts to address a more fundamental question: given a sufficiently expressive space of models, can we successfully find a good solution by gradient descentbased optimizer? This work fills this gap by exploiting the Graph Neural Tangent Kernel (GNTK), which governs the optimization trajectory under gradient descent for wide GCNs. We formulate the asymptotic behaviors of GNTK in the large depth, which enables us to reveal the dropping trainability of wide and deep GCNs at an exponential rate in the optimization process. Additionally, we extend our theoretical framework to analyze residual connection-based techniques, which are found to be only able to mildly mitigate the exponential decay of trainability. To overcome the exponential decay problem more fundamentally, we propose Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, inspired by our theoretical insights on trainability. Experimental evaluation consistently confirms using our proposed method can achieve better results compared to relevant counterparts with both infinite-width and finite-width.","This paper studies the asymptotic behavior of the graph neural tangent kernel (GNTK) in deep GCNs. The authors show that the GNTK is exponential in the depth of the network, which is a result of the over-smoothing problem of deep GCN. They also show that residual connection-based techniques are only able to mildly mitigate the exponential decay of trainability. To address this issue, the authors propose Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, inspired by the theoretical insights on trainability and propose a new sampling method based on the drop-edge technique. Experimental results show the effectiveness of the proposed method."
478,SP:25a92b3583afdc6892e59f1e769125d52c8011af,"Computer vision methods typically optimize for first-order dynamics (e.g., optical flow). However, in many cases the properties of interest are subtle variations in higher-order changes, such as acceleration. This is true in the cardiac pulse, where the second derivative can be used as an indicator of blood pressure and arterial disease. Recent developments in camera-based vital sign measurement have shown that cardiac measurements can be recovered with impressive accuracy from videos; however, the majority of research has focused on extracting summary statistics such as heart rate. Less emphasis has been put on the accuracy of waveform morphology that is necessary for many clinically impactful scenarios. In this work, we provide evidence that higher-order dynamics are better estimated by neural models when explicitly optimized for in the loss function. Furthermore, adding second-derivative inputs also improves performance when estimating second-order dynamics. By incorporating the second derivative of both the input frames and the target vital sign signals into the training procedure, our model is better able to estimate left ventricle ejection time (LVET) intervals.",This paper proposes a method for estimating the second-order dynamics of a video of a heart beat. The authors propose to use the second derivative of both the input frames and the target vital sign signals into the training procedure to estimate the left ventricle ejection time (LVET) intervals. The method is evaluated on a set of cardiac recordings of a patient's heart beat and shows that it is better able to estimate LVET intervals compared to a baseline method.
479,SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"The study of emergent communication has long been devoted to coax neural network agents to learn a language sharing similar properties with human language. In this paper, we try to find a ‘natural’ way to help agents learn a compositional and symmetric language in complex settings like dialog games. Inspired by the theory that human language was originated from simple interactions, we hypothesize that language may evolve from simple tasks to difficult tasks. We propose a novel architecture called symbolic mapping as a basic component of the communication system of agent. We find that symbolic mapping learned in simple referential games can notably promote language learning in difficult tasks. Further, we explore vocabulary expansion, and show that with the help of symbolic mapping, agents can easily learn to use new symbols when the environment becomes more complex. All in all, we probe into how symbolic mapping helps language learning and find that a process from simplicity to complexity can serve as a natural way to help multi-agent language learning.","This paper proposes to use symbolic mapping to help agents learn a compositional and symmetric language in complex settings like dialog games. Inspired by the theory that human language was originated from simple interactions, the authors hypothesize that language may evolve from simple tasks to difficult tasks. They propose a novel architecture called symbolic mapping as a basic component of the communication system of agent. They find that symbolic mapping learned in simple referential games can notably promote language learning in difficult tasks and explore vocabulary expansion, and show that agents can easily learn to use new symbols when the environment becomes more complex."
480,SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"Robotic agents performing domestic chores using natural language directives require to learn a complex task of navigating an environment and interacting with objects in it. To address such composite tasks, we propose a hierarchical modular approach to learn agents that navigate and manipulate objects in a divide-andconquer manner for the diverse nature of the entailing tasks. Specifically, our policy operates at three levels of hierarchy. We first infer a sequence of subgoals to be executed based on language instructions by high-level policy composition controller (PCC). We then discriminatively control the agent’s navigation by a master policy by alternating between navigation policy and various independent interaction policies. Finally, we infer manipulation actions with the corresponding object masks using the appropriate interaction policy. Our hierarchical agent, named HACR (Hierarchical Approach for Compositional Reasoning), generates a human interpretable and short sequence of sub-objectives, leading to efficient interaction with an environment, and achieves the state-of-the-art performance on the challenging ALFRED benchmark. Fl at P ol ic y Le ar ni ng H ie ra rc hi ca l P ol ic y Le ar ni ng RIGHT AHEAD PICKUP RIGHT AHEAD OPEN CLOSE PICKUP OPEN CLOSE PUT AHEAD","This paper proposes a hierarchical modular approach to learn agents that navigate and manipulate objects in a divide-and-conquer manner for the diverse nature of the entailed tasks. Specifically, the policy operates at three levels of hierarchy. First, a sequence of subgoals to be executed based on language instructions by high-level policy composition controller (PCC). Then discriminatively control the agent’s navigation by a master policy by alternating between navigation policy and various independent interaction policies. Finally, the manipulation actions with the corresponding object masks using the appropriate interaction policy."
481,SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is the nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We first define the nuisance-varying family, a set of distributions that differ only in the nuisance-label relationship. We then introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance within the set on every distribution in the nuisance-varying family. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.","This paper proposes Nuisance-randomized distillation (NURD), a method for training models that perform well regardless of the nuisance-label relationship between the label and the nuisance variable. The authors first define a family of distributions that differ only in the nuisance and label relationship. Then, they introduce a set of representations that are independent of the label under the nuisance distribution. They prove that the representations in this set always perform better than chance, while representations outside of this set may not. They evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, they produce models that predict pneumonia under strong spurious correlations."
482,SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised ""gold"" labels. Previous works, such as CLIP, use InfoNCE loss to train a model to predict the pairing between images and text captions. CLIP, however, is data hungry and requires more than 400M image-text pairs for training. The inefficiency can be partially attributed to the fact that the image-text pairs are noisy. To address this, we propose OTTER (Optimal TransporT distillation for Efficient zero-shot Recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. Based on pretrained image and text encoders, models trained with OTTER achieve strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero-shot evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes) from Tencent ML-Images. Over 42 evaluations on 7 different dataset/architecture settings x 6 metrics, OTTER outperforms (35) or ties (2) all baselines in 37 of them.","This paper proposes OTTER (Optimal TransporT distillation for Efficient zero-shot Recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. Based on pretrained image and text encoders, models trained with OTTER achieve strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero-shots evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes)."
483,SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms. Pix2Seq ymin=9 xmin=7 ymax=67 xmax=98 train...... ymin=8 xmin=4 ymax=99 xmax=97 motocycle...... ymin=1 xmin=57 ymax=99 xmax=72 Person...... Cmd: detect objects Figure 1: Illustration of Pix2Seq framework for object detection. The neural net perceives an image and generates a sequence of tokens that correspond to bounding boxes and class labels.","This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, this paper casts object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. The approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, the approach makes minimal assumptions about the tasks, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms."
484,SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"Deep vision models are nowadays widely integrated into visual reinforcement learning (RL) to parameterize the policy networks. However, the learned policies are overparameterized black boxes that lack interpretability, and are usually brittle under input distribution shifts. This work revisits this end-to-end learning pipeline, and proposes an alternative stage-wise approach that features hierarchical reasoning. Specifically, our approach progressively converts a policy network into the interpretable symbolic policy, composed from geometric and numerical symbols and operators. A policy regression algorithm called RoundTourMix is proposed to distill the symbolic rules as teacher-student. The symbolic policy can be treated as discrete and abstracted representations of the policy network, but are found to be more interpretable, robust and transferable. The proposed symbolic distillation approach is experimentally demonstrated to maintain the performance and “denoise” the CNN policy: on six specific environments, our distilled symbolic policy achieved compelling or even higher scores than the CNN based RL agents. Our codes will be fully released upon acceptance. Detected bounding box Velocity could be measured as one component of numerical state The proposed policy distillation learns to draw auxiliary lines and measure geometric relations The proposed policy distillation learns to condition action on distance 1 The proposed symbolic distillation learns to trigger action based on coordinates Figure 1: Four example environments adopted for distilling the CNN policy network knowledge into the symbolic policy. From left to right are: Airstriker-Genesis, Pong, CircusCharlie, Seaquest.","This paper proposes a method for distilling the learned policy network into a symbolic policy, which is composed of geometric and numerical symbols and operators. The proposed method is based on a policy regression algorithm called RoundTourMix, which distills the symbolic rules as teacher-student. The symbolic policy can be treated as discrete and abstracted representations of the policy network, but are found to be more interpretable, robust and transferable. Experiments on six different environments show that the proposed symbolic policy achieves compelling or even higher scores than the CNN based RL agents."
485,SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"One popular objective for the image-to-image translation task is to independently control the coarse-level object arrangements (posture) and the fine-grained level styling (identity) of the generated image from two exemplar sources. To approach this objective, we propose PIVQGAN with two novel techniques in the framework of StyleGAN2. First, we propose a Vector-Quantized Spatial Normalization (VQSN) module for the generator for better pose-identity disentanglement. The VQSN module automatically learns to encode the shaping and composition information from the commonly shared objects inside the training-set images. Second, we design a joint-training scheme with self-supervision methods for the GANInversion encoder and the generator. Specifically, we let the encoder and generator reconstruct images from two differently augmented variants of the original ones, one defining the pose and the other for identity. The VQSN module facilitates a more delicate separation of posture and identity, while the training scheme ensures the VQSN module learns the pose-related representations. Comprehensive experiments conducted on various datasets show better synthesis image quality and disentangling scores of our model. Moreover, we present model applications beyond posture-identity disentangling, thanks to the latent-space reducing feature of the leveraged VQSN module. Figure 1: Unsupervised image-to-image translation results of PIVQGAN with disentangled posture and identity control. In each panel, the first row has input pose images, and the first column has referential identity images. The second and third rows are “segmentation-like” masks automatically learned by PIVQGAN, and bottom-right are the synthesized images.","This paper proposes a method for image-to-image translation that aims to disentangle the pose-identity disentanglement of the generated images. The authors propose a joint-training scheme with self-supervision methods for the GANInversion encoder and the generator. The VQSN module automatically learns to encode the shaping and composition information from the commonly shared objects inside the training-set images. Moreover, the latent-space reducing feature is leveraged to improve the performance of the model. The experimental results show that the proposed method achieves better synthesis image quality and disentangling scores."
486,SP:e51a7f45493064972585109f203a867e9828eb15,"Transformers have shown outstanding performance in recent years, achieving state-of-the-art results in speech processing tasks such as speech recognition, speech synthesis and speech enhancement. In this paper, we show that, despite their success, such complex models are not needed for some important speech related tasks, which can be solved with much simpler and compact models. Thus, we propose a multi-layer perceptron (MLP) architecture, namely speech-MLP, useful for extracting information from speech signals. The model splits feature channels into non-overlapped chunks and processes each chunk individually. These chunks are then merged together and further processed to consolidate the output. By setting different numbers of chunks and focusing on different contextual window sizes, speech-MLP learns multiscale local temporal dependency. The proposed model is successfully evaluated on two tasks: keyword spotting and speech enhancement. In our experiments, two benchmark datasets are adopted for keyword spotting (Google speech command V2-35 and LibriWords) and one dataset (VoiceBank) for speech enhancement. In all experiments, speech-MLP surpassed the transformer-based solutions, achieving better performance with fewer parameters lower GFLOPS. Such results indicate that more complex models, such as transformers, are oftentimes not necessary for speech processing tasks. Hence, simpler and more compact models should always be considered as an alternative, specially in resource-constrained scenarios.","This paper proposes a multi-layer perceptron (MLP) architecture for speech processing. The model splits feature channels into non-overlapped chunks and processes each chunk individually. These chunks are then merged together and further processed to consolidate the output. The proposed model is evaluated on two tasks: keyword spotting and speech enhancement. In all experiments, speech-MLP achieves better performance with fewer parameters lower GFLOPS."
487,SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"A critical performance barrier in modern machine learning is scarcity of labeled data required for training state of the art massive models, especially in quickly emerging problems with lack of extensive data sets or scenarios where data collection and labeling is expensive/time consuming. Transfer learning is gaining traction as a promising technique to alleviate this barrier by utilizing the data of a related but different source task to compensate for the lack of data in a target task where there are few labeled training data. While there has been many recent algorithmic advances in this domain, a fundamental understanding of when and how much one can transfer knowledge from a related domain to reduce the amount of labeled training data is far from understood. We provide a precise answer to this question for binary classification problems by deriving a novel lower bound on the generalization error that can be achieved by any transfer learning algorithm (regardless of its computational complexity) as a function of the amount of source and target samples. Our lower bound depends on a natural notion of distance that can be easily computed on real world data sets. Other key features of our lower bound are that it applies to any arbitrary source/target data distributions and requires minimal assumptions that enables it application to a broad range of problems. We also consider a more general setting where there are more than one source domains for knowledge transfer to the target task and develop new bounds on generalization error in this setting. We also corroborate our theoretical findings on real image classification and action recognition data sets. These experiments demonstrate that our natural notion of distance is indicative of the difficulty of knowledge transfer between different pairs of source/target tasks, allowing us to investigate the effect of different sources on the target generalization error. Furthermore, to evaluate the sharpness of our bounds we compare our developed lower bounds with upper-bounds achieved by transfer learning base-lines that utilize weighted empirical risk minimization on the combination of source(s) and target data sets.",This paper provides a lower bound on the generalization error of transfer learning algorithms for binary classification problems. The lower bound is based on a natural notion of distance that can be easily computed on real world data sets. The authors also consider a more general setting where there are more than one source domains for knowledge transfer to the target task and develop new bounds on generalization errors in this setting. The experiments on real image classification and action recognition data sets demonstrate that the proposed lower bounds are indicative of the difficulty of knowledge transfer between different pairs of source/target tasks.
488,SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible outcomes. We employ the Generative Cellular Automata that learns the multi-modal distribution and transform the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. We formally derive that our training objective for the sparse voxel embedding maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data. We also demonstrate that our approach outperforms deterministic models even in less ambiguous cases with a small amount of missing data, which infers that probabilistic formulation is crucial for high-quality geometry completion on input scans exhibiting any levels of completeness.","This paper proposes a probabilistic method for 3D scene generation based on generative cellular auto-encoders. The method is based on the generative Cellular Automata (GCA) model, which learns the multi-modal distribution and transforms the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. Experiments show that the model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data."
489,SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"Effective exploration is a crucial challenge in deep reinforcement learning. Behavioral priors have been shown to tackle this problem successfully, at the expense of reduced generality and restricted transferability. We thus propose temporal priors as a non-Markovian generalization of behavioral priors for guiding exploration in reinforcement learning. Critically, we focus on state-independent temporal priors, which exploit the idea of temporal consistency and are generally applicable and capable of transferring across a wide range of tasks. We show how dynamically sampling actions from a probabilistic mixture of policy and temporal prior can accelerate off-policy reinforcement learning in unseen downstream tasks. We provide empirical evidence that our approach improves upon strong baselines in long-horizon continuous control tasks under sparse reward settings.",This paper proposes temporal priors as a non-Markovian generalization of behavioral priors for guiding exploration in reinforcement learning. The authors propose a probabilistic mixture of policy and temporal prior to accelerate off-policy reinforcement learning in unseen downstream tasks. They provide empirical evidence that their approach improves upon strong baselines in long-horizon continuous control tasks under sparse reward settings.
490,SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"Recent decades have witnessed great development of stochastic optimization in training deep neural networks. Learning rate scheduling is one of the most important factors that influence the performance of stochastic optimizers like Adam. Traditional methods seek to find a relatively proper scheduling among a limited number of pre-defined rules and might not accommodate a particular target problem. Instead, we propose a novel Graph-Network-based Scheduler (GNS), aiming at learning a specific scheduling mechanism without restrictions to existing principles. By constructing a directed graph for the underlying neural network of the target problem, GNS encodes current dynamics with a graph message passing network and trains an agent to control the learning rate accordingly via reinforcement learning. The proposed scheduler can capture the intermediate layer information while being able to generalize to problems of varying scales. Besides, an efficient reward collection procedure is leveraged to speed up training. We evaluate our framework on benchmarking datasets, Fashion-MNIST and CIFAR10 for image classification, and GLUE for language understanding. GNS shows consistent improvement over popular baselines when training CNN and Transformer models. Moreover, GNS demonstrates great generalization to different datasets and network structures.",This paper proposes a method to learn a scheduling mechanism for learning rate scheduling in deep neural networks. The proposed method is based on graph-network-based reinforcement learning (GNN). The authors propose a directed graph message-passing network that encodes the current dynamics with a graph message passing network and trains an agent to control the learning rate accordingly via reinforcement learning. The authors evaluate the proposed method on Fashion-MNIST and CIFAR-10 for image classification and GLUE for language understanding.
491,SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"We tackle the problem of deep object-centric learning from a point cloud which is crucial for high-level relational reasoning and scalable machine intelligence. In particular, we introduce a framework, SPAIR3D, to factorize a 3D point cloud into a spatial mixture model where each component corresponds to one object. To model the spatial mixture model on point clouds, we derive the Chamfer Mixture Loss, which fits naturally into our variational training pipeline. Moreover, we adopt an object-specification scheme that describes each object’s location relative to its local voxel grid cell. Such a scheme allows SPAIR3D to model scenes with an arbitrary number of objects. We evaluate our method on the task of unsupervised scene decomposition. Experimental results demonstrate that SPAIR3D has strong scalability and is capable of detecting and segmenting an unknown number of objects from a point cloud in an unsupervised manner.","This paper proposes a method for unsupervised object-centric learning from point clouds. The method is based on the Chamfer Mixture Loss (PML) loss, which is used to model the spatial mixture model of a point cloud. The paper also proposes an object-specification scheme that describes each object’s location relative to its local voxel grid cell. The proposed method is evaluated on the task of scene decomposition, where it is shown that the proposed method can detect and segment an unknown number of objects from a 3D point cloud and is able to perform well."
492,SP:3c57e921c1bf23e482551ceb71702931a7f07439,"Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (i.e. “make breakfast”), to a chosen set of actionable steps (i.e. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into low-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models1.","This paper proposes a method for grounding high-level tasks, expressed in natural language, to a chosen set of actionable steps (i.e. “open fridge”). The authors show that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high level tasks into low-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. To address this issue, the authors propose a procedure that conditions on existing demonstrations and semantically translates the plans into admissible actionable actions. The proposed method is evaluated in the recent VirtualHome environment and shows that the resulting method substantially improves executability over the baseline."
493,SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"In this paper, we propose a geometrical interpretation of the Variational Autoencoder framework. We show that VAEs naturally unveil a Riemannian structure of the learned latent space. Moreover, we show that using these geometrical considerations can significantly improve the generation from the vanilla VAE which can now compete with more advanced VAE models on four benchmark datasets. In particular, we propose a new way to generate samples consisting in sampling from the uniform distribution deriving intrinsically from the Riemannian manifold learned by a VAE. We also stress the proposed method’s robustness in the low data regime which is known as very challenging for deep generative models. Finally, we validate the method on a complex neuroimaging dataset combining both high dimensional data and low sample sizes.","This paper proposes a new VAE-based generative model that is based on the Riemannian manifold of the latent space of a VAE. In particular, the authors propose a new way to generate samples consisting in sampling from the uniform distribution deriving intrinsically from the manifold learned by the VAE model. The authors also show that the proposed method is robust in the low data regime which is known as very challenging for deep generative models. Finally, they validate the method on a complex neuroimaging dataset combining both high dimensional data and low sample sizes."
494,SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"Multi-head attention is a driving force behind state-of-the-art transformers which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them can be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer-MGK accelerates training and inference, has fewer parameters, and requires less FLOPs to compute while achieving comparable or better accuracy across tasks. Transformer-MGK can also be easily extended to use with linear attentions. We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications including language modeling and tasks that involve very long sequences. On the Wikitext-103 and Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or better performance to the baseline transformers with 8 heads.",This paper proposes to replace redundant heads in transformers with a mixture of Gaussian keys at each head in order to improve the performance of multi-head attention. The key idea is to use a Gaussian mixture model to replace the redundant heads. The proposed method is evaluated on the Wikitext-103 and Long Range Arena benchmark and shows comparable or better performance compared to the baseline transformers.
495,SP:82731dcce233e748f63382e09b6224a513fe9689,"Spatial navigation in biological agents relies on the interplay between allothetic (visual, olfactory, auditory,... ) and idiothetic (proprioception, linear and angular velocity,... ) signals. How to combine and exploit these two streams of information, which vastly differ in terms of availability and reliability, is a crucial issue. In the context of a new two–dimensional continuous environment we developed, we propose a direct-inverse model of environment dynamics to fuse image and action related signals, allowing reconstruction of the action relating the two successive images, as well as prediction of the new image from its current value and the action. The definition of those models naturally leads to the proposal of a minimalistic recurrent architecture, called Resetting Path Integrator (RPI), that can easily and reliably be trained to keep track of its position relative to its starting point during a sequence of movements. RPI updates its internal state using the (possibly noisy) self-motion signal, and occasionally resets it when the image signal is present. Notably, the internal state of this minimal model exhibits strong correlation with position in the environment due to the direct-inverse models, is stable across long trajectories through resetting, and allows for disambiguation of visually confusing positions in the environment through integration of past movement, making it a prime candidate for a cognitive map. Our architecture is compared to off-the-shelf LSTM networks on identical tasks, and consistently shows better performance while also offering more interpretable internal dynamics and higher-quality representations.","This paper proposes a method for integrating image-based and action-based information for navigation in a two-dimensional continuous environment. In particular, the authors propose to use a direct-inverse model of environment dynamics to fuse image and action related signals, allowing reconstruction of the action relating the two successive images, as well as prediction of the new image from its current value and the action. The authors propose a minimalistic recurrent architecture, called Resetting Path Integrator (RPI), that can easily and reliably be trained to keep track of its position relative to its starting point during a sequence of movements. RPI updates its internal state using the (possibly noisy) self-motion signal, and occasionally resets it when the image signal is present. The internal state of this minimal model exhibits strong correlation with position in the environment due to the direct inverse models, is stable across long trajectories through resetting, and allows for disambiguation of visually confusing positions in the environments through integration of past movement, making it a prime candidate for a cognitive map."
496,SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"An important characteristic of neural networks is their ability to learn representations of the input data with effective features for prediction, which is believed to be a key factor to their superior empirical performance. To better understand the source and benefit of feature learning in neural networks, we consider learning problems motivated by practical data, where the labels are determined by a set of class relevant patterns and the inputs are generated from these along with some background patterns. We prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, the structure of the input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no polynomial algorithm in the Statistical Query model can learn even weakly. These results provide theoretical evidence showing that feature learning in neural networks depends strongly on the input structure and leads to the superior performance. Our preliminary experimental results on synthetic and real data also provide positive support.","This paper studies the problem of feature learning in neural networks, where the labels are determined by a set of class relevant patterns and the inputs are generated from these patterns along with some background patterns. The authors prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, the structure of the input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, no polynomials algorithm in the Statistical Query model can learn even weakly. The results provide theoretical evidence showing that feature learning depends strongly on the input structure and leads to the superior performance."
497,SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,"Understanding the robustness of machine learning models to adversarial examples generated by test-time adversaries is a problem of great interest. Recent theoretical work has derived lower bounds on how robust any model can be, when a data distribution and attacker constraints are specified. However, these bounds only apply to arbitrary classification functions and do not account for specific architectures and models used in practice, such as neural networks. In this paper, we develop a methodology to analyze the robustness of fixed feature extractors, which in turn provides bounds on the robustness of any classifier trained on top of it. The tightness of these bounds relies on the effectiveness of the method used to find collisions between pairs of perturbed examples at deeper layers. For linear feature extractors, we provide closed-form expressions for collision finding while for arbitrary feature extractors, we propose a bespoke algorithm based on the iterative solution of a convex program that provably finds collisions. We utilize our bounds to identify the layers of robustly trained models that contribute the most to a lack of robustness, as well as compare the same layer across different training methods to provide a quantitative comparison of their relative robustness.","This paper studies the robustness of feature extractors to adversarial attacks. The authors propose a method for finding adversarial collisions between features extracted from a fixed feature extractor and a classifier trained on top of it, which can then be used to provide a lower bound on the classifier's robustness. The lower bound relies on the effectiveness of the method used to find collisions between pairs of perturbed examples at deeper layers. The upper bound is based on the iterative solution of a convex program that provably finds collisions between two points in the input space."
498,SP:874b5fa51924cbcceed490d98a0ea80f74586b32,"Offline reinforcement learning (RL) shows promise of applying RL to real-world problems by effectively utilizing previously collected data. Most existing offline RL algorithms use regularization or constraints to suppress extrapolation error for actions outside the dataset. In this paper, we adopt a different framework, which learns the V -function instead of the Q-function to naturally keep the learning procedure within the offline dataset. To enable effective generalization while maintaining proper conservatism in offline learning, we propose Expectile V -Learning (EVL), which smoothly interpolates between the optimal value learning and behavior cloning. Further, we introduce implicit planning along offline trajectories to enhance learned V -values and accelerate convergence. Together, we present a new offline method called Value-based Episodic Memory (VEM). We provide theoretical analysis for the convergence properties of our proposed VEM method, and empirical results in the D4RL benchmark show that our method achieves superior performance in most tasks, particularly in sparse-reward tasks.","This paper proposes Expectile V-Learning (EVL), a method for offline reinforcement learning that learns the V-function instead of the Q-function to naturally keep the learning procedure within the offline dataset. The authors also propose implicit planning along offline trajectories to enhance learned V-values and accelerate convergence. Empirical results on the D4RL benchmark show that the proposed method achieves superior performance in most tasks, particularly in sparse-reward tasks."
499,SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"There has been great interest in enhancing the robustness of neural network classifiers to defend against adversarial perturbations through adversarial training, while balancing the trade-off between robust accuracy and standard accuracy. We propose a novel adversarial training framework that learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization. Inspired by MAML-based approaches, we formulate weighted adversarial training as a bilevel optimization problem where the upper-level task corresponds to learning a robust classifier, and the lower-level task corresponds to learning a parametric function that maps from a sample’s multi-class margin to an importance weight. Extensive experiments demonstrate that our approach improves both clean and robust accuracy compared to related techniques and state-of-the-art baselines.","This paper proposes a novel adversarial training framework that learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization. Inspired by MAML-based approaches, the authors formulate weighted adversarial learning as a bilevel optimization problem where the upper-level task corresponds to learning a robust classifier, and the lower-level tasks corresponds to learn a parametric function that maps from a sample’s multi-class margin to an importance weight. Extensive experiments demonstrate that the proposed method improves both clean and robust accuracy compared to related techniques and state-of-the-art baselines."
500,SP:3ad36be6b6900aabe43da043461cf178ce977082,"Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E(3) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. This model, composed of steerable MLPs, is able to incorporate geometric and physical information in both the message and update functions. Through the definition of steerable node attributes, the MLPs provide a new class of activation functions for general use with steerable feature fields. We discuss ours and related work through the lens of equivariant non-linear convolutions, which further allows us to pin-point the successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable) point convolutions; steerable messages improve upon recent equivariant graph networks that send invariant messages. We demonstrate the effectiveness of our method on several tasks in computational physics and chemistry and provide extensive ablation studies.","This paper proposes a method for steerable equivariant graph neural networks (SEGNNs) that can incorporate geometric and physical information in both the message and update functions. Specifically, the authors define steerable node attributes and non-linear message aggregation. The authors also propose a new class of activation functions for general use with steerable feature fields. Experiments are conducted on several tasks in computational physics and chemistry."
501,SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"Differentiable physics modeling combines physics models with gradient-based learning to provide model explicability and data efficiency. It has been used to learn dynamics, solve inverse problems and facilitate design, and is at its inception of impact. Current successes have concentrated on general physics models such as rigid bodies, deformable sheets, etc, assuming relatively simple structures and forces. Their granularity is intrinsically coarse and therefore incapable of modelling complex physical phenomena. Fine-grained models are still to be developed to incorporate sophisticated material structures and force interactions with gradient-based learning. Following this motivation, we propose a new differentiable fabrics model for composite materials such as cloths, where we dive into the granularity of yarns and model individual yarn physics and yarn-to-yarn interactions. To this end, we propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. These forces, albeit applied to cloths, are ubiquitous in various physical systems. Through comprehensive evaluation and comparison, we demonstrate our model’s explicability in learning meaningful physical parameters, versatility in incorporating complex physical structures and heterogeneous materials, data-efficiency in learning, and high-fidelity in capturing subtle dynamics.","This paper proposes a new differentiable physics model for differentiable fabrics for composite materials such as cloths. The authors propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. They demonstrate their model’s explicability in learning meaningful physical parameters, versatility in incorporating complex physical structures and heterogeneous materials, data-efficiency in learning, and high-fidelity in capturing subtle dynamics."
502,SP:2c8358c095b10981d3015b9f6c75765419a9480d,"We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task with possibly many desirable goals can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy. Importantly, we provide two main theoretical results: we give bounds on the performance of the transferred policy on a new task, and we give bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent’s lifetime to generalise over a distribution. We verify our approach in a series of experiments, where we perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks. We also demonstrate that as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task. Finally, we demonstrate our approach in the full lifelong setting where an agent receives tasks from an unknown distribution and, starting from zero skills, is able to quickly generalise over the task distribution after learning only a few tasks—which are sub-logarithmic in the size of the task space.","This paper proposes a method for transfer learning in the context of reinforcement learning, where the goal is to learn a policy for a new task that maximizes the performance of the learned policy on the new task. The method is based on the idea of logical composition in reinforcement learning. In particular, the authors consider the case where the agent has access to a set of base tasks and a new set of tasks, and the goal of the agent is to decide whether it should learn a task-specific skill (e.g. a new skill) or not. The authors provide theoretical analysis of the transfer learning problem, and provide bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent’s lifetime to generalise over a distribution. The paper also provides experimental results in a series of experiments, where they show that the proposed method can be used to learn new tasks with sub-logarithmic learning rates. They also demonstrate that as a side effect of their approach, an agent can produce an interpretable Boolean expression of its understanding of the current task."
503,SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"Nowadays, with the rising number of sensors in sectors such as healthcare and industry, the problem of multivariate time series classification (MTSC) is getting increasingly relevant and is a prime target for machine and deep learning solutions. Their expanding adoption in real-world environments is causing a shift in focus from the pursuit of ever higher prediction accuracy with complex models towards practical, deployable solutions that balance accuracy and parameters such as prediction speed. An MTSC solution that has attracted attention recently is ROCKET, based on random convolutional kernels, both because of its very fast training process and its state-of-the-art accuracy. However, the large number of features it utilizes may be detrimental to inference time. Examining its theoretical background and limitations enables us to address potential drawbacks and present LightWaveS: a distributed solution for accurate MTSC, which is fast both during training and inference. Specifically, utilizing a wavelet scattering transformation of the time series and distributed feature selection, we manage to create a solution which employs just 2,5% of the ROCKET features, while achieving accuracy comparable to recent deep learning solutions. LightWaveS also scales well with more nodes and large numbers of channels. In addition, it can significantly reduce the input size and also provide insight to an MTSC problem by keeping only the most useful channels. We present three versions of our algorithm and their results on training time, accuracy, inference speedup and scalability. We show that we achieve speedup ranging from 9x to 65x compared to ROCKET during inference on an edge device, on datasets with comparable accuracy.","This paper proposes a distributed method for multivariate time series classification (MTSC) based on wavelet scattering. The main idea is to use wavelets to reduce the number of channels used in the original random convolutional kernel (ROCKET) method, which is a popular method for MTSC. The authors show that by using wavelets, they can use only 2,5% of the features of the original RKHS method and achieve comparable accuracy. They also show that the proposed method can scale well with more nodes and large channels."
504,SP:db43614ca016280a79448f44a97c81c8ff5ba981,"We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs’ outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on GLUE and SQuAD benchmarks for BERT base-sized models. We plan to release our pretrained models for future uses.","This paper proposes a method for pretraining text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. The main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, the authors jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. The authors propose to learn mixture weights over the auxiliary MLMs’ outputs to maximize the discriminator loss by backpropagating the gradient from the discriminators via Gumbel-Softmax. For better pretraining efficiency, they propose to assemble multiple auxiliary models into one unified auxiliary model."
505,SP:db3825633ab5d0671340390b23ab655838cc38b2,"Extracting relational knowledge from large pre-trained language models by a clozestyle sentence serving as a query has shown promising results. In particular, language models can be queried similar to knowledge graphs. The performance of the relational fact extraction task depends significantly on the query sentence, also known under the term prompt. Tuning these prompts has shown to increase the precision on standard language models by a maximum of around 12% points. However, usually large amounts of data in the form of existing knowledge graph facts and large text corpora are needed to train the required additional model. In this work, we propose using a completely different approach: Instead of spending resources on training an additional model, we simply perform an adaptive fine-tuning of the pre-trained language model on the standard fill-mask task using a small training dataset of existing facts from a knowledge graph. We investigate the differences between complex prompting techniques and adaptive fine-tuning in an extensive evaluation. Remarkably, adaptive fine-tuning outperforms all baselines, even by using significantly fewer training facts. Additionally, we analyze the transfer learning capabilities of this adapted language model by training on a restricted set of relations to show that even fewer training relations are needed to achieve high knowledge extraction quality.","This paper proposes an adaptive fine-tuning method for relational fact extraction from pre-trained language models by a clozestyle sentence serving as a query. The authors propose to use a small training dataset of existing facts from a knowledge graph to train a language model on the standard fill-mask task using a small number of facts from the knowledge graph. They evaluate the performance of the proposed method on a variety of datasets and compare it with the standard prompt-based methods. The results show that the proposed approach outperforms all baselines, even by using significantly fewer training facts. "
506,SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"Knowledge bases have multi-relations with distinctive properties. Most properties such as symmetry, inversion, and composition can be handled by the Euclidean embedding models. Nevertheless, transitivity is a special property that cannot be modeled efficiently in the Euclidean space. Instead, the hyperbolic space characterizes the transitivity naturally because of its tree-like properties. However, the hyperbolic space reveals its weakness for other relations. Therefore, building a representation learning framework for all relation properties is highly difficult. In this paper, we propose to learn the knowledge base embeddings in different geometric spaces and apply manifold alignment to align the shared entities. The aligned embeddings are evaluated on the out-of-taxonomy entity typing task, where we aim to predict the types of the entities from the knowledge graph. Experimental results on two datasets based on YAGO3 demonstrate that our approach has significantly good performances, especially in low dimensions and on small training rates.","This paper proposes a representation learning framework for knowledge base embeddings in different geometric spaces and applies manifold alignment to align the shared entities. The proposed method is evaluated on the out-of-taxonomy entity typing task, where the goal is to predict the types of entities from the knowledge graph. The authors evaluate the proposed method on two datasets based on YAGO3 and demonstrate that their approach has significantly good performances."
507,SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"Most real-world knowledge graphs are characterized by a frequency distribution with a long-tail where a significant fraction of relations occurs only a handful of times. This observation has given rise to recent interest in low-shot learning methods that are able to generalize from only a few examples per relation. The existing approaches, however, are tailored to static knowledge graphs and do not easily generalize to temporal settings, where data scarcity poses even bigger problems, e.g., due to occurrence of new, previously unseen relations. We address this shortcoming by proposing a one-shot learning framework for link prediction in temporal knowledge graphs. Our proposed method employs a self-attention mechanism to effectively encode temporal interactions between entities, and a network to compute a similarity score between a given query and a (one-shot) example. Our experiments show that the proposed algorithm outperforms the state of the art baselines for two well-studied benchmarks while achieving significantly better performance for sparse relations."," is a one-shot learning framework for link prediction in temporal knowledge graphs. The proposed method employs a self-attention mechanism to effectively encode temporal interactions between entities, and a network to compute a similarity score between a given query and a (one-shot) example. The experiments show that the proposed algorithm outperforms the state-of-the-art baselines for two well-studied benchmarks."
508,SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn – most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.",This paper proposes a method for learning a model for visual reasoning tasks. The model is based on a neural network architecture that learns to query existing modules and composes their outputs in order to produce its own output. The method is evaluated on a set of visual reasoning benchmarks and compared with a baseline model. The authors show that their model is more interpretable than an attention-based baseline. 
509,SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"Bottleneck structures with identity (e.g., residual) connection are now emerging popular paradigms for designing deep convolutional neural networks (CNN), for processing large-scale features efficiently. In this paper, we focus on the information-preserving nature of identity connection and utilize this to enable a convolutional layer to have a new functionality of channel-selectivity, i.e., redistributing its computations to important channels. In particular, we propose Selective Convolutional Unit (SCU), a widely-applicable architectural unit that improves parameter efficiency of various modern CNNs with bottlenecks. During training, SCU gradually learns the channel-selectivity on-the-fly via the alternative usage of (a) pruning unimportant channels, and (b) rewiring the pruned parameters to important channels. The rewired parameters emphasize the target channel in a way that selectively enlarges the convolutional kernels corresponding to it. Our experimental results demonstrate that the SCU-based models without any postprocessing generally achieve both model compression and accuracy improvement compared to the baselines, consistently for all tested architectures.","This paper proposes a method for improving the efficiency of deep convolutional neural networks (CNNs) with bottlenecks. In particular, the authors propose a method called Selective Convolutional Unit (SCU) that gradually learns the channel-selectivity on-the-fly via the alternative usage of (a) pruning unimportant channels, and (b) rewiring the pruned parameters to important channels. The authors demonstrate that the SCU-based models without any postprocessing generally achieve both model compression and accuracy improvement compared to the baselines, consistently for all tested architectures."
510,SP:2d80fa4bc440061be2234b5070503d3fa056baed,"We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.","This paper studies the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). The authors propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. The authors show that the proposed algorithm outperforms previous methods for PU learning on various real-world datasets."
511,SP:5f312626b0613d2e07c59214c5f00db208a98717,"One approach to deal with the statistical inefficiency of neural networks is to rely on auxiliary losses that help to build useful representations. However, it is not always trivial to know if an auxiliary task will be helpful for the main task and when it could start hurting. We propose to use the cosine similarity between gradients of tasks as an adaptive weight to detect when an auxiliary loss is helpful to the main loss. We show that our approach is guaranteed to converge to critical points of the main task and demonstrate the practical usefulness of the proposed algorithm in a few domains: multi-task supervised learning on subsets of ImageNet, reinforcement learning on gridworld, and reinforcement learning on Atari games.","This paper proposes a method to detect when an auxiliary loss is helpful to the main loss by using cosine similarity between gradients of tasks as an adaptive weight. The authors show that their approach is guaranteed to converge to critical points of the main task and demonstrate the practical usefulness of the proposed algorithm in a few domains: multi-task supervised learning on subsets of ImageNet, gridworld, and reinforcement learning on Atari games."
512,SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.","This paper proposes a geometric framework to analyze the high-dimensional geometry of adversarial examples in machine learning models. In particular, the authors show that for low-dimensional data manifolds, there are many directions off the manifold in which to construct adversarial example. The authors also show that adversarial training in balls around the data is sample inefficient, and provide sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial learning are robust."
513,SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time. To address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty. We evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.","This paper proposes a new representation learning method for time series data. The authors propose to learn discrete representations of time series using a gradient-based self-organizing map algorithm and a Markov model in the representation space. The proposed method is evaluated on three datasets: Fashion-MNIST, a chaotic Lorenz attractor system, and a challenging real-world medical time series application on the eICU data set. The results show that the proposed method achieves better clustering performance and interpretability."
514,SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors – regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space. We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.","This paper investigates the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. The authors show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. They prove that there is a trade-off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. They also provide a general method of creating non- linear interpolations, that is easily applicable to a large family of commonly used latent distributions."
515,SP:19b63ca635712f1509ca6e0141303c192f2709e0,"Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry –as opposed to Euclidean geometry– can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT’14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.","This paper proposes to use hyperbolic embeddings for the attention mechanism of deep neural networks. The main idea is to learn the parameters of a shallow neural network in a Euclidean space, where the number of objects grows exponentially for any semantic distance from the query. The authors propose to use the embedding space more efficiently by only changing the geometry of embedding of object representations. The proposed method shows improvements in generalization on neural machine translation on WMT’14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact."
516,SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary’s capability to conduct attacks on black-box networks. This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels. First, we define the threat model for these attacks: our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine where the victim’s deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network using the internal information extracted via Flush+Reload, a cache side-channel technique. Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim’s entire network architecture. In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having observed only one forward propagation. Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pretrained model in a transfer learning setting. From this meta-model, we evaluate the importance of the observed attributes in the fingerprinting process. Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker’s observations. Our empirical security analysis represents a step toward understanding DNNs’ vulnerability to cache side-channel attacks.","This paper studies the problem of cache side-channel attacks that extract the architecture information of deep neural networks (DNNs). The authors propose two attacks: DeepRecon, an attack that reconstructs the architecture of the victim network using the internal information extracted via Flush+Reload, a cache side channel technique. The authors also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pretrained model in a transfer learning setting. Finally, the authors propose and evaluate new framework-level defense techniques that obfuscate our attacker’s observations."
517,SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"In this paper we developed a hierarchical network model, called Hierarchical Prediction Network (HPNet) to understand how spatiotemporal memories might be learned and encoded in a representational hierarchy for predicting future video frames. The model is inspired by the feedforward, feedback and lateral recurrent circuits in the mammalian hierarchical visual system. It assumes that spatiotemporal memories are encoded in the recurrent connections within each level and between different levels of the hierarchy. The model contains a feed-forward path that computes and encodes spatiotemporal features of successive complexity and a feedback path that projects interpretation from a higher level to the level below. Within each level, the feed-forward path and the feedback path intersect in a recurrent gated circuit that integrates their signals as well as the circuit’s internal memory states to generate a prediction of the incoming signals. The network learns by comparing the incoming signals with its prediction, updating its internal model of the world by minimizing the prediction errors at each level of the hierarchy in the style of predictive self-supervised learning. The network processes data in blocks of video frames rather than a frame-to-frame basis. This allows it to learn relationships among movement patterns, yielding state-of-the-art performance in long range video sequence predictions in benchmark datasets. We observed that hierarchical interaction in the network introduces sensitivity to memories of global movement patterns even in the population representation of the units in the earliest level. Finally, we provided neurophysiological evidence, showing that neurons in the early visual cortex of awake monkeys exhibit very similar sensitivity and behaviors. These findings suggest that predictive self-supervised learning might be an important principle for representational learning in the visual cortex.","This paper proposes a hierarchical network model for video sequence prediction. The model is inspired by the feedforward, feedback and lateral recurrent circuits in the mammalian hierarchical visual system. It assumes that spatiotemporal memories are encoded in the recurrent connections within each level and between different levels of the hierarchy. Within each level, the feed-forward path and the feedback path intersect in a recurrent gated circuit that integrates their signals as well as the circuit’s internal memory states to generate a prediction of incoming signals. The network learns by comparing the incoming signals with its prediction, updating its internal model of the world by minimizing the prediction errors at each level of the hierarchical hierarchy in the style of predictive self-supervised learning. Experiments show that hierarchical interaction in the network introduces sensitivity to memories of global movement patterns even in the population representation of the units in the earliest level."
518,SP:fb74e57f35666742caf651e6da33b5defcf259a8,"In this work we propose a method to compute continuous embeddings for kmers from raw RNA-seq data, in a reference-free fashion. We report that our model captures information of both DNA sequence similarity as well as DNA sequence abundance in the embedding latent space. We confirm the quality of these vectors by comparing them to known gene sub-structures and report that the latent space recovers exon information from raw RNA-Seq data from acute myeloid leukemia patients. Furthermore we show that this latent space allows the detection of genomic abnormalities such as translocations as well as patient-specific mutations, making this representation space both useful for visualization as well as analysis.","This paper proposes a method to compute continuous embeddings for DNA sequences from raw RNA-seq data in a reference-free fashion. The proposed method is based on a neural network architecture that computes a latent embedding space for DNA sequence similarity and DNA sequence abundance. The authors show that their model captures information of both DNA sequences similarity as well as DNA sequences abundance in the embedding latent space. They confirm the quality of these vectors by comparing them to known gene sub-structures and report that the latent space recovers exon information from raw RNSeq data from acute myeloid leukemia patients. Furthermore, this latent space allows the detection of genomic abnormalities such as translocations and patient-specific mutations."
519,SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"In this paper we propose a novel approach to model compression termed Architecture Compression. Instead of operating on the weight or filter space of the network like classical model compression methods, our approach operates on the architecture space. A 1-D CNN encoder/decoder is trained to learn a mapping from discrete architecture space to a continuous embedding and back. Additionally, this embedding is jointly trained to regress accuracy and parameter count in order to incorporate information about the architecture’s effectiveness on the dataset. During the compression phase, we first encode the network and then perform gradient descent in continuous space to optimize a compression objective function that maximizes accuracy and minimizes parameter count. The final continuous feature is then mapped to a discrete architecture using the decoder. We demonstrate the merits of this approach on visual recognition tasks such as CIFAR-10/100, FMNIST and SVHN and achieve a greater than 20x compression on CIFAR-10.","This paper proposes a method for model compression based on the architecture of the network. The authors propose to use a 1-D CNN encoder/decoder to learn a mapping from discrete architecture space to a continuous embedding and back. This embedding is jointly trained to regress accuracy and parameter count in order to incorporate information about the architecture’s effectiveness on the dataset. During the compression phase, they first encode the network and then perform gradient descent in continuous space to optimize a compression objective function that maximizes accuracy and minimizes parameter count. The final continuous feature is then mapped to a discrete architecture using the decoder. Experiments are conducted on CIFAR-10/100, FMNIST and SVHN and achieve a greater than 20x compression."
520,SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"We propose a “plan online and learn offline” framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.","This paper proposes a method for planning online and learning offline, where the agent has an internal model and needs to continually act and learn in the world. The method is based on the synergistic relationship between local model-based control, global value function learning, and exploration. The authors study how local trajectory optimization can cope with approximation errors in the value function, and how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, trajectory optimization is used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of value function."
521,SP:771494fda4702cd8c7efbf225b19028f91b449b9,"Neural Machine Translation (NMT) systems rely on large amounts of parallel data. This is a major challenge for low-resource languages. Building on recent work on unsupervised and semi-supervised methods, we present an approach that combines zero-shot and dual learning. The latter relies on reinforcement learning, to exploit the duality of the machine translation task, and requires only monolingual data for the target language pair. Experiments on the UN corpus show that a zero-shot dual system, trained on English-French and English-Spanish, outperforms by large margins a standard NMT system in zero-shot translation performance on SpanishFrench (both directions). We also evaluate on newstest2014. These experiments show that the zero-shot dual method outperforms the LSTM-based unsupervised NMT system proposed in (Lample et al., 2018b), on the en− →fr task, while on the fr− →en task it outperforms both the LSTM-based and the Transformers-based unsupervised NMT systems.","This paper proposes a zero-shot dual learning method for neural machine translation (NMT). The method is based on reinforcement learning, where the goal is to learn a model that can be used for both zero shot and dual learning. The model is trained on English-French and English-Spanish, and is evaluated on the UN corpus and the newstest 2014 dataset. The results show that the proposed method outperforms the LSTM-based unsupervised NMT system proposed in (Lample et al., 2018b), on the en-to-en task, while on the fr-toen task it outperforms both LSTMs and Transformers-based models."
522,SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"Recent advances in Generative Adversarial Networks facilitated by improvements to the framework and successful application to various problems has resulted in extensions to multiple domains. IRGAN attempts to leverage the framework for Information-Retrieval (IR), a task that can be described as modeling the correct conditional probability distribution p(d|q) over the documents (d), given the query (q). The work that proposes IRGAN claims that optimizing their minimax loss function will result in a generator which can learn the distribution, but their setup and baseline term steer the model away from an exact adversarial formulation, and this work attempts to point out certain inaccuracies in their formulation. Analyzing their loss curves gives insight into possible mistakes in the loss functions and better performance can be obtained by using the co-training like setup we propose, where two models are trained in a co-operative rather than an adversarial fashion.","This paper studies the problem of information-retrieval (IR) in the context of GANs. The authors point out that the proposed IRGAN framework is not exactly adversarial, and propose a co-training setup where the two models are trained in a cooperative manner. The paper also proposes a new baseline term for IRGAN, which is based on the information retrieval framework. The main contribution of the paper is the analysis of the loss curves of IRGAN and the proposed co-learning setup."
523,SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"Variational auto-encoders (VAEs) offer a tractable approach when performing approximate inference in otherwise intractable generative models. However, standard VAEs often produce latent codes that are disperse and lack interpretability, thus making the resulting representations unsuitable for auxiliary tasks (e.g. classification) and human interpretation. We address these issues by merging ideas from variational auto-encoders and sparse coding, and propose to explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution. We derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efficient as in the standard VAE case. With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models. We show that these sparse representations are advantageous over standard VAE representations on two benchmark classification tasks (MNIST and Fashion-MNIST) by demonstrating improved classification accuracy and significantly increased robustness to the number of latent dimensions. Furthermore, we demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation.","This paper proposes a variational auto-encoder (VAE) based approach to model sparsity in the latent space of a VAE with a Spike and Slab prior distribution. The authors derive the evidence lower bound using a discrete mixture recognition function, thereby making approximate posterior inference as computational efficient as in the standard VAE case. The proposed approach is evaluated on MNIST and Fashion-MNIST and shows improved classification accuracy and significantly increased robustness to the number of latent dimensions."
524,SP:06a22143186fa2948fbe324ccae96a62ff12064e,"We propose a non-adversarial feature matching-based approach to train generative models. Our approach, Generative Feature Matching Networks (GFMN), leverages pretrained neural networks such as autoencoders and ConvNet classifiers to perform feature extraction. We perform an extensive number of experiments with different challenging datasets, including ImageNet. Our experimental results demonstrate that, due to the expressiveness of the features from pretrained ImageNet classifiers, even by just matching first order statistics, our approach can achieve state-of-the-art results for challenging benchmarks such as CIFAR10 and STL10.","This paper proposes a non-adversarial feature matching-based approach to train generative models. The proposed method, Generative Feature Matching Networks (GFMN), leverages pretrained neural networks such as autoencoders and ConvNet classifiers to perform feature extraction. The experimental results demonstrate that the proposed method can achieve state-of-the-art results for challenging benchmarks such as CIFAR10 and STL10."
525,SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,"Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the WeisfeilerLehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.","This paper studies the expressive power of graph neural networks (GNNs) in the context of graph classification tasks. The authors provide a theoretical analysis of the expressive properties of GNNs, and show that the discriminative power of popular GNN variants such as Graph Convolutional Networks and GraphSAGE cannot learn to distinguish certain simple graph structures. Then, the authors propose a simple architecture that is provably the most expressive among the class of Graph Neural Networks and is as powerful as the Weisfeiler-Lehman graph isomorphism test. Empirical results on several graph classification benchmarks validate the theoretical findings."
526,SP:51126f2dd37ce57d2614c9044ede1e43627f0829,"We present a framework for interpretable continual learning (ICL). We show that explanations of previously performed tasks can be used to improve performance on future tasks. ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task. The ICL idea is general and may be applied to many continual learning approaches. Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting. We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality. Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.","This paper proposes a method for interpretable continual learning (ICL) based on variational variational continual learning. The main idea is to use saliency maps to provide explanations of previously performed tasks and propose a new metric to assess the quality of the explanations. Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also as measured qualitatively and quantitatively using the proposed metric."
527,SP:27a565b3e5442b93d208652784051e640b0c1bfe,"Adversarial examples have been shown to be an effective way of assessing the robustness of neural sequence-to-sequence (seq2seq) models, by applying perturbations to the input of a model leading to large degradation in performance. However, these perturbations are only indicative of a weakness in the model if they do not change the semantics of the input in a way that would change the expected output. Using the example of machine translation (MT), we propose a new evaluation framework for adversarial attacks on seq2seq models taking meaning preservation into account and demonstrate that existing methods may not preserve meaning in general. Based on these findings, we propose new constraints for attacks on word-based MT systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs. Furthermore, we show that performing adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness without hurting test performance.","This paper proposes a new evaluation framework for adversarial attacks on neural sequence-to-sequence models taking meaning preservation into account. Specifically, the authors propose a new constraint for attacks on word-based machine translation systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs. Furthermore, they show that performing adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness without hurting test performance."
528,SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper puts forward a broad-spectrum improvement for reinforcement learning algorithms, which combines the policies using original rewards and inverse (negative) rewards. The policies using inverse rewards are competitive with the original policies, and help the original policies correct their mis-actions. We have proved the convergence of the inverse policies. The experiments for some games in OpenAI gym show that the hybrid polices based on deep Q-learning, double Q-learning, and on-policy actor-critic obtain the rewards up to 63.8%, 97.8%, and 54.7% more than the original algorithms. The improved polices are more stable than the original policies as well.","This paper proposes to combine the policies using original rewards and inverse (negative) rewards to improve the performance of reinforcement learning algorithms. In particular, the authors propose to use deep Q-learning, double-Q-learning and on-policy actor-critic as the policies. The authors show that the proposed policies are competitive with the original policies and help the original policy correct its mis-actions. The experiments on OpenAI gym show the effectiveness of the proposed methods."
529,SP:89a732b57934d08b937c93560f391b7758e54f8a,"Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.","This paper proposes a method for learning a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. The model learns to recognize the object parts via a layered image representation, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure, and model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that the proposed method works well on all three tasks: segmenting object parts, building their hierarchical structure and capturing their motion distributions."
530,SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,"Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks poorly generalize from such noisy training datasets. In this paper, we propose a novel inference method, Deep Determinantal Generative Classifier (DDGC), which can obtain a more robust decision boundary under any softmax neural classifier pre-trained on noisy datasets. Our main idea is inducing a generative classifier on top of hidden feature spaces of the discriminative deep model. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy, with neither re-training of the deep model nor changing its architectures. In particular, we show that DDGC not only generalizes well from noisy labels, but also is robust against adversarial perturbations due to its large margin property. Finally, we propose the ensemble version of DDGC to improve its performance, by investigating the layer-wise characteristics of generative classifier. Our extensive experimental results demonstrate the superiority of DDGC given different learning models optimized by various training techniques to handle noisy labels or adversarial samples. For instance, on CIFAR10 dataset containing 45% noisy training labels, we improve the test accuracy of a deep model optimized by the state-of-the-art noise-handling training method from 33.34% to 43.02%.","This paper proposes Deep Determinantal Generative Classifier (DDGC), which is a generative classifier on top of a discriminative deep model. The main idea is to use the minimum covariance determinant estimator (MCE) to estimate the parameters of the classifier. The proposed method is evaluated on CIFAR-10 dataset with noisy labels and adversarial perturbations. The experimental results show that the proposed method can improve the classification accuracy."
531,SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"Common approaches to Reinforcement Learning (RL) are seriously challenged by large-scale applications involving huge state spaces and sparse delayed reward feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address this scalability issue by learning action selection policies at multiple levels of temporal abstraction. Abstraction can be had by identifying a relatively small set of states that are likely to be useful as subgoals, in concert with the learning of corresponding skill policies to achieve those subgoals. Many approaches to subgoal discovery in HRL depend on the analysis of a model of the environment, but the need to learn such a model introduces its own problems of scale. Once subgoals are identified, skills may be learned through intrinsic motivation, introducing an internal reward signal marking subgoal attainment. In this paper, we present a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We demonstrate the efficiency of our method on two RL problems with sparse delayed feedback: a variant of the rooms environment and the ATARI 2600 game called Montezuma’s Revenge.","This paper proposes an incremental unsupervised method for subgoal discovery in Hierarchical Reinforcement Learning (HRL). The proposed method is based on incremental learning over a small memory of the most recent experiences of the agent. The method learns subgoals and skills together, based on experiences in the environment. The authors demonstrate the efficiency of their method on two RL problems with sparse delayed feedback."
532,SP:e5861538bc8bb9165cb33299bbf12dd875abf976,"Recent efforts to combine Representation Learning with Formal Methods, commonly known as Neuro-Symbolic Methods, have given rise to a new trend of applying rich neural architectures to solve classical combinatorial optimization problems. In this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem. Our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure, and an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out-of-sample generalization performance of our framework compared to the recently developed NeuroSAT method.","This paper proposes a neural architecture for solving Circuit Satisfiability problems. The architecture is based on a rich embedding architecture that encodes the problem structure, and an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out-of-sample generalization performance of the proposed method compared to the recently developed NeuroSAT method."
533,SP:ff3e5d44619df3825632b0b1a943add081364861,"Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy gradient (TD3), another off-policy deep RL algorithm which improves over DDPG. We evaluate the resulting method, CEM-RL, on a set of benchmarks classically used in deep RL. We show that CEM-RL benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.","This paper proposes a combination of cross-entropy method (CEM) and Twin Delayed Deep Deterministic Policy Gradient (TD3) to improve the sample efficiency of off-policy deep reinforcement learning (RL) algorithms. In particular, the authors propose to combine TD3 and CEM in order to improve sample efficiency. The authors evaluate the proposed method on a set of benchmarks classically used in deep RL and show that CEM-RL outperforms its competitors."
534,SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data. To this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation. In particular, IMV-LSTM is equipped with hidden state matrix and update process, so as to learn variableswise hidden states. On top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. Extensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. It also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data.",This paper proposes a multi-variable LSTM recurrent neural network (IMV-LSTM) for multi-variate time series forecasting and knowledge extraction. The proposed method is based on a mixture attention mechanism and a summarization method to quantify the temporal and variable importance in data. Extensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-lSTM in comparison to a variety of baselines.
535,SP:1c26660569b579f060f7b4a31e321c6d2356b928,"Many techniques are developed to defend against adversarial examples at scale. So far, the most successful defenses generate adversarial examples during each training step and add them to the training data. Yet, this brings significant computational overhead. In this paper, we investigate defenses against adversarial attacks. First, we propose feature smoothing, a simple data augmentation method with little computational overhead. Essentially, feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point. The intuition behind feature smoothing is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. Our experiments on MNIST and CIFAR10 datasets explore different combinations of known regularization and data augmentation methods and show that feature smoothing with logit squeezing performs best for both adversarial and clean accuracy. Second, we propose an unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. We show that under some symmetrical assumptions, label smoothing, logit squeezing, weight decay, mix up and feature smoothing all produce an unbiased estimation of the decision boundary with smaller estimated variance. All of those methods except weight decay are also stable when the assumptions no longer hold.","This paper proposes feature smoothing, a simple data augmentation method for adversarial defense against adversarial attacks. Feature smoothing trains a neural network on virtual training data as an interpolation of features from a pair of samples, with the new label remaining the same as the dominant data point. The intuition is to generate virtual data points as close as adversarial examples, and to avoid the computational burden of generating data during training. The experiments on MNIST and CIFAR-10 datasets explore different combinations of known regularization and data augmentations methods, and show that feature-smoothing with logit squeezing performs best for both adversarial and clean accuracy."
536,SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"Understanding theoretical properties of deep and locally connected nonlinear network, such as deep convolutional neural network (DCNN), is still a hard problem despite its empirical success. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework bridges data distribution with gradient descent rules, favors disentangled representations and is compatible with common regularization techniques such as Batch Norm, after a novel discovery of its projection nature. The framework is built upon teacher-student setting, by projecting the student’s forward/backward pass onto the teacher’s computational graph. We do not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc). Our framework could help facilitate theoretical analysis of many practical issues, e.g. disentangled representations in deep networks.","This paper proposes a novel theoretical framework for deep neural networks with ReLU nonlinearity. The framework is built upon teacher-student setting, by projecting the student's forward/backward pass onto the teacher's computational graph. The authors do not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc). The framework could help facilitate theoretical analysis of many practical issues, e.g. disentangled representations in deep networks."
537,SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"Prefrontal cortex (PFC) is a part of the brain which is responsible for behavior repertoire. Inspired by PFC functionality and connectivity, as well as human behavior formation process, we propose a novel modular architecture of neural networks with a Behavioral Module (BM) and corresponding end-to-end training strategy. This approach allows efficient learning of behaviors and preferences representation. This property is particularly useful for user modeling (as for dialog agents) and recommendation tasks, as allows learning personalized representations of different user states. In the experiment with video games playing, the results show that the proposed method allows separation of main task’s objectives and behaviors between different BMs. The experiments also show network extendability through independent learning of new behavior patterns. Moreover, we demonstrate a strategy for an efficient transfer of newly learned BMs to unseen tasks.","This paper proposes a modular architecture of neural networks with a Behavioral Module (BM) and corresponding end-to-end training strategy. This approach allows efficient learning of behaviors and preferences representation. This property is particularly useful for user modeling (as for dialog agents) and recommendation tasks, as it allows learning personalized representations of different user states. The experiments also show network extendability through independent learning of new behavior patterns. Moreover, the authors demonstrate a strategy for an efficient transfer of newly learned BMs to unseen tasks."
538,SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,"The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.","This paper proposes a differentiable formulation for the neuromodulation of plasticity in neural networks. This formulation is based on differentiable Hebbian plasticity, which can be used to train neural networks with gradient descent. The authors show that this formulation is able to improve the performance of neural networks on both reinforcement learning and supervised learning tasks. In particular, they show that the differentiable plasticity can be applied to a neural network with millions of parameters."
539,SP:1ab5d94d31e99351433436c026799c8aa597bf73,"Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.","This paper proposes a novel quantization method for deep neural networks. The authors propose to re-train the full precision model, followed by directly optimizing the corresponding binary model. They also propose a new loss function to regularize the weights, resulting in reduced quantization error. Experiments on CIFAR and WikiText-2 show the effectiveness of the proposed method."
540,SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"We consider visual domains in which a class label specifies the content of an image, and class-irrelevant properties that differentiate instances constitute the style. We present a domain-independent method that permits the open-ended recombination of style of one image with the content of another. Open ended simply means that the method generalizes to style and content not present in the training data. The method starts by constructing a content embedding using an existing deep metric-learning technique. This trained content encoder is incorporated into a variational autoencoder (VAE), paired with a to-be-trained style encoder. The VAE reconstruction loss alone is inadequate to ensure a decomposition of the latent representation into style and content. Our method thus includes an auxiliary loss, leakage filtering, which ensures that no style information remaining in the content representation is used for reconstruction and vice versa. We synthesize novel images by decoding the style representation obtained from one image with the content representation from another. Using this method for data-set augmentation, we obtain state-of-the-art performance on few-shot learning tasks. In any domain involving classification, entities are distinguished not only by class label but also by attributes orthogonal to class label. For example, if faces are classified by identity, within-class variation is due to lighting, pose, expression, hairstyle; if masterworks of art are classified by the painter, within-class variation is due to choice of subject matter. Following tradition (Tenenbaum & Freeman, 2000), we refer to betweenand within-class variation as content and style, respectively. What constitutes content is defined with respect to a task. For example, in a face-recognition task, identity is the content; in an emotion-recognition task, expression is the content. There has been a wealth of research focused on decomposing content and style, with the promise that decompositions might provide insight into a domain or improve classification performance. Decompositions also allow for the synthesis of novel entities by recombining the content of one entity with the style of another. Recombinations are interesting as a creative exercise (e.g., transforming the musical composition of one artist in the style of another) or for data set augmentation. We propose an approach to content-style decomposition and recombination. We refer to the method as STOC, for Style Transfer onto Open-Ended Content. Our approach is differentiated from past work in the following ways. First, STOC can transfer to","This paper proposes a method for open-ended style transfer onto open-ended content. The method is based on a VAE-VAE architecture with a style encoder and a content encoder. The authors propose to use an auxiliary loss, leakage filtering, to ensure that no style information remains in the content representation is used for reconstruction and vice versa. The proposed method is evaluated on few-shot learning tasks and achieves state-of-the-art performance."
541,SP:d37e15cde7765fca87595a242f0a4511b3346d46,"This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of permissibility are defined under SAP. The first type says that after an action at is performed in a state st and the agent reaches the new state st+1, the agent can decide whether the action at is permissible or not permissible in state st. The second type says that even without performing the action at in state st, the agent can already decide whether at is permissible or not in st. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried. We incorporate the proposed SAP property into two state-of-the-art deep RL algorithms to guide their state-action exploration. Results show that the SAP guidance can markedly speed up training.","This paper proposes a new method to speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of states are defined under SAP: (1) states where the agent can decide whether the action at is permissible or not permissible in state st+1, and (2) states that even without performing an action at in st, the agent is able to decide whether at is permitted or not in st. The paper proposes to incorporate the proposed SAP property into two state-of-the-art deep RL algorithms to guide their state-actions exploration."
542,SP:20015d8b60e13300586b67c281858cbe28825c48,"We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature. Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs “approximate inference” as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies. Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts. Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization. Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.","This paper studies the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, the analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature. Firstly, it provides a precise answer on how the random deep weight-Tied autoencoder model performs “approximate inference” as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies. Secondly, it shows that deep autoencopers display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts. Thirdly, it obtained insights on pitfalls in training initialization practice, and demonstrated experimentally that it is possible to train a deep auto-encoder, even with the tanh activation and a depth as large as 200 layers, without resorting to layer-wise pre-training or batch normalization."
543,SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks — resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code.","This paper proposes a new adversarial black-box attack method based on the discrete cosine transform (DCT) algorithm. The key idea is to randomly pick a low frequency component of the DCT and either add or subtract it to the target image. The proposed method can be used for both targeted and untargeted attacks, and it can be implemented in less than 20 lines of PyTorch code. The authors demonstrate that the proposed algorithm can produce adversarial ImageNet images with a median of 600 model queries (ResNet-50) and successfully attack Google Cloud Vision with 2500 median queries."
544,SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"Hierarchical Reinforcement Learning is a popular method to exploit temporal abstractions in order to tackle the curse of dimensionality. The options framework is one such hierarchical framework that models the notion of skills or options. However, learning a collection of task-agnostic transferable skills is a challenging task. Option discovery typically entails using heuristics, the majority of which revolve around discovering bottleneck states. In this work, we adopt a method complementary to the idea of discovering bottlenecks. Instead, we attempt to discover “landmark” sub-goals which are prototypical states of well connected regions. These sub-goals are points from which a densely connected set of states are easily accessible. We propose a new model called Successor options that leverages Successor representations to achieve the same. We also design a novel pseudo-reward for learning the intra-option policies. Additionally, we describe an Incremental Successor options model that iteratively builds options and explores in environments where exploration through primitive actions is inadequate to form the Successor representations. Finally, we demonstrate the efficacy of our approach on a collection of grid worlds and on complex high dimensional environments like Deepmind-Lab.","This paper proposes a method for option discovery in hierarchical reinforcement learning. The authors propose a new model called Successor options that leverages Successor representations to achieve the same goal of discovering “landmark” sub-goals which are points from which a densely connected set of states are easily accessible. They also design a novel pseudo-reward for learning the intra-option policies. Finally, they describe an incremental options model that iteratively builds options and explores in environments where exploration through primitive actions is inadequate to form the successor representations. They demonstrate the efficacy of their approach on a collection of grid worlds and on complex high dimensional environments."
545,SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions. This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions. Previous works only calibrate the confident prediction of classifiers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013). In contrast, this paper proposes a probabilistic way of directly estimating and fine-tuning the decision boundary between seen and unseen classes. In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the first time, are introduced to uncover and fine-tune the decision boundary of each domain. Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted confidently. Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks.","This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions. The authors propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. Two statistical tools, namely bootstrapping and KolmogorovSmirnov (K-S) Test, are introduced to uncover and fine-tune the decision boundary of each domain, and the uncertain domain is newly introduced in the framework to adopt those instances whose domain labels cannot be predicted confidently. Extensive experiments demonstrate the state-of-the-art performance on OSL and G-ZSL benchmarks."
546,SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper proposes a neural network for classification and regression, without the need to learn layout structures in the output space. Standard solutions such as softmax cross-entropy and mean squared error are effective but parametric, meaning that known inductive structures such as maximum margin separation and simplicity (Occam’s Razor) need to be learned for the task at hand. Instead, we propose polar prototype networks, a class of networks that explicitly states the structure, i.e., the layout, of the output. The structure is defined by polar prototypes, points on the hypersphere of the output space. For classification, each class is described by a single polar prototype and they are a priori distributed with maximal separation and equal shares on the hypersphere. Classes are assigned to prototypes randomly or based on semantic priors and training becomes a matter of minimizing angular distances between examples and their class prototypes. For regression, we show that training can be performed as a polar interpolation between two prototypes, arriving at a regression with higher-dimensional outputs. From empirical analysis, we find that polar prototype networks benefit from large margin separation and semantic class structure, while only requiring a minimal amount of output dimensions. While the structure is simple, the performance is on par with (classification) or better than (regression) standard network methods. Moreover, we show that we gain the ability to perform regression and classification jointly in the same space, which is disentangled and interpretable by design.","This paper proposes a neural network for classification and regression, without the need to learn layout structures in the output space. The structure is defined by polar prototypes, points on the hypersphere of the output spaces. For classification, each class is described by a single polar prototype and they are a priori distributed with maximal separation and equal shares. For regression, training can be performed as a polar interpolation between two prototypes, arriving at a regression with higher-dimensional outputs. Experiments show that polar prototype networks benefit from large margin separation and semantic class structure, while only requiring a minimal amount of output dimensions."
547,SP:d1034342785d133cf8372b8624897963cc2ee83a,"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.","This paper proposes a method for learning a policy for reinforcement learning (RL) in an environment where the state of the environment is already optimized for what the agent wants to do. The authors argue that this is because the agent has access to a set of preferences for what to do and what not to do in the environment, and that these preferences are already satisfied in our environment. The paper proposes to use this implicit preference information from the state to fill in the blanks. The algorithm is based on Maximum Causal Entropy IRL (MCE) algorithm and is evaluated in a suite of proof-of-concept environments designed to show its properties. "
548,SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"We propose a method for learning the dependency structure between latent variables in deep latent variable models. Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values. We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10. Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.","This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The proposed method combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, the authors express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure. The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective. Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variables values. Experiments on MNIST, Omniglot, and CIFAR-10 show improvements over state-of-the-art structured VAEs."
549,SP:976dedab53e69610692a563382ada1dbb82c1e9d,"A dynamical neural network consists of a set of interconnected neurons that interact over time continuously. It can exhibit computational properties in the sense that the dynamical system’s evolution and/or limit points in the associated state space can correspond to numerical solutions to certain mathematical optimization or learning problems. Such a computational system is particularly attractive in that it can be mapped to a massively parallel computer architecture for power and throughput efficiency, especially if each neuron can rely solely on local information (i.e., local memory). Deriving gradients from the dynamical network’s various states while conforming to this last constraint, however, is challenging. We show that by combining ideas of top-down feedback and contrastive learning, a dynamical network for solving the `1-minimizing dictionary learning problem can be constructed, and the true gradients for learning are provably computable by individual neurons. Using spiking neurons to construct our dynamical network, we present a learning process, its rigorous mathematical analysis, and numerical results on several dictionary learning problems.",This paper proposes a dynamical neural network (DNN) architecture for solving dictionary learning problems. The main idea is to use spiking neurons to construct the dynamical network. The authors show that the gradients for learning are provably computable by individual neurons in the DNN. The paper also provides a theoretical analysis of the dynamics of the network and its gradients. 
550,SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"For semantic image segmentation and lane detection, nets with a single spatial pyramid structure or encoder-decoder structure are usually exploited. Convolutional neural networks (CNNs) show great results on both high-level and low-level features representations, however, the capability has not been fully embodied for lane detection task. In especial, it’s still a challenge for model-based lane detection to combine the multi-scale context with a pixel-level accuracy because of the weak visual appearance and strong prior information. In this paper, we we propose an novel network for lane detection, the three main contributions are as follows. First, we employ multiple encoder-decoders module in end-to-end ways and show the promising results for lane detection. Second, we analysis different configurations of multiple encoder-decoders nets. Third, we make our attempts to rethink the evaluation methods of lane detection for the limitation of the popular methods based on IoU.","This paper proposes a novel network for lane detection. The proposed network consists of multiple encoder-decoders module in end-to-end ways and show the promising results for the lane detection task. In addition, the authors analyze different configurations of the encoder and decoder networks and propose to rethink the evaluation methods of lane detection for the limitation of the popular IoU. "
551,SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"When learning from a batch of logged bandit feedback, the discrepancy between the policy to be learned and the off-policy training data imposes statistical and computational challenges. Unlike classical supervised learning and online learning settings, in batch contextual bandit learning, one only has access to a collection of logged feedback from the actions taken by a historical policy, and expect to learn a policy that takes good actions in possibly unseen contexts. Such a batch learning setting is ubiquitous in online and interactive systems, such as ad platforms and recommendation systems. Existing approaches based on inverse propensity weights, such as Inverse Propensity Scoring (IPS) and Policy Optimizer for Exponential Models (POEM), enjoy unbiasedness but often suffer from large mean squared error. In this work, we introduce a new approach named Maximum Likelihood Inverse Propensity Scoring (MLIPS) for batch learning from logged bandit feedback. Instead of using the given historical policy as the proposal in inverse propensity weights, we estimate a maximum likelihood surrogate policy based on the logged action-context pairs, and then use this surrogate policy as the proposal. We prove that MLIPS is asymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared error than IPS. Such an error reduction phenomenon is somewhat surprising as the estimated surrogate policy is less accurate than the given historical policy. Results on multi-label classification problems and a large-scale ad placement dataset demonstrate the empirical effectiveness of MLIPS. Furthermore, the proposed surrogate policy technique is complementary to existing error reduction techniques, and when combined, is able to consistently boost the performance of several widely used approaches.","This paper proposes a new method for batch contextual bandit learning from logged bandit feedback. The proposed method is based on Maximum Likelihood Inverse Propensity Scoring (MLIPS), which is a variant of inverse propensity scoring (IPS) and Policy Optimizer for Exponential Models (POEM). The main idea is to estimate a maximum likelihood surrogate policy based on the logged action-context pairs, and then use this surrogate policy as the proposal. The authors prove that MLIPS is asymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared error than IPS. Experiments on multi-label classification problems and a large-scale ad placement dataset demonstrate the empirical effectiveness of MLIPS."
552,SP:8e0ed65c5dded23b34798499b2436b24422fd729,"Meta-learning provides a promising learning framework to address few-shot classification tasks. In existing meta-learning methods, the meta-learner is designed to learn about model optimization, parameter initialization, or similarity metric. Differently, in this paper, we propose to learn how to create an individualized feature embedding specific to a given query image for better classifying, i.e., given a query image, a specific feature embedding tailored for its characteristics is created accordingly, leading to an individualized feature space in which the query image can be more accurately classified. Specifically, we introduce a kernel generator as meta-learner to learn to construct feature embedding for query images. The kernel generator acquires meta-knowledge of generating adequate convolutional kernels for different query images during training, which can generalize to unseen categories without fine-tuning. In two standard few-shot classification data sets, i.e. Omniglot, and miniImageNet, our method shows highly competitive performance.","This paper proposes a new meta-learning method for few-shot classification. The proposed method is based on the idea of learning an individualized feature embedding specific to a given query image for better classifying, i.e., given a query image, a kernel generator is used to learn to construct feature embeddings for query images. The kernel generator acquires meta-knowledge of generating adequate convolutional kernels for different query images during training, which can generalize to unseen categories without fine-tuning. Experiments on Omniglot and miniImageNet show that the proposed method can achieve competitive performance."
553,SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in ∼4 hours on one workstation or ∼1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.","This paper proposes a population-based genetic algorithm (GA) for deep reinforcement learning (RL) that can evolve the weights of a DNN with a simple, gradient-free, population based genetic algorithm and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. The results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance."
554,SP:dfdbe3267a8160f24746884cdf5297993e424231,"Rewards are sparse in the real world and most of today’s reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself — thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward — making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory — which incorporates rich information about environment dynamics. This allows us to overcome the known “couch-potato” issues of prior work — when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in VizDoom, DMLab and MuJoCo. In navigational tasks from VizDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only. The code is available at https://github.com/google-research/episodic-curiosity.","This paper proposes a novel curiosity module for reinforcement learning. The novelty bonus is based on the comparison of the current observation with previous observations in memory. This is done based on how many environment steps it takes to reach the current observations from those in memory, which incorporates rich information about environment dynamics. The proposed method is evaluated on VizDoom, DMLab, and MuJoCo. "
555,SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"We present a representation for describing transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct a set of deictic references that determine which objects are relevant in any given state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects’ properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain in which a robot pushes stacks of objects on a cluttered table.","This paper proposes a method for modeling transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct a deictic references that determine which objects are relevant in any given state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects’ properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain."
556,SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"The advent of big data brings with it data with more and more dimensions and thus a growing need to be able to efficiently select which features to use for a variety of problems. While global feature selection has been a well-studied problem for quite some time, only recently has the paradigm of instance-wise feature selection been developed. In this paper, we propose a new instance-wise feature selection method, which we term INVASE. INVASE consists of 3 neural networks, a selector network, a predictor network and a baseline network which are used to train the selector network using the actor-critic methodology. Using this methodology, INVASE is capable of flexibly discovering feature subsets of a different size for each instance, which is a key limitation of existing state-of-the-art methods. We demonstrate through a mixture of synthetic and real data experiments that INVASE significantly outperforms state-of-the-art benchmarks.","This paper proposes a new instance-wise feature selection method, which is based on the actor-critic methodology. The method consists of 3 neural networks, a selector network, a predictor network, and a baseline network, which are used to train the selector network. The selector network is trained using the Actor-Critic methodology, and the predictor network is used to select the feature subsets of a different size for each instance. Experiments on synthetic and real data show that the proposed method outperforms the state-of-the-art methods."
557,SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"Predicting structured outputs such as semantic segmentation relies on expensive per-pixel annotations to learn strong supervised models like convolutional neural networks. However, these models trained on one data domain may not generalize well to other domains unequipped with annotations for model finetuning. To avoid the labor-intensive process of annotation, we develop a domain adaptation method to adapt the source data to the unlabeled target domain. To this end, we propose to learn discriminative feature representations of patches based on label histograms in the source domain, through the construction of a disentangled space. With such representations as guidance, we then use an adversarial learning scheme to push the feature representations in target patches to the closer distributions in source ones. In addition, we show that our framework can integrate a global alignment process with the proposed patch-level alignment and achieve state-of-the-art performance on semantic segmentation. Extensive ablation studies and experiments are conducted on numerous benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios.","This paper proposes a method for semantic segmentation in the unlabeled target domain. The authors propose to learn discriminative feature representations of patches based on label histograms in the source domain, through the construction of a disentangled space. They then use an adversarial learning scheme to push the feature representations in target patches to the closer distributions in source ones. The proposed method is evaluated on several benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios."
558,SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called OPTIMISTIC ONLINE LEARNING, we propose two new optimistic algorithms for AMSGrad and Adam, respectively, by exploiting the predictability of gradients. The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in OPTIMISTIC ONLINE LEARNING, which leads to speed up in training deep neural nets in practice.","This paper proposes two new algorithms for online learning based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called OPTIMISTIC ONLINE LEARNING, the authors propose two new optimistic algorithms for AMSGrad and Adam, respectively, by exploiting the predictability of gradients. The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in OPTIMistic Online Learning to speed up the training of deep neural networks in practice."
559,SP:52228b48f2776d57dd422edb33b82e247f056b75,"In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier’s robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.","This paper proposes a new benchmark for image classifier robustness, which is based on image corruption and perturbation. The paper is well-written and well-motivated, and the paper is easy to follow. The main contribution of this paper is the introduction of a new dataset called IMAGENET-P, which allows researchers to benchmark a classifier’s robustness to common perturbations. The authors also propose a bypassed adversarial defense method to improve the robustness of the classifier."
560,SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"We push on the boundaries of our knowledge about dropout by showing theoretically that dropout training can be understood as performing MAP estimation concurrently for an entire family of conditional models whose objectives are themselves lower bounded by the usual dropout objective. This discovery allows us to pick any model from this family after training, which leads to a substantial improvement on regularisation-heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully stochastic dropout objective. The deterministic subvariant’s bound is equal to its objective, and the highest amongst these models. It also exhibits the best model fit in our experiments. Together, these results suggest that the predominant view of deterministic dropout as a good approximation to MC averaging is misleading. Rather, deterministic dropout is the best available approximation to the true objective.","This paper shows that dropout training can be understood as performing MAP estimation for an entire family of conditional models whose objectives are lower bounded by the usual dropout objective. This discovery allows us to pick any model from this family after training, which leads to a substantial improvement on regularisation-heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds. The deterministic subvariant’s bound is equal to its objective, and the highest amongst these models. It also exhibits the best model fit in the experiments."
561,SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"This paper propose a cumulative saliency based Globally Soft Filter Pruning (GSFP) scheme to prune redundant filters of Convolutional Neural Networks (CNNs). Specifically, the GSFP adopts a robust pruning method, which measures the global redundancy of the filter in the whole model by using the soft pruning strategy. In addition, in the model recovery process after pruning, we use the cumulative saliency strategy to improve the accuracy of pruning. GSFP has two advantages over previous works: (1) More accurate pruning guidance. For a pre-trained CNN model, the saliency of the filter varies with different input data. Therefore, accumulating the saliency of the filter over the entire data set can provide more accurate guidance for pruning. On the other hand, pruning from a global perspective is more accurate than local pruning. (2) More robust pruning strategy. We propose a reasonable normalization formula to prevent certain layers of filters in the network from being completely clipped due to excessive pruning rate. Experiment results show that GSFP is effective on many classic CNN architectures and different data sets. Within my knowledge, GSFP is the first algorithm to combine global and soft pruning strategies. Notably, on MNIST and CIFAR10, it achieves a much higher compression ratio compared with prior work while maintaining the same test accuracy.","This paper proposes a new method to prune redundant filters of CNNs. The proposed method is based on cumulative saliency based soft pruning, which measures the global redundancy of the filter in the whole model by using the soft-pruning strategy. The authors also propose a reasonable normalization formula to prevent certain layers of filters in the network from being completely clipped due to excessive pruning rate. Experiments on MNIST and CIFAR-10 show that the proposed method achieves a much higher compression ratio compared with prior work."
562,SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"Text classification must sometimes be applied in situations with no training data in a target language. However, training data may be available in a related language. We introduce a cross-lingual document classification framework (CACO) between related language pairs. To best use limited training data, our transfer learning scheme exploits cross-lingual subword similarity by jointly training a character-based embedder and a word-based classifier. The embedder derives vector representations for input words from their written forms, and the classifier makes predictions based on the word vectors. We use a joint character representation for both the source language and the target language, which allows the embedder to generalize knowledge about source language words to target language words with similar forms. We propose a multi-task objective that can further improve the model if additional cross-lingual or monolingual resources are available. CACO models trained under low-resource settings rival cross-lingual word embedding models trained under high-resource settings on related language pairs.","This paper proposes a cross-lingual document classification framework (CACO) between related language pairs. CACO uses a joint character representation for both the source language and the target language, which allows the embedder to generalize knowledge about source language words to target language words with similar forms. The embedder derives vector representations for input words from their written forms, and the classifier makes predictions based on the word vectors. The authors propose a multi-task objective that can further improve the model if additional crosslingual or monolingual resources are available."
563,SP:544e421f9c747640d949f433e3091763508b7237,"In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. MAA samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. Theoretically, we prove that the MAA module with learned latent discriminative probabilities successfully reduces the difference in responses between the most salient regions and the others. Therefore, MAAN is able to generate better class activation sequences and identify dense and integral action regions in the videos. Moreover, we propose a fast algorithm to reduce the complexity of constructing MAA from O(2 ) to O(T ). Extensive experiments on two large-scale video datasets show that our MAAN achieves a superior performance on weakly-supervised temporal action localization.","This paper proposes a novel method for weakly supervised temporal action localization. The proposed method is based on the marginalized average attentional network (MAAN), which learns a set of latent discriminative probabilities in an end-to-end fashion to suppress the dominant response of the most salient regions in a principled manner. Theoretically, the authors prove that the MAA module can reduce the difference in responses between the most relevant regions and the others. The authors also propose a fast algorithm to reduce the complexity of constructing MAA from O(2) to O(T). Extensive experiments on two large-scale video datasets show that the proposed method achieves a superior performance on weakly-supervised video localization."
564,SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"The vast majority of neural models in Natural Language Processing adopt a form of structureless distributed representations. While these models are powerful at making predictions, the representational form is rather crude and does not provide insights into linguistic structures. In this paper we introduce novel language models with representations informed by the framework of Holographic Reduced Representation (HRR). This allows us to inject structures directly into our wordlevel and chunk-level representations. Our analyses show that by using HRR as a structured compositional representation, our models are able to discover crude linguistic roles, which roughly resembles a classic division between syntax and semantics.1","This paper proposes a new representation for natural language processing (NLP) models based on the Holographic Reduced Representation (HRR) framework. HRR is a compositional representation of the word-level and chunk-level embeddings of a language model. The authors show that HRR can be used as a structured representation for NLP models, and that it is able to discover linguistic roles, which roughly resembles a classic division between syntax and semantics."
565,SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"Partially observable Markov decision processes (POMDPs) are a widely-used framework to model decision-making with uncertainty about the environment and under stochastic outcome. In conventional POMDP models, the observations that the agent receives originate from fixed known distribution. However, in a variety of real-world scenarios the agent has an active role in its perception by selecting which observations to receive. Due to combinatorial nature of such selection process, it is computationally intractable to integrate the perception decision with the planning decision. To prevent such expansion of the action space, we propose a greedy strategy for observation selection that aims to minimize the uncertainty in state. We develop a novel point-based value iteration algorithm that incorporates the greedy strategy to achieve near-optimal uncertainty reduction for sampled belief points. This in turn enables the solver to efficiently approximate the reachable subspace of belief simplex by essentially separating computations related to perception from planning. Lastly, we implement the proposed solver and demonstrate its performance and computational advantage in a range of robotic scenarios where the robot simultaneously performs active perception and planning.","This paper proposes a method for learning a POMDP model for partially observable Markov Decision Processes (POMDPs) where the agent has an active role in its perception by selecting which observations to receive. The authors argue that it is computationally intractable to integrate the perception decision with the planning decision due to the combinatorial nature of such selection process. To prevent such expansion of the action space, the authors propose a greedy strategy for observation selection that aims to minimize the uncertainty in state. They develop a novel point-based value iteration algorithm that incorporates the greedy strategy to achieve near-optimal uncertainty reduction for sampled belief points. This in turn enables the solver to efficiently approximate the reachable subspace of belief simplex by essentially separating computations related to perception from planning. The proposed solver is evaluated in a range of robotic scenarios where the robot simultaneously performs active perception and planning."
566,SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"Deep neural networks, which gain great success in a wide spectrum of applications, are often time, compute and storage hungry. Curriculum learning proposed to boost training of network by a syllabus from easy to hard. However, the relationship between data complexity and network training is unclear: why hard example harm the performance at beginning but helps at end. In this paper, we aim to investigate on this problem. Similar to internal covariate shift in network forward pass, the distribution changes in weight of top layers also affects training of preceding layers during the backward pass. We call this phenomenon inverse ”internal covariate shift”. Training hard examples aggravates the distribution shifting and damages the training. To address this problem, we introduce a curriculum loss that consists of two parts: a) an adaptive weight that mitigates large early punishment; b) an additional representation loss for low weighted samples. The intuition of the loss is very simple. We train top layers on ”good” samples to reduce large shifting, and encourage ”bad” samples to learn from ”good” sample. In detail, the adaptive weight assigns small values to hard examples, reducing the influence of noisy gradients. On the other hand, the less-weighted hard sample receives the proposed representation loss. Low-weighted data gets nearly no training signal and can stuck in embedding space for a long time. The proposed representation loss aims to encourage their training. This is done by letting them learn a better representation from its superior neighbours but not participate in learning of top layers. In this way, the fluctuation of top layers is reduced and hard samples also received signals for training. We found in this paper that curriculum learning needs random sampling between tasks for better training. Our curriculum loss is easy to combine with existing stochastic algorithms like SGD. Experimental result shows an consistent improvement over several benchmark datasets.","This paper proposes a curriculum loss for training deep neural networks. The main idea is to train top layers on “good” samples to reduce the distribution shifting, and encourage “bad” examples to learn from ‘good’ samples. The authors also propose an additional representation loss for low-weighted samples to encourage their training. The proposed curriculum loss is easy to combine with existing stochastic algorithms like SGD. Experimental results on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
567,SP:8b555b9f24044bc68c204169d6a37e262361d706,"The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.","This paper presents a method for learning heuristics for combinatorial optimization problems. The authors propose a model based on attention layers with benefits over the Pointer Network and train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which they find is more efficient than using a value function. With the same hyperparameters, they learn strong heuristic for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized algorithms."
568,SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources. However, existing quantization methods often represent all weights and activations with the same precision (bit-width). In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths. We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization. Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet. Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models.",This paper proposes a method for quantizing different layers of neural networks with different bit-widths. The authors formulate the problem as a neural architecture search problem and propose a novel differentiable neural Architecture Search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization. Experiments on CIFAR-10 and ImageNet show that the proposed quantized models can achieve a 21.1x smaller model size or 103.9x lower computational cost compared to baseline quantized or full precision models.
569,SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence. We present an alternative architecture called Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes. First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.","This paper proposes a new architecture for neural networks based on posterior attention. The authors argue that the current attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence. To this end, the authors propose to change the position where attention is marginalized is changed from the input to the output, and the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirical results on 5 translation and 2 morphological inflection tasks show that the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models."
570,SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"The recent direction of unpaired image-to-image translation is on one hand very exciting as it alleviates the big burden in obtaining label-intensive pixel-to-pixel supervision, but it is on the other hand not fully satisfactory due to the presence of artifacts and degenerated transformations. In this paper, we take a manifold view of the problem by introducing a smoothness term over the sample graph to attain harmonic functions to enforce consistent mappings during the translation. We develop HarmonicGAN to learn bi-directional translations between the source and the target domains. With the help of similarity-consistency, the inherent selfconsistency property of samples can be maintained. Distance metrics defined on two types of features including histogram and CNN are exploited. Under an identical problem setting as CycleGAN, without additional manual inputs and only at a small training-time cost, HarmonicGAN demonstrates a significant qualitative and quantitative improvement over the state of the art, as well as improved interpretability. We show experimental results in a number of applications including medical imaging, object transfiguration, and semantic labeling. We outperform the competing methods in all tasks, and for a medical imaging task in particular our method turns CycleGAN from a failure to a success, halving the mean-squared error, and generating images that radiologists prefer over competing methods in 95% of cases.","This paper proposes a method for unpaired image-to-image translation. The authors propose to learn bi-directional translations between the source and the target domains by introducing a smoothness term over the sample graph to enforce consistent mappings during the translation process. The proposed method is evaluated on a variety of tasks including medical imaging, object transfiguration, and semantic labeling."
571,SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (h-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them. Our algorithm1 prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.","This paper proposes a simple stochastic algorithm (h-detach) that is specific to LSTM optimization and targeted towards addressing the exploding and vanishing gradient problem (EVGP). Specifically, the authors show that when the weights of LSTMs are large, the gradient components through the linear path (cell state) in the computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which they show empirically), their suppression can prevent LSTm from capturing them. The proposed algorithm prevents gradients flowing through this path from getting suppressed, thus allowing the LST mn to capture such dependencies better. The authors show significant improvements over vanilla L STM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization."
572,SP:9aaff3777321347d1194884af5690b0b5185eff9,"In this paper, we study the problem of training real binary weight networks (without layer-wise or filter-wise scaling factors) from scratch under the Bayesian deep learning perspective, meaning that the final objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The proposed method, named as SnapQuant, has two intriguing features: (1) The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, we generate binary weights on-the-fly since what we actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, we can sample binary weight instances for a given recognition architecture from the learnt policy network. (2) The policy network, which has a nested parameter structure consisting of layer-wise, filter-wise and kernel-wise parameter sharing designs, is applicable to any neural network architecture. Such a nested parameterization explicitly and hierarchically models the joint posterior distribution of binary weights. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet. The code will be made publicly available.","This paper proposes SnapQuant, a method for training binary weight neural networks without layer-wise or filter-wise scaling factors. The method is based on Bayesian deep learning, where the objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. During the training phase, the authors generate binary weights on the fly since what they actually maintain is the policy network, and all the binary weights are used in a burn-after-reading style. At the testing phase, they can sample binary weight instances for a given recognition architecture from the learnt policy network."
573,SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to train local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.","This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to train local neural network weights, which are modeled through the framework. An inference approach is then developed to synthesize a more expressive global network without additional supervision or data pooling. The experimental results demonstrate the efficacy of the proposed method on two popular image classification datasets."
574,SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"A growing number of learning methods are actually differentiable games whose players optimise multiple, interdependent objectives in parallel – from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in these games, accounting for player influence on others’ updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm that exploits this response and leads to cooperation in settings like the Iterated Prisoner’s Dilemma. Although experimentally successful, we show that LOLA agents can exhibit ‘arrogant’ behaviour directly at odds with convergence. In fact, remarkably few algorithms have theoretical guarantees applying across all (n-player, non-convex) games. In this paper we present Stable Opponent Shaping (SOS), a new method that interpolates between LOLA and a stable variant named LookAhead. We prove that LookAhead converges locally to equilibria and avoids strict saddles in all differentiable games. SOS inherits these essential guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally.","This paper proposes a new algorithm for learning in differentiable games. The main idea is to interpolate between LOLA and a stable variant of LookAhead, which is a variant of LOLA. Theoretical analysis is provided to show that the proposed SOS algorithm converges locally to equilibria and avoids strict saddles. Experiments show that SOS outperforms LOLA on the Iterated Prisoner’s Dilemma."
575,SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"It is usually hard for a learning system to predict correctly on the rare events, and there is no exception for segmentation algorithms. Therefore, we hope to build an alarm system to set off alarms when the segmentation result is possibly unsatisfactory. One plausible solution is to project the segmentation results into a low dimensional feature space, and then learn classifiers/regressors in the feature space to predict the qualities of segmentation results. In this paper, we form the feature space using shape feature which is a strong prior information shared among different data, so it is capable to predict the qualities of segmentation results given different segmentation algorithms on different datasets. The shape feature of a segmentation result is captured using the value of loss function when the segmentation result is tested using a Variational Auto-Encoder(VAE). The VAE is trained using only the ground truth masks, therefore the bad segmentation results with bad shapes become the rare events for VAE and will result in large loss value. By utilizing this fact, the VAE is able to detect all kinds of shapes that are out of the distribution of normal shapes in ground truth (GT). Finally, we learn the representation in the one-dimensional feature space to predict the qualities of segmentation results. We evaluate our alarm system on several recent segmentation algorithms for the medical segmentation task. The segmentation algorithms perform differently on different datasets, but our system consistently provides reliable prediction on the qualities of segmentation results.","This paper proposes an alarm system to set off alarms when the segmentation result is possibly unsatisfactory. The proposed method is based on the shape feature which is a strong prior information shared among different data, so it is capable to predict the qualities of segmentation results given different segmentation algorithms on different datasets. The VAE is able to detect all kinds of shapes that are out of the distribution of normal shapes in ground truth (GT). Finally, the representation in the one-dimensional feature space is learned by learning the representations in the low dimensional feature space. The results are evaluated on the medical segmentation task, and the system consistently provides reliable prediction."
576,SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, imagegenerating deep neural networks have a large number of parameters—typically a multiple of their output dimension—and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.","This paper proposes a simple network architecture for image denoising. The proposed method is based on a simple architecture with no convolutions and fewer weight parameters than the output dimensionality of the network. The authors show that the proposed network is on par with wavelet-based thresholding in terms of the quality of the denoised image. The network is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis."
577,SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"Program synthesis from natural language (NL) is practical for humans and, once technically feasible, would significantly facilitate software development and revolutionize end-user programming. We present SAPS, an end-to-end neural network capable of mapping relatively complex, multi-sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. The decoder features a doubly-recurrent LSTM, for which we propose novel signal propagation schemes and soft attention mechanism. When applied to a large dataset of problems proposed in a previous study, SAPS performs on par with or better than the method proposed there, producing correct programs in over 92% of cases. In contrast to other methods, it does not require post-processing of the resulting programs, and uses a fixed-dimensional latent representation as the only interface between the NL analyzer and the source code generator.","This paper presents an end-to-end neural network architecture for program synthesis from natural language (NL) specifications. The authors propose a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. The decoder features a doubly-recurrent LSTMs, for which the authors propose novel signal propagation schemes and soft attention mechanism. The experimental results show that the proposed method performs on par with or better than the method proposed in the previous work."
578,SP:d2ec231bb6153a303e5110e671dea14c2721e636,"Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L∞ defense by Madry et al. (1) has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L∞ perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.","This paper studies the problem of adversarial robustness of deep neural networks against adversarial attacks on MNIST. The authors propose a novel class-conditional adversarial defense model that performs analysis by synthesis using learned class-conditioned data distributions. They derive bounds on the robustness and go to great length to empirically evaluate their model using maximally effective attacks by (a) applying decision-based, score based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness against L0, L2 and L∞ perturbations."
579,SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"Generative Adversarial Networks (GANs), though powerful, is hard to train. Several recent works (Brock et al., 2016; Miyato et al., 2018) suggest that controlling the spectra of weight matrices in the discriminator can significantly improve the training of GANs. Motivated by their discovery, we propose a new framework for training GANs, which allows more flexible spectrum control (e.g., making the weight matrices of the discriminator have slow singular value decays). Specifically, we propose a new reparameterization approach for the weight matrices of the discriminator in GANs, which allows us to directly manipulate the spectra of the weight matrices through various regularizers and constraints, without intensively computing singular value decompositions. Theoretically, we further show that the spectrum control improves the generalization ability of GANs. Our experiments on CIFAR-10, STL-10, and ImgaeNet datasets confirm that compared to other methods, our proposed method is capable of generating images with competitive quality by utilizing spectral normalization and encouraging the slow singular value decay.","This paper proposes a reparameterization approach for the weight matrices of the discriminator in GANs, which allows us to directly manipulate the spectra of the weights matrices through various regularizers and constraints, without intensively computing singular value decompositions. Theoretically, the authors show that the spectrum control improves the generalization ability of GAN. Experiments on CIFAR-10, STL-10 and ImgaeNet datasets confirm that the proposed method is capable of generating images with competitive quality by utilizing spectral normalization and encouraging the slow singular value decay."
580,SP:8115fd9b681198d62100c36794926fb57dc0a4f5,"Acceleration for reinforcement learning methods is an important and challenging theme. We introduce the Anderson acceleration technique into the value iteration, developing an accelerated value iteration algorithm that we call Anderson Accelerated Value Iteration (A2VI). We further apply our method to the Deep Q-learning algorithm, resulting in the Deep Anderson Accelerated Q-learning (DA2Q) algorithm. Our approach can be viewed as an approximation of the policy evaluation by interpolating on historical data. A2VI is more efficient than the modified policy iteration, which is a classical approximate method for policy evaluation. We give theoretical analysis of our algorithm and conduct experiments on both toy problems and Atari games. Both the theoretical and empirical results show the effectiveness of our algorithm.","This paper proposes an accelerated value iteration method for reinforcement learning. The authors propose to accelerate the value iteration by using the Anderson acceleration technique, which is an approximation of the policy evaluation by interpolating on historical data. The paper also proposes a deep Q-learning algorithm based on the proposed method. The theoretical analysis of the proposed algorithm is provided, and experiments are conducted on both toy problems and Atari games."
581,SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,"A plain well-trained deep learning model often does not have the ability to learn new knowledge without forgetting the previously learned knowledge, which is known as catastrophic forgetting. Here we propose a novel method, SupportNet, to efficiently and effectively solve the catastrophic forgetting problem in the class incremental learning scenario. SupportNet combines the strength of deep learning and support vector machine (SVM), where SVM is used to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training so that the model can review the essential information of the old data when learning the new information. Two powerful consolidation regularizers are applied to stabilize the learned representation and ensure the robustness of the learned model. We validate our method with comprehensive experiments on various tasks, which show that SupportNet drastically outperforms the state-of-the-art incremental learning methods and even reaches similar performance as the deep learning model trained from scratch on both old and new data.","This paper proposes a novel method to solve the catastrophic forgetting problem in the class incremental learning scenario. The proposed method combines the strength of deep learning and support vector machine (SVM) to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training. Two powerful consolidation regularizers are applied to stabilize the learned representation and ensure the robustness of the learned model. The experimental results show that the proposed method significantly outperforms the state-of-the-art incremental learning methods."
582,SP:d228d213f79716774043cea253305fecece659ec,"Various methods of measuring unit selectivity have been developed in order to understand the representations learned by neural networks (NNs). Here we undertake a comparison of four such measures on AlexNet, namely, localist selectivity Bowers et al. (2014), precision (Zhou et al., 2015), class-conditional mean activity selectivity CCMAS; Morcos et al. (2018), and a new measure called top-class selectivity. In contrast with previous work on recurrent neural networks (RNNs), we fail to find any 100% selective ‘localist units’ in AlexNet, and demonstrate that the precision and CCMAS measures provide a much higher level of selectivity than is warranted, with the most selective hidden units only responding strongly to a small minority of images from within a category. We also generated activation maximization (AM) images that maximally activated individual units and found that under (5%) of units in fc6 and conv5 produced interpretable images of objects, whereas fc8 produced over 50% interpretable images. Furthermore, the interpretable images in the hidden layers were not associated with highly selective units. These findings highlight the problem with current selectivity measures and show that new measures are required in order to provide a better assessment of learned representations in NNs. We also consider why localist representations are learned in RNNs and not AlexNet.","This paper presents an empirical study of unit selectivity in AlexNet. The authors compare the performance of different measures of unit selection on the network, including localist, precision, CCMAS, class-conditional mean activity selectivity, and a new measure called top-class selectivity. They find that the precision and CCMAS measures provide a much higher level of selectivity than is warranted, with the most selective hidden units only responding strongly to a small minority of images from within a category. They also generate activation maximization (AM) images that maximally activated individual units and found that under (5%) of units in fc6 and conv5 produced interpretable images of objects, whereas fc8 produced over 50% interpretable image."
583,SP:b9deae0392e0160b400d76c549d382e235196f8c,"Community detection in graphs can be solved via spectral methods or posterior inference under certain probabilistic graphical models. Focusing on random graph families such as the stochastic block model, recent research has unified both approaches and identified both statistical and computational detection thresholds in terms of the signal-to-noise ratio. By recasting community detection as a node-wise classification problem on graphs, we can also study it from a learning perspective. We present a novel family of Graph Neural Networks (GNNs) for solving community detection problems in a supervised learning setting. We show that, in a data-driven manner and without access to the underlying generative models, they can match or even surpass the performance of the belief propagation algorithm on binary and multiclass stochastic block models, which is believed to reach the computational threshold in these cases. In particular, we propose to augment GNNs with the non-backtracking operator defined on the line graph of edge adjacencies. The GNNs are achieved good performance on real-world datasets. In addition, we perform the first analysis of the optimization landscape of using (linear) GNNs to solve community detection problems, demonstrating that under certain simplifications and assumptions, the loss value at any local minimum is close to the loss value at the global minimum/minima.","This paper proposes Graph Neural Networks (GNNs) for solving community detection problems in a supervised learning setting. In particular, the authors propose to augment GNNs with the non-backtracking operator defined on the line graph of edge adjacencies. They show that, in a data-driven manner and without access to the underlying generative models, they can match or even surpass the performance of the belief propagation algorithm on binary and multiclass stochastic block models, which is believed to reach the computational threshold in these cases. In addition, they show that under certain simplifications and assumptions, the loss value at any local minimum is close to that of the global minimum."
584,SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficients. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, we develop NOODL: a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. Our algorithm, NOODL, is also scalable and amenable for large scale distributed implementations in neural architectures, by which we mean that it only involves simple linear and non-linear operations. Finally, we corroborate these theoretical results via experimental evaluation of the proposed algorithm with the current state-of-the-art techniques.","This paper studies the problem of online dictionary learning, where the model is modeled as a linear combination of a few columns of a matrix known as a dictionary, and the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This paper proposes a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm (NOODL) which recovers both dictionary and coefficient exactly at a geometric rate, when initialized appropriately. The proposed algorithm is scalable and amenable for large scale distributed implementations in neural architectures, by which they mean that it only involves linear and non-linear operations."
585,SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"We present a powerful new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. Our loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. Our novel training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. To fully leverage the resulting hashes, we use multi-indexing. We demonstrate that these techniques provide large improvements to a similarity search tasks. We report the best results to date on competitive information retrieval tasks for ImageNet and SIFT 1M, improving MAP from 73% to 85% and reducing query cost by a factor of 2-8, respectively.","This paper proposes a new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function. The loss function uses log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target. The training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. To fully leverage the resulting hash codes, the authors use multi-indexing. The authors demonstrate that these techniques provide large improvements to a similarity search tasks."
586,SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,"Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast – they can search nearly 10× faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs.","This paper proposes Graph HyperNetwork (GHN) to automatically find the best task-specific neural network topology for neural architecture search (NAS) automatically. The authors propose to use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHN is fast – they can search nearly 10x faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs."
587,SP:65ccf43cd4e033d22239069057f5200d49f33724,"Imitation learning aims to learn an optimal policy from expert demonstrations and its recent combination with deep learning has shown impressive performance. However, collecting a large number of expert demonstrations for deep learning is time-consuming and requires much expert effort. In this paper, we propose a method to improve generative adversarial imitation learning by using additional information from non-expert demonstrations which are easier to obtain. The key idea of our method is to perform multiclass classification to learn discriminator functions where non-expert demonstrations are regarded as being drawn from an extra class. Experiments in continuous control tasks demonstrate that our method learns better policies than the generative adversarial imitation learning baseline when the number of expert demonstrations is small.",This paper proposes a method to improve imitation learning by using additional information from non-expert demonstrations which are easier to obtain. The key idea of the proposed method is to perform multiclass classification to learn discriminator functions where non-experts demonstrations are regarded as being drawn from an extra class. Experiments on continuous control tasks demonstrate that the method learns better policies than the generative adversarial imitation learning baseline when the number of expert demonstrations is small.
588,SP:e8427949a98effbd37ce7604fa11f240e2342196,"For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameterto measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task – so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.","This paper proposes a new class of neural networks, called Invertible Neural Networks (INNs), which aims to solve the inverse problem in the sense that the forward process from measurement space is well-defined, whereas the inverse process is ambiguous: multiple parameter sets can result in the same measurement. The authors argue that INNs are well-suited for this task because they focus on learning the forward processes, using additional latent output variables to capture the information otherwise lost. They prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INN are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters."
589,SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"Despite the huge success of deep neural networks (NNs), finding good mechanisms for quantifying their prediction uncertainty is still an open problem. It was recently shown, that using an ensemble of NNs trained with a proper scoring rule leads to results competitive to those of Bayesian NNs. This ensemble method can be understood as finite mixture model with uniform mixing weights. We build on this mixture model approach and increase its flexibility by replacing the fixed mixing weights by an adaptive, input-dependent distribution (specifying the probability of each component) represented by an NN, and by considering uncountably many mixture components. The resulting model can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks. We empirically show that the proposed model results in better uncertainty estimates and is more robust to adversarial examples than previous approaches.","This paper proposes a method for quantifying the uncertainty of deep neural networks (NNs) by using an adaptive, input-dependent distribution (specifying the probability of each component) represented by an NN and considering uncountably many mixture components. The proposed model can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks. The authors empirically show that the proposed model results in better uncertainty estimates and is more robust to adversarial examples than previous approaches."
590,SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in this paper, relaxing weight determinism and using a full variational distribution over weights allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, we encode the network weights using a random sample, requiring only a number of bits corresponding to the KullbackLeibler divergence between the sampled variational distribution and the encoding distribution. By imposing a constraint on the Kullback-Leibler divergence, we are able to explicitly control the compression rate, while optimizing the expected loss on the training set. The employed encoding scheme can be shown to be close to the optimal information-theoretical lower bound, with respect to the employed variational family. Our method sets new state-of-the-art in neural network compression, as it strictly dominates previous approaches in a Pareto sense: On the benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the best test performance for a fixed memory budget, and vice versa, it achieves the highest compression rates for a fixed test performance.","This paper proposes a new compression method for neural networks. The main idea is to relax weight determinism and use a full variational distribution over weights, which allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, the authors encode the network weights using a random sample, requiring only a number of bits corresponding to the Kullback-Leibler divergence between the sampled Variational distribution and the encoding distribution. The proposed encoding scheme can be shown to be close to the optimal information-theoretical lower bound, with respect to the employed variational family. On the benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, the proposed method achieves the best test performance for a fixed memory budget."
591,SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.1 1 INTRODUCTION Neural architecture search (NAS) has demonstrated much success in automating neural network architecture design for various deep learning tasks, such as image recognition (Zoph et al., 2018; Cai et al., 2018a; Liu et al., 2018a; Zhong et al., 2018) and language modeling (Zoph & Le, 2017). Despite the remarkable results, conventional NAS algorithms are prohibitively computation-intensive, requiring to train thousands of models on the target task in a single experiment. Therefore, directly applying NAS to a large-scale task (e.g. ImageNet) is computationally expensive or impossible, which makes it difficult for making practical industry impact. As a trade-off, Zoph et al. (2018) propose","This paper proposes ProxylessNAS, an architecture search algorithm that directly learns the architectures for large-scale target tasks and target hardware platforms. The main contribution of this paper is to address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. The paper also proposes to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization."
592,SP:e5b70d43d301d1980fae02623ea711976b429c14,"Many notions of fairness may be expressed as linear constraints, and the resulting constrained objective is often optimized by transforming the problem into its Lagrangian dual with additive linear penalties. In non-convex settings, the resulting problem may be difficult to solve as the Lagrangian is not guaranteed to have a deterministic saddle-point equilibrium. In this paper, we propose to modify the linear penalties to second-order ones, and we argue that this results in a more practical training procedure in non-convex, large-data settings. For one, the use of secondorder penalties allows training the penalized objective with a fixed value of the penalty coefficient, thus avoiding the instability and potential lack of convergence associated with two-player min-max games. Secondly, we derive a method for efficiently computing the gradients associated with the second-order penalties in stochastic mini-batch settings. Our resulting algorithm performs well empirically, learning an appropriately fair classifier on a number of standard benchmarks.","This paper proposes to use second-order penalties for penalizing fairness in non-convex optimization problems. In particular, the authors argue that this is a more practical training procedure in large-data settings, as it avoids the instability and potential lack of convergence associated with two-player min-max games. The authors also derive a method for efficiently computing the gradients associated with the second order penalties in stochastic mini-batch settings. The proposed algorithm is evaluated on a number of standard benchmarks, and the proposed algorithm performs well."
593,SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in highvariance gradient estimators for two primary reasons: 1. branching on the samples within the model, and 2. the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the reweighted wake-sleep (RWS) (Bornschein & Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the importance weighted autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latentvariable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.","This paper proposes a reweighted wake-sleep (RWS) algorithm for learning discrete latent variable models. RWS is based on the reweighting of the sleep-sleep algorithm (Bornschein & Bengio, 2015). The authors show that RWS outperforms the state-of-the-art methods in learning discrete models. The authors also show that the RWS algorithm can learn better models and inference networks with increasing number of particles, and that its benefits extend to continuous latent variables as well."
594,SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"In structured output prediction tasks, labeling ground-truth training output is often expensive. However, for many tasks, even when the true output is unknown, we can evaluate predictions using a scalar reward function, which may be easily assembled from human knowledge or non-differentiable pipelines. But searching through the entire output space to find the best output with respect to this reward function is typically intractable. In this paper, we instead use efficient truncated randomized search in this reward function to train structured prediction energy networks (SPENs), which provide efficient test-time inference using gradient-based search on a smooth, learned representation of the score landscape, and have previously yielded state-of-the-art results in structured prediction. In particular, this truncated randomized search in the reward function yields previously unknown local improvements, providing effective supervision to SPENs, avoiding their traditional need for labeled training data.","This paper proposes a method to train structured prediction energy networks (SPENs) for structured output prediction tasks, where the true output is unknown and the goal is to evaluate predictions using a scalar reward function, which may be easily assembled from human knowledge or non-differentiable pipelines. Instead of searching through the entire output space to find the best output with respect to this reward function is typically intractable. In this paper, the authors propose to use efficient truncated randomized search in the reward function to train SPENs, which provide efficient test-time inference using gradient-based search on a smooth, learned representation of the score landscape, and have previously yielded state-of-the-art results in structured prediction. In particular, this truncated Randomized Search (RNS) algorithm yields previously unknown local improvements, which provides effective supervision to SPEN."
595,SP:638c1bc09992029b78bd83f0127594dcccb96c06,"Robust Policy Search is the problem of learning policies that do not degrade in performance when subject to unseen environment model parameters. It is particularly relevant for transferring policies learned in a simulation environment to the real world. Several existing approaches involve sampling large batches of trajectories which reflect the differences in various possible environments, and then selecting some subset of these to learn robust policies, such as the ones that result in the worst performance. We propose an active learning based framework, EffAcTS, to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select such a subset. We apply this framework to an existing method, namely EPOpt, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous control tasks. We also present a Multi-Task Learning perspective to the problem of Robust Policy Search, and draw connections from our proposed framework to existing work on Multi-Task Learning.","This paper proposes an active learning based framework for robust policy search. The proposed method is based on EPOpt, which is a method for learning robust policies that do not degrade in performance when subject to unseen environment model parameters. The authors propose a method to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select such a subset. They also present a Multi-Task Learning perspective to the problem of Robust Policy Search, and draw connections from their proposed framework to existing work on multi-task learning. They apply this framework to an existing method, namely EPOpt and experimentally validate the gains in sample efficiency and the performance of their approach on standard continuous control tasks."
596,SP:491c239713a6489f0b1790ca26db54a1813c67ae,"A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control. Many of the algorithms for learning values, however, are designed for linear function approximation—with a fixed basis or fixed representation. Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning. In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates. We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation. We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.","This paper proposes a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. The authors prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation. They empirically demonstrate the benefits of TTNs compared to other nonlinear value function approximation algorithms, both for policy evaluation and control."
597,SP:327d606cf3813b00a009a7785e08ef9e11f89493,"Building deep reinforcement learning agents that can generalize and adapt to unseen environments remains a fundamental challenge for AI. This paper describes progresses on this challenge in the context of man-made environments, which are visually diverse but contain intrinsic semantic regularities. We propose a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target for the sub-policy to execute, and updates the semantic model based on new observations. We perform experiments in visual navigation tasks using House3D, a 3D environment that contains diverse human-designed indoor scenes with real-world objects. LEAPS outperforms strong baselines that do not explicitly plan using the semantic content.","This paper proposes a model-based and model-free approach to planning and planning with semantic regularities in the context of man-made environments. The approach consists of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. The agent plans with the semantic model to make high-level decisions, proposes the next sub-target for the sub-Policy to execute, and updates the model based on new observations. The proposed method is evaluated in visual navigation tasks using House3D, a 3D environment that contains diverse human-designed indoor scenes with real-world objects."
598,SP:d7c26f43bc68d160095b1f50447528843d79edbd,"Current end-to-end deep learning driving models have two problems: (1) Poor generalization ability of unobserved driving environment when diversity of training driving dataset is limited (2) Lack of accident explanation ability when driving models don’t work as expected. To tackle these two problems, rooted on the believe that knowledge of associated easy task is benificial for addressing difficult task, we proposed a new driving model which is composed of perception module for see and think and driving module for behave, and trained it with multi-task perception-related basic knowledge and driving knowledge stepwisely. Specifically segmentation map and depth map (pixel level understanding of images) were considered as what & where and how far knowledge for tackling easier drivingrelated perception problems before generating final control commands for difficult driving task. The results of experiments demonstrated the effectiveness of multitask perception knowledge for better generalization and accident explanation ability. With our method the average sucess rate of finishing most difficult navigation tasks in untrained city of CoRL test surpassed current benchmark method for 15 percent in trained weather and 20 percent in untrained weathers.","This paper proposes a multi-task perception-related basic knowledge and driving knowledge stepwisely. Specifically, segmentation map and depth map (pixel level understanding of images) are considered as what & where and how far knowledge for tackling easier drivingrelated perception problems before generating final control commands for difficult driving task. The results of experiments demonstrated the effectiveness of multitask perception knowledge for better generalization and accident explanation ability."
599,SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.","This paper studies the trade-off between adversarial robustness and generalization in deep neural networks. The authors show that training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. They demonstrate that this tradeoff between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. Further, they argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers."
600,SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation). Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way. In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation a method for gradient-based training of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient. Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-point using a local learning rule. After training, we can simply use this initializing network for inference, resulting in a learned feedforward network. Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation while requiring fewer steps to converge. This shows how we might go about training deep networks without using backpropagation.","This paper proposes a new method for gradient-based training of neural networks that uses only local learning rules and does not rely on neurons having a mechanism for back-propagating an error gradient. The authors propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-point using a local learning rule. The experiments show that this network appears to work as well or better than the original version of Equilibrium propagate while requiring fewer steps to converge. This shows how we might go about training deep networks without using backpropagation."
601,SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"In this paper, we design and analyze a new zeroth-order (ZO) stochastic optimization algorithm, ZO-signSGD, which enjoys dual advantages of gradient-free operations and signSGD. The latter requires only the sign information of gradient estimates but is able to achieve a comparable or even better convergence speed than SGD-type algorithms. Our study shows that ZO-signSGD requires √ d times more iterations than signSGD, leading to a convergence rate of O( √ d/ √ T ) under some mild conditions, where d is the number of optimization variables, and T is the number of iterations. In addition, we analyze the effects of different types of gradient estimators on the convergence of ZO-signSGD, and propose several variants of ZO-signSGD with O( √ d/ √ T ) convergence rate. On the application side we explore the connection between ZO-signSGD and black-box adversarial attacks in robust deep learning. Our empirical evaluations on image classification datasets MNIST and CIFAR-10 demonstrate the superior performance of ZO-signSGD on the generation of adversarial examples from black-box neural networks.","This paper proposes a new zeroth-order stochastic optimization algorithm, ZO-signSGD, which enjoys the dual advantages of gradient-free operations and signSGD. The authors analyze the convergence rate of the proposed algorithm and propose several variants of the algorithm. In the application side, the authors explore the connection between the proposed method and black-box adversarial attacks in robust deep learning. The empirical evaluations on image classification datasets MNIST and CIFAR-10 demonstrate the superior performance of ZOSignSGD on the generation of adversarial examples from black- box neural networks."
602,SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"Deep learning has been attracting enormous attention from academia as well as industry due to its great success in many artificial intelligence applications. As more applications are developed, the need for implementing a complex neural network model on an energy-limited edge device becomes more critical. To this end, this paper proposes a new optimization method to reduce the computation efforts of convolutional neural networks. The method takes advantage of the fact that some convolutional operations are actually wasteful since their outputs are pruned by the following activation or pooling layers. Basically, a convolutional filter conducts a series of multiply-accumulate (MAC) operations. We propose to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. Furthermore, a fine-tuning process is conducted to recover the accuracy drop due to the applied checkpoints. The experimental results show that the proposed method can save approximately 50% MAC operations with less than 1% accuracy drop for CIFAR-10 example model and Network in Network on the CIFAR-10 and CIFAR-100 datasets. Additionally, compared with the state-ofthe-art method, the proposed method is more effective on the CIFAR-10 dataset and is competitive on the CIFAR-100 dataset.","This paper proposes a method to reduce the computation efforts of convolutional neural networks. The authors propose to set a checkpoint in the multiply-accumulate (MAC) operations to determine whether a filter could terminate early based on the intermediate result. Furthermore, a fine-tuning process is conducted to recover the accuracy drop due to the applied checkpoints. The experimental results show that the proposed method can save approximately 50% MAC operations with less than 1% accuracy drop for CIFAR-10 example model and Network in Network on the Cifar-10 and CifAR-100 datasets."
603,SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"Recent studies have highlighted adversarial examples as a ubiquitous threat to different neural network models and many downstream applications. Nonetheless, as unique data properties have inspired distinct and powerful learning principles, this paper aims to explore their potentials towards mitigating adversarial inputs. In particular, our results reveal the importance of using the temporal dependency in audio data to gain discriminate power against adversarial examples. Tested on the automatic speech recognition (ASR) tasks and three recent audio adversarial attacks, we find that (i) input transformation developed from image adversarial defense provides limited robustness improvement and is subtle to advanced attacks; (ii) temporal dependency can be exploited to gain discriminative power against audio adversarial examples and is resistant to adaptive attacks considered in our experiments. Our results not only show promising means of improving the robustness of ASR systems but also offer novel insights in exploiting domain-specific data properties to mitigate negative effects of adversarial examples.","This paper investigates the use of temporal dependency in audio data to improve the robustness of speech recognition systems against adversarial attacks. The authors propose to exploit the temporal dependency of audio data in order to increase the discriminative power of the adversarial attack. In particular, they propose to use the input transformation from image adversarial defense to improve robustness, and to use temporal dependency to improve adversarial robustness against audio adversarial examples. They show that temporal dependency can be exploited to gain discriminate power against audio attacks and is resistant to adaptive attacks. "
604,SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"Deep generative models seek to recover the process with which the observed data was generated. They may be used to synthesize new samples or to subsequently extract representations. Successful approaches in the domain of images are driven by several core inductive biases. However, a bias to account for the compositional way in which humans structure a visual scene in terms of objects has frequently been overlooked. In this work we propose to structure the generator of a GAN to consider objects and their relations explicitly, and generate images by means of composition. This provides a way to efficiently learn a more accurate generative model of real-world images, and serves as an initial step towards learning corresponding object representations. We evaluate our approach on several multi-object image datasets, and find that the generator learns to identify and disentangle information corresponding to different objects at a representational level. A human study reveals that the resulting generative model is better at generating images that are more faithful to the reference distribution.","This paper proposes a generative model that learns to generate images by means of compositional composition. The key idea is that the generator of a GAN should consider objects and their relations explicitly, and generate images that are compositional in nature. The paper proposes to learn the relation between the generated image and the original image, and to learn to disentangle information corresponding to different objects at a representational level. Experiments are conducted on several multi-object image datasets, and show that the proposed method is able to generate more accurate images than the state-of-the-art."
605,SP:fb59990b8da0e95d8202383478a456667de60449,"Learning disentangled representations from visual data, where different high-level generative factors are independently encoded, is of importance for many computer vision tasks. Supervised approaches, however, require a significant annotation effort in order to label the factors of interest in a training set. To alleviate the annotation cost, we introduce a learning setting which we refer to as “referencebased disentangling”. Given a pool of unlabelled images, the goal is to learn a representation where a set of target factors are disentangled from others. The only supervision comes from an auxiliary “reference set” that contains images where the factors of interest are constant. In order to address this problem, we propose reference-based variational autoencoders, a novel deep generative model designed to exploit the weak supervisory signal provided by the reference set. During training, we use the variational inference framework where adversarial learning is used to minimize the objective function. By addressing tasks such as feature learning, conditional image generation or attribute transfer, we validate the ability of the proposed model to learn disentangled representations from minimal supervision.","This paper proposes a method for learning disentangled representations from unlabelled images, where the only supervision comes from an auxiliary “reference set” that contains images where the factors of interest are constant. The authors propose a variational variational autoencoder model to exploit the weak supervisory signal provided by the reference set. During training, they use the variational inference framework where adversarial learning is used to minimize the objective function. Experiments are conducted on feature learning, conditional image generation, and attribute transfer tasks."
606,SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances. Videos available at: https://sites.google.com/berkeley.edu/onlineviameta","This paper proposes a method for continual online learning from an incoming stream of data, using deep neural network models. The authors formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, the authors observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, the proposed method is applied to model-based reinforcement learning, where adapting the predictive model is critical for control."
607,SP:5665e5f006f84927beb0440e145f476e02538077,"Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyperparameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games.","This paper studies the problem of training a distributed RL agent from experience replay. The authors propose to train an RNN-based DQN with a single network architecture and a fixed set of hyperparameters. The proposed method is evaluated on Atari-57 and DMLab-30 games, where the proposed method outperforms the state-of-the-art."
608,SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.1","This paper proposes a hierarchical generative model for modeling multi-agent trajectories of basketball games. The authors propose a hierarchical framework that can learn sequential generative models that can capture long-term coordination using intermediate variables. The proposed model is inspired by recent work on leveraging programmatically produced weak labels, which extend to the spatiotemporal regime. In addition to synthetic settings, the authors show how to instantiate their framework to effectively model complex interactions between basketball players and generate realistic multi-Agent trajectories over long time periods."
609,SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"We present a method that learns to integrate temporal information, from a learned dynamics model, with ambiguous visual information, from a learned vision model, in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.","This paper presents a method to integrate temporal information, from a learned dynamics model, with ambiguous visual information, with a learned vision model, in the context of interacting agents. The method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. The proposed method outperforms various baselines on two sports datasets, one based on real basketball trajectories and one generated by a soccer game engine."
610,SP:8392f04b7265f665ba6d44d297bca245d44b4708,"Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing blackbox functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external blackbox non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this “Estimate and Replace” paradigm, we train a neural network, end to end, to compute the input to blackbox functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods.","This paper proposes a method for end-to-end training of a base neural network that integrates calls to existing blackbox functions. The base network is trained by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black box function interface during the optimization process. At inference time, the differentiable estimator is replaced with its external black box non-differentiable counterpart such that the network output matches the input arguments of the blackbox function. The integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods."
611,SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,"Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not mutually beneficial, for instance, when tasks are sufficiently dissimilar or change over time. Here, we use the connection between gradient-based meta-learning and hierarchical Bayes (Grant et al., 2018) to propose a mixture of hierarchical Bayesian models over the parameters of an arbitrary function approximator such as a neural network. Generalizing the model-agnostic metalearning (MAML) algorithm (Finn et al., 2017), we present a stochastic expectation maximization procedure to jointly estimate parameter initializations for gradient descent as well as a latent assignment of tasks to initializations. This approach better captures the diversity of training tasks as opposed to consolidating inductive biases into a single set of hyperparameters. Our experiments demonstrate better generalization performance on the standard miniImageNet benchmark for 1-shot classification. We further derive a novel and scalable non-parametric variant of our method that captures the evolution of a task distribution over time as demonstrated on a set of few-shot regression tasks.",This paper proposes a meta-learning method that combines the idea of hierarchical Bayesian models and model-agnostic meta learning (MAML) with gradient-based meta learning. The authors propose a stochastic expectation maximization procedure to jointly estimate parameter initializations for gradient descent as well as a latent assignment of tasks to initializations. This approach better captures the diversity of training tasks as opposed to consolidating inductive biases into a single set of hyperparameters. The experiments demonstrate better generalization performance on the standard miniImageNet benchmark for 1-shot classification.
612,SP:a410144dbe19713a06c63da87d9fb58b999a7492,"Auxiliary learning has been shown to improve the generalisation performance of a principal task. But typically, this requires manually-defined auxiliary tasks based on domain knowledge. In this paper, we consider that it may be possible to automatically learn these auxiliary tasks to best suit the principal task, towards optimum auxiliary tasks without any human knowledge. We propose a novel method, Meta Auxiliary Learning (MAXL), which we design for the task of image classification, where the auxiliary task is hierarchical sub-class image classification. The role of the meta learner is to determine sub-class target labels to train a multi-task evaluator, such that these labels improve the generalisation performance on the principal task. Experiments on three different CIFAR datasets show that MAXL outperforms baseline auxiliary learning methods, and is competitive even with a method which uses human-defined sub-class hierarchies. MAXL is self-supervised and general, and therefore offers a promising new direction towards automated generalisation.","This paper proposes a meta-augmentation method for image classification, where the auxiliary task is hierarchical sub-class image classification. The meta-learner is a multi-task evaluator, which is trained to select the target labels for each auxiliary task. The proposed method is self-supervised and general. Experiments on three different CIFAR datasets show that MAXL outperforms baseline auxiliary learning methods, and is competitive even with a method which uses human-defined auxiliary tasks."
613,SP:76248e1c914c60ce69de244fe7ec62488d01e161,"In this paper, we present a neural network based representation for addressing the open set recognition problem. In this representation instances from the same class are close to each other while instances from different classes are further apart, resulting in statistically significant improvement when compared to other approaches on three datasets from two different domains.","This paper presents a neural network based representation for open set recognition. The proposed method is based on the idea that instances from the same class are close to each other while instances from different classes are further apart, resulting in statistically significant improvement when compared to other approaches on three datasets from two different domains. The paper is well-written and well-motivated."
614,SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"To realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. To this end, low-precision networks offer tremendous promise because both energy and area scale down quadratically with the reduction in precision. Here, for the first time, we demonstrate ResNet-18, ResNet-34, ResNet-50, ResNet-152, Inception-v3, densenet161, and VGG-16bn networks on the ImageNet classification benchmark that, at 8-bit precision exceed the accuracy of the full-precision baseline networks after one epoch of finetuning, thereby leveraging the availability of pretrained models. We also demonstrate ResNet-18, ResNet-34, and ResNet-50 4-bit models that match the accuracy of the full-precision baseline networks – the highest scores to date. Surprisingly, the weights of the low-precision networks are very close (in cosine similarity) to the weights of the corresponding baseline networks, making training from scratch unnecessary. We find that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. The number of iterations required by stochastic gradient descent to achieve a given training error is related to the square of (a) the distance of the initial solution from the final plus (b) the maximum variance of the gradient estimates. By drawing inspiration from this observation, we (a) reduce solution distance by starting with pretrained fp32 precision baseline networks and fine-tuning, and (b) combat noise introduced by quantizing weights and activations during training, by using larger batches along with matched learning rate annealing. Sensitivity analysis indicates that these techniques, coupled with proper activation function range calibration, offer a promising heuristic to discover low-precision networks, if they exist, close to fp32 precision baseline networks.","This paper studies the problem of finding low-precision networks that are close to the accuracy of the full precision baseline networks after one epoch of fine-tuning on ImageNet. The authors find that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. They propose to reduce solution distance by starting with pretrained fp32 baseline networks and fine-tuneing, and combat noise introduced by quantizing weights and activations during training, by using larger batches along with matched learning rate annealing. They also demonstrate that these techniques, coupled with proper activation function range calibration, offer a promising heuristic to discover low precision networks, if they exist, close to fP32 precision baseline network."
615,SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,"We introduce an approach to model surface properties governing bounces in everyday scenes. Our model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer two underlying physical properties that govern bouncing restitution and effective collision normals. Our model, Bounce and Learn, comprises two modules – a Physics Inference Module (PIM) and a Visual Inference Module (VIM). VIM learns to infer physical parameters for locations in a scene given a single still image, while PIM learns to model physical interactions for the prediction task given physical parameters and observed pre-collision 3D trajectories. To achieve our results, we introduce the Bounce Dataset comprising 5K RGB-D videos of bouncing trajectories of a foam ball to probe surfaces of varying shapes and materials in everyday scenes including homes and offices. Our proposed model learns from our collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations. We show on our newly collected dataset that our model out-performs baselines, including trajectory fitting with Newtonian physics, in predicting post-bounce trajectories and inferring physical properties of a scene.",This paper proposes a method for predicting post-bounce trajectories of a ball bouncing off a surface. The method is based on a combination of a physics-based model (PIM) and a visual model (VIM) that learns to model the physical properties of a scene. The model is trained on a dataset of 5K RGB-D videos of bouncing trajectories from a foam ball bouncing on surfaces of varying shapes and materials in everyday scenes including homes and offices. The experiments show that the proposed method outperforms the state-of-the-art trajectory fitting with Newtonian physics.
616,SP:010bd055310c363d3cb0fbe0e11546de58220e15,"Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the `1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network’s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.","This paper studies the adversarial vulnerability of neural networks in the presence of adversarial perturbations. The authors show that the gradients of the training objective increase with the square root of the input size of the network. They prove that the `1-norm of these gradients grows as the square-root of the image size, and that the network becomes increasingly vulnerable with growing image size. The paper is well-written and well-motivated, and the experimental results are convincing."
617,SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"The ability of modeling the other agents, such as understanding their intentions and skills, is essential to an agent’s interactions with other agents. Conventional agent modeling relies on passive observation from demonstrations. In this work, we propose an interactive agent modeling scheme enabled by encouraging an agent to learn to probe. In particular, the probing agent (i.e., a learner) learns to interact with the environment and with a target agent (i.e., a demonstrator) to maximize the change in the observed behaviors of that agent. Through probing, rich behaviors can be observed and are used for enhancing the agent modeling to learn a more accurate mind model of the target agent. Our framework consists of two learning processes: i) imitation learning for an approximated agent model and ii) pure curiosity-driven reinforcement learning for an efficient probing policy to discover new behaviors that otherwise can not be observed. We have validated our approach in four different tasks. The experimental results suggest that the agent model learned by our approach i) generalizes better in novel scenarios than the ones learned by passive observation, random probing, and other curiositydriven approaches do, and ii) can be used for enhancing performance in multiple applications including distilling optimal planning to a policy net, collaboration, and competition. A video demo is available at https://www.dropbox.com/ s/8mz6rd3349tso67/Probing_Demo.mov?dl=0.","This paper proposes an imitation-based reinforcement learning method for interactive agent modeling. In particular, the probing agent learns to interact with the environment and with a target agent (i.e., a demonstrator) to maximize the change in the observed behaviors of that agent. Through probing, rich behaviors can be observed and are used for enhancing the agent modeling to learn a more accurate mind model of the target agent. The proposed method consists of two learning processes: i) imitation learning for an approximated agent model and ii) pure curiosity-driven reinforcement learning for a probing policy to discover new behaviors that otherwise can not be observed. The experimental results show that the agent model learned by the proposed method generalizes better in novel scenarios than the ones learned by passive observation, random probing, and other curiosity driven approaches do."
618,SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"We propose a modification to traditional Artificial Neural Networks (ANNs), which provides the ANNs with new aptitudes motivated by biological neurons. Biological neurons work far beyond linearly summing up synaptic inputs and then transforming the integrated information. A biological neuron change firing modes accordingly to peripheral factors (e.g., neuromodulators) as well as intrinsic ones. Our modification connects a new type of ANN nodes, which mimic the function of biological neuromodulators and are termed modulators, to enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns. In this manner, we enable the slope of the activation function to be context dependent. This modification produces statistically significant improvements in comparison with traditional ANN nodes in the context of Convolutional Neural Networks and Long Short-Term Memory networks.","This paper proposes a modification to neural networks inspired by biological neuromodulators. Specifically, the authors introduce a new type of neural network architecture, called modulators, which allows the network to adjust its activation function based on the input patterns. The authors show that the modulators can be used to improve the performance of neural networks in the context of image classification and long-term memory."
619,SP:287a577834fd2820a939a1113b39146a22727491,"We present a neural analysis and synthesis (NANSY) framework that can manipulate voice, pitch, and speed of an arbitrary speech signal. Most of the previous works have focused on using information bottleneck to disentangle analysis features for controllable synthesis, which usually results in poor reconstruction quality. We address this issue by proposing a novel training strategy based on information perturbation. The idea is to perturb information in the original input signal (e.g., formant, pitch, and frequency response), thereby letting synthesis networks selectively take essential attributes to reconstruct the input signal. Because NANSY does not need any bottleneck structures, it enjoys both high reconstruction quality and controllability. Furthermore, NANSY does not require any labels associated with speech data such as text and speaker information, but rather uses a new set of analysis features, i.e., wav2vec feature and newly proposed pitch feature, Yingram, which allows for fully self-supervised training. Taking advantage of fully selfsupervised training, NANSY can be easily extended to a multilingual setting by simply training it with a multilingual dataset. The experiments show that NANSY can achieve significant improvement in performance in several applications such as zero-shot voice conversion, pitch shift, and time-scale modification 1.","This paper presents a neural analysis and synthesis (NANSY) framework that can manipulate voice, pitch, and speed of an arbitrary speech signal. The authors propose a novel training strategy based on information perturbation. The idea is to perturb information in the original input signal (e.g., formant, pitch and frequency response), thereby letting synthesis networks selectively take essential attributes to reconstruct the input signal. Because NANSY does not need any bottleneck structures, it enjoys both high reconstruction quality and controllability. Furthermore, the authors do not require any labels associated with speech data such as text and speaker information, but rather uses a new set of analysis features, i.e., wav2vec feature and newly proposed pitch feature, Yingram, which allows for fully self-supervised training."
620,SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"The (gradient-based) bilevel programming framework is widely used in hyperparameter optimization and has achieved excellent performance empirically. Previous theoretical work mainly focuses on its optimization properties, while leaving the analysis on generalization largely open. This paper attempts to address the issue by presenting an expectation bound w.r.t. the validation set based on uniform stability. Our results can explain some mysterious behaviours of the bilevel programming in practice, for instance, overfitting to the validation set. We also present an expectation bound for the classical cross-validation algorithm. Our results suggest that gradient-based algorithms can be better than cross-validation under certain conditions in a theoretical perspective. Furthermore, we prove that regularization terms in both the outer and inner levels can relieve the overfitting problem in gradient-based algorithms. In experiments on feature learning and data reweighting for noisy labels, we corroborate our theoretical findings.","This paper studies the generalization properties of gradient-based bilevel programming (GBC) algorithms. The authors provide an expectation bound w.r.t. the validation set based on uniform stability. They also provide a lower bound for the classical cross-validation algorithm. In addition, they show that regularization terms in both the outer and inner levels of GBC can help alleviate the overfitting problem in gradient based algorithms. In experiments on feature learning and data reweighting for noisy labels, they corroborate their theoretical findings."
621,SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks.","This paper proposes a novel knowledge distillation method for knowledge transfer between student and teacher models. The main idea is to train a student model on top of a pre-trained teacher model, and then distill the knowledge from the student model to the teacher model. The student model is trained jointly with the teacher models, and the student models are trained jointly to obtain student-friendly representations. Experiments show that the proposed method outperforms the state-of-the-art distillation methods in terms of accuracy and convergence speed."
622,SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"Generalization to out-of-distribution (OOD) data is one of the central problems in modern machine learning. Recently, there is a surge of attempts to propose algorithms that mainly build upon the idea of extracting invariant features. Although intuitively reasonable, theoretical understanding of what kind of invariance can guarantee OOD generalization is still limited, and generalization to arbitrary out-of-distribution is clearly impossible. In this work, we take the first step towards rigorous and quantitative definitions of 1) what is OOD; and 2) what does it mean by saying an OOD problem is learnable. We also introduce a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore give a quantitative meaning of invariant features. Based on these, we prove OOD generalization error bounds. It turns out that OOD generalization largely depends on the expansion function. As recently pointed out by [21], any OOD learning algorithm without a model selection module is incomplete. Our theory naturally induces a model selection criterion. Extensive experiments on benchmark OOD datasets demonstrate that our model selection criterion has a significant advantage over baselines.","This paper studies the problem of generalization to out-of-distribution (OOD) data. The authors propose a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore give a quantitative meaning of invariant features. Based on these, the authors prove OOD generalization error bounds and show that the generalization performance depends on the expansion function. They also show that any OOD learning algorithm without a model selection module is incomplete. Extensive experiments on benchmark OOD datasets demonstrate that their model selection criterion has a significant advantage over baselines."
623,SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"Conventional meta-learning considers a set of tasks from a stationary distribution. In contrast, this paper focuses on a more complex online setting, where tasks arrive sequentially and follow a non-stationary distribution. Accordingly, we propose a Variational Continual Bayesian Meta-Learning (VC-BML) algorithm. VC-BML maintains a Dynamic Gaussian Mixture Model for meta-parameters, with the number of component distributions determined by a Chinese Restaurant Process. Dynamic mixtures at the meta-parameter level increase the capability to adapt to diverse and dissimilar tasks due to a larger parameter space, alleviating the negative knowledge transfer problem. To infer the posteriors of model parameters, compared to the previously used point estimation method, we develop a more robust posterior approximation method – structured variational inference for the sake of avoiding forgetting knowledge. Experiments on tasks from non-stationary distributions show that VC-BML is superior in transferring knowledge among diverse tasks and alleviating catastrophic forgetting in an online setting.","This paper proposes a Variational Continual Bayesian Meta-Learning (VC-BML) algorithm for online meta-learning. The authors propose a Dynamic Gaussian Mixture Model for meta-parameters, with the number of component distributions determined by a Chinese Restaurant Process (CPR). The authors also propose a more robust posterior approximation method, structured variational inference for the sake of avoiding forgetting knowledge. Experiments on tasks from non-stationary distributions show that the proposed algorithm is superior in transferring knowledge among diverse tasks and alleviating catastrophic forgetting in an online setting."
624,SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"We propose a fast algorithm for the probabilistic solution of boundary value problems (BVPs), which are ordinary differential equations subject to boundary conditions. In contrast to previous work, we introduce a Gauss–Markov prior and tailor it specifically to BVPs, which allows computing a posterior distribution over the solution in linear time, at a quality and cost comparable to that of wellestablished, non-probabilistic methods. Our model further delivers uncertainty quantification, mesh refinement, and hyperparameter adaptation. We demonstrate how these practical considerations positively impact the efficiency of the scheme. Altogether, this results in a practically usable probabilistic BVP solver that is (in contrast to non-probabilistic algorithms) natively compatible with other parts of the statistical modelling tool-chain. 1 Boundary value problems in computational pipelines This work develops a class of algorithms for solving ODE boundary value problems; that is, ordinary differential equations (ODEs) ẏ(t) = f(y(t), t) (1) subject to leftand right-hand side boundary conditions Ly(t0) = y0 and Ry(tmax) = ymax. The vector field f : R → R, as well as L ∈ RdL×d, R ∈ RdR×d, t0 ∈ R, tmax ∈ R, y0 ∈ RL, and ymax ∈ RR are given. It is no loss of generality to consider a first-order boundary value problem because higher-order problems can be transformed into first-order problems [1]. Loosely speaking, solving BVPs amounts to following the law of a dynamical system when “connecting two points”. This setting is relevant to several scientific applications of machine learning. As motivation, we consider three examples, all of which are depicted in Figure 1. First, recovering the trajectory of a pendulum between two positions amounts to solving the ODE ÿ(t) = −9.81 sin(y(t)) subject to the positions as boundary conditions. If the positions were interpolated without the ODE knowledge, the output would be physically meaningless. Second, BVPs arise when inferring the evolution of the case counts of people that fall victim to an infectious disease. A lack of counts of (a specific subset of) non-infected people at the initial time-point can be made up for by available counts of infected people at the final time-","This paper proposes a probabilistic solution of boundary value problems (BVPs), which are ordinary differential equations subject to boundary conditions. The authors propose a Gauss–Markov prior and tailor it specifically to BVPs, which allows computing a posterior distribution over the solution in linear time, at a quality and cost comparable to that of well established, non-probabilistic methods. The model further delivers uncertainty quantification, mesh refinement, and hyperparameter adaptation. The experimental results demonstrate how these practical considerations positively impact the efficiency of the scheme."
625,SP:86aac0c6b75fdc12f84bba342934865616f866d4,"Learning a near optimal policy in a partially observable system remains an elusive challenge in contemporary reinforcement learning. In this work, we consider episodic reinforcement learning in a reward-mixingMarkov decision process (MDP). There, a reward function is drawn from one of multiple possible reward models at the beginning of every episode, but the identity of the chosen reward model is not revealed to the agent. Hence, the latent state space, for which the dynamics are Markovian, is not given to the agent. We study the problem of learning a near optimal policy for two reward-mixing MDPs. Unlike existing approaches that rely on strong assumptions on the dynamics, we make no assumptions and study the problem in full generality. Indeed, with no further assumptions, even for two switching reward-models, the problem requires several new ideas beyond existing algorithmic and analysis techniques for efficient exploration. We provide the first polynomial-time algorithm that finds an -optimal policy after exploring Õ(poly(H, −1)·S2A2) episodes, whereH is time-horizon andS,A are the number of states and actions respectively. This is the first efficient algorithm that does not require any assumptions in partially observed environments where the observation space is smaller than the latent state space.","This paper studies the problem of learning a near optimal policy in episodic reinforcement learning in a reward-mixing Markov decision process (MDP). The reward function is drawn from one of multiple possible reward models at the beginning of every episode, but the identity of the chosen reward model is not revealed to the agent. The latent state space, for which the dynamics are Markovian, does not give the agent access to the state space. The authors provide a polynomial-time algorithm that finds an optimal policy after exploring $O(\sqrt{H})$ episodes, where $H$ is time-horizon and $S$ is the number of states and actions."
626,SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"Most existing methods for conditional average treatment effect estimation are designed to estimate the effect of a single cause — only one variable can be intervened on at one time. However, many applications involve simultaneous intervention on multiple variables, which leads to multi-cause treatment effect problems. The multi-cause problem is challenging because one needs to overcome the confounding bias for a large number of treatment groups, each with a different cause combination. The combinatorial nature of the problem also leads to severe data scarcity — we only observe one factual outcome out of many potential outcomes. In this work, we propose Single-cause Perturbation (SCP), a novel two-step procedure to estimate the multi-cause treatment effect. SCP starts by augmenting the observational dataset with the estimated potential outcomes under single-cause interventions. It then performs covariate adjustment on the augmented dataset to obtain the estimator. SCP is agnostic to the exact choice of algorithm in either step. We show formally that the procedure is valid under standard assumptions in causal inference. We demonstrate the performance gain of SCP on extensive synthetic and semi-synthetic experiments.",This paper proposes a method for conditional average treatment effect estimation for multi-causal interventions. The proposed method is based on augmenting the observational dataset with the estimated potential outcomes under single-cause interventions. It then performs covariate adjustment on the augmented dataset to obtain the estimator. The method is agnostic to the exact choice of algorithm in either step. The experimental results on synthetic and semi-synthetic experiments demonstrate the performance gain of the proposed method.
627,SP:247bc6675cce89d51558537daf63dadb0c4307f8,"The solution of a partial differential equation can be obtained by computing the inverse operator map between the input and the solution space. Towards this end, we introduce a multiwavelet-based neural operator learning scheme that compresses the associated operator’s kernel using fine-grained wavelets. By explicitly embedding the inverse multiwavelet filters, we learn the projection of the kernel onto fixed multiwavelet polynomial bases. The projected kernel is trained at multiple scales derived from using repeated computation of multiwavelet transform. This allows learning the complex dependencies at various scales and results in a resolution-independent scheme. Compare to the prior works, we exploit the fundamental properties of the operator’s kernel which enable numerically efficient representation. We perform experiments on the Korteweg-de Vries (KdV) equation, Burgers’ equation, Darcy Flow, and Navier-Stokes equation. Compared with the existing neural operator approaches, our model shows significantly higher accuracy and achieves state-of-the-art in a range of datasets. For the time-varying equations, the proposed method exhibits a (2X−10X) improvement (0.0018 (0.0033) relative L2 error for Burgers’ (KdV) equation). By learning the mappings between function spaces, the proposed method has the ability to find the solution of a high-resolution input after learning from lower-resolution data.","This paper proposes a multi-wavelet-based neural operator learning scheme that compresses the associated operator’s kernel using fine-grained wavelets. By explicitly embedding the inverse multiwavelet filters, the proposed method learns the projection of the kernel onto fixed multi wavelet polynomial bases. This allows learning the complex dependencies at various scales and results in a resolution-independent scheme. The proposed method achieves state-of-the-art results on the Korteweg-de Vries (KdV) equation, Burgers’ equation, Darcy Flow, and Navier-Stokes equation."
628,SP:1153785e6a016cfee2644952a772aa08927299b6,"Binary neural networks (BNNs) represent original full-precision weights and activations into 1-bit with sign function. Since the gradient of the conventional sign function is almost zero everywhere which cannot be used for back-propagation, several attempts have been proposed to alleviate the optimization difficulty by using approximate gradient. However, those approximations corrupt the main direction of factual gradient. To this end, we propose to estimate the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA). The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. In addition, we embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using our method achieves the state-of-the-art accuracy. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/FDA-BNN.","This paper proposes a novel method for training binary neural networks (BNNs) by approximating the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA). The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. In addition, the authors embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using DA achieves the state-of-the-art accuracy."
629,SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,"Recurrent neural networks (RNNs) trained on neuroscience-based tasks have been widely used as models for cortical areas performing analogous tasks. However, very few tasks involve a single cortical area, and instead require the coordination of multiple brain areas. Despite the importance of multi-area computation, there is a limited understanding of the principles underlying such computation. We propose to use multi-area RNNs with neuroscience-inspired architecture constraints to derive key features of multi-area computation. In particular, we show that incorporating multiple areas and Dale’s Law is critical for biasing the networks to learn biologically plausible solutions. Additionally, we leverage the full observability of the RNNs to show that output-relevant information is preferentially propagated between areas. These results suggest that cortex uses modular computation to generate minimal sufficient representations of task information. More broadly, our results suggest that constrained multi-area RNNs can produce experimentally testable hypotheses for computations that occur within and across multiple brain areas, enabling new insights into distributed computation in neural systems.","This paper proposes to use multi-area RNNs with neuroscience-inspired architecture constraints to derive key features of multi-Area computation. In particular, the authors show that incorporating multiple areas and Dale’s Law is critical for biasing the networks to learn biologically plausible solutions. The authors also leverage the full observability of the RNN to show that output-relevant information is preferentially propagated between areas. These results suggest that cortex uses modular computation to generate minimal sufficient representations of task information."
630,SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,"Saliency maps are popular tools for explaining the decisions of convolutional neural networks (CNNs) for image classification. Typically, for each image of interest, a single saliency map is produced, which assigns weights to pixels based on their importance to the classification. We argue that a single saliency map provides an incomplete understanding since there are often many other maps that can explain a classification equally well. In this paper, we propose to utilize a beam search algorithm to systematically search for multiple explanations for each image. Results show that there are indeed multiple relatively localized explanations for many images. However, naively showing multiple explanations to users can be overwhelming and does not reveal their common and distinct structures. We introduce structured attention graphs (SAGs), which compactly represent sets of attention maps for an image by visualizing how different combinations of image regions impact the confidence of a classifier. An approach to computing a compact and representative SAG for visualization is proposed via diverse sampling. We conduct a user study comparing the use of SAGs to traditional saliency maps for answering comparative counterfactual questions about image classifications. Our results show that user accuracy is increased significantly when presented with SAGs compared to standard saliency map baselines.",This paper proposes a method to search for multiple explanations for a single saliency map for image classification. The main idea is to use a beam search algorithm to systematically search for several explanations for each image. The authors propose structured attention graphs (SAGs) to represent sets of attention maps for an image by visualizing how different combinations of image regions impact the confidence of a classifier. An approach to computing a compact and representative SAG for visualization is proposed via diverse sampling. The paper conducts a user study comparing the use of SAGs to traditional saliency maps for answering counterfactual questions about image classifications.
631,SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.","This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. The authors show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks. The choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment, the authors find that differences among loss functions are apparent only in the last few layers of the network."
632,SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"Modern neural interfaces allow access to the activity of up to a million neurons within brain circuits. However, bandwidth limits often create a trade-off between greater spatial sampling (more channels or pixels) and the temporal frequency of sampling. Here we demonstrate that it is possible to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. Our novel neural network training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. We test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data. In electrophysiology, SBTT enables accurate inference of neuronal population dynamics with lower interface bandwidths, providing an avenue to significant power savings for implanted neuroelectronic interfaces. In applications to two-photon calcium imaging, SBTT accurately uncovers high-frequency temporal structure underlying neural population activity, substantially outperforming the current state-of-the-art. Finally, we demonstrate that performance could be further improved by using limited, highbandwidth sampling to pretrain dynamics models, and then using SBTT to adapt these models for sparsely-sampled data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). * Contributed equally. Corresponding authors: fzhu23@emory.edu, {asedler,chethan}@gatech.edu",This paper proposes a method to train a neural network that can infer the dynamics of neurons in a neural time series. The method is based on the idea of selective backpropagation through time (SBTT). The authors propose to train the model on a set of data points where the set of observed variables changes at each time step. The authors show that the learned model is able to infer activity for missing samples by combining observations with learned latent dynamics. They test SBTT applied to sequential autoencoders and demonstrate more efficient and higher fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data.
633,SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"Sequence-to-sequence learning with neural networks has become the de facto standard for sequence prediction tasks. This approach typically models the local distribution over the next word with a powerful neural network that can condition on arbitrary context. While flexible and performant, these models often require large datasets for training and can fail spectacularly on benchmarks designed to test for compositional generalization. This work explores an alternative, hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where each node in the target tree is transduced by a node in the source tree. Both the source and target trees are treated as latent and induced during training. We develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. We apply this latent neural grammar to various domains—a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation—and find that it performs respectably compared to standard baselines.","This paper proposes a hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where each node in the target tree is transduced by a node in a source tree. The source and target trees are treated as latent and induced during training. The authors develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. They apply this latent neural grammar to various domains such as SCAN, style transfer, and small-scale machine translation."
634,SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"Feature Selection and Functional Data Analysis are two dynamic areas of research, with important applications in the analysis of large and complex data sets. Straddling these two areas, we propose a new highly efficient algorithm to perform Group Elastic Net with application to function-on-scalar feature selection, where a functional response is modeled against a very large number of potential scalar predictors. First, we introduce a new algorithm to solve Group Elastic Net in ultrahigh dimensional settings, which exploits the sparsity structure of the Augmented Lagrangian to greatly reduce computational burden. Next, taking advantage of the properties of Functional Principal Components, we extend our algorithm to the function-on-scalar regression framework. We use simulations to demonstrate the CPU time gains afforded by our approach compared to its best existing competitors, and present an application to data from a Genome Wide Association Study on childhood obesity.",This paper proposes a new algorithm to solve the Group Elastic Net problem in the context of function-on-scalar feature selection. The main idea is to use the sparsity structure of the Augmented Lagrangian to reduce the computational burden of the algorithm. The authors also extend the algorithm to the function on scalar regression framework and show that the proposed algorithm can achieve better performance than existing methods. 
635,SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,"Structured point process data harvested from various platforms poses new challenges to the machine learning community. To cluster repeatedly observed marked point processes, we propose a novel mixture model of multi-level marked point processes for identifying potential heterogeneity in the observed data. Specifically, we study a matrix whose entries are marked log-Gaussian Cox processes and cluster rows of such a matrix. An efficient semi-parametric Expectation-Solution (ES) algorithm combined with functional principal component analysis (FPCA) of point processes is proposed for model estimation. The effectiveness of the proposed framework is demonstrated through simulation studies and real data analyses.","This paper proposes a method to cluster repeatedly observed marked point processes to identify potential heterogeneity in the observed data. Specifically, the authors study a matrix whose entries are marked log-Gaussian Cox processes and cluster rows of such a matrix. An efficient semi-parametric Expectation-Solution (ES) algorithm combined with functional principal component analysis (FPCA) is proposed for model estimation. The effectiveness of the proposed framework is demonstrated through simulation studies and real data analyses."
636,SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"We present an online multi-task learning approach for adaptive nonlinear control, which we call Online Meta-Adaptive Control (OMAC). The goal is to control a nonlinear system subject to adversarial disturbance and unknown environmentdependent nonlinear dynamics, under the assumption that the environmentdependent dynamics can be well captured with some shared representation. Our approach is motivated by robot control, where a robotic system encounters a sequence of new environmental conditions that it must quickly adapt to. A key emphasis is to integrate online representation learning with established methods from control theory, in order to arrive at a unified framework that yields both control-theoretic and learning-theoretic guarantees. We provide instantiations of our approach under varying conditions, leading to the first non-asymptotic endto-end convergence guarantee for multi-task nonlinear control. OMAC can also be integrated with deep representation learning. Experiments show that OMAC significantly outperforms conventional adaptive control approaches which do not learn the shared representation, in inverted pendulum and 6-DoF drone control tasks under varying wind conditions1.","This paper proposes an online meta-adaptive multi-task learning approach for adaptive nonlinear control. The main idea is to learn a shared representation of the dynamics of the system, which can then be used to adaptively control the system. The authors provide a convergence analysis for the proposed method, and show that it converges to a state-of-the-art convergence rate. The paper also provides a theoretical analysis of the convergence rate of the proposed algorithm. The experimental results show that the proposed approach outperforms existing methods in the inverted pendulum and 6-DoF tasks."
637,SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"Recently, bound propagation based certified robust training methods have been proposed for training neural networks with certifiable robustness guarantees. Despite that state-of-the-art (SOTA) methods including interval bound propagation (IBP) and CROWN-IBP have per-batch training complexity similar to standard neural network training, they usually use a long warmup schedule with hundreds or thousands epochs to reach SOTA performance and are thus still costly. In this paper, we identify two important issues in existing methods, namely exploded bounds at initialization, and the imbalance in ReLU activation states and improve IBP training. These two issues make certified training difficult and unstable, and thereby long warmup schedules were needed in prior works. To mitigate these issues and conduct faster certified training with shorter warmup, we propose three improvements based on IBP training: 1) We derive a new weight initialization method for IBP training; 2) We propose to fully add Batch Normalization (BN) to each layer in the model, since we find BN can reduce the imbalance in ReLU activation states; 3) We also design regularization to explicitly tighten certified bounds and balance ReLU activation states during wamrup. We are able to obtain 65.03% verified error on CIFAR-10 (✏ = 8 255 ) and 82.36% verified error on TinyImageNet (✏ = 1 255 ) using very short training schedules (160 and 80 total epochs, respectively), outperforming literature SOTA trained with hundreds or thousands epochs under the same network architecture. The code is available at https: //github.com/shizhouxing/Fast-Certified-Robust-Training.","This paper proposes a new method for training neural networks with certifiable robustness guarantees. The method is based on interval bound propagation (IBP) and CROWN-IBP. The authors identify two important issues in existing methods, namely exploded bounds at initialization, and the imbalance in ReLU activation states and improve IBP training. To mitigate these issues and conduct faster certified training with shorter warmup, the authors derive a new weight initialization method and propose to fully add Batch Normalization (BN) to each layer in the model, since they find BN can reduce the imbalance of ReLU activations. The paper also design regularization to explicitly tighten certified bounds and balance ReLu activation states during wamrup. "
638,SP:18ffeb199a670fb2b1f4417b8653479001944dab,"Change point detection is becoming increasingly popular in many application areas. On one hand, most of the theoretically-justified methods are investigated in an ideal setting without model violations, or merely robust against identical heavy-tailed noise distribution across time and/or against isolate outliers; on the other hand, we are aware that there have been exponentially growing attacks from adversaries, who may pose systematic contamination on data to purposely create spurious change points or disguise true change points. In light of the timely need of a change point detection method that is robust against adversaries, we start with, arguably, the simplest univariate mean change point detection problem. The adversarial attacks are formulated through the Huber ε-contamination framework, which in particular allows the contamination distributions to be different at each time point. In this paper, we demonstrate a phase transition phenomenon in change point detection. This detection boundary is a function of the contamination proportion ε and is the first time shown in the literature. In addition, we derive the minimax-rate optimal localisation error rate, quantifying the cost of accuracy in terms of the contamination proportion. We propose a computationally-feasible method, matching the minimax lower bound under certain conditions, saving for logarithmic factors. Extensive numerical experiments are conducted with comparisons to existing robust change point detection methods.","This paper studies the problem of change point detection in the context of adversarial attacks. The authors propose a novel method that is robust against the Huber-contamination framework, which allows the contamination distributions to be different at each time point. The detection boundary is a function of the contamination proportion and is the first time shown in the literature. In addition, the authors derive the minimax-rate optimal localisation error rate, which quantifies the cost of accuracy in terms of contamination proportion. Extensive numerical experiments are conducted with comparisons to existing robust change point detectors methods."
639,SP:d03617b5fc446768809cf015c9234b0c9386a690,"We study the power of learning via mini-batch stochastic gradient descent (SGD) on the population loss, and batch Gradient Descent (GD) on the empirical loss, of a differentiable model or neural network, and ask what learning problems can be learnt using these paradigms. We show that SGD and GD can always simulate learning with statistical queries (SQ), but their ability to go beyond that depends on the precision ρ of the gradient calculations relative to the minibatch size b (for SGD) and sample size m (for GD). With fine enough precision relative to minibatch size, namely when bρ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning; this extends prior work that achieved this result for b = 1. Similarly, with fine enough precision relative to the sample size m, GD can also simulate any sample-based learning algorithm based on m samples. In particular, with polynomially many bits of precision (i.e. when ρ is exponentially small), SGD and GD can both simulate PAC learning regardless of the mini-batch size. On the other hand, when bρ is large enough, the power of SGD is equivalent to that of SQ learning.","This paper studies the power of learning via mini-batch SGD and batch gradient descent (GD) on the empirical loss of a differentiable model or neural network. In particular, the authors study the precision of the gradient calculations relative to the minibatch size b (for SGD) and sample size m (for GD) and show that when bρ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning; this extends prior work that achieved this result for b = 1. Similarly, with polynomially many bits of precision (i.e. when ρ is exponentially small), GD can both simulate PAC learning regardless of the mini batch size. On the other hand, when b^{-1/\epsilon} is large enough, the learning power of SGD is equivalent of that of SQ learning."
640,SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"Several issues in machine learning and inverse problems require to generate discrete data, as if sampled from a model probability distribution. A common way to do so relies on the construction of a uniform probability distribution over a set of N points which minimizes the Wasserstein distance to the model distribution. This minimization problem, where the unknowns are the positions of the atoms, is non-convex. Yet, in most cases, a suitably adjusted version of Lloyd’s algorithm — in which Voronoi cells are replaced by Power cells — leads to configurations with small Wasserstein error. This is surprising because, again, of the non-convex nature of the problem, as well as the existence of spurious critical points. We provide explicit estimates for the convergence of this Lloyd-type algorithm, starting from a cloud of points that are sufficiently far from each other. Our estimates are tight when the algorithm is initialized from an point cloud that is evenly distributed in the ambient space. Similar bounds can be deduced for the corresponding gradient descent. These bounds naturally lead to a modified Poliak-Łojasiewicz inequality for the Wasserstein distance cost, with an error term depending on the distances between Dirac masses in the discrete distribution.","This paper studies the problem of generating a discrete probability distribution over a set of N points that minimizes the Wasserstein distance to the model distribution. This problem is non-convex in the sense that the unknowns are the positions of the atoms. The authors show that a suitably adjusted version of Lloyd’s algorithm, in which Voronoi cells are replaced by Power cells, leads to configurations with small Wassersteins error. This is surprising because, again, of the non-Convex nature of the problem, as well as the existence of spurious critical points. They provide explicit estimates for the convergence of this Lloyd-type algorithm, starting from a cloud of points that are sufficiently far from each other."
641,SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"Convolution has been arguably the most important feature transform for modern neural networks, leading to the advance of deep learning. Recent emergence of Transformer networks, which replace convolution layers with self-attention blocks, has revealed the limitation of stationary convolution kernels and opened the door to the era of dynamic feature transforms. The existing dynamic transforms, including self-attention, however, are all limited for video understanding where correspondence relations in space and time, i.e., motion information, are crucial for effective representation. In this work, we introduce a relational feature transform, dubbed the relational self-attention (RSA), that leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. Our experiments and ablation studies show that the RSA network substantially outperforms convolution and self-attention counterparts, achieving the state of the art on the standard motion-centric benchmarks for video action recognition, such as Something-Something-V1&V2, Diving48, and FineGym.","This paper proposes a dynamic self-attention method for video recognition. The proposed method is based on the idea of dynamic convolutional kernels, where the kernel is generated by aggregating relations in space and time. The authors propose to generate the kernels based on spatio-temporal relations in video frames, which are aggregated into a relational kernel, which is then aggregated to produce the final feature representation. The method is evaluated on the standard motion-centric benchmarks for video action recognition, such as Something-Something-V1&V2, Diving48, and FineGym. "
642,SP:2c2530069d5cab485629090243da464d107feadd,"The mean field theory of multilayer neural networks centers around a particular infinite-width scaling, in which the learning dynamics is shown to be closely tracked by the mean field limit. A random fluctuation around this infinite-width limit is expected from a large-width expansion to the next order. This fluctuation has been studied only in the case of shallow networks, where previous works employ heavily technical notions or additional formulation ideas amenable only to that case. Treatment of the multilayer case has been missing, with the chief difficulty in finding a formulation that must capture the stochastic dependency across not only time but also depth. In this work, we initiate the study of the fluctuation in the case of multilayer networks, at any network depth. Leveraging on the neuronal embedding framework recently introduced by Nguyen and Pham [17], we systematically derive a system of dynamical equations, called the second-order mean field limit, that captures the limiting fluctuation distribution. We demonstrate through the framework the complex interaction among neurons in this second-order mean field limit, the stochasticity with cross-layer dependency and the nonlinear time evolution inherent in the limiting fluctuation. A limit theorem is proven to relate quantitatively this limit to the fluctuation realized by large-width networks. We apply the result to show a stability property of gradient descent mean field training: in the large-width regime, along the training trajectory, it progressively biases towards a solution with “minimal fluctuation” (in fact, vanishing fluctuation) in the learned output function, even after the network has been initialized at or has converged (sufficiently fast) to a global optimum. This extends a similar phenomenon previously shown only for shallow networks with a squared loss in the empirical risk minimization setting, to multilayer networks with a loss function that is not necessarily convex in a more general setting.","This paper studies the second-order mean field limit of multilayer neural networks. The authors derive a system of dynamical equations that captures the limiting fluctuation distribution around the infinite-width limit of the network. They show that the second order limit is related to the limit of large-width networks. They also show that gradient descent mean field training progressively biases towards a solution with minimal fluctuation, even after the network has been initialized at or has converged to a global optimum."
643,SP:a3d927854d9d7fd39b8d05a79666810d585d5062,"Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with reversible dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning irreversible dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either ""black-box"" or penalty-based approaches. 1 Background and previous work Modeling time-series data as a solution to a dynamical system with learnable dynamics has been shown to be effective in both data-driven modeling for physical systems and traditional machine learning (ML) tasks. Broadly, it has been observed that imposition of physics-based structure leads to more robust architectures which generalize well [1]. On one end of the spectrum of inductive biases, universal differential equations (UDE) [2] assume an a priori known model form, thus imposing the strongest bias. On the other, neural ordinary differential equations (NODEs) [3] assume a completely black-box model form with minimal bias. Many recent approaches have turned to structure preserving models of reversible dynamics to obtain an inductive bias that lies in between [4, 5, 6, 7, 8]. One may use black-box deep neural networks (DNNs) to learn an energy of a system with unknown model form, while the algebraic structure of Hamiltonian/Lagrangian dynamics provides a flow map which conserves energy. Typically, the learned flow map has symplectic structure so that phase space trajectories are conserved. In classification problems, this mitigates the vanishing/exploding gradient problem and improves accuracy [9]; in physics, this guarantees that extrapolated states are physically realizable [10]. Such approaches are only appropriate for reversible systems lacking friction or dissipation. In the physics literature, the theory of metriplectic dynamical systems provides a generalization of the Poisson brackets of Hamiltonian/Lagrangian mechanics which model not just a conserved energy, but generalized Casimirs such as entropy [11, 12]. Physical systems which can be cast in this framework","This paper proposes a method for learning reversible dynamical systems with unknown a priori model form. In particular, the authors propose a novel parameterization of dissipative brackets from metriplectic dynamical system appropriate for learning irreversible dynamics with unknown model form, which learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, they guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. The authors provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either ""black box"" or penalty-based approaches."
644,SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"Fairness and robustness are critical elements of Trustworthy AI that need to be addressed together. Fairness is about learning an unbiased model while robustness is about learning from corrupted data, and it is known that addressing only one of them may have an adverse affect on the other. In this work, we propose a sample selection-based algorithm for fair and robust training. To this end, we formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption. Observing that solving this optimization problem is strongly NP-hard, we propose a greedy algorithm that is efficient and effective in practice. Experiments show that our algorithm obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets. Moreover, unlike other fair and robust training baselines, our algorithm can be used by only modifying the sampling step in batch selection without changing the training algorithm or leveraging additional clean data.","This paper proposes a sample selection-based algorithm for fair and robust training. The authors formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption. They propose a greedy algorithm that is efficient and effective in practice. Experiments show that their algorithm obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets."
645,SP:991127729bf067fe27fdd7ed360aab39e4df5921,"Neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. We seek to build models that ‘know what they do not know’ by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection.","This paper proposes periodic activation functions for Bayesian neural networks (BNNs). The authors show that the periodic activation function can be used as a regularizer to regularize the weights of BNNs. The authors also show that this regularizer can also be used for other functions such as triangular wave and periodic ReLU activation functions. In the experiments, the authors compare the performance of the proposed method with sinusoidal and Fourier activation functions and show that periodic activation is comparable to sinusoid activation for in-domain and out-of-domain detection. "
646,SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,"Contemporary coding education often presents students with the task of developing programs that have user interaction and complex dynamic systems, such as mouse based games. While pedagogically compelling, there are no contemporary autonomous methods for providing feedback. Notably, interactive programs are impossible to grade by traditional unit tests. In this paper we formalize the challenge of providing feedback to interactive programs as a task of classifying Markov Decision Processes (MDPs). Each student’s program fully specifies an MDP where the agent needs to operate and decide, under reasonable generalization, if the dynamics and reward model of the input MDP should be categorized as correct or broken. We demonstrate that by designing a cooperative objective between an agent and an autoregressive model, we can use the agent to sample differential trajectories from the input MDP that allows a classifier to determine membership: Play to Grade. Our method enables an automatic feedback system for interactive code assignments. We release a dataset of 711,274 anonymized student submissions to a single assignment with hand-coded bug labels to support future research.","This paper proposes a method for automatic feedback for interactive code assignments in coding classes. The authors propose to use an autoregressive model to sample trajectories from the input MDP to a classifier to determine membership of the MDP. The proposed method is evaluated on a dataset of 711,274 anonymized student submissions to a single assignment with hand-coded bug labels. "
647,SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"Interpreting Deep Reinforcement Learning (DRL) models is important to enhance trust and comply with transparency regulations. Existing methods typically explain a DRL model by visualizing the importance of low-level input features with superpixels, attentions, or saliency maps. Our approach provides an interpretation based on high-level latent object features derived from a disentangled representation. We propose a Represent And Mimic (RAMi) framework for training 1) an identifiable latent representation to capture the independent factors of variation for the objects and 2) a mimic tree that extracts the causal impact of the latent features on DRL action values. To jointly optimize both the fidelity and the simplicity of a mimic tree, we derive a novel Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. Based on this objective, we describe a Monte Carlo Regression Tree Search (MCRTS) algorithm that explores different splits to find the IB-optimal mimic tree. Experiments show that our mimic tree achieves strong approximation performance with significantly fewer nodes than baseline models. We demonstrate the interpretability of our mimic tree by showing latent traversals, decision rules, causal impacts, and human evaluation results.","This paper proposes a method for interpretability of deep reinforcement learning (DRL) models based on disentangled latent representations. The authors propose to train a latent representation for each object and a mimic tree that extracts the causal impact of the latent features on DRL action values. To jointly optimize both the fidelity and the simplicity of the mimic tree, the authors derive a novel Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. The paper also proposes a Monte Carlo Regression Tree Search (MCRTS) algorithm that explores different splits to find the IB-optimal mimic tree. Experiments show that the proposed method achieves strong approximation performance with significantly fewer nodes than baseline models."
648,SP:84560de78af979354fff83d1370d8675c1e9191f,"In settings ranging from weather forecasts to political prognostications to financial projections, probability estimates of future binary outcomes often evolve over time. For example, the estimated likelihood of rain on a specific day changes by the hour as new information becomes available. Given a collection of such probability paths, we introduce a Bayesian framework—which we call the Gaussian latent information martingale, or GLIM—for modeling the structure of dynamic predictions over time. Suppose, for example, that the likelihood of rain in a week is 50%, and consider two hypothetical scenarios. In the first, one expects the forecast to be equally likely to become either 25% or 75% tomorrow; in the second, one expects the forecast to stay constant for the next several days. A time-sensitive decision-maker might select a course of action immediately in the latter scenario, but may postpone their decision in the former, knowing that new information is imminent. We model these trajectories by assuming predictions update according to a latent process of information flow, which is inferred from historical data. In contrast to general methods for time series analysis, this approach preserves important properties of probability paths such as the martingale structure and appropriate amount of volatility and better quantifies future uncertainties around probability paths. We show that GLIM outperforms three popular baseline methods, producing better estimated posterior probability path distributions measured by three different metrics. By elucidating the dynamic structure of predictions over time, we hope to help individuals make more informed choices.","This paper proposes a Bayesian framework for modeling the structure of dynamic predictions over time. The authors model the trajectories of future binary outcomes by assuming predictions update according to a latent process of information flow, which is inferred from historical data. This approach preserves important properties of probability paths such as the martingale structure and appropriate amount of volatility and better quantifies future uncertainties around probability paths. They show that GLIM outperforms three popular baseline methods, producing better estimated posterior probability path distributions measured by three different metrics."
649,SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"We study the problem of active pure exploration with fixed confidence in generic stochastic bandit environments. The goal of the learner is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance-specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. These proportions solve an optimization problem whose tractability strongly depends on the structural properties of the environment, but may be instrumental in the design of efficient learning algorithms. We devise Frank-Wolfe-based Sampling (FWS), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. The algorithm is computationally efficient as, to learn and track the optimal proportion of arm draws, it relies on a single iteration of Frank-Wolfe algorithm applied to the lower-bound optimization problem. We apply FWS to various pure exploration tasks, including best arm identification in unstructured, thresholded, linear, and Lipschitz bandits. Despite its simplicity, FWS is competitive compared to state-of-art algorithms.","This paper studies the problem of active pure exploration with fixed confidence in stochastic bandit environments. The goal of the learner is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance-specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. The authors propose Frank-Wolfe-based Sampling (FWS), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. They apply FWS to various pure exploration tasks including best arm identification in unstructured, thresholded, linear, and Lipschitz bandits. They show that FWS is competitive compared to state-of-art algorithms."
650,SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"We consider the problem of optimizing combinatorial spaces (e.g., sequences, trees, and graphs) using expensive black-box function evaluations. For example, optimizing molecules for drug design using physical lab experiments. Bayesian optimization (BO) is an efficient framework for solving such problems by intelligently selecting the inputs with high utility guided by a learned surrogate model. A recent BO approach for combinatorial spaces is through a reduction to BO over continuous spaces by learning a latent representation of structures using deep generative models (DGMs). The selected input from the continuous space is decoded into a discrete structure for performing function evaluation. However, the surrogate model over the latent space only uses the information learned by the DGM, which may not have the desired inductive bias to approximate the target black-box function. To overcome this drawback, this paper proposes a principled approach referred as LADDER. The key idea is to define a novel structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. Our experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method, and performs better or similar to state-of-the-art methods.","This paper proposes a new method for Bayesian optimization of combinatorial spaces. The main idea is to use a structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. The experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method, and performs better or similar to state-of-the-art methods."
651,SP:37adabdc6615c5199a481553c8ccc06d57363614,"We study the role of the representation of state-action value functions in regret minimization in finite-horizon Markov Decision Processes (MDPs) with linear structure. We first derive a necessary condition on the representation, called universally spanning optimal features (UNISOFT), to achieve constant regret in any MDP with linear reward function. This result encompasses the well-known settings of low-rank MDPs and, more generally, zero inherent Bellman error (also known as the Bellman closure assumption). We then demonstrate that this condition is also sufficient for these classes of problems by deriving a constant regret bound for two optimistic algorithms (LSVI-UCB and ELEANOR). Finally, we propose an algorithm for representation selection and we prove that it achieves constant regret when one of the given representations, or a suitable combination of them, satisfies the UNISOFT condition.","This paper studies the representation of state-action value functions in regret minimization in finite-horizon MDPs with linear structure. The authors derive a necessary condition on the representation, called universally spanning optimal features (UNISOFT), to achieve constant regret in any MDP with linear reward function. They then demonstrate that this condition is sufficient for these classes of problems by deriving a constant regret bound for two optimistic algorithms (LSVI-UCB and ELEANOR). Finally, they propose an algorithm for representation selection and prove that it achieves constant regret when one of the given representations or a suitable combination of them satisfies the UNISOFT condition."
652,SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"The incorporation of appropriate inductive bias plays a critical role in learning dynamics from data. A growing body of work has been exploring ways to enforce energy conservation in the learned dynamics by encoding Lagrangian or Hamiltonian dynamics into the neural network architecture. These existing approaches are based on differential equations, which do not allow discontinuity in the states and thereby limit the class of systems one can learn. However, in reality, most physical systems, such as legged robots and robotic manipulators, involve contacts and collisions, which introduce discontinuities in the states. In this paper, we introduce a differentiable contact model, which can capture contact mechanics: frictionless/frictional, as well as elastic/inelastic. This model can also accommodate inequality constraints, such as limits on the joint angles. The proposed contact model extends the scope of Lagrangian and Hamiltonian neural networks by allowing simultaneous learning of contact and system properties. We demonstrate this framework on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction. The learned dynamics can be used as a differentiable physics simulator for downstream gradient-based optimization tasks, such as planning and control. 1 2","This paper proposes a differentiable contact model for learning dynamics in physical systems. The proposed model is based on a Lagrangian and Hamiltonian neural network architecture, which allows simultaneous learning of frictionless/frictional and elastic/inelastic properties of the system. This model can also accommodate inequality constraints, such as limits on the joint angles. Experiments are conducted on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction. The learned dynamics can be used as a simulator for downstream gradient-based optimization tasks."
653,SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"This work explores the Benevolent Training Hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. Our analysis provides evidence for BTH by relating the NN’s Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure. We first observe that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. We then show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point. Finally, we find that steady training with Dropout implies a trainingand datadependent generalization bound that grows poly-logarithmically with the number of parameters. Overall, our results support the intuition that good training behavior can be a useful bias towards good generalization.","This paper investigates the Benevolent training hypothesis (BTH) which claims that the complexity of a deep neural network can be deduced by its training dynamics. The authors show that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. They also show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point. Finally, they show that steady training with Dropout implies a training and datadependent generalization bound that grows poly-logarithmically with the number of parameters."
654,SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"A Forster transform is an operation that turns a distribution into one with good anticoncentration properties. While a Forster transform does not always exist, we show that any distribution can be efficiently decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. As the main application of this result, we obtain the first polynomial-time algorithm for distribution-independent PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity, i.e., independent of the bit complexity of the examples. Previous algorithms for this learning problem incurred sample complexity scaling polynomially with the bit complexity, even though such a dependence is not information-theoretically necessary.","This paper studies the problem of distribution-independent PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity, i.e., independent of the bit complexity of the examples. In particular, the authors show that any distribution can be decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. The main application of this result is the first polynomially-time algorithm for distribution-dependent PAC learning in the half-spaces learning of Massart Noise Model."
655,SP:e5229305af00067ae2dbabd903e585964aec8928,"Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models. An open-source implementation is available at https://github.com/xingchenwan/grabnel.","This paper proposes a Bayesian optimisation-based attack method for adversarial attacks on graph neural networks (GNNs). The proposed method is based on Bayesian Optimization (BOO) and is a query-efficient and parsimonious attack method. The authors empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. An open-source implementation is available at GitHub."
656,SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"Machine learning models often encounter distribution shifts when deployed in the real world. In this paper, we focus on adaptation to label distribution shift in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. Leveraging a novel analysis, we show that the lack of true label does not hinder estimation of the expected test loss, which enables the reduction of online label shift adaptation to conventional online learning. Informed by this observation, we propose adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. We empirically verify our findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of challenging label shift scenarios.","This paper studies the problem of online label shift adaptation in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. The authors propose adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. They empirically verify their findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of label shift scenarios."
657,SP:806515ae07fb1c9d02773592005d53d4158ef102,"We consider the detection and localization of gradual changes in the distribution of a sequence of time-ordered observations. Existing literature focuses mostly on the simpler abrupt setting which assumes a discontinuity jump in distribution, and is unrealistic for some applied settings. We propose a general method for detecting and localizing gradual changes that does not require a specific data generating model, a particular data type, or prior knowledge about which features of the distribution are subject to change. Despite relaxed assumptions, the proposed method possesses proven theoretical guarantees for both detection and localization.","This paper considers the problem of detecting and localization of gradual changes in the distribution of a sequence of time-ordered observations. The authors propose a general method for detecting and localizing gradual changes that does not require a specific data generating model, a particular data type, or prior knowledge about which features of the distribution are subject to change. Despite relaxed assumptions, the proposed method possesses proven theoretical guarantees for both detection and localization."
658,SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"The brain effortlessly solves blind source separation (BSS) problems, but the algorithm it uses remains elusive. In signal processing, linear BSS problems are often solved by Independent Component Analysis (ICA). To serve as a model of a biological circuit, the ICA neural network (NN) must satisfy at least the following requirements: 1. The algorithm must operate in the online setting where data samples are streamed one at a time, and the NN computes the sources on the fly without storing any significant fraction of the data in memory. 2. The synaptic weight update is local, i.e., it depends only on the biophysical variables present in the vicinity of a synapse. Here, we propose a novel objective function for ICA from which we derive a biologically plausible NN, including both the neural architecture and the synaptic learning rules. Interestingly, our algorithm relies on modulating synaptic plasticity by the total activity of the output neurons. In the brain, this could be accomplished by neuromodulators, extracellular calcium, local field potential, or nitric oxide.",This paper proposes a neural network architecture for blind source separation (BSS) that is based on the neural architecture of a biological neural network (NN). The authors propose a novel objective function for ICA based on neural architecture and the synaptic learning rules. The authors show that the proposed algorithm is biologically plausible and can be used to solve BSS problems in the online setting. 
659,SP:22f8b517a3df65144412938f5891c463d7bae0ab,"In recent years Recurrent Neural Networks (RNNs) were successfully used to model the way neural activity drives task-related behavior in animals, operating under the implicit assumption that the obtained solutions are universal. Observations in both neuroscience and machine learning challenge this assumption. Animals can approach a given task with a variety of strategies, and training machine learning algorithms introduces the phenomenon of underspecification. These observations imply that every task is associated with a space of solutions. To date, the structure of this space is not understood, limiting the approach of comparing RNNs with neural data. Here, we characterize the space of solutions associated with various tasks. We first study a simple two-neuron network on a task that leads to multiple solutions. We trace the nature of the final solution back to the network’s initial connectivity and identify discrete dynamical regimes that underlie this diversity. We then examine three neuroscience-inspired tasks: Delayed discrimination, Interval discrimination, and Time reproduction. For each task, we find a rich set of solutions. One layer of variability can be found directly in the neural activity of the networks. An additional layer is uncovered by testing the trained networks’ ability to extrapolate, as a perturbation to a system often reveals hidden structure. Furthermore, we relate extrapolation patterns to specific dynamical objects and effective algorithms found by the networks. We introduce a tool to derive the reduced dynamics of networks by generating a compact directed graph describing the essence of the dynamics with regards to behavioral inputs and outputs. Using this representation, we can partition the solutions to each task into a handful of types and show that neural features can partially predict them. Taken together, our results shed light on the concept of the space of solutions and its uses both in Machine learning and in Neuroscience.","This paper studies the dynamics of a two-neuron network on three tasks: delayed discrimination, interval discrimination, and time reproduction. The authors analyze the behavior of the network on these tasks and show that the network has a diverse set of solutions. They also show that one layer of variability can be found directly in the neural activity of the networks. An additional layer is uncovered by testing the trained networks’ ability to extrapolate, as a perturbation to a system often reveals hidden structure. Furthermore, extrapolation patterns to specific dynamical objects and effective algorithms found by the networks are shown."
660,SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"Modeling distributions of covariates, or density estimation, is a core challenge in unsupervised learning. However, the majority of work only considers the joint distribution, which has limited utility in practical situations. A more general and useful problem is arbitrary conditional density estimation, which aims to model any possible conditional distribution over a set of covariates, reflecting the more realistic setting of inference based on prior knowledge. We propose a novel method, Arbitrary Conditioning with Energy (ACE), that can simultaneously estimate the distribution p(xu | xo) for all possible subsets of unobserved features xu and observed features xo. ACE is designed to avoid unnecessary bias and complexity — we specify densities with a highly expressive energy function and reduce the problem to only learning one-dimensional conditionals (from which more complex distributions can be recovered during inference). This results in an approach that is both simpler and higher-performing than prior methods. We show that ACE achieves state-of-the-art for arbitrary conditional likelihood estimation and data imputation on standard benchmarks.",This paper proposes a new method for conditional density estimation of unobserved features and observed features. The proposed method is based on the idea of conditioning with energy (ACE) which can simultaneously estimate the distribution p(xu | xo) for all possible subsets of the unobserved feature xu and the observed feature xo. The authors propose to learn a one-dimensional conditionals for the conditional density estimator and reduce the problem to only learning one dimensional conditionals (from which more complex distributions can be recovered during inference). The authors show that ACE achieves state-of-the-art for arbitrary conditional likelihood estimation and data imputation.
661,SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"In low-level vision such as single image super-resolution (SISR), traditional MSE or L1 loss function treats every pixel equally with the assumption that the importance of all pixels is the same. However, it has been long recognized that texture and edge areas carry more important visual information than smooth areas in photographic images. How to achieve such spatial adaptation in a principled manner has been an open problem in both traditional model-based and modern learning-based approaches toward SISR. In this paper, we propose a new adaptive weighted loss for SISR to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty. Specifically, we introduce variance estimation characterizing the uncertainty on a pixel-by-pixel basis into SISR solutions so the targeted pixels in a high-resolution image (mean) and their corresponding uncertainty (variance) can be learned simultaneously. Moreover, uncertainty estimation allows us to leverage conventional wisdom such as sparsity prior for regularizing SISR solutions. Ultimately, pixels with large certainty (e.g., texture and edge pixels) will be prioritized for SISR according to their importance to visual quality. For the first time, we demonstrate that such uncertainty-driven loss can achieve better results than MSE or L1 loss for a wide range of network architectures. Experimental results on three popular SISR networks show that our proposed uncertainty-driven loss has achieved better PSNR performance than traditional loss functions without any increased computation during testing. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/UDL-SR.htm","This paper proposes an adaptive weighted loss for single image super-resolution (SISR) to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty. Specifically, they introduce variance estimation characterizing the uncertainty on a pixel-by-pixel basis into SISR solutions so the targeted pixels in a high-resolution image (mean) and their corresponding uncertainty (variance) can be learned simultaneously. The uncertainty estimation allows them to leverage conventional wisdom such as sparsity prior for regularizing SisR solutions. The experimental results on three popular SIsR networks show that the proposed uncertainty-driven loss has achieved better PSNR performance than traditional loss functions without any increased computation during testing."
662,SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"We propose the first general PAC-Bayesian generalization bounds for adversarial robustness, that estimate, at test time, how much a model will be invariant to imperceptible perturbations in the input. Instead of deriving a worst-case analysis of the risk of a hypothesis over all the possible perturbations, we leverage the PACBayesian framework to bound the averaged risk on the perturbations for majority votes (over the whole class of hypotheses). Our theoretically founded analysis has the advantage to provide general bounds (i) that are valid for any kind of attacks (i.e., the adversarial attacks), (ii) that are tight thanks to the PAC-Bayesian framework, (iii) that can be directly minimized during the learning phase to obtain a robust model on different attacks at test time.","This paper proposes a general PAC-Bayesian generalization bound for adversarial robustness, which is based on the PACBayesian framework. The main contribution of this paper is to derive a generalization of the worst-case risk of a hypothesis over all possible perturbations for majority votes (over the whole class of hypotheses), which is then used to derive an upper bound on the average risk of the perturbation for the majority of the hypotheses. The upper bound is proved to be tight and can be directly minimized during the learning phase to obtain a robust model on different attacks."
663,SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that can provide efficient querying mechanism over large and incomplete databases. Current approaches employ spatial geometries such as boxes to learn query representations that encompass the answer entities and model the logical operations of projection and intersection. However, their geometry is restrictive and leads to non-smooth strict boundaries, which further results in ambiguous answer entities. Furthermore, previous works propose transformation tricks to handle unions which results in non-closure and, thus, cannot be chained in a stream. In this paper, we propose a Probabilistic Entity Representation Model (PERM) to encode entities as a Multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. Additionally, we also define the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. On the logical query reasoning problem, we demonstrate that the proposed PERM significantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics. We also evaluate PERM’s competence on a COVID-19 drugrepurposing case study and show that our proposed work is able to recommend drugs with substantially better F1 than current methods. Finally, we demonstrate the working of our PERM’s query answering process through a low-dimensional visualization of the Gaussian representations.","This paper proposes a probabilistic entity representation model (PERM) for logical reasoning over Knowledge Graphs (KG). The proposed model encodes entities as a multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. The authors also define the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. The proposed PERM is evaluated on the logical query reasoning problem on various public benchmark KG datasets on standard evaluation metrics, and evaluated on a COVID-19 drug repurposing case study."
664,SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"Gradient-based hyperparameter optimization has earned a widespread popularity in the context of few-shot meta-learning, but remains broadly impractical for tasks with long horizons (many gradient steps), due to memory scaling and gradient degradation issues. A common workaround is to learn hyperparameters online, but this introduces greediness which comes with a significant performance drop. We propose forward-mode differentiation with sharing (FDS), a simple and efficient algorithm which tackles memory scaling issues with forward-mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. We provide theoretical guarantees about the noise reduction properties of our algorithm, and demonstrate its efficiency empirically by differentiating through ∼ 10 gradient steps of unrolled optimization. We consider large hyperparameter search ranges on CIFAR-10 where we significantly outperform greedy gradientbased alternatives, while achieving ×20 speedups compared to the state-of-the-art black-box methods. Code is available at: https://github.com/polo5/FDS","This paper proposes a new method for gradient-based hyperparameter optimization for few-shot meta-learning. The proposed method is based on forward-mode differentiation with sharing (FDS), which is a simple and efficient algorithm that tackles memory scaling issues and gradient degradation issues by sharing hyperparameters that are contiguous in time. The authors provide theoretical guarantees about the noise reduction properties of their algorithm, and demonstrate its efficiency empirically by differentiating through 10 gradient steps of unrolled optimization. The experimental results on CIFAR-10 show that the proposed method significantly outperforms greedy gradientbased alternatives."
665,SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"Human reasoning can be understood as an interplay between two systems: the intuitive and associative (“System 1”) and the deliberative and logical (“System 2”). Neural sequence models—which have been increasingly successful at performing complex, structured tasks—exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.","This paper proposes a method for improving neural sequence models by adding logical reasoning to them. In particular, the authors propose to use a symbolic reasoning module that can either accept or reject the generations of a neural sequence model. The authors also propose to train a neural inference module to mediate between the neural system 1 and the logical system 2. The experiments show that the proposed method can improve the coherence and accuracy of neurally-based generations. "
666,SP:d77d046095e4c8336c0c76ac48cb046923230753,"We consider off-policy evaluation (OPE) in continuous treatment settings, such as personalized dose-finding. In OPE, one aims to estimate the mean outcome under a new treatment decision rule using historical data generated by a different decision rule. Most existing works on OPE focus on discrete treatment settings. To handle continuous treatments, we develop a novel estimation method for OPE using deep jump learning. The key ingredient of our method lies in adaptively discretizing the treatment space using deep discretization, by leveraging deep learning and multiscale change point detection. This allows us to apply existing OPE methods in discrete treatments to handle continuous treatments. Our method is further justified by theoretical results, simulations, and a real application to Warfarin Dosing.","This paper considers the problem of off-policy evaluation (OPE) in continuous treatment settings, where one aims to estimate the mean outcome under a new treatment decision rule using historical data generated by a different decision rule. The authors propose a novel estimation method for OPE using deep jump learning. The key ingredient of their method lies in adaptively discretizing the treatment space using deep discretization, by leveraging deep learning and multiscale change point detection. The method is further justified by theoretical results, simulations, and a real application to Warfarin Dosing."
667,SP:4d085e57286fdd36143108a002d16914222c239a,"Switching dynamical systems provide a powerful, interpretable modeling framework for inference in time-series data in, e.g., the natural sciences or engineering applications. Since many areas, such as biology or discrete-event systems, are naturally described in continuous time, we present a model based on a Markov jump process modulating a subordinated diffusion process. We provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are however computationally intractable. Therefore, we develop a new continuous-time variational inference algorithm, combining a Gaussian process approximation on the diffusion level with posterior inference for Markov jump processes. By minimizing the path-wise Kullback-Leibler divergence we obtain (i) Bayesian latent state estimates for arbitrary points on the real axis and (ii) point estimates of unknown system parameters, utilizing variational expectation maximization. We extensively evaluate our algorithm under the model assumption and for real-world examples.","This paper proposes a variational inference algorithm for continuous-time dynamical systems based on a Markov jump process modulating a subordinated diffusion process. The authors provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are however computationally intractable. Therefore, the authors develop a new continuous time inference algorithm, combining a Gaussian process approximation on the diffusion level with posterior inference for Markov jumping processes. By minimizing the path-wise Kullback-Leibler divergence, they obtain Bayesian latent state estimates for arbitrary points on the real axis and point estimates of unknown system parameters, utilizing variational expectation maximization."
668,SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"We consider a nonlinear inverse problem y = f(Ax), where observations y ∈ R are the componentwise nonlinear transformation of Ax ∈ R, x ∈ R is the signal of interest and A is a known linear mapping. By properly specifying the nonlinear processing function, this model can be particularized to many signal processing problems, including compressed sensing and phase retrieval. Our main goal in this paper is to understand the impact of sensing matrices, or more specifically the spectrum of sensing matrices, on the difficulty of recovering x from y. Towards this goal, we study the performance of one of the most successful recovery methods, i.e. the expectation propagation algorithm (EP). We define a notion for the spikiness of the spectrum of A and show the importance of this measure in the performance of the EP. Whether the spikiness of the spectrum can hurt or help the recovery performance of EP depends on f. We define certain quantities based on the function f that enables us to describe the impact of the spikiness of the spectrum on EP recovery. Based on our framework, we are able to show that for instance, in phase-retrieval problems, matrices with spikier spectrums are better for EP, while in 1-bit compressed sensing problems, less spiky (flatter) spectrums offer better recoveries. Our results unify and substantially generalize the existing results that compare sub-Gaussian and orthogonal matrices, and provide a platform toward designing optimal sensing systems.","This paper studies the impact of the spectrum of sensing matrices on the difficulty of recovering x from y in a nonlinear inverse problem. In particular, the authors study the performance of expectation propagation algorithm (EP) and show that the spikiness of spectrum of A is an important factor in EP's performance. The authors define certain quantities based on the function f that enables them to describe the impact on the EP recovery. Based on their framework, they show that for phase-retrieval problems, matrices with spikier spectrums are better for EP, while in 1-bit compressed sensing problems, less spiky (flatter) spectrums offer better recoveries."
669,SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"Generalized Zero-Shot Learning (GZSL) aims to recognize new categories with auxiliary semantic information, e.g., category attributes. In this paper, we handle the critical issue of domain shift problem, i.e., confusion between seen and unseen categories, by progressively improving cross-domain transferability and category discriminability of visual representations. Our approach, named Dual Progressive Prototype Network (DPPN), constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively. With attribute prototypes, DPPN alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. This enables DPPN to produce visual representations with accurate attribute localization ability, which benefts the semantic-visual alignment and representation transferability. Besides, along with progressive attribute localization, DPPN further projects category prototypes into multiple spaces to progressively repel visual representations from different categories, which boosts category discriminability. Both attribute and category prototypes are collaboratively learned in a unifed framework, which makes visual representations of DPPN transferable and distinctive. Experiments on four benchmarks prove that DPPN effectively alleviates the domain shift problem in GZSL.","This paper proposes a novel method to address the problem of domain shift in zero-shot learning, i.e., confusion between seen and unseen categories, by progressively improving cross-domain transferability and category discriminability of visual representations. The proposed method is named Dual Progressive Prototype Network (DPPN), which constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively. With attribute prototypes, DPPN alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. With category prototypes, the proposed method further projects category prototypes into multiple spaces to progressively repel visual representations from different categories. Experiments on four benchmarks demonstrate the effectiveness of the proposed approach."
670,SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"Defocus blur is one kind of blur effects often seen in images, which is challenging to remove due to its spatially variant amount. This paper presents an end-to-end deep learning approach for removing defocus blur from a single image, so as to have an all-in-focus image for consequent vision tasks. First, a pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing spatially variant defocus blur kernels in an efficient linear parametric form, with higher accuracy than existing models. Then, a deep neural network called GKMNet is developed by unrolling a fixed-point iteration of the GKM-based deblurring. The GKMNet is built on a lightweight scale-recurrent architecture, with a scale-recurrent attention module for estimating the mixing coefficients in GKM for defocus deblurring. Extensive experiments show that the GKMNet not only noticeably outperforms existing defocus deblurring methods, but also has its advantages in terms of model complexity and computational efficiency.","This paper presents an end-to-end deep learning approach for removing defocus blur from a single image, so as to have an all-in-focus image for consequent vision tasks. First, a pixel-wise Gaussian kernel mixture (GKM) model is proposed to represent spatially variant defocus kernels in an efficient linear parametric form, with higher accuracy than existing models. Then, a deep neural network is developed by unrolling a fixed-point iteration of the GKM-based deblurring. Extensive experiments show that the proposed method can outperform existing defocus debLurring methods, but also has its advantages in terms of model complexity and computational efficiency."
671,SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This work concerns self-supervised video representation learning (SSVRL), one topic that has received much attention recently. Since videos are storage-intensive and contain a rich source of visual content, models designed for SSVRL are expected to be storageand computation-efficient, as well as effective. However, most existing methods only focus on one of the two objectives, failing to consider both at the same time. In this work, for the first time, the seemingly contradictory goals are simultaneously achieved by exploiting compressed videos and capturing mutual information between two input streams. Specifically, a novel Motion Vector based Cross Guidance Contrastive learning approach (MVCGC) is proposed. For storage and computation efficiency, we choose to directly decode RGB frames and motion vectors (that resemble low-resolution optical flows) from compressed videos on-the-fly. To enhance the representation ability of the motion vectors, hence the effectiveness of our method, we design a cross guidance contrastive learning algorithm based on multi-instance InfoNCE loss, where motion vectors can take supervision signals from RGB frames and vice versa. Comprehensive experiments on two downstream tasks show that our MVCGC yields new state-of-the-art while being significantly more efficient than its competitors.","This paper proposes a cross guidance contrastive learning algorithm for self-supervised video representation learning (SSVRL). The proposed method is based on the idea of cross-guidance contrastive loss (CGC), which is a multi-instance InfoNCE loss that takes into account both the RGB and the motion vectors of the input video. The method is evaluated on a variety of downstream tasks and achieves state-of-the-art performance. "
672,SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"A Bayesian treatment can mitigate overconfidence in ReLU nets around the training data. But far away from them, ReLU Bayesian neural networks (BNNs) can still underestimate uncertainty and thus be asymptotically overconfident. This issue arises since the output variance of a BNN with finitely many features is quadratic in the distance from the data region. Meanwhile, Bayesian linear models with ReLU features converge, in the infinite-width limit, to a particular Gaussian process (GP) with a variance that grows cubically so that no asymptotic overconfidence can occur. While this may seem of mostly theoretical interest, in this work, we show that it can be used in practice to the benefit of BNNs. We extend finite ReLU BNNs with infinite ReLU features via the GP and show that the resulting model is asymptotically maximally uncertain far away from the data while the BNNs’ predictive power is unaffected near the data. Although the resulting model approximates a full GP posterior, thanks to its structure, it can be applied post-hoc to any pre-trained ReLU BNN at a low cost.","This paper studies the problem of asymptotic overconfidence in Bayesian neural networks (BNNs) in the presence of infinite ReLU features. The authors show that the output variance of a BNN with finitely many features is quadratic in the distance from the data region. Meanwhile, Bayesian linear models with infinite-width features converge to a particular Gaussian process (GP) with a variance that grows cubically so that no overconfidence can occur. In this paper, the authors extend finite ReLU BNNs with infinite ReLu features via the GP and show the resulting model is asymPTotically maximally uncertain far away from the training data while the BNN’s predictive power is unaffected near the data."
673,SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper considers the problem of selecting a formula for identifying a causal quantity of interest among a set of available formulas. We assume an sequential setting in which the investigator may alter the data collection mechanism in a data-dependent way with the aim of identifying the formula with lowest asymptotic variance in as few samples as possible. We formalize this setting by using the bestarm-identification bandit framework where the standard goal of learning the arm with the lowest loss is replaced with the goal of learning the arm that will produce the best estimate. We introduce new tools for constructing finite-sample confidence bounds on estimates of the asymptotic variance that account for the estimation of potentially complex nuisance functions, and adapt the best-arm-identification algorithms of LUCB and Successive Elimination to use these bounds. We validate our method by providing upper bounds on the sample complexity and an empirical study on artificially generated data.",This paper considers the problem of selecting a formula for identifying a causal quantity of interest among a set of available formulas. The main contribution of this paper is to formalize the bestarm-identification bandit framework where the standard goal of learning the arm with the lowest loss is replaced with the goal to learn the arm that will produce the best estimate. The authors introduce new tools for constructing finite-sample confidence bounds on estimates of the asymptotic variance that account for the estimation of potentially complex nuisance functions. They validate their method by providing upper bounds on the sample complexity and an empirical study on artificially generated data.
674,SP:471361588bfc6c6033631509d1e43e77fd9721ce,"Communication cost is one major bottleneck for the scalability for distributed learning. One approach to reduce the communication cost is to compress the gradient during communication. However, directly compressing the gradient decelerates the convergence speed, and the resulting algorithm may diverge for biased compression. Recent work addressed this problem for stochastic gradient descent by adding back the compression error from the previous step. This idea was further extended to one class of variance reduced algorithms, where the variance of the stochastic gradient is reduced by taking a moving average over all history gradients. However, our analysis shows that just adding the previous step’s compression error, as done in existing work, does not fully compensate the compression error. So, we propose ErrorCompensatedX, which uses the compression error from the previous two steps. We show that ErrorCompensatedX can achieve the same asymptotic convergence rate with the training without compression. Moreover, we provide a unified theoretical analysis framework for this class of variance reduced algorithms, with or without error compensation.","This paper proposes ErrorCompensatedX, which uses the compression error from the previous two steps of the stochastic gradient descent (SGD) algorithm to improve the convergence rate of SGD. The authors show that adding the previous step’s compression error, as done in existing work, does not fully compensate for the bias in the previous steps. Instead, they propose to use the error compensation from the first two steps, and show that it can achieve the same asymptotic convergence rate with the training without compression. "
675,SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"When a graph neural network (GNN) made a prediction, one raises question about explainability: “Which fraction of the input graph is most influential to the model’s decision?” Producing an answer requires understanding the model’s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the flexibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and fine-tuning idea to develop our explainer and generate multi-grained explanations. Specifically, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the fine-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classification over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.","This paper proposes a method for generating multi-grained explanations for graph neural networks (GNNs). The authors propose a pre-training and fine-tuning approach to generate multi-class explanations for GNNs. The authors use contrastive contrastive learning to learn the class-wise and class-specific explanations for different classes, and then fine-tune the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of the proposed method. "
676,SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they are not counterfactual because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations are also counterfactual because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.",This paper proposes a method to generate counterfactual explanations for Graph Neural Networks (GNNs) by explicitly modelling the common decision logic of GNNs on similar input graphs. The authors argue that their method is robust to noise because it is based on the decision boundaries of a GNN that govern the predictions of many similar input graph. They also argue that removing the set of edges identified by an explanation from the input graph changes the prediction significantly. The experimental results on several public datasets demonstrate the superior performance of their method.
677,SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"Although recent advances in voice conversion have shown significant improvement, there still remains a gap between the converted voice and target voice. A key factor that maintains this gap is the insufficient decomposition of content and voice style from the source speech. This insufficiency leads to the converted speech containing source speech style or losing source speech content. In this paper, we present VoiceMixer which can effectively decompose and transfer voice style through a novel information bottleneck and adversarial feedback. With self-supervised representation learning, the proposed information bottleneck can decompose the content and style with only a small loss of content information. Also, for adversarial feedback of each information, the discriminator is decomposed into content and style discriminator with self-supervision, which enable our model to achieve better generalization to the voice style of the converted speech. The experimental results show the superiority of our model in disentanglement and transfer performance, and improve audio quality by preserving content information.","This paper proposes a method for voice-to-voice transfer based on self-supervised representation learning and adversarial feedback. In particular, the authors propose to decompose the content and style of the converted speech into two parts: (1) a content discriminator and (2) a style discriminator, which is trained with self- supervision. The proposed method is evaluated on a variety of datasets and compared with a few baselines. The authors show that the proposed method outperforms the baselines in terms of disentanglement and transfer performance."
678,SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"3D object tracking in point clouds is still a challenging problem due to the sparsity of LiDAR points in dynamic environments. In this work, we propose a Siamese voxel-to-BEV tracker, which can significantly improve the tracking performance in sparse 3D point clouds. Specifically, it consists of a Siamese shape-aware feature learning network and a voxel-to-BEV target localization network. The Siamese shape-aware feature learning network can capture 3D shape information of the object to learn the discriminative features of the object so that the potential target from the background in sparse point clouds can be identified. To this end, we first perform template feature embedding to embed the template’s feature into the potential target and then generate a dense 3D shape to characterize the shape information of the potential target. For localizing the tracked target, the voxel-toBEV target localization network regresses the target’s 2D center and the z-axis center from the dense bird’s eye view (BEV) feature map in an anchor-free manner. Concretely, we compress the voxelized point cloud along z-axis through max pooling to obtain a dense BEV feature map, where the regression of the 2D center and the z-axis center can be performed more effectively. Extensive evaluation on the KITTI and nuScenes datasets shows that our method significantly outperforms the current state-of-the-art methods by a large margin. Code is available at https: //github.com/fpthink/V2B.","This paper proposes a Siamese voxel-to-BEV tracker for 3D object tracking in sparse 3D point clouds. The proposed method consists of two components: (1) a shape-aware feature learning network, and (2) a target localization network. The first component is a template feature embedding to embed the template’s feature into the potential target and then generate a dense 3D shape to characterize the shape information of the target. The second component is an anchor-free network to localize the 2D and the z-axis center from the dense bird's eye view (BEV) feature map. Experiments on KITTI and nuScenes show that the proposed method significantly outperforms the state-of-the-art methods."
679,SP:8b788c78680a54c453a04f4551436763ee57585e,"Attentional mechanisms are order-invariant. Positional encoding is a crucial component to allow attention-based deep model architectures such as Transformer to address sequences or images where the position of information matters. In this paper, we propose a novel positional encoding method based on learnable Fourier features. Instead of hard-coding each position as a token or a vector, we represent each position, which can be multi-dimensional, as a trainable encoding based on learnable Fourier feature mapping, modulated with a multi-layer perceptron. The representation is particularly advantageous for a spatial multi-dimensional position, e.g., pixel positions on an image, where L2 distances or more complex positional relationships need to be captured. Our experiments based on several public benchmark tasks show that our learnable Fourier feature representation for multi-dimensional positional encoding outperforms existing methods by both improving the accuracy and allowing faster convergence.","This paper proposes a positional encoding method based on learnable Fourier features for multi-dimensional positional encoding. The proposed method is based on a multi-layer perceptron, which is modulated with a learnable feature mapping. The method is evaluated on a variety of image classification tasks, and it is shown to outperform the state-of-the-art methods."
680,SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"We consider the problem of learning the causal MAG of a system from observational data in the presence of latent variables and selection bias. Constraint-based methods are one of the main approaches for solving this problem, but the existing methods are either computationally impractical when dealing with large graphs or lacking completeness guarantees. We propose a novel computationally efficient recursive constraint-based method that is sound and complete. The key idea of our approach is that at each iteration a specific type of variable is identified and removed. This allows us to learn the structure efficiently and recursively, as this technique reduces both the number of required conditional independence (CI) tests and the size of the conditioning sets. The former substantially reduces the computational complexity, while the latter results in more reliable CI tests. We provide an upper bound on the number of required CI tests in the worst case. To the best of our knowledge, this is the tightest bound in the literature. We further provide a lower bound on the number of CI tests required by any constraint-based method. The upper bound of our proposed approach and the lower bound at most differ by a factor equal to the number of variables in the worst case. We provide experimental results to compare the proposed approach with the state of the art on both synthetic and real-world structures.","This paper proposes a new method for learning the causal MAG of a system from observational data in the presence of latent variables and selection bias. The key idea of the proposed method is to learn the structure efficiently and recursively, as this technique reduces both the number of required conditional independence (CI) tests and the size of the conditioning sets. The authors provide an upper bound on the required CI tests in the worst case, which is the tightest bound in the literature. The lower bound is also provided. The experimental results on synthetic and real-world data show that the proposed approach is competitive with the state of the art."
681,SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,"How can we make use of information parallelism in online decision making problems while efficiently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision making problems, namely, stochastic multi-arm bandit and linear contextual bandit with finitely many arms. Over a time horizon T, our batch Thompson Sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out onlyO(log T ) batch queries. To achieve this exponential reduction, i.e., reducing the number of interactions from T to O(log T ), our batch policy dynamically determines the duration of each batch in order to balance the exploration-exploitation trade-off. We also demonstrate experimentally that dynamic batch allocation dramatically outperforms natural baselines such as static batch allocations.","This paper proposes a batch Thompson Sampling (BTS) algorithm for stochastic multi-arm bandit and linear contextual bandit with finitely many arms. The algorithm is based on Thompson sampling, which is a variant of Thompson sampling. The main idea is to dynamically decide the duration of each batch in order to balance the exploration-exploitation trade-off. The authors show that the proposed algorithm achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only O(log T) batch queries. They also demonstrate experimentally that dynamic batch allocation dramatically outperforms natural baselines."
682,SP:653a519e3c799c25e0d0b4240322642040b121a3,"Domain adaptation (DA) benefits from the rigorous theoretical works that study its insightful characteristics and various aspects, e.g., learning domain-invariant representations and its trade-off. However, it seems not the case for the multiple source DA and domain generalization (DG) settings which are remarkably more complicated and sophisticated due to the involvement of multiple source domains and potential unavailability of target domain during training. In this paper, we develop novel upper-bounds for the target general loss which appeal to us to define two kinds of domain-invariant representations. We further study the pros and cons as well as the trade-offs of enforcing learning each domain-invariant representation. Finally, we conduct experiments to inspect the trade-off of these representations for offering practical hints regarding how to use them in practice and explore other interesting properties of our developed theory.","This paper studies the problem of domain adaptation (DA) and domain generalization (DG) in the multi-source and multi-domain settings. The authors propose a novel upper-bounds for the target generalization loss and define two kinds of domain-invariant representations. They further study the pros and cons as well as the trade-offs of enforcing learning each domain-specific representation. Finally, they conduct experiments to inspect the tradeoff of these representations for offering practical hints regarding how to use them in practice."
683,SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"Lightweight image super-resolution (SR) networks have obtained promising results with moderate model size. Many SR methods have focused on designing lightweight architectures, which neglect to further reduce the redundancy of network parameters. On the other hand, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable memory and computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly, because filter pruning for residual blocks is well-known tricky. To address the above issues, we propose aligned structured sparsity learning (ASSL), which introduces a weight normalization layer and applies L2 regularization to the scale parameters for sparsity. To align the pruned filter locations across different layers, we propose a sparsity structure alignment penalty term, which minimizes the norm of soft mask gram matrix. We apply aligned structured sparsity learning strategy to train efficient image SR network, named as ASSLN, with smaller model size and lower computation than state-of-the-art methods. We conduct extensive comparisons with lightweight SR networks. Our ASSLN achieves superior performance gains over recent methods quantitatively and visually.","This paper proposes an aligned structured sparsity learning (ASSL) method for lightweight image super-resolution (SR) networks. The authors propose a weight normalization layer and L2 regularization to the scale parameters for sparsity. To align the pruned filter locations across different layers, they propose a sparsity structure alignment penalty term, which minimizes the norm of soft mask gram matrix. The proposed method ASSLN is applied to train efficient image SR network with smaller model size and lower computation than state of the art methods."
684,SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"Efficient exploration in deep cooperative multi-agent reinforcement learning (MARL) still remains challenging in complex coordination problems. In this paper, we introduce a novel Episodic Multi-agent reinforcement learning with Curiosity-driven exploration, called EMC. We leverage an insight of popular factorized MARL algorithms that the “induced"" individual Q-values, i.e., the individual utility functions used for local execution, are the embeddings of local actionobservation histories, and can capture the interaction between agents due to reward backpropagation during centralized training. Therefore, we use prediction errors of individual Q-values as intrinsic rewards for coordinated exploration and utilize episodic memory to exploit explored informative experience to boost policy training. As the dynamics of an agent’s individual Q-value function captures the novelty of states and the influence from other agents, our intrinsic reward can induce coordinated exploration to new or promising states. We illustrate the advantages of our method by didactic examples, and demonstrate its significant outperformance over state-of-the-art MARL baselines on challenging tasks in the StarCraft II micromanagement benchmark.","This paper proposes a novel method for exploration in multi-agent reinforcement learning (MARL) called Episodic Multi-Agent Exploration (EMC). The main idea is to use the individual Q-values of individual agents as intrinsic rewards for coordinated exploration. The intrinsic reward is based on the observation that the dynamics of an agent’s individual utility function captures the novelty of states and the influence from other agents, and thus can induce coordinated exploration to new or promising states. Empirical results show that the proposed method outperforms the state-of-the-art MARL baselines on the StarCraft II micromanagement benchmark."
685,SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"We study the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. Specifically, we are given a set T of labeled examples (x, y) ∈ R × R and a parameter 0 < α < 1/2 such that an α-fraction of the points in T are i.i.d. samples from a linear regression model with Gaussian covariates, and the remaining (1 − α)-fraction of the points are drawn from an arbitrary noise distribution. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. Our main result is a Statistical Query (SQ) lower bound of d for this problem. Our SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible.","This paper studies the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. The main result is a Statistical Query (SQ) lower bound of d for this problem. The SQ lower bound qualitatively matches the performance of previous algorithms, providing evidence that current upper bounds for this task are nearly best possible."
686,SP:7b258252a9063514348f5fa8d9c85afd85748747,"Modeling a system’s temporal behaviour in reaction to external stimuli is a fundamental problem in many areas. Pure Machine Learning (ML) approaches often fail in the small sample regime and cannot provide actionable insights beyond predictions. A promising modification has been to incorporate expert domain knowledge into ML models. The application we consider is predicting the patient health status and disease progression over time, where a wealth of domain knowledge is available from pharmacology. Pharmacological models describe the dynamics of carefully-chosen medically meaningful variables in terms of systems of Ordinary Differential Equations (ODEs). However, these models only describe a limited collection of variables, and these variables are often not observable in clinical environments. To close this gap, we propose the latent hybridisation model (LHM) that integrates a system of expert-designed ODEs with machine-learned Neural ODEs to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. We evaluated LHM on synthetic data as well as real-world intensive care data of COVID-19 patients. LHM consistently outperforms previous works, especially when few training samples are available such as at the beginning of the pandemic.",This paper proposes a model for predicting the patient health status and disease progression over time based on pharmacological models. The model is based on the idea of integrating expert-designed ODEs with machine-learned Neural ODE models to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. The proposed model is evaluated on synthetic data as well as real-world intensive care data of COVID-19 patients.
687,SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"Representation learning has served as a key tool for meta-learning, enabling rapid learning of new tasks. Recent works like MAML learn task-specific representations by finding an initial representation requiring minimal per-task adaptation (i.e. a fine-tuning-based objective). We present a theoretical framework for analyzing a MAML-like algorithm, assuming all available tasks require approximately the same representation. We then provide risk bounds on predictors found by finetuning via gradient descent, demonstrating that the method provably leverages the shared structure. We illustrate these bounds in the logistic regression and neural network settings. In contrast, we establish settings where learning one representation for all tasks (i.e. using a “frozen representation” objective) fails. Notably, any such algorithm cannot outperform directly learning the target task with no other information, in the worst case. This separation underscores the benefit of fine-tuning-based over “frozen representation” objectives in few-shot learning.","This paper presents a theoretical framework for analyzing a MAML-like algorithm, assuming all available tasks require approximately the same representation. The authors provide risk bounds on predictors found by finetuning via gradient descent, demonstrating that the method provably leverages the shared structure. In contrast, the authors establish settings where learning one representation for all tasks (i.e. using a “frozen representation” objective) fails."
688,SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of G2L2 is a collection of lexicon entries, which map each word to a tuple of a syntactic type and a neuro-symbolic semantic program. For example, the word shiny has a syntactic type of adjective; its neuro-symbolic semantic program has the symbolic form λx.filter(x,SHINY), where the concept SHINY is associated with a neural network embedding, which will be used to classify shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries associated with each token. It then derives the meaning of the sentence as an executable neuro-symbolic program by composing lexical meanings based on syntax. The recovered meaning programs can be executed on grounded inputs. To facilitate learning in an exponentiallygrowing compositional space, we introduce a joint parsing and expected execution algorithm, which does local marginalization over derivations to reduce the training time. We evaluate G2L2 on two domains: visual reasoning and language-driven navigation. Results show that G2L2 can generalize from small amounts of data to novel compositions of words.","This paper presents Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach to learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of the approach is a collection of lexicon entries, which map each word to a syntactic type and a neuro-symbolic semantic program. The learned meaning programs can be executed on grounded inputs. To facilitate learning in an exponentially growing compositional space, the authors introduce a joint parsing and expected execution algorithm, which does local marginalization over derivations to reduce the training time. Experiments are conducted on two domains: visual reasoning and language-driven navigation."
689,SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"We propose and analyze a stochastic Newton algorithm for homogeneous distributed stochastic convex optimization, where each machine can calculate stochastic gradients of the same population objective, as well as stochastic Hessian-vector products (products of an independent unbiased estimator of the Hessian of the population objective with arbitrary vectors), with many such stochastic computations performed between rounds of communication. We show that our method can reduce the number, and frequency, of required communication rounds compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression), alongside empirical evidence.","This paper proposes a stochastic Newton algorithm for distributed distributed stochastically convex optimization, where each machine has access to a set of gradient and Hessian-vector products of the population objective. The authors show that their algorithm can reduce the number, and frequency, of required communication rounds compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression) and empirical evidence."
690,SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"Chamfer Distance (CD) and Earth Mover’s Distance (EMD) are two broadly adopted metrics for measuring the similarity between two point sets. However, CD is usually insensitive to mismatched local density, and EMD is usually dominated by global distribution while overlooks the fidelity of detailed structures. Besides, their unbounded value range induces a heavy influence from the outliers. These defects prevent them from providing a consistent evaluation. To tackle these problems, we propose a new similarity measure named Density-aware Chamfer Distance (DCD). It is derived from CD and benefits from several desirable properties: 1) it can detect disparity of density distributions and is thus a more intensive measure of similarity compared to CD; 2) it is stricter with detailed structures and significantly more computationally efficient than EMD; 3) the bounded value range encourages a more stable and reasonable evaluation over the whole test set. We adopt DCD to evaluate the point cloud completion task, where experimental results show that DCD pays attention to both the overall structure and local geometric details and provides a more reliable evaluation even when CD and EMD contradict each other. We can also use DCD as the training loss, which outperforms the same model trained with CD loss on all three metrics. In addition, we propose a novel point discriminator module that estimates the priority for another guided downsampling step, and it achieves noticeable improvements under DCD together with competitive results for both CD and EMD. We hope our work could pave the way for a more comprehensive and practical point cloud similarity evaluation. Our code will be available at https://github.com/wutong16/Density_aware_ Chamfer_Distance.","This paper proposes a new measure called density-aware Chamfer distance (DCD) to measure the similarity between two point sets. It is based on Chamfer Distance (CD) and Earth Mover’s Distance (EMD) which are two widely used metrics for measuring point cloud similarity. The authors claim that DCD is more sensitive to the local density than CD and EMD because it can detect disparity of density distributions and is more computationally efficient than EMD. In addition, the authors propose a novel point discriminator module that estimates the priority for another guided downsampling step, and it achieves noticeable improvements under DCD together with competitive results for both CD and EMD."
691,SP:e4b302009520770814ff2c096020b779a9fc38fe,"Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher — and that more closely matching the teacher paradoxically does not always lead to better student generalization.","This paper studies the problem of knowledge distillation, a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. The authors show that distillation can improve student generalization, but it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student. They identify difficulties in optimization as a key reason for why the student is unable to match the teacher. They also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher — and that more closely matching the teacher paradoxically does not always lead to better student generalisation."
692,SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"A k-decision tree t (or k-tree) is a recursive partition of a matrix (2D-signal) into k ≥ 1 block matrices (axis-parallel rectangles, leaves) where each rectangle is assigned a real label. Its regression or classification loss to a given matrix D of N entries (labels) is the sum of squared differences over every label in D and its assigned label by t. Given an error parameter ε ∈ (0, 1), a (k, ε)-coreset C of D is a small summarization that provably approximates this loss to every such tree, up to a multiplicative factor of 1 ± ε. In particular, the optimal k-tree of C is a (1 + ε)-approximation to the optimal k-tree of D. We provide the first algorithm that outputs such a (k, ε)-coreset for every such matrix D. The size |C| of the coreset is polynomial in k log(N)/ε, and its construction takes O(Nk) time. This is by forging a link between decision trees from machine learning – to partition trees in computational geometry. Experimental results on sklearn and lightGBM show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x10, while keeping similar accuracy. Full open source code is provided.","This paper studies the problem of partitioning a k-decision tree (k-tree) of a 2D matrix into k-1 block matrices, where each rectangle is assigned a real label. The authors show that a (k, \epsilon)-coreset C of the k-tree is a small summarization that approximates this loss to every such tree, up to a multiplicative factor of 1/\eps. They provide the first algorithm that outputs such a coreset for every such matrix D. The size |C| of the coreset is polynomial in k log(N)/ε, and its construction takes O(Nk) time. Experimental results on sklearn and lightGBM show that applying their coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x10, while keeping similar accuracy."
693,SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"We study the problem of the identification of m arms with largest means under a fixed error rate δ (fixed-confidence Top-m identification), for misspecified linear bandit models. This problem is motivated by practical applications, especially in medicine and recommendation systems, where linear models are popular due to their simplicity and the existence of efficient algorithms, but in which data inevitably deviates from linearity. In this work, we first derive a tractable lower bound on the sample complexity of any δ-correct algorithm for the general Top-m identification problem. We show that knowing the scale of the deviation from linearity is necessary to exploit the structure of the problem. We then describe the first algorithm for this setting, which is both practical and adapts to the amount of misspecification. We derive an upper bound to its sample complexity which confirms this adaptivity and that matches the lower bound when δ → 0. Finally, we evaluate our algorithm on both synthetic and real-world data, showing competitive performance with respect to existing baselines.","This paper studies the problem of Top-m identification for misspecified linear bandit models. The authors first derive a tractable lower bound on the sample complexity of any $\delta$-correct algorithm for the general Top-M identification problem. They then describe the first algorithm for this setting, which is both practical and adapts to the amount of misspecification. Finally, they evaluate their algorithm on both synthetic and real-world data, showing competitive performance with respect to existing baselines."
694,SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"Recently, self-supervised learning for graph neural networks (GNNs) has attracted considerable attention because of their notable successes in learning the representation of graph-structure data. However, the formation of a real-world graph typically arises from the highly complex interaction of many latent factors. The existing self-supervised learning methods for GNNs are inherently holistic and neglect the entanglement of the latent factors, resulting in the learned representations suboptimal for downstream tasks and difficult to be interpreted. Learning disentangled graph representations with self-supervised learning poses great challenges and remains largely ignored by the existing literature. In this paper, we introduce the Disentangled Graph Contrastive Learning (DGCL) method, which is able to learn disentangled graph-level representations with self-supervision. In particular, we first identify the latent factors of the input graph and derive its factorized representations. Each of the factorized representations describes a latent and disentangled aspect pertinent to a specific latent factor of the graph. Then we propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the factorized representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our method against several state-of-the-art baselines.","This paper proposes a self-supervised learning method for learning disentangled representations for graph neural networks (GNNs). The method is based on the idea of factorized representation learning, where each latent factor of the input graph is represented as a latent representation of a different aspect of the graph. The authors propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the factorized representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method."
695,SP:0a7edbbdabab11273689c40c517001eb46491113,We quantify the robustness of a trained network to input uncertainties with a stochastic simulation inspired by the field of Statistical Reliability Engineering. The robustness assessment is cast as a statistical hypothesis test: the network is deemed as locally robust if the estimated probability of failure is lower than a critical level. The procedure is based on an Importance Splitting simulation generating samples of rare events. We derive theoretical guarantees that are nonasymptotic w.r.t. sample size. Experiments tackling large scale networks outline the efficiency of our method making a low number of calls to the network function.,This paper proposes a method to evaluate the robustness of a trained network to input uncertainties with a stochastic simulation inspired by the field of Statistical Reliability Engineering (SRE). The robustness assessment is cast as a statistical hypothesis test: the network is deemed as locally robust if the estimated probability of failure is lower than a critical level. The procedure is based on an Importance Splitting simulation generating samples of rare events. Theoretical guarantees are derived that are nonasymptotic w.r.t. sample size. Experiments tackling large scale networks outline the efficiency of the method making a low number of calls to the network function.
696,SP:c1db485ff1ff9573daa421e167225654babb55ac,"Generative modeling has evolved to a notable field of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsupervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable. In this work, we introduce a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their autoand cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attributeguided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks. The source code of CoPE is available at https://github.com/grigorisg9gr/ polynomial_nets_for_conditional_generation.","This paper proposes a polynomial neural network (PNN) architecture for conditional image generation. The main idea is to use polynomials for two-variable inputs, i.e., the noise variable and the conditional variable. The authors show that the proposed architecture can be applied to a wide range of conditional generation tasks, including super-resolution, inverse problems, image-to-image translation, and attribute-guided generation. "
697,SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"We present a novel neural network Maximum Mean Discrepancy (MMD) statistic by identifying a new connection between neural tangent kernel (NTK) and MMD. This connection enables us to develop a computationally efficient and memory-efficient approach to compute the MMD statistic and perform NTK based two-sample tests towards addressing the long-standing challenge of memory and computational complexity of the MMD statistic, which is essential for online implementation to assimilating new samples. Theoretically, such a connection allows us to understand the NTK test statistic properties, such as the Type-I error and testing power for performing the two-sample test, by adapting existing theories for kernel MMD. Numerical experiments on synthetic and real-world datasets validate the theory and demonstrate the effectiveness of the proposed NTK-MMD statistic.","This paper proposes a neural tangent kernel (NTK) based method for computing the Maximum Mean Discrepancy (MMD) statistic for neural networks. Theoretically, the authors show that NTK-MMD can be used to compute the MMD statistic and perform NTK based two-sample tests towards addressing the long-standing challenge of memory and computational complexity of MMD, which is essential for online implementation to assimilating new samples. Numerical experiments on synthetic and real-world datasets validate the theory and demonstrate the effectiveness of the proposed NTK."
698,SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"What is the minimum necessary information required by a neural net D(·) from an image x to accurately predict its class? Extracting such information in the input space from x can allocate the areas D(·) mainly attending to and shed novel insights to the detection and defense of adversarial attacks. In this paper, we propose “class-disentanglement” that trains a variational autoencoder G(·) to extract this class-dependent information as x − G(x) via a trade-off between reconstructing x by G(x) and classifying x by D(x − G(x)), where the former competes with the latter in decomposing x so the latter retains only necessary information for classification in x − G(x). We apply it to both clean images and their adversarial images and discover that the perturbations generated by adversarial attacks mainly lie in the class-dependent part x − G(x). The decomposition results also provide novel interpretations to classification and attack models. Inspired by these observations, we propose to conduct adversarial detection and adversarial defense respectively on x − G(x) and G(x), which consistently outperform the results on the original x. In experiments, this simple approach substantially improves the detection and defense against different types of adversarial attacks. Code is available: https://github.com/kai-wen-yang/CD-VAE.","This paper studies the problem of adversarial attack detection and defense against adversarial attacks. In particular, the authors propose a method to decompose the input image x into x-G(x) and x-D(x), where x is the input to a neural network D(x, G(x)). The authors propose to train a variational autoencoder (VAE) to extract the class-dependent information from x, which is a trade-off between reconstructing x and classifying x, where the former competes with the latter in decomposing x so that the latter retains only necessary information for classification in x-g(x). Experiments on both clean images and adversarial images show that the perturbations generated by adversarial perturbation mainly lie in the class dependent part of x. The authors then propose to conduct adversarial detection and attack models on x-divergence and g(x)-Divergence, which consistently outperform the results on the original x."
699,SP:2789874561620ba7894c4672f935056bb911e919,"Bayesian optimization (BO) has recently been extended to the federated learning (FL) setting by the federated Thompson sampling (FTS) algorithm, which has promising applications such as federated hyperparameter tuning. However, FTS is not equipped with a rigorous privacy guarantee which is an important consideration in FL. Recent works have incorporated differential privacy (DP) into the training of deep neural networks through a general framework for adding DP to iterative algorithms. Following this general DP framework, our work here integrates DP into FTS to preserve user-level privacy. We also leverage the ability of this general DP framework to handle different parameter vectors, as well as the technique of local modeling for BO, to further improve the utility of our algorithm through distributed exploration (DE). The resulting differentially private FTS with DE (DP-FTS-DE) algorithm is endowed with theoretical guarantees for both the privacy and utility and is amenable to interesting theoretical insights about the privacy-utility trade-off. We also use real-world experiments to show that DP-FTS-DE achieves high utility (competitive performance) with a strong privacy guarantee (small privacy loss) and induces a trade-off between privacy and utility.","This paper proposes a federated Thompson sampling (FTS) algorithm for Bayesian optimization (BO) in federated learning (FL) with differential privacy (DP) and distributed exploration (DE) to improve the utility of the algorithm. The proposed algorithm is based on the DP-FTS-DE framework, which is a general framework for adding DP to iterative algorithms. Theoretical guarantees for both privacy and utility are provided for the proposed algorithm. Experiments on real-world datasets show that the proposed method achieves competitive performance with a strong privacy guarantee."
700,SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"Multi-label classification (MLC) allows complex dependencies among labels, making it more suitable to model many real-world problems. However, data annotation for training MLC models becomes much more labor-intensive due to the correlated (hence non-exclusive) labels and a potentially large and sparse label space. We propose to conduct multi-label active learning (ML-AL) through a novel integrated Gaussian Process-Bayesian Bernoulli Mixture model (GP-BM) to accurately quantify a data sample’s overall contribution to a correlated label space and choose the most informative samples for cost-effective annotation. In particular, the BM encodes label correlations using a Bayesian Bernoulli mixture of label clusters, where each mixture component corresponds to a global pattern of label correlations. To tackle highly sparse labels under AL, the BM is further integrated with a predictive GP to connect data features as an effective inductive bias and achieve a feature-component-label mapping. The GP predicts coefficients of mixture components that help to recover the final set of labels of a data sample. A novel auxiliary variable based variational inference algorithm is developed to tackle the non-conjugacy introduced along with the mapping process for efficient end-to-end posterior inference. The model also outputs a predictive distribution that provides both the label prediction and their correlations in the form of a label covariance matrix. A principled sampling function is designed accordingly to naturally capture both the feature uncertainty (through GP) and label covariance (through BM) for effective data sampling. Experiments on real-world multi-label datasets demonstrate the state-of-the-art AL performance of the proposed model.","This paper proposes a multi-label active learning (ML-AL) method that uses a Gaussian Process-Bayesian Bernoulli Mixture model (GP-BM) to estimate the overall contribution of a data sample to a correlated label space and choose the most informative samples for cost-effective annotation. In particular, the BM encodes label correlations using a BayesianBernoulli mixture of label clusters, where each mixture component corresponds to a global pattern of label correlations. The BM is further integrated with a predictive GP to connect data features as an effective inductive bias and achieve a feature-component-label mapping. A principled sampling function is designed accordingly to naturally capture both the feature uncertainty (through GP) and label covariance (through BM) for effective data sampling. The model also outputs a predictive distribution that provides both the label prediction and their correlations in the form of a label covariances matrix. Experiments on real-world datasets demonstrate the state-of-the-art AL performance."
701,SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"Recent works recognized lidars as an inherently streaming data source and showed that the end-to-end latency of lidar perception models can be reduced significantly by operating on wedge-shaped point cloud sectors rather then the full point cloud. However, due to use of cartesian coordinate systems these methods represent the sectors as rectangular regions, wasting memory and compute. In this work we propose using a polar coordinate system and make two key improvements on this design. First, we increase the spatial context by using multi-scale padding from neighboring sectors: preceding sector from the current scan and/or the following sector from the past scan. Second, we improve the core polar convolutional architecture by introducing feature undistortion and range stratified convolutions. Experimental results on the nuScenes dataset show significant improvements over other streaming based methods. We also achieve comparable results to existing non-streaming methods but with lower latencies.","This paper proposes to use a polar coordinate system to improve the spatial context of the point cloud in lidar perception models. The authors propose to use multi-scale padding from neighboring sectors: preceding sector from current scan and/or the following sector from the past scan. In addition, the authors introduce feature undistortion and range stratified convolutions to the core polar convolutional architecture. Experimental results on the nuScenes dataset show significant improvements over other streaming based methods."
702,SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"Structured latent variables allow incorporating meaningful prior knowledge into deep learning models. However, learning with such variables remains challenging because of their discrete nature. Nowadays, the standard learning approach is to define a latent variable as a perturbed algorithm output and to use a differentiable surrogate for training. In general, the surrogate puts additional constraints on the model and inevitably leads to biased gradients. To alleviate these shortcomings, we extend the Gumbel-Max trick to define distributions over structured domains. We avoid the differentiable surrogates by leveraging the score function estimators for optimization. In particular, we highlight a family of recursive algorithms with a common feature we call stochastic invariant. The feature allows us to construct reliable gradient estimates and control variates without additional constraints on the model. In our experiments, we consider various structured latent variable models and achieve results competitive with relaxation-based counterparts.","This paper proposes a method for learning structured latent variables. The authors extend the Gumbel-Max trick to define distributions over structured domains by leveraging the score function estimators for optimization. In particular, they highlight a family of recursive algorithms with a common feature we call stochastic invariant. The feature allows us to construct reliable gradient estimates and control variates without additional constraints on the model. In experiments, the authors consider various structured latent variable models and achieve results competitive with relaxation-based counterparts."
703,SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"Deep convolutional neural networks (CNNs) for image denoising are typically trained on large datasets. These models achieve the current state of the art, but they do not generalize well to data that deviate from the training distribution. Recent work has shown that it is possible to train denoisers on a single noisy image. These models adapt to the features of the test image, but their performance is limited by the small amount of information used to train them. Here we propose “GainTuning”, a methodology by which CNN models pre-trained on large datasets can be adaptively and selectively adjusted for individual test images. To avoid overfitting, GainTuning optimizes a single multiplicative scaling parameter (the “Gain”) of each channel in the convolutional layers of the CNN. We show that GainTuning improves state-of-the-art CNNs on standard image-denoising benchmarks, boosting their denoising performance on nearly every image in a held-out test set. These adaptive improvements are even more substantial for test images differing systematically from the training data, either in noise level or image type. We illustrate the potential of adaptive GainTuning in a scientific application to transmission-electronmicroscope images, using a CNN that is pre-trained on synthetic data. In contrast to the existing methodology, GainTuning is able to faithfully reconstruct the structure of catalytic nanoparticles from these data at extremely low signal-to-noise ratios.","This paper proposes GainTuning, a method to adaptively fine-tune CNNs for image denoising. The authors propose to use a single multiplicative scaling parameter (the Gain) of each channel in the convolutional layers of the CNN. The proposed method is evaluated on standard image-denoising benchmarks, and it is shown to improve state-of-the-art CNNs on nearly every image in a held-out test set. The adaptive improvements are even more substantial for test images differing systematically from the training data, either in noise level or image type. In addition, the authors demonstrate the potential of adaptive GainT tuning in a scientific application to transmission-electronmicroscope images, using a CNN that is pre-trained on synthetic data."
704,SP:90afa1102683b456bc72a54abef466326827546a,"We propose a fully differentiable architecture for simultaneous semantic and instance segmentation (a.k.a. panoptic segmentation) consisting of a convolutional neural network and an asymmetric multiway cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a panoptic labeling. Our formulation allows to directly maximize a smooth surrogate of the panoptic quality metric by backpropagating the gradient through the optimization problem. Experimental evaluation shows improvement by backpropagating through the optimization problem w.r.t. comparable approaches on Cityscapes and COCO datasets. Overall, our approach of combinatorial optimization for panoptic segmentation (COPS) shows the utility of using optimization in tandem with deep learning in a challenging large scale real-world problem and showcases benefits and insights into training such an architecture.",This paper proposes a differentiable architecture for simultaneous semantic and instance segmentation (a.k.a. panoptic segmentation) consisting of a convolutional neural network and an asymmetric multi-way cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a pan-optic labeling. The formulation allows to directly maximize a smooth surrogate of the pan-opinative quality metric by backpropagating the gradient through the optimization problem. Experimental evaluation shows improvement over the existing methods on Cityscapes and COCO datasets.
705,SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs) are widely used sequence models with complementary strengths and limitations. While PCFGs allow for nested hierarchical dependencies (tree structures), their latent variables (non-terminal symbols) have to be discrete. In contrast, DBNs allow for continuous latent variables, but the dependencies are strictly sequential (chain structure). Therefore, neither can be applied if the latent variables are assumed to be continuous and also to have a nested hierarchical dependency structure. In this paper, we present Recursive Bayesian Networks (RBNs), which generalise and unify PCFGs and DBNs, combining their strengths and containing both as special cases. RBNs define a joint distribution over tree-structured Bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. We provide two solutions: 1) For arbitrary RBNs, we generalise inside and outside probabilities from PCFGs to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variables via gradient descent, while marginalising over network structures. 2) For Gaussian RBNs, we additionally derive an analytic approximation of the marginal data likelihood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. The capacity and diverse applications of RBNs are illustrated on two examples: In a quantitative evaluation on synthetic data, we demonstrate and discuss the advantage of RBNs for segmentation and tree induction from noisy sequences, compared to change point detection and hierarchical clustering. In an application to musical data, we approach the unsolved problem of hierarchical music analysis from the raw note level and compare our results to expert annotations.","This paper proposes Recursive Bayesian Networks (RBNs), which generalize and unify PCFGs and dynamic Bayesian networks (DBNs) by combining their strengths and containing both as special cases. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. The authors provide two solutions: 1) for arbitrary RBNs, they generalise inside and outside probabilities from PCFG to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variables via gradient descent, while marginalising over network structures. 2) for Gaussian RBN, they additionally derive an analytic approximation of the marginal data likelihood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference."
706,SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"Backward propagation of errors (backpropagation) is a method to minimize objective functions (e.g., loss functions) of deep neural networks by identifying optimal sets of weights and biases. Imposing constraints on weight precision is often required to alleviate prohibitive workloads on hardware. Despite the remarkable success of backpropagation, the algorithm itself is not capable of considering such constraints unless additional algorithms are applied simultaneously. To address this issue, we propose the constrained backpropagation (CBP) algorithm based on the pseudo-Lagrange multiplier method to obtain the optimal set of weights that satisfy a given set of constraints. The defining characteristic of the proposed CBP algorithm is the utilization of a Lagrangian function (loss function plus constraint function) as its objective function. We considered various types of constraints — binary, ternary, one-bit shift, and two-bit shift weight constraints. As a posttraining method, CBP applied to AlexNet, ResNet-18, ResNet-50, and GoogLeNet on ImageNet, which were pre-trained using the conventional backpropagation. For most cases, the proposed algorithm outperforms the state-of-the-art methods on ImageNet, e.g., 66.6%, 74.4%, and 64.0% top-1 accuracy for ResNet-18, ResNet50, and GoogLeNet with binary weights, respectively. This highlights CBP as a learning algorithm to address diverse constraints with the minimal performance loss by employing appropriate constraint functions. The code for CBP is publicly available at https://github.com/dooseokjeong/CBP.","This paper proposes a post-training method called constrained backpropagation (CBP) which is based on the pseudo-Lagrange multiplier method to obtain the optimal set of weights that satisfy a given set of constraints. The authors considered various types of constraints such as binary, ternary, one-bit shift, and two-bit-shift weight constraints, and showed that CBP can address diverse constraints with the minimal performance loss by employing appropriate constraint functions. The proposed algorithm outperforms the state-of-the-art methods on ImageNet, e.g., 66.6%, 74.4%, and 64.0% top-1 accuracy for binary weights, respectively."
707,SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"Active learning sequentially selects the best instance for labeling by optimizing an acquisition function to enhance data/label efficiency. The selection can be either from a discrete instance set (pool-based scenario) or a continuous instance space (query synthesis scenario). In this work, we study both active learning scenarios for Gaussian Process Classification (GPC). The existing active learning strategies that maximize the Estimated Error Reduction (EER) aim at reducing the classification error after training with the new acquired instance in a onestep-look-ahead manner. The computation of EER-based acquisition functions is typically prohibitive as it requires retraining the GPC with every new query. Moreover, as the EER is not smooth, it can not be combined with gradient-based optimization techniques to efficiently explore the continuous instance space for query synthesis. To overcome these critical limitations, we develop computationally efficient algorithms for EER-based active learning with GPC. We derive the joint predictive distribution of label pairs as a one-dimensional integral, as a result of which the computation of the acquisition function avoids retraining the GPC for each query, remarkably reducing the computational overhead. We also derive the gradient chain rule to efficiently calculate the gradient of the acquisition function, which leads to the first query synthesis active learning algorithm implementing EER-based strategies. Our experiments clearly demonstrate the computational efficiency of the proposed algorithms. We also benchmark our algorithms on both synthetic and real-world datasets, which show superior performance in terms of sampling efficiency compared to the existing state-of-the-art algorithms.","This paper studies the problem of active learning for Gaussian process classification (GPC) in the context of query synthesis. The authors propose a novel algorithm for EER-based active learning with GPC. The main idea is to use the joint predictive distribution of label pairs as a one-dimensional integral to reduce the computation of the acquisition function, which avoids retraining the GPC for each query. They also derive the gradient chain rule to efficiently calculate the gradient of acquisition function. The proposed algorithm is evaluated on both synthetic and real-world datasets."
708,SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"A number of recent studies of continuous variational autoencoder (VAE) models have noted, either directly or indirectly, the tendency of various parameter gradients to drift towards infinity during training. Because such gradients could potentially contribute to numerical instabilities, and are often framed as a problematic phenomena to be avoided, it may be tempting to shift to alternative energy functions that guarantee bounded gradients. But it remains an open question: What might the unintended consequences of such a restriction be? To address this issue, we examine how unbounded gradients relate to the regularization of a broad class of autoencoder-based architectures, including VAE models, as applied to data lying on or near a low-dimensional manifold (e.g., natural images). Our main finding is that, if the ultimate goal is to simultaneously avoid over-regularization (high reconstruction errors, sometimes referred to as posterior collapse) and underregularization (excessive latent dimensions are not pruned from the model), then an autoencoder-based energy function with infinite gradients around optimal representations is provably required per a certain technical sense which we carefully detail. Given that both overand under-regularization can directly lead to poor generated sample quality or suboptimal feature selection, this result suggests that heuristic modifications to or constraints on the VAE energy function may at times be ill-advised, and large gradients should be accommodated to the extent possible.","This paper studies the effect of unbounded gradients on the regularization of autoencoder-based architectures, including VAE models, as applied to data lying on or near a low-dimensional manifold (e.g., natural images). The main finding is that, if the ultimate goal is to simultaneously avoid over-regularization (high reconstruction errors, sometimes referred to as posterior collapse) and under-regularisation (excessive latent dimensions are not pruned from the model), then an autoencoders-based energy function with infinite gradients around optimal representations is provably required. This result suggests that heuristic modifications to or constraints on the VAE energy function may at times be ill-advised, and large gradients should be accommodated to the extent possible."
709,SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"The bandit problem with graph feedback, proposed in [Mannor and Shamir, NeurIPS 2011], is modeled by a directed graph G = (V,E) where V is the collection of bandit arms, and once an arm is triggered, all its incident arms are observed. A fundamental question is how the structure of the graph affects the min-max regret. We propose the notions of the fractional weak domination number δ∗ and the k-packing independence number capturing upper bound and lower bound for the regret respectively. We show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual — the fractional vertex packing set respectively. Based on this connection, we utilize the strong duality theorem to prove a general regret upper bound O ( (δ∗ log |V |) 1 3 T 2 3 ) and a lower bound Ω ( (δ∗/α) 1 3 T 2 3 ) where α is the integrality gap of the dual linear program. Therefore, our bounds are tight up to a (log |V |) 1 3 factor on graphs with bounded integrality gap for the vertex packing problem including trees and graphs with bounded degree. Moreover, we show that for several special families of graphs, we can get rid of the (log |V |) 1 3 factor and establish optimal regret.","This paper studies the bandit problem with graph feedback, which is modeled by a directed graph G = (V,E) where V is the collection of bandit arms, and once an arm is triggered, all its incident arms are observed. A fundamental question is how the structure of the graph affects the min-max regret. The authors propose the notions of the fractional weak domination number and the k-packing independence number capturing upper bound and lower bound for the regret respectively. They show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual — the fractionsal vertex packing set respectively. Based on this connection, they utilize the strong duality theorem to prove a general regret upper bound O ( (δ∗ log |V |) 1 3 T 2 3 ) and a lower bound O(\epsilon/\alpha) where $\alpha$ is the integrality gap of the dual linear program. Their bounds are tight up to a (log |V|) 1.3 factor on graphs with bounded integrality gaps for the vertex packing problem including trees and graphs with a bounded degree."
710,SP:e50dec57af337839cbde4b65fb7b431785fda44d,"Shapley values provide model agnostic feature attributions for model outcome at a particular instance by simulating feature absence under a global population distribution. The use of a global population can lead to potentially misleading results when local model behaviour is of interest. Hence we consider the formulation of neighbourhood reference distributions that improve the local interpretability of Shapley values. By doing so, we find that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, we observe that Neighbourhood Shapley values identify meaningful sparse feature relevance attributions that provide insight into local model behaviour, complimenting conventional Shapley analysis. They also increase on-manifold explainability and robustness to the construction of adversarial classifiers.","This paper proposes a new neighbourhood Shapley value (N-Shapley value) based on neighbourhood reference distributions. The authors argue that the proposed neighborhood Shapley values are more interpretable than the standard Shapley Value (SV) which is based on the global Shapley Values (GV). The authors propose to use the Nadaraya-Watson estimator as a self-normalized importance sampling estimator, which can be expressed as an estimator of the importance of a neighbourhood. The proposed Neighbourhood Shapley values can be used to improve the interpretability of the SV. In particular, the authors show that the Neighbours Shapleyvalues are more explainable and robust to adversarial attacks. "
711,SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"Learning good feature representations is important for deep reinforcement learning (RL). However, with limited experience, RL often suffers from data inefficiency for training. For un-experienced or less-experienced trajectories (i.e., state-action sequences), the lack of data limits the use of them for better feature learning. In this work, we propose a novel method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, PlayVirtual predicts future states in a latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, we augment the actions to generate a large amount of virtual state-action trajectories. Being free of groudtruth state supervision, we enforce a trajectory to meet the cycle consistency constraint, which can significantly enhance the data efficiency. We validate the effectiveness of our designs on the Atari and DeepMind Control Suite benchmarks. Our method achieves the state-of-the-art performance on both benchmarks. Our code is available at https://github.com/microsoft/Playvirtual.","This paper proposes a novel method to generate cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. The proposed method is based on a dynamics model that predicts future states in a latent space based on the current state and action and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, the authors augment the actions to generate a large amount of virtual state-action trajectories. The authors validate the effectiveness of their designs on the Atari and DeepMind Control Suite benchmarks."
712,SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,"Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works — how the network’s architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network’s robustness via the predictive power in its representations — the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.","This paper proposes a theoretical framework to study the relationship between network architecture and robustness to noisy labels. The authors propose to measure the predictive power of a network using the test performance of a linear model trained on the learned representations using a small set of clean labels. They hypothesize that a network is more robust if its architecture is more aligned with the target function than the noise. To support their hypothesis, they provide both theoretical and empirical evidence across various neural network architectures and different domains."
713,SP:903727fe028684623a8ccadec210e641ecffc685,"Reinforcement learning (RL) algorithms assume that users specify tasks by manually writing down a reward function. However, this process can be laborious and demands considerable technical expertise. Can we devise RL algorithms that instead enable users to specify tasks simply by providing examples of successful outcomes? In this paper, we derive a control algorithm that maximizes the future probability of these successful outcome examples. Prior work has approached similar problems with a two-stage process, first learning a reward function and then optimizing this reward function using another RL algorithm. In contrast, our method directly learns a value function from transitions and successful outcomes, without learning this intermediate reward function. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.1","This paper proposes a method for learning a reward function that maximizes the future probability of successful outcome examples. The authors propose to learn a value function from transitions and successful outcomes, without learning an intermediate reward function. They show that their method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that the proposed method outperforms prior methods that learn explicit reward functions."
714,SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"We study differentially private stochastic optimization in convex and non-convex settings. For the convex case, we focus on the family of non-smooth generalized linear losses (GLLs). Our algorithm for the l2 setting achieves optimal excess population risk in near-linear time, while the best known differentially private algorithms for general convex losses run in super-linear time. Our algorithm for the l1 setting has nearly-optimal excess population risk Õ (√ log d nε ), and circumvents the dimension dependent lower bound of [AFKT21] for general non-smooth convex losses. In the differentially private non-convex setting, we provide several new algorithms for approximating stationary points of the population risk. For the l1-case with smooth losses and polyhedral constraint, we provide the first nearly dimension independent rate, Õ ( log d (nε)1/3 ) in linear time. For the constrained l2-case with smooth losses, we obtain a linear-time algorithm with rate Õ ( 1 n1/3 + d 1/5 (nε)2/5 ). Finally, for the l2-case we provide the first method for non-smooth weakly convex stochastic optimization with rate Õ ( 1 n1/4 + d 1/6 (nε)1/3 ) which matches the best existing non-private algorithm when d = O( √ n). We also extend all our results above for the non-convex l2 setting to the lp setting, where 1 < p ≤ 2, with only polylogarithmic (in the dimension) overhead in the rates.","This paper studies differentially private stochastic optimization in convex and non-convex settings. In the convex case, the authors focus on the family of non-smooth generalized linear losses (GLLs). Their algorithm for the l2 setting achieves optimal excess population risk in near-linear time, while the best known differentially public algorithms for general convex losses run in super-linear times. For the l1-case with smooth losses and polyhedral constraint, they provide the first nearly dimension independent rate, Õ ( log d (nε)1/3 ) in linear time. For constrained l2-case, they obtain a linear-time algorithm with rate Ú ( 1 n1/4 + d 1/5 (n\epsilon)2/5 ) for l2 weakly convex optimization. "
715,SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"The cooperative bandit problem is increasingly becoming relevant due to its applications in large-scale decision-making. However, most research for this problem focuses exclusively on the setting with perfect communication, whereas in most real-world distributed settings, communication is often over stochastic networks, with arbitrary corruptions and delays. In this paper, we study cooperative bandit learning under three typical real-world communication scenarios, namely, (a) message-passing over stochastic time-varying networks, (b) instantaneous rewardsharing over a network with random delays, and (c) message-passing with adversarially corrupted rewards, including byzantine communication. For each of these environments, we propose decentralized algorithms that achieve competitive performance, along with near-optimal guarantees on the incurred group regret as well. Furthermore, in the setting with perfect communication, we present an improved delayed-update algorithm that outperforms the existing state-of-the-art on various network topologies. Finally, we present tight network-dependent minimax lower bounds on the group regret. Our proposed algorithms are straightforward to implement and obtain competitive empirical performance.","This paper studies the cooperative bandit problem in the context of decentralized reinforcement learning. The authors study the problem in three settings: (1) message-passing over stochastic time-varying networks, (2) instantaneous reward sharing over a network with random delays, and (3) adversarially corrupted rewards, including byzantine communication. They propose decentralized algorithms that achieve competitive performance, along with near-optimal guarantees on the incurred group regret as well as an improved delayed-update algorithm that outperforms the existing state-of-the-art on various network topologies. Finally, they present tight network-dependent minimax lower bounds on the group regret."
716,SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"Recently, transformer has achieved remarkable performance on a variety of computer vision applications. Compared with mainstream convolutional neural networks, vision transformers are often of sophisticated architectures for extracting powerful feature representations, which are more difficult to be developed on mobile devices. In this paper, we present an effective post-training quantization algorithm for reducing the memory storage and computational costs of vision transformers. Basically, the quantization task can be regarded as finding the optimal low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, we introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, we thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixedprecision quantization scheme by exploiting the nuclear norm of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art posttraining quantization algorithms. For instance, we can obtain an 81.29% top-1 accuracy using DeiT-B model on ImageNet dataset with about 8-bit quantization. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/VTPTQ.","This paper proposes a post-training quantization algorithm for vision transformers. The main idea is to find the optimal low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, the authors introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, they thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature."
717,SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"Double Q-learning (Hasselt, 2010) has gained significant success in practice due to its effectiveness in overcoming the overestimation issue of Q-learning. However, the theoretical understanding of double Q-learning is rather limited. The only existing finite-time analysis was recently established in (Xiong et al., 2020), where the polynomial learning rate adopted in the analysis typically yields a slower convergence rate. This paper tackles the more challenging case of a constant learning rate, and develops new analytical tools that improve the existing convergence rate by orders of magnitude. Specifically, we show that synchronous double Q-learning attains an -accurate global optimum with a time complexity of Ω̃ ( lnD (1−γ)7 2 ), and the asynchronous algorithm achieves a time complexity of Ω̃ ( L (1−γ)7 2 ), where D is the cardinality of the state-action space, γ is the discount factor, and L is a parameter related to the sampling strategy for asynchronous double Q-learning. These results improve the existing convergence rate by the order of magnitude in terms of its dependence on all major parameters (, 1 − γ,D,L). This paper presents a substantial step toward the full understanding of the fast convergence of double-Q learning.","This paper studies the convergence of double-Q-learning in the case of constant learning rate. The authors show that synchronous double Q-learning attains an accurate global optimum with a time complexity of $O(\sqrt{L}(L^2/\epsilon^2)$ and asynchronous algorithm achieves a complexity of $\Omega(L^{-1/\gamma)$ where $L$ is the cardinality of the state-action space, $\gamma$ is a discount factor, and $D$ is related to the sampling strategy for asynchronous double Q learning. The results improve the existing convergence rate by an order of magnitude. "
718,SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"Existing semi-supervised learning (SSL) studies typically assume that unlabeled and test data are drawn from the same distribution as labeled data. However, in many real-world applications, it is desirable to have SSL algorithms that not only classify the samples drawn from the same distribution of labeled data but also detect out-of-distribution (OOD) samples drawn from an unknown distribution. In this paper, we study a setting called semi-supervised OOD detection. Two main challenges compared with previous OOD detection settings are i) the lack of labeled data and in-distribution data; ii) OOD samples could be unseen during training. Efforts on this direction remain limited. In this paper, we present an approach STEP significantly improving OOD detection performance by introducing a new technique: Structure-Keep Unzipping. It learns a new representation space in which OOD samples could be separated well. An efficient optimization algorithm is derived to solve the objective. Comprehensive experiments across various OOD detection benchmarks clearly show that our STEP approach outperforms other methods by a large margin and achieves remarkable detection performance on several benchmarks.",This paper proposes a method for semi-supervised out-of-distribution (OOD) detection. The proposed method is based on the idea of Structure-Keep Unzipping (STU) which learns a new representation space in which OOD samples could be separated well. An efficient optimization algorithm is derived to solve the objective. Experiments on various OOD detection benchmarks show that the proposed method outperforms other methods by a large margin.
719,SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"As an important step towards visual reasoning, visual grounding (e.g., phrase localization, referring expression comprehension / segmentation) has been widely explored. Previous approaches to referring expression comprehension (REC) or segmentation (RES) either suffer from limited performance, due to a two-stage setup, or require the designing of complex task-specific one-stage architectures. In this paper, we propose a simple one-stage multi-task framework for visual grounding tasks. Specifically, we leverage a transformer architecture, where two modalities are fused in a visual-lingual encoder. In the decoder, the model learns to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. With this simple but highly contextualized model, we outperform state-of-the-art methods by a large margin on both REC and RES tasks. We also show that a simple pre-training schedule (on an external dataset) further improves the performance. Extensive experiments and ablations illustrate that our model benefits greatly from contextualized information and multi-task training.","This paper proposes a multi-task model for referring expression comprehension (REC) and segmentation (RES) tasks. The proposed model is based on a transformer architecture, where two modalities are fused in a visual-lingual encoder and decoder. In the decoder, the model learns to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. Experiments show that the proposed model outperforms state-of-the-art methods on both REC and RES tasks."
720,SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"Boosting is an algorithmic approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. In this work we study multiclass boosting with a possibly large number of classes or categories. Multiclass boosting can be formulated in various ways. Here, we focus on an especially natural formulation in which the weak hypotheses are assumed to belong to an “easy-to-learn” base class, and the weak learner is an agnostic PAC learner for that class with respect to the standard classification loss. This is in contrast with other, more complicated losses as have often been considered in the past. The goal of the overall boosting algorithm is then to learn a combination of weak hypotheses by repeatedly calling the weak learner. We study the resources required for boosting, especially how they depend on the number of classes k, for both the booster and weak learner. We find that the boosting algorithm itself only requires O(log k) samples, as we show by analyzing a variant of AdaBoost for our setting. In stark contrast, assuming typical limits on the number of weak-learner calls, we prove that the number of samples required by a weak learner is at least polynomial in k, exponentially more than the number of samples needed by the booster. Alternatively, we prove that the weak learner’s accuracy parameter must be smaller than an inverse polynomial in k, showing that the returned weak hypotheses must be nearly the best in their class when k is large. We also prove a trade-off between number of oracle calls and the resources required of the weak learner, meaning that the fewer calls to the weak learner the more that is demanded on each call.","This paper studies the problem of multiclass boosting, which is an algorithmic approach to combining weak and moderately inaccurate hypotheses to a strong and accurate one. The authors study the resources required for boosting, especially how they depend on the number of classes k, for both the booster and weak learner. They show that the boosting algorithm itself only requires O(log k) samples, as they show by analyzing a variant of AdaBoost for the setting. They also prove a trade-off between number of oracle calls and the resources needed of the weak learners."
721,SP:f63b050773871338c48b778c362172e4b72477a4,"Advances in unsupervised learning of object-representations have culminated in the development of a broad range of methods for unsupervised object segmentation and interpretable object-centric scene generation. These methods, however, are limited to simulated and real-world datasets with limited visual complexity. Moreover, object representations are often inferred using RNNs which do not scale well to large images or iterative refinement which avoids imposing an unnatural ordering on objects in an image but requires the a priori initialisation of a fixed number of object representations. In contrast to established paradigms, this work proposes an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters a priori. This is used to develop a new model, GENESIS-V2, which can infer a variable number of object representations without using RNNs or iterative refinement. We show that GENESIS-V2 performs strongly in comparison to recent baselines in terms of unsupervised image segmentation and object-centric scene generation on established synthetic datasets as well as more complex real-world datasets.","This paper proposes a method for unsupervised image segmentation and object-centric scene generation based on embedding-based methods. The proposed method is based on a differentiable clustering of pixels using a stochastic stick-breaking process, which is similar to iterative refinement, but without the need of initialising a fixed number of clusters a priori. This is used to develop a new model, GENESIS-V2, which can infer a variable number of object representations without using RNNs or iterative methods. Experiments on synthetic and real-world datasets show that the proposed method outperforms the baselines."
722,SP:408deb9e5577ee7118b836fee77135df641fe545,"We develop methods for forming prediction sets in an online setting where the data generating distribution is allowed to vary over time in an unknown fashion. Our framework builds on ideas from conformal inference to provide a general wrapper that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. While previous conformal inference methods rely on the assumption that the data points are exchangeable, our adaptive approach provably achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. We accomplish this by modelling the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. We test our method, adaptive conformal inference, on two real world datasets and find that its predictions are robust to visible and significant distribution shifts.","This paper proposes a new method for online prediction in the online setting where the data generating distribution is allowed to vary over time in an unknown fashion. The proposed method is based on the idea of adaptive conformal inference, which is an extension of the Conformal Inference (CIFAR-10) method. The main idea is to model the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. The method is tested on two real world datasets and finds that its predictions are robust to visible and significant distribution shifts."
723,SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"Multi-person pose estimation in crowded scenes is challenging because overlapping and occlusions make it difficult to detect person bounding boxes and infer pose cues from individual keypoints. To address those issues, this paper proposes a direct pose-level inference strategy that is free of bounding box detection and keypoint grouping. Instead of inferring individual keypoints, the Pose-level Inference Network (PINet) directly infers the complete pose cues for a person from his/her visible body parts. PINet first applies the Part-based Pose Generation (PPG) to infer multiple coarse poses for each person from his/her body parts. Those coarse poses are refined by the Pose Refinement module through incorporating pose priors, and finally are fused in the Pose Fusion module. PINet relies on discriminative body parts to differentiate overlapped persons, and applies visual body cues to infer the global pose cues. Experiments on several crowded scenes pose estimation benchmarks demonstrate the superiority of PINet. For instance, it achieves 59.8% AP on the OCHuman dataset, outperforming the recent works by a large margin†.",This paper proposes a pose-level inference method for multi-person pose estimation in crowded scenes. The proposed method is based on the Part-based Pose Generation (PPG) and Pose Refinement (PRe) modules. The Pose Fusion (PF) module is a fusion of the PPG and PRe modules. Experiments on several crowded scenes pose estimation benchmarks demonstrate the superiority of PINet. 
724,SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,"Robust Markov decision processes (RMDPs) are a useful building block of robust reinforcement learning algorithms but can be hard to solve. This paper proposes a fast, exact algorithm for computing the Bellman operator for S-rectangular robust Markov decision processes with L∞-constrained rectangular ambiguity sets. The algorithm combines a novel homotopy continuation method with a bisection method to solve S-rectangular ambiguity in quasi-linear time in the number of states and actions. The algorithm improves on the cubic time required by leading general linear programming methods. Our experimental results confirm the practical viability of our method and show that it outperforms a leading commercial optimization package by several orders of magnitude.",This paper proposes a new algorithm for solving the Bellman operator for S-rectangular robust Markov decision processes with L-constrained rectangular ambiguity sets. The algorithm combines a novel homotopy continuation method with a bisection method to solve the ambiguity in quasi-linear time in the number of states and actions. The experimental results confirm the practical viability of the proposed algorithm and show that it outperforms a leading commercial optimization package.
725,SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,"There has been recent interest in using machine-learned predictions to improve the worst-case guarantees of online algorithms. In this paper we continue this line of work by studying the online knapsack problem, but with very weak predictions: in the form of knowing an upper and lower bound for the number of items of each value. We systematically derive online algorithms that attain the best possible competitive ratio for any fixed prediction; we also extend the results to more general settings such as generalized one-way trading and two-stage online knapsack. Our work shows that even seemingly weak predictions can be utilized effectively to provably improve the performance of online algorithms.","This paper studies the problem of online knapsack problem with very weak predictions. The paper shows that even seemingly weak predictions can be utilized effectively to improve the performance of online algorithms. In particular, the paper systematically derive online algorithms that attain the best possible competitive ratio for any fixed prediction and extend the results to more general settings such as generalized one-way trading and two-stage onlineknapsack. "
726,SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"Episodic control enables sample efficiency in reinforcement learning by recalling past experiences from an episodic memory. We propose a new model-based episodic memory of trajectories addressing current limitations of episodic control. Our memory estimates trajectory values, guiding the agent towards good policies. Built upon the memory, we construct a complementary learning model via a dynamic hybrid control unifying model-based, episodic and habitual learning into a single architecture. Experiments demonstrate that our model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments including stochastic and non-Markovian settings.","This paper proposes a new model-based episodic memory of trajectories for episodic control. The memory estimates trajectory values, guiding the agent towards good policies. A complementary learning model is built upon the memory, which is a dynamic hybrid control model. Experiments demonstrate that the proposed model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments."
727,SP:551174c1266b5f4b6aaf5432a4c713386f90898c,"The scarcity of labeled data is a critical obstacle to deep learning. Semi-supervised learning (SSL) provides a promising way to leverage unlabeled data by pseudo labels. However, when the size of labeled data is very small (say a few labeled samples per class), SSL performs poorly and unstably, possibly due to the low quality of learned pseudo labels. In this paper, we propose a new SSL method called DP-SSL that adopts an innovative data programming (DP) scheme to generate probabilistic labels for unlabeled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), we develop a multiple-choice learning (MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, we design a label model to resolve the conflict and overlap among the noisy labels, and finally infer probabilistic labels for unlabeled samples. Extensive experiments on four standard SSL benchmarks show that DP-SSL can provide reliable labels for unlabeled data and achieve better classification performance on test sets than existing SSL methods, especially when only a small number of labeled samples are available. Concretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82% annotation accuracy on unlabeled data and 93.46% classification accuracy on test data, which are higher than the SOTA results.","This paper proposes a new semi-supervised learning method based on data programming (DP) scheme to generate probabilistic labels for unlabeled data. The authors propose a multiple-choice learning (MCL) based approach to automatically generate LFs from scratch in SSL style. They design a label model to resolve the conflict and overlap among the noisy labels, and finally infer the label for unlabelled data. Extensive experiments on four standard SSL benchmarks show that DP-SSL can provide reliable labels and achieve better classification performance on test sets than existing SSL methods."
728,SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"We present Multi-view Pose transformer (MvP) for estimating multi-person 3D poses from multi-view images. Instead of estimating 3D joint locations from costly volumetric representation or reconstructing the per-person 3D pose from multiple detected 2D poses as in previous methods, MvP directly regresses the multi-person 3D poses in a clean and efficient way, without relying on intermediate tasks. Specifically, MvP represents skeleton joints as learnable query embeddings and let them progressively attend to and reason over the multi-view information from the input images to directly regress the actual 3D joint locations. To improve the accuracy of such a simple pipeline, MvP presents a hierarchical scheme to concisely represent query embeddings of multi-person skeleton joints and introduces an inputdependent query adaptation approach. Further, MvP designs a novel geometrically guided attention mechanism, called projective attention, to more precisely fuse the cross-view information for each joint. MvP also introduces a RayConv operation to integrate the view-dependent camera geometry into the feature representations for augmenting the projective attention. We show experimentally that our MvP model outperforms the state-of-the-art methods on several benchmarks while being much more efficient. Notably, it achieves 92.3% AP25 on the challenging Panoptic dataset, improving upon the previous best approach [40] by 9.8%. MvP is general and also extendable to recovering human mesh represented by the SMPL model, thus useful for modeling multi-person body shapes. Code and models are available at https://github.com/sail-sg/mvp.","This paper proposes a multi-view pose transformer (MVPT) method for estimating multi-person 3D poses from multi view images. The proposed method is based on the idea of learning a query embedding for skeleton joints, which is then used to train a neural network to estimate the 3D joint locations. The method is evaluated on the Panoptic dataset and compared to several state-of-the-art methods. "
729,SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"In this paper, we address two learning problems. Suppose a family of ` unknown sparse vectors is fixed, where each vector has at most k non-zero elements. In the first problem, we concentrate on robust learning the supports of all vectors from the family using a sequence of noisy responses. Each response to a query vector shows the sign of the inner product between a randomly chosen vector from the family and the query vector. In the second problem, we aim at designing queries such that all sparse vectors from the family can be approximately reconstructed based on the error-free responses. This learning model was introduced in the work of Gandikota et al., 2020, and these problems can be seen as generalizations of support recovery and approximate recovery problems, well-studied under the framework of 1-bit compressed sensing. As the main contribution of the paper, we prove the existence of learning algorithms for the first problem which work without any assumptions. Under a mild structural assumption on the unknown vectors, we also show the existence of learning algorithms for the second problem and rigorously analyze their query complexity.","This paper studies the problem of support recovery and approximate recovery of sparse vectors from a fixed family of unknown sparse vectors. In particular, the authors consider the case where each vector in the family has at most k non-zero elements, and the goal is to learn the supports of all vectors from the family using a sequence of noisy responses. The authors prove the existence of learning algorithms for the first problem which work without any assumptions on the unknown vectors. They also show that learning algorithms exist for the second problem and rigorously analyze their query complexity."
730,SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"Many industrial and security applications employ a suite of sensors for detecting abrupt changes in temporal behavior patterns. These abrupt changes typically manifest locally, rendering only a small subset of sensors informative. Continuous monitoring of every sensor can be expensive due to resource constraints, and serves as a motivation for the bandit quickest changepoint detection problem, where sensing actions (or sensors) are sequentially chosen, and only measurements corresponding to chosen actions are observed. We derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions. We then propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. We derive expected delay bounds for the proposed scheme and show that these bounds match our information-theoretic lower bounds at low false alarm rates, establishing optimality of the proposed method. We then perform a number of experiments on synthetic and real datasets demonstrating the effectiveness of our proposed method.","This paper studies the problem of bandit quickest changepoint detection, where only a small subset of sensors are used to detect abrupt changes in temporal behavior patterns. The authors derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions, and propose a computationally efficient online sensing scheme that seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. They derive expected delay bounds for the proposed scheme and show that these bounds match the information lower bounds at low false alarm rates, establishing optimality of the proposed method. They then perform a number of experiments on synthetic and real datasets demonstrating the effectiveness of their proposed algorithm."
731,SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"Stochastic nested optimization, including stochastic bilevel, min-max, and compositional optimization, is gaining popularity in many machine learning applications. While the three problems share a nested structure, existing works often treat them separately, thus developing problem-specific algorithms and analyses. Among various exciting developments, simple SGD-type updates (potentially on multiple variables) are still prevalent in solving this class of nested problems, but they are believed to have a slower convergence rate than non-nested problems. This paper unifies several SGD-type updates for stochastic nested problems into a single SGD approach that we term ALternating Stochastic gradient dEscenT (ALSET) method. By leveraging the hidden smoothness of the problem, this paper presents a tighter analysis of ALSET for stochastic nested problems. Under the new analysis, to achieve an -stationary point of the nested problem, it requires O( −2) samples in total. Under certain regularity conditions, applying our results to stochastic compositional, min-max, and reinforcement learning problems either improves or matches the best-known sample complexity in the respective cases. Our results explain why simple SGD-type algorithms in stochastic nested problems all work very well in practice without the need for further modifications.","This paper proposes a new method for solving stochastic nested optimization problems. The main contribution of this paper is to unify several SGD-type updates for nested problems into a single SGD approach that they term ALternating Stochastic Gradient dEscenT (ALSET) method. The authors show that under certain regularity conditions, the proposed ALSET method can converge to the stationary point of the nested problem with a sample complexity of O(2/\epsilon^2). The authors also show that the proposed method can be applied to compositional, min-max, and reinforcement learning problems."
732,SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"Recent advances in the video question answering (i.e., VideoQA) task have achieved strong success by following the paradigm of fine-tuning each clip-text pair independently on the pretrained transformer-based model via supervised learning. Intuitively, multiple samples (i.e., clips) should be interdependent to capture similar visual and key semantic information in the same video. To consider the interdependent knowledge between contextual clips into the network inference, we propose a Siamese Sampling and Reasoning (SiaSamRea) approach, which consists of a siamese sampling mechanism to generate sparse and similar clips (i.e., siamese clips) from the same video, and a novel reasoning strategy for integrating the interdependent knowledge between contextual clips into the network. The reasoning strategy contains two modules: (1) siamese knowledge generation to learn the inter-relationship among clips; (2) siamese knowledge reasoning to produce the refined soft label by propagating the weights of inter-relationship to the predicted candidates of all clips. Finally, our SiaSamRea can endow the current multimodal reasoning paradigm with the ability of learning from inside via the guidance of soft labels. Extensive experiments demonstrate our SiaSamRea achieves state-ofthe-art performance on five VideoQA benchmarks, e.g., a significant +2.1% gain on MSRVTT-QA, +2.9% on MSVD-QA, +1.0% on ActivityNet-QA, +1.8% on How2QA and +4.3% (action) on TGIF-QA.","This paper proposes a Siamese Sampling and Reasoning (SiaSamRea) method for video question answering. The authors propose a siamese sampling mechanism to generate sparse and similar clips from the same video, and a novel reasoning strategy for integrating the interdependent knowledge between contextual clips into the network. The reasoning strategy consists of two modules: (1) siamesese knowledge generation to learn the inter-relationship among clips; (2) Siameses knowledge reasoning to produce the refined soft label by propagating the weights of inter-relation to the predicted candidates of all clips. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on five VideoQA benchmarks."
733,SP:160022e2cd61159da92f92e85520b7062a337a8d,"Structured distributions, i.e. distributions over combinatorial spaces, are commonly used to learn latent probabilistic representations from observed data. However, scaling these models is bottlenecked by the high computational and memory complexity with respect to the size of the latent representations. Common models such as Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) require time and space quadratic and cubic in the number of hidden states respectively. This work demonstrates a simple approach to reduce the computational and memory complexity of a large class of structured models. We show that by viewing the central inference step as a matrix-vector product and using a low-rank constraint, we can trade off model expressivity and speed via the rank. Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces while providing practical speedups.","This paper proposes a method to reduce the computational and memory complexity of a large class of structured models. The main idea is to view the central inference step as a matrix-vector product and use a low-rank constraint to trade off model expressivity and speed via the rank. Experiments on language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that the proposed method matches the accuracy of standard models while providing practical speedups."
734,SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"Designing efficient exploration is central to Reinforcement Learning due to the fundamental problem posed by the exploration-exploitation dilemma. Bayesian exploration strategies like Thompson Sampling resolve this trade-off in a principled way by modeling and updating the distribution of the parameters of the action-value function, the outcome model of the environment. However, this technique becomes infeasible for complex environments due to the computational intractability of maintaining probability distributions over parameters of outcome models of corresponding complexity. Moreover, the approximation techniques introduced to mitigate this issue typically result in poor exploration-exploitation trade-offs, as observed in the case of deep neural network models with approximate posterior methods that have been shown to underperform in the deep bandit scenario. In this paper we introduce Sample Average Uncertainty (SAU), a simple and efficient uncertainty measure for contextual bandits. While Bayesian approaches like Thompson Sampling estimate outcomes uncertainty indirectly by first quantifying the variability over the parameters of the outcome model, SAU is a frequentist approach that directly estimates the uncertainty of the outcomes based on the value predictions. Importantly, we show theoretically that the uncertainty measure estimated by SAU asymptotically matches the uncertainty provided by Thompson Sampling, as well as its regret bounds. Because of its simplicity SAU can be seamlessly applied to deep contextual bandits as a very scalable drop-in replacement for epsilongreedy exploration. We confirm empirically our theory by showing that SAU-based exploration outperforms current state-of-the-art deep Bayesian bandit methods on several real-world datasets at modest computation cost, and make the code to reproduce our results available at https://github.com/ibm/sau-explore.","This paper proposes Sample Average Uncertainty (SAU) as a new exploration method for deep contextual bandits. SAU is a frequentist approach that directly estimates the uncertainty of the outcomes based on the value predictions. The authors show theoretically that SAU asymptotically matches the uncertainty provided by Thompson Sampling, as well as its regret bounds. Empirically, the authors show that the proposed SAU-based exploration outperforms current state-of-the-art deep Bayesian bandit methods on several real-world datasets at modest computation cost."
735,SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"To understand the relationship between behavior and neural activity, experiments in neuroscience often include an animal performing a repeated behavior such as a motor task. Recent progress in computer vision and deep learning has shown great potential in the automated analysis of behavior by leveraging large and high-quality video datasets. In this paper, we design Disentangled Behavior Embedding (DBE) to learn robust behavioral embeddings from unlabeled, multi-view, high-resolution behavioral videos across different animals and multiple sessions. We further combine DBE with a stochastic temporal model to propose Variational Disentangled Behavior Embedding (VDBE), an end-to-end approach that learns meaningful discrete behavior representations and generates interpretable behavioral videos. Our models learn consistent behavior representations by explicitly disentangling the dynamic behavioral factors (pose) from time-invariant, non-behavioral nuisance factors (context) in a deep autoencoder, and exploit the temporal structures of pose dynamics. Compared to competing approaches, DBE and VDBE enjoy superior performance on downstream tasks such as fine-grained behavioral motif generation and behavior decoding.","This paper proposes a method for disentangled behavior embedding from videos. The method is based on disentangling the dynamic behavioral factors (pose) from time-invariant, non-behavioral nuisance factors (context) in a deep autoencoder, and exploit the temporal structures of pose dynamics. The authors further combine DBE with a stochastic temporal model to propose Variational Disentangled Behavior Embedding (VDBE), an end-to-end approach that learns meaningful discrete behavior representations and generates interpretable behavioral videos. Compared to competing approaches, DBE and VDBE enjoy superior performance on downstream tasks such as fine-grained behavioral motif generation and behavior decoding."
736,SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"We introduce DMTET, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTET directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTET includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.",This paper proposes a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. The core of DMTET includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh.
737,SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"Mutual information (MI) is a fundamental measure of statistical dependence, with a myriad of applications to information theory, statistics, and machine learning. While it possesses many desirable structural properties, the estimation of highdimensional MI from samples suffers from the curse of dimensionality. Motivated by statistical scalability to high dimensions, this paper proposes sliced MI (SMI) as a surrogate measure of dependence. SMI is defined as an average of MI terms between one-dimensional random projections. We show that it preserves many of the structural properties of classic MI, while gaining scalable computation and efficient estimation from samples. Furthermore, and in contrast to classic MI, SMI can grow as a result of deterministic transformations. This enables leveraging SMI for feature extraction by optimizing it over processing functions of raw data to identify useful representations thereof. Our theory is supported by numerical studies of independence testing and feature extraction, which demonstrate the potential gains SMI offers over classic MI for high-dimensional inference.","This paper proposes sliced mutual information (SMI) as a surrogate measure of statistical dependence. SMI is defined as an average of MI terms between one-dimensional random projections. The authors show that SMI preserves many of the structural properties of classic MI, while gaining scalable computation and efficient estimation from samples. Furthermore, SMI can grow as a result of deterministic transformations. This enables leveraging SMI for feature extraction by optimizing it over processing functions of raw data to identify useful representations thereof."
738,SP:e220b348901b476c2afd95f97630fb5400582f40,"Recent advances in computationally efficient non-myopic Bayesian optimization offer improved query efficiency over traditional myopic methods like expected improvement, with only a modest increase in computational cost. These advances have been largely limited to unconstrained BO methods with only a few exceptions which require heavy computation. For instance, one existing multi-step lookahead constrained BO method [1] relies on computationally expensive unreliable bruteforce derivative-free optimization of a Monte Carlo rollout acquisition function. Methods that use the reparameterization trick for more efficient derivative-based optimization of non-myopic acquisition functions in the unconstrained setting, like sample average approximation and infinitesimal perturbation analysis, do not extend: constraints introduce discontinuities in the sampled acquisition function surface. Moreover, we argue here that being non-myopic is even more important in constrained problems because fear of violating constraints pushes myopic methods away from sampling the boundary between feasible and infeasible regions, slowing the discovery of optimal solutions with tight constraints. In this paper, we propose a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) supporting both sequential and batch settings. To enable fast acquisition function optimization, we develop a novel likelihoodratio-based unbiased estimator of the gradient of the two-step optimal acquisition function that does not use the reparameterization trick. In numerical experiments, 2-OPT-C typically improves query efficiency by 2x or more over previous methods, and in some cases by 10x or more.","This paper proposes a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) for both sequential and batch settings. The authors argue that being non-myopic is even more important in constrained problems because fear of violating constraints pushes myopic methods away from sampling the boundary between feasible and infeasible regions, slowing the discovery of optimal solutions with tight constraints. To enable fast acquisition function optimization, the authors develop a novel likelihoodratio-based unbiased estimator of the gradient of the two step optimal acquisition function that does not use the reparameterization trick. In numerical experiments, the proposed method is shown to improve query efficiency by 2x or more over previous methods, and in some cases by 10x."
739,SP:51fbd861422647912f275b48861ea3c4812afdc8,"A growing trend for value-based reinforcement learning (RL) algorithms is to capture more information than scalar value functions in the value network. One of the most well-known methods in this branch is distributional RL, which models return distribution instead of scalar value. In another line of work, hybrid reward architectures (HRA) in RL have studied to model source-specific value functions for each source of reward, which is also shown to be beneficial in performance. To fully inherit the benefits of distributional RL and hybrid reward architectures, we introduce Multi-Dimensional Distributional DQN (MD3QN), which extends distributional RL to model the joint return distribution from multiple reward sources. As a by-product of joint distribution modeling, MD3QN can capture not only the randomness in returns for each source of reward, but also the rich reward correlation between the randomness of different sources. We prove the convergence for the joint distributional Bellman operator and build our empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distribution and its Bellman target. In experiments, our method accurately models the joint return distribution in environments with richly correlated reward functions, and outperforms previous RL methods utilizing multi-dimensional reward functions in the control setting.","This paper proposes Multi-Dimensional Distributional DQN (MD3QN), which extends distributional RL to model the joint return distribution from multiple reward sources. The authors prove the convergence for the joint distributional Bellman operator and build an empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distributions and its Bellman target. In experiments, the authors show that the proposed method can accurately model the return distribution in environments with richly correlated reward functions and outperforms previous RL methods utilizing multi-dimensional reward functions in the control setting."
740,SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"In this paper we introduce CorticalFlow, a new geometric deep-learning model that, given a 3-dimensional image, learns to deform a reference template towards a targeted object. To conserve the template mesh’s topological properties, we train our model over a set of diffeomorphic transformations. This new implementation of a flow Ordinary Differential Equation (ODE) framework benefits from a small GPU memory footprint, allowing the generation of surfaces with several hundred thousand vertices. To reduce topological errors introduced by its discrete resolution, we derive numeric conditions which improve the manifoldness of the predicted triangle mesh. To exhibit the utility of CorticalFlow, we demonstrate its performance for the challenging task of brain cortical surface reconstruction. In contrast to current state-of-the-art, CorticalFlow produces superior surfaces while reducing the computation time from nine and a half minutes to one second. More significantly, CorticalFlow enforces the generation of anatomically plausible surfaces; the absence of which has been a major impediment restricting the clinical relevance of such surface reconstruction methods.","This paper proposes a method for 3D surface reconstruction based on a flow-based method. The method is based on the idea of learning to deform a reference template towards a target object, which is then used to train a neural network to generate a 3D model of the target object. The model is trained on a set of diffeomorphic transformations of the reference template, which are then used as input to the neural network. Theoretical analysis is performed to show that the proposed method is able to generate more realistic 3D surfaces than the state-of-the-art methods."
741,SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"Data deletion algorithms aim to remove the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. However, for sequences of deletions, most prior work in the non-convex setting gives valid guarantees only for sequences that are chosen independently of the models that are published. If people choose to delete their data as a function of the published models (because they don’t like what the models reveal about them, for example), then the update sequence is adaptive. In this paper, we give a general reduction from deletion guarantees against adaptive sequences to deletion guarantees against non-adaptive sequences, using differential privacy and its connection to max information. Combined with ideas from prior work which give guarantees for non-adaptive deletion sequences, this leads to extremely flexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletion guarantees for adaptive deletion sequences. We show in theory how prior work for non-convex models fails against adaptive deletion sequences, and use this intuition to design a practical attack against the SISA algorithm of Bourtoule et al. [2021] on CIFAR-10, MNIST, Fashion-MNIST.","This paper studies the problem of data deletion in the non-convex setting. In particular, the authors consider the case where data is deleted from a model (e.g., when the model is adaptive) and the deletion algorithm is adaptive. In this setting, the paper provides a general reduction from deletion guarantees for adaptive sequences to deletion guarantees against non-adaptive sequences, using differential privacy and its connection to max information. The paper also provides a practical attack against the SISA algorithm of Bourtoule et al. [2021] on CIFAR-10, MNIST, Fashion-MNIST."
742,SP:7150006590e268ab732c9be6c9048f67a377f956,"In this work, we address risk-averse Bayes-adaptive reinforcement learning. We pose the problem of optimising the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs). We show that a policy optimising CVaR in this setting is risk-averse to both the epistemic uncertainty due to the prior distribution over MDPs, and the aleatoric uncertainty due to the inherent stochasticity of MDPs. We reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Our experiments demonstrate that our approach significantly outperforms baseline approaches for this problem.","This paper studies the problem of risk-averse Bayes-adaptive reinforcement learning in MDPs. In particular, the authors consider the conditional value at risk (CVaR) of the total return in Bayesian Markov decision processes (MDPs). The authors show that a policy optimising CVaR in this setting is risk averse to both the epistemic uncertainty due to the prior distribution over MDP and the aleatoric uncertainty of the MDP. The authors reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. The experiments demonstrate that their approach significantly outperforms baseline approaches for this problem."
743,SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"This work studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero. In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also in terms of calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model. Lastly, while it is not shown that early stopping is necessary, it is shown that any univariate classifier satisfying a local interpolation property is inconsistent. 1 Overview and main result Deep networks trained with gradient descent seem to have no trouble adapting to arbitrary prediction problems, and are steadily displacing stalwart methods across many domains. In this work, we provide a mathematical basis for this good performance on arbitrary binary classification problems, considering the simplest possible networks: shallow ReLU networks where only the inner (inputfacing) weights are trained via vanilla gradient descent with a constant step size. The central contributions are as follows. 1. Fully general classification tasks. The joint distribution generating the (x, y) pairs only requires x to be bounded, and is otherwise arbitrary. In particular, the underlying distribution may be noisy, meaning the true conditional model of the labels, Pr[Y = 1|X = x], is arbitrary. In this setting, we show that as data, width, and training time increase, the logistic loss measured over the population converges to optimality over all measurable functions, which moreover implies that the induced conditional model (defined by a sigmoid mapping) converges to the true model, and the population misclassification rate also converges to optimality. This is in contrast with prior analyses of gradient descent, which either only consider the training risk [Allen-Zhu et al., 2018b, Du et al., 2019, Zou et al., 2018, Oymak and Soltanolkotabi, 2019, Song and Yang, 2019], or can only handle restricted conditional models [Allen-Zhu et al., 2018a, Arora et al., 2019, Cao and Gu, 2019, Nitanda and Suzuki, 2019, Ji and Telgarsky, 2020b, Chen et al., 2021]. 2. Adaptivity to data simplicity.","This paper studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero. In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of logistic and misclassification losses, but also in terms calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. The necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model. While it is not shown that early stopping is necessary, it was shown that any univariate classifier satisfying a local interpolation property is inconsistent."
744,SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"Recent years have witnessed an increasing use of coordinated accounts on social media, operated by misinformation campaigns to influence public opinion and manipulate social outcomes. Consequently, there is an urgent need to develop an effective methodology for coordinated group detection to combat the misinformation on social media. However, the sparsity of account activities on social media limits the performance of existing deep learning based coordination detectors as they can not exploit useful prior knowledge. Instead, the detectors incorporated with prior knowledge suffer from limited expressive power and poor performance. Therefore, in this paper we propose a coordination detection framework incorporating neural temporal point process with prior knowledge such as temporal logic or pre-defined filtering functions. Specifically, when modeling the observed data from social media with neural temporal point process, we jointly learn a Gibbs distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, we design a theoretically guaranteed variational inference approach to learn a mean-field approximation for it. Experimental results on a real-world dataset show the effectiveness of our proposed method compared to state-of-the-art model in both unsupervised and semi-supervised settings. We further apply our model on a COVID-19 Vaccine Tweets dataset. The detection result suggests presence of suspicious coordinated efforts on spreading misinformation about COVID-19 vaccines.", social media. This paper proposes a method for coordinated group detection based on neural temporal point process. The proposed method is based on a Gibbs distribution of group assignment based on how consistent an assignment is to the account embedding space and prior knowledge. The authors also design a theoretically guaranteed variational inference approach to learn a mean-field approximation for the Gibbs distribution. The experimental results on a real-world dataset show the effectiveness of the proposed method compared to state-of-the-art model in both unsupervised and semi-supervised settings.
745,SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"Data with low-dimensional nonlinear structure are ubiquitous in engineering and scientific problems. We study a model problem with such structure—a binary classification task that uses a deep fully-connected neural network to classify data drawn from two disjoint smooth curves on the unit sphere. Aside from mild regularity conditions, we place no restrictions on the configuration of the curves. We prove that when (i) the network depth is large relative to certain geometric properties that set the difficulty of the problem and (ii) the network width and number of samples are polynomial in the depth, randomly-initialized gradient descent quickly learns to correctly classify all points on the two curves with high probability. To our knowledge, this is the first generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties. Our analysis proceeds by a reduction to dynamics in the neural tangent kernel (NTK) regime, where the network depth plays the role of a fitting resource in solving the classification problem. In particular, via fine-grained control of the decay properties of the NTK, we demonstrate that when the network is sufficiently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization.","This paper studies the problem of binary classification in the neural tangent kernel (NTK) regime, where the network depth plays the role of a fitting resource in solving the classification problem. In particular, the authors show that when the network is sufficiently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization. This is the first generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties."
746,SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"Conditional Generative Adversarial Networks (cGAN) generate realistic images by incorporating class information into GAN. While one of the most popular cGANs is an auxiliary classifier GAN with softmax cross-entropy loss (ACGAN), it is widely known that training ACGAN is challenging as the number of classes in the dataset increases. ACGAN also tends to generate easily classifiable samples with a lack of diversity. In this paper, we introduce two cures for ACGAN. First, we identify that gradient exploding in the classifier can cause an undesirable collapse in early training, and projecting input vectors onto a unit hypersphere can resolve the problem. Second, we propose the Data-to-Data Cross-Entropy loss (D2D-CE) to exploit relational information in the class-labeled dataset. On this foundation, we propose the Rebooted Auxiliary Classifier Generative Adversarial Network (ReACGAN). The experimental results show that ReACGAN achieves state-of-the-art generation results on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets. We also verify that ReACGAN benefits from differentiable augmentations and that D2D-CE harmonizes with StyleGAN2 architecture. Model weights and a software package that provides implementations of representative cGANs and all experiments in our paper are available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.","This paper proposes a new auxiliary classifier for conditional generative adversarial networks (cGANs) based on the idea of projecting input vectors onto a unit hypersphere. The authors also propose a data-to-data cross-entropy loss (D2D-CE) to exploit relational information in the class-labeled dataset. The experimental results show that ReACGAN achieves state-of-the-art generation results on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets."
747,SP:080e80746a87228b156408ff649ab7a17f44e92d,"Policy Space Response Oracles (PSRO) is a reinforcement learning (RL) algorithm for two-player zero-sum games that has been empirically shown to find approximate Nash equilibria in large games. Although PSRO is guaranteed to converge to an approximate Nash equilibrium and can handle continuous actions, it may take an exponential number of iterations as the number of information states (infostates) grows. We propose Extensive-Form Double Oracle (XDO), an extensive-form double oracle algorithm for two-player zero-sum games that is guaranteed to converge to an approximate Nash equilibrium linearly in the number of infostates. Unlike PSRO, which mixes best responses at the root of the game, XDO mixes best responses at every infostate. We also introduce Neural XDO (NXDO), where the best response is learned through deep RL. In tabular experiments on Leduc poker, we find that XDO achieves an approximate Nash equilibrium in a number of iterations an order of magnitude smaller than PSRO. Experiments on a modified Leduc poker game and Oshi-Zumo show that tabular XDO achieves a lower exploitability than CFR with the same amount of computation. We also find that NXDO outperforms PSRO and NFSP on a sequential multidimensional continuous-action game. NXDO is the first deep RL method that can find an approximate Nash equilibrium in high-dimensional continuous-action sequential games. Experiment code is available at https://github.com/indylab/nxdo.","This paper proposes an extensive-form double oracle algorithm for two-player zero-sum games that is guaranteed to converge to an approximate Nash equilibrium linearly in the number of infostates. Unlike PSRO, which mixes best responses at the root of the game, XDO mixes best response at every infostate. The authors also introduce Neural XDO (NXDO), where the best response is learned through deep RL. In tabular experiments on Leduc poker, the authors show that XDO achieves an approximate equilibrium in a number of iterations an order of magnitude smaller than PSRO. The experimental results on Oshi-Zumo and Leduc Poker show that tabular XDO can achieve a lower exploitability than CFR with the same amount of computation. The experiments also show that the proposed XDO outperforms PSRO and NFSP on a sequential multidimensional continuous-action game."
748,SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either nodeor graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g., node clustering). Despite its wide range of possible applications, graph-level unsupervised representation learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency matrices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. We demonstrate the effectiveness of our proposed model for graph reconstruction, generation and interpolation and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.","This paper proposes a permutation-invariant variational autoencoder for graph-level unsupervised representation learning. The proposed method is based on the idea of variational auto-encoder (VAE), which learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. The authors demonstrate the effectiveness of their proposed model for graph reconstruction, generation and interpolation, and evaluate the expressive power of extracted representations for downstream graph level classification and regression."
749,SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"State-of-the-art Graph Neural Networks (GNNs) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the scope (i.e., receptive field). Beyond just a few layers, two fundamental challenges emerge: 1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. We propose a design principle to decouple the depth and scope of GNNs – to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local neighborhood into informative representation rather than oversmoothing the global graph into “white noise”. Theoretically, decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE) and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, our design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost.","This paper proposes to decouple the depth and scope of Graph Neural Networks (GNNs) by first extracting a localized subgraph as the bounded-size scope, and then applying a GNN of arbitrary depth on top of the subgraph. Theoretically, the paper shows that this decoupling improves the expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE), and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, the proposed method achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost."
750,SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"Normalizing flows are a widely used class of latent-variable generative models with a tractable likelihood. Affine-coupling models [Dinh et al., 2014, 2016] are a particularly common type of normalizing flows, for which the Jacobian of the latent-to-observable-variable transformation is triangular, allowing the likelihood to be computed in linear time. Despite the widespread usage of affine couplings, the special structure of the architecture makes understanding their representational power challenging. The question of universal approximation was only recently resolved by three parallel papers [Huang et al., 2020, Zhang et al., 2020, Koehler et al., 2020] – who showed reasonably regular distributions can be approximated arbitrarily well using affine couplings—albeit with networks with a nearly-singular Jacobian. As ill-conditioned Jacobians are an obstacle for likelihood-based training, the fundamental question remains: which distributions can be approximated using well-conditioned affine coupling flows? In this paper, we show that any log-concave distribution can be approximated using well-conditioned affine-coupling flows. In terms of proof techniques, we uncover and leverage deep connections between affine coupling architectures, underdamped Langevin dynamics (a stochastic differential equation often used to sample from Gibbs measures) and Hénon maps (a structured dynamical system that appears in the study of symplectic diffeomorphisms). Our results also inform the practice of training affine couplings: we approximate a padded version of the input distribution with iid Gaussians—a strategy which Koehler et al. [2020] empirically observed to result in better-conditioned flows, but had hitherto no theoretical grounding. Our proof can thus be seen as providing theoretical evidence for the benefits of Gaussian padding when training normalizing flows.","This paper studies the problem of approximating any log-concave distribution using affine-coupling flows. The authors show that any logconcavity distribution can be approximated using well-conditioned affine coupling flows. In particular, the authors prove that the Jacobian of the latent-to-observable-variable transformation is triangular, which allows the likelihood to be computed in linear time. The paper also provides theoretical evidence for the benefits of Gaussian padding when training normalizing flows. "
751,SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"Coupons allocation is an important tool for enterprises to increase the activity and loyalty of users on the e-commerce market. One fundamental problem related is how to allocate coupons within a fixed budget while maximizing users’ retention on the e-commerce platform. The online e-commerce environment is complicated and ever changing, so it requires the coupons allocation policy learning can quickly adapt to the changes of the company’s business strategy. Unfortunately, existing studies with a huge computation overhead can hardly satisfy the requirements of real-time and fast-response in the real world. Specifically, the problem of coupons allocation within a fixed budget is usually formulated as a Lagrangian problem. Existing solutions need to re-learn the policy once the value of Lagrangian multiplier variable λ is updated, causing a great computation overhead. Besides, a mature e-commerce market often faces tens of millions of users and dozens of types of coupons which construct the huge policy space, further increasing the difficulty of solving the problem. To tackle with above problems, we propose a budget constrained offline reinforcement learning and evaluation with λ-generalization (BCORLE(λ)) framework. The proposed method can help enterprises develop a coupons allocation policy which greatly improves users’ retention rate on the platform while ensuring the cost does not exceed the budget. Specifically, λ-generalization method is proposed to lead the policy learning process can be executed according to different λ values adaptively, avoiding re-learning new polices from scratch. Thus the computation overhead is greatly reduced. Further, a novel offline reinforcement learning method and an off-policy evaluation algorithm are proposed for policy learning and policy evaluation, respectively. Finally, experiments on the simulation platform and real-world e-commerce market validate the effectiveness of our approach. ∗Work was done during an internship at Alibaba Group. †Correspondence:yangqingyu@mail.xjtu.edu.cn 35th Conference on Neural Information Processing Systems (NeurIPS 2021).",This paper proposes a method for online coupons allocation in the context of online e-commerce. The authors propose a budget constrained offline reinforcement learning and evaluation with λ-generalization (BCORLE(λ)) framework. The proposed method can help enterprises develop a coupons allocation policy which greatly improves users’ retention rate on the platform while ensuring the cost does not exceed the budget. The experiments on the simulation platform and real-world e-Commerce market validate the effectiveness of the proposed method.
752,SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"Domain adaptation (DA) aims to alleviate the domain shift between source domain and target domain. Most DA methods require access to the source data, but often that is not possible (e.g. due to data privacy or intellectual property). In this paper, we address the challenging source-free domain adaptation (SFDA) problem, where the source pretrained model is adapted to the target domain in the absence of source data. Our method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. We capture this intrinsic structure by defining local affinity of the target data, and encourage label consistency among data with high local affinity. We observe that higher affinity should be assigned to reciprocal neighbors, and propose a self regularization loss to decrease the negative impact of noisy neighbors. Furthermore, to aggregate information with more context, we consider expanded neighborhoods with small affinity values. In the experimental results we verify that the inherent structure of the target features is an important source of information for domain adaptation. We demonstrate that this local structure can be efficiently captured by considering the local neighbors, the reciprocal neighbors, and the expanded neighborhood. Finally, we achieve state-of-the-art performance on several 2D image and 3D point cloud recognition datasets. Code is available in https://github.com/Albert0147/SFDA_neighbors.","This paper proposes a method for source-free domain adaptation (SFDA) where the source pretrained model is adapted to the target domain in the absence of source data. The method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. To capture this intrinsic structure, the authors define local affinity of the target data and encourage label consistency among data with high local affinity. The authors observe that higher affinity should be assigned to reciprocal neighbors, and propose a self regularization loss to decrease the negative impact of noisy neighbors. To aggregate information with more context, they consider expanded neighborhoods with small affinity values. The experimental results verify that the inherent structure of target features is an important source of information for domain adaptation."
753,SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"Learning representations from sets has become increasingly important with many applications in point cloud processing, graph learning, image/video recognition, and object detection. We introduce a geometrically-interpretable and generic pooling mechanism for aggregating a set of features into a fixed-dimensional representation. In particular, we treat elements of a set as samples from a probability distribution and propose an end-to-end trainable Euclidean embedding for sliced-Wasserstein distance to learn from set-structured data effectively. We evaluate our proposed pooling method on a wide variety of set-structured data, including point-cloud, graph, and image classification tasks, and demonstrate that our proposed method provides superior performance over existing set representation learning approaches. Our code is available at https://github.com/navid-naderi/PSWE.","This paper proposes a method for pooling features from a set of features into a fixed-dimensional representation. The proposed method is based on the idea of sampling from a probability distribution. The authors propose an end-to-end trainable Euclidean embedding for sliced-Wasserstein distance to learn from set-structured data effectively. They evaluate their proposed pooling method on a wide variety of set- structured data, including point cloud, graph, and image classification tasks, and demonstrate that their proposed method provides superior performance over existing set representation learning approaches."
754,SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"In this paper we consider the training stability of recurrent neural networks (RNNs), and propose a family of RNNs, namely SBO-RNN, that can be formulated using stochastic bilevel optimization (SBO). With the help of stochastic gradient descent (SGD), we manage to convert the SBO problem into an RNN where the feedforward and backpropagation solve the lower and upper-level optimization for learning hidden states and their hyperparameters, respectively. We prove that under mild conditions there is no vanishing or exploding gradient in training SBO-RNN. Empirically we demonstrate our approach with superior performance on several benchmark datasets, with fewer parameters, less training data, and much faster convergence. Code is available at https://zhang-vislab.github.io.","This paper proposes a new family of RNNs that can be formulated using stochastic bilevel optimization (SBO). The main idea is to convert the SBO problem into an RNN where the feedforward and backpropagation solve the lower and upper-level optimization for learning hidden states and their hyperparameters, respectively. The authors prove that under mild conditions there is no vanishing or exploding gradient in training SBO-RNN. Empirically, the authors demonstrate the effectiveness of the proposed method with fewer parameters, less training data, and faster convergence."
755,SP:d3a4300e21ca215334f256f0467a428470548fe4,"We study the online problem of minimizing power consumption in systems with multiple power-saving states. During idle periods of unknown lengths, an algorithm has to choose between power-saving states of different energy consumption and wake-up costs. We develop a learning-augmented online algorithm that makes decisions based on (potentially inaccurate) predicted lengths of the idle periods. The algorithm’s performance is near-optimal when predictions are accurate and degrades gracefully with increasing prediction error, with a worst-case guarantee almost identical to the optimal classical online algorithm for the problem. A key ingredient in our approach is a new algorithm for the online ski rental problem in the learning augmented setting with tight dependence on the prediction error. We support our theoretical findings with experiments.","This paper studies the online problem of minimizing power consumption in systems with multiple power-saving states. During idle periods of unknown lengths, an algorithm has to choose between different power saving states of different energy consumption and wake-up costs. The authors develop a learning-augmented online algorithm that makes decisions based on (potentially inaccurate) predicted lengths of the idle periods. The algorithm’s performance is near-optimal when predictions are accurate and degrades gracefully with increasing prediction error, with a worst-case guarantee almost identical to the optimal classical online algorithm for the problem."
756,SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"Current transfer learning algorithm designs mainly focus on the similarities between source and target tasks, while the impacts of the sample sizes of these tasks are often not sufficiently addressed. This paper proposes a mathematical framework for quantifying the transferability in multi-source transfer learning problems, with both the task similarities and the sample complexity of learning models taken into account. In particular, we consider the setup where the models learned from different tasks are linearly combined for learning the target task, and use the optimal combining coefficients to measure the transferability. Then, we demonstrate the analytical expression of this transferability measure, characterized by the sample sizes, model complexity, and the similarities between source and target tasks, which provides fundamental insights of the knowledge transferring mechanism and the guidance for algorithm designs. Furthermore, we apply our analyses for practical learning tasks, and establish a quantifiable transferability measure by exploiting a parameterized model. In addition, we develop an alternating iterative algorithm to implement our theoretical results for training deep neural networks in multi-source transfer learning tasks. Finally, experiments on image classification tasks show that our approach outperforms existing transfer learning algorithms in multi-source and few-shot scenarios.","This paper proposes a mathematical framework for quantifying the transferability in multi-source transfer learning problems, with both the task similarities and the sample complexity of learning models taken into account. In particular, the authors consider the setup where the models learned from different tasks are linearly combined for learning the target task, and use the optimal combining coefficients to measure transferability. The authors demonstrate the analytical expression of this transferability measure, characterized by the sample sizes, model complexity, and the similarities between source and target tasks, which provides fundamental insights of the knowledge transferring mechanism and the guidance for algorithm designs. Furthermore, they apply their analyses for practical learning tasks, and establish a quantifiable transferable measure by exploiting a parameterized model. In addition, they develop an alternating iterative algorithm to implement their theoretical results for training deep neural networks in multi source transfer learning tasks."
757,SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"Visual search is a ubiquitous and often challenging daily task, exemplified by looking for the car keys at home or a friend in a crowd. An intriguing property of some classical search tasks is an asymmetry such that finding a target A among distractors B can be easier than finding B among A. To elucidate the mechanisms responsible for asymmetry in visual search, we propose a computational model that takes a target and a search image as inputs and produces a sequence of eye movements until the target is found. The model integrates eccentricity-dependent visual recognition with target-dependent top-down cues. We compared the model against human behavior in six paradigmatic search tasks that show asymmetry in humans. Without prior exposure to the stimuli or task-specific training, the model provides a plausible mechanism for search asymmetry. We hypothesized that the polarity of search asymmetry arises from experience with the natural environment. We tested this hypothesis by training the model on augmented versions of ImageNet where the biases of natural images were either removed or reversed. The polarity of search asymmetry disappeared or was altered depending on the training protocol. This study highlights how classical perceptual properties can emerge in neural network models, without the need for task-specific training, but rather as a consequence of the statistical properties of the developmental diet fed to the model. All source code and data are publicly available at https: //github.com/kreimanlab/VisualSearchAsymmetry.","This paper proposes a model for visual search that aims to explain why there is an asymmetry in visual search tasks. Specifically, the authors propose a model that takes a target and a search image as inputs and produces a sequence of eye movements until the target is found. The model integrates eccentricity-dependent visual recognition with target-dependent top-down cues. The authors propose that the polarity of search asymmetry arises from experience with the natural environment and test this hypothesis by training the model on augmented versions of ImageNet where the biases of natural images are either removed or reversed depending on the training protocol."
758,SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"We study the problem of training certifiably robust models against adversarial examples. Certifiable training minimizes an upper bound on the worst-case loss over the allowed perturbation, and thus the tightness of the upper bound is an important factor in building certifiably robust models. However, many studies have shown that Interval Bound Propagation (IBP) training uses much looser bounds but outperforms other models that use tighter bounds. We identify another key factor that influences the performance of certifiable training: smoothness of the loss landscape. We find significant differences in the loss landscapes across many linear relaxation-based methods, and that the current state-of-the-arts method often has a landscape with favorable optimization properties. Moreover, to test the claim, we design a new certifiable training method with the desired properties. With the tightness and the smoothness, the proposed method achieves a decent performance under a wide range of perturbations, while others with only one of the two factors can perform well only for a specific range of perturbations. Our code is available at https://github.com/sungyoon-lee/LossLandscapeMatters.","This paper studies the problem of certifiable training for adversarial robustness. The authors propose a new method for certifiable robustness based on smoothing the loss landscape of linear relaxation-based methods. They show that the current state-of-the-art method, Interval Bound Propagation (IBP) training, uses much looser bounds but outperforms other models that use tighter bounds. In addition, they identify another key factor that influences the performance of certified training: smoothness of loss landscape. To test the claim, they design a new certified training method with the desired properties. "
759,SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"We consider the problem of online linear regression in the stochastic setting. We derive high probability regret bounds for online ridge regression and the forward algorithm. This enables us to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. Our study advocates for the use of the forward algorithm in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, we explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. We showcase this modification in linear bandit settings where it yields improved regret bounds. Last, we provide numerical experiments to illustrate our results and endorse our intuitions.","This paper studies the problem of online linear regression in the stochastic setting. The authors derive high probability regret bounds for online ridge regression and the forward algorithm. This enables them to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. In particular, they advocate for the use of forward algorithm in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, they explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. They showcase this modification in linear bandit settings where it yields improved regret bounds."
760,SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"Modern minimax problems, such as generative adversarial network and adversarial training, are often under a nonconvex-nonconcave setting, and developing an efficient method for such setting is of interest. Recently, two variants of the extragradient (EG) method are studied in that direction. First, a two-time-scale variant of the EG, named EG+, was proposed under a smooth structured nonconvexnonconcave setting, with a slowO(1/k) rate on the squared gradient norm, where k denotes the number of iterations. Second, another variant of EG with an anchoring technique, named extra anchored gradient (EAG), was studied under a smooth convex-concave setting, yielding a fast O(1/k) rate on the squared gradient norm. Built upon EG+ and EAG, this paper proposes a two-time-scale EG with anchoring, named fast extragradient (FEG), that has a fast O(1/k) rate on the squared gradient norm for smooth structured nonconvex-nonconcave problems; the corresponding saddle-gradient operator satisfies the negative comonotonicity condition. This paper further develops its backtracking line-search version, named FEG-A, for the case where the problem parameters are not available. The stochastic analysis of FEG is also provided.","This paper proposes a fast extragradient method for nonconvex-nonconcave minimax problems. The main contribution of this paper is the development of a two-time-scale variant of the standard EG method with an anchoring technique, named FEG, which has a fast O(1/k) rate on the squared gradient norm for smooth structured non-convolutional problems. This paper further develops its backtracking line-search version, called FEG-A, for the case where the problem parameters are not available. The theoretical analysis of FEG is provided."
761,SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"We study the problem of uniformity testing for statistical data that consists of rankings over m items, where the alternative class is restricted to Mallows models. Testing ranking data is challenging because of the size of the large domain that is factorial in m, therefore the tester needs to take advantage of some structure of the alternative class. We show that uniform distribution can be distinguished from Mallows model with O(m 1/2) samples based on simple pairwise statistics, which allows us to test uniformity using only two samples, if m is large enough. We also consider uniformity testing with central and local differential privacy (DP) constraints. We present a central DP algorithm that requires O(max{1/✏0, 1/ p m}), where ✏0 is the privacy budget parameter. Interestingly, our uniformity testing algorithm is straightforward to apply to the local DP scenario, since it works with binary statistics that is extracted from the ranking data. We carry out large-scale experiments, including m = 10, 000, to show that our uniformity testing algorithms scale gracefully with m.","This paper studies the problem of uniformity testing for statistical data that consists of rankings over m items, where the alternative class is restricted to Mallows models. The authors show that uniform distribution can be distinguished from Mallows model with O(m 1/2) samples based on simple pairwise statistics, which allows us to test uniformity using only two samples, if m is large enough. In addition, the authors consider uniformity test with central and local differential privacy (DP) constraints. The central DP algorithm requires O(max{1/\sqrt{0, 1/p m}), where $p$ is the privacy budget parameter. The local DP algorithm is straightforward to apply to binary statistics that is extracted from the ranking data."
762,SP:99a835191a3ba8372e391b6d3316e9b68e543295,"Greedy algorithms have long been a workhorse for learning graphical models, and more broadly for learning statistical models with sparse structure. In the context of learning directed acyclic graphs, greedy algorithms are popular despite their worst-case exponential runtime. In practice, however, they are very efficient. We provide new insight into this phenomenon by studying a general greedy scorebased algorithm for learning DAGs. Unlike edge-greedy algorithms such as the popular GES and hill-climbing algorithms, our approach is vertex-greedy and requires at most a polynomial number of score evaluations. We then show how recent polynomial-time algorithms for learning DAG models are a special case of this algorithm, thereby illustrating how these order-based algorithms can be rigorously interpreted as score-based algorithms. This observation suggests new score functions and optimality conditions based on the duality between Bregman divergences and exponential families, which we explore in detail. Explicit sample and computational complexity bounds are derived. Finally, we provide extensive experiments suggesting that this algorithm indeed optimizes the score in a variety of settings.","This paper studies a general greedy score-based algorithm for learning directed acyclic graphs (DAGs). The main idea is to learn a score function for each vertices of a DAG, which can be expressed as a function of the number of vertices in the graph. The authors show that this score function can be interpreted as a special case of order-based algorithms for learning DAGs, and provide a theoretical analysis of the duality between Bregman divergences and exponential families. They also provide a computational complexity bound for the algorithm. Finally, they provide extensive experiments to show that the proposed algorithm indeed optimizes the score."
763,SP:b60989706296b963b6671c01f22384978a334be1,"With the tremendous advances in the architecture and scale of convolutional neural networks (CNNs) over the past few decades, they can easily reach or even exceed the performance of humans in certain tasks. However, a recently discovered shortcoming of CNNs is that they are vulnerable to adversarial attacks. Although the adversarial robustness of CNNs can be improved by adversarial training, there is a trade-off between standard accuracy and adversarial robustness. From the neural architecture perspective, this paper aims to improve the adversarial robustness of the backbone CNNs that have a satisfactory accuracy. Under a minimal computational overhead, the introduction of a dilation architecture is expected to be friendly with the standard performance of the backbone CNN while pursuing adversarial robustness. Theoretical analyses on the standard and adversarial error bounds naturally motivate the proposed neural architecture dilation algorithm. Experimental results on real-world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm to balance the accuracy and adversarial robustness.",This paper proposes a neural architecture dilation algorithm to improve the adversarial robustness of the backbone CNNs. The proposed method is based on the fact that the standard and adversarial error bounds are well-known and can be derived from the theoretical analysis. Theoretical analysis is provided to show that the proposed method can achieve a trade-off between accuracy and robustness. Empirical results on real-world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm.
764,SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"We study the model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). In this setting, the agent works in two phases. In the exploration phase, the agent interacts with the environment and collects samples without the reward. In the planning phase, the agent is given a specific reward function and uses samples collected from the exploration phase to learn a good policy. We propose a new provably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP assumption, where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. We show that to obtain an ε-optimal policy for arbitrary reward function, UCRL-RFE needs to sample at most Õ(H5d2ε−2) episodes during the exploration phase. Here, H is the length of the episode, d is the dimension of the feature mapping. We also propose a variant of UCRL-RFE using Bernstein-type bonus and show that it needs to sample at most Õ(Hd(H + d)ε−2) to achieve an ε-optimal policy. By constructing a special class of linear Mixture MDPs, we also prove that for any reward-free algorithm, it needs to sample at least Ω̃(H2dε−2) episodes to obtain an ε-optimal policy. Our upper bound matches the lower bound in terms of the dependence on ε and the dependence on d if H ≥ d.","This paper studies model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). In this setting, in the exploration phase, the agent interacts with the environment and collects samples without the reward, and in the planning phase, in which the agent is given a specific reward function and uses samples collected from exploration phase to learn a good policy. The authors propose a new provably efficient algorithm called UCRL-RFE under the Linear Mixture MDP assumption, where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. They show that to obtain an $\epsilon$-optimal policy for arbitrary reward function, the algorithm needs to sample at most $O(\sqrt{H})$ episodes during exploration phase. The upper bound matches the lower bound in terms of the dependence on $H$ and $d$."
765,SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"Given taxi-ride counts information between departure and destination locations, how can we forecast their future demands? In general, given a data stream of events with seasonal patterns that innovate over time, how can we effectively and efficiently forecast future events? In this paper, we propose Shifting Seasonal Matrix Factorization approach, namely SSMF, that can adaptively learn multiple seasonal patterns (called regimes), as well as switching between them. Our proposed method has the following properties: (a) it accurately forecasts future events by detecting regime shifts in seasonal patterns as the data stream evolves; (b) it works in an online setting, i.e., processes each observation in constant time and memory; (c) it effectively realizes regime shifts without human intervention by using a lossless data compression scheme. We demonstrate that our algorithm outperforms state-of-the-art baseline methods by accurately forecasting upcoming events on three real-world data streams.",This paper proposes a method for forecasting future events in a data stream of events with seasonal patterns that evolve over time. The proposed method is based on shifting seasonal matrix factorization (SSMF) that learns multiple seasonal patterns (called regimes) as well as switching between them. The method is evaluated on three real-world data streams and compared to state-of-the-art methods. The results show that the proposed method can accurately forecast future events.
766,SP:e4bb07033001be4d04695ef058f426d49fe440be,"Assignment, a task to match a limited number of elements, is a fundamental 1 problem in informatics. Many assignment problems have no exact solvers due 2 to their NP-hardness or incomplete input, and their approximation algorithms 3 have been studied for a long time. However, individual practical applications 4 have various objective functions and prior assumptions, which usually differ from 5 academic studies. This gap hinders applying the algorithms to real problems 6 despite their theoretically ensured performance. In contrast, a learning-based 7 method can be a promising solution to fill the gap. To open a new vista for 8 real-world assignment problems, we propose a novel neural network architecture, 9 WeaveNet. Its core module, feature weaving layer, is stacked to model frequent 10 communication between elements in a parameter-efficient way for solving the 11 combinatorial problem of assignment. To evaluate the model, we approximated 12 one of the most popular non-linear assignment problems, stable matching with two 13 different strongly NP-hard settings. The experimental results showed its impressive 14 performance among the learning-based baselines. Furthermore, we achieved better 15 or comparative performance to the state-of-the-art algorithmic method, depending 16 on the size of problem instances. 17","This paper proposes a neural network architecture for solving the assignment problem. The core module, feature weaving layer, is stacked to model frequent communication between elements in a parameter-efficient way to solve the combinatorial problem of assignment. The experimental results showed its impressive performance among the learning-based baselines."
767,SP:8a559e21d45661eef427b310e5fe8488d5749137,"3D point cloud data is increasingly used in safety-critical applications such as autonomous driving. Thus, the robustness of 3D deep learning models against adversarial attacks becomes a major consideration. In this paper, we systematically study the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, we study MLP-based (PointNet), convolution-based (DGCNN), and transformer-based (PCT) 3D architectures. Through extensive experimentation, we demonstrate that appropriate applications of self-supervision can significantly enhance the robustness in 3D point cloud recognition, achieving considerable improvements compared to the standard adversarial training baseline. Our analysis reveals that local feature learning is desirable for adversarial robustness in point clouds since it limits the adversarial propagation between the point-level input perturbations and the model’s final output. This insight also explains the success of DGCNN and the jigsaw proxy task in achieving stronger 3D adversarial robustness.","This paper studies the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, the authors study MLP-based (PointNet), convolution-based DGCNN, and transformer-based PCT architectures. The authors demonstrate that appropriate applications of self supervision can significantly enhance the robustness in 3D 3d point cloud recognition. The analysis reveals that local feature learning is desirable for adversarial robustness since it limits the adversarial propagation between the point-level input perturbations and the model’s final output. This insight also explains the success of D GCNN and the jigsaw proxy task in achieving stronger 3D adversarial attacks."
768,SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"Optimization algorithms such as projected Newton’s method, FISTA, mirror descent and its variants enjoy near-optimal regret bounds and convergence rates, but suffer from a computational bottleneck of computing “projections"" in potentially each iteration (e.g., O(T ) regret of online mirror descent) [1, 2, 3, 4]. On the other hand, conditional gradient variants solve a linear optimization in each iteration, but result in suboptimal rates (e.g., O(T ) regret of online Frank-Wolfe) [5, 6, 7]. Motivated by this trade-off in runtime v/s convergence rates, we consider iterative projections of close-by points over widely-prevalent submodular base polytopes B(f). We develop a toolkit to speed up the computation of projections using both discrete and continuous perspectives (e.g., [8, 9, 10]). We subsequently adapt the away-step Frank-Wolfe algorithm to use this information and enable early termination. For the special case of cardinality based submodular polytopes, we improve the runtime of computing certain Bregman projections by a factor of Ω(n/ log(n)). Our theoretical results show orders of magnitude reduction in runtime in preliminary computational experiments.",This paper considers the problem of computing iterative projections of close-by points over submodular base polytopes. The authors propose a toolkit to speed up the computation of projections using both discrete and continuous perspectives. They also adapt the away-step Frank-Wolfe algorithm to use this information and enable early termination. The theoretical results show orders of magnitude reduction in runtime in preliminary computational experiments.
769,SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"We consider the question of learning the natural parameters of a k-parameter minimal exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We provide finite sample guarantees to achieve an (`2) error of α in the parameter estimation with sample complexity O(poly(k/α)) and computational complexity O(poly(k/α)). To establish these results, we show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family.","This paper studies the problem of learning the natural parameters of a k-parameter minimal exponential family from i.i.d. samples in a computationally and statistically efficient manner. The authors propose a new estimator that is consistent as well as asymptotically normal under mild conditions. They provide finite sample guarantees to achieve an (`2) error of α in the parameter estimation with sample complexity O(poly(k/alpha)) and computational complexity O(\sqrt{k}/\alpha) and show that, at the population level, their method can be viewed as the maximum likelihood estimation of a re-parametrized distribution belonging to the same class of exponential family."
770,SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"We consider the challenging problem of predicting intrinsic object properties from a single image by exploiting differentiable renderers. Many previous learning-based approaches for inverse graphics adopt rasterization-based renderers and assume naive lighting and material models, which often fail to account for non-Lambertian, specular reflections commonly observed in the wild. In this work, we propose DIBR++, a hybrid differentiable renderer which supports these photorealistic effects by combining rasterization and ray-tracing, taking the advantage of their respective strengths—speed and realism. Our renderer incorporates environmental lighting and spatially-varying material models to efficiently approximate light transport, either through direct estimation or via spherical basis functions. Compared to more advanced physics-based differentiable renderers leveraging path tracing, DIB-R++ is highly performant due to its compact and expressive shading model, which enables easy integration with learning frameworks for geometry, reflectance and lighting prediction from a single image without requiring any ground-truth. We experimentally demonstrate that our approach achieves superior material and lighting disentanglement on synthetic and real data compared to existing rasterization-based approaches and showcase several artistic applications including material editing and relighting.","This paper proposes a differentiable renderer for inverse graphics that combines the advantages of rasterization and ray-tracing. The main idea is to use a differentiability-based renderer to predict intrinsic object properties from a single image. The proposed renderer is based on the DIB-R++ architecture, which is a combination of differentiable physics-based and path tracing based renderers. In particular, the authors propose to use two different ways of rendering the light: direct estimation and spherical basis functions. The authors evaluate the performance of the proposed method on synthetic and real-world data. "
771,SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"Soft-argmax operation is commonly adopted in detection-based methods to localize the target position in a differentiable manner. However, training the neural network with soft-argmax makes the shape of the probability map unconstrained. Consequently, the model lacks pixel-wise supervision through the map during training, leading to performance degradation. In this work, we propose sampling-argmax, a differentiable training method that imposes implicit constraints to the shape of the probability map by minimizing the expectation of the localization error. To approximate the expectation, we introduce a continuous formulation of the output distribution and develop a differentiable sampling process. The expectation can be approximated by calculating the average error of all samples drawn from the output distribution. We show that sampling-argmax can seamlessly replace the conventional soft-argmax operation on various localization tasks. Comprehensive experiments demonstrate the effectiveness and flexibility of the proposed method. Code is available at https://github.com/Jeff-sjtu/sampling-argmax.","This paper proposes a differentiable training method for localization based on sampling-argmax. The main idea is to minimize the expectation of the localization error, which can be approximated by the average error of all samples drawn from the output distribution. The authors propose a continuous formulation of the distribution and develop a sampling process to approximate the expectation. Experiments show that the proposed method can seamlessly replace the conventional soft-arg max operation on various localization tasks."
772,SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"Graph Contrastive Learning (GCL) has emerged to learn generalizable representations from contrastive views. However, it is still in its infancy with two concerns: 1) changing the graph structure through data augmentation to generate contrastive views may mislead the message passing scheme, as such graph changing action deprives the intrinsic graph structural information, especially the directional structure in directed graphs; 2) since GCL usually uses predefined contrastive views with hand-picking parameters, it does not take full advantage of the contrastive information provided by data augmentation, resulting in incomplete structure information for models learning. In this paper, we design a directed graph data augmentation method called Laplacian perturbation and theoretically analyze how it provides contrastive information without changing the directed graph structure. Moreover, we present a directed graph contrastive learning framework, which dynamically learns from all possible contrastive views generated by Laplacian perturbation. Then we train it using multi-task curriculum learning to progressively learn from multiple easy-to-difficult contrastive views. We empirically show that our model can retain more structural features of directed graphs than other GCL models because of its ability to provide complete contrastive information. Experiments on various benchmarks reveal our dominance over the state-of-the-art approaches.","This paper proposes a directed graph data augmentation method called Laplacian perturbation to generate contrastive views for graph contrastive learning (GCL). The proposed method is based on the idea that the graph structure of directed graphs should not be changed too much, as it may mislead the message passing scheme. The paper also proposes a multi-task curriculum learning method to learn from multiple contrastive view. Experiments on various benchmarks show that the proposed method outperforms the state-of-the-art approaches."
773,SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"Existing work in language grounding typically study single environments. How do we build unified models that apply across multiple environments? We propose the multi-environment Symbolic Interactive Language Grounding benchmark (SILG), which unifies a collection of diverse grounded language learning environments under a common interface. SILG consists of grid-world environments that require generalization to new dynamics, entities, and partially observed worlds (RTFM, Messenger, NetHack), as well as symbolic counterparts of visual worlds that require interpreting rich natural language with respect to complex scenes (ALFWorld, Touchdown). Together, these environments provide diverse grounding challenges in richness of observation space, action space, language specification, and plan complexity. In addition, we propose the first shared model architecture for RL on these environments, and evaluate recent advances such as egocentric local convolution, recurrent state-tracking, entity-centric attention, and pretrained LM using SILG. Our shared architecture achieves comparable performance to environment-specific architectures. Moreover, we find that many recent modelling advances do not result in significant gains on environments other than the one they were designed for. This highlights the need for a multi-environment benchmark. Finally, the best models significantly underperform humans on SILG, which suggests ample room for future work. We hope SILG enables the community to quickly identify new methodologies for language grounding that generalize to a diverse set of environments and their associated challenges.","This paper proposes a new benchmark for language grounding that unifies a collection of diverse grounded language learning environments under a common interface. It consists of grid-world environments that require generalization to new dynamics, entities, and partially observed worlds (RTFM, Messenger, NetHack), as well as symbolic counterparts of visual worlds that require interpreting rich natural language with respect to complex scenes (ALFWorld, Touchdown). In addition, the authors propose the first shared model architecture for RL on these environments, and evaluate recent advances such as egocentric local convolution, recurrent state-tracking, entity-centric attention, and pretrained LM using SILG."
774,SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are “dense”, that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-ofthe-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.","This paper proposes a sparse version of the Vision Transformer (V-Transformer) that is scalable and competitive with the largest dense networks. In particular, the authors propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. The authors demonstrate the potential of V-V-transformer to scale vision models and train a 15B parameter model that attains 90.35% on ImageNet."
775,SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"Modern neural networks are often quite wide, causing large memory and computation costs. It is thus of great interest to train a narrower network. However, training narrow neural nets remains a challenging task. We ask two theoretical questions: Can narrow networks have as strong expressivity as wide ones? If so, does the loss function exhibit a benign optimization landscape? In this work, we provide partially affirmative answers to both questions for 1-hidden-layer networks with fewer than n (sample size) neurons when the activation is smooth. First, we prove that as long as the width m ≥ 2n/d (where d is the input dimension), its expressivity is strong, i.e., there exists at least one global minimizer with zero training loss. Second, we identify a nice local region with no local-min or saddle points. Nevertheless, it is not clear whether gradient descent can stay in this nice region. Third, we consider a constrained optimization formulation where the feasible region is the nice local region, and prove that every KKT point is a nearly global minimizer. It is expected that projected gradient methods converge to KKT points under mild technical conditions, but we leave the rigorous convergence analysis to future work. Thorough numerical results show that projected gradient methods on this constrained formulation significantly outperform SGD for training narrow neural nets.","This paper studies the problem of training narrow neural networks with fewer than n neurons. The authors prove that as long as the width m > 2n/d (where d is the input dimension), there exists at least one global minimizer with zero training loss. They also identify a nice local region with no local-min or saddle points and show that it is not clear whether gradient descent can stay in this nice region. In addition, they consider a constrained optimization formulation where the feasible region is the nice region, and prove that every KKT point is a nearly global minimiser. They show that projected gradient methods converge to KKT points under mild technical conditions."
776,SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"Existing risk-aware multi-armed bandit models typically focus on risk measures of individual options such as variance. As a result, they cannot be directly applied to important real-world online decision making problems with correlated options. In this paper, we propose a novel Continuous Mean-Covariance Bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent’s objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, we consider three feedback settings, i.e., full-information, semi-bandit and full-bandit feedback. We propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the superiority of our algorithms. To the best of our knowledge, this is the first work that considers option correlation in risk-aware bandits and explicitly quantifies how arbitrary covariance structures impact the learning performance. The novel analytical techniques we developed for exploiting the estimated covariance to build concentration and bounding the risk of selected actions based on sampling strategy properties can likely find applications in other bandit analysis and be of independent interests.","This paper proposes a continuous mean-covariance bandit (CMCB) model that explicitly takes into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent’s objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, the authors consider three feedback settings, i.e., full-information, semi-bandit, and full-Bandit feedback. They propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the superiority of their algorithms."
777,SP:472a90bb175b0286765c5a47b040e1a58f594a05,"Given a data matrixX ∈ Rm×n + with non-negative entries, a Positive Semidefinite (PSD) factorization of X is a collection of r × r-dimensional PSD matrices {Ai} and {Bj} satisfying the condition Xij = tr(AiBj) for all i ∈ [m], j ∈ [n]. PSD factorizations are fundamentally linked to understanding the expressiveness of semidefinite programs as well as the power and limitations of quantum resources in information theory. The PSD factorization task generalizes the Nonnegative Matrix Factorization (NMF) problem in which we seek a collection of r-dimensional non-negative vectors {ai} and {bj} satisfying Xij = ai bj, for all i ∈ [m], j ∈ [n] – one can recover the latter problem by choosing matrices in the PSD factorization to be diagonal. The most widely used algorithm for computing NMFs of a matrix is the Multiplicative Update algorithm developed by Lee and Seung, in which non-negativity of the updates is preserved by scaling with positive diagonal matrices. In this paper, we describe a non-commutative extension of Lee-Seung’s algorithm, which we call the Matrix Multiplicative Update (MMU) algorithm, for computing PSD factorizations. The MMU algorithm ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSD matrices, and it retains the simplicity of implementation that the multiplicative update algorithm for NMF enjoys. Building on the MajorizationMinimization framework, we show that under our update scheme the squared loss objective is non-increasing and fixed points correspond to critical points. The analysis relies on Lieb’s Concavity Theorem. Beyond PSD factorizations, we show that the MMU algorithm can be also used as a primitive to calculate blockdiagonal PSD factorizations and tensor PSD factorizations. We demonstrate the utility of our method with experiments on real and synthetic data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).","This paper proposes a non-commutative extension of Lee-Seung’s Multiplicative Update (MMU) algorithm for computing Positive Semidefinite (PSD) factorization of a data matrix X, which is a collection of r-dimensional PSD matrices satisfying the condition Xij = tr(AiBj) for all i\in [m, j\in n]. The authors show that the MMU algorithm ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSD matrix matrices, and it retains the simplicity of implementation that the multiplicative update algorithm for NMF enjoys. The authors also show that under their update scheme the squared loss objective is non-increasing and fixed points correspond to critical points. The analysis relies on Lieb's concavity theorem."
778,SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"Domain Generalization (DG) aims to train a model, from multiple observed source domains, in order to perform well on unseen target domains. To obtain the generalization capability, prior DG approaches have focused on extracting domaininvariant information across sources to generalize on target domains, while useful domain-specific information which strongly correlates with labels in individual domains and the generalization to target domains is usually ignored. In this paper, we propose meta-Domain Specific-Domain Invariant (mDSDI) a novel theoretically sound framework that extends beyond the invariance view to further capture the usefulness of domain-specific information. Our key insight is to disentangle features in the latent space while jointly learning both domain-invariant and domainspecific features in a unified framework. The domain-specific representation is optimized through the meta-learning framework to adapt from source domains, targeting a robust generalization on unseen domains. We empirically show that mDSDI provides competitive results with state-of-the-art techniques in DG. A further ablation study with our generated dataset, Background-Colored-MNIST, confirms the hypothesis that domain-specific is essential, leading to better results when compared with only using domain-invariant.","This paper proposes a meta-learning framework for domain generalization (DG) based on meta-domain specific-domain invariant (mDSDI) that extends beyond the invariance view to further capture the usefulness of domain-specific information. The key insight is to disentangle features in the latent space while jointly learning both domain-invariant and domainspecific features in a unified framework. The domain specific representation is optimized through the meta learning framework to adapt from source domains, targeting a robust generalization on unseen domains. Empirical results show that mDSDI provides competitive results with state-of-the-art techniques in DG."
779,SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128⇥128, 4.59 on ImageNet 256⇥256, and 7.72 on ImageNet 512⇥512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256⇥256 and 3.85 on ImageNet 512⇥512.","This paper presents a method for improving the quality of image synthesis using a combination of ablations and classifier guidance. The ablations are used for unconditional image synthesis, while the guidance is used for conditional image synthesis. The authors show that the ablations improve FID on ImageNet 128, 256, and 512 by a factor of 4. The guidance is a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. The paper also shows that the guidance can be combined with upsampling diffusion models to further improve the quality."
780,SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"In this work, we propose to leverage out-of-distribution samples, i.e., unlabeled samples coming from outside target classes, for improving few-shot learning. Specifically, we exploit the easily available out-of-distribution samples (e.g., from base classes) to drive the classifier to avoid irrelevant features by maximizing the distance from prototypes to out-of-distribution samples while minimizing that to in-distribution samples (i.e., support, query data). Our approach is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures. Our code is available at https://github.com/VinAIResearch/poodle.","This paper proposes a method to improve few-shot learning by leveraging out-of-distribution samples. The proposed method is based on the idea of Poodle, which is an extension of the Poodle algorithm. The key idea is to minimize the distance from prototypes to out- of-distributed samples while maximizing the distance between prototypes and out of distribution samples (i.e., support, query data). The method is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures."
781,SP:b1f65724926f136979829b7a6c870bc31f38f591,"In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World.","This paper studies the problem of prioritized sampling in reinforcement learning. In particular, the authors propose two methods to compute the prioritization weight, namely ReMERN and ReMERT. Theoretical analysis is provided to show that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. The authors also provide theoretical justifications for previous criteria, such as TD error and recentness, which are mostly heuristically designed. In addition, they propose two new algorithms to compute prioritization weights: ReMerner, which learns an error network, and reMERT, which exploits the temporal ordering of states."
782,SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,"We address the problem of sequential prediction with expert advice in a nonstationary environment with long-term memory guarantees in the sense of Bousquet and Warmuth [4]. We give a linear-time algorithm that improves on the best known regret bounds [27]. This algorithm incorporates a relative entropy projection step. This projection is advantageous over previous weight-sharing approaches in that weight updates may come with implicit costs as in for example portfolio optimization. We give an algorithm to compute this projection step in linear time, which may be of independent interest.",This paper studies the problem of sequential prediction with expert advice in a nonstationary environment with long-term memory guarantees in the sense of Bousquet and Warmuth [4]. The authors give a linear-time algorithm that improves on the best known regret bounds [27]. This algorithm incorporates a relative entropy projection step. This projection is advantageous over previous weight-sharing approaches in that weight updates may come with implicit costs as in for example portfolio optimization.
783,SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"We consider the following variant of contextual linear bandits motivated by routing applications in navigational engines and recommendation systems. We wish to learn a hidden d-dimensional value w∗. Every round, we are presented with a subset Xt ⊆ R of possible actions. If we choose (i.e. recommend to the user) action xt, we obtain utility 〈xt, w∗〉 but only learn the identity of the best action arg maxx∈Xt〈x,w〉. We design algorithms for this problem which achieve regret O(d log T ) and exp(O(d log d)). To accomplish this, we design novel cutting-plane algorithms with low “regret” – the total distance between the true point w∗ and the hyperplanes the separation oracle returns. We also consider the variant where we are allowed to provide a list of several recommendations. In this variant, we give an algorithm with O(d log d) regret and list size poly(d). Finally, we construct nearly tight algorithms for a weaker variant of this problem where the learner only learns the identity of an action that is better than the recommendation. Our results rely on new algorithmic techniques in convex geometry (including a variant of Steiner’s formula for the centroid of a convex set) which may be of independent interest.","This paper studies the problem of contextual linear bandits, which is motivated by routing applications in navigational engines and recommendation systems. In this problem, the learner is presented with a subset Xt ⊆ R of possible actions and can only learn the identity of the best action arg maxx. The authors propose algorithms for this problem which achieve regret O(d log T^T) and exp(O(dlog d)). To accomplish this, they design novel cutting-plane algorithms with low “regret” – the total distance between the true point w∗ and the hyperplanes the separation oracle returns. In addition, they also consider the variant where we are allowed to provide a list of several recommendations and give an algorithm with O(log d log d) regret and list size poly(d). Finally, they construct nearly tight algorithms for a weaker variant of this problem where the learners only learns an action that is better than the recommendation. Their results rely on new algorithmic techniques in convex geometry."
784,SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"Automated machine learning (AutoML) can make data scientists more productive. But if machine learning is totally automated, that leaves no room for data scientists to apply their intuition. Hence, data scientists often prefer not total but gradual automation, where they control certain choices and AutoML explores the rest. Unfortunately, gradual AutoML is cumbersome with state-of-the-art tools, requiring large non-compositional code changes. More concise compositional code can be achieved with combinators, a powerful concept from functional programming. This paper introduces a small set of orthogonal combinators for composing machinelearning operators into pipelines. It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers. On that foundation, this paper presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study.","This paper introduces a small set of orthogonal combinators for composing machine learning operators into pipelines. It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers. The paper presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study."
785,SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"Finding neural network weights that generalize well from small datasets is difficult. A promising approach is to learn a weight initialization such that a small number of weight changes results in low generalization error. We show that this form of meta-learning can be improved by letting the learning algorithm decide which weights to change, i.e., by learning where to learn. We find that patterned sparsity emerges from this process, with the pattern of sparsity varying on a problem-byproblem basis. This selective sparsity results in better generalization and less interference in a range of few-shot and continual learning problems. Moreover, we find that sparse learning also emerges in a more expressive model where learning rates are meta-learned. Our results shed light on an ongoing debate on whether meta-learning can discover adaptable features and suggest that learning by sparse gradient descent is a powerful inductive bias for meta-learning systems.","This paper studies the problem of meta-learning in the context of sparse gradient descent (SGD). The authors propose to learn a weight initialization such that a small number of weight changes results in low generalization error. They show that patterned sparsity emerges from this process, with the pattern of sparsity varying on a problem-by-problem basis. This selective sparsity results in better generalization and less interference in a range of few-shot and continual learning problems. The authors also show that sparse learning also emerges in a more expressive model where learning rates are meta-learned."
786,SP:05037e1850003a725a466b64d3e32aa2aed458fb,"We consider shared response modeling, a multi-view learning problem where one wants to identify common components from multiple datasets or views. We introduce Shared Independent Component Analysis (ShICA) that models each view as a linear transform of shared independent components contaminated by additive Gaussian noise. We show that this model is identifiable if the components are either non-Gaussian or have enough diversity in noise variances. We then show that in some cases multi-set canonical correlation analysis can recover the correct unmixing matrices, but that even a small amount of sampling noise makes Multiset CCA fail. To solve this problem, we propose to use joint diagonalization after Multiset CCA, leading to a new approach called ShICA-J. We show via simulations that ShICA-J leads to improved results while being very fast to fit. While ShICA-J is based on second-order statistics, we further propose to leverage non-Gaussianity of the components using a maximum-likelihood method, ShICA-ML, that is both more accurate and more costly. Further, ShICA comes with a principled method for shared components estimation. Finally, we provide empirical evidence on fMRI and MEG datasets that ShICA yields more accurate estimation of the components than alternatives.","This paper proposes a new method for multi-view learning based on shared independent component analysis (ShICA) that models each view as a linear transform of shared independent components contaminated by additive Gaussian noise. The authors show that this model is identifiable if the components are either non-Gaussian or have enough diversity in noise variances. They then show that in some cases multi-set canonical correlation analysis can recover the correct unmixing matrices, but that even a small amount of sampling noise makes Multiset CCA fail. To solve this problem, the authors propose to use joint diagonalization after multiset correlation analysis, leading to a new approach called ShICA-J, which is based on second-order statistics. They further propose to leverage non-gaussianity of the components using a maximum-likelihood method, which can be both more accurate and more costly. Finally, they provide empirical evidence on fMRI and MEG datasets that ShICA yields more accurate estimation of components than alternatives."
787,SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train “human-aware” agents (“behavioral cloning play”, or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines.","This paper studies the problem of how to train agents that collaborate well with human partners without using human data. The authors argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, they find that a surprisingly simple approach is highly effective. They train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method they call Fictitious Co-Play (FCP). Their experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. They find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans report a strong subjective preference to partnering with FCP agent over all baselines."
788,SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"We propose FACtored Multi-Agent Centralised policy gradients (FACMAC), a new method for cooperative multi-agent reinforcement learning in both discrete and continuous action spaces. Like MADDPG, a popular multi-agent actor-critic method, our approach uses deep deterministic policy gradients to learn policies. However, FACMAC learns a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX, a popular multi-agent Q-learning algorithm. However, unlike QMIX, there are no inherent constraints on factoring the critic. We thus also employ a nonmonotonic factorisation and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics. In addition, FACMAC uses a centralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent’s action space separately as in MADDPG. This allows for more coordinated policy changes and fully reaps the benefits of a centralised critic. We evaluate FACMAC on variants of the multi-agent particle environments, a novel multi-agent MuJoCo benchmark, and a challenging set of StarCraft II micromanagement tasks. Empirical results demonstrate FACMAC’s superior performance over MADDPG and other baselines on all three domains.","This paper proposes a new multi-agent actor-critic method for cooperative multi-Agent reinforcement learning. The proposed method is based on the factored critic method of MADDPG, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX. The authors also employ a centralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent’s action space separately. The experimental results demonstrate the superior performance of the proposed method on MuJoCo and StarCraft II tasks."
789,SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"In neuroscience, classical Hopfield networks are the standard biologically plausible model of long-term memory, relying on Hebbian plasticity for storage and attractor dynamics for recall. In contrast, memory-augmented neural networks in machine learning commonly use a key-value mechanism to store and read out memories in a single step. Such augmented networks achieve impressive feats of memory compared to traditional variants, yet their biological relevance is unclear. We propose an implementation of basic key-value memory that stores inputs using a combination of biologically plausible three-factor plasticity rules. The same rules are recovered when network parameters are meta-learned. Our network performs on par with classical Hopfield networks on autoassociative memory tasks and can be naturally extended to continual recall, heteroassociative memory, and sequence learning. Our results suggest a compelling alternative to the classical Hopfield network as a model of biological long-term memory.",This paper proposes a new neural network architecture for long-term memory. The key idea is to use a key-value mechanism to store and read out memories in a single step. The authors propose a combination of biologically plausible three-factor plasticity rules. The network is trained using meta-learning. The experimental results show that the proposed architecture is comparable to the classical Hopfield network in terms of performance on memory tasks.
790,SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"Pairwise learning refers to learning tasks where the loss function depends on a pair of instances. It instantiates many important machine learning tasks such as bipartite ranking and metric learning. A popular approach to handle streaming data in pairwise learning is an online gradient descent (OGD) algorithm, where one needs to pair the current instance with a buffering set of previous instances with a sufficiently large size and therefore suffers from a scalability issue. In this paper, we propose simple stochastic and online gradient descent methods for pairwise learning. A notable difference from the existing studies is that we only pair the current instance with the previous one in building a gradient direction, which is efficient in both the storage and computational complexity. We develop novel stability results, optimization, and generalization error bounds for both convex and nonconvex as well as both smooth and nonsmooth problems. We introduce novel techniques to decouple the dependency of models and the previous instance in both the optimization and generalization analysis. Our study resolves an open question on developing meaningful generalization bounds for OGD using a buffering set with a very small fixed size. We also extend our algorithms and stability analysis to develop differentially private SGD algorithms for pairwise learning which significantly improves the existing results.","This paper studies the problem of pairwise learning, where the loss function depends on a pair of instances. The authors propose stochastic and online gradient descent methods for solving the problem. The main contribution of this paper is to develop stability results, optimization, and generalization error bounds for both convex and nonconvex problems. The paper also introduces novel techniques to decouple the dependency of models and the previous instance in both the optimization and the generalization analysis."
791,SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"We introduce REDO, a class-agnostic framework to REconstruct the Dynamic Objects from RGBD or calibrated videos. Compared to prior work, our problem setting is more realistic yet more challenging for three reasons: 1) due to occlusion or camera settings an object of interest may never be entirely visible, but we aim to reconstruct the complete shape; 2) we aim to handle different object dynamics including rigid motion, non-rigid motion, and articulation; 3) we aim to reconstruct different categories of objects with one unified framework. To address these challenges, we develop two novel modules. First, we introduce a canonical 4D implicit function which is pixel-aligned with aggregated temporal visual cues. Second, we develop a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. We study the efficacy of REDO in extensive experiments on synthetic RGBD video datasets SAIL-VOS 3D and DeformingThings4D++, and on real-world video data 3DPW. We find REDO outperforms state-of-the-art dynamic reconstruction methods by a margin. In ablation studies we validate each developed component.","This paper proposes REDO, a class-agnostic framework to reconstruct the Dynamic Objects from RGBD or calibrated videos. Compared to prior work, the problem setting is more realistic yet more challenging for three reasons: 1) due to occlusion or camera settings an object of interest may never be entirely visible, but the aim is to reconstruct its complete shape; 2) the authors aim to handle different object dynamics including rigid motion, non-rigid motion, and articulation; 3) they aim to reconstruct different categories of objects with one unified framework. To address these challenges, the authors develop two novel modules. First, they introduce a canonical 4D implicit function which is pixel-aligned with aggregated temporal visual cues. Second, they develop a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. The experiments on synthetic RGBD video datasets SAIL-VOS 3D and DeformingThings4D++, and on real-world video data 3DPW, show that REDO outperforms state-of-the-art dynamic reconstruction methods."
792,SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. This family of methods arises in many machine learning tasks and is used to obtain approximate solutions of a linear system Ā✓ = b̄ for which Ā and b̄ can only be accessed through random estimates {(An,bn) : n 2 N⇤}. Our analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. We derive high probability bounds on the performance of LSA under weaker conditions on the sequence {(An,bn) : n 2 N⇤} than previous works. However, in contrast, we establish polynomial concentration bounds with order depending on the stepsize. We show that our conclusions cannot be improved without additional assumptions on the sequence of random matrices {An : n 2 N⇤}, and in particular that no Gaussian or exponential high probability bounds can hold. Finally, we pay a particular attention to establishing bounds with sharp order with respect to the number of iterations and the stepsize and whose leading terms contain the covariance matrices appearing in the central limit theorems.","This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. The analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. The authors derive high probability bound on the performance of LSA under weaker conditions on the sequence {(An,bn) : n 2 N⇤} than previous works. However, in contrast, the authors establish polynomial concentration bounds with order depending on the stepsize and the leading terms contain the covariance matrices."
793,SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"We extend the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes (MDPs) to average-reward MDPs. Our contributions include general convergent off-policy inter-option learning algorithms, intra-option algorithms for learning values and models, as well as samplebased planning variants of our learning algorithms. Our algorithms and convergence proofs extend those recently developed by Wan, Naik, and Sutton. We also extend the notion of option-interrupting behavior from the discounted to the average-reward formulation. We show the efficacy of the proposed algorithms with experiments on a continuing version of the Four-Room domain.","This paper extends the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes (MDPs) to average-reward MDPs. The authors propose general convergent off-policy inter-option learning algorithms, intra-option algorithms for learning values and models, as well as sample-based planning variants of the learning algorithms. They also extend the notion of option-interrupting behavior from the discounted to the average reward formulation. They show the efficacy of the proposed algorithms with experiments on a continuing version of the Four-Room domain."
794,SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"Visual Transformers (VTs) are emerging as an architectural paradigm alternative to Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose an auxiliary selfsupervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data is scarce. Our task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different VTs and datasets, we show that our method can improve (sometimes dramatically) the final accuracy of the VTs. Our code is available at: https://github.com/ yhlleo/VTs-Drloc.","This paper proposes an auxiliary self-supervised task for visual transformers (VTs) that is designed to encourage the VTs to learn spatial relations within an image and makes the VT training much more robust when training data is scarce. The proposed task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. The experimental results show that the proposed task can improve the final accuracy of the trained VTs and it can improve (sometimes dramatically) the final performance."
795,SP:0132ef17585e293b23e9dc45189c0989d829b52a,"Label-free alignment between datasets collected at different times, locations, or by different instruments is a fundamental scientific task. Hyperbolic spaces have recently provided a fruitful foundation for the development of informative representations of hierarchical data. Here, we take a purely geometric approach for label-free alignment of hierarchical datasets and introduce hyperbolic Procrustes analysis (HPA). HPA consists of new implementations of the three prototypical Procrustes analysis components: translation, scaling, and rotation, based on the Riemannian geometry of the Lorentz model of hyperbolic space. We analyze the proposed components, highlighting their useful properties for alignment. The efficacy of HPA, its theoretical properties, stability and computational efficiency are demonstrated in simulations. In addition, we showcase its performance on three batch correction tasks involving gene expression and mass cytometry data. Specifically, we demonstrate high-quality unsupervised batch effect removal from data acquired at different sites and with different technologies that outperforms recent methods for label-free alignment in hyperbolic spaces.","This paper proposes a new method for label-free alignment of hierarchical datasets in hyperbolic spaces. The proposed method is based on Procrustes analysis, which consists of three components: translation, scaling, and rotation. The three components are based on the Riemannian geometry of the Lorentz model of the space. The authors analyze the proposed components, highlighting their useful properties for alignment. The efficacy of HPA, its theoretical properties, stability and computational efficiency are demonstrated in simulations. In addition, the authors demonstrate its performance on three batch correction tasks. "
796,SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"Privacy-protected microdata are often the desired output of a differentially private algorithm since microdata is familiar and convenient for downstream users. However, there is a statistical price for this kind of convenience. We show that an uncertainty principle governs the trade-off between accuracy for a population of interest (“sum query”) vs. accuracy for its component sub-populations (“point queries”). Compared to differentially private query answering systems that are not required to produce microdata, accuracy can degrade by a logarithmic factor. For example, in the case of pure differential privacy, without the microdata requirement, one can provide noisy answers to the sum query and all point queries while guaranteeing that each answer has squared error O(1/ ). With the microdata requirement, one must choose between allowing an additional log(d) factor (d is the number of point queries) for some point queries or allowing an extra O(d) factor for the sum query. We present lower bounds for pure, approximate, and concentrated differential privacy. We propose mitigation strategies and create a collection of benchmark datasets that can be used for public study of this problem.","This paper studies the trade-off between accuracy for a population of interest (“sum query”) vs. accuracy for its component sub-populations (‘point queries’) in differentially private query answering systems that are not required to produce microdata. The authors show that an uncertainty principle governs the tradeoff between the accuracy for the sum query and the accuracy of the other sub-population. They provide lower bounds for pure, approximate, and concentrated differential privacy. They propose mitigation strategies and create a collection of benchmark datasets that can be used for public study."
797,SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"Goal-conditioned reinforcement learning (RL) usually suffers from sparse reward and inefficient exploration in long-horizon tasks. Planning can find the shortest path to a distant goal that provides dense reward/guidance but is inaccurate without a precise environment model. We show that RL and planning can collaboratively learn from each other to overcome their own drawbacks. In “CO-PILOT”, a learnable path-planner and an RL agent produce dense feedback to train each other on a curriculum of tree-structured sub-tasks. Firstly, the planner recursively decomposes a long-horizon task to a tree of sub-tasks in a top-down manner, whose layers construct coarse-to-fine sub-task sequences as plans to complete the original task. The planning policy is trained to minimize the RL agent’s cost of completing the sequence in each layer from top to bottom layers, which gradually increases the sub-tasks and thus forms an easy-to-hard curriculum for the planner. Next, a bottom-up traversal of the tree trains the RL agent from easier sub-tasks with denser rewards on bottom layers to harder ones on top layers and collects its cost on each sub-task train the planner in the next episode. CO-PILOT repeats this mutual training for multiple episodes before switching to a new task, so the RL agent and planner are fully optimized to facilitate each other’s training. We compare CO-PILOT with RL (SAC, HER, PPO), planning (RRT*, NEXT, SGT), and their combination (SoRB) on navigation and continuous control tasks. CO-PILOT significantly improves the success rate and sample efficiency. Our code is available at https://github.com/Shuang-AO/CO-PILOT.","This paper proposes a method to train a planner and a goal-conditioned RL agent to jointly learn from each other on a curriculum of tree-structured sub-tasks. The planning policy is trained to minimize the RL agent’s cost of completing the sequence in each layer from top to bottom layers of the tree, which gradually increases the sub-task complexity. The bottom-up traversal of the trees trains the planner from easier sub- tasks with denser rewards on bottom layers to harder ones on top layers and collects its cost to train the planner in the next episode. The method is evaluated on navigation and continuous control tasks."
798,SP:9911693a04a300b5a93634fb0267ef83e5489d77,"As black box explanations are increasingly being employed to establish model credibility in high stakes settings, it is important to ensure that these explanations are accurate and reliable. However, prior work demonstrates that explanations generated by state-of-the-art techniques are inconsistent, unstable, and provide very little insight into their correctness and reliability. In addition, these methods are also computationally inefficient, and require significant hyper-parameter tuning. In this paper, we address the aforementioned challenges by developing a novel Bayesian framework for generating local explanations along with their associated uncertainty. We instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP which output credible intervals for the feature importances, capturing the associated uncertainty. The resulting explanations not only enable us to make concrete inferences about their quality (e.g., there is a 95% chance that the feature importance lies within the given range), but are also highly consistent and stable. We carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence. This work makes the first attempt at addressing several critical issues with popular explanation methods in one shot, thereby generating consistent, stable, and reliable explanations with guarantees in a computationally efficient manner. Experimental evaluation with multiple real world datasets and user studies demonstrate that the efficacy of the proposed framework.1","This paper proposes a Bayesian framework for generating black box explanations for black box models. The proposed framework is based on Bayesian versions of LIME and KernelSHAP, which output credible intervals for the feature importances, capturing the associated uncertainty. The authors carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence. Experimental evaluation with multiple real world datasets and user studies demonstrate the efficacy of the proposed framework."
799,SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"Adder neural networks (ANNs) are designed for low energy cost which replace expensive multiplications in convolutional neural networks (CNNs) with cheaper additions to yield energy-efficient neural networks and hardware accelerations. Although ANNs achieve satisfactory efficiency, there exist gaps between ANNs and CNNs where the accuracy of ANNs can hardly be compared to CNNs without the assistance of other training tricks, such as knowledge distillation. The inherent discrepancy lies in the similarity measurement between filters and features, however how to alleviate this difference remains unexplored. To locate the potential problem of ANNs, we focus on the property difference due to similarity measurement. We demonstrate that unordered heavy tails in ANNs could be the key component which prevents ANNs from achieving superior classification performance since fatter tails tend to overlap in feature space. Through pre-defining Multivariate Skew Laplace distributions and embedding feature distributions into the loss function, ANN features can be fully controlled and designed for various properties. We further present a novel method for tackling existing heavy tails in ANNs with only a modification of classifier where ANN features are clustered with their tails wellformulated through proposed angle-based constraint on the distribution parameters to encourage high diversity of tails. Experiments conducted on several benchmarks and comparison with other distributions demonstrate the effectiveness of proposed approach for boosting the performance of ANNs.","This paper studies the problem of heavy tails in adversarial neural networks (ANNs). The authors argue that unordered heavy tails could be the key component which prevents ANNs from achieving superior classification performance since fatter tails tend to overlap in feature space. To address this issue, the authors propose to pre-define Multivariate Skew Laplace distributions and embed the feature distributions into the loss function. The authors further propose a novel method for tackling existing heavy tails with only a modification of classifier where ANN features are clustered with their tails wellformulated through proposed angle-based constraint on the distribution parameters to encourage high diversity of tails. Experiments conducted on several benchmarks and comparison with other distributions demonstrate the effectiveness of proposed approach for boosting the performance of ANNs."
800,SP:cbccb65457564992d534504c0d060da44cafce8c,"We identify and formalize a fundamental gradient descent phenomenon leading to a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This work provides a theoretical explanation for the emergence of such feature imbalances in neural networks. Using tools from Dynamical Systems theory, we identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on our proposed formalism, we develop guarantees for a novel but simple regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation. We illustrate our findings with simple and realworld out-of-distribution (OOD) generalization experiments.","This paper studies the phenomenon of gradient starvation in deep neural networks. The authors provide a theoretical explanation for the phenomenon and propose a novel regularization method to decouple feature learning dynamics. The paper is well-written and well-motivated. However, there are a few issues that need to be addressed in the paper."
801,SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI teammates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We find that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance. 4","This paper presents a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, the paper also quantify subjective measures of human’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. The authors find that humans have a clear preference toward a rule based AI teammate (SmartBot) over a state-of-the-art learning based AI agent (Other-Play) across nearly all subjective metrics, and generally view the learning based agent negatively. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming."
802,SP:2a05e333fc1a14057515ef3addde9a40152373db,"The task of visual question generation (VQG) aims to generate human-like neural questions from an image and potentially other side information (e.g., answer type or the answer itself). Existing works often suffer from the severe one image to many questions mapping problem, which generates uninformative and non-referential questions. Recent work has demonstrated that by leveraging double visual and answer hints, a model can faithfully generate much better quality questions. However, visual hints are not available naturally. Despite they proposed a simple rule-based similarity matching method to obtain candidate visual hints, they could be very noisy practically and thus restrict the quality of generated questions. In this paper, we present a novel learning approach for double-hints based VQG, which can be cast as a weakly supervised learning problem with noises. The key rationale is that the salient visual regions of interest can be viewed as a constraint to improve the generation procedure for producing high-quality questions. As a result, given the predicted salient visual regions of interest, we can focus on estimating the probability of being ground-truth questions, which in turn implicitly measures the quality of predicted visual hints. Experimental results on two benchmark datasets show that our proposed method outperforms the state-of-the-art approaches by a large margin on a variety of metrics, including both automatic machine metrics and human evaluation.","This paper proposes a novel approach for visual question generation (VQG) based on double-hints. The key idea is that the salient visual regions of interest can be viewed as a constraint to improve the generation procedure for producing high-quality questions. To this end, the proposed method estimates the probability of being ground-truth questions, which in turn implicitly measures the quality of predicted visual hints. Experimental results on two benchmark datasets show that the proposed model outperforms the state-of-the-art approaches by a large margin on a variety of metrics."
803,SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"Label noise and class imbalance are two major issues coexisting in real-world datasets. To alleviate the two issues, state-of-the-art methods reweight each instance by leveraging a small amount of clean and unbiased data. Yet, these methods overlook class-level information within each instance, which can be further utilized to improve performance. To this end, in this paper, we propose Generalized Data Weighting (GDW) to simultaneously mitigate label noise and class imbalance by manipulating gradients at the class level. To be specific, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. In this way, GDW achieves remarkable performance improvement on both issues. Aside from the performance gain, GDW efficiently obtains class-level weights without introducing any extra computational cost compared with instance weighting methods. Specifically, GDW performs a gradient descent step on class-level weights, which only relies on intermediate gradients. Extensive experiments in various settings verify the effectiveness of GDW. For example, GDW outperforms state-of-the-art methods by 2.56% under the 60% uniform noise setting in CIFAR10. Our code is available at https://github.com/GGchen1997/GDW-NIPS2021.","This paper proposes Generalized Data Weighting (GDW) to simultaneously mitigate label noise and class imbalance by manipulating gradients at the class level. Specifically, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. Extensive experiments in various settings verify the effectiveness of GDW."
804,SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"Language is an interface to the outside world. In order for embodied agents to use it, language must be grounded in other, sensorimotor modalities. While there is an extended literature studying how machines can learn grounded language, the topic of how to learn spatio-temporal linguistic concepts is still largely uncharted. To make progress in this direction, we here introduce a novel spatio-temporal language grounding task where the goal is to learn the meaning of spatio-temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, we train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. We test models on two classes of generalization: 1) generalization to randomly held-out sentences; 2) generalization to grammar primitives. We observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance. We then discuss how this opens new perspectives for language-guided autonomous embodied agents. We also release our code under open-source license as well as pretrained models and datasets to encourage the wider community to build upon and extend our work in the future.","This paper proposes a novel task of grounding language in spatio-temporal representations of behavioral traces of an embodied agent. The grounding is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio temporal references to objects in the scene. To study the role of architectural biases in this task, the authors train several models including multimodal Transformer architectures, which implement different attention computations between words and objects across space and time. They test models on two classes of generalization: 1) generalization to randomly held-out sentences; 2) generalisation to grammar primitives. They observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance."
805,SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"Multiple object tracking and segmentation requires detecting, tracking, and segmenting objects belonging to a set of given classes. Most approaches only exploit the temporal dimension to address the association problem, while relying on single frame predictions for the segmentation mask itself. We propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich spatio-temporal information for online multiple object tracking and segmentation. PCAN first distills a space-time memory into a set of prototypes and then employs cross-attention to retrieve rich information from the past frames. To segment each object, PCAN adopts a prototypical appearance module to learn a set of contrastive foreground and background prototypes, which are then propagated over time. Extensive experiments demonstrate that PCAN outperforms current video instance tracking and segmentation competition winners on both Youtube-VIS and BDD100K datasets, and shows efficacy to both one-stage and two-stage segmentation frameworks. Code and video resources are available at http://vis.xyz/pub/pcan.","This paper proposes Prototypical Cross-Attention Network (PCAN) for multiple object tracking and segmentation. PCAN first distills a space-time memory into a set of prototypes and then employs cross-attention to retrieve rich information from the past frames. To segment each object, PCAN adopts a prototypical appearance module to learn the contrastive foreground and background prototypes, which are propagated over time. Extensive experiments demonstrate that PCAN outperforms current video instance tracking and video segmentation competition winners on both Youtube-VIS and BDD100K datasets."
806,SP:1175ad16382b349ab1a39895150172d266abe571,"Existing analyses of optimization in deep learning are either continuous, focusing on (variants of) gradient flow, or discrete, directly treating (variants of) gradient descent. Gradient flow is amenable to theoretical analysis, but is stylized and disregards computational efficiency. The extent to which it represents gradient descent is an open question in the theory of deep learning. The current paper studies this question. Viewing gradient descent as an approximate numerical solution to the initial value problem of gradient flow, we find that the degree of approximation depends on the curvature around the gradient flow trajectory. We then show that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. This finding allows us to translate an analysis of gradient flow over deep linear neural networks into a guarantee that gradient descent efficiently converges to global minimum almost surely under random initialization. Experiments suggest that over simple deep neural networks, gradient descent with conventional step size is indeed close to gradient flow. We hypothesize that the theory of gradient flows will unravel mysteries behind deep learning.1","This paper studies the relationship between gradient flow and gradient descent in the context of deep learning. In particular, the authors show that gradient flow can be viewed as an approximate numerical solution to the initial value problem of gradient descent, and that the degree of approximation depends on the curvature around the gradient flow trajectory. The authors then show that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. This finding allows them to translate an analysis of gradient flow over deep linear neural networks into a guarantee that gradient descent efficiently converges to global minimum almost surely under random initialization. Experiments suggest that over simple neural networks, gradient descent is indeed close to gradient flow."
807,SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"We consider a stochastic multi-armed bandit (MAB) problem with delayed impact of actions. In our setting, actions taken in the past impact the arm rewards in the subsequent future. This delayed impact of actions is prevalent in the real world. For example, the capability to pay back a loan for people in a certain social group might depend on historically how frequently that group has been approved loan applications. If banks keep rejecting loan applications to people in a disadvantaged group, it could create a feedback loop and further damage the chance of getting loans for people in that group. In this paper, we formulate this delayed and longterm impact of actions within the context of multi-armed bandits. We generalize the bandit setting to encode the dependency of this “bias"" due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. We propose an algorithm that achieves a regret of Õ(KT 2/3) and show a matching regret lower bound of ⌦(KT 2/3), where K is the number of arms and T is the learning horizon. Our results complement the bandit literature by adding techniques to deal with actions with long-term impacts and have implications in designing fair algorithms.","This paper considers a stochastic multi-armed bandit problem with delayed impact of actions. In this setting, actions taken in the past impact the arm rewards in the subsequent future. The authors generalize the bandit setting to encode the dependency of this “bias” due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. They propose an algorithm that achieves a regret of $O(KT^2/3)$ and show a matching regret lower bound of $KT^3/3$."
808,SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. However, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality. In this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip. Specifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. We validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay. The code is available at https://github.com/sukjunhwang/IFC.","This paper proposes an end-to-end solution for video instance segmentation (VIS) based on transformers. Specifically, the authors propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. The authors validate their method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS)."
809,SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"Graph embedding maps a graph into a convenient vector-space representation for graph analysis and machine learning applications. Many graph embedding methods hinge on a sampling of context nodes based on random walks. However, random walks can be a biased sampler due to the structural properties of graphs. Most notably, random walks are biased by the degree of each node, where a node is sampled proportionally to its degree. The implication of such biases has not been clear, particularly in the context of graph representation learning. Here, we investigate the impact of the random walks’ bias on graph embedding and propose residual2vec, a general graph embedding method that can debias various structural biases in graphs by using random graphs. We demonstrate that this debiasing not only improves link prediction and clustering performance but also allows us to explicitly model salient structural properties in graph embedding.",This paper proposes a method for graph embedding based on residual2vec (R2vec) that can debias various structural biases in graphs by using random graphs. R2vec is a general method that can be applied to embeddings of graphs. The main contribution of this paper is to study the impact of random walks’ bias on graph representation learning and propose a method to debias the bias by sampling from a random graph. The experimental results show that the proposed method can improve link prediction and clustering performance and can explicitly model salient structural properties in graphs. 
810,SP:851eac96135b577a5014166edcb43db6a190cf4b,"We study the problem of estimating non-linear functionals of discrete distributions in the context of local differential privacy. The initial data x1,..., xn ∈ [K] are supposed i.i.d. and distributed according to an unknown discrete distribution p = (p1,..., pK). Only α-locally differentially private (LDP) samples z1,..., zn are publicly available, where the term ’local’ means that each zi is produced using one individual attribute xi. We exhibit privacy mechanisms (PM) that are sequentially interactive (i.e. they are allowed to use already published confidential data) or non-interactive. We describe the behavior of the quadratic risk for estimating the power sum functional Fγ = ∑K k=1 p γ k, γ > 0 as a function of K, n and α. In the non-interactive case, we study two plug-in type estimators of Fγ, for all γ > 0, that are similar to the MLE analyzed by Jiao et al. [18] in the multinomial model. However, due to the privacy constraint the rates we attain are slower and similar to those obtained in the Gaussian model by Collier et al. [9]. In the sequentially interactive case, we introduce for all γ > 1 a two-step procedure which attains the parametric rate (nα2)−1/2 when γ ≥ 2. We give lower bounds results over all α-LDP mechanisms and all estimators using the private samples.","This paper studies the problem of estimating the power sum functional of a function of a discrete distribution under local differential privacy. In particular, the paper considers the case where the initial data x1,..., xn are supposed i.i.d. and distributed according to an unknown discrete distribution p = (p1,..., pK). Only α-locally differentially private (LDP) samples are publicly available, where the term ‘local’ means that each zi is produced using one individual attribute xi. The paper considers privacy mechanisms that are sequentially interactive (i.e. they are allowed to use already published confidential data) or non-interactive. The authors describe the behavior of the quadratic risk for estimating the function of the discrete power sum of the power of K, n, and α as well as a two-step procedure which attains the parametric rate (nα2-1/2) for all $k \epsilon$ when $k=1$. The paper also provides lower bounds on the lower bound of the lower bounds for all estimators using the private samples."
811,SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"We study the problem of online multiclass classification in a setting where the learner’s feedback is determined by an arbitrary directed graph. While including bandit feedback as a special case, feedback graphs allow a much richer set of applications, including filtering and label efficient classification. We introduce GAPPLETRON, the first online multiclass algorithm that works with arbitrary feedback graphs. For this new algorithm, we prove surrogate regret bounds that hold, both in expectation and with high probability, for a large class of surrogate losses. Our bounds are of order B √ ρKT, where B is the diameter of the prediction space, K is the number of classes, T is the time horizon, and ρ is the domination number (a graph-theoretic parameter affecting the amount of exploration). In the full information case, we show that GAPPLETRON achieves a constant surrogate regret of order BK. We also prove a general lower bound of order max { BK, √ T } showing that our upper bounds are not significantly improvable. Experiments on synthetic data show that for various feedback graphs our algorithm is competitive against known baselines.","This paper studies the problem of online multiclass classification in the setting where the learner’s feedback is determined by an arbitrary directed graph. The paper proposes a new algorithm called GAPPLETRON that works with arbitrary feedback graphs. The authors prove surrogate regret bounds that hold, both in expectation and with high probability, for a large class of surrogate losses. They also prove a general lower bound of order max {BK,\sqrt{T}}. Experiments on synthetic data show that for various feedback graphs our algorithm is competitive against known baselines."
812,SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"We study the problem of explainable clustering in the setting first formalized by Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML 2020). A k-clustering is said to be explainable if it is given by a decision tree where each internal node splits data points with a threshold cut in a single dimension (feature), and each of the k leaves corresponds to a cluster. We give an algorithm that outputs an explainable clustering that loses at most a factor of O(log k) compared to an optimal (not necessarily explainable) clustering for the k-medians objective, and a factor of O(k log k) for the k-means objective. This improves over the previous best upper bounds of O(k) and O(k), respectively, and nearly matches the previous Ω(log k) lower bound for k-medians and our new Ω(k) lower bound for k-means. The algorithm is remarkably simple. In particular, given an initial not necessarily explainable clustering in R, it is oblivious to the data points and runs in time O(dk log k), independent of the number of data points n. Our upper and lower bounds also generalize to objectives given by higher `p-norms.","This paper studies the problem of explainable clustering in the context of decision trees. In particular, the authors consider the case of a decision tree where each node splits data points with a threshold cut in a single dimension (feature), and each of the k leaves corresponds to a cluster. The authors propose an algorithm that outputs an explainable cluster that is at most a factor of O(log k) compared to an optimal (not necessarily explainable) clustering for the k-medians objective. The algorithm is remarkably simple and runs in time O(dk log k), independent of the number of data points n. "
813,SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"The pre-trained language model (PrLM) demonstrates domination in downstream natural language processing tasks, in which multilingual PrLM takes advantage of language universality to alleviate the issue of limited resources for low-resource languages. Despite its successes, the performance of multilingual PrLM is still unsatisfactory, when multilingual PrLMs only focus on plain text and ignore obvious universal linguistic structure clues. Existing PrLMs have shown that monolingual linguistic structure knowledge may bring about better performance. Thus we propose a novel multilingual PrLM that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in our model, which brings unprecedented PrLM interpretability and convenience in downstream task use. Our model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets, demonstrating the effectiveness and stronger cross-lingual modeling capabilities of our approach.","This paper proposes a multilingual pre-trained language model (PrLM) that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in the model, which brings unprecedented PrLM interpretability and convenience in downstream task use. The model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets."
814,SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"Recently, Transformer has become a prevailing deep architecture for solving vehicle routing problems (VRPs). However, it is less effective in learning improvement models for VRP because its positional encoding (PE) method is not suitable in representing VRP solutions. This paper presents a novel Dual-Aspect Collaborative Transformer (DACT) to learn embeddings for the node and positional features separately, instead of fusing them together as done in existing ones, so as to avoid potential noises and incompatible correlations. Moreover, the positional features are embedded through a novel cyclic positional encoding (CPE) method to allow Transformer to effectively capture the circularity and symmetry of VRP solutions (i.e., cyclic sequences). We train DACT using Proximal Policy Optimization and design a curriculum learning strategy for better sample efficiency. We apply DACT to solve the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). Results show that our DACT outperforms existing Transformer based improvement models, and exhibits much better generalization performance across different problem sizes on synthetic and benchmark instances, respectively.","This paper proposes a dual-aspect collaborative transformer architecture for vehicle routing problems. The authors propose to learn embeddings for the node and positional features separately, instead of fusing them together as done in existing ones, so as to avoid potential noises and incompatible correlations. The positional features are embedded through a novel cyclic positional encoding (CPE) method to capture the circularity and symmetry of VRP solutions (i.e., cyclic sequences). The authors train DACT using Proximal Policy Optimization and design a curriculum learning strategy for better sample efficiency. They apply DACT to solve the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). Results show that DACT outperforms existing Transformer based improvement models, and exhibits much better generalization performance across different problem sizes on synthetic and benchmark instances."
815,SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"Evaluating the inherent difficulty of a given data-driven classification problem is important for establishing absolute benchmarks and evaluating progress in the field. To this end, a natural quantity to consider is the Bayes error, which measures the optimal classification error theoretically achievable for a given data distribution. While generally an intractable quantity, we show that we can compute the exact Bayes error of generative models learned using normalizing flows. Our technique relies on a fundamental result, which states that the Bayes error is invariant under invertible transformation. Therefore, we can compute the exact Bayes error of the learned flow models by computing it for Gaussian base distributions, which can be done efficiently using Holmes-Diaconis-Ross integration. Moreover, we show that by varying the temperature of the learned flow models, we can generate synthetic datasets that closely resemble standard benchmark datasets, but with almost any desired Bayes error. We use our approach to conduct a thorough investigation of state-of-the-art classification models, and find that in some — but not all — cases, these models are capable of obtaining accuracy very near optimal. Finally, we use our method to evaluate the intrinsic ""hardness"" of standard benchmark datasets.","This paper presents a method for computing the exact Bayes error of generative models learned using normalizing flows. The method is based on the invertible transformation theorem, which states that the Bayesian error is invariant under the normalizing flow. The authors then show that they can compute the exact error of the learned flow models by computing it for Gaussian base distributions, which can be done efficiently using Holmes-Diaconis-Ross integration. Moreover, the authors show that by varying the temperature of the model, they can generate synthetic datasets that closely resemble standard benchmark datasets, but with almost any desired Bayes errors. Finally, they use their method to evaluate the intrinsic ""hardness"" of benchmark datasets."
816,SP:2896679f0472522bc3334178cd7574494cf12b7b,"Innovations in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. A number of architecture-specific initialization schemes have been proposed, but these schemes are not always portable to new architectures. This paper presents GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. Code is available at https://github.com/zhuchen03/gradinit.","This paper proposes a new method for initialization of neural networks. The method is based on a simple heuristic: the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. Gradinit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation."
817,SP:f69731403592fa5bdd4ca327708582d615aa131c,"Linear mixed-effect models provide a natural baseline for estimating disease progression using longitudinal data. They provide interpretable models at the cost of modeling assumptions on the progression profiles and their variability across subjects. A significant improvement is to embed the data in a Riemannian manifold and learn patient-specific trajectories distributed around a central geodesic. A few interpretable parameters characterize subject trajectories at the cost of a prior choice of the metric, which determines the shape of the trajectories. We extend this approach by learning the metric from the data allowing more flexibility while keeping the interpretability. Specifically, we learn the metric as the push-forward of the Euclidean metric by a diffeomorphism. This diffeomorphism is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The metric update allows us to improve the forecasting of imaging and clinical biomarkers in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) cohort. Our results compare favorably to the 56 methods benchmarked in the TADPOLE challenge. ∗This work was partially supported by NIH R01AG027161 and R01EY032284. †This research has also received funding from the program ""Investissements d’avenir"" ANR-10-IAIHU-06. This work was also funded in part by the French government under management of Agence Nationale de la Recherche as part of the ""Investissements d’avenir"" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute) and by ANR under the joint programme in neurodegenerative diseases (JPND) ANR-19-JPW2-000 (E-DADS). ‡Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wpcontent/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf 35th Conference on Neural Information Processing Systems (NeurIPS 2021).",This paper proposes a method for estimating disease progression using longitudinal data. The authors propose to learn the metric from the data by learning the metric as the push-forward of the Euclidean metric by a diffeomorphism. The metric update is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The proposed method is evaluated on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset and compared with 56 other methods in the TADPOLE challenge.
818,SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"Recent Convolutional Neural Networks (CNNs) have achieved significant success by stacking multiple convolutional blocks, named procedures in this paper, to extract semantic features. However, they use the same procedure sequence for all inputs, regardless of the intermediate features. This paper proffers a simple yet effective idea of constructing parallel procedures and assigning similar intermediate features to the same specialized procedures in a divide-and-conquer fashion. It relieves each procedure’s learning difficulty and thus leads to superior performance. Specifically, we propose a routing-by-memory mechanism for existing CNN architectures. In each stage of the network, we introduce parallel Procedural Units (PUs). A PU consists of a memory head and a procedure. The memory head maintains a summary of a type of features. For an intermediate feature, we search its closest memory and forward it to the corresponding procedure in both training and testing. In this way, different procedures are tailored to different features and therefore tackle them better. Networks with the proposed mechanism can be trained efficiently using a four-step training strategy. Experimental results show that our method improves VGGNet, ResNet, and EfficientNet’s accuracies on Tiny ImageNet, ImageNet, and CIFAR-100 benchmarks with a negligible extra computational cost.","This paper proposes a routing-by-memory mechanism for existing CNN architectures. In each stage of the network, they introduce parallel Procedural Units (PUs) which consists of a memory head and a procedure. For an intermediate feature, they search its closest memory and forward it to the corresponding procedure in both training and testing. In this way, different procedures are tailored to different features and therefore tackle them better."
819,SP:d240173080cd3647dbaa5173a6422396f226775b,"There has been enormous progress in the last few years in designing neural networks that respect the fundamental symmetries and coordinate freedoms of physical law. Some of these frameworks make use of irreducible representations, some make use of high-order tensor objects, and some apply symmetry-enforcing constraints. Different physical laws obey different combinations of fundamental symmetries, but a large fraction (possibly all) of classical physics is equivariant to translation, rotation, reflection (parity), boost (relativity), and permutations. Here we show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetries, or under the Euclidean, Lorentz, and Poincaré groups, at any dimensionality d. The key observation is that nonlinear O(d)-equivariant (and related-group-equivariant) functions can be universally expressed in terms of a lightweight collection of scalars—scalar products and scalar contractions of the scalar, vector, and tensor inputs. We complement our theory with numerical examples that show that the scalar-based method is simple, efficient, and scalable.","This paper studies the equivariance of polynomials in physics. The authors show that it is possible to parameterize polynomial functions that are equivariant under symmetries of the Euclidean, Lorentz, and Poincaré groups. This is achieved by using scalar products and scalar contractions of the scalar, vector, and tensor inputs. The scalar-based method is simple, efficient, and scalable. The paper also provides numerical examples that show that scalar based methods are simple and efficient."
820,SP:72c0f47566904deb27d8157da30807ec1d6b5685,"Bounding box (bbox) regression is a fundamental task in computer vision. So far, the most commonly used loss functions for bbox regression are the Intersection over Union (IoU) loss and its variants. In this paper, we generalize existing IoUbased losses to a new family of power IoU losses that have a power IoU term and an additional power regularization term with a single power parameter α. We call this new family of losses the α-IoU losses and analyze properties such as order preservingness and loss/gradient reweighting. Experiments on multiple object detection benchmarks and models demonstrate that α-IoU losses, 1) can surpass existing IoU-based losses by a noticeable performance margin; 2) offer detectors more flexibility in achieving different levels of bbox regression accuracy by modulating α; and 3) are more robust to small datasets and noisy bboxes.",This paper proposes a new family of loss functions for bounding box regression. The main idea is to generalize existing IoU-based losses to a family of power IoU losses that have a power-IoU term and an additional power regularization term with a single power parameter α. The authors analyze properties such as order preservingness and loss/gradient reweighting. Experiments on multiple object detection benchmarks and models demonstrate that the proposed loss can surpass existing IoI-based loss functions by a noticeable performance margin.
821,SP:397125177d7007316d67194ec00d5dc57b44ac79,"We consider the imitation learning problem of learning a policy in a Markov Decision Process (MDP) setting where the reward function is not given, but demonstrations from experts are available. Although the goal of imitation learning is to learn a policy that produces behaviors nearly as good as the experts’ for a desired task, assumptions of consistent optimality for demonstrated behaviors are often violated in practice. Finding a policy that is distributionally robust against noisy demonstrations based on an adversarial construction potentially solves this problem by avoiding optimistic generalizations of the demonstrated data. This paper studies Distributionally Robust Imitation Learning (DROIL) and establishes a close connection between DROIL and Maximum Entropy Inverse Reinforcement Learning. We show that DROIL can be seen as a framework that maximizes a generalized concept of entropy. We develop a novel approach to transform the objective function into a convex optimization problem over a polynomial number of variables for a class of loss functions that are additive over state and action spaces. Our approach lets us optimize both stationary and non-stationary policies and, unlike prevalent previous methods, it does not require repeatedly solving an inner reinforcement learning problem. We experimentally show the significant benefits of DROIL’s new optimization method on synthetic data and a highway driving environment.","This paper studies the problem of imitation learning in a Markov decision process (MDP) setting where the reward function is not given, but demonstrations from experts are available. The authors propose Distributionally Robust Imitation Learning (DROIL) which aims to learn a policy that is distributionally robust against noisy demonstrations based on an adversarial construction to avoid optimistic generalizations of the demonstrated data. DROIL can be seen as a framework that maximizes a generalized concept of entropy. They develop a novel approach to transform the objective function into a convex optimization problem over a polynomial number of variables for a class of loss functions that are additive over state and action spaces. They show the significant benefits of DROIL on synthetic data and a highway driving environment."
822,SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"Post-processing in algorithmic fairness is a versatile approach for correcting bias in ML systems that are already used in production. The main appeal of postprocessing is that it avoids expensive retraining. In this work, we propose general post-processing algorithms for individual fairness (IF). We consider a setting where the learner only has access to the predictions of the original model and a similarity graph between individuals guiding the desired fairness constraints. We cast the IF post-processing problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired “treat similar individuals similarly” interpretation. Our theoretical results demonstrate the connection of the new objective function to a local relaxation of the original individual fairness. Empirically, our post-processing algorithms correct individual biases in large-scale NLP models such as BERT, while preserving accuracy.","This paper proposes a general post-processing algorithm for individual fairness (IF) based on graph Laplacian regularization. In particular, the authors cast the individual fairness problem as a graph smoothing problem, which is a graph-regularized version of individual fairness. Theoretical analysis is provided to show the connection of the new objective function to a local relaxation of the original individual fairness objective. Empirically, the proposed algorithm is shown to correct individual biases in large-scale NLP models."
823,SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"The Text-to-SQL task, aiming to translate the natural language of the questions into SQL queries, has drawn much attention recently. One of the most challenging problems of Text-to-SQL is how to generalize the trained model to the unseen database schemas, also known as the cross-domain Text-to-SQL task. The key lies in the generalizability of (i) the encoding method to model the question and the database schema and (ii) the question-schema linking method to learn the mapping between words in the question and tables/columns in the database schema. Focusing on the above two key issues, we propose a Structure-Aware Dual Graph Aggregation Network (SADGA) for cross-domain Text-to-SQL. In SADGA, we adopt the graph structure to provide a unified encoding model for both the natural language question and database schema. Based on the proposed unified modeling, we further devise a structure-aware aggregation method to learn the mapping between the question-graph and schema-graph. The structure-aware aggregation method is featured with Global Graph Linking, Local Graph Linking and DualGraph Aggregation Mechanism. We not only study the performance of our proposal empirically but also achieved 3rd place on the challenging Text-to-SQL benchmark Spider at the time of writing.","This paper proposes a new model for cross-domain Text-to-SQL task. The proposed model is based on Graph-Aware Dual Graph Aggregation Network (SADGA) and aims to generalize the question-schema linking method to the unseen database schemas. SADGA adopts the graph structure to provide a unified encoding model for both the natural language question and database schema, and further devise a structure-aware aggregation method to learn the mapping between question-graph and schema-graph. Experiments on the Spider benchmark show that the proposed model achieves 3rd place on the challenging task."
824,SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph’s execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.","This paper studies the problem of learning end-to-end learnable discrete-continuous models with multiple discrete components. The authors analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components and show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. They then propose two new strategies to overcome these challenges. First, they show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, they propose dropout residual connections specifically tailored to stochastically, discrete, continuous computation graphs."
825,SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"Approximate Bayesian inference for neural networks is considered a robust alternative to standard training, often providing good performance on out-of-distribution data. However, Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo achieve poor generalization under covariate shift, even underperforming classical estimation. We explain this surprising result, showing how a Bayesian model average can in fact be problematic under covariate shift, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. We additionally show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, we propose novel priors that improve the robustness of BNNs to many sources of covariate shift.","This paper studies the problem of Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo (HMC) in the context of covariate shift. The authors show that BNNs with HMC with Bayesian model average can be problematic in cases where linear dependencies in the input features cause a lack of posterior contraction. They also show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, the authors propose novel priors that improve the robustness of BNN to many sources of covariances shift."
826,SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"We categorize meta-learning evaluation into two settings: in-distribution [ID], in which the train and test tasks are sampled iid from the same underlying task distribution, and out-of-distribution [OOD], in which they are not. While most metalearning theory and some FSL applications follow the ID setting, we identify that most existing few-shot classification benchmarks instead reflect OOD evaluation, as they use disjoint sets of train (base) and test (novel) classes for task generation. This discrepancy is problematic because—as we show on numerous benchmarks— meta-learning methods that perform better on existing OOD datasets may perform significantly worse in the ID setting. In addition, in the OOD setting, even though current FSL benchmarks seem befitting, our study highlights concerns in 1) reliably performing model selection for a given meta-learning method, and 2) consistently comparing the performance of different methods. To address these concerns, we provide suggestions on how to construct FSL benchmarks to allow for ID evaluation as well as more reliable OOD evaluation. Our work† aims to inform the meta-learning community about the importance and distinction of ID vs. OOD evaluation, as well as the subtleties of OOD evaluation with current benchmarks.","This paper studies the problem of few-shot meta-learning in the out-of-distribution (OOD) setting. The authors point out that most existing few shot classification benchmarks do not reflect OOD evaluation, as they use disjoint sets of train (base) and test (novel) classes for task generation. This discrepancy is problematic because meta-learners that perform better on existing OOD datasets may perform significantly worse in the ID setting. In addition, in the OOD setting, even though current FSL benchmarks seem befitting, the study highlights concerns in 1) reliably performing model selection and 2) consistently comparing the performance of different methods. To address these concerns, the authors provide suggestions on how to construct benchmarks to allow for ID evaluation as well as more reliable OOD evaluations."
827,SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"Rules have a number of desirable properties. It is easy to understand, infer new knowledge, and communicate with other inference systems. One weakness of the previous rule induction systems is that they only find rules within a knowledge base (KB) and therefore cannot generalize to more open and complex real-world rules. Recently, the language model (LM)-based rule generation are proposed to enhance the expressive power of the rules. In this paper, we revisit the differences between KB-based rule induction and LM-based rule generation. We argue that, while KB-based methods inducted rules by discovering data commonalities, the current LM-based methods are “learning rules from rules”. This limits these methods to only produce “canned” rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text. Therefore, in this paper, we propose the open rule induction problem, which aims to induce open rules utilizing the knowledge in LMs. Besides, we propose the Orion (open rule induction) system to automatically mine open rules from LMs without supervision of annotated rules. We conducted extensive experiments to verify the quality and quantity of the inducted open rules. Surprisingly, when applying the open rules in downstream tasks (i.e. relation extraction), these automatically inducted rules even outperformed the manually annotated rules. 2","This paper studies the problem of rule induction in the context of language model-based rule generation. In particular, the authors argue that the current methods are “learning rules from rules”, which limits these methods to only produce “canned” rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text. The authors propose the open rule induction problem, which aims to induce open rules utilizing the knowledge in LMs. Besides, they propose the Orion (open rule induction) system to automatically mine open rules from LMs without supervision of annotated rule. They conducted extensive experiments to verify the quality and quantity of inducted open rules."
828,SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,"Learning from datasets without interaction with environments (Offline Learning) is an essential step to apply Reinforcement Learning (RL) algorithms in real-world scenarios. However, compared with the single-agent counterpart, offline multiagent RL introduces more agents with the larger state and action space, which is more challenging but attracts little attention. We demonstrate current offline RL algorithms are ineffective in multi-agent systems due to the accumulated extrapolation error. In this paper, we propose a novel offline RL algorithm, named Implicit Constraint Q-learning (ICQ), which effectively alleviates the extrapolation error by only trusting the state-action pairs given in the dataset for value estimation. Moreover, we extend ICQ to multi-agent tasks by decomposing the joint-policy under the implicit constraint. Experimental results demonstrate that the extrapolation error is successfully controlled within a reasonable range and insensitive to the number of agents. We further show that ICQ achieves the state-of-the-art performance in the challenging multi-agent offline tasks (StarCraft II). Our code is public online at https://github.com/YiqinYang/ICQ.",This paper proposes Implicit Constraint Q-learning (ICQ) for offline multi-agent reinforcement learning. The main idea of ICQ is to decompose the joint-policy under the implicit constraint. The authors show that the extrapolation error is controlled within a reasonable range and insensitive to the number of agents. The experimental results demonstrate that ICQ achieves the state-of-the-art performance in StarCraft II.
829,SP:1939b24b68970c33ca16ce238deed257f76d009e,"Robustness of machine learning models is critical for security related applications, where real-world adversaries are uniquely focused on evading neural network based detectors. Prior work mainly focus on crafting adversarial examples (AEs) with small uniform norm-bounded perturbations across features to maintain the requirement of imperceptibility. However, uniform perturbations do not result in realistic AEs in domains such as malware, finance, and social networks. For these types of applications, features typically have some semantically meaningful dependencies. The key idea of our proposed approach1 is to enable non-uniform perturbations that can adequately represent these feature dependencies during adversarial training. We propose using characteristics of the empirical data distribution, both on correlations between the features and the importance of the features themselves. Using experimental datasets for malware classification, credit risk prediction, and spam detection, we show that our approach is more robust to real-world attacks. Finally, we present robustness certification utilizing non-uniform perturbation bounds, and show that non-uniform bounds achieve better certification.","This paper proposes a method for adversarial training of deep neural networks with non-uniform perturbations to improve the robustness of the model against adversarial attacks. The method is based on the observation that the features of a deep neural network often have semantically meaningful dependencies. The authors propose to train a neural network with non uniform norm-bounded adversarial perturbation, and then certify the model's robustness to adversarial attack. The proposed method is evaluated on malware classification, credit risk prediction, and spam detection datasets."
830,SP:417b30930b245667d777e5d90ee80dd41546760e,"The theory of spectral filtering is a remarkable tool to understand the statistical properties of learning with kernels. For least squares, it allows to derive various regularization schemes that yield faster convergence rates of the excess risk than with Tikhonov regularization. This is typically achieved by leveraging classical assumptions called source and capacity conditions, which characterize the difficulty of the learning task. In order to understand estimators derived from other loss functions, Marteau-Ferey et al. [1] have extended the theory of Tikhonov regularization to generalized self concordant loss functions (GSC), which contain, e.g., the logistic loss. In this paper, we go a step further and show that fast and optimal rates can be achieved for GSC by using the iterated Tikhonov regularization scheme, which is intrinsically related to the proximal point method in optimization, and overcomes the limitation of the classical Tikhonov regularization.","This paper proposes an iterated Tikhonov regularization scheme for generalized self-concordant loss functions (GSC), which is an extension of the work of Marteau-Ferey et al. (2018) on spectral filtering. The main contribution of this paper is to extend the theoretical analysis of spectral filtering to the case of generalized self concordant losses. The authors show that the iterated regularization is equivalent to the proximal point method in optimization, and that it can be used to improve the convergence rate of the excess risk for GSC. "
831,SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"We introduce a new kind of linear transform named Deformable Butterfly (DeBut) that generalizes the conventional butterfly matrices and can be adapted to various input-output dimensions. It inherits the fine-to-coarse-grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. We apply DeBut as a drop-in replacement of standard fully connected and convolutional layers, and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy. The natural complexity-accuracy tradeoff arising from the myriad deformations of a DeBut layer also opens up new rooms for analytical and practical research. The codes and Appendix are publicly available at: https://github.com/ruilin0212/DeBut.","This paper proposes a new neural network layer called Deformable butterfly (DeBut) that generalizes the conventional butterfly matrices and can be adapted to various input-output dimensions. The proposed layer is a drop-in replacement of standard fully connected and convolutional layers and demonstrates its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy. The paper also opens up new rooms for analytical and practical research."
832,SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"When learning tasks over time, artificial neural networks suffer from a problem known as Catastrophic Forgetting (CF). This happens when the weights of a network are overwritten during the training of a new task causing forgetting of old information. To address this issue, we propose MetA Reusable Knowledge or MARK, a new method that fosters weight reusability instead of overwriting when learning a new task. Specifically, MARK keeps a set of shared weights among tasks. We envision these shared weights as a common Knowledge Base (KB) that is not only used to learn new tasks, but also enriched with new knowledge as the model learns new tasks. Key components behind MARK are two-fold. On the one hand, a metalearning approach provides the key mechanism to incrementally enrich the KB with new knowledge and to foster weight reusability among tasks. On the other hand, a set of trainable masks provides the key mechanism to selectively choose from the KB relevant weights to solve each task. By using MARK, we achieve state of the art results in several popular benchmarks, surpassing the best performing methods in terms of average accuracy by over 10% on the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness using 55% of the number of parameters. Furthermore, an ablation study provides evidence that, indeed, MARK is learning reusable knowledge that is selectively used by each task.","This paper proposes MetA Reusable Knowledge (MARK) to address the problem of catastrophic forgetting in deep neural networks. The authors propose to use a common knowledge base (KB) for each task, which consists of a set of shared weights among tasks. The idea is that the KB can be used to selectively choose the relevant weights to solve each task. The paper also proposes a metalearning approach to incrementally enrich the KB with new knowledge and to foster weight reusability among tasks to avoid forgetting of old information."
833,SP:722c52467e384058f8fdffa254d0e8db47440a64,"Primal heuristics play a crucial role in exact solvers for Mixed Integer Programming (MIP). While solvers are guaranteed to find optimal solutions given sufficient time, real-world applications typically require finding good solutions early on in the search to enable fast decision-making. While much of MIP research focuses on designing effective heuristics, the question of how to manage multiple MIP heuristics in a solver has not received equal attention. Generally, solvers follow hard-coded rules derived from empirical testing on broad sets of instances. Since the performance of heuristics is problem-dependent, using these general rules for a particular problem might not yield the best performance. In this work, we propose the first data-driven framework for scheduling heuristics in an exact MIP solver. By learning from data describing the performance of primal heuristics, we obtain a problem-specific schedule of heuristics that collectively find many solutions at minimal cost. We formalize the learning task and propose an efficient algorithm for computing such a schedule. Compared to the default settings of a state-of-the-art academic MIP solver, we are able to reduce the average primal integral by up to 49% on two classes of challenging instances.","This paper proposes a data-driven framework for scheduling heuristics in exact MIP solvers for mixed integer programming (MIP). In particular, the authors propose a method for learning a schedule of primal heuristic policies for a specific MIP problem. The proposed method is based on the idea of learning from data describing the performance of different primal heuristic policies. The authors also propose an efficient algorithm for computing such a schedule. Experiments show that the proposed method can reduce the average primal integral by up to 49% on two classes of challenging instances."
834,SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"We study a theory of reinforcement learning (RL) in which the learner receives binary feedback only once at the end of an episode. While this is an extreme test case for theory, it is also arguably more representative of real-world applications than the traditional requirement in RL practice that the learner receive feedback at every time step. Indeed, in many real-world applications of reinforcement learning, such as self-driving cars and robotics, it is easier to evaluate whether a learner’s complete trajectory was either “good” or “bad,” but harder to provide a reward signal at each step. To show that learning is possible in this more challenging setting, we study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sublinear regret.","This paper studies the problem of binary feedback in reinforcement learning, where the learner receives binary feedback only once at the end of an episode. The authors study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sublinear regret. This is an extreme test case for theory, but it is also arguably more representative of real-world applications than the traditional requirement in RL practice that the learners receive feedback at every time step."
835,SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-theart graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message-passing.1","This paper proposes a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows the authors to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, the authors then cluster or drop edges to obtain holistic graph-level edge representations. The proposed method is evaluated on graph reconstruction and graph generation tasks, and graph classification tasks for which the edges are important for discrimination."
836,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information (MI) maximization provides an appealing formalism for learning representations of data. In the context of reinforcement learning (RL), such representations can accelerate learning by discarding irrelevant and redundant information, while retaining the information necessary for control. Much prior work on these methods has addressed the practical difficulties of estimating MI from samples of high-dimensional observations, while comparatively less is understood about which MI objectives yield representations that are sufficient for RL from a theoretical perspective. In this paper, we formalize the sufficiency of a state representation for learning and representing the optimal policy, and study several popular MI based objectives through this lens. Surprisingly, we find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. We corroborate our theoretical results with empirical experiments on a simulated game environment with visual observations.","This paper studies the sufficiency of different mutual information maximization (MI) objectives for representation learning in the context of reinforcement learning (RL). The authors consider the case of two popular MI objectives: (1) Mutual Information Maximization (MIM) and (2) mutual information minimization. The authors show that these two objectives are not sufficient for RL from a theoretical perspective. The paper also provides theoretical analysis of why these objectives do not yield sufficient representations for RL. Finally, the authors conduct experiments on a simulated game environment to verify the theoretical results."
837,SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"As a basic component of SE(3)-equivariant deep feature learning, steerable convolution has recently demonstrated its advantages for 3D semantic analysis. The advantages are, however, brought by expensive computations on dense, volumetric data, which prevent its practical use for efficient processing of 3D data that are inherently sparse. In this paper, we propose a novel design of Sparse Steerable Convolution (SS-Conv) to address the shortcoming; SS-Conv greatly accelerates steerable convolution with sparse tensors, while strictly preserving the property of SE(3)-equivariance. Based on SS-Conv, we propose a general pipeline for precise estimation of object poses, wherein a key design is a Feature-Steering module that takes the full advantage of SE(3)-equivariance and is able to conduct an efficient pose refinement. To verify our designs, we conduct thorough experiments on three tasks of 3D object semantic analysis, including instance-level 6D pose estimation, category-level 6D pose and size estimation, and categorylevel 6D pose tracking. Our proposed pipeline based on SS-Conv outperforms existing methods on almost all the metrics evaluated by the three tasks. Ablation studies also show the superiority of our SS-Conv over alternative convolutions in terms of both accuracy and efficiency. Our code is released publicly at https://github.com/Gorilla-Lab-SCUT/SS-Conv.","This paper proposes a novel method for steerable convolution for 3D object semantic analysis. The proposed method is based on a feature-steering module that takes advantage of the SE(3)-equivariance of the convolutional layers. The method is evaluated on 3 tasks: instance-level 6D pose estimation, category-level pose and size estimation, and category level 6D tracking. The results show that the proposed method outperforms existing methods on almost all the metrics evaluated by the three tasks."
838,SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31% ∼ 37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/ raoyongming/DynamicViT.","This paper proposes a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, a lightweight prediction module is added to different layers to estimate the importance score of each token given the current features. To optimize the prediction module in an end-to-end manner, an attention masking strategy is proposed to differentiably prune a token by blocking its interactions with other tokens. By hierarchically pruning 66% of the input tokens, the method greatly reduces 31% to 37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5%."
839,SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"In data analysis problems where we are not able to rely on distributional assumptions, what types of inference guarantees can still be obtained? Many popular methods, such as holdout methods, cross-validation methods, and conformal prediction, are able to provide distribution-free guarantees for predictive inference, but the problem of providing inference for the underlying regression function (for example, inference on the conditional mean E [Y |X]) is more challenging. In the setting where the features X are continuously distributed, recent work has established that any confidence interval for E [Y |X] must have non-vanishing width, even as sample size tends to infinity. At the other extreme, if X takes only a small number of possible values, then inference on E [Y |X] is trivial to achieve. In this work, we study the problem in settings in between these two extremes. We find that there are several distinct regimes in between the finite setting and the continuous setting, where vanishing-width confidence intervals are achievable if and only if the effective support size of the distribution of X is smaller than the square of the sample size.","This paper studies the problem of inference on the conditional mean E [Y |X] in the continuous case, where the features X are continuously distributed. In particular, the authors consider the case where the support size of the distribution of X is smaller than the square of the sample size. In this case, they show that the confidence intervals of the confidence interval of E can have vanishing width, even as sample size tends to infinity. On the other hand, if X takes only a small number of possible values, then inference on E is trivial to achieve. "
840,SP:123952325765c040c3078fc7dca2b6d370e55590,"Existing bias mitigation methods for DNN models primarily work on learning debiased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs? To this end, we propose a new mitigation technique, namely, Representation Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The key idea of RNF is to discourage the classification head from capturing undesirable correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination of DNN models with minimal degradation in task-specific performance.","This paper proposes Representation Neutralization for Fairness (RNF), a method for reducing discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs. The proposed method is based on a bias-amplified model to generate proxy annotations for sensitive attributes. The paper is well-written and well-motivated. The experimental results on several benchmark datasets demonstrate the effectiveness of the proposed RNF framework."
841,SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"For many applications in image analysis, learning models that are invariant to translations and rotations is paramount. This is the case, for example, in medical imaging where the objects of interest can appear at arbitrary positions, with arbitrary orientations. As of today, Convolutional Neural Networks (CNN) are one of the most powerful tools for image analysis. They achieve, thanks to convolutions, an invariance with respect to translations. In this work, we present a new type of convolutional layer that takes advantage of Bessel functions, well known in physics, to build Bessel-CNNs (B-CNNs) that are invariant to all the continuous set of possible rotation angles by design.","This paper proposes a new convolutional layer that takes advantage of Bessel functions, well known in physics, to build Bessel-CNNs (B-CNN) that are invariant to all the continuous set of possible rotation angles by design. B-CNN is an extension of the Bessel convolution layer of the standard CNN, which is used to learn models that can be used in medical imaging applications. The paper is well-written and well-motivated. The experimental results are convincing."
842,SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,"We introduce ParK, a new large-scale solver for kernel ridge regression. Our approach combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. In particular, constructing suitable partitions directly in the feature space rather than in the input space, we promote orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. We characterize the statistical-computational tradeoff of our model, and demonstrate the effectiveness of our method by numerical experiments on large-scale datasets.","This paper proposes a new method for kernel ridge regression, which combines partitioning with random projections and iterative optimization to reduce space and time complexity while maintaining the same statistical accuracy. In particular, constructing suitable partitions directly in the feature space rather than in the input space, the authors promote orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. The authors characterize the statistical-computational tradeoff of their model, and demonstrate the effectiveness of their method by numerical experiments on large-scale datasets."
843,SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"Neural agents trained in reinforcement learning settings can learn to communicate among themselves via discrete tokens, accomplishing as a team what agents would be unable to do alone. However, the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero-shot understanding. Inspired by word embedding techniques from natural language processing, we propose neural agent architectures that enables them to communicate via discrete tokens derived from a learned, continuous space. We show in a decision theoretic framework that our technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, we validate that our trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, we demonstrate both that agents using our method can effectively respond to novel human communication and that humans can understand unlabeled emergent agent communication, outperforming the use of one-hot communication.","This paper proposes a method for learning discrete communication tokens for reinforcement learning agents. The authors propose to learn discrete tokens from a learned, continuous space, instead of using one-hot vectors, which is the standard for discrete communication in RL. The proposed method is based on word embedding techniques from natural language processing (NLP). The authors show that the learned discrete tokens can be used for communication in a variety of scenarios, and that the proposed method can outperform the standard one hot communication method in terms of zero-shot understanding. "
844,SP:8630ccc627534f9033bced04e2137a897ffef701,"Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets (pronounced “coat” nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.","This paper proposes CoAtNets (pronounced “coat” nets), a family of hybrid models built from two key insights: (1) depthwise convolution and self-attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that the proposed model achieves state-of-the-art performance under different resource constraints across various datasets."
845,SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"We present a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the ChebyshevCantelli inequality (a.k.a. one-sided Chebyshev’s), which is amenable to efficient minimization. The new form resolves the optimization challenge faced by prior oracle bounds based on the Chebyshev-Cantelli inequality, the C-bounds [Germain et al., 2015], and, at the same time, it improves on the oracle bound based on second order Markov’s inequality introduced by Masegosa et al. [2020]. We also derive a new concentration of measure inequality, which we name PAC-Bayes-Bennett, since it combines PAC-Bayesian bounding with Bennett’s inequality. We use it for empirical estimation of the oracle bound. The PAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality of Seldin et al. [2012]. We provide an empirical evaluation demonstrating that the new bounds can improve on the work of Masegosa et al. [2020]. Both the parametric form of the ChebyshevCantelli inequality and the PAC-Bayes-Bennett inequality may be of independent interest for the study of concentration of measure in other domains.","This paper presents a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev-Cantelli inequality, which is amenable to efficient minimization. The authors also derive a new concentration of measure inequality which combines PAC-Bayesian bounding with Bennett’s inequality. The empirical evaluation demonstrates that the new bounds can improve on the work of Masegosa et al. [2020]."
846,SP:5bac542a6532d43cf100e085398b4a4783719814,"The audio-visual video parsing task aims to temporally parse a video into audio or visual event categories. However, it is labor-intensive to temporally annotate audio and visual events and thus hampers the learning of a parsing model. To this end, we propose to explore additional cross-video and cross-modality supervisory signals to facilitate weakly-supervised audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, our method explores event co-occurrence across audio, visual, and audio-visual streams. We leverage the explored cross-modality co-occurrence to localize segments of target events while excluding irrelevant ones. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations. Quantitative and qualitative results demonstrate that the proposed method performs favorably against existing methods on weakly-supervised audio-visual video parsing.","This paper proposes a method for weakly supervised audio-visual video parsing. The authors propose to leverage cross-video and cross-modality supervisory signals to facilitate the learning of a parsing model. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, the authors explore event co-occurrence across audio, visual, and audio- visual streams to localize segments of target events while excluding irrelevant ones. Experiments show that the proposed method performs favorably against existing methods on the weakly-supervised video parsing task."
847,SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"Traditionally, federated learning (FL) aims to train a single global model while collaboratively using multiple clients and a server. Two natural challenges that FL algorithms face are heterogeneity in data across clients and collaboration of clients with diverse resources. In this work, we introduce a quantized and personalized FL algorithm QuPeD that facilitates collective (personalized model compression) training via knowledge distillation (KD) among clients who have access to heterogeneous data and resources. For personalization, we allow clients to learn compressed personalized models with different quantization parameters and model dimensions/structures. Towards this, first we propose an algorithm for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. When each client participating in the (federated) learning process has different requirements for the compressed model (both in model dimension and precision), we formulate a compressed personalization framework by introducing knowledge distillation loss for local client objectives collaborating through a global model. We develop an alternating proximal gradient update for solving this compressed personalization problem, and analyze its convergence properties. Numerically, we validate that QuPeD outperforms competing personalized FL methods, FedAvg, and local training of clients in various heterogeneous settings.","This paper proposes a personalized federated learning algorithm called QuPeD, which compresses the model of each client based on the knowledge distillation (KD) loss. The authors propose to learn quantized models through a relaxed optimization problem, where quantization values are also optimized over. For personalization, the authors propose an alternating proximal gradient update for solving the compressed personalization problem, and analyze its convergence properties. The experimental results show that the proposed algorithm outperforms FedAvg and local training of clients."
848,SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"Constrained clustering has gained significant attention in the field of machine learning as it can leverage prior information on a growing amount of only partially labeled data. Following recent advances in deep generative models, we propose a novel framework for constrained clustering that is intuitive, interpretable, and can be trained efficiently in the framework of stochastic gradient variational inference. By explicitly integrating domain knowledge in the form of probabilistic relations, our proposed model (DC-GMM) uncovers the underlying distribution of data conditioned on prior clustering preferences, expressed as pairwise constraints. These constraints guide the clustering process towards a desirable partition of the data by indicating which samples should or should not belong to the same cluster. We provide extensive experiments to demonstrate that DC-GMM shows superior clustering performances and robustness compared to state-of-the-art deep constrained clustering methods on a wide range of data sets. We further demonstrate the usefulness of our approach on two challenging real-world applications.","This paper proposes a generative model for constrained clustering based on probabilistic relations. Specifically, the authors propose to use a variational variational inference framework to learn the underlying distribution of data conditioned on prior clustering preferences, expressed as pairwise constraints. The proposed method is evaluated on two real-world datasets and compared with several state-of-the-art methods. The results show that DC-GMM outperforms the state of the art in terms of clustering performance."
849,SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide neural networks trained under least squares loss by gradient descent. Recent works also report that NTK regression can outperform finitely-wide neural networks trained on small-scale datasets. However, the computational complexity of kernel methods has limited its use in large-scale learning tasks. To accelerate learning with NTK, we design a near input-sparsity time approximation algorithm for NTK, by sketching the polynomial expansions of arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK) can transform any image using a linear runtime in the number of pixels. Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by combining random features (based on leverage score sampling) of the arc-cosine kernels with a sketching algorithm. We benchmark our methods on various large-scale regression and classification tasks and show that a linear regressor trained on our CNTK features matches the accuracy of exact CNTK on CIFAR-10 dataset while achieving 150× speedup.","This paper proposes a sketching-based approximation method for the Neural Tangent Kernel (NTK) and its convolutional counterpart (CNTK). The authors propose a near input-sparsity time approximation algorithm for NTK, by sketching the polynomial expansions of arc-cosine kernels. They also prove a spectral approximation guarantee for the NTK matrix, by combining random features (based on leverage score sampling) of the arc- cosine kernels with the sketching algorithm. The authors evaluate their methods on various large-scale regression and classification tasks and show that a linear regressor trained on our CNTK features matches the accuracy of exact NTK."
850,SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"We propose a novel framework for multi-person 3D motion trajectory prediction. Our key observation is that a human’s action and behaviors may highly depend on the other persons around. Thus, instead of predicting each human pose trajectory in isolation, we introduce a Multi-Range Transformers model which contains of a local-range encoder for individual motion and a global-range encoder for social interactions. The Transformer decoder then performs prediction for each person by taking a corresponding pose as a query which attends to both local and global-range encoder features. Our model not only outperforms state-of-the-art methods on long-term 3D motion prediction, but also generates diverse social interactions. More interestingly, our model can even predict 15-person motion simultaneously by automatically dividing the persons into different interaction groups. Project page with code is available at https://jiashunwang.github.io/MRT/.","This paper proposes a multi-person 3D motion trajectory prediction model that combines a local-range encoder for individual motion prediction and a global-range decoder for social interactions. The proposed model is based on the Transformer architecture. The model is evaluated on long-term motion prediction, and it is shown to outperform state-of-the-art methods. The paper is well-written and well-motivated."
851,SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"A key challenge for reinforcement learning is solving long-horizon planning problems. Recent work has leveraged programs to guide reinforcement learning in these settings. However, these approaches impose a high manual burden on the user since they must provide a guiding program for every new task. Partially observed environments further complicate the programming task because the program must implement a strategy that correctly, and ideally optimally, handles every possible configuration of the hidden regions of the environment. We propose a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. It trains a generative model to predict the unobserved portions of the world, and then synthesizes a program based on samples from this model in a way that is robust to its uncertainty. In our experiments, we show that our approach significantly outperforms non-program-guided approaches on a set of challenging benchmarks, including a 2D Minecraft-inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent. Our results demonstrate that our approach can obtain the benefits of program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task.","This paper proposes a method for training a generative model to predict the unobserved portions of the world and then synthesizing a program based on samples from this model in a way that is robust to its uncertainty. The method is evaluated on a set of long-horizon planning tasks in a 2D Minecraft-inspired environment, where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent."
852,SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"“Monkey see monkey do"" is an age-old adage, referring to naïve imitation without a deep understanding of a system’s underlying mechanics. Indeed, if a demonstrator has access to information unavailable to the imitator (monkey), such as a different set of sensors, then no matter how perfectly the imitator models its perceived environment (SEE), attempting to reproduce the demonstrator’s behavior (DO) can lead to poor outcomes. Imitation learning in the presence of a mismatch between demonstrator and imitator has been studied in the literature under the rubric of causal imitation learning (Zhang et al., 2020), but existing solutions are limited to single-stage decision-making. This paper investigates the problem of causal imitation learning in sequential settings, where the imitator must make multiple decisions per episode. We develop a graphical criterion that is necessary and sufficient for determining the feasibility of causal imitation, providing conditions when an imitator can match a demonstrator’s performance despite differing capabilities. Finally, we provide an efficient algorithm for determining imitability and corroborate our theory with simulations.",This paper studies the problem of causal imitation learning in the presence of a mismatch between the demonstrator and imitator. The authors propose a graphical criterion that is necessary and sufficient for determining the feasibility of such imitation learning. They provide an efficient algorithm for determining imitability and corroborate their theory with simulations.
853,SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,"We present a slot-wise, object-based transition model that decomposes a scene into objects, aligns them (with respect to a slot-wise object memory) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using transition losses at the level of the object-structured representation rather than pixels. Thanks to the introduction of our novel alignment module, the model deals properly with two issues that are not handled satisfactorily by other transition models, namely object persistence and object identity. We show that the combination of an objectlevel loss and correct object alignment over time enables the model to outperform a state-of-the-art baseline, and allows it to deal well with object occlusion and re-appearance in partially observable environments.","This paper presents a slot-wise, object-based transition model that decomposes a scene into objects, aligns them (with respect to a slotwise object memory) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using transition losses at the level of the object-structured representation rather than pixels. Thanks to the introduction of a novel alignment module, the model deals properly with two issues that are not handled satisfactorily by other transition models: object persistence and object identity. The combination of an objectlevel loss and correct object alignment over time enables the model to outperform a state-of-the-art baseline, and allows it to deal well with object occlusion and re-appearance in partially observable environments."
854,SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"Empirical risk minimization (ERM) is the workhorse of machine learning, whether for classification and regression or for off-policy policy learning, but its modelagnostic guarantees can fail when we use adaptively collected data, such as the result of running a contextual bandit algorithm. We study a generic importance sampling weighted ERM algorithm for using adaptively collected data to minimize the average of a loss function over a hypothesis class and provide first-of-their-kind generalization guarantees and fast convergence rates. Our results are based on a new maximal inequality that carefully leverages the importance sampling structure to obtain rates with the good dependence on the exploration rate in the data. For regression, we provide fast rates that leverage the strong convexity of squared-error loss. For policy learning, we provide regret guarantees that close an open gap in the existing literature whenever exploration decays to zero, as is the case for bandit-collected data. An empirical investigation validates our theory.","This paper studies the problem of adaptive risk minimization (ERM) in the context of bandit-collected data. In particular, the authors propose a weighted importance sampling weighted ERM algorithm for using adaptively collected data to minimize the average of a loss function over a hypothesis class and provide first-of-their-kind generalization guarantees and fast convergence rates. Their results are based on a new maximal inequality that carefully leverages the importance sampling structure to obtain rates with the good dependence on the exploration rate in the data. For regression, they provide fast rates that leverage the strong convexity of squared-error loss. For policy learning, the regret guarantees close an open gap in the existing literature whenever exploration decays to zero."
855,SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,"Many machine learning tasks that involve predicting an output response can be solved by training a weighted regression model. Unfortunately, the predictive power of this type of models may severely deteriorate under low sample sizes or under covariate perturbations. Reweighting the training samples has aroused as an effective mitigation strategy to these problems. In this paper, we propose a novel and coherent scheme for kernel-reweighted regression by reparametrizing the sample weights using a doubly non-negative matrix. When the weighting matrix is confined in an uncertainty set using either the log-determinant divergence or the Bures-Wasserstein distance, we show that the adversarially reweighted estimate can be solved efficiently using first-order methods. Numerical experiments show that our reweighting strategy delivers promising results on numerous datasets.","This paper proposes a novel reweighting strategy for kernel-reweighted regression. The authors propose to reparametrize the sample weights using a doubly non-negative matrix, which can be used to solve the adversarially reweighted estimate problem. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet, and the authors show that the proposed method outperforms the baseline."
856,SP:fe12e13602925b9400fd596a987755beb10aa3d1,"Training models with discrete latent variables is challenging due to the high variance of unbiased gradient estimators. While low-variance reparameterization gradients of a continuous relaxation can provide an effective solution, a continuous relaxation is not always available or tractable. Dong et al. (2020) and Yin et al. (2020) introduced a performant estimator that does not rely on continuous relaxations; however, it is limited to binary random variables. We introduce a novel derivation of their estimator based on importance sampling and statistical couplings, which we extend to the categorical setting. Motivated by the construction of a stick-breaking coupling, we introduce gradient estimators based on reparameterizing categorical variables as sequences of binary variables and Rao-Blackwellization. In systematic experiments, we show that our proposed categorical gradient estimators provide state-of-the-art performance, whereas even with additional Rao-Blackwellization, previous estimators (Yin et al., 2019) underperform a simpler REINFORCE with a leave-one-out-baseline estimator (Kool et al., 2019).","This paper proposes a novel estimator for categorical gradient estimators based on importance sampling and Rao-Blackwellization. The main idea is to reparameterize categorical variables as sequences of binary variables and use Rao-blackwellization to reduce the variance of the gradient estimator. The proposed estimator outperforms the state-of-the-art REINFORCE estimator, which is based on a leave-one-out-baseline estimator (Kool et al., 2019)."
857,SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"Neural Architecture Search (NAS) often trains and evaluates a large number of architectures. Recent predictor-based NAS approaches attempt to alleviate such heavy computation costs with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Given limited samples, these predictors, however, are far from accurate to locate top architectures due to the difficulty of fitting the huge search space. This paper reflects on a simple yet crucial question: if our final goal is to find the best architecture, do we really need to model the whole space well?. We propose a paradigm shift from fitting the whole architecture space using one strong predictor, to progressively fitting a search path towards the high-performance sub-space through a set of weaker predictors. As a key property of the weak predictors, their probabilities of sampling better architectures keep increasing. Hence we only sample a few well-performed architectures guided by the previously learned predictor and estimate a new better weak predictor. This embarrassingly easy framework, dubbed WeakNAS, produces coarse-to-fine iteration to gradually refine the ranking of sampling space. Extensive experiments demonstrate that WeakNAS costs fewer samples to find top-performance architectures on NAS-Bench-101 and NAS-Bench-201. Compared to state-of-the-art (SOTA) predictor-based NAS methods, WeakNAS outperforms all with notable margins, e.g., requiring at least 7.5x less samples to find global optimal on NAS-Bench-101. WeakNAS can also absorb their ideas to boost performance more. Further, WeakNAS strikes the new SOTA result of 81.3% in the ImageNet MobileNet Search Space. The code is available at: https://github.com/VITA-Group/WeakNAS.",This paper proposes a new predictor-based neural architecture search method called WeakNAS. The main idea is to progressively refine the search path towards the high-performance sub-space through a set of weaker predictors. The paper argues that the probability of sampling better architectures is increasing as the number of predictors increases. The proposed method is evaluated on NAS-Bench-101 and NAS-bench-201 and achieves better results than the state-of-the-art predictor-only NAS methods.
858,SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"An agent might be said, informally, to have mastery of its environment when it has maximised the effective number of states it can reliably reach. In practice, this often means maximizing the number of latent codes that can be discriminated from future states under some short time horizon (e.g. [15]). By situating these latent codes in a globally consistent coordinate system, we show that agents can reliably reach more states in the long term while still optimizing a local objective. A simple instantiation of this idea, Entropic Desired Dynamics for Intrinsic ConTrol (EDDICT), assumes fixed additive latent dynamics, which results in tractable learning and an interpretable latent space. Compared to prior methods, EDDICT’s globally consistent codes allow it to be far more exploratory, as demonstrated by improved state coverage and increased unsupervised performance on hard exploration games such as Montezuma’s Revenge.","This paper proposes a novel method for learning a global coordinate system for exploration in deep reinforcement learning. The method is based on the idea of entropic Desired Dynamics for Intrinsic ConTrol (EDDICT), which is an extension of Entropic DQN (DQN) that uses a fixed additive latent dynamics. The authors show that the proposed method is tractable and interpretable in terms of interpretable latent codes, and that it can be used in combination with other methods for exploration. Experiments show that EDDICT outperforms the state-of-the-art in Montezuma’s Revenge on a variety of tasks."
859,SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"Recently, utilizing reinforcement learning (RL) to generate molecules with desired properties has been highlighted as a promising strategy for drug design. A molecular docking program – a physical simulation that estimates protein-small molecule binding affinity – can be an ideal reward scoring function for RL, as it is a straightforward proxy of the therapeutic potential. Still, two imminent challenges exist for this task. First, the models often fail to generate chemically realistic and pharmacochemically acceptable molecules. Second, the docking score optimization is a difficult exploration problem that involves many local optima and less smooth surfaces with respect to molecular structure. To tackle these challenges, we propose a novel RL framework that generates pharmacochemically acceptable molecules with large docking scores. Our method – Fragment-based generative RL with Explorative Experience replay for Drug design (FREED) – constrains the generated molecules to a realistic and qualified chemical space and effectively explores the space to find drugs by coupling our fragment-based generation method and a novel error-prioritized experience replay (PER). We also show that our model performs well on both de novo and scaffold-based schemes. Our model produces molecules of higher quality compared to existing methods while achieving state-of-the-art performance on two of three targets in terms of the docking scores of the generated molecules. We further show with ablation studies that our method, predictive error-PER (FREED(PE)), significantly improves the model performance.",This paper proposes a method to generate molecules with pharmacochemically acceptable properties for drug design. The authors propose a fragment-based generative RL with Explorative Experience replay for Drug design (FREED) method that constrains the generated molecules to a realistic and qualified chemical space and effectively explores the space to find drugs by coupling the fragment generation method and a novel error-prioritized experience replay (PER). The authors also show that their model performs well on both de novo and scaffold-based schemes and achieves state-of-the-art performance on two of three targets.
860,SP:b938bca513e7de1231212064caf8877a78d8b612,"We analyze the complexity of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. Our approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. Perhaps surprisingly, we show that for certain graph ensembles, a simple forward greedy search algorithm (i.e. without a backward pruning phase) suffices to learn the Markov boundary of each node. This substantially improves the sample complexity, which we show is at most polynomial in the number of nodes. This is then applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature. As a matter of independent interest, we establish finite-sample guarantees for the problem of recovering Markov boundaries from data. Moreover, we apply our results to the special case of polytrees, for which the assumptions simplify, and provide explicit conditions under which polytrees are identifiable and learnable in polynomial time. We further illustrate the performance of the algorithm, which is easy to implement, in a simulation study. Our approach is general, works for discrete or continuous distributions without distributional assumptions, and as such sheds light on the minimal assumptions required to efficiently learn the structure of directed graphical models from data.","This paper studies the problem of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. The authors propose an information-theoretic algorithm that uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. They show that for certain graph ensembles, a simple forward greedy search algorithm (i.e. without a backward pruning phase) suffices to learn the Markov boundaries of each node. This substantially improves the sample complexity, which is at most polynomial in the number of nodes. This is then applied to the entire graph under a novel identifiability condition that generalizes existing conditions from the literature."
861,SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"Most works in learning with differential privacy (DP) have focused on the setting where each user has a single sample. In this work, we consider the setting where each user holds m samples and the privacy protection is enforced at the level of each user’s data. We show that, in this setting, we may learn with a much fewer number of users. Specifically, we show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an ("", )DP algorithm using only O(log(1/ )/"") users. For ""-DP algorithms, we show that we can learn using only O""(d) users even in the local model, where d is the probabilistic representation dimension. In both cases, we show a nearly-matching lower bound on the number of users required. A crucial component of our results is a generalization of global stability [BLM20] that allows the use of public randomness. Under this relaxed notion, we employ a correlated sampling strategy to show that the global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the number of samples.","This paper studies the problem of learning with differential privacy (DP) in the setting where each user holds m samples and the privacy protection is enforced at the level of each user’s data. The authors show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an ("", )DP algorithm using only O(log(1/\sqrt{1}) users. The main contribution of this paper is a generalization of global stability [BLM20] that allows the use of public randomness. The global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the number of samples. "
862,SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"Estimating the per-state expected cumulative rewards is a critical aspect of reinforcement learning approaches, however the experience is obtained, but standard deep neural-network function-approximation methods are often inefficient in this setting. An alternative approach, exemplified by value iteration networks, is to learn transition and reward models of a latent Markov decision process whose value predictions fit the data. This approach has been shown empirically to converge faster to a more robust solution in many cases, but there has been little theoretical study of this phenomenon. In this paper, we explore such implicit representations of value functions via theory and focused experimentation. We prove that, for a linear parametrization, gradient descent converges to global optima despite nonlinearity and non-convexity introduced by the implicit representation. Furthermore, we derive convergence rates for both cases which allow us to identify conditions under which stochastic gradient descent (SGD) with this implicit representation converges substantially faster than its explicit counterpart. Finally, we provide empirical results in some simple domains that illustrate the theoretical findings.","This paper studies the effect of implicit representations of value functions on the convergence of SGD in the context of reinforcement learning. In particular, the authors consider the case of value iteration networks (VINs) and show that the implicit representation of the value function can lead to a faster convergence rate than the explicit representation. The authors also provide some theoretical analysis of the convergence rate for the linear case and the non-linear case. Finally, they provide empirical results in some simple domains that illustrate the theoretical findings."
863,SP:992aa07d4f815d1c81f967374590eece933833b1,"Knowledge Graphs (KGs) extracted from text sources are often noisy and lead to poor performance in downstream application tasks such as KG-based question answering. While much of the recent activity is focused on addressing the sparsity of KGs by using embeddings for inferring new facts, the issue of cleaning up of noise in KGs through KG refinement task is not as actively studied. Most successful techniques for KG refinement make use of inference rules and reasoning over ontologies. Barring a few exceptions, embeddings do not make use of ontological information, and their performance in KG refinement task is not well understood. In this paper, we present a KG refinement framework called IterefinE which iteratively combines the two techniques – one which uses ontological information and inferences rules, viz.,PSL-KGI, and the KG embeddings such as ComplEx and ConvE which do not. As a result, IterefinE is able to exploit not only the ontological information to improve the quality of predictions, but also the power of KG embeddings which (implicitly) perform longer chains of reasoning. The IterefinE framework, operates in a co-training mode and results in explicit type-supervised embeddings of the refined KG from PSL-KGI which we call as TypeE-X. Our experiments over a range of KG benchmarks show that the embeddings that we produce are able to reject noisy facts from KG and at the same time infer higher quality new facts resulting in upto 9% improvement of overall weighted F1 score.","This paper proposes a method to refine Knowledge Graphs (KGs) by combining PSL-KGI and KG embeddings such as ComplEx and ConvE. The proposed method is based on the idea of co-training a KG-based model with a type-supervised embedding of the refined KG, which is called TypeE-X. The authors evaluate the proposed method on a variety of KG benchmarks and show that it can improve the quality of the KG and the new facts."
864,SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"Knowledge base completion (KBC) methods aim at inferring missing facts from the information present in a knowledge base (KB). Such a method thus needs to estimate the likelihood of candidate facts and ultimately to distinguish between true facts and false ones to avoid compromising the KB with untrue information. In the prevailing evaluation paradigm, however, models do not actually decide whether a new fact should be accepted or not but are solely judged on the position of true facts in a likelihood ranking with other candidates. We argue that consideration of binary predictions is essential to reflect the actual KBC quality, and propose a novel evaluation paradigm, designed to provide more transparent model selection criteria for a realistic scenario. We construct the data set FB14k-QAQ with an alternative evaluation data structure: instead of single facts, we use KB queries, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. We randomly remove some of these correct answers from the data set, simulating the realistic scenario of real-world entities missing from a KB. This way, we can explicitly measure a model’s ability to handle queries that have more correct answers in the real world than in the KB, including the special case of queries without any valid answer. The latter especially contrasts the ranking setting. We evaluate a number of state-of-the-art KB embeddings models on our new benchmark. The differences in relative performance between ranking-based and classification-based evaluation that we observe in our experiments confirm our hypothesis that good performance on the ranking task does not necessarily translate to good performance on the actual completion task. Our results motivate future work on KB embedding models with better prediction separability and, as a first step in that direction, we propose a simple variant of TransE that encourages thresholding and achieves a significant improvement in classification F1 score relative to the original TransE.","This paper proposes a novel evaluation paradigm for knowledge base completion (KBC) methods. The authors argue that consideration of binary predictions is essential to reflect the actual KBC quality, and propose a new evaluation paradigm, designed to provide more transparent model selection criteria for a realistic scenario. They construct the data set FB14k-QAQ with an alternative evaluation data structure: instead of single facts, they use KB queries, i.e., facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. This way, they can explicitly measure a model’s ability to handle queries that have more correct answers in the real world than in the KB, including the special case of queries without any valid answer. They evaluate a number of state-of-the-art KB embeddings models on the new benchmark. The differences in relative performance between ranking-based and classification-based evaluation that they observe in their experiments confirm their hypothesis that good performance on the ranking task does not necessarily translate to good performance in the actual completion task."
865,SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) have suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Roles Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pretrained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is capable of generating human-like responses to persuade people to donate to a charity.","This paper proposes a new model for dialog system models that uses a pre-trained language model to model each speaker separately and takes advantage of the large pretrained language model. The proposed model, Alternating Roles Dialog Model (ARDM), is trained on two task-oriented dialog datasets: CamRest676 and MultiWOZ. ARDM outperforms or is on par with state-of-the-art methods on both tasks. It can generalize to more challenging, non-collaborative tasks such as persuasion."
866,SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,"How well can we estimate the probability that the classification predicted by a deep neural network is correct (or in the Top 5)? It is well-known that the softmax values of the network are not estimates of the probabilities of class labels. However, there is a misconception that these values are not informative. We define the notion of implied loss and prove that if an uncertainty measure is an implied loss, then low uncertainty means high probability of correct (or top k) classification on the test set. We demonstrate empirically that these values can be used to measure the confidence that the classification is correct. Our method is simple to use on existing networks: we proposed confidence measures for Top k which can be evaluated by binning values on the test set.","This paper studies the problem of estimating the probability that the classification predicted by a deep neural network is correct (or in the Top 5) on the test set. It is well-known that the softmax values of the network are not estimates of the probabilities of class labels. However, there is a misconception that these values are not informative. The authors define the notion of implied loss and prove that if an uncertainty measure is an implied loss, then low uncertainty means high probability of correct (top k) classification on the tests set. The paper proposes a confidence measures for Top k which can be evaluated by binning values on the Test set."
867,SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"A fundamental goal in deep learning is the characterization of trainability and generalization of neural networks as a function of their architecture and hyperparameters. In this paper, we discuss these challenging issues in the context of wide neural networks at large depths where we will see that the situation simplifies considerably. To do this, we leverage recent advances that have separately shown: (1) that in the wide network limit, random networks before training are Gaussian Processes governed by a kernel known as the Neural Network Gaussian Process (NNGP) kernel, (2) that at large depths the spectrum of the NNGP kernel simplifies considerably and becomes “weakly data-dependent”, and (3) that gradient descent training of wide neural networks is described by a kernel called the Neural Tangent Kernel (NTK) that is related to the NNGP. Here we show that in the large depth limit the spectrum of the NTK simplifies in much the same way as that of the NNGP kernel. By analyzing this spectrum, we arrive at a precise characterization of trainability and a necessary condition for generalization across a range of architectures including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). In particular, we find that there are large regions of hyperparameter space where networks can only memorize the training set in the sense they reach perfect training accuracy but completely fail to generalize outside the training set, in contrast with several recent results. By comparing CNNs withand without-global average pooling, we show that CNNs without average pooling have very nearly identical learning dynamics to FCNs while CNNs with pooling contain a correction that alters its generalization performance. We perform a thorough empirical investigation of these theoretical results and finding excellent agreement on real datasets.","This paper studies the generalization properties of neural networks in the context of wide neural networks at large depths. The authors show that in the wide network limit, random networks before training are Gaussian processes governed by a kernel known as the Neural Network Gaussian Process (NNGP) kernel, and that at large depth the spectrum of the NNGP kernel simplifies considerably and becomes “weakly data-dependent”. In contrast, gradient descent training of wide networks is described by the Neural Tangent Kernel (NTK), which is related to the neural network Gaussian process kernel (NNGP) kernel. The spectrum of NTK is shown to be much the same way as that of NNGGP kernel in the large depth limit. By analyzing this spectrum, the authors arrive at a precise characterization of trainability and a necessary condition for generalization across a range of architectures including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs)."
868,SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"Proteins are ubiquitous molecules whose function in biological processes is determined by their 3D structure. Experimental identification of a protein’s structure can be time-consuming, prohibitively expensive, and not always possible. Alternatively, protein folding can be modeled using computational methods, which however are not guaranteed to always produce optimal results. GRAPHQA is a graph-based method to estimate the quality of protein models, that possesses favorable properties such as representation learning, explicit modeling of both sequential and 3D structure, geometric invariance and computational efficiency. In this work, we demonstrate significant improvements over the state-ofthe-art for both hand-engineered and representation-learning approaches, as well as carefully evaluating the individual contributions of GRAPHQA components.","This paper proposes a graph-based method to estimate the quality of protein models. The authors propose GRAPHQA, which is a graph based method that is able to model both sequential and 3D structure of proteins. The method is based on Graph-based representation learning, and the authors show that it can be used for both hand-engineered and representation-learning approaches. The experimental results show that the proposed method outperforms the state-of-the-art in terms of computational efficiency."
869,SP:5188280131b58a35d3deda126a0754aea8fa6e58,"The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network’s weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. We use geometric properties of determinantal varieties to derive new results on the landscape of linear networks with different loss functions and different parameterizations. Our analysis clearly illustrates that the absence of “bad” local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear maps (“filling architectures”) but it holds only for the quadratic loss when the functional space is a determinantal variety (“non-filling architectures”). Without any assumption on the architecture, smooth convex losses may lead to landscapes with many bad minima.","This paper studies the critical locus of the loss function of a linear neural network, which is determined by the geometry of the functional space and by the parameterization of this space by the network’s weights. The functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. The authors introduce a natural distinction between pure critical points and spurious critical points, which arise from the parameters of the network. The analysis clearly illustrates that the absence of “bad” local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express linear maps (“filling architectures”) but it holds only for the quadratic loss when the functional spaces is a determinantsal variety. Without any assumption on the architecture, smooth conveX losses may lead to landscapes with many bad minima."
870,SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"Inductive and unsupervised graph learning is a critical technique for predictive or information retrieval tasks where label information is difficult to obtain. It is also challenging to make graph learning inductive and unsupervised at the same time, as learning processes guided by reconstruction error based loss functions inevitably demand graph similarity evaluation that is usually computationally intractable. In this paper, we propose a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Instead of directly dealing with the computational challenges raised by graph similarity evaluation, given an input graph, the SEED framework samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of subgraph vectors, and employs the embedding of the subgraph vector distribution as the output vector representation for the input graph. By theoretical analysis, we demonstrate the close connection between SEED and graph isomorphism. Using public benchmark datasets, our empirical study suggests the proposed SEED framework is able to achieve up to 10% improvement, compared with competitive baseline methods.","This paper proposes SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. The proposed SEED framework samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of sub graph vectors, and uses the embedding of the sub graph vector distribution as the output vector representation for the input graph. Theoretical analysis is provided to demonstrate the close connection between SEED and graph isomorphism. Empirical results on several benchmark datasets demonstrate the effectiveness of SEED."
871,SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,"Counterfactual regret minimization (CFR) methods are effective for solving twoplayer zero-sum extensive games with imperfect information. However, the vanilla CFR has to traverse the whole game tree in each round, which is time-consuming in large-scale games. In this paper, we present Lazy-CFR, a CFR algorithm that adopts a lazy update strategy to avoid traversing the whole game tree in each round. We prove that the regret of Lazy-CFR is almost the same as the regret of the vanilla CFR and only needs to visit a small portion of the game tree. Thus, Lazy-CFR is provably faster than CFR. Empirical results consistently show that Lazy-CFR is fast in practice.",This paper proposes a new counterfactual regret minimization algorithm for zero-sum extensive games. The main idea is to use a lazy update strategy to avoid traversing the whole game tree in each round. The authors prove that the regret of Lazy-CFR is almost the same as that of the vanilla CFR and only needs to visit a small portion of the game tree. Empirical results show that the proposed algorithm is fast in practice.
872,SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"State-of-the-art Unsupervised Domain Adaptation (UDA) methods learn transferable features by minimizing the feature distribution discrepancy between the source and target domains. Different from these methods which do not model the feature distributions explicitly, in this paper, we explore explicit feature distribution modeling for UDA. In particular, we propose Distribution Matching Prototypical Network (DMPN) to model the deep features from each domain as Gaussian mixture distributions. With explicit feature distribution modeling, we can easily measure the discrepancy between the two domains. In DMPN, we propose two new domain discrepancy losses with probabilistic interpretations. The first one minimizes the distances between the corresponding Gaussian component means of the source and target data. The second one minimizes the pseudo negative log likelihood of generating the target features from source feature distribution. To learn both discriminative and domain invariant features, DMPN is trained by minimizing the classification loss on the labeled source data and the domain discrepancy losses together. Extensive experiments are conducted over two UDA tasks. Our approach yields a large margin in the Digits Image transfer task over state-of-the-art approaches. More remarkably, DMPN obtains a mean accuracy of 81.4% on VisDA 2017 dataset. The hyper-parameter sensitivity analysis shows that our approach is robust w.r.t hyper-parameter changes.","This paper proposes a novel method for Unsupervised Domain Adaptation (UDA) based on explicit feature distribution modeling. Specifically, the authors propose to model the deep features from each domain as Gaussian mixture distributions. The authors also propose two new domain discrepancy losses with probabilistic interpretations. The first one minimizes the distance between the corresponding Gaussian component means of the source and target data. The second one is the pseudo negative log likelihood of generating the target features from source feature distribution. To learn both discriminative and domain invariant features, DMPN is trained by minimizing the classification loss on the labeled source data and the domain discrepancy loss together. Extensive experiments are conducted over two UDA tasks. The proposed method achieves state-of-the-art performance on VisDA 2017 dataset."
873,SP:40be996e8bb86e887077b762b87c7c34a786ac98,"Continuous Normalizing Flows (CNFs) have emerged as promising deep generative models for a wide range of tasks thanks to their invertibility and exact likelihood estimation. However, conditioning CNFs on signals of interest for conditional image generation and downstream predictive tasks is inefficient due to the highdimensional latent code generated by the model, which needs to be of the same size as the input data. In this paper, we propose InfoCNF, an efficient conditional CNF that partitions the latent space into a class-specific supervised code and an unsupervised code that shared among all classes for efficient use of labeled information. Since the partitioning strategy (slightly) increases the number of function evaluations (NFEs), InfoCNF also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. We show empirically that InfoCNF improves the test accuracy over the baseline while yielding comparable likelihood scores and reducing the NFEs on CIFAR10. Furthermore, applying the same partitioning strategy in InfoCNF on time-series data helps improve extrapolation performance. Additional details at https://sites.google.com/view/infocnf-iclr/","This paper proposes InfoCNF, a conditional continuous normalizing flow (CNF) model that partitions the latent space into a class-specific supervised code and an unsupervised code that is shared among all classes for efficient use of labeled information. The partitioning strategy (slightly) increases the number of function evaluations (NFEs) and the authors also employ gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. Experiments on CIFAR-10 show that the proposed method improves the test accuracy over the baseline while yielding comparable likelihood scores and reducing the NFEs. The authors also apply the same partitioning method to time-series data to improve extrapolation performance."
874,SP:97764e3393216106ff2ac3f550845acf4636119f,"We discuss the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the Temporal-Difference (TD) learning algorithm. We consider this problem under a certain scaling of the approximating function, leading to a regime called lazy training. In this regime the parameters of the model vary only slightly during the learning process, a feature that has recently been observed in the training of neural networks, where the scaling we study arises naturally, implicit in the initialization of their parameters. Both in the underand over-parametrized frameworks, we prove exponential convergence to local, respectively global minimizers of the above algorithm in the lazy training regime. We then give examples of such convergence results in the case of models that diverge if trained with non-lazy TD learning, and in the case of neural networks.","This paper studies the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the Temporal-Difference (TD) learning algorithm. The authors consider the problem under a certain scaling of the approximating function, leading to a regime called lazy training. In this regime the parameters of the model vary only slightly during the learning process, a feature that has recently been observed in the training of neural networks, where the scaling arises naturally, implicit in the initialization of their parameters. Both in the under and over-parametrized frameworks, the authors prove exponential convergence to local and global minimizers of the above algorithm in the lazy training regime. They then give examples of such convergence results in the case of models that diverge if trained with non-lazy TD learning."
875,SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses – they can be formulated as triplets (pre-condition, action sequence, post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses. Our work takes a step towards a “scientist agent” that develops an understanding of the world by generating and testing hypotheses about its environment.","This paper proposes a reinforcement learning method for hypothesis verification. The main idea is to train an agent to generate observations that can be used to predict whether the hypothesis is true or false. The agent is trained to take actions that generate observations which can help predict the hypothesis. The authors propose to train the agent on a set of triplets (pre-condition, action sequence, post-condition) and then fine-tune the agent to verify more general hypotheses."
876,SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of this reasoning, we perform sequences of rewrite steps both in formula space and in latent space, and compare the quality of embeddings of the resulting formulas to their predicted embeddings. Our experiments show that graph neural networks can make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps. Since our corpus of mathematical formulas includes a wide variety of mathematical disciplines, this experiment is a strong indicator for the feasibility of deduction in latent space in general.","This paper proposes a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. The paper proposes to compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether it can be rewritten by other theorems. The experiments show that graph neural networks are able to make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps."
877,SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"Natural intelligent agents learn to perceive the three dimensional structure of the world without training on large datasets and are unlikely to have the precise equations of projective geometry hard-wired in the brain. Such skill would also be valuable to artificial systems in order to avoid the expensive collection of labeled datasets, as well as tedious tuning required by methods based on multi-view geometry. Inspired by natural agents, who interact with the environment via visual and haptic feedback, this paper explores a new approach to learning depth from images and very sparse depth measurements, just a few pixels per image. To learn from such extremely sparse supervision, we introduce an appropriate inductive bias by designing a specialized global-local network architecture. Experiments on several datasets show that the proposed model can learn monocular dense depth estimation when trained with very sparse ground truth, even a single pixel per image. Moreover, we find that the global parameters extracted by the network are predictive of the metric agent motion.","This paper proposes a method for learning depth estimation from images with very sparse ground truth. The method is inspired by natural agents, who interact with the environment via visual and haptic feedback. To learn from such extremely sparse supervision, the authors introduce an appropriate inductive bias by designing a specialized global-local network architecture. Experiments on several datasets show that the proposed model can learn monocular dense depth estimation when trained with a single pixel per image. The global parameters extracted by the network are predictive of the metric agent motion."
878,SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size.","This paper extends the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. The authors show that by applying a multi-layer Transformer to these Bloom filter digests, they are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, model of a much larger size trained using sampled softmax with the same computational budget."
879,SP:745dd86d7f7bba79a02d27922003b764b620f83e,"We address the problem of discovering 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learningbased agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen categories. On the largescale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four shape segmentation baselines shows that our approach achieve the state-of-the-art performance. [Project Page]","This paper proposes a method for learning 3D parts for objects in unseen categories. The proposed method is based on a learning-based agglomerative clustering framework, which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. The method is evaluated on the PartNet dataset and compared against four shape segmentation baselines."
880,SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"While generative neural networks can learn to transform a specific input dataset into a specific target dataset, they require having just such a paired set of input/output datasets. For instance, to fool the discriminator, a generative adversarial network (GAN) exclusively trained to transform images of black-haired men to blond-haired men would need to change gender-related characteristics as well as hair color when given images of black-haired women as input. This is problematic, as often it is possible to obtain a pair of (source, target) distributions but then have a second source distribution where the target distribution is unknown. The computational challenge is that generative models are good at generation within the manifold of the data that they are trained on. However, generating new samples outside of the manifold or extrapolating “out-of-sample” is a much harder problem that has been less well studied. To address this, we introduce a technique called neuron editing that learns how neurons encode an edit for a particular transformation in a latent space. We use an autoencoder to decompose the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in a latent trained space, we encode fairly complex and non-linear transformations to the data with much simpler distribution shifts to the neuron’s activations. Our technique has the advantage of being generally applicable to a wide variety of data domains, modalities, and applications. We first demonstrate it on image transformations and then move to our two main applications in biology: removal of batch artifacts representing unwanted noise and modeling the effect of drug treatments to predict synergy between drugs.","This paper proposes a method for generating new samples from out-of-sample data. The method is based on an autoencoder that decomposes the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in a latent trained space, the authors encode fairly complex and non-linear transformations to the data with much simpler distribution shifts to the neuron’s activations. The technique has the advantage of being generally applicable to a wide variety of data domains, modalities, and applications. The authors demonstrate it on image transformations and then move to two main applications in biology: removal of batch artifacts representing unwanted noise and modeling the effect of drug treatments to predict synergy between drugs."
881,SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"While meta-learning approaches that utilize neural network representations have made progress in few-shot image classification, reinforcement learning, and, more recently, image semantic segmentation, the training algorithms and model architectures have become increasingly specialized to the few-shot domain. A natural question that arises is how to develop learning systems that scale from few-shot to many-shot settings while yielding competitive performance in both. One scalable potential approach that does not require ensembling many models nor the computational costs of relation networks, is to meta-learn an initialization. In this work, we study first-order meta-learning of initializations for deep neural networks that must produce dense, structured predictions given an arbitrary amount of training data for a new task. Our primary contributions include (1), an extension and experimental analysis of first-order model agnostic meta-learning algorithms (including FOMAML and Reptile) to image segmentation, (2) a novel neural network architecture built for parameter efficiency and fast learning which we call EfficientLab, (3) a formalization of the generalization error of meta-learning algorithms, which we leverage to decrease error on unseen tasks, and (4) a small benchmark dataset, FP-k, for the empirical study of how meta-learning systems perform in both fewand many-shot settings. We show that meta-learned initializations for image segmentation provide value for both canonical few-shot learning problems and larger datasets, outperforming random and ImageNet-trained initializations for up to 400 densely labeled examples. Finally, we show both theoretically and empirically that a key limitation of MAML-type algorithms is that when adapting to new tasks, a single update routine is used that is not conditioned on the available data for a new task. We find that our network, with an empirically estimated optimal update procedure yields state of the art results on the FSS-1000 dataset, while only requiring one forward pass through a single model at evaluation time.","This paper proposes a meta-learning method for image segmentation. The method is based on the idea of first-order meta learning of initializations for deep neural networks that must produce dense, structured predictions given an arbitrary amount of training data for a new task. The authors propose a novel neural network architecture for parameter efficiency and fast learning which they call EfficientLab, a formalization of the generalization error of meta learning algorithms, which they leverage to decrease error on unseen tasks, and a small benchmark dataset, FP-k, for the empirical study of how meta learning systems perform in both few-shot and many-shot settings. They show that their network, with an empirically estimated optimal update procedure yields state-of-the-art results on the FSS-1000 dataset, while only requiring one forward pass through a single model at evaluation time."
882,SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"Learning from a few examples is a key characteristic of human intelligence that inspired machine learning researchers to build data efficient AI models. Recent progress has shown that few-shot learning can be improved with access to unlabelled data, known as semi-supervised few-shot learning(SS-FSL). We introduce an SS-FSL approach, dubbed as Prototypical Random Walk Networks(PRWN), built on top of Prototypical Networks (PN) (Ren et al., 2018). We develop a random walk semi-supervised loss that enables the network to learn representations that are compact and well-separated. Our work is related to the very recent development on graph-based approaches for few-shot learning. However, we show that compact and well-separated class representations can be achieved by modeling our prototypical random walk notion without needing additional graph-NN parameters or requiring a transductive setting where collective test set is provided (e.g., Kim et al. (2019)). Our model outperforms prior art in most benchmarks with significant improvements in some cases. For example, in a mini-Imagenet 5-shot classification task, we obtain 69.65% accuracy to the 64.59% state-of-theart. Our model, trained with 40% of the data as labelled, compares competitively against fully supervised prototypical networks, trained on 100% of the labels, even outperforming it in the 1-shot mini-Imagenet case with 50.89% to 49.4% accuracy. We also show that our model is resistant to distractors, unlabeled data that does not belong to any of the training classes, and hence reflecting robustness to labelled/unlabelled class distribution mismatch. We also performed a challenging discriminative power test, showing a relative improvement on top of the baseline of≈14% on 20 classes on mini-Imagenet and≈60% on 800 classes on Omniglot. Our code will be released upon acceptance.","This paper proposes a novel method for semi-supervised few-shot learning. The method is based on Prototypical Networks (PN) and a random walk loss. The authors show that the proposed method is able to learn representations that are compact and well-separated. They also show that their model is resistant to distractors, unlabeled data that does not belong to any of the training classes, and hence reflects robustness to labelled/unlabelled class distribution mismatch."
883,SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent selfsupervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused. Our code is available at https://storage.cloud.google.com/public-published-datasets/csf_code.zip. Bare Rock Breakwater Bridge Dam Farm (building) Farmland Forest Golf Course Quarry Stadium Substation Water water stadium","This paper proposes a new self-supervised training objective for multi-sensor representation learning in remote sensing. The proposed objective is based on contrastive sensor fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. The authors train an encoder to produce semantically meaningful representations from any combination of channels from the input sensors. The encoder is trained on a dataset of 47 million unlabeled image triplets, which is used to train the encoder."
884,SP:4d8e054f07006b4f896721b5c24da805727d2c22,"Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al. (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.","This paper studies the problem of neural network pruning. The authors compare the performance of weight rewinding (WRT) and learning rate rewiring (LRT) on a variety of tasks, including fine-tuning, network compression, and network-agnostic pruning, and show that WRT and LRT outperform the standard fine-tune-based pruning methods. In particular, LRT is shown to outperform WRT in terms of accuracy and compression ratio. In addition, the authors show that LRT can be used to train the weights of a network to their values from earlier in training and retrain them from there."
885,SP:3bb1c79f9482e09828eda45fbb2e654f37219365,"For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound – a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the “all-layer margin.” Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.","This paper studies the relationship between output margin and generalization in deep neural networks. The authors propose to analyze a new notion of margin, which they call the “all-layer margin” and show that it has a clear and direct relationship with generalization for deep models. In particular, the authors show that by analyzing the all layer margin, they obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth. In addition, they provide a theoretically inspired training algorithm for increasing the alllayer margin."
886,SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge-grounded dialogues, as training data for learning such a response generation model, are difficult to obtain. Motivated by the challenge in practice, we consider knowledge-grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a disentangled response decoder in order to isolate parameters that depend on knowledge-grounded dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be well fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only 1/8 training data, our model can achieve the state-of-the-art performance and generalize well on out-of-domain knowledge.",This paper proposes a method for knowledge-grounded dialogue generation. The authors propose a disentangled response decoder to isolate the parameters that depend on knowledge-based dialogues from the entire generation model. The proposed method is evaluated on two benchmark datasets and compared with the state-of-the-art. 
887,SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"Training neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of language pairs and scenarios, including resource-rich and low-resource situations.","This paper proposes a mirror-generative neural machine translation model (MGNMT) that simultaneously integrates the source to target translation model, the target to source translation model and two language models. The main idea is that both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of language pairs and scenarios."
888,SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms. Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we show how streamlined algorithms without entropy maximization can match the performance of SAC. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.","This paper studies the role of the entropy term in Soft Actor Critic (SAC) in Deep Reinforcement Learning (DRL) algorithms. The authors show that SAC's entropy term is limited to the action spaces of the Mujoco benchmark, and propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. The proposed method is shown to outperform SAC and other SAC-based algorithms on the MuJoco benchmark."
889,SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. We discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks. These vulnerabilities are especially apparent for neural network based systems. As proof of concept, we describe a well-known music identification method and implement this system in the form of a neural net. We then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube’s Content ID system. Our goal is to raise awareness of the threats posed by adversarial examples in this space and to highlight the importance of hardening copyright detection systems to attacks.","This paper studies the problem of adversarial attacks on industrial copyright detection systems. The authors propose to use a neural network-based system to detect the copyright of a piece of music, and then attack this system using simple gradient methods. They demonstrate the effectiveness of the proposed method on YouTube's Content ID system, AudioTag copyright detector, and YouTube's Music ID system. "
890,SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"This work explores the visual explanation for deep metric learning and its applications. As an important problem for learning representation, metric learning has attracted much attention recently, while the interpretation of such model is not as well studied as classification. To this end, we propose an intuitive idea to show where contributes the most to the overall similarity of two input images by decomposing the final activation. Instead of only providing the overall activation map of each image, we propose to generate point-to-point activation intensity between two images so that the relationship between different regions is uncovered. We show that the proposed framework can be directly deployed to a large range of metric learning applications and provides valuable information for understanding the model. Furthermore, our experiments show its effectiveness on two potential applications, i.e. cross-view pattern discovery and interactive retrieval.",This paper proposes to decompose the final activation map of deep metric learning (DML) into two parts: (1) the overall activation map and (2) the point-to-point activation intensity between two images. The proposed decomposition is based on the idea that the point to point activation intensity can be used to show the relationship between different regions of the image. The authors show that the proposed method can be directly deployed to a large range of metric learning applications and provides valuable information for understanding the model. The experiments show its effectiveness on two potential applications: cross-view pattern discovery and interactive retrieval.
891,SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"We study learning control in an online lifelong learning scenario, where mistakes can compound catastrophically into the future and the underlying dynamics of the environment may change. Traditional model-free policy learning methods have achieved successes in difficult tasks due to their broad flexibility, and capably condense broad experiences into compact networks, but struggle in this setting, as they can activate failure modes early in their lifetimes which are difficult to recover from and face performance degradation as dynamics change. On the other hand, model-based planning methods learn and adapt quickly, but require prohibitive levels of computational resources. Under constrained computation limits, the agent must allocate its resources wisely, which requires the agent to understand both its own performance and the current state of the environment: knowing that its mastery over control in the current dynamics is poor, the agent should dedicate more time to planning. We present a new algorithm, Adaptive Online Planning (AOP), that achieves strong performance in this setting by combining model-based planning with model-free learning. By measuring the performance of the planner and the uncertainty of the model-free components, AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times. We show that AOP gracefully deals with novel situations, adapting behaviors and policies effectively in the face of unpredictable changes in the world – challenges that a continual learning agent naturally faces over an extended lifetime – even when traditional reinforcement learning methods fail.","This paper studies the problem of continual learning in an online lifelong learning scenario. The authors propose a new algorithm called Adaptive Online Planning (AOP) that combines model-based planning with model-free learning. The main idea is to use a planner to estimate the performance of the planner and the uncertainty of the model-focussed components of the policy, and then use the planner to decide when to use more extensive planning only when necessary, leading to reduced computation times. Experiments show that AOP is able to deal with novel situations, adapting behaviors and policies effectively in the face of unpredictable changes in the world."
892,SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"Visual attention mechanisms have been widely used in image captioning models. In this paper, to better link the image structure with the generated text, we replace the traditional softmax attention mechanism by two alternative sparsity-promoting transformations: sparsemax and Total-Variation Sparse Attention (TVMAX). With sparsemax, we obtain sparse attention weights, selecting relevant features. In order to promote sparsity and encourage fusing of the related adjacent spatial locations, we propose TVMAX. By selecting relevant groups of features, the TVMAX transformation improves interpretability. We present results in the Microsoft COCO and Flickr30k datasets, obtaining gains in comparison to softmax. TVMAX outperforms the other compared attention mechanisms in terms of humanrated caption quality and attention relevance.","This paper proposes to replace the traditional softmax attention mechanism by two alternative sparsity-promoting transformations: sparsemax and Total-variation Sparse Attention (TVMAX). With sparsemax, we obtain sparse attention weights, selecting relevant features. With TVMAX, we propose to fusing of the related adjacent spatial locations. By selecting relevant groups of features, the TVMAX transformation improves interpretability."
893,SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,"Neural networks for structured data like graphs have been studied extensively in recent years. To date, the bulk of research activity has focused mainly on static graphs. However, most real-world networks are dynamic since their topology tends to change over time. Predicting the evolution of dynamic graphs is a task of high significance in the area of graph mining. Despite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature. In this paper, we propose a model that predicts the evolution of dynamic graphs. Specifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs. Then, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology. We evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets. Results demonstrate the effectiveness of the proposed model.","This paper proposes a model for predicting the evolution of dynamic graphs. The model is based on a graph neural network and a recurrent architecture. The authors propose a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology. They evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets."
894,SP:ff722957a1765c0568426ed88dd910a6b74054ef,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method for imputing missing features and estimating the distribution of target assignments given incomplete data. The proposed method is based on a generator network that generates imputations that a discriminator network is tasked to distinguish. Then, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values."
895,SP:c051b0fe779d9e4131016970b7ba469b596f3009,"Off-policy estimation for long-horizon problems is important in many real-life applications such as healthcare and robotics, where high-fidelity simulators may not be available and on-policy evaluation is expensive or impossible. Recently, Liu et al. (2018) proposed an approach that avoids the curse of horizon suffered by typical importance-sampling-based methods. While showing promising results, this approach is limited in practice as it requires data be drawn from the stationary distribution of a known behavior policy. In this work, we propose a novel approach that eliminates such limitations. In particular, we formulate the problem as solving for the fixed point of a certain operator. Using tools from Reproducing Kernel Hilbert Spaces (RKHSs), we develop a new estimator that computes importance ratios of stationary distributions, without knowledge of how the off-policy data are collected. We analyze its asymptotic consistency and finite-sample generalization. Experiments on benchmarks verify the effectiveness of our approach.","This paper proposes an off-policy estimation method for long-horizon optimization problems. The main idea is to use RKHSs to estimate the importance ratio of the stationary distribution of a known behavior policy. The paper is well-written and well-motivated. However, there are a few issues that need to be addressed. "
896,SP:065c900843011a71b70ed35357a2f71fe83872a7,"Mixture Model (MM) is a probabilistic framework which allows us to define a dataset containing K different modes. When each of the modes is associated with a Gaussian distribution, we refer it as Gaussian MM, or GMM. Given a data point x, GMM may assume the existence of a random index k ∈ {1,...,K} identifying which Gaussian the particular data is associated with. In a traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional likelihood p(x|k, θ), as well as responsibility probability p(k|x, θ) which describes the distribution index corresponds to the data. Computing the responsibility allows us to retrieve many important statistics of the overall dataset, including the weights of each of the modes. Modern large datasets often contain multiple unlabelled modes, such as paintings dataset containing several styles; fashion images containing several unlabelled categories. In its raw representation, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it’s feasible to compute responsibility distribution, making GMM unable to apply. To this paper, we utilize the Generative Adversarial Network (GAN) framework to achieve an alternative plausible method to compute these probabilities at the data’s latent space z instead of x. Instead of defining p(x|k, θ) explicitly, we devised a modified GAN to allow us to define the distribution using p(z|k, θ), where z is the corresponding latent representation of x, as well as p(k|x, θ) through an additional classification network which is trained with the GAN in an “end-toend” fashion. These techniques allow us to discover interesting properties of an unsupervised dataset, including dataset segments as well as generating new “outdistribution” data by smooth linear interpolation across any combinations of the modes in a completely unsupervised manner.","This paper proposes a method to compute the conditional likelihood of a Gaussian mixture model (GMM) using a generative adversarial network (GAN) framework. The authors propose to use the latent space z instead of x in the GMM model, which allows them to compute p(z|k,\theta) instead of p(x|x, \theta). The authors also propose an end-to-end GAN model that is trained with the GAN framework. Experiments are conducted on a synthetic dataset and an unsupervised dataset. "
897,SP:2da1608209058d214f8671062cc9eb0833ba4831,"We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples.","This paper proposes a method to train large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. The method is based on gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, the authors introduce a new residual block architecture and introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. The authors show that their method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy."
898,SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"Deep neural networks (DNNs) can be huge in size, requiring a considerable amount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels. We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN’s outputs using a nonparemetric scoring test and keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques."," is a probabilistic importance inference approach for pruning DNNs. Specifically, the authors test the significance of the relevance of a connection in a DNN to the DNN’s outputs using a nonparemetric scoring test and keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques."
899,SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"Many approaches to hierarchical reinforcement learning aim to identify sub-goal structure in tasks. We consider an alternative perspective based on identifying behavioral ‘motifs’—repeated action sequences that can be compressed to yield a compact code of action trajectories. We present a method for iteratively compressing action trajectories to learn nested behavioral hierarchies of arbitrary depth, with actions of arbitrary length. The learned temporally extended actions provide new action primitives that can participate in deeper hierarchies as the agent learns. We demonstrate the relevance of this approach for tasks with non-trivial hierarchical structure and show that the approach can be used to accelerate learning in recursively more complex tasks through transfer.","This paper presents a method for learning hierarchical reinforcement learning by iteratively compressing action trajectories to learn nested behavioral hierarchies of arbitrary depth, with actions of arbitrary length. The learned temporally extended actions provide new action primitives that can participate in deeper hierarchies as the agent learns. The authors demonstrate the relevance of this approach for tasks with non-trivial hierarchical structure and show that the approach can be used to accelerate learning in recursively more complex tasks through transfer."
900,SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the Hierarchical Bayes Autoencoder (HBAE). The HBAE contains a multimodal decoder in the form of an energybased model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.","This paper proposes a probabilistic autoencoder model for generating complex image sets. The model is based on the Hierarchical Bayes Autoencoders (HBAE) framework, where the decoder is modeled as an energy-based model (EBM) instead of the commonly adopted unimodal Gaussian distribution. The proposed model is trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. In both single image and set cases, the proposed model generates plausible variations consistent with the input data, and generates realistic unconditional samples."
901,SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"Off-policy temporal difference (TD) methods are a powerful class of reinforcement learning (RL) algorithms. Intriguingly, deep off-policy TD algorithms are not commonly used in combination with feature normalization techniques, despite positive effects of normalization in other domains. We show that naive application of existing normalization techniques is indeed not effective, but that well-designed normalization improves optimization stability and removes the necessity of target networks. In particular, we introduce a normalization based on a mixture of onand off-policy transitions, which we call cross-normalization. It can be regarded as an extension of batch normalization that re-centers data for two different distributions, as present in off-policy learning. Applied to DDPG and TD3, cross-normalization improves over the state of the art across a range of MuJoCo benchmark tasks.","This paper proposes a new normalization technique for off-policy temporal difference (TD) methods. The idea is to use a mixture of on-policy transitions and cross-policy transition, which is an extension of batch normalization. The authors show that the proposed cross-normalization improves the performance of TD3 and DDPG on a range of MuJoCo tasks."
902,SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"Presence of bias and confounding effects is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in the recent years. Such challenges range from spurious associations of confounding variables in medical studies to the bias of race in gender or face recognition systems. One solution is to enhance datasets and organize them such that they do not reflect biases, which is a cumbersome and intensive task. The alternative is to make use of available data and build models considering these biases. Traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables. However, these techniques are generally not suitable for end-to-end deep learning methods. In this paper, we propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder(s). This is enabled by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features. We apply our method to synthetic data, medical images, and a gender classification (Gender Shades Pilot Parliaments Benchmark) dataset. Our results show that the learned features by our method not only result in superior prediction performance but also are uncorrelated with the bias or confounder variables. The code is available at http://blinded_for_review/.","This paper proposes an adversarial training strategy to learn discriminative features that are unbiased and invariant to the confounder(s). The proposed method is based on the adversarial loss function that encourages a vanished correlation between the bias and learned features. The authors apply their method to synthetic data, medical images, and a gender classification (Gender Shades Pilot Parliaments Benchmark) dataset and show that the learned features by their method not only result in superior prediction performance but also are uncorrelated with the bias or confounders variables."
903,SP:783049ff463edd1283c058c6106a3e1f9a033df4,"Character-level language modeling based on Transformer has brought great success by alleviating limitation of recursive operation. However, existing Transformer-based models require substantial computational resources, which hinders the usability of character-level language models in applications with limited resources. In this paper, we propose a lightweight model, called GroupTransformer, that factorizes the calculation paths by grouped embedding operators. Additionally, Group-Transformer employs inter-group linear operators to prevent performance degradation from the group strategy. With comparison experiments on about five times larger and the best performing LSTM-based models and compatible parameter size of Transformer-based models, we show that GroupTransformer has better performance on two benchmark tasks, enwik8 and text8. Further experiments including ablation studies and qualitative analysis revealed that the proposed work contributes to the effective lightweight model for practical application. The implementation code will be available.",This paper proposes a lightweight Transformer-based model for character-level language modeling. The proposed method is based on the idea of grouped embedding operators. The authors propose to use inter-group linear operators to prevent performance degradation from the group strategy. The experimental results on enwik8 and text8 show that the proposed method can achieve better performance than existing Transformer models.
904,SP:946c26d371297c88d0ac246257104099b4585edc,"Probabilistic models with hierarchical-latent-variable structures provide state-ofthe-art results amongst non-autoregressive, unsupervised density-based models. However, the most common approach to training such models based on Variational Autoencoders often fails to leverage deep-latent hierarchies; successful approaches require complex inference and optimisation schemes. Optimal Transport is an alternative, non-likelihood-based framework for training generative models with appealing theoretical properties, in principle allowing easier training convergence between distributions. In this work we propose a novel approach to training models with deep-latent hierarchies based on Optimal Transport, without the need for highly bespoke models and inference networks. We show that our method enables the generative model to fully leverage its deep-latent hierarchy, and that in-so-doing, it is more effective than the original Wasserstein Autoencoder with Maximum Mean Discrepancy divergence.","This paper proposes Optimal Transport, a method for training generative models with hierarchical-latent-variable structures based on Variational Autoencoders (VAEs). The main idea is to train a generative model with deep-latency hierarchies based on a variational autoencoder (VAE) that is trained using Optimal transport. Theoretical analysis is provided to show that optimal transport is a non-likelihood-based framework that allows for easier training convergence between distributions. Experiments are conducted on MNIST, CIFAR-10, and Fashion MNIST. "
905,SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models often attempt to address these issues by combining sometimes complex, usually video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, we show that conceptually simple autoregressive video generation models based on a three-dimensional self-attention mechanism achieve competitive results across multiple metrics on popular benchmark datasets, for which they produce continuations of high fidelity and realism. We also present results from training our models on Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions and diverse human movement. While modeling these phenomena consistently remains elusive, we hope that our results, which include occasional realistic continuations encourage further research on comparatively complex, large scale datasets such as Kinetics.","This paper presents an autoregressive video generation model that uses a three-dimensional self-attention mechanism to generate high-quality continuations of videos. The model is based on the three-dimensionality of the input video, which is modeled as a 3D image. The authors evaluate the model on a variety of video datasets, and show that the proposed model can achieve state-of-the-art results. The paper also presents results on Kinetics, a large-scale action recognition dataset."
906,SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"The International Classification of Diseases (ICD) is a list of classification codes for the diagnoses. Automatic ICD coding is a multi-label text classification task with noisy clinical document inputs and extremely long-tailed label distribution, making it difficult to perform fine-grained classification on both frequent and zeroshot codes at the same time. In this paper, we propose a latent feature generation framework for generalized zero-shot ICD coding, where we aim to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. Our framework generates semantically meaningful features for zero-shot codes by exploiting ICD code hierarchical structure and a novel cycle architecture that reconstructs the relevant keywords. To the best of our knowledge, this is the first adversarial generative model for the generalized zero-shot learning on multi-label text classification. Extensive experiments demonstrate the effectiveness of our approach. On the public MIMIC-III dataset, our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot codes, and increase the AUC score by 3% (absolute improvement) from previous state of the art.","This paper proposes a latent feature generation framework for generalized zero-shot ICD coding, where they aim to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. The framework generates semantically meaningful features by exploiting ICD code hierarchical structure and a novel cycle architecture that reconstructs the relevant keywords. Extensive experiments demonstrate the effectiveness of the proposed method on the public MIMIC-III dataset."
907,SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,"In this paper we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment’s dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.","This paper proposes a self-supervised representation learning method to improve sample efficiency in reinforcement learning (RL). The authors propose a forward prediction objective for simultaneously learning embeddings of states and action sequences, which capture the structure of the environment’s dynamics, enabling efficient policy learning. The authors demonstrate that their action embedding alone improves the sample efficiency and peak performance of model-free RL on control from low-dimensional states. The proposed method achieves efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps."
908,SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"In order to efficiently learn with small amount of data on new tasks, meta-learning transfers knowledge learned from previous tasks to the new ones. However, a critical challenge in meta-learning is the task heterogeneity which cannot be well handled by traditional globally shared meta-learning methods. In addition, current task-specific meta-learning methods may either suffer from hand-crafted structure design or lack the capability to capture complex relations between tasks. In this paper, motivated by the way of knowledge organization in knowledge bases, we propose an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the meta-learner. As a result, the proposed framework not only addresses the challenge of task heterogeneity by a learned meta-knowledge graph, but also increases the model interpretability. We conduct extensive experiments on 2D toy regression and few-shot image classification and the results demonstrate the superiority of ARML over state-of-the-art baselines.",This paper proposes an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. ARML is motivated by the way of knowledge organization in knowledge bases and proposes a method to automatically extract the cross task relations and construct the meta knowledge graph. The proposed method is evaluated on 2D toy regression and few-shot image classification and shows the superiority of ARML over state-of-the-art baselines.
909,SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM’s hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.","This paper proposes a method for controlling attributes of the generated language (e.g. switching topic or sentiment) without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. The authors propose a simple alternative: the Plug and Play Language Model (PPLM) which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. The PPLM is flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper."
910,SP:12d0980bfea2de880905a0b87b40856969bb1c58,"While deep neural networks have been shown to perform remarkably well in many machine learning tasks, labeling a large amount of supervised data is usually very costly to scale. Therefore, learning robust representations with unlabeled data is critical in relieving human effort and vital for many downstream applications. Recent advances in unsupervised and self-supervised learning approaches for visual data have benefited greatly from domain knowledge. Here we are interested in a more generic unsupervised learning framework that can be easily generalized to other domains. In this paper, we propose to learn data representations with a novel type of denoising autoencoder, where the noisy input data is generated by corrupting clean data in the gradient domain. This can be naturally generalized to span multiple scales with a Laplacian pyramid representation of the input data. In this way, the agent learns more robust representations that exploits the underlying data structures across multiple scales. Experiments on several visual benchmarks demonstrate that better representations can be learned with the proposed approach, compared to its counterpart with single-scale corruption. Furthermore, we also demonstrate that the learned representations perform well when transferring to other vision tasks.","This paper proposes a novel unsupervised learning method for learning representations with unlabeled data. The proposed method is based on denoising autoencoders, where the noisy input data is generated by corrupting clean data in the gradient domain. The authors propose to learn representations with a Laplacian pyramid representation of the input data. Experiments on several visual benchmarks demonstrate that better representations can be learned with the proposed approach, compared to its counterpart with single-scale corruption."
911,SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"Neural networks are widely used in Natural Language Processing, yet despite their empirical successes, their behaviour is brittle: they are both over-sensitive to small input changes, and under-sensitive to deletions of large fractions of input text. This paper aims to tackle under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. We develop a novel technique for formal verification of this specification for models based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. Using this method we can efficiently prove, given a model, whether a particular sample is free from the under-sensitivity problem. We compare different training methods to address under-sensitivity, and compare metrics to measure it. In our experiments on the SNLI and MNLI datasets, we observe that IBP training leads to a significantly improved verified accuracy. On the SNLI test set, we can verify 18.4% of samples, a substantial improvement over only 2.8% using standard training.",This paper proposes a method for verifying the under-sensitivity of neural networks in the context of natural language inference. The main idea is to use interval bound propagation (IBP) to verify whether a particular sample is free from the under sensitivity problem. The authors propose to use the decomposable attention mechanism to train the model. The proposed method is evaluated on the SNLI and MNLI datasets and compared with standard training methods.
912,SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"In state of the art model-free off-policy deep reinforcement learning (RL), a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions. We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simple Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in – resulting in a QGRAPH. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in TD learning, our method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. QGRAPHs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm’s sensitivity to the replay memory capacity.","This paper proposes a method for off-policy deep reinforcement learning (RL) that uses a data graph to represent transitions in a Markov Decision Process (MDP) for which exact Q-values can be computed efficiently as more data comes in – resulting in a QGRAPH. The authors show that the Q-value for each transition in the simplified MDP is a lower bound of the lower bound for the same transition in a continuous Q-learning problem. By using these lower bounds in TD learning, the proposed method is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters."
913,SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,"Unsupervised domain adaptation aims to generalize the hypothesis trained in a source domain to an unlabeled target domain. One popular approach to this problem is to learn domain-invariant embeddings for both domains. In this work, we study, theoretically and empirically, the effect of the embedding complexity on generalization to the target domain. In particular, this complexity affects an upper bound on the target risk; this is reflected in experiments, too. Next, we specify our theoretical framework to multilayer neural networks. As a result, we develop a strategy that mitigates sensitivity to the embedding complexity, and empirically achieves performance on par with or better than the best layer-dependent complexity tradeoff.","This paper studies the problem of unsupervised domain adaptation, i.e., generalization from a source domain to an unlabeled target domain. In particular, the authors study the effect of the embedding complexity on the generalization to the target domain, and provide a theoretical analysis of this effect. The authors then propose a multilayer neural network architecture that mitigates the sensitivity of the complexity of the encoder and decoder to the domain-invariant embedding. Experiments are conducted on MNIST, CIFAR-10, and Fashion MNIST."
914,SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"Generalization error (also known as the out-of-sample error) measures how well the hypothesis learned from training data generalizes to previously unseen data. Proving tight generalization error bounds is a central question in statistical learning theory. In this paper, we obtain generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. We develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalization error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. Applying the Bayes-Stability method, we obtain new data-dependent generalization bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Our result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al. (2018). Our experiments demonstrate that our data-dependent bounds can distinguish randomly labelled data from normal data, which provides an explanation to the intriguing phenomena observed in Zhang et al. (2017a). We also study the setting where the total loss is the sum of a bounded loss and an additional `2 regularization term. We obtain new generalization bounds for the continuous Langevin dynamic in this setting by developing a new Log-Sobolev inequality for the parameter distribution at any time. Our new bounds are more desirable when the noise level of the process is not very small, and do not become vacuous even when T tends to infinity.","This paper studies generalization error bounds for learning general non-convex objectives. The authors develop a new framework, termed Bayes-Stability, which combines PAC-Bayesian theory and the notion of algorithmic stability. They obtain new data-dependent generalization bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., momentum, mini-batch and acceleration, Entropy-SGD). Their result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al (2018). They also study the setting where the total loss is the sum of a bounded loss and an additional `2 regularization term."
915,SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"The hippocampus has long been associated with spatial memory and goal-directed spatial navigation. However, the region’s independent role in continual learning of navigational strategies has seldom been investigated. Here we analyse populationlevel activity of hippocampal CA1 neurons in the context of continual learning of two different spatial navigation strategies. Demixed Principal Component Analysis (dPCA) is applied on neuronal recordings from 612 hippocampal CA1 neurons of rodents learning to perform allocentric and egocentric spatial tasks. The components uncovered using dPCA from the firing activity reveal that hippocampal neurons encode relevant task variables such decisions, navigational strategies and reward location. We compare this hippocampal features with standard reinforcement learning algorithms, highlighting similarities and differences. Finally, we demonstrate that a standard deep reinforcement learning model achieves similar average performance when compared to animal learning, but fails to mimic animals during task switching. Overall, our results gives insights into how the hippocampus solves reinforced spatial continual learning, and puts forward a framework to explicitly compare biological and machine learning during spatial continual learning.","This paper investigates the role of the hippocampus in continual learning in the context of continual learning of two different spatial navigation strategies. The authors analyze population-level activity of 612 hippocampal CA1 neurons of rodents learning to perform allocentric and egocentric spatial tasks. The results show that the components uncovered using dPCA from the firing activity reveal that hippocampal neurons encode relevant task variables such as decisions, navigational strategies and reward location. They compare this hippocampal features with standard reinforcement learning algorithms, highlighting similarities and differences. They demonstrate that a standard deep reinforcement learning model achieves similar average performance when compared to animal learning, but fails to mimic animals during task switching."
916,SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"Monte Carlo Tree Search (MCTS) has achieved impressive results on a range of discrete environments, such as Go, Mario and Arcade games, but it has not yet fulfilled its true potential in continuous domains. In this work, we introduce TPO, a tree search based policy optimization method for continuous environments. TPO takes a hybrid approach to policy optimization. Building the MCTS tree in a continuous action space and updating the policy gradient using off-policy MCTS trajectories are non-trivial. To overcome these challenges, we propose limiting tree search branching factor by drawing only a few action samples from the policy distribution and defining a new loss function based on the trajectories’ mean and standard deviations. Our approach led to some non-intuitive findings. MCTS training generally requires a large number of samples and simulations. However, we observed that bootstrapping tree search with a pre-trained policy allows us to achieve high quality results with a low MCTS branching factor and few simulations. Without the proposed policy bootstrapping, continuous MCTS would require a much larger branching factor and simulation count, rendering it prohibitively expensive. In our experiments, we use PPO as our baseline policy optimization algorithm. TPO significantly improves the policy on nearly all the environments. For example, in complex environments such as Humanoid with a 17 dimensional action space, we achieve a 2.5× improvement over the baseline algorithm.","This paper proposes a tree search based policy optimization method for continuous environments. The proposed method is based on bootstrapping tree search with a pre-trained policy, which allows to achieve high quality results with a low MCTS branching factor and few simulations. In the experiments, the proposed method significantly improves the policy on nearly all the environments and achieves a 2.5x improvement over the baseline algorithm."
917,SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain “good” tickets without supervision? We find that winning tickets found in these scenarios are, perhaps surprisingly, competitive with winning tickets generated on the full ImageNet dataset when evaluated on ImageNet classification task.","This paper studies the lottery ticket hypothesis, which claims that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. The paper investigates the properties of winning tickets, especially the importance of supervision in the generating process, and aims to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain “good” tickets without supervision? The paper finds that winning tickets found in these scenarios are, perhaps surprisingly, competitive with winning tickets generated on the full ImageNet dataset when evaluated on ImageNet classification task."
918,SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"Neural reading comprehension models have recently achieved impressive generalisation results, yet still perform poorly when given adversarially selected input. Most prior work has studied semantically invariant text perturbations which cause a model’s prediction to change when it should not. In this work we focus on the complementary problem: excessive prediction undersensitivity where input text is meaningfully changed, and the model’s prediction does not change when it should. We formulate a noisy adversarial attack which searches among semantic variations of comprehension questions for which a model still erroneously produces the same answer as the original question – and with an even higher probability. We show that – despite comprising unanswerable questions – SQuAD2.0 and NewsQA models are vulnerable to this attack and commit a substantial fraction of errors on adversarially generated questions. This indicates that current models—even where they can correctly predict the answer—rely on spurious surface patterns and are not necessarily aware of all information provided in a given comprehension question. Developing this further, we experiment with both data augmentation and adversarial training as defence strategies: both are able to substantially decrease a model’s vulnerability to undersensitivity attacks on held out evaluation data. Finally, we demonstrate that adversarially robust models generalise better in a biased data setting with a train/evaluation distribution mismatch; they are less prone to overly rely on predictive cues only present in the training set and outperform a conventional model in the biased data setting by up to 11% F1.","This paper studies the problem of adversarial attacks on neural reading comprehension models. The authors propose a noisy adversarial attack that searches among semantic variations of comprehension questions for which a model still erroneously produces the same answer as the original question – and with an even higher probability. They show that, despite comprising unanswerable questions, SQuAD2.0 and NewsQA models are vulnerable to this attack and commit a substantial fraction of errors on adversarially generated questions. This indicates that current models—even where they can correctly predict the answer —rely on spurious surface patterns and are not necessarily aware of all information provided in a given comprehension question. The paper further proposes data augmentation and adversarial training as defense strategies: both are able to substantially decrease a model’s vulnerability to undersensitivity attacks on held out evaluation data. Finally, the paper demonstrates that adversarically robust models generalise better in a biased data setting with a train/evaluation distribution mismatch; they are less prone to overly rely on predictive cues only present in the training set."
919,SP:5da870060778de460c1abe91562d6f3e707efef4,"With the recent proliferation of the usage of reinforcement learning (RL) agents for solving real-world tasks, safety emerges as a necessary ingredient for their successful application. In this paper, we focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives. In complex domains, these approaches are simply intractable, as they require knowing apriori all the possible unsafe scenarios an agent could encounter. We propose a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. We learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Our imaginative module can be seen as a “plug-and-play” approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space. Our method induces the agent to act safely while learning to solve the task. We experimentally validate our proposal on two gridworld environments and a self-driving car simulator, demonstrating that our approach to safety visits unsafe states significantly less frequently than a baseline.","This paper proposes a model-based approach to ensuring the safety of reinforcement learning agents. The proposed method is based on the idea that the agent can look into the future and be aware of the future consequences of its actions. To this end, the authors learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. Experiments on two gridworld environments and a self-driving car simulator demonstrate that the proposed approach to safety visits unsafe states significantly less frequently than a baseline."
920,SP:c2796f28fb067138303df8d424d646f4ada31558,"Sparsely available data points cause numerical error on finite differences which hinders us from modeling the dynamics of physical systems. The discretization error becomes even larger when the sparse data are irregularly distributed or defined on an unstructured grid, making it hard to build deep learning models to handle physics-governing observations on the unstructured grid. In this paper, we propose a novel architecture, Physics-aware Difference Graph Networks (PA-DGN), which exploits neighboring information to learn finite differences inspired by physics equations. PA-DGN leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. We demonstrate the superiority of PA-DGN in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations.","This paper proposes a novel architecture for learning finite differences in graph neural networks inspired by physics equations. Specifically, the authors propose a graph neural network architecture that leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. The authors demonstrate the superiority of PA-DGN in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations."
921,SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"In this paper, we consider the problem of training structured neural networks (NN) with nonsmooth regularization (e.g. `1-norm) and constraints (e.g. interval constraints). We formulate training as a constrained nonsmooth nonconvex optimization problem, and propose a convergent proximal-type stochastic gradient descent (ProxSGD) algorithm. We show that under properly selected learning rates, with probability 1, every limit point of the sequence generated by the proposed ProxSGD algorithm is a stationary point. Finally, to support the theoretical analysis and demonstrate the flexibility of ProxSGD, we show by extensive numerical tests how ProxSGD can be used to train either sparse or binary neural networks through an adequate selection of the regularization function and constraint set.","This paper studies the problem of training structured neural networks with nonsmooth regularization (e.g. `1-norm) and constraints. The authors propose a proximal-type stochastic gradient descent (ProxSGD) algorithm to solve this problem. They show that under proper learning rates, with probability 1, every limit point of the sequence generated by the proposed Prox SGD algorithm is a stationary point. Theoretical analysis and numerical experiments are provided to support the theoretical analysis and demonstrate the flexibility of the proposed algorithm."
922,SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"Standard compression algorithms work by mapping an image to discrete code using an encoder from which the original image can be reconstructed through a decoder. This process, due to the quantization step, is inherently non-differentiable so these algorithms must rely on approximate methods to train the encoder and decoder end-to-end. In this paper, we present an innovative framework for lossy image compression which is able to circumvent the quantization step by relying on a non-deterministic compression codec. The decoder maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e. it is bitsback efficient. The result is a principled, end-to-end differentiable compression framework that can be straight-forwardly trained using standard gradient-based optimizers. To showcase the efficiency of our method, we apply it to lossy image compression by training Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset and show that their rate-distortion curves on the Kodak dataset are competitive with the state-of-the-art on low bitrates.","This paper proposes a novel framework for lossy image compression that is end-to-end differentiable. The proposed method is based on a non-deterministic compression codec that maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e. it is bitsback efficient. The method is trained using standard gradient-based optimizers. Experiments on CLIC 2018 and Kodak show that the proposed method outperforms the state-of-the-art on low bitrates."
923,SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"Super Resolution (SR) is a fundamental and important low-level computer vision (CV) task. Different from traditional SR models, this study concentrates on a specific but realistic SR issue: How can we obtain satisfied SR results from compressed JPG (C-JPG) image, which widely exists on the Internet. In general, C-JPG can release storage space while keeping considerable quality in visual. However, further image processing operations, e.g., SR, will suffer from enlarging inner artificial details and result in unacceptable outputs. To address this problem, we propose a novel SR structure with two specifically designed components, as well as a cycle loss. In short, there are mainly three contributions to this paper. First, our research can generate high-qualified SR images for prevalent C-JPG images. Second, we propose a functional sub-model to recover information for C-JPG images, instead of the perspective of noise elimination in traditional SR approaches. Third, we further integrate cycle loss into SR solver to build a hybrid loss function for better SR generation. Experiments show that our approach achieves outstanding performance among state-of-the-art methods.","This paper proposes a method for super-resolution (SR) generation from compressed JPG images. The authors propose a novel SR structure with two specifically designed components, as well as a cycle loss. First, they propose a functional sub-model to recover information for C-JPG images, instead of the perspective of noise elimination in traditional SR approaches. Second, they further integrate cycle loss into SR solver to build a hybrid loss function for better SR generation. Experiments show that their approach achieves outstanding performance among state-of-the-art methods."
924,SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"We propose a fully convolutional network architecture that is able to estimate a full surface of pass probabilities from single-location labels derived from high frequency spatio-temporal data of professional soccer matches. The network is able to perform remarkably well from low-level inputs by learning a feature hierarchy that produces predictions at different sampling levels that are merged together to preserve both coarse and fine detail. Our approach presents an extreme case of weakly supervised learning where there is just a single pixel correspondence between ground-truth outcomes and the predicted probability map. By providing not just an accurate evaluation of observed events but also a visual interpretation of the results of other potential actions, our approach opens the door for spatiotemporal decision-making analysis, an as-yet little-explored area in sports. Our proposed deep learning architecture can be easily adapted to solve many other related problems in sports analytics; we demonstrate this by extending the network to learn to estimate pass-selection likelihood.",This paper presents a neural network architecture that is able to estimate a full surface of pass probabilities from single-location labels derived from high frequency spatio-temporal data of professional soccer matches. The network is trained with a low-level feature hierarchy that produces predictions at different sampling levels that are merged together to preserve both coarse and fine detail. The proposed deep learning architecture can be easily adapted to solve many other related problems in sports analytics such as pass-selection likelihood.
925,SP:1ae31baf383fc520687b255d9cac14c3b040e253,"We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user’s age or movie’s genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive – it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.","This paper proposes an inductive matrix completion model without using side information. It trains a graph neural network (GNN) based on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps them to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive – it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks."
926,SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"We consider the problem of unconstrained minimization of a smooth objective function in R in setting where only function evaluations are possible. We propose and analyze stochastic zeroth-order method with heavy ball momentum. In particular, we propose, SMTP, a momentum version of the stochastic three-point method (STP) Bergou et al. (2019). We show new complexity results for non-convex, convex and strongly convex functions. We test our method on a collection of learning to continuous control tasks on several MuJoCo Todorov et al. (2012) environments with varying difficulty and compare against STP, other state-of-the-art derivative-free optimization algorithms and against policy gradient methods. SMTP significantly outperforms STP and all other methods that we considered in our numerical experiments. Our second contribution is SMTP with importance sampling which we call SMTP_IS. We provide convergence analysis of this method for non-convex, convex and strongly convex objectives.","This paper considers the unconstrained minimization of a smooth objective function in R, where only function evaluations are possible. The authors propose and analyze stochastic zeroth-order method with heavy ball momentum. They show new complexity results for non-convex, convex and strongly convex functions. They test their method on a collection of learning to continuous control tasks on several MuJoCo Todorov et al. (2012) environments with varying difficulty."
927,SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"In multiagent systems (MASs), each agent makes individual decisions but all of them contribute globally to the system evolution. Learning in MASs is difficult since each agent’s selection of actions must take place in the presence of other co-learning agents. Moreover, the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. Previous works borrow various multiagent coordination mechanisms into deep learning architecture to facilitate multiagent coordination. However, none of them explicitly consider action semantics between agents that different actions have different influence on other agents. In this paper, we propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents. ASN characterizes different actions’ influence on other agents using neural networks based on the action semantics between them. ASN can be easily combined with existing deep reinforcement learning (DRL) algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show ASN significantly improves the performance of state-of-the-art DRL approaches compared with several network architectures.","This paper proposes Action Semantics Network (ASN), a neural network architecture for multi-agent reinforcement learning (MAS) systems. The main idea of the proposed network is to use the action semantics between agents to represent the influence of different actions on other agents. The network is trained using deep reinforcement learning. The proposed network can be easily combined with existing DRL algorithms to boost their performance. The experimental results on StarCraft II micromanagement and Neural MMO show that the proposed ASN significantly improves the performance of state-of-the-art DRL approaches compared with several network architectures."
928,SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the lowrank structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q functions in the context of control and deep RL tasks. As our key contribution, by leveraging Matrix Estimation (ME) techniques, we propose a general framework to exploit the underlying low-rank structure in Q functions. This leads to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on “low-rank” tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of our approach.","This paper proposes to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. Specifically, the authors investigate the lowrank structure, which widely exists for big data matrices, and verify empirically the existence of low-rank Q functions in the context of control and deep reinforcement learning tasks. By leveraging Matrix Estimation (ME) techniques, this paper proposes a general framework that exploits the underlying low rank structure in Q functions. This leads to a more efficient planning procedure for classical control, and a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on “low-rank” tasks."
929,SP:430336893b247b7bd45687d78b0d0511a7369e87,"The field of Deep Reinforcement Learning (DRL) has recently seen a surge in research in batch reinforcement learning, which aims for sample-efficient learning from a given data set without additional interactions with the environment. In the batch DRL setting, commonly employed off-policy DRL algorithms can perform poorly and sometimes even fail to learn altogether. In this paper we propose a new algorithm, Best-Action Imitation Learning (BAIL), which unlike many offpolicy DRL algorithms does not involve maximizing Q functions over the action space. Striving for simplicity as well as performance, BAIL first selects from the batch the actions it believes to be high-performing actions for their corresponding states; it then uses those state-action pairs to train a policy network using imitation learning. Although BAIL is simple, we demonstrate that BAIL achieves state of the art performance on the Mujoco benchmark.","This paper proposes a new algorithm for off-policy reinforcement learning in the context of batch reinforcement learning. The main idea is to use imitation learning to select high-performing actions for their corresponding states, and then use them to train a policy network using imitation learning. This is an interesting idea, and the paper is well-written and well-motivated. The experimental results on the Mujoco benchmark show that the proposed algorithm can achieve state-of-the-art performance."
930,SP:94078964876667e8a5d9ae7728d779d5b91a576e,"The objective in deep extreme multi-label learning is to jointly learn feature representations and classifiers to automatically tag data points with the most relevant subset of labels from an extremely large label set. Unfortunately, state-of-theart deep extreme classifiers are either not scalable or inaccurate for short text documents. This paper develops the DeepXML algorithm which addresses both limitations by introducing a novel architecture that splits training of head and tail labels. DeepXML increases accuracy by (a) learning word embeddings on head labels and transferring them through a novel residual connection to data impoverished tail labels; (b) increasing the amount of negative training data available by extending state-of-the-art negative sub-sampling techniques; and (c) re-ranking the set of predicted labels to eliminate the hardest negatives for the original classifier. All of these contributions are implemented efficiently by extending the highly scalable Slice algorithm for pretrained embeddings to learn the proposed DeepXML architecture. As a result, DeepXML could efficiently scale to problems involving millions of labels that were beyond the pale of state-of-the-art deep extreme classifiers as it could be more than 10x faster at training than XML-CNN and AttentionXML. At the same time, DeepXML was also empirically determined to be up to 19% more accurate than leading techniques for matching search engine queries to advertiser bid phrases. Source code for DeepXML can be downloaded from (Anonymous, 2019).",This paper proposes a novel architecture for deep extreme multi-label learning. The proposed DeepXML algorithm splits training of head and tail labels by learning word embeddings on head labels and transferring them through a novel residual connection to data impoverished tail labels. The authors also extend the state-of-the-art negative sub-sampling techniques to increase the amount of negative training data available by extending the Slice algorithm for pretrained embedding to learn the proposed deepXML architecture.
931,SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. To this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user’s preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.","This paper proposes an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user’s preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. The authors evaluate their approach against state-of-the-art baselines on 4 datasets."
932,SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"Generative adversarial networks (GANs) nowadays are capable of producing images of incredible realism. One concern raised is whether the state-of-the-art GAN’s learned distribution still suffers from mode collapse. Existing evaluation metrics for image synthesis focus on low-level perceptual quality. Diversity tests of samples from GANs are usually conducted qualitatively on a small scale. In this work, we devise a set of statistical tools, that are broadly applicable to quantitatively measuring the mode collapse of GANs. Strikingly, we consistently observe strong mode collapse on several state-of-the-art GANs using our toolset. We analyze possible causes, and for the first time present two simple yet effective “black-box” methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data.","This paper presents a method for quantitatively measuring the mode collapse of GANs. Specifically, the authors propose a set of statistical tools that are broadly applicable to quantitatively measure mode collapse. The authors analyze possible causes of mode collapse and propose two simple yet effective “black-box” methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data. Experiments are conducted on several state-of-the-art GAN models and show that the proposed method is effective in detecting mode collapse on several models."
933,SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"Recent theoretical work has established connections between over-parametrized neural networks and linearized models governed by the Neural Tangent Kernels (NTKs). NTK theory leads to concrete convergence and generalization results, yet the empirical performance of neural networks are observed to exceed their linearized models, suggesting insufficiency of this theory. Towards closing this gap, we investigate the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. We bring forward the idea of randomizing the neural networks, which allows them to escape their NTK and couple with quadratic models. We show that the optimization landscape of randomized two-layer networks are nice and amenable to escaping-saddle algorithms. We prove concrete generalization and expressivity results on these randomized networks, which lead to sample complexity bounds (of learning certain simple functions) that match the NTK and can in addition be better by a dimension factor when mild distributional assumptions are present. We demonstrate that our randomization technique can be generalized systematically beyond the quadratic case, by using it to find networks that are coupled with higher-order terms in their Taylor series.","This paper studies the problem of training over-parametrized neural networks that are beyond the NTK regime and are still governed by the Taylor expansion of the network. The authors propose to randomize the neural networks, which allows them to escape their NTK and couple with quadratic models. They show that the optimization landscape of randomized two-layer networks is nice and amenable to escaping-saddle algorithms. They also prove concrete generalization and expressivity results on these randomized networks which lead to sample complexity bounds (of learning certain simple functions) that match NTK."
934,SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"Graph Neural Networks (GNNs) have recently received tremendous attention due to their power in handling graph data for different downstream tasks across different application domains. Many GNN models have been proposed, which mainly differ in their graph filter design. However, most of these models believe there is a best filter for all the graph data. Instead, we attempt to provide in-depth analysis on (1) Whether there exists an optimal filter that performs the best on all graph data; (2) Which graph properties should be considered for finding the optimal graph filter; and (3) How to design appropriate filters that adapt to a given graph. In this paper, we focus on addressing the above three questions for the semi-supervised node classification task. We propose a novel assessment tool, called Graph Filter Discriminant Score (GFD), for evaluating the effectiveness of graph convolutional filters for a given graph in terms of node classification. Using the assessment tool, we find out that there is no single filter as a “silver bullet” that performs the best on all possible graphs, and graphs with different properties are in favor of different graph convolutional filters. Based on these findings, we develop Adaptive Filter Graph Neural Network (AFGNN), a simple but powerful model that can adaptively learn data-specific filters. For a given graph, AFGNN leverages graph filter assessment as an extra loss term and learns to combine a set of base filters. Experiments on both synthetic and real-world benchmark datasets have demonstrated that our proposed model has the flexibility in learning an appropriate filter and consistently provides state-of-the-art performance across all the datasets.","This paper presents a method for evaluating the effectiveness of different graph convolutional filters for semi-supervised node classification. The authors propose a novel assessment tool, Graph Filter Discriminant Score (GFD), which is based on the fact that there is no single filter as a “silver bullet” that performs the best on all possible graphs. Based on these findings, the authors propose Adaptive Filter Graph Neural Network (AFGNN), a simple but powerful model that can adaptively learn data-specific filters. AFGNN leverages graph filter assessment as an extra loss term and learns to combine a set of base filters. Experiments on both synthetic and real-world benchmark datasets have demonstrated that the proposed model has the flexibility in learning an appropriate filter."
935,SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization—a stronger-than-typical `2 penalty or early stopping—we achieve substantially higher worst-group accuracies, with 10–40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.","This paper studies the problem of distributionally robust optimization (DRO) for overparameterized neural networks. The authors propose a new DRO method for group DRO, where the objective is to minimize the worst-case training loss over a set of pre-defined groups. In particular, the authors argue that existing DRO methods can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst case training loss. Instead, the poor worst case performance arises from poor generalization on some groups. To address this issue, the paper proposes two regularization methods: (1) a stronger-than-typical `2 penalty or early stopping penalty, and (2) a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRo models. Experiments are conducted on a natural language inference task and two image tasks, and the authors show that the proposed method can achieve 10-40% improvement in the worst group accuracy."
936,SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"Existing local explanation methods provide an explanation for each decision of black-box classifiers, in the form of relevance scores of features according to their contributions. To obtain satisfying explainability, many methods introduce ad hoc constraints into the classification loss to regularize these relevance scores. In this paper, we discuss some shortcomings of these methods and address them by a simple but effective mask predictor. Specifically, we present the concept of distribution controllers and integrate it with a neural network to directly guide the distribution of relevance scores. Then we introduce the classification loss to optimize the proposed predictor. The benefit of this strategy is to enable discriminative scores over supporting features, and facilitate the setting of involved hyperparameters. The experimental results demonstrate that the proposed method also outperforms others in terms of faithfulness and explainability. The code is available at https://github.com/iclrlocal/.","This paper proposes a new local explanation method for black-box classifiers. The proposed method is based on the idea of distribution controllers and integrates it with a neural network to directly guide the distribution of relevance scores. Then, the classification loss is introduced to optimize the proposed predictor. The experimental results demonstrate that the proposed method also outperforms other methods in terms of faithfulness."
937,SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"We propose a deep network that can be trained to tackle image reconstruction and classification problems that involve detection of multiple object instances, without any supervision regarding their whereabouts. The network learns to extract the most significant K patches, and feeds these patches to a task-specific network – e.g., auto-encoder or classifier – to solve a domain specific problem. The challenge in training such a network is the non-differentiable top-K selection process. To address this issue, we lift the training optimization problem by treating the result of top-K selection as a slack variable, resulting in a simple, yet effective, multi-stage training. Our method is able to learn to detect recurring structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and in doing so it outperforms the state-of-the-art.","This paper proposes a method to train a network that can be used for image reconstruction and classification problems that involve detection of multiple object instances, without any supervision regarding their whereabouts. The network learns to extract the most significant K patches, and feeds these patches to a task-specific network – e.g., auto-encoder or classifier – to solve a domain specific problem. The method is able to learn to detect recurring structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and it outperforms the state-of-the-art."
938,SP:da1c5f6351d531482e90b86c3cceb52850c520de,"Neural inductive program synthesis is a task generating instructions that can produce desired outputs from given inputs. In this paper, we focus on the generation of a chunk of assembly code that can be executed to match a state change inside the CPU and RAM. We develop a neural program synthesis algorithm, AutoAssemblet, learned via self-learning reinforcement learning that explores the large code space efficiently. Policy networks and value networks are learned to reduce the breadth and depth of the Monte Carlo Tree Search, resulting in better synthesis performance. We also propose an effective multi-entropy policy sampling technique to alleviate online update correlations. We apply AutoAssemblet to basic programming tasks and show significant higher success rates compared to several competing baselines.","This paper proposes a neural program synthesis algorithm, AutoAssemblet, which learns to generate a chunk of assembly code that can be executed to match a state change inside the CPU and RAM. The algorithm is learned via self-learning reinforcement learning, where policy networks and value networks are learned to reduce the breadth and depth of the Monte Carlo Tree Search. The paper also proposes an effective multi-entropy policy sampling technique to alleviate online update correlations. Experiments are conducted on a variety of programming tasks and show significant higher success rates compared to several competing baselines."
939,SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or resource requirements). However, our theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, we study the impact of model architecture on the speed of training in the context of gradient descent optimization. We use the ideas from prior work that shows gradient descent can be modeled as a first-order ODE and use ODE’s coefficient matrix H to characterize the convergence rate. We introduce a simple analysis technique that enumerates H in terms of all possible “paths” in the network. We show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. We believe our analysis technique is useful in reasoning about more complex model architecture modifications.",This paper studies the impact of model architecture on the speed of training in the context of gradient descent optimization. The authors use the ideas from prior work that shows gradient descent can be modeled as a first-order ODE and use ODE’s coefficient matrix H to characterize the convergence rate. They introduce a simple analysis technique that enumerates H in terms of all possible “paths” in the network. They show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path. They also show that this analysis technique is useful in reasoning about more complex model architecture modifications.
940,SP:3e3bc8f617df742a395e7d315ec3810a42071294,"The recently developed link between strongly overparametrized neural networks (NNs) and kernel methods has opened a new way to understand puzzling features of NNs, such as their convergence and generalization behaviors. In this paper, we make the bias of initialization on strongly overparametrized NNs under gradient descent explicit. We prove that fully-connected wide ReLU-NNs trained with squared loss are essentially a sum of two parts: The first is the minimum complexity solution of an interpolating kernel method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences: (a) the second part becomes negligible in the regime of small initialization variance, which allows us to transfer generalization bounds from minimum complexity interpolating kernel methods to NNs; (b) in the opposite regime, the test error of wide NNs increases significantly with the initialization variance, while still interpolating the training data perfectly. Our work shows that – contrary to common belief – the initialization scheme has a strong effect on generalization performance, providing a novel criterion to identify good initialization strategies.","This paper studies the bias of initialization on strongly overparametrized neural networks (NNs) under gradient descent. The authors prove that fully-connected wide ReLU-NNs trained with squared loss are essentially a sum of two parts: the first is the minimum complexity solution of an interpolating kernel method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences: (a) the second part becomes negligible in the regime of small initialization variance, which allows to transfer generalization bounds from minimum complexity interpolating kernels methods to NNs; (b) in the opposite regime, test error of wide NNs increases significantly with the initialization variance."
941,SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects — currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depthpropagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection — outperforming the previous state-of-the-art detection accuracy for faraway objects by 40%. Our code is available at https://github.com/mileyan/Pseudo_Lidar_V2.",This paper proposes a new pseudo-LiDAR method for stereo 3D object detection. The main contribution of this paper is to improve the depth estimation of the pseudo-Lidar method. The authors propose a depthpropagation algorithm to diffuse the few exact measurements across the entire depth map. The proposed method is evaluated on the KITTI object detection benchmark and achieves state-of-the-art results.
942,SP:983d84502264633f3385d426c1d4601a0744ea9a,"The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we propose a principled adversarial example detection method that can withstand norm-constrained white-box attacks. Inspired by one-versus-the-rest classification, in a K class classification problem, we train K base detectors where the i-th detector is trained to discriminate natural data of class i from adversarial examples perturbed from other classes. At inference time, we first get the predicted label (say k) of the input, and then use the k-th detector to identify whether the input is a natural sample (of class k) or an adversarial example (perturbed from other classes). We further devise a generative approach to detecting/classifying adversarial examples by interpreting each base detector as an unnormalized density model of the classconditional data. We provide comprehensive evaluation of the above adversarial example detection/classification methods, and demonstrate their competitive performances and compelling properties. Code is available at https://github. com/xuwangyin/GAT-Generative-Adversarial-Training 1.","This paper proposes a novel adversarial example detection method that can withstand norm-constrained white-box attacks. Inspired by one-versus-the-rest classification, in a K class classification problem, they train K base detectors where the i-th detector is trained to discriminate natural data of class i from adversarial examples perturbed from other classes. They further devise a generative approach to detect/classify adversarial samples by interpreting each base detector as an unnormalized density model of the classconditional data. They provide comprehensive evaluation of the proposed methods and demonstrate their competitive performances and compelling properties."
943,SP:461e9308d050bc3dc7b35233452668bb31f5d491,"Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control.","This paper proposes a novel intrinsic reward for model-free reinforcement learning that encourages the agent to take actions that lead to significant changes in its learned state representation. The proposed intrinsic reward is based on the idea that the agent should explore states that it can control. The authors evaluate their method on multiple procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. The experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally generated MiniGrid environments. "
944,SP:c002c20b5e8696588e029c0f65e88860418826c4,"We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three.","This paper studies the embedding-based retrieval model for large-scale query-document retrieval problems. The authors propose a set of paragraph-level pre-training tasks for Transformer-based embedding models. They show that the key ingredient of learning a strong embedding based Transformer model is the set of pre-trained tasks. Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three. The results show that with these tasks, the Transformer models can outperform BM-25 as well as embedding model without Transformers."
945,SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"Graph neural networks have been adopted in numerous applications ranging from learning relational representations to modeling data on irregular domains such as point clouds, social graphs, and molecular structures. Though diverse in nature, graph neural network architectures remain limited by the graph convolution operator whose input and output graphs must have the same structure. Due to this restriction, representational hierarchy can only be built by graph convolution operations followed by non-parameterized pooling or expansion layers. This is very much like early convolutional network architectures, which later have been replaced by more effective parameterized strided and transpose convolution operations in combination with skip connections. In order to bring a similar change to graph convolutional networks, here we introduce the bipartite graph convolution operation, a parameterized transformation between different input and output graphs. Our framework is general enough to subsume conventional graph convolution and pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. By replacing the sequence of graph convolution and pooling in hierarchical architectures with a single parametric bipartite graph convolution, we (i) answer the question of whether graph pooling is necessary, and (ii) accelerate computations and lower memory requirements in hierarchical networks by eliminating pooling layers. Further, with concrete examples, we demonstrate that the general BiGraphNet formalism (iii) provides the modeling flexibility to build efficient architectures such as graph skip connections, and graph autoencoders.","This paper proposes to replace graph convolution and pooling layers in graph neural networks with a single parametric bipartite convolution operation, which is a parameterized transformation between different input and output graphs. The proposed method is general enough to subsume conventional graph convolutions and graph pooling as its special cases and supports multi-graph aggregation leading to a class of flexible and adaptable network architectures, termed BiGraphNet. Experiments are conducted on graph skip connections, graph autoencoders, and graph aggregation networks."
946,SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"Few-shot classification aims to recognize novel categories with only few labeled images in each class. Existing metric-based few-shot classification algorithms predict categories by comparing the feature embeddings of query images with those from a few labeled images (support examples) using a learned metric function. While promising performance has been demonstrated, these methods often fail to generalize to unseen domains due to large discrepancy of the feature distribution across domains. In this work, we address the problem of few-shot classification under domain shifts for metric-based methods. Our core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. To capture variations of the feature distributions under different domains, we further apply a learning-to-learn approach to search for the hyper-parameters of the feature-wise transformation layers. We conduct extensive experiments and ablation studies under the domain generalization setting using five few-shot classification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. Experimental results demonstrate that the proposed feature-wise transformation layer is applicable to various metric-based models, and provides consistent improvements on the few-shot classification performance under domain shift.","This paper proposes a novel method for few-shot classification for metric-based methods. The main idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. The authors further apply a learning-to-learn approach to search for the hyper-parameters of the feature-based transformation layers. The proposed method is evaluated on 5 datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. The experimental results show that the proposed method can generalize to unseen domains."
947,SP:df46627cb984a56bba36d510bfc52e00751e9107,"We present an approach to Lagrangian fluid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of N-D convolutions to the continuous domain. We show that our network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, we demonstrate that our continuous convolutions outperform prior formulations in terms of accuracy and speed.","This paper proposes a convolutional neural network architecture for fluid simulation. The authors propose to use spatial convolutions as the main differentiable operation that relates particles to their neighbors. This is a simple extension of N-D convolutions to the continuous domain. The proposed network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, the authors demonstrate that their continuous convolutions outperform prior formulations in terms of accuracy and speed."
948,SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"Ensembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. However, an ensemble’s cost for both training and testing increases linearly with the number of networks, which quickly becomes untenable. In this paper, we propose BatchEnsemble1, an ensemble method whose computational and memory costs are significantly lower than typical ensembles. BatchEnsemble achieves this by defining each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. Unlike ensembles, BatchEnsemble is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. Across CIFAR-10, CIFAR-100, WMT14 EN-DE/EN-FR translation, and out-of-distribution tasks, BatchEnsemble yields competitive accuracy and uncertainties as typical ensembles; the speedup at test time is 3X and memory reduction is 3X at an ensemble of size 4. We also apply BatchEnsemble to lifelong learning, where on Split-CIFAR-100, BatchEnsemble yields comparable performance to progressive neural networks while having a much lower computational and memory costs. We further show that BatchEnsemble can easily scale up to lifelong learning on Split-ImageNet which involves 100 sequential learning tasks.","This paper proposes BatchEnsemble, an ensemble method for training neural networks. The method is based on the Hadamard product of a shared weight matrix among all ensemble members and a rank-one matrix per member. It is parallelizable across devices, where one device trains one member, and parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. The proposed method is evaluated on CIFAR-10, Cifar-100, WMT14 EN-DE/EN-FR translation, and out-of-distribution tasks. The speedup at test time is 3x and memory reduction is 3X at an ensemble of size 4. The authors also apply Batchensemble to lifelong learning, where on Split-CifAR-100 it yields comparable performance to progressive neural networks while having a much lower computational and memory costs."
949,SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"We introduce a novel neural network-based partial differential equations solver for forward and inverse problems. The solver is grid free, mesh free and shape free, and the solution is approximated by a neural network. We employ an unsupervised approach such that the input to the network is a points set in an arbitrary domain, and the output is the set of the corresponding function values. The network is trained to minimize deviations of the learned function from the strong PDE solution and satisfy the boundary conditions. The resulting solution in turn is an explicit smooth differentiable function with a known analytical form. Unlike other numerical methods such as finite differences and finite elements, the derivatives of the desired function can be analytically calculated to any order. This framework therefore, enables the solution of high order non-linear PDEs. The proposed algorithm is a unified formulation of both forward and inverse problems where the optimized loss function consists of few elements: fidelity terms of L2 and L∞ norms that unlike previous methods promote a strong solution. Robust boundary conditions constraints and additional regularizers are included as well. This setting is flexible in the sense that regularizers can be tailored to specific problems. We demonstrate our method on several free shape 2D second order systems with application to Electrical Impedance Tomography (EIT), diffusion and wave equations.",This paper proposes a neural network-based method for solving PDEs. The network is trained to minimize deviations of the learned function from the strong PDE solution and satisfy the boundary conditions. The resulting solution is an explicit smooth differentiable function with a known analytical form. The proposed algorithm is a unified formulation of both forward and inverse problems where the optimized loss function consists of few elements: fidelity terms of L2 and L∞ norms that unlike previous methods promote a strong solution.
950,SP:973d0ad0faadcf7298300f2758de9154205e7113,"Analyzing the behavior of neural networks is one of the most pressing challenges in deep learning. Binarized Neural Networks are an important class of networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT solvers. Such tools can be used to answer existential and probabilistic queries about the network, perform explanation generation, etc. However, the main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this work, we analyze architectural design choices of BNNs and discuss how they affect the performance of logic-based reasoners. We propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solvers without sacrificing accuracy on the primary task. Our experimental results demonstrate that our approach scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets.","This paper proposes a method to improve the performance of logic-based SAT solvers by reducing the size of Binarized Neural Networks (BNNs). The authors argue that the main bottleneck of existing methods is their ability to reason about large BNNs efficiently. The authors propose changes to the BNN architecture and the training procedure to get a simpler network without sacrificing accuracy on the primary task. The experimental results demonstrate that their approach scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets."
951,SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp’s depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation.","This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNMP can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs."
952,SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of density estimation tasks.","This paper proposes a localised generative flow (LGF) method for density estimation. The authors argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose LGFs to address this problem. The proposed LGFs are composed of stacked continuous mixtures of bijection, which enables each bijection to learn a local region of the target rather than its entirety. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but the authors propose a simple variational scheme that performs well in practice. The experiments show that LGFs yield improved performance across a variety of density estimation tasks."
953,SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"Vision-and-Language Navigation (VLN) requires an agent to follow naturallanguage instructions, explore the given environments, and reach the desired target locations. These step-by-step navigational instructions are extremely useful in navigating new environments that the agent does not know about previously. Most recent works that study VLN observe a significant performance drop when tested on unseen environments (i.e., environments not used in training), indicating that the neural agent models are highly biased towards training environments. Although this issue is considered as one of the major challenges in VLN research, it is still under-studied and needs a clearer explanation. In this work, we design novel diagnosis experiments via environment re-splitting and feature replacement, looking into possible reasons for this environment bias. We observe that neither the language nor the underlying navigational graph, but the low-level visual appearance conveyed by ResNet features directly affects the agent model and contributes to this environment bias in results. According to this observation, we explore several kinds of semantic representations which contain less low-level visual information, hence the agent learned with these features could be better generalized to unseen testing environments. Without modifying the baseline agent model and its training method, our explored semantic features significantly decrease the performance gap between seen and unseen on multiple datasets (i.e., 8.6% to 0.2% on R2R, 23.9% to 0.1% on R4R, and 3.74 to 0.17 on CVDN) and achieve competitive unseen results to previous state-of-the-art models.","This paper studies the problem of vision-and-language navigation (VLN) in unseen environments. The authors propose two methods to address the issue of performance drop in unseen VLN testing: environment re-splitting and feature replacement. The first method is based on the observation that the low-level visual appearance of ResNet features directly affects the agent model and contributes to the environment bias in results. Based on this observation, the authors explore several kinds of semantic representations which contain less low level visual information, hence the agent learned with these features could be better generalized to unseen testing environments. Experiments show that the explored semantic features significantly decrease the performance gap between seen and unseen on multiple datasets."
954,SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"We consider the following central question in the field of Deep Reinforcement Learning (DRL): How can we use implicit human feedback to accelerate and optimize the training of a DRL algorithm? State-of-the-art methods rely on any human feedback to be provided explicitly, requiring the active participation of humans (e.g., expert labeling, demonstrations, etc.). In this work, we investigate an alternative paradigm, where non-expert humans are silently observing (and assessing) the agent interacting with the environment. The human’s intrinsic reactions to the agent’s behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent’s learning in the RL tasks. We develop a system to obtain and accurately decode the implicit human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. As a baseline contribution, we demonstrate the feasibility of capturing error-potentials of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Building atop the baseline, we then make the following novel contributions in our work: (i) We argue that the definition of error-potentials is generalizable across different environments; specifically we show that error-potentials of an observer can be learned for a specific game, and the definition used as-is for another game without requiring re-learning of the error-potentials. (ii) We propose two different frameworks to combine recent advances in DRL into the error-potential based feedback system in a sample-efficient manner, allowing humans to provide implicit feedback while training in the loop, or prior to the training of the RL agent. (iii) Finally, we scale the implicit human feedback (via ErrP) based RL to reasonably complex environments (games) and demonstrate the significance of our approach through synthetic and real user experiments.",This paper proposes to use human feedback to accelerate and optimize the training of a deep reinforcement learning algorithm. The authors propose to use an electroencephalogram (EEG) cap to monitor the human EEG signals and use them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. The EEG signals are used to decode the implicit human feedback (specifically error-related event potentials) for state-action pairs in an Atari-type environment. The paper also proposes two different frameworks to combine recent advances in DRL into the error-potential based feedback system in a sample-efficient manner.
955,SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"We propose laconic classification as a novel way to understand and compare the performance of diverse image classifiers. The goal in this setting is to minimise the amount of information (aka. entropy) required in individual test images to maintain correct classification. Given a classifier and a test image, we compute an approximate minimal-entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The notion of entropy offers a unifying metric that allows to combine and compare the effects of various types of reductions (e.g., crop, colour reduction, resolution reduction) on classification performance, in turn generalising similar methods explored in previous works. Proposing two complementary frameworks for computing the minimal-entropy positive images of both human and machine classifiers, in experiments over the ILSVRC test-set, we find that machine classifiers are more sensitive entropy-wise to reduced resolution (versus cropping or reduced colour for machines, as well as reduced resolution for humans), supporting recent results suggesting a texture bias in the ILSVRC-trained models used. We also find, in the evaluated setting, that humans classify the minimal-entropy positive images of machine models with higher precision than machines classify those of humans.","This paper proposes a method for laconic classification, where the goal is to minimize the amount of information (aka. entropy) required in individual test images to maintain correct classification. Given a classifier and a test image, the authors compute an approximate minimal-entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The notion of entropy offers a unifying metric that allows to combine and compare the effects of various types of reductions (e.g., crop, colour reduction, resolution reduction) on classification performance, in turn generalizing similar methods explored in previous works. The authors propose two complementary frameworks for computing the minimal entropy positive images of both human and machine classifiers, in experiments over the ILSVRC test-set, they find that machine classifier are more sensitive entropy-wise to reduced resolution (versus cropping or reduced colour for machines, as well as reduced resolution for humans), supporting recent results suggesting a texture bias in the I LSVRC-trained models used. They also find, in the evaluated setting, that humans classify the minimal-ENTropy positive images with higher precision than machines classify those of humans."
956,SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,"Many defenses for Convolutional Neural Networks are based on a simple observation that the adversarial examples are not robust and small perturbations to the attacking input often recover the desired prediction. Intuitively, the adversarial examples occupy a very small cone in the decision space which is surrounded by a large area corresponding to the correct class. While the intuition is simple, a detailed understanding of this phenomenon is missing from the research literature. We identify a family of defense techniques that are based on the instability assumption. The defenses include deterministic lossy compression algorithms and randomized perturbations to the input that all lead to similar gains in robustness. We present a comprehensive experimental analysis of when and why perturbation defenses work and potential mechanisms that could explain their effectiveness (or ineffectiveness) in different settings.","This paper studies adversarial defense methods for convolutional neural networks. In particular, the authors focus on perturbation methods that are based on the instability assumption. The authors identify a family of defense techniques based on deterministic lossy compression algorithms and randomized perturbations to the input that all lead to similar gains in robustness. They provide a comprehensive analysis of when and why these defenses work and potential mechanisms that could explain their effectiveness (or ineffectiveness) in different settings."
957,SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our retinas. This paper explores the role of view prediction in the development of 3D visual recognition. We propose neural 3D mapping networks, which take as input 2.5D (color and depth) video streams captured by a moving camera, and lift them to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data. We show that the proposed model learns visual representations useful for (1) semi-supervised learning of 3D object detectors, and (2) unsupervised learning of 3D moving object detectors, by estimating the motion of the inferred 3D feature maps in videos of dynamic scenes. To the best of our knowledge, this is the first work that empirically shows view prediction to be a scalable self-supervised task beneficial to 3D object detection.","This paper proposes a neural 3D mapping network that takes as input 2.5D (color and depth) video streams captured by a moving camera, and lift them to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its feature maps to novel viewpoints, to predict and match against target views. The authors propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data. The proposed model learns visual representations useful for (1) semi-supervised learning of 3D object detectors, and (2) unsupervised learn for 3D moving object detectors."
958,SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"Unsupervised Domain Translation (UDT) consists in finding meaningful correspondences between two domains, without access to explicit pairings between them. Following the seminal work of CycleGAN, many variants and extensions of this model have been applied successfully to a wide range of applications. However, these methods remain poorly understood, and lack convincing theoretical guarantees. In this work, we define UDT in a rigorous, non-ambiguous manner, explore the implicit biases present in the approach and demonstrate the limits of theses approaches. Specifically, we show that mappings produced by these methods are biased towards low energy transformations, leading us to cast UDT into an Optimal Transport (OT) framework by making this implicit bias explicit. This not only allows us to provide theoretical guarantees for existing methods, but also to solve UDT problems where previous methods fail. Finally, making the link between the dynamic formulation of OT and CycleGAN, we propose a simple approach to solve UDT, and illustrate its properties in two distinct settings. Given pairs of elements from two different domains, domain translation consists in learning a mapping from one domain to another, linking these paired elements together. If we consider a photograph of a given scene, and an artistic painting of the same scene, we may want to learn the map associating a photograph to paintings describing the same scene, and conversely, for paintings to photographs. A wide range of problems can be formulated as translation, including image-toimage (Isola et al. (2016)), or video-to-video (Wang et al. (2018)), image captioning (Zhang et al. (2016)), natural language translation (Bahdanau et al. (2015)), etc... However, obtaining paired examples can be difficult thus motivating the unpaired setting where only samples from both domains are available which allows us to tackle a wider range of problems. A seminal work in this direction has been the CycleGAN model proposed in Zhu et al. (2017a), which has led to extensions for many applications and has given impressive results. The starting point of this work is to understand and study this successful approach as there remains little theoretical understanding of why these models work. Galanti et al. (2018); Yang et al. (2018) have observed that the approach first introduced in Zhu et al. (2017a) is ill-posed: in most cases, any pairing between samples of both domains – many of which are unwanted pairings – satisfies their objective function. This is in contradiction with the empirical results of the model","This paper studies the problem of unsupervised domain translation (UDT) from a theoretical point of view. The authors propose to cast UDT into an Optimal Transport (OT) framework by making the implicit bias explicit, which allows them to provide theoretical guarantees for existing methods, and to solve UDT problems where previous methods fail. They also propose a simple approach to solve the UDT problem, and illustrate its properties in two distinct settings. "
959,SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"In this paper, we propose a novel regularization method, RotationOut, for neural networks. Different from Dropout that handles each neuron/channel independently, RotationOut regards its input layer as an entire vector and introduces regularization by randomly rotating the vector. RotationOut can also be used in convolutional layers and recurrent layers with small modifications. We further use a noise analysis method to interpret the difference between RotationOut and Dropout in co-adaptation reduction. Using this method, we also show how to use RotationOut/Dropout together with Batch Normalization. Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed method. Codes are available at https://github.com/ RotationOut/RotationOut.","This paper proposes a novel regularization method, RotationOut, for neural networks. The idea is to treat the input layer as an entire vector and introduce regularization by randomly rotating the vector. The proposed method can also be used in convolutional layers and recurrent layers with small modifications. Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed method."
960,SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"We introduce a method to create Universal Adversarial Perturbations (UAP) for a given CNN in a data-free manner. Data-free approaches suite scenarios where the original training data is unavailable for crafting adversaries. We show that the adversary generation with full training data can be approximated to a formulation without data. This is realized through a sequential optimization of the adversarial perturbation with the proposed dilate loss. Dilate loss basically maximizes the Euclidean norm of the output before nonlinearity at any layer. By doing so, the perturbation constrains the ReLU activation function at every layer to act roughly linear for data points and thus eliminate the dependency on data for crafting UAPs. Extensive experiments demonstrate that our method not only has theoretical support, but achieves higher fooling rate than the existing data-free work. Furthermore, we evidence improvement in limited data cases as well.","This paper proposes a method to create adversarial perturbations (UAPs) for a given CNN in a data-free manner. The authors propose a dilate loss that maximizes the Euclidean norm of the output before nonlinearity at any layer. By doing so, the perturbation constrains the ReLU activation function at every layer to act roughly linear for data points and thus eliminate the dependency on data for crafting UAPs. Extensive experiments demonstrate that the proposed method not only has theoretical support, but achieves higher fooling rate than the existing data free work."
961,SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"Recently, Neural Architecture Search (NAS) has been successfully applied to multiple artificial intelligence areas and shows better performance compared with hand-designed networks. However, the existing NAS methods only target a specific task. Most of them usually do well in searching an architecture for single task but are troublesome for multiple datasets or multiple tasks. Generally, the architecture for a new task is either searched from scratch, which is neither efficient nor flexible enough for practical application scenarios, or borrowed from the ones searched on other tasks, which might be not optimal. In order to tackle the transferability of NAS and conduct fast adaptation of neural architectures, we propose a novel Transferable Neural Architecture Search method based on meta-learning in this paper, which is termed as T-NAS. T-NAS learns a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the specific task. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost, which demonstrates the effectiveness of our method.","This paper proposes a transferable neural architecture search method based on meta-learning. The main idea is to learn a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the specific task. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost."
962,SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning, adversarial defense and learning with label noise.","This paper proposes a simple and effective stochastic neural network architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, the proposed SE-SNN is simpler to implement and faster to train, and produces state-of-the-art results on network compression by pruning, adversarial defense and learning with label noise."
963,SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent’s life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. We formulate the problem of generating curious behavior as one of meta-learning: an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent’s reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta-RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, we instead propose to meta-learn algorithms: pieces of code similar to those designed by humans in ML papers. Our rich language of programs combines neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. We demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human-designed published curiosity algorithms in domains as disparate as grid navigation with image inputs, acrobot, lunar lander, ant and hopper.","This paper proposes a meta-learning algorithm for meta-reinforcement learning in the context of curiosity. The motivation is that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent’s life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. The paper proposes to meta-learn algorithms that combine neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. Experiments are conducted on grid navigation with image inputs, acrobot, lunar lander, ant and hopper."
964,SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"We address the problem of Any-Code-to-Code Generation (AnyC2C) – generating code given its surrounding code without any restriction on the vocabulary or structure. The state-of-the-art in this problem is the sequence-to-sequence (seq2seq) approach, which treats code as a sequence and does not leverage any structural information. We introduce a new approach to AnyC2C that leverages the strict syntax of programming languages to model a code snippet as a tree – structural language modeling (SLM). SLM estimates the probability of the program’s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. We present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Unlike previous structural techniques that have severely restricted the kinds of expressions that can be generated in this task, our approach can generate arbitrary expressions in any programming language. Our model significantly outperforms both seq2seq and a variety of existing structured approaches in generating Java and C# code. We make our code, datasets, and models available online.","This paper proposes a method for generating code given its surrounding code without any restriction on the vocabulary or structure. The proposed method is based on a neural model that models a code snippet as a tree and uses conditional probabilities to estimate the probability of the program’s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. Unlike previous structural techniques that have severely restricted the kinds of expressions that can be generated in this task, this approach can generate arbitrary expressions in any programming language. The model significantly outperforms both seq2seq and a variety of existing structured approaches in generating Java and C# code."
965,SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank. If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are randomly initialized, the gradient decent algorithms converge to a global minimum of zero loss in probability.","This paper studies the problem of learning large-scale neural networks (NN) in non-convex optimization. The authors prove that the objective functions in learning NNs are convex in the canonical model space. They further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, they prove that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank."
966,SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"Large-scale ground truth data sets are of crucial importance for deep learning based segmentation models, but annotating per-pixel masks is prohibitively time consuming. In this paper, we investigate interactive graph-based segmentation algorithms that enforce connectivity. To be more precise, we introduce an instanceaware heuristic of a discrete Potts model, and a class-aware Integer Linear Programming (ILP) formulation that ensures global optimum. Both algorithms can take RGB, or utilize the feature maps from any DCNN, whether trained on the target dataset or not, as input. We present competitive semantic (and panoptic) segmentation results on the PASCAL VOC 2012 and Cityscapes dataset given initial scribbles. We also demonstrate that our interactive approach can reach 90.6% mIoU on VOC validation set with an overhead of just 3 correction scribbles. They are thus suitable for interactive annotation on new or existing datasets, or can be used inside any weakly supervised learning framework on new datasets.","This paper proposes an interactive graph-based segmentation algorithm that uses a discrete Potts model and a class-aware integer linear programming (ILP) formulation that ensures global optimum. The proposed algorithm can take RGB, or utilize the feature maps from any DCNN, whether trained on the target dataset or not, as input. The authors present competitive semantic (and panoptic) segmentation results on the PASCAL VOC 2012 and Cityscapes dataset given initial scribbles. They also demonstrate that their interactive approach can reach 90.6% mIoU on VOC validation set with an overhead of just 3 correction scribbles, and can be used inside any weakly supervised learning framework on new datasets."
967,SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"Adversarial perturbations cause a shift in the salient features of an image, which often results in misclassification. Previous work has suggested that these salient features could be used as a defense, arguing that with saliency tools we could successfully detect adversarial examples. While the idea itself is appealing, we show that prior work which used gradient-based saliency tools is ineffective as an adversarial defense – it fails to beat a simple baseline which uses the same model but with the saliency map removed. To remedy this, we demonstrate that learnt saliency models can capture the shifts in saliency due to adversarial perturbations, while also having a low computational cost. This allows saliency models to be used effectively as a real-time defense. Further, using the learnt saliency model, we propose a novel defense: a CNN that distinguishes between adversarial images and natural images using salient pixels as its input. On MNIST, CIFAR-10, and ASSIRA, our defense improves on using the saliency map alone, and can detect various adversarial attacks. Lastly, we show that even when trained on weak defenses, we can detect adversarial images generated by strong attacks such as C&W and DeepFool.","This paper proposes to use learned saliency models as a real-time adversarial defense against adversarial attacks. The proposed method is based on the idea that the salient features of an image can be used to detect adversarial perturbations. The authors propose a CNN that distinguishes between adversarial images and natural images using salient pixels as its input. The method is evaluated on MNIST, CIFAR-10, and ASSIRA, and can detect various adversarial attack types."
968,SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"We investigate global adversarial robustness guarantees for machine learning models. Specifically, given a trained model we consider the problem of computing the probability that its prediction at any point sampled from the (unknown) input distribution is susceptible to adversarial attacks. Assuming continuity of the model, we prove measurability for a selection of local robustness properties used in the literature. We then show how concentration inequalities can be employed to compute global robustness with estimation error upper-bounded by, for any > 0 selected a priori. We utilise the methods to provide statistically sound analysis of the robustness/accuracy trade-off for a variety of neural networks architectures and training methods on MNIST, Fashion-MNIST and CIFAR. We empirically observe that robustness and accuracy tend to be negatively correlated for networks trained via stochastic gradient descent and with iterative pruning techniques, while a positive trend is observed between them in Bayesian settings.","This paper studies the problem of adversarial robustness for machine learning models. Specifically, given a trained model, the authors consider the probability that its prediction at any point sampled from the (unknown) input distribution is susceptible to adversarial attacks. The authors prove that concentration inequalities can be employed to compute global robustness with estimation error upper-bounded by, for any > 0 selected a priori. They then provide statistically sound analysis of the robustness/accuracy trade-off for a variety of neural networks architectures and training methods on MNIST, Fashion-MNIST and CIFAR."
969,SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"Robust Reinforcement Learning aims to find the optimal policy with some extent of robustness to environmental dynamics. Existing learning algorithms usually enable the robustness though disturbing the current state or simulated environmental parameters in a heuristic way, which lack quantified robustness to the system dynamics (i.e. transition probability). To overcome this issue, we leverage Wasserstein distance to measure the disturbance to the reference transition kernel. With Wasserstein distance, we are able to connect transition kernel disturbance to the state disturbance, i.e. reduce an infinite-dimensional optimization problem to a finite-dimensional risk-aware problem. Through the derived risk-aware optimal Bellman equation, we show the existence of optimal robust policies, provide a sensitivity analysis for the perturbations, and then design a novel robust learning algorithm—Wasserstein Robust Advantage Actor-Critic algorithm (WRAAC). The effectiveness of the proposed algorithm is verified in the Cart-Pole environment.","This paper studies the problem of robust reinforcement learning, where the goal is to find the optimal policy with some extent of robustness to environmental dynamics. The authors propose to use the Wasserstein distance to measure the disturbance to the reference transition kernel, and derive risk-aware optimal Bellman equation to reduce the infinite-dimensional optimization problem to a finite-dimensional risk aware problem. They also provide a sensitivity analysis for the perturbations, and design a novel robust learning algorithm called Wassersteins Robust Advantage Actor-Critic algorithm (WRAAC). The effectiveness of the proposed algorithm is verified in the Cart-Pole environment."
970,SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"Nash equilibrium has long been a desired solution concept in multi-player games, especially for those on continuous strategy spaces, which have attracted a rapidly growing amount of interests due to advances in research applications such as the generative adversarial networks. Despite the fact that several deep learning based approaches are designed to obtain pure strategy Nash equilibrium, it is rather luxurious to assume the existence of such an equilibrium. In this paper, we present a new method to approximate mixed strategy Nash equilibria in multi-player continuous games, which always exist and include the pure ones as a special case. We remedy the pure strategy weakness by adopting the pushforward measure technique to represent a mixed strategy in continuous spaces. That allows us to generalize the Gradient-based Nikaido-Isoda (GNI) function to measure the distance between the players’ joint strategy profile and a Nash equilibrium. Applying the gradient descent algorithm, our approach is shown to converge to a stationary Nash equilibrium under the convexity assumption on payoff functions, the same popular setting as in previous studies. In numerical experiments, our method consistently and significantly outperforms recent works on approximating Nash equilibrium for quadratic games, general blotto games, and GAMUT games.","This paper proposes a method to approximate mixed strategy Nash equilibria in multi-player continuous games, which always exist and include the pure ones as a special case. The proposed method is based on the pushforward measure technique to represent a mixed strategy in continuous spaces. This allows the authors to generalize the Gradient-based Nikaido-Isoda (GNI) function to measure the distance between the players’ joint strategy profile and a Nash equilibrium. In numerical experiments, the proposed method consistently and significantly outperforms recent works on approximating Nash equilibrium for quadratic games, general blotto games, and GAMUT games."
971,SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework1 to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.","This paper proposes a neural execution tree (NExT) framework to augment training data for text classification using natural language explanations (NL explanations). The idea is to transform NL explanations into executable logical forms by semantic parsing, and then generalize different types of actions specified by the logical forms for labeling data instances. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate the superiority of NExT over baseline methods. The extension to multi-hop question answering achieves performance gain with light annotation effort."
972,SP:a9b5f7257dedd719cfe341fca275776734af1d98,"Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to misclassification under perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.","This paper proposes a method for verifying the robustness of recurrent neural networks (RNNs) trained with verified training. The method is based on the idea of verifying the consistency of the trained RNNs with a set of specifications. The authors extend the verification procedure from simple adversarial robustness to more complex specifications that capture temporal properties, such as requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. The main contribution of the paper is the extension of the verified training procedure to RNN architectures and specifications."
973,SP:3903680e07b676409e3cf6a1044b67291fe38630,"Producing agents that can generalize to a wide range of visually different environments is a significant challenge in reinforcement learning. One method for overcoming this issue is visual domain randomization, whereby at the start of each training episode some visual aspects of the environment are randomized so that the agent is exposed to many possible variations. However, domain randomization is highly inefficient and may lead to policies with high variance across domains. Instead, we formalize the visual domain randomization problem, and show that minimizing the policy’s Lipschitz constant with respect to the randomization parameters leads to low variance in the learned policies. We propose a regularization method where the agent is only trained on one variation of the environment, and its learned state representations are regularized during training to minimize this constant. We conduct experiments that demonstrate that our technique leads to more efficient and robust learning than standard domain randomization, while achieving equal generalization scores.","This paper proposes a regularization method for visual domain randomization, which aims to reduce the variance in the learned policies. The authors propose to regularize the state representations of the learned policy by minimizing the Lipschitz constant of the policy’s state representation with respect to the randomization parameters. The proposed regularization is motivated by the fact that the policy is only trained on one variation of the environment, and its learned state representations are regularized during training to minimize this constant. The experimental results show that the proposed method leads to more efficient and robust learning than standard domain randomisation while achieving equal generalization scores."
974,SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"Deep metric learning (DML) has received much attention in deep learning due to its wide applications in computer vision. Previous studies have focused on designing complicated losses and hard example mining methods, which are mostly heuristic and lack of theoretical understanding. In this paper, we cast DML as a simple pairwise binary classification problem that classifies a pair of examples as similar or dissimilar. It identifies the most critical issue in this problem— imbalanced data pairs. To tackle this issue, we propose a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the uncertainty decision set of the dual variable allows us to recover state-of-the-art complicated losses and also to induce novel variants. Empirical studies on several benchmark data sets demonstrate that our simple and effective method outperforms the state-of-the-art results.","This paper studies the problem of imbalanced data pairs in deep metric learning (DML). The authors propose a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the uncertainty decision set of the dual variable allows the authors to recover state-of-the-art complicated losses and also to induce novel variants. Empirical studies on several benchmark data sets demonstrate that the proposed method outperforms the state of the art results."
975,SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"We target the problem of finding a local minimum in non-convex finite-sum minimization. Towards this goal, we first prove that the trust region method with inexact gradient and Hessian estimation can achieve a convergence rate of order O(1/k) as long as those differential estimations are sufficiently accurate. Combining such result with a novel Hessian estimator, we propose a sample-efficient stochastic trust region (STR) algorithm which finds an (, √ )-approximate local minimum within Õ( √ n/ ) stochastic Hessian oracle queries. This improves the state-of-the-art result by a factor ofO(n). Finally, we also develop Hessian-free STR algorithms which achieve the lowest runtime complexity. Experiments verify theoretical conclusions and the efficiency of the proposed algorithms.","This paper studies the problem of finding a local minimum in non-convex finite-sum minimization. The authors first prove that the trust region method with inexact gradient and Hessian estimation can achieve a convergence rate of order O(1/k) as long as those differential estimations are sufficiently accurate. Combining such result with a novel Hessian estimator, the authors propose a sample-efficient stochastic trust region (STR) algorithm which finds an approximate local minimum within $\sqrt{n/\sqrt{\frac{n}{\epsilon})$. The authors also develop Hessian-free STR algorithms which achieve the lowest runtime complexity. Experiments verify theoretical conclusions and the efficiency of the proposed algorithms."
976,SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"Successfully training deep neural networks often requires either batch normalization, appropriate weight initialization, both of which come with their own challenges. We propose an alternative, geometrically motivated method for training. Using elementary results from linear programming, we introduce Farkas layers: a method that ensures at least one neuron is active at a given layer. Focusing on residual networks with ReLU activation, we empirically demonstrate a significant improvement in training capacity in the absence of batch normalization or methods of initialization across a broad range of network sizes on benchmark datasets.","This paper proposes a method for training deep neural networks without batch normalization or weight initialization. The proposed method is based on the idea of Farkas layers, which is a geometrically motivated method that ensures at least one neuron is active at a given layer. The method is evaluated on a variety of networks with ReLU activations, and it is shown that the proposed method outperforms the existing methods in terms of training capacity."
977,SP:1d325b148e3efe407241c1f1cbe8d17400499741,"A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For any perturbation of the input with a magnitude smaller than the certificate value, the classification output will provably remain unchanged. Computing exact robustness certificates for deep classifiers is difficult in general since it requires solving a nonconvex optimization. In this paper, we provide computationally-efficient robustness certificates for deep classifiers with differentiable activation functions in two steps. First, we show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded, we can compute a robustness certificate in the l2 norm efficiently using convex optimization. Second, we derive a computationallyefficient differentiable upper bound on the curvature of a deep network. We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness against adversarial examples. Putting these results together leads to our proposed Curvature-based Robustness Certificate (CRC) and Curvature-based Robust Training (CRT). Our numerical results show that CRC outperforms CROWN’s certificate when trained with our regularizer while CRT leads to higher certified accuracy compared to standard adversarial training.","This paper studies the problem of computing exact robustness certificates for deep classifiers with differentiable activation functions in two steps. First, they show that if the eigenvalues of the Hessian of the network (curvatures of the networks) are bounded, they can compute a robustness certificate in the l2 norm efficiently using convex optimization. Second, they derive a computationally efficient differentiable upper bound on the curvature of a deep network and use it as a regularization term during the training of a network to boost its certified accuracy against adversarial examples. The proposed method, called Curvature-based Robustness Certificate (CRC), is a combination of CROWN and CRT."
978,SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"We propose a novel method for compressed sensing recovery using untrained deep generative models. Our method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. We show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods. Unlike various learned approaches based on generative models, our method does not require pre-training over large datasets. We further introduce a novel learned regularization technique, which incorporates prior information on the network weights. This reduces reconstruction error, especially for noisy measurements. Finally we prove that, using the DIP optimization approach, moderately overparameterized single-layer networks trained can perfectly fit any signal despite the nonconvex nature of the fitting problem. This theoretical result provides justification for early stopping.","This paper proposes a method for compressed sensing recovery using untrained deep generative models. The method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. The authors show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods, and does not require pre-training over large datasets. They further introduce a novel learned regularization technique, which incorporates prior information on the network weights. This reduces reconstruction error, especially for noisy measurements."
979,SP:23c0f621e6041003b59bf0532130760694cf6a4a,"Applying reinforcement learning (RL) to real-world problems will require reasoning about action-reward correlation over long time horizons. Hierarchical reinforcement learning (HRL) methods handle this by dividing the task into hierarchies, often with hand-tuned network structure or pre-defined subgoals. We propose a novel HRL framework TAIC, which learns the temporal abstraction from past experience or expert demonstrations without task-specific knowledge. We formulate the temporal abstraction problem as learning latent representations of action sequences and present a novel approach of regularizing the latent space by adding information-theoretic constraints. Specifically, we maximize the mutual information between the latent variables and the state changes. A visualization of the latent space demonstrates that our algorithm learns an effective abstraction of the long action sequences. The learned abstraction allows us to learn new tasks on higher level more efficiently. We convey a significant speedup in convergence over benchmark learning problems. These results demonstrate that learning temporal abstractions is an effective technique in increasing the convergence rate and sample efficiency of RL algorithms.","This paper proposes TAIC, a hierarchical reinforcement learning algorithm that learns temporal abstractions for long-term reinforcement learning problems. TAIC learns the temporal abstraction from past experience or expert demonstrations without task-specific knowledge. The authors propose to regularize the latent space by adding information-theoretic constraints to maximize the mutual information between the latent variables and the state changes. The learned abstraction allows the algorithm to learn new tasks on higher level more efficiently. The experimental results show that TAIC can improve the convergence rate and sample efficiency."
980,SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training, it still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.","This paper proposes a novel layer-wise sampling strategy for graph convolutional networks (GCN). The proposed sampling strategy is based on the factors of the bi-directional diffusion between layers. The authors also apply the self-attention mechanism to learn suitable weights for the sampled nodes, which allows the model to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model."
981,SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"When humans observe a physical system, they can easily locate objects, understand their interactions, and anticipate future behavior. For computers, however, learning such models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. STOVE predicts videos with convincing physical behavior over thousands of timesteps, outperforms previous unsupervised models, and even approaches the performance of supervised baselines. We further demonstrate the strength of our model as a simulator for sample efficient model-based control in a task with heavily interacting objects.","This paper presents a state-space model for videos that explicitly reasons about objects and their positions, velocities, and interactions. The model is constructed by combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. The authors also demonstrate the strength of the model as a simulator for sample efficient model-based control in a task with heavily interacting objects. The proposed model outperforms previous unsupervised models and approaches the performance of supervised baselines."
982,SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"We propose a new form of an autoencoding model which incorporates the best properties of variational autoencoders (VAE) and generative adversarial networks (GAN). It is known that GAN can produce very realistic samples while VAE does not suffer from mode collapsing problem. Our model optimizes λ-Jeffreys divergence between the model distribution and the true data distribution. We show that it takes the best properties of VAE and GAN objectives. It consists of two parts. One of these parts can be optimized by using the standard adversarial training, and the second one is the very objective of the VAE model. However, the straightforward way of substituting the VAE loss does not work well if we use an explicit likelihood such as Gaussian or Laplace which have limited flexibility in high dimensions and are unnatural for modelling images in the space of pixels. To tackle this problem we propose a novel approach to train the VAE model with an implicit likelihood by an adversarially trained discriminator. In an extensive set of experiments on CIFAR-10 and TinyImagent datasets, we show that our model achieves the state-of-the-art trade-off between generation and reconstruction quality and demonstrate how we can balance between mode-seeking and mass-covering behaviour of our model by adjusting the weight λ in our objective.",This paper proposes a new autoencoding model based on variational autoencoders (VAE) and generative adversarial networks (GAN). The main idea of the paper is to replace the VAE loss with an implicit likelihood by an adversarially trained discriminator. The proposed model is evaluated on CIFAR-10 and TinyImagent datasets. The experimental results show that the proposed model achieves the state-of-the-art trade-off between generation and reconstruction quality.
983,SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"Adversarial attacks on CNN classifiers can make an imperceptible change to an input image and alter the classification result. The source of these failures is still poorly understood, and many explanations invoke the “unreasonably linear extrapolation” used by CNNs (Goodfellow et al., 2018) along with the geometry of high dimensions. In this paper we show that similar attacks can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to such attacks. We present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. We introduce new datasets of realistic images of faces and digits where the Bayes-Optimal classifier can be calculated efficiently and show that for some of these datasets the optimal classifier is robust and for others it is vulnerable to adversarial examples. In systematic experiments with many such datasets, we find that standard CNN training consistently finds a vulnerable classifier even when the optimal classifier is robust while large-margin methods often find a robust classifier with the exact same training data. Our results suggest that adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may be the result of the specific distributions of commonly used datasets or of suboptimal training methods used in current practice.","This paper studies the problem of adversarial attacks on CNN classifiers. The authors show that adversarial examples can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to such attacks. They show that under certain conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. They also introduce new datasets of realistic images of faces and digits where the Bayesian classifier can be calculated efficiently. The experimental results show that standard CNN training consistently finds a vulnerable classifier, while large-margin methods often find a robust classifier with the exact same training data."
984,SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"Neural network pruning techniques have demonstrated it is possible to remove the majority of weights in a network with surprisingly little degradation to top1 test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by pruning. We find that certain examples, which we term pruning identified exemplars (PIEs), and classes are systematically more impacted by the introduction of sparsity. Removing PIE images from the test-set greatly improves top-1 accuracy for both sparse and non-sparse models. These hard-to-generalize-to images tend to be mislabelled, of lower image quality, entail abstract representations, atypical examples or require fine-grained classification.","This paper studies the effect of pruning on the top-1 accuracy of a neural network. The authors study the impact of different pruning methods on different classes and images. They find that certain examples, which they term pruning identified exemplars (PIEs), and classes are systematically more impacted by the introduction of sparsity. Removing PIE images from the test-set greatly improves the performance for both sparse and non-sparse models."
985,SP:4b17edaa7ec6201891433320d85f9a415656b763,"Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present KG-A2C1, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. We contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a wide variety of IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size.","This paper proposes a novel method for interactive fiction games, where the agent interacts with the world purely through natural language. The method is based on the idea that the knowledge graph can be used to reason about the game state and to constrain the natural language generation of natural language actions. The proposed method is evaluated on a variety of text-based games, and it is shown to outperform the state-of-the-art."
986,SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"For typical sequence prediction problems such as language generation, maximum likelihood estimation (MLE) has commonly been adopted as it encourages the predicted sequence most consistent with the ground-truth sequence to have the highest probability of occurring. However, MLE focuses on once-to-all matching between the predicted sequence and gold-standard, consequently treating all incorrect predictions as being equally incorrect. We refer to this drawback as negative diversity ignorance in this paper. Treating all incorrect predictions as equal unfairly downplays the nuance of these sequences’ detailed token-wise structure. To counteract this, we augment the MLE loss by introducing an extra Kullback– Leibler divergence term derived by comparing a data-dependent Gaussian prior and the detailed training prediction. The proposed data-dependent Gaussian prior objective (D2GPo) is defined over a prior topological order of tokens and is poles apart from the data-independent Gaussian prior (L2 regularization) commonly adopted in smoothing the training of MLE. Experimental results show that the proposed method makes effective use of a more detailed prior in the data and has improved performance in typical language generation tasks, including supervised and unsupervised machine translation, text summarization, storytelling, and image captioning.","This paper proposes a new method for maximum likelihood estimation (MLE) for language generation. The main idea is to use a data-dependent Gaussian prior (D2GPo) instead of the data-independent L2 regularization (L2-regularization) which is commonly used in smoothing the training of MLE. The proposed method is based on the Kullback-Leibler divergence term, which is derived by comparing the prior and the detailed training prediction. Experimental results show that the proposed method makes effective use of a more detailed prior in the data and has improved performance in typical language generation tasks."
987,SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"Miscalibration – a mismatch between a model’s confidence and its correctness – of Deep Neural Networks (DNNs) makes their predictions hard for downstream components to trust. Ideally, we want networks to be accurate, calibrated and confident. Temperature scaling, the most popular calibration approach, will calibrate a DNN without affecting its accuracy, but it will also make its correct predictions underconfident. In this paper, we show that replacing the widely used cross-entropy loss with focal loss allows us to learn models that are already very well calibrated. When combined with temperature scaling, focal loss, whilst preserving accuracy and yielding state-of-the-art calibrated models, also preserves the confidence of the model’s correct predictions, which is extremely desirable for downstream tasks. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to theoretically justify the empirically excellent performance of focal loss. We perform extensive experiments on a variety of computer vision (CIFAR-10/100) and NLP (SST, 20 Newsgroup) datasets, and with a wide variety of different network architectures, and show that our approach achieves state-of-the-art accuracy and calibration in almost all cases.","This paper proposes to replace the widely used cross-entropy loss with focal loss to improve the calibration of deep neural networks. The authors argue that focal loss preserves the confidence of the model’s correct predictions, which is extremely desirable for downstream tasks. They provide a thorough analysis of the factors causing miscalibration, and use the insights to theoretically justify the empirically excellent performance of focal loss. They perform extensive experiments on a variety of computer vision (CIFAR-10/100) and NLP (SST, 20 Newsgroup) datasets, and show that their approach achieves state-of-the-art accuracy and calibration in almost all cases."
988,SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,"We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the `∞-Lipschitz constant, our approach yields superior estimates, compared to baselines available in the literature.","This paper proposes LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. The authors show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks, and the experiments on networks with random weights and networks trained on MNIST show that the proposed approach yields superior estimates compared to baselines available in the literature."
989,SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. Our method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). We also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition), and show that such cross-modal training (when possible) helps even more.","This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation). The method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). The authors also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition), and show that such cross-modal training helps even more."
990,SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"Data have often to be moved between servers and clients during the inference phase. This is the case, for instance, when large amounts of data are stored on a public storage server without the possibility for the users to directly execute code and, hence, apply machine learning models. Depending on the available bandwidth, this data transfer can become a major bottleneck. We propose a simple yet effective framework that allows to select certain parts of the input data needed for the subsequent application of a given neural network. Both the associated selection masks as well as the neural network are trained simultaneously such that a good model performance is achieved while, at the same time, only a minimal amount of data is selected. During the inference phase, only the parts selected by the masks have to be transferred between the server and the client. Our experiments indicate that it is often possible to significantly reduce the amount of data needed to be transferred without affecting the model performance much.","This paper proposes a method to reduce the amount of data that needs to be transferred between servers and clients during the inference phase of a neural network. The authors propose a simple yet effective framework that allows to select certain parts of the input data needed for the subsequent application of a given neural network and train both the selection masks and the neural network simultaneously such that a good model performance is achieved while, at the same time, only a minimal amount of the data is selected. The experiments indicate that it is often possible to significantly reduce the number of data needed to transfer without affecting the model performance much."
991,SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"Deep neural networks have achieved great success in classification tasks during the last years. However, one major problem to the path towards artificial intelligence is the inability of neural networks to accurately detect samples from novel class distributions and therefore, most of the existent classification algorithms assume that all classes are known prior to the training stage. In this work, we propose a methodology for training a neural network that allows it to efficiently detect outof-distribution (OOD) examples without compromising much of its classification accuracy on the test examples from known classes. Based on the Outlier Exposure (OE) technique, we propose a novel loss function that achieves state-of-the-art results in out-of-distribution detection with OE both on image and text classification tasks. Additionally, we experimentally show that the combination of our method with the Mahalanobis distance-based classifier achieves state-of-the-art results in the OOD detection task.","This paper proposes a method for detecting out-of-distribution (OOD) samples from novel class distributions. The method is based on the Outlier Exposure (OE) technique, and the authors propose a novel loss function for OOD detection. The authors also propose a Mahalanobis distance-based classifier for the detection of OOD samples. The proposed method is evaluated on image classification and text classification tasks."
992,SP:89bc528ef801182365ac279e8963803afccb391d,"In this paper, we propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior performance of E2Efold: it predicts significantly better structures compared to previous SOTA (especially for pseudoknotted structures), while being as efficient as the fastest algorithms in terms of inference time."," for RNA secondary structure prediction. This paper proposes an end-to-end deep learning model, called E2Efold, which can effectively take into account the inherent constraints in the problem. The key idea is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. The experiments on benchmark datasets demonstrate the superior performance of E2efold."
993,SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"We consider a setting where biases are involved when agents internalise an environment. Agents have different biases, all of which resulting in imperfect evidence collected for taking optimal actions. Throughout the interactions, each agent asynchronously internalises their own predictive model of the environment and forms a virtual simulation within which the agent plays trials of the episodes in entirety. In this research, we focus on developing a collective policy trained solely inside agents’ simulations, which can then be transferred to the real-world environment. The key idea is to let agents imagine together; make them take turns to host virtual episodes within which all agents participate and interact with their own biased representations. Since agents’ biases vary, the collective policy developed while sequentially visiting the internal simulations complement one another’s shortcomings. In our experiment, the collective policies consistently achieve significantly higher returns than the best individually trained policies.","This paper proposes a method to train a collective policy that can be transferred to the real-world environment. The idea is to train an ensemble of agents, each of which has its own biased representation of the environment, and each agent takes turns to play a series of episodes in which they interact with their own biased representations. The goal is to learn a policy that maximizes the performance of the ensemble. The authors propose to train the collective policy sequentially, sequentially visiting the internal simulations of the other agents, and train the policy on top of the learned ensemble policy. Experiments show that the proposed method outperforms the best individually trained policies."
994,SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"Generic responses are a known issue for open-domain dialog generation. Most current approaches model this one-to-many task as a one-to-one task, hence being unable to integrate information from multiple semantically similar valid responses of a prompt. We propose a novel dialog generation model that learns a semantic latent space, on which representations of semantically related sentences are close to each other. This latent space is learned by maximizing correlation between the features extracted from prompt and responses. Learning the pair relationship between the prompts and responses as a regression task on the latent space, instead of classification on the vocabulary using MLE loss, enables our model to view semantically related responses collectively. An additional autoencoder is trained, for recovering the full sentence from the latent space. Experimental results show that our proposed model eliminates the generic response problem, while achieving comparable or better coherence compared to baselines.","This paper proposes a dialog generation model that learns a semantic latent space, on which representations of semantically related sentences are close to each other. This latent space is learned by maximizing correlation between the features extracted from prompt and responses. An additional autoencoder is trained, for recovering the full sentence from the latent space. Experimental results show that the proposed model eliminates the generic response problem, while achieving comparable or better coherence compared to baselines."
995,SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"Explaining the prediction of deep models has gained increasing attention to increase its applicability, even spreading it to life-affecting decisions. However there has been no attempt to pinpoint only the most discriminative features contributing specifically to separating different classes in a fine-grained classification task. This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light and shadow in nature. GLAS provides a useful coarseto-fine control benefiting from scalability of Gaussian mask. We also devised the ability to identify multiple instances through recursive GLAS. We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset. To show the general applicability, we also illustrate that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224×224 image) via the ImageNet Large Scale Visual Recognition Challenge.",This paper proposes Gaussian light and shadow (GLAS) to explain the spatial impact of deep models by the feature perturbation inspired by light and shadows in nature. GLAS provides a useful coarseto-fine control benefiting from scalability of Gaussian mask. The authors also devised the ability to identify multiple instances through recursive GLAS. The experimental results show that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224×224 image) via the ImageNet large scale visual recognition challenge.
996,SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"Convolution is a central operation in Convolutional Neural Networks (CNNs), which applies a kernel to overlapping regions shifted across the image. However, because of the strong correlations in real-world image data, convolutional kernels are in effect re-learning redundant data. In this work, we show that this redundancy has made neural network training challenging, and propose network deconvolution, a procedure which optimally removes pixel-wise and channel-wise correlations before the data is fed into each layer. Network deconvolution can be efficiently calculated at a fraction of the computational cost of a convolution layer. We also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Filtering with such kernels results in a sparse representation, a desired property that has been missing in the training of neural networks. Learning from the sparse representation promotes faster convergence and superior results without the use of batch normalization. We apply our network deconvolution operation to 10 modern neural network models by replacing batch normalization within each. Extensive experiments show that the network deconvolution operation is able to deliver performance improvement in all cases on the CIFAR-10, CIFAR-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets."," of removing pixel-wise and channel-wise correlations before the data is fed into each layer of a neural network. The paper proposes a method called network deconvolution to remove pixel and channel correlations in the first layer of the network. It is shown that the proposed method can be efficiently calculated at a fraction of the computational cost of a convolution layer. Experiments are conducted on CIFAR-10, Cifar-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets to demonstrate the effectiveness of the method."
997,SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"The intensive computation and memory requirements of generative adversarial neural networks (GANs) hinder its real-world deployment on edge devices such as smartphones. Despite the success in model reduction of convolutional neural networks (CNNs), neural network quantization methods have not yet been studied on GANs, which are mainly faced with the issues of both the effectiveness of quantization algorithms and the instability of training GAN models. In this paper, we start with an extensive study on applying existing successful CNN quantization methods to quantize GAN models to extreme low bits. Our observation reveals that none of them generates samples with reasonable quality because of the underrepresentation of quantized weights in models, and the generator and discriminator networks show different sensitivities upon the quantization precision. Motivated by these observations, we develop a novel quantization method for GANs based on EM algorithms, named as QGAN. We also propose a multi-precision algorithm to help find an appropriate quantization precision of GANs given image qualities requirements. Experiments on CIFAR-10 and CelebA show that QGAN can quantize weights in GANs to even 1-bit or 2-bit representations with results of quality comparable to original models.","This paper proposes a novel quantization method for GANs based on EM algorithms, named as QGAN. The authors propose a multi-precision algorithm to help find an appropriate quantization precision of GAN models given image qualities requirements. Experiments on CIFAR-10 and CelebA show that QGAN can quantize weights in GAN to even 1-bit or 2-bit representations with results of quality comparable to original models."
998,SP:58c4905f59f04a50b30d27c99521126a6455d38a,"While classic work in convex-concave min-max optimization relies on averageiterate convergence results, the emergence of nonconvex applications such as training Generative Adversarial Networks has led to renewed interest in last-iterate convergence guarantees. Proving last-iterate convergence is challenging because many natural algorithms, such as Simultaneous Gradient Descent/Ascent, provably diverge or cycle even in simple convex-concave min-max settings, and previous work on global last-iterate convergence rates has been limited to the bilinear and convex-strongly concave settings. In this work, we show that the HAMILTONIAN GRADIENT DESCENT (HGD) algorithm achieves linear convergence in a variety of more general settings, including convex-concave problems that satisfy a novel “sufficiently bilinear” condition. We also prove convergence rates for stochastic HGD and for some parameter settings of the Consensus Optimization algorithm of Mescheder et al. (2017).","This paper studies the last-iterate convergence of Hamiltonian gradient descent (HGD) algorithm for convex-concave min-max optimization problems. The main contribution of this paper is to prove that HGD converges to a linear solution in a variety of general settings, including a novel “sufficiently bilinear” condition. The authors also prove convergence rates for stochastic HGD and for some parameter settings of the Consensus Optimization algorithm of Mescheder et al. (2017)."
999,SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"ResNet structure has achieved great success since its debut. In this paper, we study the stability of learning ResNet. Specifically, we consider the ResNet block hl = φ(hl−1 + τ · g(hl−1)) where φ(·) is ReLU activation and τ is a scalar. We show that for standard initialization used in practice, τ = 1/Ω( √ L) is a sharp value in characterizing the stability of forward/backward process of ResNet, where L is the number of residual blocks. Specifically, stability is guaranteed for τ ≤ 1/Ω( √ L) while conversely forward process explodes when τ > L− 1 2 +c for a positive constant c. Moreover, if ResNet is properly over-parameterized, we show for τ ≤ 1/Ω̃( √ L) gradient descent is guaranteed to find the global minima 1, which significantly enlarges the range of τ ≤ 1/Ω̃(L) that admits global convergence in previous work. We also demonstrate that the over-parameterization requirement of ResNet only weakly depends on the depth, which corroborates the advantage of ResNet over vanilla feedforward network. Empirically, with τ ≤ 1/ √ L, deep ResNet can be easily trained even without normalization layer. Moreover, adding τ = 1/ √ L can also improve the performance of ResNet with normalization layer.","This paper studies the stability of learning ResNet block hl = φ(hl−1 + tau · g(hl-1) where hl is a ReLU activation and tau is a scalar. The authors show that for standard initialization of ResNet, the stability is guaranteed for tau = 1/\sqrt{L}(L) where L is the number of residual blocks. Moreover, if ResNet is properly over-parameterized, gradient descent is guaranteed to find the global minima 1, which significantly enlarges the range of $\tau$ that admits global convergence in previous work. Empirically, deep ResNet can be easily trained even without normalization layer."
1000,SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"Sparse neural networks have been shown to be more parameter and compute efficient compared to dense networks and in some cases they are even successfully used to decrease wall clock inference times. There is a large body of work on training dense networks to yield sparse networks for inference (Molchanov et al., 2017; Zhu & Gupta, 2018; Louizos et al., 2017; Li et al., 2016; Guo et al., 2016). This limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results with ResNet-50, MobileNet v1 and MobileNet v2 on the ImageNet-2012 dataset, WideResNets on the CIFAR-10 dataset and RNNs on the WikiText-103 dataset. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static.","This paper proposes a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. The method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. The authors show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. They demonstrate state-of-the-art sparse training results on several datasets."
1001,SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"Recent deep generative models are able to provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent work has shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like the position or scale of the object in the image. Our method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations of the generated image, such as translation, zoom or color variations. We demonstrate the effectiveness of our method qualitatively and quantitatively, both for GANs and variational auto-encoders. Figure 1: Images generated with our approach and a BigGAN model (Brock et al., 2018), showing that the position of the object can be controlled within the image.","This paper proposes a method to find meaningful directions in the latent space of any generative model along which we can move to control specific properties of the generated image like the position or scale of the object in the image. The method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations such as translation, zoom or color variations. The experimental results demonstrate the effectiveness of the proposed method qualitatively and quantitatively."
1002,SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"We propose a model that is able to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a physics-as-inverse-graphics approach that brings together vision-as-inverse-graphics and differentiable physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. Our approach significantly outperforms related unsupervised methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. We further show the value of this tight vision-physics integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system. We also show that the controller’s interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation.","This paper proposes a physics-as-inverse-graphics model for unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. This framework allows the model to perform long term extrapolative video prediction, as well as vision-based model-predictive control for a pendulum system. The proposed model significantly outperforms related methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. The controller’s interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation."
1003,SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"In this work we consider the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross-entropy loss function, and then the GCN-inferred “clean” probability is exploited as a relevance measure. Each noisy example is weighted by its relevance when learning a classifier for the end task. We evaluate our method on an extended version of a few-shot learning problem, where the few clean examples of novel classes are supplemented with additional noisy data. Experimental results show that our GCN-based cleaning process significantly improves the classification accuracy over not cleaning the noisy data and standard few-shot classification where only few clean examples are used. The proposed GCN-based method outperforms the transductive approach (Douze et al., 2018) that is using the same additional data without labels.","This paper proposes a method for learning a classifier from noisy labels when a few clean labeled examples are given. The proposed method is based on Graph Convolutional Networks (GCN) which is used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross-entropy loss function, and then the “clean” probability is exploited as a relevance measure. Experimental results show that the proposed method outperforms the transductive approach (Douze et al., 2018) that is using the same additional data without labels."
1004,SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"Graph Neural Networks (GNNs) broadly follow the scheme that the representation vector of each node is updated recursively using the message from neighbor nodes, where the message of a neighbor is usually pre-processed with a parameterized transform matrix. To make better use of edge features, we propose the Edge Information maximized Graph Neural Network (EIGNN) that maximizes the Mutual Information (MI) between edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. We theoretically show that the newly introduced objective enables the model to preserve edge information, and empirically corroborate the enhanced performance of MI-maximized models across a broad range of learning tasks including regression on molecular graphs and relation prediction in knowledge graphs.","This paper proposes a new objective for graph neural networks (GNNs) that maximizes the Mutual Information (MI) between edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. The paper theoretically shows that the newly introduced objective enables the model to preserve edge information, and empirically corroborates the enhanced performance of MI-maximized models across a broad range of learning tasks including regression on molecular graphs and relation prediction in knowledge graphs."
1005,SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"Generative networks are promising models for specifying visual transformations. Unfortunately, certification of generative models is challenging as one needs to capture sufficient non-convexity so to produce precise bounds on the output. Existing verification methods either fail to scale to generative networks or do not capture enough non-convexity. In this work, we present a new verifier, called APPROXLINE, that can certify non-trivial properties of generative networks. APPROXLINE performs both deterministic and probabilistic abstract interpretation and captures infinite sets of outputs of generative networks. We show that APPROXLINE can verify interesting interpolations in the network’s latent space.","This paper proposes a method to certify the properties of generative models. The method is based on the idea of probabilistic abstract interpretation, which is a combination of deterministic and probabilistically abstract interpretation. The authors show that the proposed method is able to verify interpolations in the network’s latent space. The paper is well-written and well-motivated. The experimental results are convincing."
1006,SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node’s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.","This paper proposes GRESNET (Graph Residual Network) to address the problem of suspended animation in existing graph neural networks (GNNs) based on the spectral graph convolutional operator (GCN). In this paper, the authors further identify the suspended animation problem with the existing GNNs and propose a new graph residual network (GRESNET) framework to resolve the problem. The authors also provide a theoretical analysis about the causes of the problem with existing models. Experiments on real-world benchmark datasets demonstrate the effectiveness of the proposed method."
1007,SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"Recovering 3D geometry shape, albedo and lighting from a single image has wide applications in many areas, which is also a typical ill-posed problem. In order to eliminate the ambiguity, face prior knowledge like linear 3D morphable models (3DMM) learned from limited scan data are often adopted to the reconstruction process. However, methods based on linear parametric models cannot generalize well for facial images in the wild with various ages, ethnicity, expressions, poses, and lightings. Recent methods aim to learn a nonlinear parametric model using convolutional neural networks (CNN) to regress the face shape and texture directly. However, the models were only trained on a dataset that is generated from a linear 3DMM. Moreover, the identity and expression representations are entangled in these models, which hurdles many facial editing applications. In this paper, we train our model with adversarial loss in a semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections. A novel center loss is introduced to make sure that different facial images from the same person have the same identity shape and albedo. Besides, our proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that our model produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions.","This paper proposes a semi-supervised method for face reconstruction from unlabeled and labeled images. The proposed method is based on an adversarial loss that aims to disentangle the identity, expression, pose, lighting, and lighting representations. The paper also proposes a novel center loss to make sure that different facial images from the same person have the same identity shape and albedo. Experiments show that the proposed method outperforms the state-of-the-art methods on various facial reconstruction tasks."
1008,SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"Model-based imitation learning methods require full knowledge of the transition kernel for policy evaluation. In this work, we introduce the Expert Induced Markov Decision Process (eMDP) model as a formulation of solving imitation problems using Reinforcement Learning (RL), when only partial knowledge about the transition kernel is available. The idea of eMDP is to replace the unknown transition kernel with a synthetic kernel that: a) simulate the transition of state components for which the transition kernel is known (sr), and b) extract from demonstrations the state components for which the kernel is unknown (su). The next state is then stitched from the two components: s = {sr, su}. We describe in detail the recipe for building an eMDP and analyze the errors caused by its synthetic kernel. Our experiments include imitation tasks in multiplayer games, where the agent has to imitate one expert in the presence of other experts for whom we cannot provide a transition model. We show that combining a policy gradient algorithm with our model achieves superior performance compared to the simulation-free alternative.","This paper proposes a method for imitation learning in the presence of other experts. In particular, the authors propose to replace the unknown transition kernel with a synthetic kernel that simulates the transition of state components for which the transition kernel is known (SR) and extract from demonstrations the state components which the kernel is unknown (su). The next state is then stitched from the two components: s = {sr, su}. The authors describe in detail the recipe for building an eMDP and analyze the errors caused by its synthetic kernel. They show that combining a policy gradient algorithm with their model achieves superior performance compared to simulation-free alternative."
1009,SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"Learning useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new selfsupervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym and a navigation task in the Gazebo simulator. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/l5KaYJWWu70.","This paper proposes a self-supervised reinforcement learning approach for learning to control states of interest without any external reward function. The authors formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the state of interest. They evaluate their approach for different simulated robotic manipulation tasks from OpenAI Gym and a navigation task in the Gazebo simulator. They show that their method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards."
1010,SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"Neural network (NN) trojaning attack is an emerging and important attack that can broadly damage the system deployed with NN models. Different from adversarial attacks, it hides malicious functionality in the weight parameters of NN models. Existing studies have explored NN trojaning attacks in some small datasets for specific domains, with limited numbers of fixed target classes. In this paper, we propose a more powerful trojaning attack method for large models, which outperforms existing studies in capability, generality, and stealthiness. First, the attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim’s deployment. Second, our trojaning attack is not limited in a small domain; one trojaned model on a large-scale dataset can affect applications of different domains that reuses its general features. Third, our trojan shows no biased behavior for different target classes, which makes it more difficult to defend.",This paper proposes a trojan attack method for neural network (NN) trojaning attacks. The attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim’s deployment. The authors show that the attack is not limited in a small domain; one trojaned model on a large-scale dataset can affect applications of different domains that reuses its general features. The trojan shows no biased behavior for different target classes which makes it more difficult to defend.
1011,SP:35ea626ee4dd1a7a368a660eb852192924966b7f,"Due to the significant costs of data generation, many prediction tasks within drug 1 discovery are by nature few-shot regression (FSR) problems, including accurate 2 modelling of biological assays. Although a number of few-shot classification and 3 reinforcement learning methods exist for similar applications, we find relatively 4 few FSR methods meeting the performance standards required for such tasks under 5 real-world constraints. Inspired by deep kernel learning, we develop a novel FSR 6 algorithm that is better suited to these settings. Our algorithm consists of learning 7 a deep network in combination with a kernel function and a differentiable kernel 8 algorithm. As the choice of kernel is critical, our algorithm learns to find the 9 appropriate kernel for each task during inference. It thus performs more effectively 10 with complex task distributions, outperforming current state-of-the-art algorithms 11 on both toy and novel, real-world benchmarks that we introduce herein. By 12 introducing novel benchmarks derived from biological assays, we hope that the 13 community will progress towards the development of FSR algorithms suitable for 14 use in noisy and uncertain environments such as drug discovery. 15","This paper proposes a new few-shot regression algorithm for drug discovery. The proposed method is based on deep kernel learning. The authors propose to learn a deep network in combination with a kernel function and a differentiable kernel algorithm. The choice of kernel is critical, and the proposed algorithm learns to find the appropriate kernel for each task during inference. Experiments are conducted on both toy and real-world drug discovery tasks."
1012,SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.","This paper proposes a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. The authors define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations."
1013,SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"Conditional Generative Adversarial Networks (cGANs) are finding increasingly widespread use in many application domains. Despite outstanding progress, quantitative evaluation of such models often involves multiple distinct metrics to assess different desirable properties, such as image quality, conditional consistency, and intra-conditioning diversity. In this setting, model benchmarking becomes a challenge, as each metric may indicate a different “best” model. In this paper, we propose the Fréchet Joint Distance (FJD), which is defined as the Fréchet distance between joint distributions of images and conditioning, allowing it to implicitly capture the aforementioned properties in a single metric. We conduct proof-of-concept experiments on a controllable synthetic dataset, which consistently highlight the benefits of FJD when compared to currently established metrics. Moreover, we use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities (e.g. class labels, object masks, bounding boxes, images, and text captions). We show that FJD can be used as a promising single metric for cGAN benchmarking and model selection.","This paper proposes a new metric for evaluating conditional generative adversarial networks (cGANs) based on the Fréchet Joint Distance (FJD), which is defined as the distance between joint distributions of images and conditioning distributions. The authors propose to use the FJD as a single metric for cGAN benchmarking and model selection. Experiments are conducted on a controllable synthetic dataset and compare the proposed FJD with existing metrics. "
1014,SP:fa822e8472efae17c7dfde8258057898383ecbbb,"We learn to identify ‘decision states’, namely the parsimonious set of states where decisions meaningfully affect the future states an agent can reach in an environment. We utilize the VIC framework (Gregor et al., 2016), which maximizes an agent’s ‘empowerment’, i.e. the ability to reliably reach a diverse set of states – and formulate a sandwich bound on the empowerment objective that allows identification of decision states. Unlike previous work (Goyal et al., 2019), our decision states are discovered without extrinsic rewards – simply by interacting with the world. Our results show that our decision states are: 1) often interpretable, and 2) lead to better exploration on downstream goal-driven tasks in partially observable environments.","This paper proposes a method for learning to identify ‘decision states’, i.e. the parsimonious set of states where decisions meaningfully affect the future states an agent can reach in an environment. The authors use the VIC framework (Gregor et al., 2016), which maximizes an agent’s ‘empowerment’ and formulate a sandwich bound on the empowerment objective that allows identification of decision states. Unlike previous work, the decision states are discovered without extrinsic rewards – simply by interacting with the world."
1015,SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"Despite the eminent successes of deep neural networks, many architectures are often hard to transfer to irregularly-sampled and asynchronous time series that occur in many real-world datasets, such as healthcare applications. This paper proposes a novel framework for classifying irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency. Our method SEFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. We extensively compare our method to competitors on multiple healthcare time series datasets and show that it performs competitively whilst significantly reducing runtime.","This paper proposes a method to classify irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency. The method SEFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. The authors extensively compare their method to competitors on multiple healthcare time series datasets and show that it performs competitively while significantly reducing runtime."
1016,SP:4ae89d64460b08749acc192004545c1fa8b7553b,"Convolutional neural networks (CNNs) excel in image recognition and generation. Among many efforts to explain their effectiveness, experiments show that CNNs carry strong inductive biases that capture natural image priors. Do deep networks also have inductive biases for audio signals? In this paper, we empirically show that current network architectures for audio processing do not show strong evidence in capturing such priors. We propose Harmonic Convolution, an operation that helps deep networks model priors in audio signals by explicitly utilizing the harmonic structure. This is done by engineering the kernels to be supported by sets of harmonic series, instead of by local neighborhoods as convolutional kernels. We show that networks using Harmonic Convolution can reliably model audio priors and achieve high performance on unsupervised audio restoration. With Harmonic Convolution, they also achieve better generalization performance for supervised musical source separation. Code and examples are available at our project page: http://dap.csail.mit.edu.","This paper proposes a new convolutional neural network architecture for audio processing based on the harmonic structure of audio signals. Specifically, the authors propose a convolution operation called ""Harmonic Convolution"" which is based on convolution kernels that are composed of sets of harmonic series instead of local neighborhoods. The authors show that the proposed method can be used to model audio priors and achieve high performance on unsupervised audio restoration tasks. They also show that it can achieve better generalization performance for supervised musical source separation tasks."
1017,SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"In the twilight of Moore’s law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce “data echoing,” which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline’s predictive performance using less upstream computation. We measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.","This paper proposes a method to reduce the total computation used by earlier pipeline stages and speed up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipeline stage in order to reclaim idle capacity. The authors investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. They find that in all settings, at least one data echoing algorithm can match the baseline’s predictive performance using less upstream computation."
1018,SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"It has been established that diverse behaviors spanning the controllable subspace of a Markov decision process can be trained by rewarding a policy for being distinguishable from other policies (Gregor et al., 2016; Eysenbach et al., 2018; Warde-Farley et al., 2018). However, one limitation of this formulation is the difficulty to generalize beyond the finite set of behaviors being explicitly learned, as may be needed in subsequent tasks. Successor features (Dayan, 1993; Barreto et al., 2017) provide an appealing solution to this generalization problem, but require defining the reward function as linear in some grounded feature space. In this paper, we show that these two techniques can be combined, and that each method solves the other’s primary limitation. To do so we introduce Variational Intrinsic Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework. We empirically validate VISR on the full Atari suite, in a novel setup wherein the rewards are only exposed briefly after a long unsupervised phase. Achieving human-level performance on 12 games and beating all baselines, we believe VISR represents a step towards agents that rapidly learn from limited feedback.","This paper proposes a method for learning controllable features that can be leveraged to improve the generalization ability of successor features. The proposed method is based on the Variational Intrinsic Successor FeatuRes (VISR) algorithm, which is a variant of the successor features framework. The main idea is to learn a feature space that is a linear function of the reward function, and then use this feature space to train a successor policy that can generalize beyond the finite set of behaviors that are explicitly learned in the first task. The authors evaluate the proposed method on the Atari suite, where the rewards are only exposed briefly after a long unsupervised phase. The experimental results show that VISR can achieve human-level performance on 12 games and beats all baselines."
1019,SP:83500230586a9134f910ad067b7233dc563dc1ba,"Despite their popularity and successes, deep neural networks are poorly understood theoretically and treated as ’black box’ systems. Using a functional view of these networks gives us a useful new lens with which to understand them. Theoretical (in shallow) and experimentally probing properties of these networks reveals insights into the effect of standard initializations, the value of depth, the underlying loss surface, and the origins of generalization. One key result is that generalization results from smoothness of the functional approximation, combined with a flat initial approximation. This smoothness increases with number of units, explaining why massively overparamaterized networks continue to generalize well.","This paper studies the generalization properties of deep neural networks (DNNs) from a functional point of view. In particular, the authors study the smoothness of the functional approximation of DNNs, which is a function of the depth of the network, the number of units, and the initialization of the networks. Theoretical results are provided for both shallow and deep networks, and experiments are conducted on MNIST, CIFAR-10, and ImageNet. The authors show that generalization results from smoothness and flat initializations of the DNN."
1020,SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"Recently, Generative Adversarial Network (GAN) and numbers of its variants have been widely used to solve the image-to-image translation problem and achieved extraordinary results in both a supervised and unsupervised manner. However, most GAN-based methods suffer from the imbalance problem between the generator and discriminator in practice. Namely, the relative model capacities of the generator and discriminator do not match, leading to mode collapse and/or diminished gradients. To tackle this problem, we propose a GuideGAN based on attention mechanism. More specifically, we arm the discriminator with an attention mechanism so not only it estimates the probability that its input is real, but also does it create an attention map that highlights the critical features for such prediction. This attention map then assists the generator to produce more plausible and realistic images. We extensively evaluate the proposed GuideGAN framework on a number of image transfer tasks. Both qualitative results and quantitative comparison demonstrate the superiority of our proposed approach.","This paper proposes a new method for image-to-image translation based on the attention mechanism in GANs. Specifically, the proposed method is based on an attention mechanism that estimates the probability that its input is real, and also does it create an attention map that highlights the critical features for such prediction. This attention map then assists the generator to produce more plausible and realistic images. Experiments are conducted on a number of image transfer tasks and show the superiority of the proposed approach."
1021,SP:41c089ba65393174dae1dc136f79030a0a4fc532,"We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others. Multiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. We begin by showing that such layers strictly enrich the representable function classes of neural networks. We conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. We therefore argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, we back up our claims and demonstrate the potential of multiplicative interactions by applying them in large-scale complex RL and sequence modelling tasks, where their use allows us to deliver state-of-the-art results, and thereby provides new evidence in support of multiplicative interactions playing a more prominent role when designing new neural network architectures.","This paper studies the role of multiplicative interaction layers as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions. The authors conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. They argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, they back up their claims and demonstrate the potential of multiplicator interactions by applying them in large-scale complex RL and sequence modelling tasks, where their use allows them to deliver state-of-the-art results."
1022,SP:5144391584e6d3825e12684b7c053e4e282cff2b,"We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.","This paper proposes a new algorithm for active learning with deep neural network models. Specifically, the authors propose to sample groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, which is a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, the proposed algorithm consistently performs as well or better, making it a useful option for real world active learning problems."
1023,SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions†.",This paper proposes a method for self-explaining deep neural networks (DNNs). The authors argue that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. The authors propose a novel feature leveling architecture that isolates low-level features from high-level feature on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that the modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self explainable.
1024,SP:b70ceead1bf6c7dc684c74501716e7012b891022,"Training a classifier over a large number of classes, known as ’extreme classification’, has become a topic of major interest with applications in technology, science, and e-commerce. Traditional softmax regression induces a gradient cost proportional to the number of classes C, which often is prohibitively expensive. A popular scalable softmax approximation relies on uniform negative sampling, which suffers from slow convergence due a poor signal-to-noise ratio. In this paper, we propose a simple training method for drastically enhancing the gradient signal by drawing negative samples from an adversarial model that mimics the data distribution. Our contributions are three-fold: (i) an adversarial sampling mechanism that produces negative samples at a cost only logarithmic in C, thus still resulting in cheap gradient updates; (ii) a mathematical proof that this adversarial sampling minimizes the gradient variance while any bias due to non-uniform sampling can be removed; (iii) experimental results on large scale data sets that show a reduction of the training time by an order of magnitude relative to several competitive baselines.","This paper proposes an adversarial training method for extreme classification. The proposed method is based on the adversarial sampling mechanism that produces negative samples at a cost only logarithmic in C, thus still resulting in cheap gradient updates. The paper also provides a mathematical proof that this adversarial model minimizes the gradient variance while any bias due to non-uniform sampling can be removed. The experimental results on large scale data sets show a reduction of the training time by an order of magnitude relative to several competitive baselines."
1025,SP:29b52fee83309268d9864f3b1fc3617948577d41,"We present a new approach for efficient exploration which leverages a lowdimensional encoding of the environment learned with a combination of modelbased and model-free objectives. Our approach uses intrinsic rewards that are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty. We then leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational space. One key element of our approach is that we perform more gradient steps in-between every environment step in order to ensure the model accuracy. We test our approach on a number of maze tasks, as well as a control problem and show that our exploration approach is more sample-efficient compared to strong baselines.",This paper proposes an exploration method that leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. The intrinsic rewards are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty. The authors leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational spaces. One key element of the approach is that the authors perform more gradient steps in-between every environment step in order to ensure the model accuracy.
1026,SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"In many real-world settings, a learning model must perform few-shot classification: learn to classify examples from unseen classes using only a few labeled examples per class. Additionally, to be safely deployed, it should have the ability to detect out-of-distribution inputs: examples that do not belong to any of the classes. While both few-shot classification and out-of-distribution detection are popular topics, their combination has not been studied. In this work, we propose tasks for outof-distribution detection in the few-shot setting and establish benchmark datasets, based on four popular few-shot classification datasets. Then, we propose two new methods for this task and investigate their performance. In sum, we establish baseline out-of-distribution detection results using standard metrics on new benchmark datasets and show improved results with our proposed methods.","This paper studies the problem of out-of-distribution detection in the few-shot classification setting. The authors propose two new methods for this task and establish benchmark datasets based on four popular few shot classification datasets. Then, they propose two methods for out of distribution detection and investigate their performance. The proposed methods are evaluated on two benchmark datasets and show improved results with the proposed methods. "
1027,SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. The problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT’14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.","This paper proposes a generative model that unifies decoding in directed and undirected neural sequence models. Specifically, the generative process is modeled as an autoregressive process, and the authors propose to model the decoding process as a special case of the generation process. The authors also propose a method to adapt decoding algorithms originally developed for directed sequence models to unify decoding in undirectED models. The proposed method is evaluated on a cross-lingual masked translation task, where it is shown to outperform the state of the art on WMT’14 English-German translation."
1028,SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"Although mathematical expressions (MEs) recognition have achieved great progress, the development of MEs recognition in real scenes is still unsatisfactory. Inspired by the recent work of neutral network, this paper proposes a novel two-stage approach which takes a printed mathematical expression image as input and generates LaTeX sequence as output. In the first stage, this method locates and recognizes the math symbols of input image by object detection algorithm. In the second stage, it translates math symbols with position information into LaTeX sequences by seq2seq model equipped with attention mechanism. In particular, the detection of mathematical symbols and the structural analysis of mathematical formulas are carried out separately in two steps, which effectively improves the recognition accuracy and enhances the generalization ability. The experiment demonstrates that the two-stage method significantly outperforms the end-to-end method. Especially, the ExpRate(expression recognition rate) of our model is 74.1%, 20.3 percentage points higher than that of the end-to-end model on the test data that doesn’t come from the same source as training data.","This paper proposes a novel two-stage approach for the recognition of mathematical expressions (MEs) in real-world scenes. In the first stage, an object detection algorithm is used to detect the math symbols of the input image, and in the second stage, a seq2seq model is trained to generate LaTeX sequences with position information. The proposed method is evaluated on a variety of datasets and compared with the end-to-end method. The experimental results show that the proposed method outperforms the state-of-the-art."
1029,SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"In this paper, we address the problem of reducing the memory footprint of convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The principle of our approach is that it minimizes the loss reconstruction error for in-domain inputs. Our method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using bytealigned codebooks to store the compressed weights. We validate our approach by quantizing a high performing ResNet-50 model to a memory size of 5 MB (20× compression factor) while preserving a top-1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26× factor.1",This paper proposes a vector quantization method to reduce the memory footprint of convolutional neural networks. The method is based on the idea that the quality of the reconstruction of the network output should be preserved rather than its weights. The authors propose to use a set of unlabelled data at quantization time and use bytealigned codebooks to store the compressed weights. They validate their approach by quantizing a high performing ResNet-50 model to a memory size of 5 MB while preserving a top-1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26x factor.
1030,SP:74850ad70241948f93fed95ba1f0ac11360437c1,"We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our TensorProduct Transformer (TP-Transformer) sets a new state of the art on the recentlyintroduced Mathematics Dataset containing 56 categories of free-form math wordproblems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer’s attention maps give better insights into how it is capable of solving the Mathematics Dataset’s challenging problems. Pretrained models and code will be made available after publication.","This paper proposes a new Transformer-based model for the Mathematics Dataset, which is a set of 56 free-form math word-problems. The model is based on the Transformer architecture, but with the addition of a Tensor-Product Transformer (TP-Transformer) module that encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. The proposed TP-Attention is a novel attention mechanism that explicitly encodes relations between Transformer cells and other cells. The paper is well-written and well-motivated. The experimental results show that the proposed model is able to achieve state-of-the-art performance on the mathematics dataset."
1031,SP:d319df820c6630c409fab32097652a083e8f53ea,"Deep artificial neural networks can achieve an extremely small difference between training and test accuracies on identically distributed training and test sets, which is a standard measure of generalization. However, the training and test sets may not be sufficiently representative of the empirical sample set, which consists of real-world input samples. When samples are drawn from an underrepresented or unrepresented subset during inference, the gap between the training and inference accuracies can be significant. To address this problem, we first reformulate a learning algorithm as a procedure for searching for a source code that maps input features to classes. We then derive a necessary and sufficient condition for generalization using a universal cognitive similarity metric, namely information distance, based on Kolmogorov complexity. Using this condition, we formulate an optimization problem to learn a more general classification function. To achieve this end, we extend the input features by concatenating encodings of them, and then train the classifier on the extended features. As an illustration of this idea, we focus on image classification, where we use channel codes on the input features as a systematic way to improve the degree to which the training and test sets are representative of the empirical sample set. To showcase our theoretical findings, considering that corrupted or perturbed input features belong to the empirical sample set, but typically not to the training and test sets, we demonstrate through extensive systematic experiments that, as a result of learning a more general classification function, a model trained on encoded input features is significantly more robust to common corruptions, e.g., Gaussian and shot noise, as well as adversarial perturbations, e.g., those found via projected gradient descent, than the model trained on uncoded input features.","This paper studies the problem of generalization in deep neural networks, where the training and test sets may not be sufficiently representative of the empirical sample set, which consists of real-world input samples. To address this problem, the authors reformulate a learning algorithm as a procedure for searching for a source code that maps input features to classes, and derive a necessary and sufficient condition for generalization using a universal cognitive similarity metric, namely information distance, based on Kolmogorov complexity. To achieve this end, they extend the input features by concatenating encodings of them, and then train the classifier on the extended features. They demonstrate through extensive systematic experiments that, as a result of learning a more general classification function, a model trained on encoded input features is significantly more robust to common corruptions, e.g., Gaussian and shot noise, as well as adversarial perturbations, than the models trained on uncoded input features."
1032,SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"Deep Graph Neural Networks (GNNs) are instrumental in graph classification and graph-based regression tasks. In these tasks, graph pooling is a critical ingredient by which GNNs adapt to input graphs of varying size and structure. We propose a new graph pooling operation based on compressive Haar transforms, called HaarPooling. HaarPooling is computed by following a chain of sequential clusterings of the input graph. The input of each pooling layer is transformed by the compressive Haar basis of the corresponding clustering. HaarPooling operates in the frequency domain by the synthesis of nodes in the same cluster and filters out fine detail information by compressive Haar transforms. Such transforms provide an effective characterization of the data and preserve the structure information of the input graph. By the sparsity of the Haar basis, the computation of HaarPooling is of linear complexity. The GNN with HaarPooling and existing graph convolution layers achieves state-of-the-art performance on diverse graph classification problems.","This paper proposes a new graph pooling operation based on compressive Haar transforms, called HaarPooling. The proposed method is based on following a chain of sequential clusterings of the input graph. The input of each pooling layer is transformed by the compressive haar basis of the corresponding clustering. By the sparsity of the Haar basis, the computation of Haarpooling is of linear complexity. Experiments are conducted on graph classification and graph-based regression tasks."
1033,SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"Point clouds are a flexible and ubiquitous way to represent 3D objects with arbitrary resolution and precision. Previous work has shown that adapting encoder networks to match the semantics of their input point clouds can significantly improve their effectiveness over naive feedforward alternatives. However, the vast majority of work on point-cloud decoders are still based on fully-connected networks that map shape representations to a fixed number of output points. In this work, we investigate decoder architectures that more closely match the semantics of variable sized point clouds. Specifically, we study sample-based point-cloud decoders that map a shape representation to a point feature distribution, allowing an arbitrary number of sampled features to be transformed into individual output points. We develop three sample-based decoder architectures and compare their performance to each other and show their improved effectiveness over feedforward architectures. In addition, we investigate the learned distributions to gain insight into the output transformation. Our work is available as an extensible software platform to reproduce these results and serve as a baseline for future work.","This paper proposes a sample-based point-cloud decoder architecture that maps a shape representation to a point feature distribution, allowing an arbitrary number of sampled features to be transformed into individual output points. The proposed method is based on a fully-connected network that maps shape representations to a fixed number of output points, while the proposed method maps a feature distribution to a set of sampled points. Experiments show that the proposed methods outperform the state-of-the-art feedforward architectures."
1034,SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"Performing controlled experiments on noisy data is essential in thoroughly understanding deep learning across a spectrum of noise levels. Due to the lack of suitable datasets, previous research have only examined deep learning on controlled synthetic noise, and real-world noise has never been systematically studied in a controlled setting. To this end, this paper establishes a benchmark of realworld noisy labels at 10 controlled noise levels. As real-world noise possesses unique properties, to understand the difference, we conduct a large-scale study across a variety of noise levels and types, architectures, methods, and training settings. Our study shows that: (1) Deep Neural Networks (DNNs) generalize much better on real-world noise. (2) DNNs may not learn patterns first on real-world noisy data. (3) When networks are fine-tuned, ImageNet architectures generalize well on noisy data. (4) Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. (5) Robust learning methods that work well on synthetic noise may not work as well on real-world noise, and vice versa. We hope our benchmark, as well as our findings, will facilitate deep learning research on noisy data.","This paper presents a benchmark of real-world noisy labels at 10 controlled noise levels. The authors conduct a large-scale study across a variety of noise levels and types, architectures, methods, and training settings. They show that: (1) Deep Neural Networks (DNNs) generalize much better on real world noise. (2) DNNs may not learn patterns first on real- world noisy data. (3) When networks are fine-tuned, ImageNet architectures generalize well on noisy data, yet it is more difficult for robust DNN methods to improve. (4) Robust learning methods that work well on synthetic noise may not work as well on real data."
1035,SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.","This paper proposes a method for combining human supervision with supervised learning for denoising rules. The proposed method is based on the idea that human supervision is natural for humans and synergistic for learning. The supervision is coupled with a soft-implicit loss that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that the proposed algorithm is more accurate than several existing methods."
1036,SP:6f2c656dbb7629f652a4291d6971625184d8118b,Graph neural networks (GNNs) are a class of deep models that operate on data with arbitrary topology represented as graphs. We introduce an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. We also introduce two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results show that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data. Code and reference implementations are released at: https://github.com/amirkhas/GraphMemoryNet,This paper proposes a memory-based GNN (MemGNN) and graph memory network (GMN) for graph neural networks (GNNs) that can jointly learn node representations and coarsen the graph. The authors also introduce two new networks based on this layer: MemGNN and GMN. The experimental results show that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. The learned representations could correspond to chemical features in the molecule data.
1037,SP:81bc52d734c86975d741b6482d65ca71a9d81620,"The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. We show that for deep networks, the width needed for efficient convergence to a global minimum with orthogonal initializations is independent of the depth, whereas the width needed for efficient convergence with Gaussian initializations scales linearly in the depth. Our results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for the recent empirical successes found by initializing very deep non-linear networks according to the principle of dynamical isometry.","This paper analyzes the effect of initialization in deep linear networks on the convergence of deep neural networks. The authors prove that orthogonal initialization speeds up convergence relative to the standard Gaussian initialization with iid weights. They show that for deep networks, the width needed for efficient convergence to a global minimum is independent of the depth and scales linearly in the depth. Their results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for recent empirical successes found by initializing very deep non-linear networks according to the principle of dynamical isometry."
1038,SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"State-of-the-art quantization methods can compress deep neural networks down to 4 bits without losing accuracy. However, when it comes to 2 bits, the performance drop is still noticeable. One problem in these methods is that they assign equal bit rate to quantize weights and activations in all layers, which is not reasonable in the case of high rate compression (such as 2-bit quantization), as some of layers in deep neural networks are sensitive to quantization and performing coarse quantization on these layers can hurt the accuracy. In this paper, we address an important problem of how to optimize the bit allocation of weights and activations for deep CNNs compression. We first explore the additivity of output error caused by quantization and find that additivity property holds for deep neural networks which are continuously differentiable in the layers. Based on this observation, we formulate the optimal bit allocation problem of weights and activations in a joint framework and propose a very efficient method to solve the optimization problem via Lagrangian Formulation. Our method obtains excellent results on deep neural networks. It can compress deep CNN ResNet-50 down to 2 bits with only 0.7% accuracy loss. To the best of our knowledge, this is the first paper that reports 2-bit results on deep CNNs without hurting the accuracy.","This paper studies the problem of how to optimize the bit allocation of weights and activations for deep CNNs compression. The authors propose a Lagrangian-based method to solve the optimization problem of bit allocation in the case of 2-bit quantization. The main contribution of this paper is to study the additivity of output error caused by quantization and find that additivity property holds for deep neural networks which are continuously differentiable in the layers. Based on this observation, the authors formulate the optimal bit allocation problem of weights/activations in a joint framework and propose a very efficient method. The proposed method can compress deep CNN ResNet-50 down to 2 bits with only 0.7% accuracy loss."
1039,SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"Generative Adversarial Networks (GANs) have been impactful on many problems and applications but suffer from unstable training. Wasserstein GAN (WGAN) leverages the Wasserstein distance to avoid the caveats in the minmax two-player training of GANs but has other defects such as mode collapse and lack of metric to detect the convergence. We introduce a novel inference WGAN (iWGAN) model, which is a principled framework to fuse auto-encoders and WGANs. The iWGAN jointly learns an encoder network and a generative network using an iterative primal dual optimization process. We establish the generalization error bound of iWGANs. We further provide a rigorous probabilistic interpretation of our model under the framework of maximum likelihood estimation. The iWGAN, with a clear stopping criteria, has many advantages over other autoencoder GANs. The empirical experiments show that our model greatly mitigates the symptom of mode collapse, speeds up the convergence, and is able to provide a measurement of quality check for each individual sample. We illustrate the ability of iWGANs by obtaining a competitive and stable performance with state-of-the-art for benchmark datasets.","This paper proposes a novel inference WGAN (iWGAN) model, which is a principled framework to fuse auto-encoders and WGANs. The iWGAN jointly learns an encoder network and a generative network using an iterative primal dual optimization process. The authors establish the generalization error bound of iWgan and provide a rigorous probabilistic interpretation of the model under the framework of maximum likelihood estimation. The empirical experiments show that the proposed model mitigates the symptom of mode collapse, speeds up the convergence, and is able to provide a measurement of quality check for each individual sample."
1040,SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"The availability of large datasets is essential for progress in coreference and other areas of NLP. Crowdsourcing has proven a viable alternative to expert annotation, offering similar quality for better scalability. However, crowdsourcing require adjudication, and most models of annotation focus on classification tasks where the set of classes is predetermined. This restriction does not apply to anaphoric annotation, where coders relate markables to coreference chains whose number cannot be predefined. This gap was recently covered with the introduction of a mention pair model of anaphoric annotation (MPA). In this work we extend MPA to alleviate the effects of sparsity inherent in some crowdsourcing environments. Specifically, we use a nonparametric partially pooled structure (based on a stick breaking process), fitting jointly with the ability of the annotators hierarchical community profiles. The individual estimates can thus be improved using information about the community when the data is scarce. We show, using a recently published large-scale crowdsourced anaphora dataset, that the proposed model performs better than its unpooled counterpart in conditions of sparsity, and on par when enough observations are available. The model is thus more resilient to different crowdsourcing setups, and, further provides insights into the community of workers. The model is also flexible enough to be used in standard annotation tasks for classification where it registers on par performance with the state of the art.","This paper proposes a new model for crowdsourced crowdsourced coreference. The proposed model is based on the mention pair model of anaphoric annotation (MPA). The authors propose to use a nonparametric partially pooled structure (based on a stick breaking process) to alleviate the effects of sparsity inherent in some crowdsourcing environments. The authors show that the proposed model performs better than its unpooled counterpart in conditions of data sparsity, and on par when enough observations are available."
1041,SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"Exploration in sparse reward reinforcement learning remains an open challenge. Many state-of-the-art methods use intrinsic motivation to complement the sparse extrinsic reward signal, giving the agent more opportunities to receive feedback during exploration. Commonly these signals are added as bonus rewards, which results in a mixture policy that neither conducts exploration nor task fulfillment resolutely. In this paper, we instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, we introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. It takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. We evaluate our proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. The results show a substantially improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives. A video of our experimental results can be found at https://gofile.io/?c=HpEwTd.","This paper proposes a method for sparse reward reinforcement learning that combines intrinsic and extrinsic rewards. The intrinsic reward is modeled as a linear combination of the intrinsic reward and the task reward, and the extrinsics are modeled as the successor feature control (SFC) reward, which takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. The authors evaluate their proposed intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. The results show a substantially improved exploration efficiency with SFC and the hierarchical usage of intrinsic drives."
1042,SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"Given a video and a sentence, the goal of weakly-supervised video moment retrieval is to locate the video segment which is described by the sentence without having access to temporal annotations during training. Instead, a model must learn how to identify the correct segment (i.e. moment) when only being provided with video-sentence pairs. Thus, an inherent challenge is automatically inferring the latent correspondence between visual and language representations. To facilitate this alignment, we propose our Weakly-supervised Moment Alignment Network (wMAN) which exploits a multi-level co-attention mechanism to learn richer multimodal representations. The aforementioned mechanism is comprised of a Frame-By-Word interaction module as well as a novel Word-Conditioned Visual Graph (WCVG). Our approach also incorporates a novel application of positional encodings, commonly used in Transformers, to learn visual-semantic representations that contain contextual information of their relative positions in the temporal sequence through iterative message-passing. Comprehensive experiments on the DiDeMo and Charades-STA datasets demonstrate the effectiveness of our learned representations: our combined wMAN model not only outperforms the state-of-the-art weakly-supervised method by a significant margin but also obtains an improvement of 10% for the Recall@1 accuracy metric over stronglysupervised state-of-the-art methods on the DiDeMo dataset.",", the paper proposes a method for weakly supervised video moment retrieval. The method is based on a multi-level co-attention mechanism to learn richer multimodal representations. The proposed method consists of a Frame-By-Word interaction module as well as a Word-Conditioned Visual Graph (WCVG). The paper also incorporates a novel application of positional encodings, commonly used in Transformers, to learn visual-semantic representations."
1043,SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours & sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts. To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects. Based on these estimations, we are able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects. To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results. Using this image-guided approach, the network does not have to allocate capacity on “remembering” object appearance, instead it learns how to combine the appearance of captured images. We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.","This paper proposes a method for image-guided re-rendering of reconstructed objects for virtual and augmented reality applications. The main idea is to train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the objects via multi-view stereo. Based on this proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view- dependent effects, such as specular highlights, it leads to artifacts. To this end, the paper proposes EffectsNet, a neural network that predicts view-independent effects. The paper demonstrates the effectiveness of the approach both qualitatively and quantitatively on synthetic as well as on real data."
1044,SP:257d124367b1da9a595dc11a9df750d6bade298e,"This paper presents a sparse representation of model uncertainty for deep neural networks (DNNs) that relies on an inverse formulation of Multivariate Normal Distribution (MND): an information form. We show that the model uncertainty can be estimated in this form using a scalable Laplace Approximation scheme, which involves a diagonal correction of the Kronecker-factored eigenbasis. As this makes the inversion of the information matrix intractable an operation that is required for a full Bayesian analysis, we further devise a novel low-rank approximation of this eigenbasis that exploits spectral sparsity of DNNs. Methods to realize this sparsification are provided that develops into a memory-wise tractable sampling computations. Both of our theoretical analysis and empirical evaluations over various benchmarks show the superiority of our approach over existing methods.","This paper presents a sparse representation of model uncertainty for deep neural networks (DNNs) that relies on an inverse formulation of Multivariate Normal Distribution (MND): an information form. The paper shows that the model uncertainty can be estimated in this form using a scalable Laplace Approximation scheme, which involves a diagonal correction of the Kronecker-factored eigenbasis. As this makes the inversion of the information matrix intractable an operation that is required for a full Bayesian analysis, the paper further devise a novel low-rank approximation of this eigenBasis that exploits spectral sparsity of DNNs."
1045,SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"Minwise Hashing (MinHash) is a fundamental method to compute set similarities and compact high-dimensional data for efficient learning and searching. The bottleneck of MinHash is computing k (usually hundreds) MinHash values. One Permutation Hashing (OPH) only requires one permutation (hash function) to get k MinHash values by dividing elements into k bins. One drawback of OPH is that the load of the bins (the number of elements in a bin) could be unbalanced, which leads to the existence of empty bins and false similarity computation. Several strategies for densification, that is, filling empty bins, have been proposed. However, the densification is just a remedial strategy and cannot eliminate the error incurred by the unbalanced load. Unlike the densification to fill the empty bins after they undesirably occur, our design goal is to balance the load so as to reduce the empty bins in advance. In this paper, we propose a load-balanced hashing, Amortization Hashing (AHash), which can generate as few empty bins as possible. Therefore, AHash is more load-balanced and accurate without hurting runtime efficiency compared with OPH and densification strategies. Our experiments on real datasets validate the claim. All source codes and datasets have been released on GitHub anonymously.","This paper proposes a load-balanced hashing method for Minwise hashing (MH) that aims to reduce the number of empty bins in Minwise Hashing (MH). In particular, the authors propose to balance the load of the bins (the number of elements in a bin) in the MinHash algorithm, which is based on one-permutation hashing (OPH). The authors claim that the proposed method AHash is more load balanced and accurate without hurting runtime efficiency compared with OPH and densification strategies. The experiments on real datasets validate the claim."
1046,SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,"We propose a feature extraction for periodic signals. Virtually every mechanized transportation vehicle, power generation, industrial machine, and robotic system contains rotating shafts. It is possible to collect data about periodicity by measuring a shaft’s rotation. However, it is difficult to perfectly control the collection timing of the measurements. Imprecise timing creates phase shifts in the resulting data. Although a phase shift does not materially affect the measurement of any given data point collected, it does alter the order in which all of the points are collected. It is difficult for classical methods, like multi-layer perceptron, to identify or quantify these alterations because they depend on the order of the input vectors’ components. This paper proposes a robust method for extracting features from phase shift data by adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with cyclic permutation. Simulation and experimental results illustrate its effectiveness.",This paper proposes a method for extracting features from phase shift data by adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with cyclic permutation. The proposed method is based on graph neural networks (GNNs) which are trained to extract features from periodic signals. The authors show that the proposed method outperforms the state-of-the-art baselines in both synthetic and real-world experiments.
1047,SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,"Neural conditional text generation systems have achieved significant progress in recent years, showing the ability to produce highly fluent text. However, the inherent lack of controllability in these systems allows them to hallucinate factually incorrect phrases that are unfaithful to the source, making them often unsuitable for many real world systems that require high degrees of precision. In this work, we propose a novel confidence oriented decoder that assigns a confidence score to each target position. This score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on a structured data-to-text dataset – WikiBio (Lebret et al., 2016) – show that our approach is more faithful to the source than existing state-of-the-art approaches, according to both automatic metrics and human evaluation.","This paper proposes a method to train conditional text generation systems that are more faithful to the source text. The authors propose a confidence-based decoder that assigns a confidence score to each target position. This score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on a structured data-to-text dataset – WikiBio (Lebret et al., 2016) show that their approach is more faithful than existing state-of-the-art approaches, according to automatic metrics and human evaluation."
1048,SP:03307deac29173b2968fbd08f95fc77eb1f82410,"Magnitude-based pruning is one of the simplest methods for pruning neural networks. Despite its simplicity, magnitude-based pruning and its variants demonstrated remarkable performances for pruning modern architectures. Based on the observation that magnitude-based pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer, we develop a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. Our experimental results demonstrate that the proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. See https://github.com/alinlab/lookahead_pruning for codes.","This paper proposes a new method for pruning neural networks based on magnitude-based pruning. The proposed method is based on the observation that magnitude pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer. Based on this observation, the authors propose to extend the single layer optimization to a multi-layer optimization. The experimental results demonstrate that the proposed method consistently outperforms magnitude based pruning on various networks including VGG and ResNet, particularly in the high-sparsity regime."
1049,SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased or linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms. We also show that Moniqua is robust to very low bit-budgets, allowing less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.","This paper proposes Moniqua, a method for decentralized stochastic gradient descent (SGD) that uses quantized communication. Theoretical analysis shows that the proposed method can communicate a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Empirical results on CIFAR-10 and VGG-16 show that the method is robust to very low bit-budgets, allowing less than 4-bits-per-parameter communication without affecting convergence."
1050,SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"In reinforcement learning, we can learn a model of future observations and rewards, and use it to plan the agent’s next actions. However, jointly modeling future observations can be computationally expensive or even intractable if the observations are high-dimensional (e.g. images). For this reason, previous works have considered partial models, which model only part of the observation. In this paper, we show that partial models can be causally incorrect: they are confounded by the observations they don’t model, and can therefore lead to incorrect planning. To address this, we introduce a general family of partial models that are provably causally correct, yet remain fast because they do not need to fully model future observations.","This paper studies the problem of learning a model of future rewards for reinforcement learning. The authors propose a family of partial models that are provably causally correct, yet remain fast because they do not need to fully model future observations. In particular, they show that partial models can be causally incorrect: they are confounded by the observations they don’t model, and can therefore lead to incorrect planning. To address this, the authors introduce a general family of models that is provably correct, but still fast. "
1051,SP:c70479b2096a52584b242de58272ca8d8565feea,"A new variational autoencoder (VAE) model is proposed that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is based on two information theoretic problems—distributed simulation and channel synthesis—in which Wyner’s common information arises as the fundamental limit of the succinctness of the common representation. The Wyner VAE decomposes a pair of correlated data variables into their common representation (e.g., a shared concept) and local representations that capture the remaining randomness (e.g., texture and style) in respective data variables by imposing the mutual information between the data variables and the common representation as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images. Experimental results show that learning a succinct common representation achieves better generative performance and that the proposed model outperforms existing VAE variants and the variational information bottleneck method.","This paper proposes a new variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed model is based on two information theoretic problems: distributed simulation and channel synthesis, in which Wyner’s common information arises as the fundamental limit of the succinctness of the common representation. Experimental results show that the proposed model outperforms existing VAE variants and the variational information bottleneck method."
