paper_id,summary
SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"The paper proposes a hierarchical multi - agent learning framework based on role - based RL. The proposed method, called RODE, first decomposes joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Learning a role selector based on action effects makes role discovery much easier because it forms a bi - level learning hierarchy : role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action - observation spaces. The authors further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. By virtue of these advances, RODE outperforms the current state - of - the - art MARL algorithms on 9 out of the 14 scenarios that comprise the challenging StarCraft II benchmark. Moreover, the learned RODE policy can be transferred to new environments with three times as many agents."
SP:7deb61890d97422a0fe141ca807f968c70ab239a,"This paper studies the stochastic subgradient descent ( SSGD ) method applied to over - parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of the empirical risk minimization problems, they prove that SSGD converges with rates O(1/ ) and O(log(1 / ), respectively, for convex and strongly - convex objectives when interpolation holds. These rates coincide with established rates for the stoenastic gradient descent ( SGD ) algorithm applied to smooth problems that also satisfy an intermediary condition. The analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nons mooth machine learning models. The authors also prove that the rate O( 1/ ) is optimal for the subgradient method in the convex   setting."
SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"This paper introduces a non - linear transformer architecture that can be trained in a "" reservoir "" setting, where some of the hidden layers are randomly initialized and never updated, but the rest of the layers are trained to converge to some fixed point in the output space. This is achieved by replacing the regular transformer layers in the reservoir layer with one or more of these “ reservoir ” layers mixed with a mixture of transformer layers, such that the output of each reservoir layer approximates the gradient of the top layer in the transformer. This way, the transformer can be approximated by back - propagating through the reservoir layers in a way that minimizes the gradients of the transformer layers ( thus avoiding the "" backward pass "" ). The authors show that this approach results in a reduction in wall - clock compute time until convergence as measured by the newly introduced AUCC metric. They also provide some empirical evidence that backward pass can be entirely skipped, in order to obtain better test set generalization.   This paper is inspired by both hybrid and non - hybrid transformer architectures. For hybrid, the authors consider a convolutional reservoir layer and a recurrent neural network reservoir layer. For non - recurrent reservoir layers, convolution layers are used, as well as a hybrid one, of course, by considering the fact that they can be combined with regular transformers. They show that their reservoir layers outperform their regular reservoir layers on various tasks."
SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"This paper proposes FILTRA ( Filmed Transformer - based Steerable CNN ), a novel and simple method to implement steerable convolution operators based on group representation theory. In particular, the authors show that kernel constructed by filter transform can be interpreted as a function in the cyclic group CN and dihedral group DN. This interpretation complete the puzzle of steerable CNN theory and provides a simple and simple way to implement a steerable operator. The authors propose to use filter transform to establish steerability between features in different group representation in cyclic groups CN and Dihedral groups DN. They verify the feasibility of FILTA for the classification and regression tasks on different datasets."
SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"This paper proposes an optimal neural program synthesis approach where the goal is to find a program that satisfies user - provided constraints while maximizing the program ’s score with respect to a neural model. Specifically, this paper focuses on multimodal program synthesis tasks in which the user intent is expressed using combination of natural language ( NL ) and input - output examples. At the core of their method is a top - down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but also leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs that satisfy the user's constraints. The experimental results on a multimodale program synthesis dataset ( STRUCTUREDREGEX ) show that their method substantially outperforms prior state - of - the - art techniques in terms of accuracy and explores fewer states during search."
SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"This paper proposes a protein convolutional neural network ( PGCN ) to predict the specificity of proteases from data and a 3D structure - based molecular interaction energy graph, which is used to solve the classification problem for substrate specificity. The Rosetta energy function was used to generate a three - dimensional structure for each protease - substrate complex, which was converted into a connected graph that encoded potential energies for each single residue and each pair of residues. Using the subgraph that includes the bound peptide and neighboring protease residues, the P GCN predicted the behavior of the interaction and reached the same accuracy as the combination of sequence and energetic features. Furthermore, variable importance analysis was performed to identify the nodes and edges that are most influential in determining protease specificity."
SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper studies the effect of the underestimation bias in Double Q - Learning, a well - known method to tackle the overestimation problem. The paper shows that under certain assumptions on the perturbation of approximation error in the model provided by Thrun - Schwartz ( 1993 ), double Q - learning may incur in having non - optimal fixed points. To address this problem, the paper proposes a doubly bounded estimator, which utilizes an abstracted dynamic programming as a lower bound estimation to rule out the potential non - optimal fixed points in the Thrun and Schwartz model. The proposed method is evaluated on several Atari games and achieves a significant improvement over baseline algorithms."
SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"The paper proposes a compute - efficient GAN model, called NOT - SO - BIG - GAN, that achieves comparable image quality to the current SoTA DGM ( BigGAN ), but consumes half as much compute ( 256 TPU - v3 cores ). The method consists of two steps : 1 ) generate a low - resolution sampler by training a wavelet - based generative model in the wavelet domain, 2 ) super - resolve the generated wavelet back to the pixel domain using a novel wavelet super - resolution decoder network. Both the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces, thus reducing the training cost. On ImageNet 512×512, the model achieves a Fréchet Inception Distance ( FID ) of 10.59 – beating the baseline BigGAN model – at half the compute ( $ 32 $ \ell_2 $ )."
SP:b943a73b1ec34867371325748dc3a91ff4011947,"This paper analyzes why pre - trained embedding networks with self - supervised training can provide comparable representations to those learned by supervised methods for few - shot classification in the setting of FSL. The authors first summarized the supervised FSL methods and explained why SSL is suitable for FSL using a simple contrastive framework. Then, the authors analyzed the main difference between supervised training and self - supervised training on FSL and obtained the bound for the gap between self - supervision loss and supervised loss. Finally, they proposed potential ways to improve the test accuracy under the setting   self - Supervised FSL."
SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"This paper studies the possibility of non - global convergence of neural networks with finite widths to local minima in the training setting of teacher - student neural networks. In particular, this paper focuses on analyzing certain mathematical properties of a class of functions that are called angular distance ( AD ) functions. Specifically, the authors show that the minimum determinant of the associated matrices are always non - positive unless perfect alignment or the problem degenerates, and that the further from being degenerated, the further the AD function is from being non - negative. The paper also provides empirical evidence that the such properties hold for the specific AD function, which in term serve as a sufficient condition for the existence of no non - trivial local minimums for general m = 3.    The theoretical contribution of this paper is the following :   1. Proving that under the most basic settings, all student neurons must align with the teacher neuron at any local minimum ( e.g., at some random initialization ). 2. Extending this result to more general cases, where the proof can be reduced to analyzing the properties of   AD function. 3. Providing numerical guarantees that these properties can be easily verified numerically."
SP:0f62846913ec10b44ed32845770da0565479dc75,"This paper proposes Deep Adaptive Semantic Logic ( DASL ), a framework for automating the generation of deep neural networks ( DNN ) that incorporates user - provided formal knowledge to improve learning from data in order to unify machine reasoning and machine learning. It improves deep learning by supplementing training data with declarative knowledge expressed in first order logic. The vocabulary of the domain is realized as a collection of neural networks, and the DNNs are trained using backpropagation to satisfy both data and knowledge. The paper provides formal semantics that demonstrate that our knowledge representation captures all of first - order logic, and that finite sampling from infinite domains converges to correct truth values. Empirical results on two computer vision problems with limited training data, demonstrating that knowledge reduces data requirements for learning deep models e.g. factor of 1000 for the learning of the MNIST toy problem and 10.7% improvement in accuracy for visual relationship detection in conditions of data scarcity."
SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"This paper studies the relationship between feedforward residual neural networks ( ResNets ) and iterative methods. It notes that there has been previous work that such networks approximate iterative recurrent computations and proposes a regularization approach to encourage the learning of iterative solutions. To do so, it firstly modifies the weight sharing across layers of the network by using soft gradient coupling to encourage more iterative convergence. Then, it imposes a Lipschitz constraint on the residual function of each ResNet layer in order to encourage it to converge to an iterative solution. Finally, it uses recurrence regularization and spectral normalization to improve the performance of the networks. The experiments show that neither of these approaches improve classification accuracy on MNIST, CIFAR-10 or CIFar-100 tasks. The results also show that iterative convergence does not provide an advantage over non - iterative computations on these tasks."
SP:6c14506b8b2b06043409d912e6bf877651aaa665,"This paper presents two methods, SelfNorm and CrossNorm, to improve out - of - distribution ( OOD ) robustness. SelfNorm uses attention to recalibrate statistics ( channel - wise mean and variance ), while CrossNorm exchanges the statistics between feature maps. Experiments on different domains ( vision and language ), tasks ( classification and segmentation ), and settings ( supervised and semi - supervised ) show their effectiveness. They are domain agnostic and can advance SOTA robustness performance.   Beyond their extensive applications, they may shed light on developing domain - agnostic methods applicable to multiple fields such as vision, language, and OOD generalization."
SP:2774abdc11917321dd4994af0f0da1ff824bea03,"This paper proposes and studies the effectiveness of augmenting a simple attention module in the convolutional encoder of an RL agent with an attention mechanism in order to improve the performance on continuous control tasks in the DeepMind Control Suite. The attention mechanism consists of two modules : ( 1 ) a Location module that tracks the agent's location and ( 2 ) an Attention module that predicts actions based on the visual inputs. The input to the Location module is encoded as a RGB image to the attention module and the output as a convolution of the two RGB images. The proposed approach is evaluated on standard benchmarks in the control suite and compared against representation learning and data augmentation methods, which are the current state - of - the - art methods in the literature. The results show that the proposed approach with the attention mechanism significantly improves the sample - efficiency and final performance of the agents."
SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"GradNorm ( GradNorm ) is a widely used gradient - based approach for training multitask neural networks. In this paper, the authors propose Rotograd, an extension to GradNorm that addresses this problem by dynamically homogenizing not only the gradient magnitudes but also their directions across tasks. For this purpose, it adds a layer of task - specific rotation matrices that aligns all the task gradients. Importantly, the proposed algorithm is analyzed through the lens of game theory, which provides theoretical guarantees on the algorithm stability and convergence. Experiments are conducted on several real - world datasets and network architectures and the proposed method outperforms previous approaches for multitask learning."
SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"This paper proposes a new constraint for unsupervised geometry - invariant image translation, called minimal geometry - distortion constraint ( MGC ) as a general I2I translation constraint to guarantee the consistency of geometry structure of source and translated images, and thus reduce translation mismatch in the translation process. In addition, it proposes an expression of mutual information called relative Squared - loss Mutual Information ( rSMI ) with an analytical method for estimation. The experimental results demonstrate that MGC achieves high quality translation to maintain the geometry of images in original domain. The quantitative and qualitative comparisons with models without MGC and state - of - the - art methods on several datasets demonstrate the superiority of the proposed MGC constraint."
SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,"This paper studies the sampling sensitivity of the discriminators in 3D point cloud GANs. The authors propose a sampling spectrum to depict the different sampling sensitivities of the different discriminators. They further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several metrics, which together form the sampling spectrum of metrics. Guided by the proposed sampling spectrum, they discover a middle - point sampling - aware discriminator, PointNet - Mix, which improves all existing point cloud generators by a large margin on sampling - related metrics. Interestingly, even the most naive fully - connected generator that is trained with Point - Net - Mix beats all the start - of - the - art generators. This discovery conveys an important message to the community : instead of focusing on the generator design, people should invest more time into the discriminator and seek for more powerful sampling aware discriminators, the authors argue."
SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"This paper studies adversarial attacks on capsule neural networks ( CapsNets ), a recently proposed probabilistic classifier - based classifier alternative to standard convolutional neural networks. This paper proposes a novel method to attack capsules directly, bypassing the multi - step attack method used in standard CNNs and a class - conditional reconstruction method ( C - CMR ) for class adversarial example detection. The proposed method is based on two observations : ( 1 ) Adversarial examples can mislead CapsNet by manipulating the votes from primary capsules, and ( 2 ) the construction of capsules is computationally expensive due to the need to route the attack samples to the output capsules. Motivated by these observations, the authors propose a vote attack where they attack the votes directly instead of routing the attacks to the capsules. Then, they integrate the proposed method in the detection - aware attack paradigm, which uses CMR for class detection. Experiments are conducted on standard and adversarial training settings to show that the proposed vote attack is more effective and efficient than the standard CNN attack. Moreover, extensive experiments demonstrate the superior attack performance of our vote attack against CMR and other attack methods for adversarial detection."
SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"This paper proposes IMPORT, a meta - reinforcement learning method that learns a task embedding by using privileged information in the form of a task description as input to a policy. The task description is used to motivate the policy to explore the environment and exploit the rewards. IMPORT also regularizes the training of the recurrent policy through parameters sharing and an auxiliary objective. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the task and exploiting them efficiently. The learnt task embeddings of IMPORT make it robust to irrelevant or minimally informative task descriptors, and able to generalize when learning on few training tasks. They evaluate IMPORT against the main approaches to online adaptation on environments that require sophisticated exploration / exploitation strategies. It is shown to learn better strategies than Thompson Sampling approaches, and faster than recurrent neural network policies and Task Inference approaches."
SP:bd89d254fbf31db61db237d08ab42981e27c52df,"This paper proposes a method to learn an RL policy from offline data in the real - world sequential recommendation system ( SRS ). Instead of increasing the fidelity of models for policy learning, the proposed method handles the distortion issue via learning to adapt to diverse simulators generated by the offline dataset. The adaptive policy is designed to handle stochasticity in the offline setting and is suitable to training in DEMER. The results show that the method overcomes the distortion problem and produces robust recommendations in the unseen real world. This is the first study on the dynamics model and the environment - context representation problem in SRS."
SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"This paper proposes a goal - reaching reinforcement learning algorithm that uses imitation learning to acquire goal reaching policies without expert demonstrations. The method relies on the intuition that any trajectory sampled from a sub - optimal agent can be turned into an optimal one using hindsight relabelling, and that imitation of these trajectories enables an agent to (iteratively ) learn goal - reachable behaviors. The proposed method, called GCSL, iteratively generates trajectories by iteratively sampling from the optimal policy and maximizes the likelihood of the actions along those trajectories under the goal that was actually reached, so as to improve the policy. It is shown that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal - reached performance and robustness over current RL algorithms in several benchmark tasks."
SP:c306530164d677e670554eeba8203c66bb3d9f7a,"This paper presents FastSpeech 2, a non - autoregressive text - to - speech ( TTS ) model that improves the training speed and quality of FastSpseech ( Ren et al., 2019 ). The main contributions of the paper are :   1 ) improve the training of the model by directly training the model with ground truth mel - spectrograms instead of the simplified output from the teacher model in FastSpech ; 2 ) introduce more variation information of speech ( e.g. pitch, energy and duration ) as conditional inputs to improve the accuracy of duration prediction in training and inference ; and 3 ) improve pitch prediction by introducing continuous wavelet transform.   In addition to that, the paper also proposes a new model, called Speech2 - based, which is the first attempt to directly generate speech waveform from text in parallel. This new model enjoys the benefit of fully end to end inference and achieves faster inference speed. The experimental results show that it can achieve a 3x training speed-up over FastSPseech and can attain comparable voice quality with comparable accuracy."
SP:79e9fb20d383816f54738ce70d137131ebc10290,"The paper proposes to reformulate unsupervised dimension reduction problem ( UDR ) in the language of tempered distributions, i.e. as a problem of approximating an empirical probability density function pemp by another tempered distribution q(x ) whose support is in a k - dimensional subspace. They introduce a nonnegative penalty function R(f ) that “forces ” the support of f to be k -dimensional, and propose an algorithm for minimization of I(f) + \�R(f, f ), based on the idea of two - step iterative computation, which is briefly described as a ) an adaptation to real data and to fake data sampled around a k-dimensional subspace found at a previous iteration, and b ) calculation of a new k - dimension subspace within a new set of generalized functions. They demonstrate the method on 4 examples ( 3 UDR and 1 SDR ) using synthetic data and standard datasets. The alternating scheme for the optimization task demonstrates both the computational efficiency and the applicability to real - world data. The algorithm performs quite stably when we vary most of the hyperparameters, though it crucially depends on two parameters, the bandwidth of the “ smoothing ” kernelM, σ, and the penalty parameter \�."
SP:93e54522e6c2b805905d21fc968fc40866f2898b,"This paper proposes Feature Contrastive Learning ( FCL ), a novel approach to balance robustness and sensitivity in deep neural network training. Unlike previous work that only enforces robustness, FCL aims to promote sensitivity to perturbations of high utility features, and inhibits sensitivity of low utility features. The performance of FCL is validated on both synthetic and real image classification datasets."
SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"This paper proposes a method for the problem of observational imitation learning, i.e. learning to imitate an expert by watching them perform a task, without access to the full set of optimal state and action examples. The method proposes a new algorithm, Disentangling Generative Adversarial Imitation Learning ( DisentanGAIL ), that makes use of adversarial learning with a latent representation inside the discriminator network, which is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert’s and the agent ’s domains. Empirically, the algorithm is shown to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of environment appearance and agent embodiment."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper provides the first theoretical analysis of learning one - hidden - layer pruned neural networks, which offers formal justification of the improved generalization of the lottery ticket hypothesis ( LTH ), which states that learning on a properly pruned network ( the winning ticket ) improves test accuracy over the original unpruned network. The authors characterizes the performance of training a pruned model by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error by showing that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, the number of samples required for achieving zero generalisation error is proportional to the   number of the non - pruned weights in the hidden layer, which provides a theoretical analysis to show that training a Pruned Network enjoys a faster convergence rate to the desired model than training the original one, providing a formal justification to support the LTH theory. The theoretical results are acquired from learning a prune neural network of one hidden layer while experimental results are provided to justify the implications in pruning multi - layer neural networks."
SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,"This paper proposes AutoLabel to automatically adapt labels for augmented data, and shows it is beneficial to both model accuracy and calibration, compared to reusing one - hot labels as in common in existing data augmentation works. The proposed AutoLabel is a generic framework that can be easily applied to AugMix, mixup, and adversarial training. The authors found that AutoLabel significantly improves the calibration of models ( and accuracy, although less dramatically ) on both clean and corrupted data for CIFAR10, CifAR100 and ImageNet. In addition, AutoLabel also helps bridge the gap between the accuracy and the adversarial robustness."
SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"This paper proposes a novel self - supervised learning objective, Representation Learning via Invariant Causal Mechanisms ( RELIC ), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer. In addition, the paper generalizes contrastive learning to RELIC and shows that learning on refinements is a sufficient condition for learning useful representations. Empirically, RELIC significantly outperforms competing methods in terms of robustness and out - of - distribution generalization on ImageNet while also significantly outperforming these methods on Atari achieving above human - level performance on 51 out of 57 games."
SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,"This paper proposes a new method for object goal navigation, namely, Visual Transformer Network ( VTNet ).   The main idea of the VTNet is to learn informative visual representation for navigation. Specifically, the visual representations are extracted from all the object instances in a scene by exploiting the relationships among them, and spatial locations of objects and image regions are emphasized so that directional navigation signals can be learned. In addition, a pre - training scheme is introduced to associate the representations with navigation signals, and thus facilitate navigation policy learning. Experiments in the artificial environment AI2 - Thor demonstrate that VTNet significantly outperforms state - of - the - art methods in unseen testing environments."
SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,"This paper considers the problem of data privacy of federated learning ( FL ), which is used to train neural network models using data distributed over multiple clients without a need to share private data. In this setting, it has been shown that adversaries may be able to extract certain information on local data from the model parameters transmitted during federatedLearning. A recent solution based on the secure aggregation primitive enables privacy - preserving federatedlearning, but at the expense of significant extra communication / computation resources. In contrast, this paper proposes a communication - computation efficient secure aggregation ( CCESA ) which reduces the amount of communication / compute resources at least by a factor of \sqrt{n/log n } relative to the existing secure aggregation solution without sacrificing data privacy, where n is the number of clients. The key idea behind the suggested scheme is to design the topology of the secret - sharing nodes ( denoted by the assignment graph G ) as sparse random graphs instead of the complete graph corresponding to the already proposed solution in this paper. The authors obtain a sufficient condition on G to guarantee reliable and private FL data privacy. Then, they suggest using the Erdős - Rényi graph as G, and provide theoretical guarantees on the reliability / privacy guarantees of the proposed scheme. Through extensive real - world experiments, the authors demonstrate that their scheme, using only 50% of the resources required in the conventional scheme, achieves the same levels of reliability and privacy leakage, while using only half as many resources as the conventional wisdom."
SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"This paper proposes a new method for learning competitive or strictly improved auctions compared to prior work. It builds upon recent results in theoretical auction design to introduce a time - independent Lagrangian objective function. The objective function is a non - stationary utility function that is easier to train. The method is based on a novel formulation of auction learning as a twoplayer game between an Auctioneer and a Misreporter and a new architecture ALGnet, which is proposed as an extension to the stationary stationary lagrangian approach of Duetting et al. ( 2019 ). It is claimed that the optimization procedure in previous work uses an inner maximization loop to compute optimal misreports but amortizes this process through the introduction of an additional neural network. The proposed amortization method is shown to lead to significantly fewer hyperparameters than the objective function in prior work and is able to compare the performance of two auctions ( RegretNet and SSIM )."
SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,"This paper proposes Bi - Tuning, a general learning approach to finetune both supervised and unsupervised pre - trained representations to downstream tasks. It generalizes the vanilla fine - tuning by integrating two heads upon the backbone : a classifier head with an improved contrastive cross - entropy loss to better leverage the label information in an instancecontrast way, and a projector head with a newly - designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category - consistent way. The proposed bi - tuning achieves state - of - the - art results for fine -tuning tasks of both supervised    and un - supervised pretrained models by large margins ( e.g. 10.7% absolute rise in accuracy on CUB )."
SP:87e5b552c13d73bd85249062a152c6c140e594a9,"This paper proposes a new metric for measuring the robustness of classifiers, called “ genuine adversarial accuracy ”. This new metric avoids the tradeoff between test accuracy and accuracy on the adversarially perturbed samples by only measuring the difference between the test accuracy on clean and perturbed data. Moreover, it does not favor a model with an invariance - based adversarial example, samples whose predicted classes are unchanged even if the perceptual classes are changed. The authors prove that a single nearest neighbor ( 1 - N ) classifier is the most robust classifier according to the new metric when the class for each data point is unique."
SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"This paper studies the problem of dyadic fairness in learning neural networks using graph neural networks. Dyadic fairness is defined as the notion that a predictive relationship between two instances should be independent of the sensitive attributes. The authors first theoretically analyzed how the connections in graph links affect dyadic fair of demographic parity when employing graph neural network for representation learning and then proposed a method, FairAdj, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation on six real - world social and citation networks to demonstrate the effectiveness of the proposed method. They conduct evaluations towards seven measurements of both utility and dyadic unfairness. Comparisons to other baseline methods ( Kipf & Welling, 2016b, Grover & Leskovec, 2016 ; Rahman et al., 2019 ; Bose & Hamilton, 2019a, 2019b ) are provided. The results show that fairAdj achieves comparable performance as those obtained by baselines while enjoying a better fairness - utility tradeoff."
SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"This paper proposes a disentangled exploration autoencoder ( DEAE ) that aims to improve generative power of VAE - based autoencoders without GAN - based adversarial training. To achieve controllable latent representation, DEAE first disentangles the input image representation by disentangling it with disanganglement, and then uses it to explore the latent space through directed interpolation. To further improve the quality of the exploration, the decoder uses a regularizer to ensure that the disentanglement of the disangled representation is tight. Experiments demonstrate that DEAE can improve the performance of downstream tasks by synthesizing attribute - controLLABLE augmented samples.   "
SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"This paper proposes a method for improving the memory architecture of the Kanerva Machine by differentiating episodic and semantic blocks in a hierarchical latent variable model. Inspired by the traditional ( computer - science ) memory model of heap allocation, the authors propose a novel differentiable memory allocation scheme called Kanerva ++ ( K++ ) that learns to compress an episode of samples, referred to as pointers, into a latent multi - dimensional memory. The K++ model infers a key distribution as a proxy to the pointers and is able to embed similar samples to an overlapping latent representation space, thus enabling it to be more efficient on compressing input distributions. The authors demonstrate that this allocation scheme improves performance on memory conditional image generation tasks and achieves new state - of - the - art conditional likelihood values on binarized MNIST."
SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,"This paper studies the sample complexity and loss landscape of attention - based neural networks. Theoretical analysis shows that every local minimum of the attention model has low prediction error, and attention models require lower sample complexity than models without attention. Besides theoretical analysis, this paper also provides guidelines for designing future attention models. Experiments on various datasets validate theoretical findings."
SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,"This paper proposes a new view of active inference ( aka Bayesian active inference ) from the perspective of reinforcement learning ( RL ). Inspired by the expected free energy ( EFE ), which is a core quantity in active inference, the authors claim that EFE can be treated as a negative value function in an RL perspective. Motivated by this connection, they propose a novel inverse RL algorithm for designing EFE - based rewards, by learning a prior preference from expert demonstrations. This illustrates that the problem with inverse RL can be approached with a new perspective from that of active Inference. Experimental results of prior preference learning show the possibility of using active inference with EFE-based rewards and its application to an inverse RL problem."
SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"This paper proposes a method, out - of - distribution ( OOD ) data augmented training ( OAT ), to leverage OOD data for adversarial and standard learning, and theoretical analyses demonstrate how the proposed method can improve robust and standard generalization. It is shown that undesirable features are shared among diverse OOD image datasets, and it is also demonstrated that OAT can extend training distribution by comparison with other data augmentation methods, which can be employed in the absence of UID data. Experiments are performed on various OOD datasets, which surprisingly demonstrate that even datasets that seem to have little correlation with the target dataset from the human perspective can help standard and robust generalization through the proposed OAT method. The proposed method further improves the existing state of the art adversarial training method, which is found to further improve by incorporating the OAT data."
SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"This paper introduces Fast Linearized Adaptive Policy ( FLAP ), a meta - RL method that learns a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights instead of taking gradient steps. A separate adapter network is trained simultaneously with the policy such that during adaptation, FLAP can directly use the adapter network to predict these linear weights. Experiments on standard continuous - control metaRL benchmarks show FLAP presenting significantly stronger performance on out - of - distribution tasks with up to double the average return and up to 8X faster adaptation run - time speeds compared to prior methods.   The contribution in the FLAP algorithm presents a completely new view into meta - reinforcement learning by applying linear function approximations and predicting the adaptation weights directly instead of take gradient steps or using context encoders. To my knowledge, this is the first meta -RL method that directly learns and predicts a ( linear - linear ) structure successfully in adapting to new tasks."
SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"This paper proposes a federated kernel k - means ( FK - means ) algorithm. This algorithm is based on a distributed stochastic proximal gradient descent ( DSPGD ) method. In order to be more efficient, a communication efficient mech anism ( CEM ) is introduced to reduce the communication cost. Besides, the authors propose to federate kernelk - means in order to provide two levels of privacy protection : ( 1 ) users ’ local data are not exposed to the cloud server and ( 2 ) cloud server can not recover users'local data from the local computational results via matrix operations. Theoretical analysis shows that DSPgd with CEM converges to an O(1 / T ) rate, where T is the number of iterations. The authors further show that the communication costs of FK-means is linear in the dimension of the right singular vector times the number   of users, which can be much smaller than the   number of data samples. Experiments are conducted on MNIST, CIFAR-10, and Fashion MNIST. The experimental results show that FKA - means achieves the highest clustering quality with the reduced communication cost compared to the state - of - the - art."
SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"The paper proposes CompOFA, a reduction in the search space of OFA networks based on the idea of compound coupling between model dimensions. The idea is to reduce search to models close to the accuracy - latency Pareto frontier, i.e. those within $ L_0 $ range of $ \ell_2 $, by introducing a heuristic that selects dimensions that are close to each other in terms of accuracy and distance to the deployment target. The heuristic is motivated by the notion of “ once - for - all ”, an OFA network that trains several models at once on a single GPU but whose training time and cost are exponential inversely proportional to the number of models trained. The paper proposes to use this heuristic to encourage the training of models in a search space that is $ L_{i,j}$ where $ i$ is the size of the input space and $ j$ the dimensions of the output dimensions. By using this search space to select models that are within $ l_1 $ of each other ’s deployment targets, the paper shows that one can achieve a 216x speedup in model search / extraction time compared to the state of the art, without losing on the optimality achieved by OFA."
SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,"This paper studies the general meta - learning with adversarial samples. In particular, this paper proposes a meta - learner, called ADML ( ADversarial Meta - Learner ), which leverages clean and adversarial data to optimize the initialization of a learning model in an adversarial manner. The proposed algorithm has been evaluated on two datasets, MiniImageNet and CIFAR100, and compared with MAML, AD - AML, and several other meta - learners. The experimental results justify the effectiveness and superiority of ADML in terms of both accuracy and robustness."
SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,"This paper proposes a self - attention mechanism to improve decoding of linear error correction codes by leveraging the power of permutation groups and DL to enhance the decoding capabilities of constrained model - based decoders. The proposed method firstly embeds all the permutations of a given code in a group by extracting relevant features, which is done once before the test phase during a pre - process. Then, a trained NN is used to embed the embeddings into a permutation embedding space, such that the extracted features can be used to predict the probability for successful decoding of each permutation. Permutations are further divided into groups based on their similarity with respect to the input, and the best of these groups is then used to decode the input. Finally, a set of either one, five or ten most probable to decode permutations is chosen, and decoding is carried out on the permuted channel words, rather than decoding an arbitrary dataset with all permutations, and empirically choosing the best subset of them. The performance of the proposed method is evaluated on the Bose - Chaudhuri - Hocquenghem ( BCH ) code, and it is shown to improve bit error rate."
SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"The paper proposes an interesting idea to perform an unsupervised clustering task before fine - tuning BERT for an intermediate text classification task. The idea is to partition the unlabeled training data into relatively homogeneous clusters of text instances, and train BERT on these clusters to learn a label for each instance. This is similar to how clustering is used to obtain labels for training deep networks in computer vision. However, instead of training a classification model, BERT is trained to predict the cluster labels. The paper provides extensive experimental results to demonstrate the practical value of this strategy on a variety of benchmark data, most prominently when the training data available for the target task is relatively small and the classification task is of topical nature. The results show that this additional classification step can significantly reduce the demand for labeled examples mainly for topical classification tasks."
SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"This paper studies the generative model - based RL using a random control agent in the form of Gaussian mixture model. The control is assumed to be Gaussian, and the goal of the paper is to find a model that learns to predict the mean and covariance of inputs in a mult multimodal environment. The authors compare 5 models : 1 ) deterministic mean prediction, 2 ) mixture model, 3 ) heteroscedasticity at training time, 4 ) stochastic mean prediction. They find that the mixture model performs better than the deterministic models when multimodality is not required, and 5 ) hetero - mean model performs marginally worse than probabilistic models. The main contributions of this paper are :   ( a ) This paper contributes to the fine - grained evaluation of model performance in the context of MBRL by using a fixed ( random - shooting ) control ; ( b ) this paper provides a reproducible and extensible benchmark to reproduce the results of ( Nagabandi et al., 2018 ). The reproducible benchmark is made publicly available at : https://www.cocosci.princeton.edu/~nagabandi et al. ; ( c ) The paper also contributes an experimental protocol to validate the performance of the models by measuring various metrics on a static and dynamic dataset."
SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"This paper proposes a GAN model named Affine Disentangled GAN ( ADIS - GAN ), which is inspired by InfoGAN, where an affine regularizer acts as the inductive bias. The idea is to explicitly learn the affine transformations via disentangled representations, which are then used to generate adversarial samples to disentangle the transformations. The affine transformation properties of the underlying images are taken as input, and transformations such as rotation, zoom, skew, horizontal and vertical skew, and translation are explicitly selected and learned in an explicitly inductive manner. The paper successfully disentangles these features on the MNIST, CelebA, and dSprites datasets."
SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"This paper proposes CLSA, a supervised method that leverages the information learned from strongly augmented images to improve the performance of contrastive learning. The main idea is to supervise the retrieval of strongly augmented queries from a pool of candidates by minimizing the distributional divergence between the representation bank and the retrieval pool. This avoids an overoptimistic assumption that could overfit queries containing distorted visual structures into the positive targets, while still being able to distinguish them from the negative samples by leveraging the distributions of weakly augmented counterparts. The proposed method achieves top - 1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single - layer classifier fine - tuned. This is almost as high as 76.5% of top-1 accuracy with a fully supervised ResNet - 50. Meanwhile, it also outperforms the previous self - supervised and supervised methods on both the transfer learning and object detection tasks."
SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"The paper proposes C - DeID - GAN, a deep learning framework for the de - identification of magnetic resonance images ( MRI ) based on GANs. The core idea is to train a GAN that generates a 3D volume of a subject's MRI scan, where the 3D model of the brain is preserved but the face of the subject is de - identified. The proposed method is different from the typical approach of removing parts of the scan that are deemed'medically relevant'( such as brain regions ), as this work does not aim to remove certain regions, but rather aims to remodel privacy - relevant information while keeping medically relevant information untouched ( i.e., the brain in this work is not included in the generated 3D MRI images ).   The proposed model is trained using a multi - scale, multi - headed neural network ( MNIST-50 GAN ) that takes a patient's scan as input and outputs 3D voxels, where each voxel corresponds to an individual layer of the proposed model. Each layer is trained with a max - pool of depth information, such that each layer is deep enough to cover an entire dimension of the previous layer, but shallow enough not to cover any significant portion of the original image. The layers are trained in a way that ensures that they cover the entire 3D image space ( not just any part of it ), and that they do not cover any particular region of the image ( e.g., regions that are not relevant to the patient ). The model output is trained to be a convex hull, which is then fed into the GAN to produce the final 3D images. The GAN - generated voxen is trained on all of the layers, and the output is fed into a Gaussian autoencoder, which generates the final brain volume. In the experiments, the paper demonstrates that the proposed method significantly improves upon the performance of some common MRI classification tools over the baselines that are commonly used in medical imaging research, such as MRIs and CT scans."
SP:0ac3964bd2320341488476d60f57b75d2a79f92c,"This paper proposes a multi - headed attention based global pooling layer for hierarchical graph pooling, called Graph Multiset Transformer (GMT ), to capture the interaction between nodes according to their structural dependencies. The authors claim that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler - Lehman graph isomorphism test. Experiments are conducted on 10 graph classification datasets, and the proposed method outperforms state - of - the - art pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains larger performance gain on graph reconstruction and generation tasks. They strongly believe that the proposed pooling method will bring substantial practical impact, and it is generally applicable to many graph - related tasks."
SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"The paper proposes a new explanation for the long - range communication problem of graph neural networks ( GNN ). The authors claim that there is a bottleneck that causes the message - aggregating process of GNN to get stuck due to an exponentially growing amount of information squashed into a fixed - length vector, which prevents the model from propagating long range information. The paper tests the theory on GCN, GIN, GGNN, GAT, and GGNN and shows that they are all susceptible to the same bottleneck. They also show that prior work that extensively tuned GNNs to real - world datasets suffer from over - squashing and that breaking the bottleneck improves their state - of - the - art results without any tuning or additional weights."
SP:90d8fa381446923902e42b259392e5e975e6caa1,"This paper proposes a new method for cross - domain sentiment analysis ( CSA ) based on learning a prototypical distribution for the source and the target, which is trained to be domain - agnostic. The intuition is that by having large margins between classes in the source domain, the effect of “ domain shift ” on the performance of a trained classifier in the target domain will be reduced. The proposed method is based on a Gaussian mixture modal ( GMM ) distribution that is learned for both source and target. The authors provide a theoretical proof to demonstrate that their method minimizes an upperbound for the target domains expected error. Empirical results demonstrate that the proposed method outperforms state - of - the - art sentiment analysis algorithms."
SP:893fd7440b82f5da0d4c0944928810322eaee2f0,"This paper proposes a new evaluation methodology to measure the gender bias in the natural language understanding ( NLI ) pretrained on MNLI and SNLI data - sets by using a "" gender - neutral "" premise paired with a gender - specific hypothesis. Three models are considered : BERT, RoBERTa and BART. Experiments show that BERT has the most gender - induced bias ( out of the three ), followed by RoBER Ta and BART in terms of accuracy. The distribution of bias seems to follow the gender - distribution in the real world hence adhering to the social stereotypes. The paper also shows that augmenting the training dataset with a more gender - balanced dataset can reduce the bias for BERT."
SP:a32ab755bd249c393b70938036ce8e810c0c439f,"This paper revisits the variational intrinsic control ( VIC ) algorithm by Gregor et al. ( 2016 ). In it, two VIC algorithms were proposed : one that represents the options explicitly and the other that does it implicitly. The main contribution of this paper is showing that the implicit reward in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To compensate this bias and achieve the maximal empowerment, the authors suggest two modifications of implicit VIC : the environment dynamics modeling with the transitional probability ( section 3 ) and Gaussian mixture model ( section 4 )."
SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,"This paper studies the use of neural ensembles in the small data domain through an extensive validation using popular datasets and architectures. They show that deep ensembling is a simple yet effective technique that outperforms current state - of - the - art approaches for learning from small datasets. They compare different ensemble configurations to their deeper and wider competitors given a total fixed computational budget and provide empirical evidence of their advantage. Furthermore, they investigate the effectiveness of different losses and show that their choice should be made considering different factors."
SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,"This paper proposes Sparse Binary Neural Network ( SBNN ), a method to further compress Binary Neural Networks ( BNNs ) by introducing sparsity, while reducing their required computations. Their approach is based on quantization of weights in the 0/1 binary domain and a highly sparse initialization. It is formulated as a mixed optimization problem and solved using a modified version of the BNN training algorithm with -1/+1 weights. The method has been evaluated on feed - forward linear and convolutional network on MNIST and CIFAR-10 data sets, respectively. The achievable compression rate of SBNN is much higher than simple BNNes, making it a feasible alternative for IoT devices and sensors. However, sparsity and speed - up come at the cost of reduced performance accuracy."
SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"The paper proposes a new method for post hoc calibration of deep neural network models on corrupted out - of - distribution ( OOD ) datasets. The key idea is to replace the softmax ( i.e., the output of the model ) as a surrogate for class membership probability with the current model's prediction as a measure of uncertainty. The method relies on the Brier score ( Brier et al., 1983 ) to compute the expected calibration error ( ECE ). The paper also proposes a simple extra calibration step to detect the level of corruption in the dataset and adjust the model parameters accordingly. The proposed approach is model - agnostic and can be used for training future OOD models."
SP:ea503f67e38fce7dee9cc4996b55b8959911f030,"This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. The authors build a dataset that contains instances of different families of graphs. Then, they compare the graph representations and similarities produced by GNNs and graph.graph kernels against those generated by an intractable graph similarity function which they consider to be an oracle function that outputs the true similarity between graphs. They perform a large number of experiments where they compare several different kernels, architectures, and pooling functions. They also investigate the impact of node attributes on the performance of the different models and kernels. Their results reveal interesting findings, for instance, that theoretically more powerful models do not necessarily yield higher - quality representations, while graph kernels are shown to be very competitive with graph Neural networks."
SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"This paper proposes a new method for improving self - supervised image animation by prepping the source and the vehicle images for inpainting when the pose of the source image is different from the driving image due to the large motion of the vehicle. The proposed method, named PriorityCut, is based on CutMix ( Yun et al., 2019 ), a data augmentation that cuts and mixes patches of different images to regularize discriminator predictions. Prioritised cut masks are applied to the foreground and background of each of the two source images to pre - paint the top - k percent of the foreground image for consistency regularization. The method is evaluated on three datasets ( VoxCeleb, BAIR, and Tai - Chi - HD ) and compared with several SOTA image animation methods.   The main contributions of the paper are :   1 ) Prioritization of the occlusion information in image animation indicating the locations of warping artifacts   2 ) Improvement in pixel - wise similarity between the generated source and foreground images through pre - training and fine - tuning 3 ) Proposed method outperforms SOTA methods in terms of pixel wise difference, low - level similarity, and keypoint distance"
SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"This paper presents a method for learning disentangled representations of independent causal mechanisms ( ICM ). An ICM is a model that directly models multiple data generation processes ( mechanisms ) in a coarse - granularity, such that it can be learned to approximate the ground truth mechanisms from observational data. The paper first learns a generative model, i.e., a neural network, to learn two types of mechanisms : generative mechanisms ( G ) and shared mechanisms ( CMs ). The shared mechanism is learned by conditioning on the shared latent variable through an unlabeled prior ; the generative mechanism learns itself through a mixture prior.   The authors then show that the disentanglement of these two mechanisms allows the learned model to learn the causal mechanisms. The authors also provide sufficient conditions under which the shared mechanism can be identified through a self - supervised learning approach. Finally, the authors show the equivalence between the learned mechanisms and the ones learned by an ICM. The main contributions of the paper are :   ( 1 ) Identification of the necessary conditions for learning the shared and generative models. ( 2 ) Comparison of the learned representations to disentangle representations on various downstream tasks."
SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"This paper considers the problem of learning f : U : U, V, W, where f factors into U → V → W, and aims to do so for graph aligning tasks such as chemical graph recognition ( CP ) and molecular graph structure prediction ( MG ). To achieve this task, the authors propose a new way of labeling 2D molecular graphs : they use the so - called “ fully mediating layer ”, which is an embedding of the graph structure into a higher dimensional space, and they learn a mapping from U to V in order to get a label for each graph in the graph space. However, observing V requires expensive labels which may not be available in the source domain, so they propose a graph alignment approach to generate more detailed labels given normal labels W. The idea is to allow the model to generalize better to a new domain ( e.g. graph alignment ) without the need of expensive labels in the target domain. The authors then apply the proposed rich labeling approach to two datasets : 1 ) the Indigo dataset and 2 ) the Maybridge dataset. The results show that the proposed method is able to adapt the proposed label learning strategy to the new domain of graph recognition and achieve better performance than the baselines on both of these datasets. In particular, the performance on the graph recognition domain outperforms the baseline on Indigo by a large margin ( almost 4x ) on a set of only 4000 data points. On the other hand, on the CMSE dataset, the proposed self - labeling method achieves higher performance ( 39 % ) than the baseline ( 32 % )."
SP:ad906dd9a176cffd283593321ff6b9ad19595528,"This paper proposes a monotonic neural network ( MNN ) to solve the chiller plants energy optimization problems. The motivation of this paper is that the energy consumption estimation of most chillers can be physically viewed as an input - output monotonically constrained problem, i.e., energy consumption can be measured as a function of temperature, CO2, and other parameters. To solve this problem, the authors propose to use a neural network with monotonicity constraints to mimic the physical behavior of the system. In particular, the MNN constrain the output - output function to be a mixture of Radon transform and Radon amplitude, and the temperature and CO2 bound to be inversely proportional to one another. The authors verify the proposed method in a cooling system of a data center, and experimental results show the superiority of their framework in energy optimization compared to the existing methods."
SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,"This paper proposes a multi - headed temporal - spatio - temporal fusion transformer, called CausalTrans, to address the problem of multi - target forecasting for supply and demand forecasting in large - scale two - sided markets. Based on causal inference theory, the model consists of three components : ( 1 ) CATE, ( 2 ) Fast multi - head attention derived from Taylor ’s expansion, and ( 3 ) spatial graph fusion mechanism. Experiments are conducted to demonstrate the interpretability of the model, the effectiveness of the various model components, and the time efficiency of the proposed model. In particular, for the multivariate dataset, the proposed Transformer achieves similar performance as TFT on the univariate group and outperforms all competing methods including TFT. For the multivariances, the approach achieves up to 15 % error reduction."
SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"The paper proposes a new unsupervised joint learning framework, coupled mixture VAE ( cpl - mixVAE ), which consists of multiple pairwise - coupled autoencoding agents learning a shared categorical variable, while independently learning a mixture of discrete and continuous factors of variability. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. The authors formulate the collective decision making as a variational inference problem, where an approximation of Aitchison distance is used to compare categorical assignment of the agents, which avoids mode collapse. They benchmark the proposed method and display its superiority over comparable approaches using the MNIST and dSprites datasets. Finally, they apply the method to a challenging single cell gene expression dataset for a population of neurons, and demonstrate the utility of this approach in jointly identifying cell types and type - specific, activity - regulated genes."
SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"This paper studies the characterization of the kernel space of G - steerable kernels in the framework of group equivariant convolutional networks ( GCNN ). In this setting, the authors provide a characterization for the practically relevant case of compact ( point - symmetry groups ) G and homogeneous spaces X under the assumption that G is any compact group. The authors make use of the Wigner - Eckart theorem and the Peter - Weyl Theorem to derive a new characterization that characterizes the three ingredients that make up a "" kernel space "" in terms of generalized reduced matrix elements, Clebsch - Gunnars, and harmonic basis functions. The latter two forms the basis of the endomorphism bases that are used to define the kernel kernels in GCNNs. The analysis is motivated by drawing analogies between the constraints of the G - steerable kernels and those of the spherical tensor operators in quantum mechanics, and is motivated to provide a more general characterization for G - symmetries of homogeneous groups under compact settings."
SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"This paper analyzes the effect of selective classification, in which models abstain from making predictions because of uncertainty, on the accuracy of some groups in vision and NLP datasets. It shows that while selective classification can improve average accuracy, it can simultaneously magnify existing accuracy disparities between groups within a population, especially in the presence of spurious correlations. The margin distribution, which captures the model ’s confidences over all predictions, is studied. The authors prove that whether selective classification improves or worsens accuracy is fully determined by the accuracy at full coverage ( i.e., without any abstentions ) and whether the margin distribution satisfies a property called “ left - log - concavity ”. The analysis also shows that selective classification tends to magnify full - coverage accuracy disparities. Motivated by the analysis, the authors train distributionally robust models that achieve similar full coverage accuracies across groups to leverage selective classification to improve accuracy. Then selective classification uniformly improves each group on these models."
SP:f1d57ee27e901daf7e4e2b84139019e945818911,"This paper proposes a new nonnegative CANDECOMP / PARAFAC ( CP ) decomposition for hierarchical topic modeling of multi - modal tensors, called hierarchical NCPD. It also proposes a training method, called Neural NCPD, to train this decomposition, which uses a neural network architecture and backpropagation to mitigate error propagation. The proposed method is evaluated on real and synthetic data, and some brief conclusions are provided."
SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"This paper proposes a new adversarial robustness certificate based on graph neural networks ( GNNs ) for tasks like node classification, image segmentation, and named - entity recognition. It considers a setting where there is a shared set of labels ( labels based on a single graph, image, or document ), but each classifier outputs multiple predictions ( labels of a vector of labels ) based on the single input : i.e., they assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. The key idea is to fuse multiple single - node certificates into a drastically stronger collective certificate, which is called the collective certificate. It is based on Graph Neural Networks, whose locality guarantees that perturbations to the input graph only affect predictions in a close neighborhood. The authors evaluate the proposed approach on multiple semi - supervised node classification datasets with different classifier architectures and base certificates. Their empirical results show that the proposed collective approach yields much stronger certificates than existing methods, which assume that    each adversary can attack predictions independently with different graphs."
SP:cc93dd2f68e415e2457166e78627865dc1b44697,"This paper proposes Quantile Regression GAN ( QRGAN ), a modification of least squares GAN and wasserstein GAN that aim to solve the mode collapse problem of GAN by minimizing 1 - Wasserstein distance between the output distribution of discriminator and generator. The key idea of QRGAN is to use a Hessian - based loss function instead of Hessian as in LSGAN and WGAN to achieve this goal. The main difference between QRGAN and LSGAN is that QRGAN uses Hessian in the discriminator to minimize the Hessian rather than the one used in WGAN. Experiments are conducted on a mixture of gussian dataset and image generated by CIFAR-10. QRGAN outperforms LSGAN, WGAN, and several other variants in mode collapse and non - convergence analysis."
SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,This paper investigates relevance metrics that can provide reasonable explanations for similarity - based explanation of machine learning models. The authors propose to use three tests to evaluate various relevance metrics in terms of their appropriateness : 1 ) cosine similarity of the gradients of the loss. 2 ) identical class and identical subclass test. 3 ) model randomization test to evaluate whether the metrics are model - dependent.   Experiments on ImageNet and CIFAR-10 show that cosine similarities of gradients is the most relevant metric to use for instance - based similarity explanation. Some metrics perform poorly in the tests and the authors analyze the reasons behind their failure. The main takeaway from the paper is that some metrics are not model - specific and thus should not be used for similarity-based explanation.
SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"The paper proposes Low - rank global attention ( LRGA ), a variant of dot - product attention applied to GNNs, to improve the generalization power of the Graph Neural Networks ( GNN ). Specifically, the authors show that adding the LRGA module provides algorithmic alignment to a powerful graph isomorphism test, namely the 2 - FWL algorithm. To theoretically prove this, they consider the Random Graph Neural Network ( RGNN ) framework and prove that it is universal in probability. Then they show that a GNN trained with LRGA aligns with a specific family of polynomial kernels, and bound the sample complexity of the kernel ’s feature map when learned with a randomly initialized two - layer MLP. Empirical results show that LRGA improves state - of - the - art results in current GNN layers with the SOTA and SOTA - GA performance on several benchmark datasets."
SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"This paper proposes an adaptive label smoothing method to improve the calibration and performance of convolutional neural networks ( CNNs ). The key idea is to use the bounding box information pertaining to objects to compute a smoothing factor adaptively during the training of the CNNs. To do this, the authors use bounding boxes from the ImageNet - COCO dataset ( 1 K samples ) to train different classifiers. The main idea of the smoothing is to compute an adaptive factor that is based on the relative object size within an image. Experiments are conducted on ImageNet-1K, MS-COCO, and CIFAR-10 to demonstrate that the proposed method outperforms existing methods and has a lower confidence."
SP:5254658923e594294b69d124a8d004166852822a,"This paper proposes a two - layer fully - convolutional ReLU denoising network that is amenable to convex optimization. In particular, a convex dual program is formulated that offers optimal training using convex solvers, and gains interpretability by learning a dual network that coincides with the non - convex neural network, and interprets the learned dual network. Experiments on the MNIST and fastMRI datasets confirm the efficacy of the dual network optimization problem. The utility of the convex formulation for deeper networks is also discussed using greedy unrolling."
SP:085cad6bc143c8713580bddfaa71f06496dac314,"The paper presents a method for learning to synthesise speech from text or phonemes using an end - to - end ( feed - forward ) method. The method uses a differentiable alignment scheme based on token - length prediction to learn a sequence of character encoders for each phoneme. Each character encoder then takes a phoneme encoder as input, while the decoder converts the phoneme sequence into the speech audio. The decoder output is concatenated with the corresponding duration and mel spectrogram. The model is trained using adversarial feedback and prediction loss. The prediction loss uses soft dynamic time warping to capture temporal variation in the generated audio.   The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state - of - the - art models relying on multi - stage training and supervision."
SP:01148cea55db606aa78d27e900818684a8bce9ab,"This paper proposes a non - parametric method for learning node representations with incomplete attribute information. The proposed method embeds nodes into a discrete discrete Wasserstein space through matrix decomposition, which is then diffused along the graph using graph diffusion. This allows the method to produce a representation capable of representing both structure and attributes. The method is then adapted for two applications : node classification and matrix completion. Experiments are conducted on both missing node and attribute information to show the representation capacity of the proposed method."
SP:aeeb5909f7123ef631f569b469af9715205c881f,"This paper proposes an approach for reinforcement learning ( RL ) in environments with sparse extrinsic rewards. The goal is to generate a curriculum of goals that can be used as an intrinsic reward to motivate the agent to explore the environment. This is achieved by having a goal generator as a teacher, and a policy that acts as a student, to maximize goals that are generated by the teacher. In particular, the teacher is rewarded via a simple but effective “ constructive adversarial ” objective. The teacher is also incentivized to generate increasingly difficult goals. The proposed approach, AMIGO, is evaluated on 6 procedurally generated tasks. It is shown to outperform state - of - the - art intrinsic motivation methods in a series of 114 experiments across 6 tasks."
SP:3d05bc7dca97681cb582298e318b9b973841eed3,"The paper considers the problem of information retrieval from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. Specifically, a user requesting a file from the dataset should be able to reconstruct the requested file with a prescribed distortion, and in addition, the identity of that file should be kept private from the server. The proposed model can be seen as an extension to the well - known concept of private information retrieval by allowing for distortion in the retrieval process and relaxing the perfect privacy requirement. The authors study the study of the tradeoff between download rate, distortion and user privacy leakage, and show that the optimal rate - distortion - leakage tradeoff is convex. Moreover, the authors propose a new data - driven framework by leveraging recent advancements in generative adversarial models which allows a user to learn efficient schemes in terms of download rate from the data itself."
SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"This paper proposes a method for parallel training graph neural networks ( GNNs ) based on decoupling the GNN into smaller modules and associates each module with a greedy auxiliary task. The proposed decoupled greedy learning algorithm is called DGL - GNN and it is claimed that it achieves update - unlocking by introducing greedy auxiliary objectives during training, and enables parallelization by decouples the   GNN layer to be trained in parallel. Next, the paper proposes to leverage a lazy - update scheme to improve the training efficiency of the proposed method. Empirically, the method is compared with other sampling - based methods and a lazy update scheme, and shows its effectiveness and efficiency through a range of experiments."
SP:5ecb1b288f7fc02aead4493f81640867bc349290,"This paper proposes a method for efficiently answering complex queries on incomplete Knowledge Graphs. The core idea is to translate each query into an end - to - end differentiable objective, where the truth value of each atom is computed by a pre - trained neural link predictor. The authors then analyse two solutions to the optimisation problem, including gradient - based and combinatorial search. The proposed method is called Complex Query Decomposition ( CQD ) for answering Existential Positive First - Order logical queries by reasoning over sets of entities in embedding space. In particular, answering a complex query is reduced to answering each of its sub - queries, and aggregating the resulting scores via t - norms.   Experimental results show that the proposed approach produces more accurate results than state - of - the - art methods — black - box neural models trained on millions of generated queries — without the need of training on a large set of complex queries."
SP:f04a522fd04c503754fdb8c52da68646d31271a4,"This paper proposes a method for checking the local robustness of piecewise linear activation functions ( SLF ) in feed - forward neural networks ( FNNs ). The proposed method is based on the following : 1. Assume that the input space of the FNN is partitioned into a set of convex polyhedral regions ( called decision boundaries ), and assume that the network ’s behavior is linear. 2. Consider a point in each of these decision boundaries and draw a line through it. 3. Then compute the distance between any two points in the decision boundaries of each of those decision boundaries. 4. Measure the distance in each region using a projection from the point to the next point. 5. Count the distances in the region using the projection as a measure of the distance to each other.   The method is evaluated on both the $ \ell_2$ norm of the set of decision boundaries, and compared with previous methods ( Jordan et al., 2019 ; Tjeng & Tedrake, 2017 ; Weng et a.k.a., 2018 ). It is shown that the proposed method generally outperforms the previous methods in terms of lower - bound accuracy, robustness certifiability, and even generalization to larger networks. Further experiments are also conducted to show that the method scales better to deeper networks than previous methods."
SP:5297651ff873f97c07b9c47ed3eff52251661844,"This paper proposes an embedding of objects in an affordance space, in which each dimension corresponds to an aspect of meaning shared by many actions, using text corpora. This embedding makes it possible to predict which verbs will be applicable to a given object, as captured in human judgments of object affordance. The authors also show that the dimensions learned are interpretable, and that they correspond to typical patterns of interaction with objects. Finally, the dimensions can be used to predict a state - of - the - art mental representation of objects, derived purely from human judgements of object similarity.   The main contribution of this paper is to develop and evaluate an approach for creating an analogous affordance embedding space for objects, where each dimension groups together actions corresponding to a particular “ mode of interaction ”affordance mining. The second goal is to understand the degree to which affordance knowledge underlies the mental representation, as instantiated in SPoSE."
SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"This paper proposes a method for the emergence of individuality in multi - agent reinforcement learning ( MARL ). The proposed EOI learns a probabilistic classifier that predicts a probability distribution over agents given their observation, and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, and learning the classifiers by such observations makes the intrinsic reward signals stronger and in turn makes the agents more identifiable. Two regularizers are proposed to increase the discriminability of the classifer. The authors implement the proposed method on top of two popular MARL algorithms : centralized training and decentralized execution ( CTDE ) and QMIX ( Rashid et al., 2018 ). Empirically, the authors demonstrate that EOWI outperforms existing methods in a variety of multi - agents cooperative scenarios."
SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"This paper proposes a Smoothed Weighted Ensembling ( SWEEN ) method to improve the performance of randomized smoothed classifiers. The authors show the ensembling generality that SWEen can help achieve optimal certified robustness. They also provide an adaptive prediction algorithm to reduce the prediction and certification cost. Extensive experiments are conducted on image classification tasks such as MNIST, CIFAR-10, Tiny ImageNet, MNIST-50, and Fashion MNIST. On all these tasks, the proposed method consistently outperforms the upper envelopes of their respective candidate models in terms of the approximated certified accuracy by a large margin. In addition, the authors show comparable or superior performance to a large individual model using a few candidates with a notable reduction in total training time."
SP:ea892e3d199ed6121279b20061a87f43afae8796,"This paper proposes a method to recover the subtask boundaries in an unstructured demonstration by learning from the actions. The method proposes to use an ordered memory policy network ( OMPN ) that consists of two components : a memory - update rule and a memory expansion rule. The update rule updates the memory at each level based on a two - stage process : a first update at the lowest level and then at the highest level, using a differentiable stick - breaking process. The memory at the upper level is updated based on the current state of the sub - tasks and the updated memory is then transferred to the lower level. The policy network then decides the expansion position from which to expand at each time step. The lower level sub - task memory is used to retrieve actions from the upper - level memory and vice versa. The model is trained in an end - to - end fashion using behavior cloning. The proposed method is shown to outperform the state - of - the - art methods on both grid - world and grid - based navigation tasks in both unsupervised and weakly supervised settings."
SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"This paper proposes a Causal Semantic Generative model for out - of - distribution ( OOD ) classification based on a variational Bayes - based method. The proposed model is designed for single - domain OOD prediction from a single training set, which is common and challenging. It is based on the Causal Variational Generative Model ( CGM ), which learns to separate the semantic ( label ) and variation ( distribution ) factors, and uses them as independent variables in the neural network representation. The variational variational model is applied to both learning and prediction tasks. Theoretically, the authors prove that under certain conditions, the CGM can identify the semantic factor by fitting training data, and prove the boundedness of OOD generalization error. Empirical study shows improved OOD performance over prevailing baselines."
SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"This paper studies the problem of robust algorithms for learning in adversarially corrupted settings, where an online algorithm receives a stochastic reward from an adversary that can be arbitrarily corrupted. The paper proposes to design robust algorithms with small regret over a period of time steps, while the algorithm observes corrupted rewards, such that its regret is at least as large as the true uncorrupted reward distribution. This work builds upon recent advances in robust estimation for unsupervised learning problems, such as robust UCRL2 and robust exploration, to design algorithms with near optimal regret in three different settings : Stochastic multi - armed bandits, linear contextual bandits, and Markov Decision Processes ( MDPs ). The main contributions of this work are the following :   1. An extension of a UCB style exploration scheme that achieves an optimal penalty of O(T ) by maintaining robust optimistic estimates of rewards at each state - action pair in the MDP with deterministic transition. 2. It is shown that an extension of UCB - like exploration scheme achieves an robust penalty of $ O(\sqrt{T}(T)$ in the case of a linear contextual bandit. 3. It provides empirical evidence regarding the robustness of our proposed algorithms on synthetic and real datasets."
SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"This paper proposes Rewriter - Evaluator, a framework that uses rewriter - rewriter and evaluator architecture for neural machine translation ( NMT ). The rewriter generates a new translation for the evaluation at every pass of the source sentence, while evaluating the quality of the previous one. To achieve this task jointly, the authors propose a prioritized gradient descent ( PGD ) method to compute the rewriter ( G ) and the evaluation ( Eq. ) objective jointly, which can be trained with similar computational cost as training an encoder - decoder model. The authors apply the proposed framework to improve the general NMT models, such as RNNSearch and Transformer. They conduct extensive experiments on two translation tasks, Chinese - English and English - German, and show that the proposed method significantly improves the performance of the popular NMT model, and significantly outperforms prior baselines."
SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"This paper proposes a new method to learn a multimodal segmentation model, i.e., a distribution over predictions that closely matches the corresponding labels in the training set. The method consists of a two - stage process. In the first stage, a categorical likelihood is explicitly modeled, and an adversarial network is trained to generate an arbitrary number of predictions. These predictions are sampled from a Gaussian distribution, such that the empirical frequency of the sampled predictions closely matches that of the corresponding label in the learning set.   In the second stage, the learned segmentation map is integrated into a deterministic, black - box semantic segmentation framework, and benchmark results on the LIDC dataset and a modified Cityscapes dataset are evaluated. Additionally, a toy regression problem is used to demonstrate that the method is also applicable to a more general class of regression problems."
SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"This paper proposes a new method to overcome the communication overhead incurred by biased compressors in distributed learning via compressed communication with error feedback ( EF ). EF is the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top - K or PowerSGD. In this paper, the authors propose a new and theoretically and practically better alternative to EF : induced unbiased compressors. In particular, they propose a construction which can transform any contractive compressor into an induced unbiased compressor, such that existing methods able to work with unbiased compressor can be applied to their framework. They show that their approach leads to vast improvements over EF, including reduced memory requirements, reduced communication complexity, better convergence guarantees and fewer assumptions. They further extend their results to federated learning with partial participation following an arbitrary distribution over the nodes, and demonstrate the benefits thereof. Finally, they provide an experimental evaluation on an array of classification tasks with CIFAR10 dataset."
SP:4fd702490293e481c79614852ba27dd3ce9215a4,"This paper proposes a new research framework : hyperparameter transfer across adjustments ( HT - AA ), which aims to automatically apply results of an old hyper - parameter optimization ( HPO ) algorithm to improve the performance of a new one when making adjustments at the developer level. The motivation is that an old HPO that performs poorly on a particular task might not be applicable to a new task that is designed to solve a problem that arises at the application stage, i.e., the search space of hyper - parameters. The idea is to apply existing HPO algorithms that have been successfully applied in the past to the new task in order to improve its performance. The paper provides 4 simple baseline algorithms for HPO with transfer and 8 benchmarks that can be easily adapted to implement the new HPO on either one of the four baselines or 8 benchmarks. On average, the new baseline reaches a given performance 1.2 – 3.6x faster than a prominent HPO algorithm without transfer. The authors also provide a python package to help practitioners implement the baselines."
SP:e8f99bae5853de525450fcb8facd23cf973fc161,"The paper studies the role of label representation in training deep neural networks. Specifically, the paper considers the image classification task by regressing audio labels ( spectrograms ) over categorical probabilities, and training neural networks using a mixture of Gaussians, constant matrices, shuffled spectrogram, Gaussian mixtures, or uniform random matrices of various dimensionalities. The paper finds that such high - dimensional, high - entropy representations produce more robust and data - efficient neural networks, while low - dimensional and low - entropy label representations do not have these benefits and may even lead to worse performance.   The paper also provides a set of quantitative and qualitative analyses to study and understand the learned feature representations of our networks."
SP:4e8d924cba7367af0999b30d79250b4dc40413e1,"This paper proposes MIMO ( Multi - input - multi - output ), a method to train multiple independent subnetworks within a single neural network, each learning their own predictions for a single task. The subnetwork networks are trained independently by sampling inputs and outputs from a central processing unit ( CPU ), and the goal is to maximize the output of each subnetwork as best as possible without changing the architecture of the network ( i.e. without changing any of the parameters in the encoder / decoder architecture ). The authors argue that this preserves the model's robustness and uncertainty performance, which were previously limited by the need for multiple forward passes for each prediction. However, they show that with the proposed approach, the benefits of using multiple predictions can be achieved ‘for free ’ under a single model ’s forward pass if only a small change in the number of parameters is made ( up to a constant factor in a constant step ).   The proposed approach is evaluated on CIFAR-10, Cifar-100, ImageNet, and ImageNet with out - of - distribution encoders. They show that the proposed method improves model robustness ( NLL ), accuracy ( accuracy > 90 % ) and NLL uncertainty ( > 80 % ) compared to the baselines. They also show that their approach is computationally tractable ( down to wall - clock time )."
SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"This paper proposes a method to transfer intermediate knowledge obtained from one Convolutional Neural Network ( CNN ) to another by utilizing sparse representation learning. The proposed method, called Sparse Representation Matching ( SRM ), first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixellevel and image - level labels for training intermediate feature maps of the student network. The authors formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plugand - play manner. The experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets, especially in transfer learning tasks."
SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,"This paper proposes a novel and theoretically motivated policy similarity metric ( PSM ) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. The authors also present a contrastive representation learning procedure to embed any state similarity metric, which they instantiate with PSM to obtain policy similarity embeddings ( PSE ). They demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite."
SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"The paper studies the problem of learning neural networks to disentangle natural factors of variation in data ( e.g., object shape vs pose ). A popular approach to learn this task is to map each of these factors to distinct subspaces of a model ’s latent representation. This paper proposes a distributed equivariant operator approach to this task that relies on equivariance operators in the encoder to potentially act on the entire latent space. The authors prove that this approach has the property to introduce discontinuities if and only if the corresponding affine transformations ( translation, rotation, and combinations thereof ) have non - isotropic actions on the input. They then theoretically and empirically demonstrate the disentanglement properties of their approach.    The main contributions of the paper are : ( 1 ) Theorem 1. Theorem 2. A distribution of equivariants over the action space of an affine transformation provably disentangles a set of simple transformations such as translational, rotational, and translation transformations in the latent space of a neural network ; and ( 3 ) Applications of the Distribution Equivariant Operator to the Problem of Denoising Networks ( DINO )."
SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,"This paper proposes a continuous - time model of neural spike trains based on the nonlinear multivariate Hawkes process. Inspired by the Gaussian - Gamma process in Hawkes ( Donner & Opper, 2017 ), three auxiliary latent variable sets : Pólya Gamma variables, latent marked Poisson processes and sparsity variables are augmented to make functional connection weights in a Gaussian form, which allows for an efficient expectationmaximization ( EM ) algorithm to obtain the maximum a posteriori ( MAP ) estimate. The accuracy and efficiency performance of the EM algorithm are evaluated on both synthetic and real data. The proposed model is proposed for the neuroscience domain, although it can be applied to other applications, e.g. in the coronavirus ( COVID-19 ) spread, the inhibitive effect may represent the medical treatment or cure, or the forced isolation by government."
SP:1156d3deac022829bda930ffcb081947609d972b,"This paper studies the dynamics of the gradient descent ( GD ) algorithm for training two - layer neural networks in the under - parameterized and over - parametrized settings. It is found that there are two distinct phases in the GD dynamics that are distinct from the more commonly studied “ mean - field ” and “ random feature ” regimes : ( i ) An early phase, in which the network ’s dynamics closely resembles that of the corresponding random feature model, followed by a late phase where the neurons are divided into two groups : a group of a few “ activated ” neurons that dominate the dynamics and another group of “ quenched “ neurons that support the continued activation and deactivation process. In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching - activation process biases GD to picking sparse solutions. This neural network - like behavior is continued into the mildly overparameterized regime where it undergoes a transition to a random featurelike behavior, where the inner - layer parameters are effectively frozen during the training process. Finally, the paper shows that in the milder settings, the quenchings process seems to provide a clear mechanism for “ implicit regularization ”."
SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"This paper proposes a method to solve a constrained Markov decision process ( CMDP ) by decomposing the problem into a pair of MDPs : Reconnaissance MDP ( R - MDP ) and Planning MDP( P -MDP ). In R - RDP, the threat function, the Q - function analogue of danger, is used to train a reward - seeking policy while using a fixed threat function to determine the safeness of each action. In P - PDP, a generative model is trained to provide a next state sample for any given state - action pair. The proposed method is shown to outperform existing methods on benchmark datasets and complex collision - free navigation tasks."
SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,"This paper criticizes the popular belief that the cross - entropy loss is better than the square loss in training neural networks for various classification tasks, mainly in NLP and computer vision tasks. The authors argue that there is little compelling empirical or theoretical evidence to support such a claim. They compare the performance of models trained with and trained with the cross entropy and square loss across a range of datasets and architectures and find that cross entropy performs comparably or better to square loss, even after equalizing computational resources. They conjecture that the performance gap might be because cross entropy is less sensitive to the randomness in the initialization of neural networks and that square loss needs to be a part of "" best practices "" for classification."
SP:915f1f0fc4850507c28c1d609239b41775863ebe,"This paper proposes a new unsupervised self - supervised learning framework, called Self - Predictive Representations ( SPR ), which combines future prediction and data augmentation to improve sample efficiency in RL from pixels. The method is evaluated on Atari, and achieves a median human - normalized score of 0.415 on the Atari benchmark with only 100 steps of interaction. This represents a 55% relative improvement over the previous state - of - the - art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games."
SP:983f01c170909c8c67fd3be25f121bd61bdd8307,"This paper studies the problem of learning compact graph embeddings, i.e., learning a d - dimensional embedding for each node in a graph, in order to be useful for various downstream tasks, e.g. visualization, node classification, link prediction, etc. The authors propose to solve this problem via a local method based on the Personalized PageRank ( PPR ) algorithm. In particular, the algorithm iteratively iterates through all the nodes in a given graph at a given time $ t$ time, one at a time, and compute the PPR - based embedding of each node at that time based on its nearest neighbor's PPR. Then, the computation of the final graph embedding is back - propagated through the iterate of PPR computations. The paper provides theoretical guarantees on the locality of the computation, as well as the proof of the global consistency. Empirically, the authors conduct extensive experiments on real - world datasets with over a billion edges. They show that InstantEmbedding is faster and more efficient than the state - of - the - art for unsupervised representation learning on tasks like node classification and link prediction. They also show empirically that their method is able to produce high - quality representations on par with state - art methods, with efficiency several orders of magnitude better in clock time and memory consumption."
SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"The paper proposes GOREN, a GNN - based method for learning graph coarsening algorithms based on deep learning on graph data. The authors claim that the current methods do not carefully choose the Laplace operator and the projection / lift map depending on the goal which can lead to suboptimal results. Motivated by this observation, the paper proposes to train GNNs on the original graph and coarse graphs to learn the edge weights of the coarse graph to improve the quality of the coarsened version. The paper shows that GNN learning with different auxiliary losses such as differentiable and non - differentiable losses improves the proposed method significantly over existing methods. Experiments are conducted on both synthetic graphs and real networks to demonstrate the effectiveness of the method."
SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,This paper proposes a geometric deep learning algorithm based on discrete - laplacian and implicit encoders to compute the scattering characteristics of general 3D objects at interactive rates using a point cloud approximation. Each point is encoded in a high - dimensional latent space. The multi - layer network can accurately estimate these acoustic properties for arbitrary topologies and takes less than 1ms per object. They also prove that their learning method is permutation and rotation invariant. They demonstrate high accuracy on objects that are quite different from the training data. They highlight its application to generating environmental acoustic effects in dynamic environments.
SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"This paper proposes Risk Extrapolation ( REx ), a method that extends MMR ( Risk Expolation by Monte Carlo ) to deal with distributional shift. In particular, REx can be viewed as a form of robust optimization over a perturbation set of targets ( MMMREx ), and proposing a penalty on the variance of training risks ( V - REx, a simpler variant ) as a simpler alternative. The authors prove that REx recovers the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution ( “ covariate shift ” ). By appropriately trading - off robustness ( i.e., appropriately compensating for covariate and interventional shift ),   REx is able to outperform alternative methods such as Invariant Risk Minimization ( IRM ) in situations where these types of shift co -occur, and outperforms MMR."
SP:411d5bcf7698d534ad60f581d479ff74849ba4de,"This paper proposes a neural operator for learning partial differential equations ( PDEs ). The neural operator learns the mapping from any functional parametric dependence to the solution. The proposed neural operator can approximate highly non - linear operators. The power of neural operators comes from combining linear, global integral operators ( via the Fourier transform ) and nonlinear, local activation functions. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning - based solvers under fixed resolution."
SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"This paper studies the implicit bias of gradient flow ( GD ) on linear neural networks. Following recent progress on this topic, the authors consider classification and regression problems that have multiple solutions with zero training error. The analyses apply to a general class of networks, and prove both convergence and implicit bias, providing a more complete characterization of the algorithm trajectory without relying on convergence assumptions.    The authors propose a general tensor formulation of nonlinear neural networks which includes many network architectures considered in the literature. In this paper, focus on the linear version of this formulation, which is called linear tensor networks. For linearly separable classification, the gradient flow finds the `2 / depth max - margin solution in the singular value space, leading the parameters to converge to the top singular vectors of the tensor when depth = 2. This theorem subsumes known results on linear convolutional networks and diagonal networks proved in Gunasekar et al. ( 2018b ), without using convergence assumptions! For underdetermined linear regression with deep linear fully - connected networks, the network converges to the minimum `2 norm solutions as we scale the initialization to zero. Lastly, simple experiments are provided to verify the theoretical analysis."
SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"This paper proposes PareCO, a method to jointly optimize the width - multipliers for different layers and the shared weights in a slimmable neural network. The proposed method is motivated by the observation that different layers affect the network ’s prediction accuracy differently and have different FLOP requirements. To address this, the authors formulate the problem of optimizing the weight and width multipliers jointly from a multi - objective perspective and propose a stochastic gradient descent ( SGD ) method to approximate the problem. Experiments are conducted on 15 datasets and 5 networks with 2 types of objectives : FLOPs and memory footprint. Comparative results on ImageNet dataset show improvements of up to 1.7% and 8% in top - 1 accuracy on the predicted accuracy of ImageNet with MobileNetV2 considering FLOPS and Memory footprint, respectively."
SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"This paper introduces a new federated learning problem, namely Federated Semi - supervised Learning ( FSSL ), where each client learns with only partly labeled data ( Labels - at - Client scenario ) or supervised labels are only available at the server, and proposes a new method, FedMatch, which introduces the inter - client consistency loss that aims to maximize the agreement between the models trained at different clients, and the parameter decomposition for disjoint learning which decomposes the parameters into one for labeled data and the other for unlabeled data. The authors study two essential scenarios of FSSL based on the location of the labeled data : 1 ) conventional case where clients have both labeled and unlabeling data ( labeled - client scenario ), and 2 ) the novel labels-at - server scenario, where clients work with completely unlabelED data ( labels - at-server scenario ). Experiments are conducted on two scenarios, and FedMatch is shown to outperform both local semi - supervised learning methods and naive combination of FL with SSL algorithms, across multiple clients with both non - i.i.d. and i.d. data."
SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,"This paper proposes a self - supervised learning method, called Contrastive Learning for Event Sequences ( CoLES ), to learn event embeddings from a set of event sequences using contrastive learning in a semi - supervised setting. CoLES is based on augmenting the event embedding with a fixed - length vector drawn from a Dirichlet of length $ L(x)$, as a form of augmentation. The paper shows that this augmentation results in a learned embedding $ x^2 $, where $ x$ is the length of the event, $ z$ the embedding of the corresponding entry in the event sequence, and $ a_t $ is the distance between the entry and all other entries in the sequence.   The paper evaluates CoLES on several public datasets and shows that CoLES representations consistently outperform other methods on different downstream tasks. In particular, CoLES outperforms both handcrafted features and hand - crafted features by a large margin when applied to the EventNet dataset. In semi - supervised settings, where only a small number of labeled examples are available, the performance of CoLES deteriorates further, as is the case for handcrafted data."
SP:385942a5bcee7384bb722a1669b541f2fac0cd36,"This paper introduces a new unsupervised parsing method, called StructFormer, that can induce dependency and constituency grammars in the same language model. The main idea is to jointly generate a dependency graph and constituency tree. To achieve this, the parsing process is divided into two steps : ( 1 ) induce the dependency graph ( G ) that models the one - to - one correspondence between words, and ( 2 ) induce a constituency graph ( C ) that represents the assembly of one or several words. The proposed StructFormer is differentiable and can be applied in differentiable language models, such as Transformer. The authors also introduce a dependency - constrained attention mechanism in the form of self - attention. Experiments shows that StructFormer can induce meaningful dependency structures, and leverage the parsing results to achieve strong performance on masked language model tasks."
SP:078966ff62775bba6031e47d374bda95f4a7dde3,"This paper proposes a new method to learn a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome ( VG ) and Visual Relation Detection ( VRD ) datasets verify that their model post an improvement on scene graph grounding task over current state - of - the - art approaches. Also, the proposed model is applied to scene graph parsing task and further experiments on weakly supervised dataset verify the grounding found by our model can reinforce the performance of the existing method."
SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"The paper proposes a new variant of sliced fused Gromov wasserstein ( SFG ), called spherical sliced fused sliced fused frankenstein ( SSFG ), to replace the uniform distribution over slicing direction in SFG with a von Mises - Fisher distribution that can cover the most informative area of directions. Two variants of SSFG are proposed : MSSFG and PSSFG. MSSFgi replaces the vMF distribution by a mixture of vMF to capture several important areas of directions ; PSFG improves the sampling speed by replacing vMF with a power spherical distribution. These variants are applied to the relational regularized auto - encoder ( RAE ) framework of the DRAE. Experiments are conducted on image classification, latent manifold structure generation, and reconstruction tasks. They show that the proposed methods outperform the baselines."
SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"This paper proposes a weight sharing method to speed up training of deep networks with repeated layers and showed promising empirical results on BERT training. The method is motivated by the successes of weight sharing models in the literature as well as the theoretic analysis on deep linear models. For future work, the empirical studies will extend   deep network   tasks, and analyze under which conditions our method will be helpful.  "
SP:a51710551142316b67e2fccd969fea1ece35ba39,"This paper analyzes the interaction between perturbation units in order to improve the adversarial transferability of adversarial perturbations. The authors claim that there is a negative correlation between the transferability and the interaction in the perturbing process, and propose to use this correlation to motivate a new loss, i.e., directly penalizing interactions during the attacking process. The proposed penalization, which penalizes interactions only during the attack process, is based on a game - based approach.    The main contributions of the paper are as follows :   1 ) The authors prove that the multi - step attack tends to generate adversarially perturbed pertubations with large interactions. 2 ) This negative correlation is further verified through different DNNs with various inputs. 3 ) This provides a unified view to understand transferability - boosting methods and provides a strong motivation to use penalization to directly penalize interactions during attacking. 4 ) This paper proposes to directly apply the proposed penalized loss directly to the output of a classifier to penalise interactions."
SP:f1565319075c1442c2cb52d96443facb492c06c2,"This paper provides a quantitative analysis of catastrophic forgetting through the lens of neural network ( NN ) representations. Through this analysis, it is shown that catastrophic forgetting is concentrated in the higher layers of the neural network, which erase earlier task representations through sequential training. It is also shown that mitigation methods stabilize higher layers, but vary on whether they enforce more feature reuse, or store tasks in orthogonal subspaces. Based on these insights, it develops an analytic model to investigate the connection between catastrophic forgetting and task semantic similarity. It finds that maximal forgetting occurs for task sequences that have intermediate similarity, and together these show that forgetting is most severe for tasks with intermediate similarity. Finally, it proposes a new approach for developing and measuring mitigation methods."
SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"This paper proposes EarlyBERT, an efficient training framework for BERT pre - training and fine - tuning. The method is inspired by the early - bird lottery ticket approach for training computer vision models. The key idea is to first prune the self - attention layer and fully connected layer in a transformer, and then use the pruned network for efficient training. Experiments on BERT, GLUE and SQuAD show that the proposed method can reduce the training time by up to 45 % compared to standard BERT."
SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"This paper studies the robustness of divergences when f - divergence measures are maximized in the presence of label noise in supervised learning with noisy labels. The main contributions are :   ( 1 ) The paper identifies and derives a new family of divergence measures that is robust to label noise ( called Robust F - Divergences ) based on the variational difference between the difference on the clean distribution and the bias term introduced due to the noise ; the new f - divergence measures have the nice property of decoupling when the noise level is less than some threshold value. The paper shows that such robustness can be measured with respect to a classifier ’s predictions and the supervised labels, where the divergence is defined to be a linear combination of the Variational Difference on the Clean Distribution and the Bias Term. ( 2 ) This paper empirically shows that optimizing f - diversities when there is label noise makes the resulting robustness measure asymptotically robust to the label noise. ( 3 ) A number of practical solutions are proposed to make the Robust f - Divergence Metrics more robust. ( 4 ) Experiments are provided to verify the existence of the new robustness measures."
SP:841888179dcdac901889c8d62cb5234311fe28f1,"This paper proposes SUNRISE, an ensemble - based off - policy deep reinforcement learning ( DRL ) method that uses a Q - ensemble to estimate the uncertainty in the target Q - values to guide the weighting of the Bellman updates in an actor - critic and critic algorithm. The authors claim that the method stabilizes and improves learning on both continuous and discrete control tasks. They also specifically investigate the signal - to - noise aspect by studying environments with noisy rewards, and find that weighted Bellman backups significantly outperform standard Bellman backbones. Furthermore, since the proposed method relies on maintaining an ensemble of DRL agents, they integrate it with UCB Exploration by enforcing the diversity between agents using bootstrap. They show that the UCB exploration idea is largely orthogonal and can be integrated with the ensemble idea.   Experiments are conducted on Soft Actor - Critic ( SAC ) and Rainbow DQN, and outperform state - of - the - art RL algorithms on both low - dimensional and high - dimensional environments in OpenAI Gym and Atari."
SP:afc08f203562b841180811aef943bfb63a1659ea,"This paper proposes a new method to measure the distributional mismatch between support and query sets via class - wise similarities in few - shot classification through meta - training. It is claimed that typical meta - learning algorithms do not consider this discrepancy when training models. The proposed method is algorithm - agnostic and can be applied to a range of meta learning models, including neural networks and reinforcement learning models. First, it quantifies the query set's distributional discrepancy with respect to the support set and the class label's similarity to query set. Second, it proposes a training strategy to train the model to avoid being indiscriminately confident in the predictions. Third, it applies the proposed method to test whether task calibration can be achieved without loss of accuracy by training the model on calibrated data."
SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"This paper proposes a novel method for cross - model retrieval based on text - to - video and video - text representation learning by using a generative model. The key idea is to consider a captioning task as an additional learning objective to be learned alongside the video representation learning. The paper shows that the proposed model outperforms the state - of - the - art methods on MSR - VTT, VATEX, ActivityNet, and MSVD for video-to - text and text - - text retrieval ( on three datasets ).   The main contributions are : ( 1 ) A captioning pretext that encourages the representation to pull together videos that share a similar caption, and are thus likely to be equivalent for retrieval ; ( 2 ) A model that explicitly encode semantics that are shared between samples, unlike noise contrastive learning, in which the semantics are learned separately."
SP:8a71d8fad25a126aff01431cacf348c05de75667,"This paper proposes a gtok ( or seg tok ) method to re - build the vocabulary of the pre - trained language model BERT with the help of Chinese word segmentation ( CWS ) and subword tokenization. Then, this paper proposes three multi - vocabulary pre - training ( MVP ) strategies to improve the performance of the language model, i.e., gt tok, ctok, ktok and ktok, where each gtk is a prefix of a Chinese word, followed by a tokenization of the corresponding Chinese word. Experiments are conducted on sentence - level classification and sequence labeling tasks, and the proposed methods are shown to improve on various NLP tasks.    The main contribution of this paper is the introduction of a new method to pre - train a Chinese language model ( BERT ) using the learned segmentation and tokenization from CWS and CWS. The authors also propose three MVP strategies to further improve the performances of the BERT language model."
SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"This paper proposes a method for distributed training of graph convolutional neural networks ( GCN ) based on unbiased graph partitioning. The authors claim that the proposed method, called BDS - GCN, can reduce the memory usage and upto 58 % of GCN training time in Reddit and ogbn - products datasets while maintaining the same or even better accuracy. The method is based on partitioning the vertices of each graph subgraphs into equal subsets, and sampling these subsets according to their graph partition probabilities. Each subgraph is partitioned into equal K subsets based on the adjacency matrices computed from the K - means of each subgraph in the partitioned set. Each subset is propagated through a Gumbel Softmax process to get a final graph representation. Each node in a subset is sampled unbiasedly according to its partition probability score, and the average score is used to compute the score of the subgraph as a proxy of partition probability. The entire training procedure is repeated on a fixed number of replicas of the dataset, and all training steps are repeated on these replicas. The proposed method is applied on top of a fixed set of datasets ( Reddit and OGBbn- products ), and it is claimed that it can achieve a speedup of up to 500 % and a memory usage down to 58 % in terms of total training time and GCN memory usage respectively."
SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"This paper proposes a GNN model, called ForceNet, for the prediction of per - atom atomic forces. The model is based on graph neural networks ( GNNs ), and is trained on a large quantum chemistry dataset, called OC20 ( OC20 ), which contains 200 M atom relaxations relevant to the discovery of new catalysts for renewable energy storage and other energy applications. The key challenge of the model is to capture the highly complex and non - linear quantum interactions of atoms in 3D space, on which forces are dependent. The proposed model has three key components : 1 ) expressive message passing architecture, 2 ) appropriate choice of non -linear activation functions, and 3 ) model scaling in terms of network depth and width. They show ForceNet reduces the estimation error of atomic forces by 30 % compared to existing ML models, and generalizes well to out - of - distribution structures. They also use ForceNet to perform quantum chemistry simulations, where ForceNet is able to achieve 4 % higher success rate than existing ML model. Overall, the work demonstrates the potential for ML - based simulations to achieve practical usefulness while being orders of magnitude faster than physics -based simulations."
SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,"This paper investigates different regularisation methods for fine - tuning neural networks. It provides two new bounds on the generalisation performance of neural networks based on the distance of the final weights from their initial values. The main result is that the MARS distance is a more appropriate metric to measure the distance in the parameter space of convolutional networks than Frobenius distance. The experimental results are presented that enable an experimental comparison between different regularization methods : projected subgradient methods improve performance over using penalty terms, and MARS - regularised methods outperform projection - based methods."
SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"The paper investigates what happens when the hyperparameters of mask generation ( Hfind ) and evaluation ( Heval ) are decoupled, in contrast to previous work which assumed that the best training configuration for Hfind is also the best configuration for mask discovery ( e.g., EWC ). The paper shows that when LPR is fixed to a “ good ” LPR ( i.e., a mask generation LPR ), the previously bad hyperparameter will lead to better performance and better mask generation. It is also shown that different Hfind values yield masks with materially different layerwise pruning ratios. Finally, the paper provides clear insights into the mechanisms underlying this counterintuitive effect. The results demonstrate the practical utility of decoupling Hfind and Heval."
SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"This paper proposes a new metric, m - coherence, to study the memorization of per - example gradients in neural networks. The motivation is to better understand the recently proposed Coherent Gradients ( CG ) theory, which proposes a simple explanation for memorization and generalization [ 1 ]. The paper studies ResNet and EfficientNet models on ImageNet, and several variants with label noise. In particular, it introduces the metric based on Zielinski et al. ( 2020 ) and Chatterjee 2020.   Compared to other commonly used metrics, the proposed m -coherence is more interpretable, cheaper to compute ( O(m ), and mathematically cleaner. It is also closely connected to gradient diversity, a quantity previously used in some theoretical bounds. Using m - Coherence, it is able to identify the peak coherence with real labels is much higher ( each example helps 104 other examples ) than the peak with random labels, which indicates that over - parameterized neural networks find common patterns even in scenarios where generalization is not possible. Moreover, it provides insights into what is missing from the CG explanation for generalization, and thus provides suggestions for future work."
SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"This paper proposes an infomax - based approach to automatically constructing sufficient statistics for likelihood - free inference of implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The proposed method is based on learning sufficient statistics by learning mutual information maximizing representations of the data with the help of a deep neural network. In particular, the proposed method SMCABC+ and SNL+ aim to learn sufficient statistics that do not need to estimate any density or density ratio. The obtained sufficient statistics can be used to boost the performance of traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks.    With the proposed statistics, two new likelihood - based inference methods namely SMCBC+ ( SMCBABC+ ) and SNLI+ ( SNLI+) are proposed. Experiments on tasks with various types of data demonstrate their effectiveness."
SP:c5997bf2348e94949684f45fbd418661e85220c1,"This paper proposes TUNIT, an unsupervised image - to - image translation model that is trained in an unpaired setting, i.e., neither paired images nor domain labels are available. Instead of using image - level or set - level supervision, the model learns to separate the input images into separate image domains and to translate the corresponding output images into the estimated domains using a guiding network that provides pseudo labels. Experiments are conducted on a variety of image datasets ( CIFAR10, Tiny Imagenet, ImageNet, Tiny - ImageNet ) with different number of pseudo domains. Comparative results with the SOTA SOTA Transformer model ( TATE ) are reported, as well as results on a semi - supervised setting ( Kipf - DQN ), where labels are provided only on a fraction of the samples."
SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"This paper studies the implicit bias of gradient descent on regression problems, focusing on the case of wide ReLU networks. For 1D regression, the trained network outputs a function that interpolates the training data and has the minimum possible weighted 2 - norm of the second derivative with respect to the input. The space of interpolating splines is a linear space which has a dimension that is linear in the number of data points. The result of this paper generalizes to multivariate regression and different activation functions. Moreover, the training trajectories are captured by trajectories of spatially adaptive smoothing splines with decreasing regularization strength."
SP:8b885142facbb3b8db41ec9d83822cee81324694,"This paper studies the problem of weight decay in adaptive gradient methods, such as Adaptive Momentum Estimation ( Adam ), and proposed Adam with Decoupled Weight Decay ( AdamW ). The authors found that the popular L2 regularization and decoupled weight decay implementations in modern deep learning libraries usually damage performance. The main contribution of this paper is to propose the SWD method to fix the unstable weight decay problem from a dynamical perspective. The proposed SWD makes weight decay rate stable and makes it possible to decouple weight decay. The empirical results of SWD shows that SWD can make Adam outperform other Adam variants and Padam with AdaBound."
SP:a3206dc71e32ba1830895bf442d3840f3331a532,"This paper proposes TMG - NMT, a method to combine the strengths of Translation Memory ( TM ) and Neural Machine Translation ( NMT ). The TM encoder is firstly enhanced by a pre - trained language model ( PLM ) to encode the TM information and source sentence together. Then, the TM - guided decoder is proposed to copy the information flow from TM to NMT decoder. The proposed method applies the Copy Mechanism to alleviate the rare - word problem. Experiments on English to French translation shows that the proposed methods can significantly improve translation quality. Further in - depth analysis demonstrates that the models show excellent adaptability to the unknown or new domain."
SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,"This paper proposes a new interpretation of deep convolutional neural networks ( DNNs ) based on the principles of rate reduction and shift invariant classification. It shows that the basic iterative gradient ascent scheme for maximizing the rate reduction of learned features naturally leads to a deep network, one iteration per layer. The architectures, operators ( linear or nonlinear ), and parameters of the network are all explicitly constructed layer - by - layer in a forward propagation fashion. All components of this “ white box ” network have precise optimization, statistical, and geometric interpretation. Experiments indicate that such a network can already learn a good discriminative deep representation without any back propagation training. Moreover, all linear operators of the so - derived network naturally become multi - channel convolutions when we enforce classification to be shift - invariant."
SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"This paper studies the implicit bias of gradient flow in over - parameterized two - layer neural networks, focusing on the case of symmetric asymmetric matrix factorization of input - output neural networks with weight factorization with an arbitrary initialization ( e.g., $ L = -1 $, where $ L$ and $ \gamma$ are the output and input matrices of the network, respectively. The authors show that the acceleration depends on the magnitude of $ l$-1 difference between the input and output matrices, which is conserved by a rotational symmetry that arises due to the over - parametrized setting, as well as the spectrum of the data. They also provide a characterization of the acceleration without the assumptions of spectral or small initializations of the weights, and establish connections with Riccati differential equations.   In Section 4, they analyze the implicit acceleration properties of   gradient flow for gradient flow with an   $ L=1 $-1 $ parameter ( in a similar way to previous work ), and provide a more general characterization without the assumption of spectral initializations. Finally, they provide an analysis of the gradient flow dynamics in a simple asymmetric factorization setting, and show that there is an implicit bias that follows certain trajectories."
SP:e5f086c806be88d50e461a782b5b00124f4656fb,"The paper proposes a model - agnostic explanation framework called CLIME to mitigate the problem of out - of - distribution ( OOD ) sampling in the context of explainable AI, which relies on a surrogate explainable model trained to be locally faithful on perturbed samples. The main idea of the paper is to design a model that operates in a constrained subspace of the input domain, such that adversarial attacks that arise due to OOD sampling are mitigated. In order to measure the level of fidelity of the surrogate explainer model, a new metric is introduced that quantifies the severity of the OOD problem. Experiments are conducted on image classification tasks and machine translation tasks, and extensive experiments demonstrate the effectiveness of the CLIME framework."
SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,"This paper proposes a multi - grained language model called AMBERT ( A Multi - Grained BERT ), which is an extension of the BERT pre - trained language model. The proposed model takes as input two sequences of words ( English words ) and one sequence of phrases ( Chinese words ), and uses one encoder for processing the words and the other one for the phrases. Then, the encoders output the shared parameters between the two encodings, and finally creates a sequence of contextualized representations of the word and the phrase. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD, and RACE. The results show that the proposed model outperforms the existing best performing models in almost all cases, particularly the improvements are significant for Chinese."
SP:fd1cfe80343d3789227d99d836a5674374a234f5,"This paper proposes a new Transformer architecture for semantic parsing. The main idea is to incorporate LSTM ( Long Short - Term Memory ) into the Self - Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong competitive performance on Geo, MSParS datasets, and leads to SOTA performance on Atis dataset in methods using Neural Network."
SP:2056a65a7500d79465685af883083cd706277c1f,"This paper proposes composite adversarial training ( CAT ), a method to improve the robustness of deep neural networks ( DNNs ) against the combinations of multiple perturbations. The authors claim that the existing adversarial defenses are ineffective when applied to the composite attacks, i.e., the model tries to predict the output of each individual perturbation, while the composite attack can be thought of as the combined effect of the individual attacks. To address this issue, CAT is proposed as an adversarial loss function, which allows to integrate and optimise multiple adversarial losses. In order to do so, it is necessary to first design a new adversarial attack, and then train the corresponding DNN with it. The proposed method is evaluated on a variety of datasets and models. On ImageNet and CIFAR-10, CAT outperforms the state - of - the - arts in terms of robustness against the compositions of pixel perturbational models and spatial transformations."
SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"The paper proposes a new variable - binding neural network, called the Emergent Symbol Binding Network ( ESBN ), which is a recurrent network augmented with an external memory that enables a form of variable binding and indirection. The authors argue that this new network is needed to enable generalization of learned abstract rules to novel entities given only a limited number of training examples. The ESBN is evaluated on a series of tasks that test the ability to generalize abstract rules learned from a limited set of examples to a larger set of novel entities, and outperforms a number of other competitive neural network architectures in terms of generalization performance.   The authors conclude from these results that a capacity for variable binding is a necessary component for human - like abstraction and generalization, and that the ESBN could be a promising candidate for how to incorporate such a capacity into neural network algorithms."
SP:4171ce45966ac499f51450a19fb233934c0847f0,"This paper proposes a generative model for structured prediction tasks such as entity extraction, entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. The proposed model is a text - to - text model ( TANL ) that translates from an input to an output in augmented natural languages ( AUs ). Different from discriminative models common in the literature, the proposed model in this paper is generative, as it is generating the structures from the input to the output of the AUs. The authors show that their model can match or outperform task - specific models on all tasks, and in particular, achieves new state - of - the - art results on joint entity extraction ( CoNLL04, ADE, NYT, and ACE2005 datasets ) as well as relation classification ( FewRel, TACRED, and Semantic role labeling ). They accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time ( multi - task learning ). Finally, the framework can also significantly improve the performance of the model in a low - resource regime, thanks to better use of label semantics."
SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"This paper proposes a general approach to avoid training NER models with unlabeled entities by using negative sampling. The key idea is to use negative sampling that, to a large extent, avoids training language models ( e.g., self - attention ) that would normally train with such entities. Empirical studies are performed on a variety of synthetic and real - world datasets, and the proposed approach is compared with the state - of - the - art method ( CoNLL - 2003 ) and a naive approach ( EC ). On well -annotated datasets, the proposed method is competitive with the current SOTA method. However, on well - labeled datasets, it significantly outperforms the previous baselines ( NER - C and EC - C )."
SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"This paper proposes a novel acoustic word embedding called Acoustic Neighbor Embeddings where speech or text of arbitrary length are mapped to a vector space of fixed, reduced dimensions by adapting stochastic neighbor embedding ( SNE ) to sequential inputs. The Euclidean distance between coordinates in the embedding space reflects the phonetic confusability between their corresponding sequences. Two encoder neural networks are trained : acoustic encoder that accepts speech signals in the form of frame - wise subword posterior probabilities obtained from an acoustic model and a text encoder accepting text as subword transcriptions. Compared to a triplet loss criterion, the proposed method is shown to have more effective gradients for neural network training. Experimentally, it also gives more accurate results with low - dimensional embeddings when the two encoder networks are used in tandem   in a word ( name ) recognition task, and when the speech encoder network is used standalone in an approximate phonetic matching task."
SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,"This paper proposes a reinforcement learning algorithm for stationary mean - field games where the goal is to learn a pair of state and policy that satisfy the Nash equilibrium. In particular, the authors consider a setting where the state is the $ \ell_1$-th dimension of the all - one vector $ \mathbb R^\mathbb{R}^n$ and the policy is the policy gradient of the mean of the state $ y$. The authors propose a fictitious play algorithm that iteratively updates the mean $ y$ via gradient descent and proximal policy optimization, respectively.   The authors prove that their algorithm converges to the equilibrium at a sublinear rate. This seems to be the first provably convergent RL algorithm for mean field games that learns the mean and policy via iterative updates. The authors also provide theoretical guarantees that the proposed algorithm is equivalent to the best known one - shot learning algorithm."
SP:c498f8a199da1818fe64ed88b0825c5aad688aec,"This paper proposes a variational inference framework for approximate probabilistic inference on the joint distribution defined by a normalizing flow model, i.e., given a pre - trained flow model p(x ) and some arbitrary partitioning of the variables x = ( x1, x2 ), the authors show that this task is computationally hard for a large class of flow models. Motivated by this hardness result, they propose an approximate inference framework that trains a new generative model with the property that its composition with the given model approximates the target conditional distribution, and parametrize this new distribution as another flow model. They then show how to efficiently train it using variational infercation and also handle conditioning under arbitrary differentiable transformations. Since the resulting approximate posterior remains a flow, it offers exact likelihood evaluation, inversion, and efficient sampling. The authors provide an extensive empirical evidence showcasing the flexibility of their method on a variety of inference tasks with applications to inverse problems."
SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,"This paper aims to solve the two bottlenecks in the development of cell membrane segmentation : lack of sufficient high - quality data and lack of suitable evaluation criteria. To solve the first problem, it introduces the U - RISC dataset, the largest annotated Electron Microscopy ( EM ) dataset for the Cell membrane with multiple iterative annotations and uncompressed high - resolution raw images. During the analysis process, it is found that the current popular segmentation evaluation criteria are inconsistent with human perception. To resolve this inconsistency, it proposes a new evaluation criterion called Perceptual Hausdorff Distance ( PHD ) to measure the quality of cellmembrane segmentation results. Experiments are conducted on a small scale to validate the superiority of PHD over existing evaluation criteria ( e.g. F1 score, IoU ) and to motivate the further design of the segmentation method to match the human performance."
SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"This paper proposes a modular neural network architecture for continual learning ( CL ) with the aim of overcoming catastrophic forgetting and knowledge transfer. The authors propose a new suite of benchmarks to measure the performance on two new metrics : ( 1 ) the ability to transfer knowledge from previous tasks ; ( 2 ) ability to scale memory and compute / memory usage ; and ( 3 ) modularity to represent atomic skills that can be composed to perform a certain task. In particular, the authors propose to model each task as a collection of modules, where each task is solved by the combination of a handful of skills extracted from past tasks or freshly trained on the new task. The modules are associated with a classifier that predicts the best way to solve the current task based on a task - driven prior over the space of all possible ways to combine modules. The proposed approach is evaluated on a set of standard CL benchmarks ( CTrL - Benchmark ), which measure both the catastrophic forgetting ( which is observed only once in a long sequence of tasks ) as well as knowledge transfer and scaling to new tasks. The results show that this modular architecture and learning algorithm perform competitively on standard benchmarks while yielding superior performance on the more challenging benchmarks ( i.e. with a hundred tasks )."
SP:cc819c61f408e88f247eb87946187ccec3dad32e,"This paper proposes LASIUM, an unsupervised meta - learning method that uses a generative model to generate synthetic few - shot classifiers for a set of synthetic meta - tasks. The proposed method is based on the LAtent Space Interpolation ( LISA ) algorithm. LISA generates a latent classifier that maps a pair of samples ( either in - class or out - of - class ) to a classifier by taking samples from the latent space of a given pair of classes. Then, the classifier is trained by averaging over the pairs of samples generated by the LISA classifier. This method is evaluated on Omniglot, CIFAR-10, Tiny Imagenet, and ImageNet, where it is shown to outperform transfer learning, and in some cases, to approach the performance of SOTA meta - learners such as PyTorch."
SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"The paper studies injectivity of generative models.    Injectivity is defined in terms of the Lipschitz constant of the inverse of the forward function. The paper first characterizes injectivity through a layerwise analysis, showing that an expansivity factor of two is necessary and sufficient for injectivity by constructing appropriate weight matrices. Next, using an argument based on random projections, the paper characterizes global injectivity, and shows that an end - to - end -- rather than layerwise --doubling of the dimension suffices for injectivities. Finally, the authors provide theoretical guarantees for the stability of inverting an injective network, and use arguments from differential topology to study injectivities of deep networks. The results provide a theoretical basis for the study of nonlinear inverse and inference problems using neural networks, and establish a theoretical connection between injectivity and stability."
SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"This paper proposes a conditional generative adversarial network ( CcGAN ) for image generation conditional on continuous, scalar conditions ( regression labels ). Existing conditional GANs ( cGANs ) are mainly designed for categorical conditions, whereas conditioning on regression labels is mathematically distinct and raises two fundamental problems :   1. There may be very few ( zero ) real images for some regression labels, minimizing existing empirical cGAN losses often fails in practice ; 2. Since regression labels are scalar and infinitely many, conventional label input methods ( e.g. combining a hidden map of the generator/discriminator with a one - hot encoded label ) are not applicable. The proposed CcGan solves the above problem by ( 1 ) reformulating existing empirical gan losses to be appropriate for the continuous scenario ; and ( 2 ) proposing a novel method to incorporate regression labels into the generator and the discriminator. The reformulation in ( 1) leads to two empirical discriminator losses, termed the hard vicinal discriminator loss ( HVDL ) and the soft vicinal discernment loss ( SVDL respectively, and a novel empirical generator loss, which are studied in ( 2 ): a new benchmark dataset, RC - 49, is also proposed for continuous scenario. Finally, experiments are conducted on the Circular 2 - D Gaussians, RC-49, and UTKFace datasets, and demonstrate the superiority of the proposed ccGAN to cGAN."
SP:10dd09ab315870631d1451d200f2c87a023f8226,"This paper proposes a novel active learning ( AL ) scheme that combines semi - supervised learning ( SSL ) and fully - supervised ( SL ) learning by querying unlabeled instances. The main idea of the scheme is to control the convergence rate of a classification network by actively querying instances to improve the rate of convergence upon inclusion to the labeled set. The proposed AL scheme is named as convergence rate control ( CRC ), and the experiments show that a deep neural network trained using a combination of the proposed scheme and a recently proposed SSL algorithm can achieve high performance using far less labeled samples than SL in practice. 	This work demonstrates that DL can benefit from AL algorithms which use relatively small networks, and this work provides a novel algorithm that outperformed uncertainty - based AL algorithms 	 and an AL algorithm with a similar goal of maximizing the model’s change."
SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"This paper proposes FedDyn, a federated learning ( FL ) method for training neural network models distributed to a subset of participating devices. Different from prior FL methods that either utilize devices to parallelize gradient computation or employ inexact minimization, the authors propose a dynamic regularizer for each participating device at each round, so that in the limit the global and device solutions are aligned. They demonstrate both through empirical results on real and synthetic data as well as analytical results that their scheme leads to efficient training, in both convex and non - convex settings, while being robust to device heterogeneity and robust to large number of devices, partial participation and unbalanced data."
SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"This paper proposes a new method to speed up the training of contrastive learning algorithms for self - supervised learning with little or no loss of accuracy. The authors claim that the main insight of the proposed method is that the intermediate contrastive loss is a good surrogate of the final similarity between two pairs of images, and thus, we can use it to improve the training time by up to 30 %. The proposed method uses two tricks : ( 1 ) truncating the gradient descent update step by only updating a small fraction of the parameters in the loss ; ( 2 ) using the intermediate loss to filter easy regions based on the similarity of the intermediate layers ; and ( 3 ) using softmax to select an easy region of each image for the final classifier. They test their method on MOCO ( He et al., 2020 ), SimCLR ( Chen et al, 2020a ), SwAV ( Caron et at. 2020 ; Caron, 2020 ; Chen, 2020b ), and MocOCO V2 ( Chen. et. al. 2020 ) and show that it can achieve the same performance as these algorithms, but with much less training time. They also show that their method can reduce the computational cost with little loss on the performance of ImageNet linear classification, PASCAL VOC object detection and segmentation."
SP:5b5e705ea1ee1b857e17e64d560a39052804949d,"This paper studies global convergence of actor - critic algorithm, which is one of the most popular families of reinforcement learning algorithms. In this paper, the authors focus on the single - time - varying setting where only the actor and critic updates are computed at each time step. In particular, the critic update is obtained by applying the Bellman evaluation operator only once, while the actor is updated in the policy gradient direction computed using the critic. The authors also consider two function approximation settings, where both the agent and critic are represented by linear or deep neural networks. In both cases, they prove that the actor sequence converges to a globally optimal policy at a sublinear O(K−1/2 ) rate, where K is the number of iterations. In addition, under the broader scope of policy optimization with nonlinear function approximation, they also prove that actorcritic with deep neural network finds the globally optima policy   for the first time with linear function approximation."
SP:26705a4dc305cce336f657c5937d1f5b4209548a,"This paper proposes a vector - based representation for representing log data at the field level, log level, and log sequence level. The field level representation can be computed from the previous level, while the log level representation is in vector format. The sequences can be represented in transformer networks ( TNNs ). The authors show how a number of log processing applications can be readily solved with their representation. The proposed representation is validated on a set of log data collected for anomaly detection, prediction analysis, log search, log generation / synthesis, and a Recommender system."
SP:165c51a16f17fb8726e968f8b34742b62011d60e,"The paper proposes a novel and mathematical interpretation of deep convolutional neural networks ( CNNs ) based on wavelet decompositions modulated by mixture weights. The decomposition of a convolution layer into a sequence of wavelet modules is modeled by a conjugate mirror filter ( CMF ). The authors claim that the decomposition resembles Gabor filters in that it can provide a mathematical description of the observed behavior of convolution layers.    The authors evaluate their model with three variants of wavelets and AlexNet, using the first wavelet module as an example. They achieve the accuracy of standard AlexNet but with a significantly lower number of parameters, and an interpretation of the network that is grounded in mathematical theory rather than physics."
SP:d0a284da462584724ba6a3a48c9e986d391233f6,"This paper proposes a framework to coordinate multi - agent teams with a coach who has a "" partial view "" of the environment, while the players have a "" complete view "". The coach coordinates the players by distributing individual strategies. The framework includes an attention mechanism for both the players and the coach, which introduces a variational objective to regularize learning. It also proposes an adaptive communication method to let the coach decide when to communicate with different players. The proposed framework is tested on a resource collection task with varying numbers of heterogeneous agents. The results show comparable or better performance against methods where players have full observation but no coach. Moreover, there is almost no performance degradation even when the coach communicates as little as 13 % of the time with the players."
SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"This paper studies the effectiveness of influence functions in the context of neural networks with non - convex loss functions. For linear models, influence functions are well - understood due to the convexity of the underlying loss function and are generally accurate even in difficult settings where model changes are fairly large. In contrast, for non convex settings such as deep learning, where the loss function does not have a convex function, it is not well understood how well influence functions work in deep learning. This paper provides a comprehensive and large - scale empirical study of successes and failures of influence function in neural network models trained on datasets such as Iris, MNIST, CIFAR-10, and ImageNet. Through their extensive experiments, they show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of the influence functions. In particular, they find that influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous. They also demonstrate that training with weight - decay regularization is important to get high - quality influence estimates, and for certain network architectures and datasets, influence estimates can vary significantly depending on the examined test points."
SP:5fea74a2031d097a99dacf613bedcb054b0c3831,"This paper studies why autoregressive language models, pretrained using large text corpora, do well on next word prediction tasks. The paper hypothesizes that the classification tasks of interest can be reformulated as sentence completion tasks, i.e., tasks that can be solved linearly using a conditional distribution over words following an input text. Theoretically, the paper shows that language models that are - optimal in cross - entropy ( log - perplexity ) learn features that can linearly solve such classification tasks with O(O(\sqrt{(\� ) ) ) error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. Experiments are conducted on standard benchmarks. Theorem 4.2 shows a stronger result for low - dimensional softmax models, leveraging a new tool, conditional mean features ( Definition 4.1 ), which is shown to be effective in practice. The usefulness of the language model features themselves is demonstrated by arguing a weak linear relationship between them and conditional mean feature. In Section 5.2, a new mathematically motivated objective ( Quad ) is introduced that provably learns good features for these natural tasks, and a new objective function, called Quad - XL, is used to design a model that performs well on some classification tasks."
SP:a67da438e9821010284416170c3699ae7ff96c99,"This paper proposes a new method to perform membership inference ( MIA ) on a shadow model trained on a single training image, e.g. an image of a target image. The main idea is to use the reconstruction error to detect if the training set contains images that are difficult to learn or easy to learn, i.e. difficult images whose reconstruction error is large and easy previously unseen images that have low reconstruction error. Based on this idea, the authors propose to use a training - free membership attack based on a combination of reconstruction error and a new image difficulty score. This approach is empirically shown to achieve high MIA accuracy on multiple benchmarks. The performance gain is attributed to the boost in accuracy due to overfitting to the training images, and the improvement due to the use of a hard - to - learn classifier to classify the difficult images."
SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"This paper proposes a differentiable architecture search method by formulating the search problem as a distribution learning problem, where the architecture mixing weight is modeled by a Dirichlet distribution, and the parameters of this distribution can be optimized via a gradient - based algorithm. This formulation induces stochasticity that naturally encourages exploration in the search space. To alleviate the large memory consumption of differentiable NAS, the authors propose a simple yet effective progressive learning scheme that enables searching directly on large - scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of the proposed method, which achieves a test error of 2.46% for CIFAR-10, 23.7% for ImageNet under the mobile setting, and achieves state - of - the - art results on NASBench-201."
SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"The paper proposes a new way of representing low - dimensional functions, namely multiplicative filter networks ( MFNs ). In contrast to SIREN and Fourier feature networks, MFNs simply multiply functions of sinusoidal or Gabor wavelet functions applied to the input, and thus represent the entire function as a linear function approximator over an exponential number of these functions. The paper empirically shows that MFNs are as good as or better than SIRen or Fourier features networks when applied on the same or similar number of parameters as those used in the original papers. In particular, the paper focuses on a particular setting where the depth or width of the network is chosen to be much deeper than other commonly used networks ( e.g., neural radiance fields ), and further investigates when MFNs achieve better performance deltas when increasing the depth."
SP:f5be855300f63c185a006834302bd4b033b56258,"This paper proposes a teacher - student scheme to enable gradient - based meta - learning algorithms to explore long horizons by the inner loop. The key idea is to employ a student network to adequately explore the search space of task - specific models ( e.g. by more than 10 steps ), and a teacher then takes a “leap ” toward the regions probed by the student. The teacher not only arrives at a high - quality model but also defines a lightweight computation graph for meta - gradients. This approach is generic and it performs well when applied to four meta - learners algorithms over three tasks : few - shot learning, long - tail classification, and meta - attack. The experimental results provide an affirmative answer to the first question above : long - horizon inner loop exploration improves a meta - learner’s performance."
SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,This paper proposes a new algorithm for offline RL based on behavior regularization to tackle limitations of standard off - policy RL algorithms. The proposed algorithm BRAC+ learns a KL divergence bound on the expected deviation between in - distribution and out - of - distribution samples to penalize the gradient of the Q values of rare out of distribution actions in the dataset. The authors also propose a Lagrange multiplier to allow more freedom of deviation to high probability states leading to better rewards while simultaneously restricting low probability ( less explored ) states. Experiments on the D4RL benchmark show that the proposed algorithm outperforms existing model - free and model - based offline RL algorithms in various datasets.
SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,"This paper proposes an approach to regularize and compress CNN - based architectures such as Resnet-50 and ResNet-50 by training the larger and smaller one - shot learning models together. The larger model is trained first, while the smaller one is trained together in a one - step learning paradigm where the parameters of the smaller model are shared across both models. The method is evaluated on Imagenet and CIFAR-100 where it is shown to be able to reduce the number of parameters by up to 99.7x and improve inference speed up to 5x on some datasets. It is also shown that it is better to train the two models together than to train one model separately."
SP:dba40073f79143e5355d194aa16db9eee0267a5d,"This paper proposes a simple extension of greedy exploration, i.e., repeating an action ( or action sequences of actions ) for a finite number of timesteps called z - greedy. The basic idea is that greedy exploration suffers from lack of persistence, as it can not escape local optima. The authors propose to address this limitation by replacing an action with a sequence of temporally extended options, or simply sampling an option and following it for a random duration. They then propose a simple instantiation of greedy algorithm, which they call z - greed. Experimentally, they show that z- greedy improves exploration and performance in sparse - reward environments with minimal loss in performance on easier - to - explore environments such as DeepSea, GridWorld, HalfDQN, and PPO. They also provide some empirical results on a class of reward distributions inspired by ecological models of foraging animals, and show that for these distributions greedy algorithm performs better than greedy algorithm."
SP:5efb581a368ace3bd085d48801a899559d6a43ef,"This paper analyzes the implicit regularization of gradient descent for the problem of depth - 2 matrix factorization. The authors prove that gradient flow with infinitesimal initialization is equivalent to Greedy Low - rank learning ( GLRL ), a type of heuristic rank - minimization algorithm. They also extend the results to the case where the depth is greater than $ \epsilon$ and show that the convergence does not depend on the initialization size. Experiments are conducted on simple toy models."
SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"The paper proposes a model patching framework for robustness improvement based on patching. Model patching first models subgroup features within a class and learns semantic transformations between them, then trains a classifier with data augmentations that deliberately manipulate sub - group features. The proposed framework, called CAMEL, first learns intra - class and inter - subgroup augmentations using a CycleGAN, and then balances subgroup performance using a theoretically - motivated subgroup consistency loss and robust objective. The authors demonstrate CAMEL’s effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33 % relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real - world skin cancer dataset."
SP:de6cea1e35a0555175e17546a93422e9a96a511e,"This paper proposes a new classifier, named Rule - based Representation Learner ( RRL ), that automatically learns interpretable nonfuzzy rules for data representation and classification. To train the non - differentiable RRL effectively, it project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end - to - end. The experimental results show that RRL enjoys both high classification performance and low model complexity on data sets with different scales."
SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"This paper proposes a method for regret minimization for generalization across structured biomedical domains such as molecular scaffolds or protein families. The method builds from invariant risk minimization ( IRM ) by recasting simultaneous optimality condition in terms of predictive regret. The main idea is to find a representation that enables the predictor to compete against an oracle with hindsight access to held - out environments. The proposed method adaptively highlights variation due to complex environments via specialized domain perturbations. They evaluate the method on molecular property prediction, protein homology and stability prediction and show that RGM significantly outperforms previous state - of - the - art baselines."
SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,"This paper proposes a novel architecture, cross - probe ( CP ) BERT, for cross - modal retrieval. It builds on top of the existing text - vision BERT model, which uses vision and text probes to retrieve language and image features, respectively. The idea is to reduce the computation cost of the query document by only using a small number of probes. The proposed CP BERT is motivated by the recent success of split - merge BERT ( Xie et al., 2021 ), a model which exploits language features of query documents by first ranking them according to the number of times they appear in the document. Then, the extracted language features are concatenated with the image features and the query's features, which are fed into the vision tower to generate the attended vision probes. In parallel, the text probes are obtained by concatenating the word embeddings in the query with the vision feature and the image feature ( together with the other features ) into the text tower, which generates the attended text probes. Finally, the attended probes are used in the attention layer to combine the features from both the image and text tower. Experiments on two benchmark datasets demonstrate the proposed method's effectiveness and efficiency."
SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,"This paper proposes a new type of Actor, named forward - looking actor or FORK, for Actor - Critic algorithms. For short, FORK can be integrated into a model - free ActorCritic algorithm. The experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement FORK   can bring to the state - of - the - art algorithms. In particular, TD3 - FORK performs the best among the all the tested environments. For Ant - v3, it improves the average cumulative reward by more than 50% than TD3, and achieves TD3 ’s best performance using only 35% of training samples. A variation of TD - V3 - K that can solve BipedalWalkerHardcore in as few as four hours using a single GPU."
SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"This paper proposes FedBE, a Bayesian aggregation method for federated learning ( FL ). It is motivated by the need to aggregate local models into a global model, which has been shown to be challenging when users have non - i.i.d. data. The authors propose to aggregate the local models by fitting Gaussian or Dirichlet distributions to the global models. Then, the aggregation aggregation can be performed by sampling higher - quality global models and combining them via Bayesian model Ensemble, leading to much robust aggregation. Experiments are conducted on CIFAR-10/100 and Tiny - ImageNet under different client conditions (i.e., i.e., i.d. and non - blurry / noisy data ), using ConvNet, ResNet, and MobileNetV2. FEDBE consistently outperforms FEDAVG, especially when the neural network architecture goes deeper. Moreover, it is demonstrated to be compatible with existing FL algorithms that regularize clients ’ learning or leverage server momentum."
SP:3ac5f437fc349a33810d0645664d1c448528af74,This paper proposes a multi - partite influence analysis for understanding the representations produced by Bayesian Registration and Entropy ( BERT ) classification models. The idea is to use BERT ’s representations as a basis for understanding how the subject - word agreement and reflexive anaphora of DNN representations are obtained. The paper proposes two methods for quantifying the sufficieny and sparsity of influence patterns in BERT by way of compression experiments and the influence concentration of discovered patterns. They quantitatively and visually demonstrated BERT’s contextualization in the two tasks using their methodology. The formalism and methods are general enough to apply to the analysis of other aspects of BERT and other models.
SP:efa2343ead47263a0d09e1c17f9aa044605b9650,"This paper studies the training of a deep neural network from a control theory perspective. Specifically, the authors pose the supervised learning problem as a control problem by jointly designing loss function as Lyapunov function and weight update as temporal derivative of the Lyap unov function. Control theory principles are then applied to provide guarantees on finite time convergence and settling time of the neural network. Through experiments on benchmark datasets, the proposed method converges within the a priori bounds derived from theory. It is also observed that in some cases our proposed method enforces faster convergence as compared to standard L1 and L2 loss functions."
SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,"This paper criticizes Sorrenson et al. ( 2020 )'s GIN - based representation learning for nonlinear ICA ( using the nonlinear independent component analysis technique developed in nonlinear - flow network ( NIT ). They show that the representation learned by GIN is not disentangled enough to identify informative latent variables. Instead, based on NIT, they propose to use the mutual information between each learned latent variable and the auxiliary variable to correctly identify the relevant informative latent variable. They further show the advantage of their method on various downstream tasks including classification, outlier detection and adversarial attack defence on both synthetic and real data."
SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"The paper proposes two new pooling methods based on the classical lifting scheme from signal processing. The proposed method, named LiftPool, consists of two pooling layers, namely LiftDownPool and LiftUpPool. In LiftPool the feature map is first down - pooled, and the pooling function is invertible. Then, the corresponding up - pooling layer is performed to generate a refined upsampled feature map. Experiments show that the proposed methods achieve better results on image classification and semantic segmentation, using various ConvNet backbones."
SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"This paper proposes a binary embedding method to transform a high - dimensional dataset T ⊆ R into binary sequences in the cube {±1 }, when T consists of well - spread ( i.e. non - sparse ) vectors. The embeddings are applied via a stable noise - shaped quantization scheme to Ax where A A is a sparse Gaussian random matrix. This method preserves the distance - preserving properties of binary codes, e.g., Euclidean distances among the elements of T are approximated by the `1 norm under a fast linear transformation. The proposed method is both fast and memory efficient, with time complexity O(m^2 log n ) and space complexity $ m$, depending on the number of vectors. It is shown to preserve the pairwise `2 distances. Further, it is proved that the method is accurate and its associated error is comparable to that of a continuous Johnson - Lindenstrauss embedding."
SP:f65e229bca3904095743e7a501b1083cc60f1e22,"The paper proposes that plasticity rules in neural networks can be learned by following Gradient Descent ( GD ) on the rule parameters. The authors provide both theoretical and empirical support for this hypothesis through experiments. In particular, the authors first demonstrate that GD recovers the Perceptron algorithm and multiplicative weights algorithm in the RNN with no hidden layer. Then, they show that GD on the plasticity rule recovers ( and improves upon ) the perceptron algorithm, and finally, they argue that applying GD to learning rules is biologically plausible, in the sense that it is plausible to learn over evolutionary time. In the last experiment, the classifiers learned using GD exhibit surprising levels of tolerance to adversarial perturbations."
SP:f435530146fa975cb27cd375a857df9bcbd87682,"This paper proposes a new method for the task of visual question generation ( VQG ), namely, generate human - like questions from an image and potentially other side information ( e.g. answer type or the answer itself ). In particular, it aims to ask the right visual questions with Double Hints textual answers and visual regions of interests, effectively mitigating the existing one - to - many mapping issue. To this end, it develops a simple methodology to self - learn the visual hints without introducing any additional human annotations. To capture the sophisticated relationships between visual objects in an image, they propose a new double - hints guided Graph-to - Sequence learning framework, which first models them as a dynamic graph and learns the implicit topology end- to - end, and then utilize a graph to sequence model to generate the questions with double hints. Their experiments on VQA2.0 and COCO - QA datasets demonstrate that their proposed model on this new setting can significantly outperform existing state - of - the - art baselines by a large margin."
SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,"This paper studies non - monotonic test performance of generalization algorithms, such as neural networks and linear regression, in terms of the quantities such as the sample size and the model size. This phenomenon is referred to as “ double descent ”, and has raised questions of if we need to re - think our current understanding of generalisation. In this paper, the authors study whether the double descent phenomenon can be avoided by using optimal regularization. Theoretically, they prove that ( 1 ) for certain linear regression models with isotropic covariates, optimally - tuned $ \ell_2 $ regularization achieves monotonicity as we grow $ |k$-samples, and ( 2 ) regularization can mitigate double descent for more general models, including neural networks. The last two results above highlight the importance of the form of the regularizer, which leads to the open question : “How do we design good regularizers which mitigate or remove double descent? ”"
SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,"The paper proposes a new neural network, called Spatial dependency networks ( SDN ), to improve generative modeling by better exploiting spatial regularities and coherence in images. The feature maps at each level of a deep neural net are computed in a spatially coherent way, using a sequential gating - based mechanism that distributes contextual information across 2 - D space. The proposed SDN is tailored to image generators that operate in a non - autoregressive way, i.e. synthesize all pixels ‘ at once ’. The goal of the paper is to improve the density estimation of a variational autoencoder ( VAE ), where the state - of - the - art among the models within the same class. Experiments are conducted in two different settings : ( 1 ) density modeling, where SDN improves density estimation over a baseline convolutional layer in a hierarchical VAE, and ( 2 ) learning disentangled representations in a vanilla VAE.    The main contribution of this paper is the introduction of a novel spatial dependency layer suitable for deep neural networks that produce images – image generators, and the main motivation behind introducing it is its application to VAE image decoders. The accompanying source code is given at."
SP:db91512a90e75675af03c2f197751c8526d6f5e9,"This paper proposes a simple extension of the BCQ algorithm to offline RL, Expected - Max Q - Learning ( EMaQ ), by removing the heuristic perturbation network and replacing it with the expected - max Q - learning operator, which is more closely related to the resulting practical algorithm, BCQ, in two ways : 1 ) EMaq explicitly considers the number of actions, and 2 ) it derives new sub - optimality bounds, which can serve as a novel measure of complexity for offline RL problems, in the setting of D4RL ( Fu et al., 2020a ). In the offline RL setting – the main focus of this work – EMa Q outperforms prior state - of - the - art offline RL algorithms, including BCQ and SAC, and is competitive with Soft Actor Critic ( SAC ) in the online RL setting, and surpasses SAC in the deployment - efficient setting. The key contributions of the empirical findings are demonstrating the importance of careful considerations in the generative model design for estimating behavior policies, and an intuitive notion of complexity."
SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"This paper proposes a bilevel optimization framework where the inner optimizer is SGD, and the outer optimizer performs batch selection to improve model fairness. The proposed algorithm, which is called FairBatch, implements this optimization and supports three prominent fairness measures : equal opportunity, equalized odds, and demographic parity. Moreover, it is compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes. The authors conducted both on synthetic and benchmark real data and demonstrate that it can achieve comparable ( or even greater ) performances against the state of the arts."
SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"The paper studies the Lipschitz constants of deep neural networks, and in particular, the monotone DEQ, a recently proposed class of impicit layer networks that can be viewed as representing an infinitely - deep network.   The paper first shows that the output - output and weight - output of monDEQs satisfy certain conditions, i.e., that the strong monotonicity parameter ( m ) of the network is bounded as a function of the weight, and that this bound can be used to derive PAC - Bayesian generalization bounds that do not depend exponentially on the network's depth. Next, the paper shows how to use these bounds to derive simple yet tight bounds that are small relative to those for comparable standard DNNs, e.g., comparable to those derived by AutoLip and SeqLip for the single - and multi - convolutional MonDEQ, as well as a generalization bound that is not subject to the exponential dependence on network depth."
SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,"This paper proposes a unified view on two settings : imitation learning and goal - conditioned reinforcement learning. In imitation learning, a state - action distribution matching agent's and expert's distribution is defined by sampling states that the agent is currently not likely to visit and using a value - function to guide the agent towards those states. In goal conditioned RL, states that are visited by the agent are sampled from a distribution that is similar to the one held by the expert. The value function and density estimate are trained using self - supervised roll - out alone.   The paper proposes to extend UVD to match a distribution of expert states and introduce Value Density Imitation Learning ( VDI ). VDI is motivated to circumvent the problem of sparse rewards while addressing hindsight bias in stochastic domains. The paper shows that VDI achieves state - of - the - art results on a common benchmark."
SP:d57550b2f323b356d7e609acc35ee33039f376b4,"This paper proposes a variational multi - task learning ( VMTL ) framework for simultaneously learning multiple related tasks. In particular, the paper proposes to use Gumbel - softmax priors to condition the prior of each task on related tasks ; each prior is represented as a mixture of variational posteriors of other related tasks and the mixing weights are learned in a data - driven manner for each task. The posteriors over representations and classifiers are inferred jointly for all tasks and individual tasks are able to improve their performance by using the shared inductive bias. Experiments are conducted on four benchmark datasets and the proposed method consistently outperforms the state - of - the - art methods."
SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"This paper proposes a systematic and unified benchmark, Long - Range Arena ( LRA ), specifically focused on evaluating model quality under long - context scenarios. The proposed LRA is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual - spatial reasoning. The authors evaluate 10 well - established long - range Transformer models ( Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers ) on their newly proposed benchmark suite. For the first time, they conduct an extensive side - by - side comparison of 10 recently proposed efficient Transformers models. The experimental results show that these tasks are very challenging even for long - reach Transformers. The overall results shows that there is no one - size - fits - all solution and trade - offs have to be made in terms of model quality and speed / memory."
SP:e12e410c3335b76133ceda4c865b244fbbab8580,"This paper proposes a language - agnostic code summarization model, named GREAT, which jointly learns on Context and Structure of source code. The key difference from previous work is that GREAT shares only source code and features that can be computed directly from the AST, while the previous work shared the AST and its parsed abstract syntax tree. GREAT is state - of - the - art on monolingual code summarisation in all five programming languages considered in this work, and it is the first multilingual code summarizer model. Experiments show that jointly training on non - parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low - resource languages. However, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code."
SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"This paper proposes a reinforcement learning approach to audio - visual ( AV ) navigation, where an agent is tasked with finding a waypoint in an 3D environment that consists of ( i ) the current detected waypoint, ( ii ) a set of sounds that the agent is likely to hear in the environment, ( iii ) a memory of the detected waypoints, and ( iv ) a spatial representation of the sounds. The key idea of the approach is to dynamically set the waypoints based on a reward function that is learned using reinforcement learning. The rewards function is trained using a loss function that penalizes the agent for not reaching the waypoint when it arrives at it. The acoustic representation is also trained using an adversarial loss. The approach is validated on two datasets : Replica and Matterport3D. The results show that the proposed approach outperforms the state - of - the - art for AudioGoal navigation by a substantial margin, and the experiments reveal that learning the links between sights, sounds, and space is essential for audio - video navigation."
SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"This paper studies the effect of initial weight perturbations on the convergence of convolutional neural networks to solutions in Conway's Game of Life. The authors consider a simple update rule, the update rules of which can be implemented efficiently in a small CNN, and train CNNs on the following task : predict n steps in the two - dimensional cellular automaton, and then update the parameters of the CNN accordingly. The goal is to train a network that converges quickly to a solution given a fixed initial weight. To do so, the authors first perturb the weights of all cells in the CNN by randomly changing one of them ( i.e., a sign change of $ -1 $ ) and observe that their converged solution is sensitive to this perturbation. Next, they test whether the convergence rate depends on $ d$, a probability measure that indicates how likely it is that each cell in a given cell is alive or dead when the next example is taken. Surprisingly, they find that training the CNN with $ d0$ significantly increases the chance of convergence. The results are consistent with the lottery ticket hypothesis.  "
SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"This paper proposes a semi - supervised learning ( SSL ) method based on BatchMean - Triplet loss. It is motivated from the observation that the inputs having the same label should have the similar model outputs, i.e., the outputs from the same class should be close to each other. The proposed method considers not only the perturbed inputs but also the similarity among the inputs that have same label. It introduces a new objective function, dubbed Batch mean - triplet loss, which has the advantage of being computationally efficient while taking into account all input samples. The extensive experiments show that this method exhibits good performance and achieves state - of - the - art results across many standard SSL benchmarks."
SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"This paper studies the problem of few - shot meta - learning, i.e. learning a small classifier from a small set of examples, by meta - training the classifier on large amounts of static data from a set of previous tasks. The paper considers the problem in the setting of online incremental learning, where data for each new task arrives incrementally, and the task - specific learning rate is controlled by a scaling rule that scales with the number of shots. Motivated by the challenges posed by the variable nature of the data in sequential learning, the paper proposes a new approach to adapt the meta - learner to learn from many - shots learning at the start, to zero - shot learning towards the end, and to varying amounts of data depending on the sequence of tasks. In particular, it proposes a variable - shot version of the standard incremental learning algorithm, which can be derived from the standard many - shot method ( Finn et al., 2019 ), and which is adapted to the sequential learning setting by taking into account the fact that each task comes with a different number of "" shots "". Then, the authors propose a modification of the regularized incremental learning method, by adapting the learning rate ( gradient clipping ) to be adaptive to the amount of data ( up to a fixed amount ) that arrives at each step of the learning process. Finally, they propose a deep neural network - based adaptation method that combines the adapted learning rate and scaling rule, and show results on two online image classification problems consisting of sequences of classification tasks and one online regression problem."
SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"This paper presents a series of experiments to test the Transformer - type models'sensitivity to word position and structure perturbations with three different permutations of the input, aimed at understanding how much the model is sensitive to global and local language structure in a sentence. Each of the three tests involves swapping two words in the sentence and comparing the representations from perturbed sentences against the original one. The paper also connects these tests to the attention mechanism of a Transformer model by relating the syntactic distance between the words to syntactic   distance between two words. Results from the three probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. In particular, sensitivity to local phrase structure increases along deeper layers. This is at least partly explained by larger attention weights between syntactically distant words, the paper suggests."
SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"This paper studies the few - shot image synthesis task for GAN with minimum computing cost. The authors propose a light - weight GAN structure that gains superior quality on 1024 x 1024 resolution. The model converges from scratch with just a few hours of training on a single GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute their work, a skip - layer channel - wise excitation module and a self - supervised discriminator trained as a feature - encoder. They provide 13 datasets covering a wide variety of image domains. They show their model’s superior performance compared to the state - of - the - art StyleGAN2, when data and computing budget are limited. Meanwhile, self - supervision on D has been shown to be an effective method to stabilize the GAN training as studied in Tran et al. ( 2019 ), but the auxiliary self - supervision tasks in prior works have limited using scenario and image domain. Moreover, prior works only studied on low resolution images, and without a computing resource limitation. The performance on art - painting datasets, FFHQ and FFHQ, even compared to a baseline model, shows a downgrade in performance."
SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"This paper proposes a specialised specialised dual solver for neural network bounding. The main motivation is that the existing solvers are often too loose to verify more challenging properties of the neural network. This lack of tightness is linked to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise linear activations exists, it comes at the cost of exponentially many constraints and thus currently lacks an efficient customised solver. This paper alleviates this deficiency via a novel dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. The proposed method recovers the strengths of the newly introduced relaxation in the dual space : tightness and a linear separation oracle. At the same time, it shares the benefits of previous dual approaches for weaker relaxations : massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time."
SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"This paper proposes a novel method to augment pre - trained language models with concept - aware language models to improve their generalization ability to tasks that require commonsense reasoning. The method consists of two parts : ( 1 ) A concept aware language model ( CALM ), which is trained by iteratively adding new tokens and sentences to the corpus of text - to - text models, and ( 2 ) a contrastive loss that encourages the model to discriminate between representations that are related to a certain class of concepts ( e.g., pairs of words or concepts ), and that are difficult to understand ( i.e. common sense examples ).   The CALM model is trained incrementally by fine - tuning on a small number of steps ( 20 K steps ) on each of four CommonsenseQA datasets, and on the dataset of commonsense - related NLG called CommonsenseGeneration, where it is trained alongside a BERT - like model and a span - based model ( T5 model ). The model is further fine - tuned on PIQA, ANLI, CATER, PEARL, and COMMONSHEUSQA using a different extension of CALM. Finally, CALM is also trained on a dataset of unlabeled text generated from a Commonsense-related dataset called CommonsensGenerated by Language ( CGC ). CALM consistently outperforms T5 - base and other baseline methods on all the datasets. The performance gain is even larger when the size of the model is increased ( from T5 to larger model ) on CGC."
SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"This paper proposes a neural network that learns to segment 3D physical objects from video using self - supervised self - supervision. The model is called POD - Net and it is designed to solve the problem of unsupervised physical object discovery. The goal is to discover 3D physically - related 3D objects from a video that are observable observable and partially occluded, of varying sizes, by using multi - scale pixel cues and physical motion cues to learn segmentation. The key insight is that by using motion as a grouping cue to constrain the learning of object segmentations and representations, it achieves better image segmentation and learns scene representations that are useful for physical reasoning. The discovered object properties can also be used to reason about physical events. The experiments are conducted on synthetic and real scenes."
SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"This paper proposes Increasing Margin Adversarial ( IMA ) training to increase the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness during training. The IMA method is evaluated on six publicly available datasets ( including a COVID-19 CT image dataset ) under strong 100 - PGD white - box adversarial attacks with different noise levels ( L - inf norm ). The results show that IMA has the strongest overall performance and MMA is slightly better than IMA. However, the proposed method significantly improves classification accuracy on noisy data while keeping a relatively high accuracy on clean data. The authors hope that this approach may facilitate the development of robust DNN applications, especially for robust classification on COVID - 19 diagnosis using CT images."
SP:276ffd59fbf49e3ee02756da8920218102214917,This paper proposes a method for knowledge distillation ( KD ) by projecting the supervision signals of a teacher model into the student ’s parameter space. The projection is implemented by decomposing the training objective into local intermediate targets with approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optima. Experiments on both image and text datasets show that the proposed ProKT consistently achieves the state - of - the - art performance comparing to all existing KD methods.
SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"This paper proposes a channel - wise pruning method for CNNs. It uses a hyper structure network ( HSN ) to capture inter - channel and inter - layer relationships. An architecture vector can be generated from HSN to select a subnetwork from the original model. At the same time, the HSN can be updated by the gradients from them. The authors identify the problem of FLOPs constraint ( bias towards the later layers ) and learnable layer - wise scaling factors to address it. Experiments on CIFAR-10 and ImageNet show that the proposed method is competitive with state - of - the - art methods."
SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"This paper proposes a method for theorem provers to explore and learn from a corpus of lower - order logic proofs without the need of human supervision. The method relies on an exploration mechanism inspired by DeepHOL Zero ( Kolotouros et al., 2019 ), a RL - based algorithm trained to generate proofs from proofs given by humans. The exploration strategy is based on a tf - IDF ( term frequency - inverse document frequency ) based lookup in a deep RL framework. The paper shows that a theorem prover trained with this exploration strategy but no human supervision outperforms a comparable algorithm trained only on human proofs. It also approaches the performance of a system trained by a combination of imitation and reinforcement learning.  "
SP:88209417a8ad07e6103084e41709be900303ce5f,"This paper proposes MODALS ( Modalityagnostic Automated Data Augmentation in the Latent Space ) to augment data for any modality in a generic way. MODALS exploits automated data augmentation to fine - tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Through comprehensive experiments, the authors demonstrate the effectiveness of MODALS on multiple datasets for text, tabular, time - series and image modalities using various deep learning models. The proposed modality - agnostic transformation operators are trained with additional loss metrics and hard example augmentation techniques."
SP:6d84670d321b0d584b097c630574bd748e85c9a2,"This paper studies the convergence of three - layer neural networks with fixed - width neural networks in the mean field regime, which is defined as those regimes in which the number of neurons in a neural network tends to infinity and the learning dynamics tends to a non - linear and nontrivial dynamical limit. The authors first develop a rigorous framework to establish the mean - field limit of such networks under stochastic gradient descent training. They propose the idea of a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove a global convergence guarantee, which, unlike previous works on two - layer networks, does not rely critically on convexity and instead is motivated by a universal approximation property, natural of neural networks, which importantly is shown to hold at any finite training time ( not necessarily at convergence ) via an algebraic topology argument."
SP:b90f893f927db9c439595fd119a565cf43c971f4,"This paper proposes learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to “ what if ” outcomes : Given the current history of observations, what would happen if we took a particular action? To learn these costbenefit tradeoffs associated with the expert ’s actions, they integrate counterfactual reasoning into batch inverse reinforcement learning. This offers principled way of defining reward functions and explaining expert behavior. It satisfies constraints of real - world decision - making, where active experimentation is often impossible ( e.g. in healthcare ). By estimating the effects of different actions, it provides interpretable explanation to tackle the off - policy nature of policy evaluation in the batch setting. It can accommodate settings where the expert policies depend on histories of observations rather than just current states."
SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"This paper investigates the role of explicit morphological information in graph - based continuous control methods, such as SMP and NERVENET, in improving performance in incompatible multitask continuous control settings. Specifically, the authors consider a setting where the state and action space dimensions are not the same across tasks. Graph Neural Networks ( GNN ) are one way to address incompatible environments, because they can process graphs of arbitrary size. They also allow practitioners to inject biases encoded in the structure of the input graph. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, this paper proposes AMORPHEUS, a transformer - based approach to improving performance of GNN - based methods that use the morphology information to define the message - passing scheme. The ablations ablations are conducted : 1. Numerical ablations that show that the existing methods exploiting the morphology does not improve their performance ; 2. Results ablation that show GNN-based methods that exploit the morphology in the graph does improve the performance. 3. Experimental results on incompatible MTRL benchmarks that provide evidence against the belief that these methods improve performance by exploiting the morphological structure encoded in graph edges."
SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"This paper proposes a method called MoVie, short for Modulated Convolutional bottlenecks, for the task of visual counting, which aims to predict the number of occurrences given a natural image and a query ( e.g., a question or a category ). Different from previous works that perform explicit, symbolic reasoning, counting is done implicitly and holistically in MoVIE and only needs a single forward - pass during inference. The proposed method significantly outperforms state - of - the - art on three major benchmarks in visual counting : HowMany - QA, Tally -QA and COCO. The strong performance helped them secure the first place of 2020 VQA challenge when integrated as a module for ‘ number ’ related questions in generic VQC and GQA. Finally, the authors provide evidence that modulated convolutions can serve as a mechanism for reasoning tasks beyond counting and could be useful for counting."
SP:c64e77507e562f236cb69361b22fb1a7951ffb22,"This paper proposes a model - targeting poisoning attack that provably trains a classifier based on online convex optimization based on a loss function that is strongly convex for any Lipschitz and convex loss function. The paper also provides a lower bound on the number of poisoning points that are needed to reach the classifier of interest, which is inversely proportional to the square root of the total number of poison points. This lower bound is used to estimate the optimality of model - targeted poisoning attacks and also indicates the intrinsic hardness of attacking different targets.   The paper provides a general poisoning framework with provable guarantees to reach any achievable target classifier, as well as a proof that the model induced by the attack converges to the one obtained by minimizing the loss function given that the loss is convex. Experiments are performed on MNIST, CIFAR-10, and ImageNet, and show that the proposed method performs better than the state - of - the - art attacks in terms of attack success rate and distance to the target model."
SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"This paper introduces a new point - based classification and segmentation model, called BiPointNet, for real - time point cloud applications that run on edge devices. It is based on PointNet++, PointCNN, DGCNN, and PointConv, and it outperforms these existing binarization methods by convincing margins, at the level comparable to the full precision counterpart. The authors claim that the performance drop of binarized models for point clouds mainly stems from two challenges : aggregation - induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale - sensitive structures. With theoretical justifications and in - depth analysis, it introduces Entropy - Maximizing Aggregation ( EMA ) to modulate the distribution before aggregation for the maximum information entropy and Layer - wise Scale Recovery ( LSR ) to efficiently restore feature representation capacity. Extensive experiments show that Bi pointNet outperforms its baselines and it is generic, guaranteeing significant improvements on various tasks and mainstream backbones. Moreover, it gives an impressive 14.7x speedup and 18.9x storage saving on real - world resource - constrained devices."
SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"This paper presents three variants of Transformer models with memory modules : MemTransformer, MemCtrl and MemBottleneck. The Mem modules store non - general purpose tokens that can be used as placeholders to process global or copy of local representations. Experiments on various tasks such as language modeling and machine translation show that the presence of memory improves the performance of these models. The paper also presents an attention map showing that the memory modules can control the memory access and write operations."
SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,"This paper proposes Prototypical Contrastive Learning ( PCL ), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low - level features for the task of instance discrimination, but more importantly, it encodes semantic structures discovered by clustering into the learned embedding space. Specifically, the authors introduce prototypes as latent variables to help find the maximum - likelihood estimation of the network parameters in an Expectation - Maximization framework. They iteratively perform E - step as finding the distribution of prototypes via clustering and M -step as optimizing the network via contrastive training. The authors propose ProtoNCE loss, a generalized version of the InfoNCE losses, which encourages representations to be closer to their assigned prototypes.   Experiments on multiple benchmarks demonstrate the advantage of PCL over the state - of - the - art instance - wise contrastive learners methods."
SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,"This paper proposes Orthogonal Multi - Path ( OMP ) blocks to make neural networks more robust to adversarial attacks. The idea is to embed orthogonal multiple paths ( MPs ) into a neural network to enhance the robustness. The proposed OMP blocks are the following steps :   - At each layer : - At the next layer : 	 - The feature prediction layer ( called "" follow - up layer "" ) tries to learn features that are robust to all paths. The parameters of these follow - ups layers ( called backpropagation layers ) are the same across the layers. 	- At the top layer : The prediction layer tries to get features that fit all paths by minimizing the distance between the current layer and the previous one ( called forward learning ). This step is repeated until all paths are found. - The following OMP block ( at layer 3 ) is the one that ensures that the predicted features are the ones that are the most appropriate for all paths at that layer. The path - centricity constraint is imposed on the weights of the following layers that are used to compute the prediction error.   The authors conduct experiments on defending white - box and black - box PGD attacks under CIFAR10 and VGG16 to demonstrate the effectiveness of their approach. They achieve better results than the state - of - the - art defense methods."
SP:776df66274ed12449fde8dcef873a593980f397c,"This paper proposes a self - supervised graph attention network ( called SuperGAT ) for noisy graphs. The proposed network learns to self - supervise based on two characteristics of the graph : Homophily and average degree. The authors analyze GO and DP on label - agreement and link prediction tasks, and find that GO is better at label agreement and DP is worse at link prediction. They also propose recipes to design graph attention concerning homophily, and their models designed by recipe show improved performance over baselines. Finally, the authors conduct experiments on 17 real - world datasets and the recipe generalizes across 15 datasets."
SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,"This paper proposes a new dialogue system for medical automatic diagnosis ( DSMAD ), named Introspective Diagnosis System ( INS - DS ). It consists of two modules : an inquiry module for proposing symptoms and an introspective module for deciding when to inform a disease. The inquiry module is responsible for selecting the most valuable symptom to inquire about, while the introspection module intervenes the potential answers of this inquiry to decide whether to inquire the symptom or inform the disease. Besides, it also proposes two evaluation metrics, i.e., reliability and robustness, to measure the reliability of DSMAD. The reliability metric measures the diagnostic accuracy while the robustness metric is based on the extent to which the system is able to cope with noise during the interaction process with the patient. Extensive experimental results demonstrate that the proposed system achieves the new state - of - the - art performance in various settings. The extensive experimental results also demonstrate that it achieves superior performance to other DSMAD baselines."
SP:10ae09d90d465125433a9b4f15b1405ab017920d,The paper proposes an adaptive confusion energy - based regularization method for fine - grained visual classification ( FGVC ) and long - tailed classification ( LTC ) based on the Batch Confusion Norm ( BCN ). The BCN term is designed to account for the inter - class similarities and intra - class variations that result from the long - tail distribution of FGVC and the natural world distribution of LTC. The authors also propose an approximation of the BCN so that gradient backpropagation can be easily carried out to learn the regularizer. The proposed BCN is evaluated on several datasets and achieves state - of - the - art results on several FGVC datasets.
SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"This paper proposes a method, Approximate Variational Reward Imitation Learning ( AVRIL ), for learning an approximate variational posterior distribution over the reward in an imitation learning ( IL ) framework, for scaling Bayesian IRL to environments beyond the small - tabular setting that is typically used in the Bayesian Inverse Reinforcement Learning ( BIRL ) literature. The main idea of the method is to jointly learn an approximate posterior over the latent reward that allows for approximate reward inference using an arbitrarily flexible class of functions, in any environment, without costly inner - loop operations, and importantly entirely offline. This leads to the formulation of the algorithm, which is shown to recover a reward that is informative for retraining policies and also offers useful insight into the preferences of the demonstrator. The method is applied to real medical data alongside classic control simulations to demonstrate Bayesian reward inference in environments that are beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms, in both real and toy settings."
SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,"This paper introduces a method for learning counterfactual belief distributions in environments that are partially observable ( POMDPs ). The method, called Learned Belief Search ( LBS ), is based on applying an auto - regressive belief distribution that is learned via a supervised learning task similar to that used in Reinforcement Learning ( RL ). However, unlike in RL, LBS does not require the agent to maintain an exact belief distribution. Instead, it relies on augmenting the belief distribution with simulated data from a simulator that the agent will visit when it arrives at test time based on a blueprint policy that is trained on the simulated data. In particular, the public - private model ( PPM ) of the blueprint policy is used in LBS to efficiently evaluate the performance of the policy during rollouts. The PPM is also adapted to be applicable to cooperative settings such as multi - agent games such as Hanabi. LBS achieves strong performance on the benchmark domain of Hanabi, while using a fraction of the computational cost compared to the baselines."
SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"This paper proposes a new tree search method, titled “ Shoot Tree Search ” ( STS ), which aims to address the dilemma between depth and breadth search in large state spaces. The core improvement is multi - step expansion, which may be used to control the depth of search and inject into planning more randomness via random multi -step expansions. The authors empirically verified the efficiency of this extension in many challenging scenarios, and argue that it could be included in a standard MCTS toolbox. The method could also be combined with different statistical tree search methods, where statistics other than the expected value are stored and updated."
SP:5efc271ccc555fd9aa542548838170bd4c98e957,"This paper proposes a new pre - training methodology called LIME ( Learning Inductive bias for Mathematical Reasoning ) to learn inductive bias from datasets. It is inspired by Peirce ’s view that deduction, induction, and abduction form an irreducible set of reasoning primitives, and proposes three synthetic tasks that are intended to require the model to have these three abilities. The paper specifically design these synthetic tasks in a way that they are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new methodology called “ LIME ” ( learning inductive biases in datasets ). Models trained with LIME significantly outperform vanilla transformers on three mathematical reasoning benchmarks. Notably, LIME requires only a small fraction of the computation cost of the typical downstream task, unlike being the dominating factor in previous pretraining methods."
SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"This paper studies the inductive bias of weight normalization ( EWN ) for smooth homogeneous neural networks trained on exponential or cross - entropy loss. EWN is defined as an extension of normalized gradient descent with an adaptive learning rate that encourages weight updates in the radial direction. This paper shows that EWN induces asymptotically sparse solutions, and shows that it has a faster convergence rate than unnormalized or standard weight normalized networks. Experiments on simple data sets and architectures support the claim on sparse EWN solutions. This demonstrates its potential applications in learning prunable neural networks."
SP:c71f9d2a602516865a0b103028186e83b52e5f00,"This paper proposes a GAN framework that dynamically adds additional discriminators to the generator to prevent mode collapse. The authors claim that mode collapse is caused by catastrophic forgetting by the discriminator, where some modes of the target distribution are ignored by the generator. Motivated by this observation, they introduce a training procedure that dynamically spawns additional discriminator to remember previous modes of generation. They show that their training scheme can be plugged - in to existing GAN frameworks to mitigate mode collapse, improve standard metrics for GAN evaluation, and coverage of the generated samples."
SP:52c48198c95826e042f9e5a512ef3265daaff882,"This paper proposes an approach to regularize BERT by pruning its attention heads based on a proxy score for head importance. The proposed approach AUBER leverages reinforcement learning to automatically prune attention heads from BERT. Unlike existing heuristics such as rule - based or gradient - based pruning methods, which use a heuristic to determine the order in which attention heads should be pruned based on the current state of the classifier or the current query, AUBERSPIRN automatically prunes attention heads by using a policy that is learned via reinforcement learning. The policy is trained using a combination of supervised learning and reinforcement learning where the reward for successful pruning is a reward associated with the selected pruned attention head, and the reward is a score that is proportional to the importance of each attention head in the pruned set. Experiments are conducted on CIFAR-10, MS - NLG, and MNLI datasets to demonstrate the effectiveness of the pruning approach."
SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"This paper proposes a new transfer learning method for transfer learning between two different robotics domains : vision and motion. Different from prior work, the key novelty of the proposed method is that it does not require paired data ( as in prior work ) to learn the transfer function. Instead, it relies solely on unpaired and randomly collected data from both domains to learn transfer functions. The key idea is to use dynamics cycles that align dynamic robot behavior across two domains using a cycle - consistency constraint. Once this is done, one can directly transfer a policy trained on one domain to the other without any additional fine - tuning on the second domain. The method is evaluated on a variety of tasks in simulation and on real robots. In simulation, the authors adopt the MuJoCo ( Todorov et al., 2012 ) physics engine and show that their model can find correspondence and align two domains across different modalities, physical parameters, and morphologies. In real robot experiments, the method is able to align dynamic state - action trajectories of a simulated arm without paired data and achieved similar results as when it does with paired data."
SP:006434d56992836ab9420d7d4215bc70664de304,"The paper studies the problem of explainability of the model - agnostic Shapley value function. The main motivation for this paper is that existing work assumes that the model ’s features are “ uncorrelated ” with respect to the data manifold, in contrast to the more commonly used on - manifold approach of EWC. The paper proposes two methods to overcome this assumption : ( 1 ) generative modeling based imputation of the features ; ( 2 ) learning the value function directly from the data. In experiments, the paper shows that both methods fail to explain well. In particular, the generative method fails to explain the high - dimensional features while the off - manifold method ( e.g. EWC ) does not.   The paper then presents experimental results showing that “ off - manifold ” value functions may : ( i ) give rise to incorrect explanations, ( ii ) hide implicit model dependence on sensitive attributes, and ( iii ) lead to unintelligible explanations in higher - dimensional data."
SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,"This paper proposes a method for learning an opponent model conditioned only on the local observations of the agent under control. The opponent model is modeled using variational autoencoders ( VAE ), where the encoder is trained to reconstruct the opponent's local embeddings based on the observed world state, chosen actions, and received rewards. In contrast to existing opponent modeling approaches, which require access to opponent trajectories during execution, this paper proposes to only use the local information to augment the decision policy of the controlled agent. The decision policy is made using reinforcement learning ( RL ) based on a learned embedding from the opponent model. The proposed method, called Local Information Opponent Modelling ( LIOM ), is evaluated in two environments : the multi - agent particle environment ( MPE ) and level - based foraging ( LBF ). In the MPE, the average returns are comparable to those achieved by an ideal baseline which has full access to the opponent’s trajectory. In LBF, the same RL algorithm achieves higher average returns when combined with the learned opponent embedding than without, and in some cases the average return is slightly higher. Similarly, the ablation study shows that a relatively small decrease in performance of LIOM is observed in the LBF."
SP:c239bc531bcf7293032748af29a1b786e9d893dd,"The paper proposes a new contrastive learning framework called Consistent Contrast ( CO2 ). CO2 is based on the instance discrimination framework of Momentum Contrast ( MoCo ). In contrast to MoCo, CO2 labels crops from the same image as positives, and crops from other unlabeled images as negatives. The authors argue that MoCo does not take into account the similarity between the query crop and each crop from other images, while some of them may belong to the same semantic class as the query. To address this issue, they introduce a consistency regularization term into the current contrastive training framework, which introduces a consistency corresponding to each query crop to each labeled image as “ unlabeled ”. The paper claims that CO2 improves MoCo by 2.9% on top - 1 accuracy and 3.8% and 1.1% top - 5 accuracy on 1% and 10% labeled semi - supervised settings, respectively. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC."
SP:d18bab21790713e2facb053c47298fc9079ab783,"This paper studies last - iterate convergence of Optimistic Gradient Descent Ascent ( OGDA ) and Optimistic Multiplicative Weight Update ( OMWU ) in constrained bilinear games over the probability simplex. In particular, this paper focuses on the setting where the objective function is a simplex, and the learning rate is a function of the smoothness of the function. Under this constrained setting, the authors first show that under a unique and general - purpose assumption that the loss function is isotropic and that the equilibrium function is strongly convex, OGDA exhibits linear last - iteration convergence rates with a constant learning rate whose value is a universal constant. Then, under the same general assumption, they provide experimental results to support their theory for matrix games, where they show that OMWu converges faster than OGDA even without the unique assumption."
SP:bbc7f77308b298c332a39747f693bc396f00a89f,"This paper proposes Federated User Verification ( UV ) models in federated setup, where each user has access to the data of only one class and user embeddings cannot be shared with the server or other users. To address this problem, this paper proposes a framework for private and secure training of UV models, called FedAvg. In FedAvg, users jointly learn a set of vectors and maximize the correlation of their instance embedding with a secret user - defined linear combination of those vectors, which is then used to train the model using a loss function that only uses their own vector. The experimental results for user verification with voice, face and handwriting data show FedAvg performs on par with existing approaches, while not sharing the embedding vectors with other users or the server."
SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"This paper proposes a new method to measure the dimension of class manifolds and multi - way class boundary manifolds in the space of inputs for deep neural network classifiers. It proposes a simple technique to estimate the effective dimension of CMs as well as boundaries between multiple CMs, by computing their intersection with random affine subspaces of varying dimension. The authors provide a theory for the technique and verify that their theoretical predictions agree with measurements on real neural networks. Through extensive experiments, they leverage this method to show deep connections between the geometry of CM, generalization, and robustness. In particular, they investigate how CM dimension depends on 1 ) the dataset, 2 ) architecture, 3 ) training mode, 4 ) dataset size, 5 ) training architecture, 6 ) ensemble size, 7 ) class, 8 ) training set size, and 9 ) model robustness to data corruption. Experiments show that well - performing, robust models have higher - dimensional CMs than worse performing models. Moreover, it offers a unique perspective on ensembling via intersection of CM via the newly proposed method."
SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"This paper proposes Curiosity - Aware Entropy Temperature for SAC ( CAT - SAC ), an extension of the SAC framework that uses the curiosity mechanism to adjust the instance - level entropy of the soft actor - critic policy to encourage exploration in unfamiliar states while capping the entropy in familiar states. The intuition is that an unfamiliar state will have a larger prediction error while familiar states are more likely to have a large prediction error. The curiosity is added to the state prediction error to increase the entropy for unfamiliar states and decrease the target entropy for familiar states, while the temperature is adjusted adaptively based on the instance. The idea is that when the curiosity is high, the state should be explored more, while when it is low, the temperature should be low. The proposed approach is evaluated on the MuJoCo continuous control task and compared with other SAC - based and model - free baselines. The experimental results show that the proposed approach outperforms the other baselines in terms of the number of samples per state, as well as the average reward per state."
SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"This paper proposes a meta - reinforcement learning framework that adapts well to out - of - distribution samples. The proposed framework consists of model identification ( ID ) and experience relabeling ( ERL ). ID provides the meta - RL framework, while ERL provides the training data for adapting the model to new samples. ID adapts the model with gradient descent, and ERL relabels the learned model to generate synthetic data to continue training the policy. MIER is shown to outperform baselines that do not incorporate extrapolation or require a large amount of off - policy data."
SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"This paper proposes a few - shot learning approach to address the problem of overfitting and overfitting in the meta - learning setting due to sampling and label noise. The paper first proposes to view the problem as a gradient noise problem, that is, having few available samples can cause meta - learner to overfit on existing examples ( clean or corrupted ) of an individual task at every gradient step. Then, this paper proposes Eigen - Reptile ( ER ), a method to update meta - parameter through the main direction of historical taskspecific parameters to alleviate gradient noise. Specifically, main direction is computed by a mechanism that takes into account the parameter ’s large size. In addition, Introspective Self - paced Learning ( ISPL ) constructs a plurality of prior models to determine which sample should be abandoned in order to get a more accurate main direction. The effectiveness of ER and ISPL are proved theoretically and experimentally, respectively, theoretically and empirically, on different tasks."
SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"The paper proposes AdvBN, an approach for training deep neural networks that is robust to stylization and corruption induced by changes in the mean and variance of deep image features. The key idea is to adapt the adversarial training by adversarially perturbing these feature statistics, rather than image pixels, to produce models that are robust to distributional shifts. The proposed AdvBN significantly improves the performance of ResNet-50 on ImageNet-C, Stylized - ImageNet, and ImageNetInstagram over standard training practices. The paper also demonstrates that AdvBN can also improve the generalization on semantic segmentation."
SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"This paper proposes a new metric, Variance of Gradients ( VoG ), to measure the variation of gradients ( gradients ) across a set of training examples, and to automatically surface a subset of the most difficult examples for human - in - the - loop auditing. Data points with high VoG scores are far more difficult for the model to learn and over - index on corrupted or memorized examples. It is becoming increasingly important for deep neural networks ( DNNs ) to make decisions that are interpretable to both researchers and end - users. For example, in sensitive domains such as health care diagnostics, self - driving cars, and hiring, providing tools for domain experts to audit models is of upmost importance. The proposed VoG is straight - forward to compute and can take advantage of current best practices of storing multiple checkpoints over the course of training. In practice, a domain expert may choose to compute VoG for a class of particular interest which would further reduce the computational cost. Critically, VoG can be computed using the predicted label, which makes it an unsupervised auditing tool at test time."
SP:074bfacc75837bb19049be8a2890e10de073dd8e,"This paper proposes a method to improve samples from deep generative models by refining them using gradient flow of f - divergences between the real and the generator data distributions. The method is called the Discriminator Gradient f - low ( DGf low ). It is based on a non - linear Fokker - plank equation, which can be simulated by sampling from the equivalent McKean - Vlasov process. The proposed method is applied to GANs as well as VAEs and Normalizing Flows. Results show that it outperforms discriminator - based methods such as DRS and MH - GAN and outperforms DDLS on a variety of datasets."
SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,"This paper presents VECO, a variable encoder - decoder transformer pre - training approach for cross - lingual language modeling tasks. It is motivated by recent advances in transformer - based models for learning multilingual representations, which can be seen as a more general model architecture for understanding and generation tasks. The proposed approach splits the standard Transformer block into several sub - modules trained with both innersequence and cross - sequence masked language modeling, and correspondingly reorganizes certain sub - module for each task during inference. The main contributions of the paper are :   ( 1 ) It proposes a module sharing mechanism between the encoder and decoder modules to allow the two modules to be optimized for inner - sequence generation, and ( 2 ) it suggests ways to reorganize the transformer blocks in the transformer architecture so that different parts of the transformer can be used for different types of tasks. Experiments are conducted on various tasks in the XTREME benchmark, which shows consistent performance gain compared to the state - of - the - art."
SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"This paper proposes an intrinsic reward function for reinforcement learning ( RL ) motivated by auditory event prediction. The idea is to use the errors incurred by a neural network trained to predict auditory events as a reward function to encourage exploration. The neural network is trained using K - means clustering to find clusters of auditory event clusters that are similar enough to be expected to be learned by a discriminator. The discriminator is trained to be able to discriminate between real - world and imagined world events. Then, the RL policy uses the observed errors generated by the neural network as an exploration bonus when training a new RL policy. The method is tested on two environments : audio - visual exploration in the Habitat simulator and active learning in the ThreeDWorld simulator. The experiments show that the method outperforms vision - based reward functions."
SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,"This paper proposes an end - to - end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid overfitting the learnt embedding to labelled data, they take inspiration from self - supervised representation learning by noise - contrastive estimation and extend it to jointly handle labelled and unlabeled data. In particular, they proposed using category discrimination on labelled data and cross - modal discrimination on multimodal data to augment instance discrimination used in conventional contrastive learning approaches. They further employ Winner - Take - All ( WTA ) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabelling data to better predict cluster assignments. They thoroughly evaluate their framework on large - scale multi - lingual video benchmarks Kinetics - 400, VGG - Sound, CIFAR10 and ImageNet, obtaining state - of - the - art results."
SP:4df640f502e88ddba2d7e183625231d70b083e82,"The paper proposes SPML, a weakly supervised segmentation method that can be applied to any semi - supervised metric learning problem, where pixels of the same ( different ) semantics need to be mapped to the same features in the feature space. The paper proposes 4 types of contrastive relationships between pixels and segments capturing low - level image similarity, semantic annotation, co - occurrence, and feature affinity. They act as priors ; the pixel - wise feature can be learned from training images with any partial annotations in a data - driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images. Experiments on Pascal VOC and DensePose demonstrate consistent gains over the state - of - the - art ( SOTA ). The gain is substantial especially for the sparsest keypoint supervision."
SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"This paper proposes a simple but effective distillation strategy for unsupervised learning. The key is that the relationship among similar samples counts and can be seamlessly transferred to the student to boost the performance. The method, termed as BINGO, which is short for Bag of InstaNces aGgregatiOn, targets at transferring the relationship learned by the teacher   within a bag of instances that are likely to be similar. The goal of distillation is to aggregate compact representations over the student with respect to instances in the bag, such that the student's representation becomes closer to the teacher's. Notably, the proposed method achieves new state - of - the - art performance on small scale models, i.e., 65.5% and 68.9% top - 1 accuracies with linear evaluation on ImageNet, using ResNet-18 and ResNet - 34 as backbone."
SP:328866aad6544c81ded8980934df31dc4472435f,"This paper proposes a Simulation - based Inference ( SBI ) method based on GANs that uses an adversarial approach to approximate posterior distributions. The method is named GATSBI ( Generative Adversarial Training for Simulated Inference ). It is shown to amortise across observations, works in high - dimensional posterior spaces, and supports implicit priors. Experiments are performed on two SBI benchmark problems and two high - dimension simulators. On the wave propagation problem, it infers a high -dimensional posterior given an implicit prior, and performs better than a state - of - the - art SBI approach. Similarly, it is shown that it can also perform sequential posterior estimation, i.e., it can refine posterior estimates for specific observations."
SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"The paper proposes a method for the identification and estimation of individualized treatment effects ( TEs ) under limited overlap of treatment samples when the features of the different treatment groups belong to different subsets of the population. The key idea is to use a generative prognostic score ( i.e., a measure of how likely it is that a treatment group's features are similar enough to form a treatment effect ) that can be used to identify individualized individualized TEs. The method relies on a variational autoencoder ( VAE ) that is trained to be identifiably represented as a regularized iVAE, named β - Intact - VAE, to achieve this goal. The corresponding bounds on the TE error are also provided. The proposed method is compared with ( semi-)synthetic methods on ( somewhat synthetic ) datasets."
SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"This paper proposes a formalism and benchmark for autonomous reinforcement learning ( ARL ), including an evaluation of state - of - the - art RL algorithms with explicit emphasis on autonomy, in two different settings : ( 1 ) a continual state - free learning, where the agent learns through its own experience, ( 2 ) an environment that does n’t reset between trials, and ( 3 ) a learning task that requires the agent to interact with the environment in an irreversible way, such that it can not be stopped or changed arbitrarily early in the learning process.    The paper formalizes the problem as an RL problem and provides a concrete and general definition of ARL. It also provides a number of instantiations of the problems that can be instantiated to fit the general framework. The paper introduces a simulated benchmark EARL1 around this framework, containing a set of challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. The experiments show that standard RL methods and methods designed for reset - free RL struggle to solve the problems in the benchmark and often get stuck in parts of the state space, underscoring the need for algorithms that can learn with greater autonomy and suggesting a path towards the development of such methods."
SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"This paper analyzes Graph Neural Networks ( GNN ) for the task of question answering in question answering ( QA ). It is claimed that existing GNN - based QA systems are over - parameterized and over - complicated, and that existing knowledge - aware GNN modules may only be able to perform simple reasoning such as counting. The authors propose a simple soft / hard counter model, which achieves comparable or better experimental results as the GNN-based methods on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets."
SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,"This paper proposes a compression method for Deep Neural Networks ( DNNs ) called “Succinct Compression ” that improves inference performance with compressed neural networks during inference. The key insight of the method leverages the concept of Succinct Data Structures, which supports fast queries directly on compressed representation without decompression. The method operates by proper combinations of newly introduced formulations, Succaption data structures, and carefully - engineered inference. They show that, their method keeps near - optimal compression, and achieves at least 8.7X/11.5X speedup on AlexNet / VGG-16 inference, compared with Huffman Coding. They also experimentally show that their method is quite synergistic with Pruning and Quantization."
SP:94c395afc794a9cc163e362078769ff83f3d20d0,"This paper proposes Network Augmentation ( NetAug ), a training method for improving the performance of tiny neural networks. The idea is to augment the model, rather than augmenting the data, since tiny models tend to suffer from under - fitting rather than over - fitting due to limited capacity. To alleviate this issue, NetAug augments the network ( reverse dropout ) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub - model of larger models to get extra supervision, in addition to functioning as an independent model. The authors demonstrate the effectiveness of NetAug on image classification and object detection by applying it to image classification datasets, achieving up to 2.3% accuracy improvement on ImageNet, and 4.3 % improvement on Cars while adding only 16.7% training cost overhead and zero inference overhead."
SP:9c24549b980e415616f818acbf4cf680ef8edb52,"This paper proposes a GAN model for upsampling dynamic point cloud sequences without point correspondence annotation. The proposed model is called Temporal Point cloud Upsampling GAN ( TPU - GAN ). It has two main components : temporal coherence encoder and masking module. The temporal encoder encodes the time - varying spatiotemporal coherence from the point cloud sequence, which in turn guides the generator to produce temporally coherent output. The masking network upsample points that have a irregular density distribution, which is determined by a point - wise downsampling. Experiments are conducted on two datasets : particles in the fluid dynamical system and human action scanned data. The quantitative and qualitative evaluation demonstrates the effectiveness of the proposed method on upsample task as well as learning time - tracking by temporal discriminator."
SP:67efe60ad37807505369b7852bc0abed29ffdda8,"This paper proposes FP - DETR, a method that fully pre - trains an object detection transformer, which is then fine - tuned for object detection via a task adapter. The proposed method is inspired by the success of textual prompts in NLP, and treats query positional embeddings as visual prompts to help the model attend to the target area ( prompts ) and recognize the object.   Experiments show that FP - DetR achieves better robustness on the COCO dataset, and generalizes better to small - size datasets than state - of - the - art detection transformers."
SP:a1f9897496303984fc7ad469222106b14b4a6233,"This paper proposes FedPAGE, a method to reduce the communication complexity of FedAvg for both convex and nonconvex optimization. In FedAvg, clients run multiple local gradient descent steps before communicating their update to an orchestrating server. In contrast, in this paper, the authors propose to apply the recent optimal PAGE method to the federated averaging in order to achieve fewer communication rounds. The result is claimed to improve the best known result of SCAFFOLD ( Karimireddy et al., 2020 ) by a factor of $ O(N^{(3/4 ) } ) $, where N is the total number of clients, S is the number of iterations, and $ S/\sqrt{N }$ is the target error. The authors also conduct several numerical experiments showing the effectiveness of multiple local update steps in each round."
SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"This paper proposes a new characterization of the decision boundary geometry of artificial neural networks (ANNs ) based on adversarial perturbations. The authors define an adversarial subspace, consisting of all the directions in which the adversarial examples can come from any direction other than the direction of the input. The decision boundary lies in this subspace and the paper analyzes its geometry to characterize the curvature and the distance to the boundary. The analysis is based on the most widely used defense method for defending against adversarial attacks, which is adversarial training, where one incorporates the perturbed examples into the training procedure of the neural network. The new characterization provides new insight into the consequences of this defense method, which quantifies the increase in boundary distance, the redistribution of proximal class labels, and the decrease in boundary curvature. The main conclusion is that increasing the boundary linearity causes increased robustness while increasing the alternative label curvature leads to decreased robustness, which corroborates the claim of Goodfellow et al. ( 2014 ) that adversarial trained neural networks are less linear in decision boundary."
SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"This paper proposes a weakly - supervised contrastive learning approach to learn similar representations within the same cluster and dissimilar representations for data from different clusters using auxiliary information. The auxiliary information comes in the form of hashtags ( for instance, an Instagram image will be semantically more similar with the same hashtags compared to another Instagram image ). The proposed approach consists of two stages : ( 1 ) cluster data according to its auxiliary information and ( 2 ) learn similar representation for similar clusters. Experiments are conducted on learning visual representations from UT - zappos50K, CUB - 200 - 2011, Wider Attribute, and ImageNet-100 datasets. The paper shows that the proposed method outperforms the baselines ( CMC, Prototypical Contrastive Learning, K - means clustering ) and performs better than the CMC method in most of the cases. It is also shown that the method also works well with unsupervised constructed clusters ( Cl - InfoNCE )."
SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"This paper proposes a method to learn algorithms automatically from data by unrolling a classic path - following algorithm, with some components being more flexible and learnable, in order to solve the problem of recovering sparse parameters from observational data. The resulting algorithm, called PLISA ( Provable Learning - based Iterative Sparse recovery algorithm ), is theoretically shown to improve the recovery accuracy by a factor of sqrt(L_\theta ) w.r.t. the true parameter $ \beta$, which is obtained by setting $ \tilde{B}$-sparse $ \phi$-epsilon $ \mathbb{R}^n$ where $ p$ is the number of samples in the training set, and $ \alpha$ and $ -alpha$ are the parameters of the recovery algorithm. The paper also analyzes the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training sets.   The paper contains :   ( 1 ) theoretical analysis of the PLISA algorithm to show that it is generically applicable to a broad class of sparse estimation problems, and ( 2 ) a connection is made between the generalization and algorithmic properties of the unrolled algorithm, which leads to a tighter bound that can explain the empirical observations."
SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"This paper proposes a method for learning in discrete - continuous hybrid action space for DRL agents. The main idea is to learn a compact and decodable latent representation space ( called HyAR ) for the original discrete and continuous action space to capture the dependence between discrete action and continuous parameter via an embedding table and conditional Variational Auto - Encoder ( VAE ). In addition, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Then, the agent learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. The experiments evaluate HyAR in a variety of environments and the results demonstrate the superiority of HyAR when compared with previous baselines, especially for high - dimensional action spaces."
SP:5128bf712f6b197de113c7a371b4bec36f978eca,"This paper proposes SGEM, Stochastic Gradient with Energy and Momentum to solve general nonconvex stochastic optimization problems. It is based on the AEGD method that originated in the work of ( Xie et al., 2021 ). The main difference between SGEM and AEGC is that SGEM incorporates both energy and momentum at the same time, whereas AEGA incorporates only energy. The authors derive energy - dependent convergence rates and regret bounds for SGEM in the convex setting, and energy dependent regret bounds and a lower bound for the energy variable are also provided. They empirically show that AEGE outperforms SGDM in terms of variance and generalizes better on several deep learning benchmarks."
SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"This paper proposes a non - autoregressive ( NAR ) model named CMLMC, which is based on the Conditional Masked Language Model with Correction ( CMLM ) from [ 1 ]. Specifically, the decoder is decomposed into two modules : a positional encoder and a decoder - decoder module. The encoder exposes the positional information in the tokens to reveal the relative positions of the tokens in the sentence. The decoder also uses causal attention layers to differentiate adjacent tokens. The authors also propose a correction loss that teaches the model how to correct translation mistakes made in early decoding iterations from the fully masked sentence. Empirically, the proposed model achieves new state - of - the - art undistilled NAR results, and approaches AR performance on multiple datasets, a first for NAR models."
SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,"This paper proposes a neural network inspired by the WaveNet architecture, named WaveSense, that mimics the behavior of a spiking neural network ( SNN ), but is more specifically designed for neuromorphic devices. The motivation of this work is that SNNs suffer from the lack of computational efficiency due to the dilatation of their activations compared to the more memory efficient temporal convolutions of a recurrent neural network, and that they lack the ability to store the past activations, thus they are difficult to process efficiently. The authors instead propose to use a simple feed - forward architecture inspired by WaveNet, by replacing the spike neurons in the network with LIF neurons, and replacing the activations with fixed - time constants that can be tuned according to the task. The choice of time constants and the number of layers are informed by the total amount of temporal memory required. The experimental results show that the proposed network can attain performance close to the state - of - the - art, and reaches near the performance of artificial neural networks such as CNNs and LSTMs. The performance improvement is mainly attributed to the fact that after the neuron and synaptic states have been produced, the information is retained in the memory and is not transferred to other neurons."
SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"This paper proposes a class of algorithms, called Shifty, that provide high - confidence behavioral guarantees that hold under demographic shift. Shifty is the first technique of its kind that demonstrates an effective strategy for designing algorithms to overcome demographic shift’s challenges, which may pose as a challenge to designing algorithms that are fair on social networks. The proposed algorithms allow the proportions of demographics to change after training, provided the user has some information describing this change. The experimental results on real - world data, such as university entrance exams and student success, show that the learned models avoid bias unlike existing methods, i.e., Shifty algorithms can be used when the new demographic proportions are known, or when these proportions are bounded in known intervals. Finally, the authors evaluated Shifty - TTest, an implementation of Shifty based on the inversion of the Student's t - test, and found that the fairness guarantees it provides are empirically valid."
SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"This paper proposes a neural - based extension to the popular stochastic dual dynamic programming ( SDDP ) method, called Neural Stochastic Dual Dynamic Programming ( N - SDDP, Section 2 ), which learns a piecewise linear value function within intrinsic low - dimension space ( V - function ) to interact with a base SDDP solver ( Section 2.1 ). The motivation of this approach is to overcome the worst - case complexity issue of SDDP that scales exponentially in the number of decision variables, which limits applicability to only low - dimensional problems. To this end, the proposed approach introduces a trainable neural model that learns to map a problem instance to a piece - wise linear value space, and the objective of the model is to maximize the output of the linear function in each successive instance, while minimizing the overall dimension of the space. The learning objective is motivated by two objectives : ( 1 ) to exploit the experience of solving similar MSSO problems ( Figure 2 ) and ( 2 ) to learn a neural network that can provide a better planning strategy ( Figure 3 ) to improve the planning process ( Figure 4 ).   The proposed approach is evaluated on a range of synthetic and real - world optimization problems, where it is shown to outperform the state - of - the - art supervised learning ( SL ) and reinforcement learning ( RL ) algorithms ( Section 4.1, Section 5.2 ), and in some longer - horizon problems ( Section 6 )."
SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,"This paper proposes a new protocol, called SUBMIX, for private next - token prediction. It is designed to prevent violations of privacy of language models that were fine - tuned on a private corpus after pre - training on a public corpus. The privacy violation occurs when the fine - tuning takes place on the public corpus while the private corpus is used to train the language model. The motivation is that the language models can memorize some training samples verbatim. This is a vulnerability that can compromise the privacy of the model’s training data. The proposed method relaxes a group - differenceially private prediction condition to group - specific private prediction. This relaxement provides a privacy bound that limits the leakage of information unique to any individual user. The paper shows that this privacy bound is tight enough to only upper bound the privacy loss when answering a variable - length query sequence.   The paper also shows that it can effectively prevent data extraction attacks against GPT-2."
SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,"The paper proposes an unsupervised method to detect out - of - distribution ( OOD ) samples by using a k - NN density estimator with respect to a classification model ’s intermediate activations on indistribution samples. The proposed method leverages a recent insight about label smoothing, which the authors call the Label Smoothed Embedding Hypothesis, to theoretically and empirically prove that the proposed method outperforms other OOD detection methods. The authors also provide new finite - sample high - probability statistical results.   The key insight of the paper is that the k - n - N density estimate can capture the separability of intermediate layer embeddings, which allows us to leverage the recently proposed k - smoothing - based hypothesis. The paper then proposes to train the proposed model with k - smoothness training, where the model is trained by minimizing the sum of the label smoothed features and the labels of the samples sampled from the model. Then, the paper shows that the model trained with k- smoothing performs identically to the model that was trained without it. Finally, the method is shown to outperform other methods that rely on the density estimators of the classification model."
SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"Diffusion - based methods represented as stochastic differential equations have recently proven successful as a non - adversarial generative model. Training such models relies on denoising score matching, which can be seen as multi - scale denoised autoencoders. This paper proposes to learn an infinite - dimensional latent code which achieves improvements of state - of - the - art models on semi - supervised image classification. As a side contribution, the authors show how adversarial training in diffusion - based models can improve sample quality and improve sampling speed using a new approximation of the prior at smaller noise scales."
SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"The paper proposes C - Planning, a method for incorporating graph planning into the training of goal - conditioned RL algorithms. The method enables the automatic generation of a curriculum over intermediate waypoints that correspond to the waypoint that corresponds to the optimal state distribution, which is proovably optimal in some settings. The paper evaluates the method on navigation and manipulation tasks and achieves state - of - the - art results on both the 2D navigation and the 18D robotic manipulation tasks. The proposed method improves the sample efficiency of prior goal - conditioned RL algorithms and manages to solve more difficult manipulations tasks. To the best of my knowledge, no prior method has learned tasks of such difficulty without requiring additional assumptions."
SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"This paper proposes a k - mixup variant of standard mixup, where k is the number of training samples in the training set that need to be averaged. Standard mixup is equivalent to ( 1, k ) where k = 1, and it is commonly used to train deep neural networks ( DNNs ). This paper extends k = k to perturbate the training data in the direction of other k - samples by using displacement interpolation, i.e. interpolation under the Wasserstein metric. The main idea is to prevent the averaged training data points from having labels in other clusters or at the wrong locations on the manifold. This can happen if the training distribution is not well supported on an embedded manifold, due to the high - dimensional nature of the data, or if the distribution is too far away from the training dataset. The authors argue that this perturbing can lead to poor regularization, which is why it is important to regularize it as much as possible. To address this issue, they propose to ( k ) mixup regularize the original training set by averaging k times the training samples, but only if k is less than 1. They then show that when k > 1, classic mixup still works fine, but with k=1 ( or equivalently, k-mixup ), it doesn't hurt. When k = more than 1, the regularization is better, as it more accurately reflects the local structure of the dataset. This is illustrated in Figure 1, which illustrates by comparing the output of a fully connected network with that of a naive network trained with standard mix up to k=4. They also compare the performance of mixup to that of plain DNN on different architectures and datasets. Finally, they show that mixup improves performance on low - dimensional datasets ( binary classification ), while it does not improve performance on high dimensions."
SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,"This paper proposes a nonlinear classifier for embedding learning in deep neural networks aiming to extract the best possible classifier with embeddings produced by a given representation learner. The proposed classifier maps embedding vectors to a high dimensional RKHS while automatically learning the optimal kernel that enables this mapping. The authors theoretically establish that it is possible to optimize, within the deep network itself, over all possible kernel functions we care about to find the best one that optimally separates embedding and classifier. Then, the authors empirically demonstrate the usefulness of this layer in learning more model - efficient classifiers on a number of tasks, including image classification, natural language understanding, distillation and active learning settings."
SP:01ee8ec81619784788eb0ce9785098e437d17a7c,"This paper theoretically analyzes the source of bias in Graph Neural Networks ( GNN ), and proposes several ways to improve the fairness of GNNs. Specifically, the paper focuses on the role of nodal features and graph structure in the representation learning process. Theoretical analysis is performed on both graph structure and nodal feature, and several fairness - aware data augmentation schemes are introduced to address the issues. Experiments are performed on real networks for node classification and link prediction tasks, and compared to state - of - the - art graph contrastive learning methods, the proposed methods improve fairness metrics while providing comparable utility measures."
SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"This paper considers the problem of estimating treatment effects from observational data in the presence of unmeasured confounders. One way to estimate treatment effects is to utilize an instrumental variable ( IV ) for two - stage regression, i.e., 2SLS and variants. The main contribution of this paper is to propose a Confounder - Balanced IV Regression ( CB - IV ) algorithm to jointly remove the bias from the unme measured confounds with IV regression and achieve better bias - variance trade - off in imbalanced treatment distributions due to the observed confounder by balancing for treatment effect estimation. The proposed algorithm consists of three main modules :   1 ) treatment regression : regressing the treatment with IVs and confounds like previous nonlinear IV methods for removing the confounding from unmeasurement ;   2 ) confoundER balancing : learning a balanced representation of the expected and the actual confounding to eliminate the bias induced by the observed Confounders ; and   3 ) outcome regression : Regressing the outcome with the predicted treatment and the balanced confoundERS representation to achieve treatment effect estimator. Experiments are conducted on both synthetic and real - world datasets to validate effectiveness of the proposed algorithm."
SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"This paper analyzes how model - agnostic meta - learning ( MAML ) outperforms standard non - adaptive learning ( NAL ) in a linear regression setting, where the goal is to find a solution that outperforms NAL on the more difficult to optimize ( hard ) tasks, as long as the hard tasks are sufficiently similar to the easy ones. The authors prove two results :   ( 1 ) There must be a discrepancy in the hardness of the easy and hard tasks in order for MAMl to outperform NAL, and ( 2 ) the solution of the hard task must be closely packed with the hard - tasks "" easy "" solutions, with the center far from the center of the difficult - to - learn solutions, in order to provide good performance on hard tasks. They also give numerical and analytical results suggesting that these insights apply to two - layer neural networks. Finally, they provide few - shot image classification experiments that support their insights and emphasize the importance of training MamL on hard, difficult tasks in practice."
SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"The paper proposes an unrolled variant of the Proximal alternate linearized minimization ( PALM ) algorithm for sparse source separation ( semi - blind ) based on unfolding / unrolling. The proposed method learns the hyperparameters and mixing matrices of PALM by individually updating them during the training and testing phases. The LPALM algorithm is shown to outperform its classical counterpart, PALM, on a multispectral source separation task with simulated data and ground truth images. The performance gain is attributed to the better quality of the simulated data compared to ground truth and the fewer number of iterations needed compared to PALM. The paper also presents results on unrolled variants of the unrolled PALM and LISTA for sparse image classification in astrophysics and remote sensing."
SP:7716315001949ab88c8a216302fe51bae872fc87,"This paper proposes a new model based on the Legendre Memory Unit for the autoregressive language modelling task. The paper shows that the proposed model exhibits the same power - laws as transformers and LSTMs on the large - scale, and at the small - medium scale. The main contribution of the paper is the introduction of a new attention module called implicit self - attention, which is motivated by the observation that the performance of transformers scales as a power - law with the model size ( excluding embedding parameters ), dataset size, and the amount of compute used for training the model. The authors also propose a new loss function, the "" Legendre Cross - entropy Loss "", which improves upon the previous loss functions ( LSTM and transformers ) when it comes to the number of tokens required to compute the cross - entropy loss."
SP:832f422b3554e89702e13c8c5690ee26f2289e3b,"The paper introduces LatentKeypointGAN, a GAN that generates an image encoder that generates a set of keypoint embeddings for each image. These keypoints are associated with an image embedding that controls the position and style of the generated objects and their parts. The paper demonstrates that the keypoint encoder can be used to re - arrange the generated images by re - positioning and exchanging keypoints in different images. In addition, the paper proposes a new GAN - based method for keypoint detection, which is trained end - to - end on the classical GAN objective with internal conditioning and external generation of keypoints and matching images.   The paper compares and contrasts the performance of the proposed method against the state - of - the - art key - point encoder - decoder GAN as well as keypoint2GAN and keypoint3GAN. It is found that the proposed approach performs marginally better than both of them in terms of accuracy."
SP:9206ae6e31077569313838504ef6daa89ad3b59c,"This paper studies non - leaky fully - connected neural networks with layer normalization using the mean field formalism, and performs a non - perturbative analysis of signal propagation. It shows that increasing the depth of the neural network can lead to gradient explosion or to another undesirable phenomenon we call representation shrinkage. The paper also shows that many popular normalization techniques such as ResNet, ResNets, and Gumbel Softmax are unable to mitigate these problems. This paper also applies the results to residual networks to guide the choice of initialization variances ( RMT ) to analyse its full spectrum. They find that the maximum and variance of its singular values grow with depth even if the mean remains O(1 )."
SP:2177be818b5843c580c787f1b2d725154846feb6,"This paper proposes a new line search method to estimate the step size of update steps in stochastic gradient descent ( STL ) based on the observation that the full batch loss locally behaves locally in the direction of the noisy update step directions, and the trend of the optimal update step size changes slowly with increasing batch size. Based on these observations, the paper proposes to approximate the full - batch loss with a parabola estimated over several mini - batches, and derives learning rates using a piece - wise constant learning rate schedule that is learned from the variance of the parabolas estimated during training. In the experiments conducted, the proposed line search approach mostly outperforms SGD tuned with a piece-wise constant learning rates schedule and other line search approaches for STL on various datasets and batch sizes on validation and test accuracy. The paper also provides an analysis of the performance of line search across datasets, models, and gradient noise levels."
SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"This paper studies the problem of improving the convergence of noise - contrastive - estimation ( NCE ) with an uninformative noise distribution. The paper claims that the main reason why NCE performs poorly with an invalid noise distribution is because of an ill - behaved loss landscape, i.e., when the number of steps required to solve an NCE problem is exponential in the dimension and polynomial in the distance to the ground truth, due to an exponential family of losses. To address this problem, the paper proposes a new loss called eNCE, which replaces the log loss in NCE with an exponential loss, and proves that the resulting condition number is Polynomial if and only if the target and noise distribution are in an exponentially family. Then, normalized gradient descent is used to solve the optimization problem to obtain a ground truth solution. Experiments are conducted on MNIST, CIFAR-10, and CelebA - MNIST. On all of these benchmarks, NCE outperforms its competitors, especially when the noise is chosen appropriately."
SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"This paper studies the problem of integrating differential privacy ( DP ) and Byzantine resilience ( BR ) in distributed deep learning, in particular, the integration of DP and BR. It is shown that existing results on the convergence of distributed SGD under Byzantine fault ( e.g. those relying on ( α, f)-Byzantine resilience, are invalidated when honest workers enforce DP. To circumvent this shortcoming, the paper proposes to use the theory of ( α, f)-BR to obtain an approximate convergence guarantee, which provides insights on how to improve this guarantee through hyperparameter optimization. The convergence result obtained using the relaxed, approximated, version of the VN condition is shown to outperform existing results in the literature of BR. The paper also proposes a suitable and sufficient condition for the condition to be satisfied. Experiments are conducted to demonstrate that even when the relaxed   $ \tilde{gamma } : - approximate $ \gamma^2 $ condition is violated, the algorithm obtains reasonable learning accuracy."
SP:bc783f0c829f90931535e63687d13172879631b3,"This paper proposes a method for code editing in environments where there are few exemplars of support snippets and few examples of the query code snippets. The proposed method is based on a multi - extended similarities ensemble ( MEET ) approach. MEET computes the query and support snippets from different branches of a language - specific syntax tree using a similarity - based loss. The authors then aggregate the samples from multiple branches of the tree using the similarity - ranking error estimator, and use the ensemble results to compute a representation of the editing exemplar. The representation is then used for decoding the query snippets from the support snippets. Evaluations are conducted on C# and Python code editing datasets, and show up to 8.6% absolute accuracy improvements compared to non - MEET - based methods. In addition, experimental results show that enabling few support exemplars can improve code editing."
SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,"This paper proposes a program synthesis method for generating sequence data with high - level structure, e.g. lines of a song, or measures of music. The key idea is to use constraints placed by a program to guide the generative model to generate sequences that have the same structure as the program. To do this, the authors propose a program generation algorithm that generates a set of constraints for each component of the data. These constraints are then used by a neural network to find sequences that satisfy the given constraints. The neural network is trained to solve the program generation problem. Finally, the generated sequences are used to find matching sequences. The method is evaluated on two tasks : ( 1 ) generating sequences with human - like structure, and ( 2 ) generating examples whose relational structure resembles that of the training data. The results show that the proposed method achieves better global coherence than the baselines ( i.e., those that do n’t explicitly account for structure )."
SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"This paper proposes a novel method for set - to - hypergraph ( SET ) prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. Two scaling challenges that are often encountered in SET tasks are the exponentially growing number of hyperedges and the run - time complexity, both of which lead to higher memory requirements. This paper proposes to address these scaling challenges by combining two contributions that are not related in a related way : 1 ) a method to predict and supervise only positive edges, which changes the asymptotic memory scaling from exponential to linear ; 2 ) a training method that encourages iterative refinement of the predicted hypergraph, which allows to skip iterations in the backward pass for improved efficiency and constant memory usage. 3 ) a way to combine both contributions in a single set to hypergraph model that enables to address problems with larger input set sizes. The paper provides ablations for each of these contributions and shows that their model outperforms prior state - of - the - art, especially for larger sets of refinement steps."
SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"This paper proposes a new post - processing approach to mitigate bias in face recognition systems. It consists in learning a shallow neural network, called the Ethical Module, which transforms the deep embeddings of a pre - trained model to give more representation power to the discriminated subgroups. Its training is supervised by the von Mises - Fisher loss, whose hyperparameters allow to control the space allocated to each subgroup in the latent space. Besides being very simple, the resulting methodology is more stable and faster than most current methods of bias mitigation. In order to illustrate the idea in a concrete use case, the authors focus here on gender bias in facial recognition and conduct extensive numerical experiments on standard datasets. They compare the proposed method with several existing bias - mitigating methods, including reinforcement learning, adversarial methods, transfer learning, and reinforcement learning. The experimental results demonstrate the soundness of the methodology on several pre -trained models, and strongly believe it could also be used to alleviate other types of bias."
SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"This paper proposes a novel method PlaceboCIL for solving the class - incremental learning ( CIL ) problem, which leverages the existing knowledge distillation techniques such as logit distillation and feature distillation. The key idea of these methods is to penalize inconsistencies in the previous models of subsequent phases as old - class data is scarce, and the KD loss mainly uses new class data. However, the authors empirically observe that this can hurt the learning of new classes, and also underperforms to distil old class knowledge from the previous phase model. To address this issue, they propose to compute KD using placebo data chosen from a free image stream, which is both simple and surprisingly effective even when there is no class overlap between the placebos and the old data. When the image stream is available, they use an evaluation function to quickly judge the quality of candidate images ( good or bad placebos ) and collect good ones. For training this function, they sample pseudo CIL tasks from the data in the 0 - th phase and design a reinforcement learning algorithm. Their method does not require any additional supervision or memory budget, and can significantly improve a number of top - performing CIL methods, in particular on higher - resolution benchmarks, e.g. ImageNet-1k and ImageNet - Subset, and with a lower memory budget."
SP:506e0a888c03a955b708464eed3670c04baf4912,"This paper proposes an efficient sampling method for sampling from discrete Markov chains in energy - based models. In contrast to existing work, which uses MCMC to sample from a discrete distribution, this paper proposes a sampling method that uses a composition of local moves to efficiently explore large neighborhoods. The authors empirically evaluate their methods in inference, sampling, and learning on various discrete EBMs. They demonstrate that their methods significantly improve the efficiency on parity model, weighted permutation model, Ising model, RBM, and FHMM, and factorized Hidden Markov Model. They also show their method can also learn competitive deep EBMs on discrete image data."
SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"This paper proposes Variational Predictive Routing ( VPR ), a neural probabilistic inference system that learns to predict event boundaries by using the latent representations of video features. The key insight of the method is to model continuous data as a hierarchical renewal process, i.e., by employing an event detection mechanism that relies solely on the system’s latent representations ( without the need of a separate model ), VPR is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model ’s hierarchical latent hierarchy. Experiments are conducted on several video datasets, demonstrating the ability of VPR to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate timeagnostic rollouts of the future.   The approach integrates insights from neuroscience and introduces a framework with high potential for applications in model - based reinforcement learning, where flexible and informative state - space rollouts are of particular interest."
SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,"This paper proposes a single - stage model for image retrieval that combines global and local features. The proposed model is called UGALR ( Unite Global and Attention - based Local Features Retrieval methods ). It consists of three components : ( 1 ) global feature extraction, ( 2 ) local feature matching using convolutional neural networks, and ( 3 ) spatial and channel attention. Global feature extraction uses RANSAC, which is suboptimal because of its time and space consuming nature.   The main contribution of this paper is to propose a method that firstly extract global features then re - rank images using local features, which uses a CNN to learn homography transformation in images, and then replace the re - ranking process with information fusion to obtain more powerful features and overcome the low efficiency of local features in storage and matching. The effectiveness of UGLAR has been validated with comprehensive experiments, which achieves state - of - the - art performance with less memory and faster extraction speed compared with other popular methods."
SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"This paper proposes RotoGrad, a multi - task learning algorithm that aims to avoid negative negative transfer by homogenizing the gradient magnitude across tasks. The motivation of this approach is to avoid the disparities in gradient magnitudes and directions across tasks when optimizing the shared network parameters. The theoretical analysis of the problem is provided in the paper along with a theoretical motivation for the proposed method. Empirically, the authors conduct extensive experiments to demonstrate the effectiveness of the method, which outperforms several baselines and scales to new architectures with respect to the computational cost."
SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,"This paper proposes a model fusion method for heterogeneous neural networks, namely, unequal width and unequal depth neural networks. The proposed method is named as Cross - Layer Alignment Fusion ( CLAFusion ). It consists of three parts : cross - layer alignment, layer balancing and layer - wise model fusion. The cross layer alignment problem, which is an unbalanced assignment problem, can be solved efficiently using dynamic programming. Based on the cross - Layer alignment, our framework balances the number of layers of neural networks before applying layer-wise model fusion, which can be fast. Experiments indicate that the fused network from CLAFusions achieves a more favorable performance compared to the individual networks trained on heterogeneous data without the need for any retraining. Furthermore, it shows potential applications for model compression and knowledge distillation when applying to the teacher - student setting."
SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"This paper studies how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. The authors derive a theoretical analysis that when existing models of implicit regularisation are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive “aliasing ”, in stark contrast to the supervised learning case, where SGD favors parsimonious solutions that generalize well on test inputs. Inspired by this derivation, the authors propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer and can be added to existing offline RL methods, such as REM, to improve performance on offline RL tasks."
SP:6fd793b27123bf80504e2ad5957455b7ec311612,"This paper proposes a new exploration method based on deep reinforcement learning, called HyperDQN, which combines DQN ( McInnes et al., 2015 ), OPIQ ( Rashid et al, 2020 ), NoisyNet ( Fortunato et a., 2018 ), and BootDQ N ( Osband et a., Osband. 2018 ) in combination with a probabilistic hypermodel ( meta model ). The meta model outputs the parameter of the base model, and the hypermodel is trained to generate approximate posterior samples of the Q - value function. The paper evaluates the method on the Atari suite and shows that it can outperform the previous methods in terms of the human - normalized score ( normalized by $ \nabla_{normalized}$ )."
SP:b428383660928374c953f659ea1e05852dbdcd6e,"This paper proposes a counterfactual loss - based representation learning method for robust classification. The authors argue that learning a representation that preserves mostly the cause information will improve the generalizability of the model. The proposed method is based on learning a model based on a hypothetical graph showing the relationship between labels ( e.g., y, z ) and the causal factors ( i.e., the factors that affect the label, such as the effect, the cause and the effect on the label ). Then, the model is regularized according to the mutual information measures between the labels ( based on the hypothetical graph ). Under certain assumptions, the authors show that under certain learning conditions, the proposed method leads to reduced sample complexity as well as reduced loss dimension. Extensive experiments on real data set show that the models trained on causal representations learned by our approach is robust to adversarial attacks and distribution shift."
SP:1258c05a80a17949b50e6dae13deea1d2235f456,"This paper proposes ProgFed, a progressive training framework for efficient and effective federated learning. It inherently reduces computation and two - way communication costs while maintaining the strong performance of the final models. Theoretically, they theoretically prove that Progfed converges at the same asymptotic rate as standard training on full models. Extensive results on different architectures from small CNNs to U - net show that their method is communicationand computation - efficient, especially when the training budgets are limited. Interestingly, they found that a progressive learning scheme has even led to improved performance in vanilla learning and more robust results when learning is perturbed, which highlights progressive learning as a promising technique in itself."
SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"This paper studies the generalization of adversarial training through the lens of Rademacher complexity. Adversarial training is one of the most effective algorithms to increase the model’s robustness. However, the trained models often fail to generalize well to the adversarial examples on the test set. This paper proposes a method to overcome this issue and provides upper bounds that provide an upper bounds of the adversary ’s adversary complexity. The upper bounds also includes the product of weight norms, which the authors provide experiments to show. The authors claim that the adversarially trained weight norms are larger than the standard trained weight norm, thus providing an explanation for the poor generalization performance."
SP:925d6bb051e9b384669fb695085b678c11f7c11a,"This paper proposes a differentiable kernel - based estimator for differential entropy ( DE ) and mutual information ( MI ), called Knife. The differentiable nature of the estimator makes it possible to use it in practically any training objective for DE or mutual information estimation. The main advantage of this approach is that it does n’t require any additional assumptions for the learning parameters. Experiments are conducted on image classification, fair classification, textual fine - tuning, and visual domain adaptation tasks. The experiments show that the method outperforms other DE and MI estimators on these tasks."
SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"The paper proposes a new greedy operator, called resmax, based on a probability matching scheme originally developed in the contextual bandit setting. Theoretically, the paper shows that resmax is a non - expansion, regardless of the hyperparameter settings, and thus converges under generalized approximate policy iteration. Moreover, resmax ensures state - action space coverage and it avoids softmax’s fundamental issue of overemphasizing. The empirical results show that resMax outperforms softmax and ε - greedy, especially when softmax suffers from overestimation."
SP:792ae8808aa6902758146aef1548c975492b833c,"This paper introduces and investigates a new concept called “ learnability lock ” for controlling the model’s learnability on a specific dataset with a special key. The key consists of a class - wise adversarial perturbation that applies a universal transformation function on data samples of the same label to create an adversarial invertible transformation, such that it is difficult to be detected or reverse - engineered. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. This is achieved by performing the inverse transformation applied to the input image. The experimental results demonstrate the effectiveness of the proposed method on visual classification tasks."
SP:9af10703605e620e563241e2602a50b629f3d37a,"This paper introduces a method for handling missing features in Graph Neural Networks ( GNNs ). In this line of work, nodes and edges are assumed to be available but features of other nodes or edges are not. The proposed method, Feature Propagation ( FP ), is based on minimzing the energy minimizer of the Dirichlet energy, which leads to a diffusion - type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which is called Feature propagation ( SPI ). The authors empirically show that FP outperforms previous methods on seven common node - classification benchmarks and can withstand surprisingly high rates of missing features ( up to 4 % ), while also being extremely scalable ( only takes 10 seconds to run on OGBNProducts with 2.5 M nodes ).   "
SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"This paper introduces an integer optimization problem for selecting a core set that minimizes the discrete Wasserstein distance from the unlabeled data set to label in active learning. The authors derive a Generalized Bender Decomposition ( GDC ) algorithm to solve this problem. They demonstrate that it is tractable by globally converging on a globally convergent bound. They provide numerical results on several data sets showing that their optimization approach is competitive with baselines and outperforms them in the low budget regime, where less than 1 % of the data set is labeled. They consider low budget active learning for image classification and domain adaptation."
SP:4c72923f78ca6590dc11e10d1a2403076a583718,"This paper proposes a novel approach to solve de novo genome assembly based on graph neural networks and finding a path through the assembly graph. The authors trained a graph neural network ( GNN ) on a dataset of assembly graphs generated from real human genomic data on which the developed model was trained and evaluated. The model was also evaluated against a naive greedy approach, an exhaustive search using the positional information of reads, and an existing genome assembler Raven ( Vaser & Šikić, 2021 ). It was shown that the proposed GNN consistently outperforms the greedy approach in terms of reconstructed length and reconstructed length, and also outperforms Raven in terms the execution speed. More interestingly, the model outperformed Raven when given a highly complex graph from a repetitive region. The results are particularly promising to solve challenging regions more accurately and in far less time than the existing assemblers can."
SP:24de906e4289c9073b6c55c747b0913b8df5e053,"This paper proposes a meta - learning method that incorporates experience replay ( ER ) into its meta - training in order to overcome catastrophic forgetting in continual learning. The idea is that the batch nature of ER training interferes with the online nature of OML, resulting in a loss of gradient and Hessian information during the meta - update step. To alleviate this problem, the paper proposes to store the samples’ representations, instead of the samples themselves, into the replay buffer. This ensures the batchnature of ER does not conflict with the offline nature of meta - continuing learning. Besides, it also proposes the predictive sample selection, which selects the most sensitive samples for inclusion in the buffer. Experimental results on a number of real - world continual learning benchmark datasets demonstrate that the proposed method outperforms the state - of - the - art. Moreover, the learned representations have better clustering structures and are more discriminative."
SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"This paper proposes a new IGM - based multi - agent Q - learning method called Explicit Credit Assignment Joint Q - Learning ( ECAQ ). This method learns the credit assignment among different agents for joint Q - values by using the suggestion of each agent on how to maximize the Q - value, besides guaranteeing the Bellman optimality of the joint Q-value. Theoretically, the authors give a gradient ascent solution for this problem. Empirically, they approximate the core idea with deep neural networks and propose Explicit Credit assignments in ECAU on ImageNet. Experiments are conducted on MNIST, CIFAR-10, and UCI tasks and compare with IGM and ICAQ. The results demonstrate that ELAQ achieves interpretable credit assignment and superior performance compared to several advanced baselines."
SP:0d2b225ac697679d10df25f371b2a718d4949b42,"This paper proposes a new method to evaluate the robustness of transductive learning based defenses. In this line of work, the authors use the idea that the defense mechanisms “dynamically learn ” the model based on test - time input, and theoretically, attacking these defenses reduces to solving a bilevel optimization problem, which poses difficulty in crafting adaptive attacks. To address this, they formulate and analyze threat models for defense mechanisms based on adversarial training, and point out important subtleties. They propose the principle of attacking the space of the model for solving the objectives of bileve attack objectives, and present Greedy Model Space Attack ( GMSA ), which is an attack framework that can serve as a new baseline for evaluating such defenses. They perform a systematic empirical study on various defenses, and show that even weak instantiations of GMSA can break respective defenses, such as DENT, RMC, and URejectron. Specifically, for defenses which are adversarially trained, GMSA reduces the robust accuracy to that of adversarial learning alone. However, on the positive side, they report a somewhat surprising empirical result of “ transductivi - learning adversarial trained ” : Adversarially retraining the model using fresh private randomness at the test time gives a significant increase in robustness against attacks."
SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"This paper studies the problem of per - example normalization of batch normalization. Batch normalization is a popular technique to speed up training neural networks. It is commonly used to normalize the gradient of the model parameters of a batch of neural networks trained on the same dataset. The authors argue that it is an approximation to the case when the entire dataset is normalized jointly, and explore other ways to approximate the gradient from this limiting case. In this work, they aim to develop a method for performing per - examples normalization in a way that does not modify the inference - time architecture of the network. The resulting method combines per - instance gradient computation, the maintenance of moving first and second moments, and aggregation step that joins the information from multiple examples similar to the way that the gradients are commonly averaged over the minibatch in SGD. Unlike previously proposed methods, their normalization does not change the function class of the inference model, and performs well in the absence of identity shortcuts."
SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,"The paper proposes Low - rank Adaptation ( LoRA ), a method to adapt the pre - trained Transformer model to downstream tasks by freezing the weights and using rank matrices instead of weights. The idea is to reduce the number of parameters that the fine - tuned model has to use for adaptation to different downstream tasks. The paper provides an empirical study that shows that LoRA outperforms full fine - tuning, which retrains all the parameters, on RoBERTa and DeBERTa, and GPT-2 / GPT - 3, despite having fewer trainable parameters, a higher training throughput, and no additional inference latency."
SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"This paper proposes a method to relax the Markov assumption of linear - chain conditional random fields ( CRFs ), in order to enforce non - local constraints on the labels to be predicted. The method relies on a regular language to constrain the space of possible label sequences in the CRF to be the same as that of a standard CRF. The resulting regular - constrained CRF ( RegCCRF ), however, assigns zero probability to sequences that are not in the regular language L.   The paper shows how to incorporate constraints into a CRF such that it is guaranteed to output label sequences from that language. The domain knowledge specifying this constraint is defined via regular expressions. The paper also provides a Python implementation of the method and shows how it can be used as a drop - in replacement for standard CRFs. The main contributions of the paper are :   1. The construction of a regular - restricted CRF by constraining the label sequences to be in the same space as the language L through regular language induction ; 2. Theoretical proof that ( regular ) language - based constraints that are imposed at encoding time ( training ) are better than those imposed at ( decode ) time ( prediction ), and 3. Experiments on a semantic role labeling dataset show that the proposed method achieves state - of - the - art performance on the task."
SP:74c186a96c12adff178264aa84ace8d04dc7d725,"This paper proposes two end - to - end neural models for the application of camera - based vitals measurement, i.e. without the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. The proposed models achieve state - of - the - art accuracy on three public datasets. They show that this is the case whether using a visual transformer or convolutional backbone. They further evaluate the latency of the proposed networks and show that their most light weight network also achieves a 33% improvement in efficiency."
SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"This paper proposes a method for hardware - aware pruning, called HALP, for image classification and detection networks based on ImageNet and VOC. The idea is to use pruning as a global resource allocation problem to maximize the accuracy while constraining the latency under a predefined budget. The paper leverages the latency - aware latency table and the latency aware neuron grouping scheme to further improve the performance. Experiments are conducted on ResNet, MobileNet, VGG, ImageNet, PASCAL VOC and compare to prior art when pruning ResNet and demonstrate that HALP consistently outperforms prior art, sometimes by large margins."
SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"This paper proposes GraphEBM, an energy - based model ( EBM ) for generating permutation - invariant molecular graphs, and proposes to use EBMs for single - objective, single - molecule, and multi - objective molecule generation. The key advantages of the proposed method are :   1. It defines a permutation invariant distribution over the energy of the molecular graph. This is achieved thanks to the following two innovations : ( 1 ) learn the energy function by contrastive divergence and generate samples by Langevin dynamics ; ( 2 ) to generate molecules with a specific desirable property, it proposes a simple yet effective learning strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules ; and ( 3 ) further explore to use the method for generating molecules towards multiple objectives via compositional generation, which is practically desired in drug discovery. Empirically, the authors conduct comprehensive experiments on various settings, and show that their method is effective and promising."
SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"The paper proposes a neural - based program synthesis model, CROSSBEAM, that learns to search within an enormous space of programs, instead of relying on a combinatorial search algorithm. The neural model is trained using supervised learning, where it is given a list of programs to search for, and is encouraged to choose one of them to execute as a program, taking into account the search history and partial program executions. Experiments are conducted in string manipulation and inductive logic programming domains, and show that the model significantly outperforms the state - of - the - art methods in both domains."
SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,"This paper proposes a new objective function, called FR Squared Bellman Error, to replace the squared Bellman error commonly used in deep reinforcement learning ( DQN ) training. The squared bellman error is used to stabilize training due to the non - stationary nature of the regression problem. In contrast, the new objective uses a lagging set of parameters to stabilize estimation. The difference is that the lagging parameters are decoupled from the target value estimation, instead taking the form of a functional prior. In this way, the functional prior can be controlled to either use up - to - date target value estimates, thereby propagating reward information more quickly, or separately control the degree of stabilization via the FR weight, $ \phi$. Experiments on various Atari environments show that the proposed objective can recover the true value function and learn quickly compared to target networks. The proposed objective performs well across a wide range of environments."
SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"This paper proposes GraphSNN, a GNN model that attempts to be more expressive than the Weisfeiler Lehman test ( WL ) when aggregating message - passing graph neural networks ( MNNs ). Specifically, the authors consider a setting where the goal is to aggregate a set of isomorphic graphs in the local neighborhood based on overlap subgraph isomorphism ( i.e., the degree of overlap between two vertices of a graph is that of a neighboring graph if those vertices have edges that are at least as large as the next largest node in the neighborhood. The authors show that if one takes as input the set of vertices that have overlap greater than some threshold value, then the resulting graph is said to be “ expressive ” under the WL test. They then propose a new hierarchy of local isomorphisms that can be used as a basis for constructing GNNs. They show that the proposed SNN is more expressive ( by a large margin ) than 1 - WL on node classification and graph classification tasks. They empirically verify the strength of their model on different graph learning tasks and show that it consistently outperforms existing GNN models."
SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"This paper proposes a novel prediction interval ( PI ) method, PI3NN, which generates a set of PIs for a given confidence level using a combination of three neural networks, each of which is independently trained using the standard mean squared error loss. Then, the coefficients of the linear combinations of the neural networks are computed using root - finding algorithms to generate PIs that do not cross when computing multiple PIs. The proposed method avoids the crossing issue of existing PI methods, which can result in over - confident PIs when computing different confidence levels. In addition, it also addresses the challenge of OOD detection by introducing an initialization scheme that provides reasonably larger PIs of the OOD samples than those of the in - distribution samples. On benchmark and real - world experiments, the proposed method outperforms several state - of - the - art approaches."
SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"The paper proposes a meta - learning method, FOML, that attempts to learn a model that can simultaneously adapt to new tasks, changing input distributions, and changing environmental conditions by meta - training the model in an online fashion. The model consists of a vector of online parameters, $ \phi$, and a set of meta - parameters $ \tau, which are updated via gradient updates on a buffer of previously seen data. The meta - parameter vectors are sampled from a distribution that is learned as part of a MAML task sampling scheme. The authors argue that a sampling scheme that selects the datapoints at random from the buffer can be used to meta - train the model to learn new tasks quickly, while at the same time enabling the regularizer to influence the online updates via a regularizer. To achieve this, the authors propose a random sampling scheme of tasks from the Rainbow - MNIST dataset and the CIFAR-100 dataset. The sampling is done in a way that ensures that the task boundaries are known, and that the model does not reset the online parameters back to the meta - learned parameters for every task. Then, the model is trained to adapt the learned parameters to the new task boundaries using the buffer and regularizer, and the learning process is repeated until the model reaches a state that is comparable to or better than baselines and prior methods. Empirically, the proposed method achieves better performance than the state - of - the - art online meta learning methods, and also achieves a learning rate that is significantly faster."
SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"This paper proposes a differentiable scaffolding tree ( DST ) to make a molecular graph locally differentiable, allowing a continuous gradient - based optimization. To the best of my knowledge, this is the first attempt to make the molecular optimization problem differentiable at the substructure level, rather than resorting to latent spaces or using RL / Evolutionary algorithms. The authors constructed a general molecular optimization strategy based on DST, corroborating the empirical studies. They demonstrate encouraging preliminary results on de novo molecular optimization with multiple computational objective functions. The local derivative shows consistency with chemical intuition, providing interpretability of the chemical structure - property relationship."
SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"This paper proposes a knowledge - augmented personalized lab test result prediction ( KALP ) method. It learns a strong patient representation incorporating both patient information accumulated over the visits as well as information from similar patients. To increase the prediction accuracy for patients with complex co - morbidities, it has augmented the patient representation with external knowledge on lab interactions and patients’ historical responses to the target lab test. The proposed method is applied to MIMIC - III EHR and a proprietary outpatient dataset. Experiments on real - world datasets demonstrate the effectiveness of the proposed solution in reducing prediction errors by a significant margin. The identified factors for influencing the predicted results are consistent with clinicians’ understanding."
SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"This paper proposes CrossMatch, a method for open - set single domain generalization ( OS - SDG ), which aims to generalize a single source domain to unseen target domains, and simultaneously deals with unknown classes. The proposed method uses a multi - binary classifier to generate auxiliary labels that are potentially out of the source label space, called CrossMatch. The authors also adopt a consistency regularization on generated auxiliary samples between multibinary classifiers and the model trained by SDG methods, to improve the model’s capability on unknown class identification. Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance of existing single - domain generalizaiton methods."
SP:126f8ffb855aa22eda4d681a499953879ed3679e,"This paper proposes two natural extensions of TRPO and PPO to trust - region optimization with Wasserstein and Sinkhorn metrics, namely W.PO and SPO. The key difference between these two methods is that SPO directly optimizes the policy distribution, while WPO uses the Lagrangian dual of the state - action distribution. Theoretically, the authors show that WPO guarantees a monotonic performance improvement, while SPO provably converges to WPO as the entropic regularizer diminishes. Experiments across tabular domains and robotic locomotion tasks demonstrate the performance improvement of both approaches, more robustness of WPO to sample insufficiency, and faster convergence of SPO, over state - of - art policy gradient methods."
SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"This paper proposes a forgetting - and - re - learn framework that proposes to view training neural networks as an iterative process in which two stages : ( 1 ) a forgetting step ( i.e., weight perturbation ) and ( 2 ) a "" re - training "" step ( called "" forgetting "" ). In the forgetting step, the authors propose to selectively remove undesirable information from the model, and ( ii ) to re - train the model with features that are consistently useful under different conditions.    The forgetting operation is proposed to be a combination of two steps : ( i ) a first - step forgetting operation that selectively removes undesirable information ( eg. information that is distracting from the learning process ), and( ii ) a second - step retraining step, where the learned feature is selectively selected from the forgetting pool. The authors then propose to study the history of many well - known iterative training algorithms in the image classification and language emergence literature, and propose to analyze the success of these algorithms in terms of the disproportionate forgetting of undesirable information. The hope is that this understanding can be leveraged to improve upon existing algorithms by designing more targeted forgetting operations ( e.g., more selective forgetting )."
SP:2789859517b6624730b14a7e010444a72d3dd3ed,"This paper investigates the offline - online setting of training agents in batch RL, where the agent receives an offline dataset to train on and is also allowed to learn during the evaluation phase of the training in an online manner. This setting is an extension of batch RL ( e.g., training on a batch of data and learning during evaluation ) to offline RL. The authors compare the performance of agents trained in this way to agents trained only online or with offline data. They find that standard RL agents trained using offline data often outperform agents trained online or by a large margin. Further experiments measure the performance to the size of the dataset, the datacollection policy, and the amount of training done. The results show that larger datasets and a good policy can be critical to good results."
SP:76625a25e770415599a34122110d61cb3b7e614c,"This paper proposes a discrepancy - optimal meta - learning method for domain generalization ( DG ) by learning to reduce domain shift with an episodic training procedure. In particular, the domain shift is measured with Y - discrepancy and learned to optimize Y - discrepancy between the unseen target domain and source domains only using source - domain samples. The authors derive a PAC - style generalization bound for discrepancy optimal meta learning and further make comparisons with other DG bounds including ERM and domain - invariant learning. Theoretical analyses show that there is a tradeoff between classification performance and computational complexity for discrepancy optimality of the proposed method. The theoretical results also shed light on a bilevel optimization algorithm for DG. Empirically, the authors evaluate the algorithm with DomainBed and achieves state - of - the - art results on two DG benchmarks."
SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"This paper studies the role of the policy and value networks in solving hard PSPACE problems with a DNN - based best - first search algorithm on the Sokoban domain. The authors first study the existence of left heavy tails in the search space and propose an abstract tree model that can empirically explain the appearance of these tails. They then show that for a given search budget, more frequent restarts are more effective. The experiments show the importance of using the policy network and random restarts for the search. Finally, they show how uncertainty - aware networks provide an effective way to introduce randomness into the search process leading to increased efficiency."
SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"This paper proposes a meta - imitation learning method for robots that relies only on human video demonstrations to learn a new task from observing one or a few human demonstrations. Compared to prior works, this approach is able to translate human videos into practical robot demonstrations and train the meta - policy with adaptive loss based on the quality of the translated data. The authors combine this approach with existing meta - learning methods which learns the meta-policy with both human and robot demonstrations on a set of challenging tasks and show comparable results to the baselines in one - shot new task learning.    The main contributions of this paper are :   - A new generative model A - CycleGAN, which automatically translates human videos to robot demonstrations using a new - designed generative models A - CycleGAN and trains the meta policy in the imagined compact latent space with the proposed adaptive loss. - A meta - algorithm based on Gumbel - Softmax, which generates sequences of actions that are sampled from the latent space using a softmax - based loss."
SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"This paper proposes LAWN ( Light Adaptive Weight Normalized Weight Normalization of Batch Gradients ), a method to enhance the generalization of adaptive gradient descent models trained with large batch sizes using gradient - based optimizers such as Adam. Adaptive optimizers are commonly used to train large - batch models due to their tendency to over - parameterize training and suffer from poor generalization. The main idea of LAWN is to replace Adam with a weight - norm constrained regularizer ( LAWN - SNR ) that tries to keep the model's output and weight close to the ground truth, while keeping the optimizer's distance from the output in proportion to its gradient. The idea is that this will help the model adapt to changes in the output that might occur in the environment ( e.g., changes in batch size or in the learning rate or batch size of the model ). The authors perform experiments on several image classification tasks ( CIFAR-10, ImageNet-50, MNIST-100, CelebA-100 ) and one drug discovery task ( HIV-1 ), and show that LAWN improves on all of them. They also compare the performance of Adam with and without LAWN on the aforementioned tasks and find that Adam training with LAWN leads to better generalization compared to no LAWN training."
SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"Group equivariant convolutional neural networks ( G - CNNs ) are group neural networks that use group equivariance to ensure that certain symmetries of the probability distribution ( e.g. rotation in [ -90, -90 ] ) are respected, which leads to better generalization and performance. However, if certain equivariances ( i.e., the level of group - equivariancy ) are not maintained, such as full group - eq, they can lead to overly - constrained models and worse performance. In this paper, the authors propose a method that maintains / learns different levels of full and partial group - Equivariance at every level of the network.    The main contributions of the paper are :   1. An entirely new family of equivariantly - trained group CNNs, called Partial Group Equivariant CNNs ( PGECNNs ) ;   2. A set of experiments that compare the performance of the proposed network to that of a standard full - group CNN, called P - CNN, on rotated MNIST, rotated CIFAR, and natural image classification tasks. The experiments show that P - GECNNs perform on par with standard G -CNNs ( with the right choice of hyperparameters ) in these settings, and slightly better than them ( with P - GCNN ) in other settings. 3. A theoretical justification for the existence of PGEUCCNNs, and a proof that the method is computationally tractable, is given in Sec. 5."
SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,"This paper proposes an efficient MCMC method, called amortized Langevin dynamics ( ALD ), to replace the datapoint - wise MCMC in the estimation of the latent variable in an autoencoder model with an inference model that maps observations into latent variables. Based on ALD, the authors propose a new deep generative model, called Langevin auto - encoder ( LAE ), which uses ALD for the posterior inference and sampling from the latent space EBM. The authors prove that ALD can obtain samples from the target distributions in both conditional and unconditional cases, and ALD converges significantly faster than traditional LD. They also evaluate LAE on the image generation task using three datasets ( SVHN, CIFAR - 10, CelebA - HQ ). They show that LAE performs better than non - amortised MCMC methods, and LAE can also generate better samples."
SP:5631097031c7e599bdeae64366ffa6e4558837c6,"This paper proposes a model for the problem of hypergraph reasoning over large domains, e.g. predicting the relationship between several entities based on the input facts. The authors observe that logical rules usually apply locally, and sparsely, in the hypergraph domain. Inspired by this observation, they propose Sparse and Local Neural Logic Machines ( SpaLoc ). SpaLoc represents the grounding of relationships such as parent and grandparent as sparse tensors and uses neural networks and finite - domain quantification operations to infer new facts. They also introduce a sparsification loss to regularize the number of hyperedges in intermediate layers of a SpaLoc model. To enable training on large - scale graphs such as real - world knowledge graphs, SpaLoc makes training and inference - time sub - graph sampling. Specifically, it proposes a novel sampling and label calibration paradigm based on an information - theoretic measure information sufficiency to remedy the information loss in sampled subgraphs.   The paper shows superior accuracy and efficiency on synthetic datasets compared with prior art and achieves state - of - the - art performance on   real   knowledge graph reasoning benchmarks. On two benchmarks : relational reasoning in synthetic datasets ( family trees and general graph reasoning ) and real world knowledge - graph reasoning, the model outperforms the base model NLM."
SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"The paper proposes a new family of differentiable top - k cross - entropy classification loss based on differentiable sorting and relax of the assumption of a fixed k. This is in contrast to existing works such as NeuralNet and Sinkhorn, which use log - probability based loss. The key idea of this work is to draw k from a probability distribution, such as PK, during training and then use it as a differentiable classification loss. They derive new state - of - the - art results on ImageNet with an improved top - 1 / top - 5 accuracy of 88.37 % and 98.68 %, respectively. They also propose splitter selection nets, which are differentiable and require fewer layers than existing selection nets.   The main contribution of this paper is the differentiable method of obtaining the loss, i.e., the relaxation of the constraint of the fixed k in order to compute the classification loss, that is based on the probability distribution. They find that relaxing k does not only produce better top -5 accuracies, but also makes models more robust, which leads to top - one accuracy improvements."
SP:cb3188f435c54a365890e20e4d582c250d919833,"This paper proposes a new method for solving large - scale optimal transport ( OT ) problems, based on the Douglas - Rachford splitting technique. The proposed method enjoys an iteration complexity of O(1/\sqrt(1 ) ) compared to the best - known O(k(1 / ) ) of the Sinkhorn method. In addition, it establishes a linear convergence rate of the proposed method for the original formulation of the OT problem. The authors detail an efficient GPU implementation that maintains a primal - dual stopping criterion at no extra cost. Experiments demonstrate the effectiveness of the method, both in terms of computation times and robustness.   In short, this paper advances the-state - of - the - art in this direction."
SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"This paper presents a systematic study of generalization in federated learning ( FL ), aiming at disentangling performance gaps between client data ( out - of - sample gap ) and performance gaps from interaction with clients ( participation gap ). It presents a dataset synthesis strategy to simulate natural federated datasets without naturally - partitioned data, which it claims provides a realistic simulation without synthetically partitioning data. The authors compare generalization behavior of FL agents on four tasks : MNIST, CIFAR-10, UCI Sentiment Treebank, MNIST-2, and UCI Clustering. They find that generalization performance is significantly different on all tasks when using synthetic federated dataset. They propose a semantic synthesis strategy for generalization and use it to inform a series of recommendations for future FL works."
SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"This paper studies few - shot language models from the BERT family in the zero - shot setting. It finds that the null prompt method outperforms manually created prompts without engineering in achieving much better and stable performance with the accuracy of 74.06 % (±13.04 ), 75.54 %(±11.77 ) for comparison. However, it also observes some limitations of PLMs under the zero-shot setting, particularly for the language understanding tasks ( e.g., GLUE ). To further investigate, it adapts some basic models adapted from the few shot setting and proposes two new strategies for zero - shots setting. Finally, it analyzes the performance of coarse - to - fine study."
SP:9817dccb1a121058b23a2ef825ed339cf8b53674,This paper proposes a new method to improve the alignment and interpretability of the attention mechanism. The proposed method uses a target - specific sharpener module. The sharpener maps the relevant parts of the input image to the target output sequence. The paper claims that this helps build clear alignment because the aligned parts are unable to well represent the target. The key idea of the sharpener is that it deliberately locates the target in an image region and refines representation to be target specific. Experiments on synthetic handwritten digits and real - world scene text recognition datasets show that the proposed method outperforms the mainstream ones such as soft and hard attention.
SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"This paper proposes a method for solving the vehicle routing problem ( VRP ) using deep reinforcement learning. Unlike existing RL - based approaches that only considers a fixed number of vehicles, this paper trains a supervised model to jointly learn the VRP assignment problem and the route length minimization. The key advantages of this approach over the existing methods is that it is fast to train, is able to be applied to problems with unbounded fleet sizes, and can incorporate the practical aspect of vehicle costs. However, this approach is not suited for real - world applications, as logistic service providers rely on solutions provided for a specific bounded fleet size and cannot accommodate short term changes in the number ofvehicles. In contrast, this work proposes a powerful supervised deep learning framework that constructs a complete tour plan from scratch, respecting an apriori fixed number   of available vehicles, and an efficient post - processing scheme. Experiments are conducted to show competitive performance of the proposed method to multiple state - of - the - art approaches where we demonstrate stable performance, while utilizing less vehicles and utilizing less time ( in terms of train time )."
SP:594a813c0d0baa66738b9c8331370f861ad3c416,"This paper proposes a new method for link prediction based on counterfactual questions. Specifically, the authors ask the question : “ would the link exist or not if the observed graph structure became different? ” and derive causal models based on global graph structure as the treatment, and link existence as the outcome.   The proposed method, called CFLP, first generates “ factual ” links from observed links and then learns from both observed and counter -factual links. It uses GNNs to generate the links and learns representations from both them. The method is evaluated on several graph - related datasets and achieves state - of - the - art performance on link prediction."
SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"This paper proposes a two - stage unsupervised feature selection framework named SOFT, which incorporates second - order covariance matrix with first - order data. In the first stage, the framework learns a sparse attention matrix that can represent second order relations between features, and in the second stage, it builds a relational graph based on the learned attention matrix and performs graph segmentation to select features. Experiments on 12 public datasets show that SOFT outperforms classical and recent state - of - the - art methods, which demonstrates the effectiveness of the proposed method."
SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"The paper proposes a new multimodal variational autoencoders ( VAEs ) that aim to capture both a shared distribution over heterogeneous data ( e.g. vision, language ), while also capturing a shared representation across such modalities. The proposed model is called Mutually supErvised Multimodal VAE ( MEME ). MEME is a semisupervised VAE that attempts to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially - observed data where some modalities can be entirely missing, which is something that most existing approaches either can not handle ( or do so to a limited extent ). The paper evaluates MEME on standard metrics on the MNIST - SVHN ( image - image ) and CUB (image - text ) datasets, and shows that MEME outperforms baselines on both partial and complete observation schemes. The quality of the representations learnt by mutual supervision is also compared against standard approaches and observes interesting trends in its ability to capture relatedness, both explicit and implicit."
SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"This paper proposes Deep Explore Options, revising Explore Options within the Deep Reinforcement Learning paradigm to tackle complex visual problems.    In contrast to the widespread method of a weighted sum of rewards, Explore Options let the agent call an intrinsically motivated agent in order to observe and learn from interesting behaviors in the environment. Such options have only been established for simple tabular cases, and are unfit to high dimensional spaces. Next, the authors propose to consider intrinsic reward learning as an auxiliary task, with a resulting architecture achieving a 50% faster wall - clock speed and building a stronger, shared representation. The authors test Deep explore options on hard and easy exploration games of the Atari Suite, following a benchmarking study to ensure fairness. They convincingly beat the baselines in 4 of the 6 tested environments, and with comparable performances in the other 2. They also showed that Deep explore Options can efficiently learn from several intrinsic reward signals, ignore harmful intrinsic rewards, and extract exploiting behavior ; all while maintaining a strong shared representation!"
SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,"This paper proposes a new method for learning Hamiltonian dynamical systems from data. The proposed method SANN identifies and splits the training data into stiff and nonstiff portions based on a stiffness - aware index, a simple yet effective metric they introduce to quantify the stiffness of the dynamical system. They validate the SANN method with complex Hamiltonian dynamics including a three - body problem and billiard model and extensive numerical results show that SANN can accurately predict the the stiff dynamics and significantly outperform the existing methods."
SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"This paper proposes a new approach to train Transformer - based models to perform unbounded multi - step computations, such as adding integers or executing programs. The approach is called “ scratchpad generation ” and consists of : ( 1 ) generate a sequence of scratchpad tokens ( up to a count of 512 ) for each input ( e.g., input x, x+1 ), ( 2 ) predict the output of each of those scratchpads by taking the corresponding intermediate steps ( intermediate tokens ) as input, ( 3 ) evaluate the result of each intermediate step, and ( 4 ) repeat the process for each scratchpad step until a given input is reached ( or until all tokens have been generated ). The paper shows that this approach, when combined with a few - shot learning strategy leads to improved performance when compared to vanilla Transformer models ( in the naive “ step by step ” regime ) on a series of increasingly complex tasks ranging from long addition to execution of arbitrary programs.   The main contributions of the paper are :   1 ) Scratchpad generation provides a way to train language models to generate sequences of intermediate steps for unbounded computations by simply “ writing out ” intermediate steps on the scratchpad ( instead of using the full output of the model ). This leads to a set of synthetic programs that can be executed on given inputs ( inputs x, y ), and the learned Transformer model is shown to be able to generate the corresponding output ( outputting intermediate steps ) on these synthetic programs. This is then evaluated ( in a few shot / iterative fashion ) against vanilla language models. 2 ) it shows that a program execution trained on a scratchpad generated from a single input ( x, z ) yields output that is linearly independent of the input ( output x ), thus provides a trace of the result when executed on each input. 3 ) it also shows that training a model to generate programs that output traces of previous steps, line by line annotated with local variables, improves the performance of executing said programs."
SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"The paper proposes a novel method to generate adversarial perturbations to the network's feature representation that are interpretable, universal to any source image, and physically - realizable. The method is based on a novel objective that encourages the network to perturb the feature of a target image by minimizing an objective that penalizes the probability that the target image is of a particular class. The paper shows that this approach is able to generate attacks on the ImageNet scale that are at least as effective as standard adversarial attacks ( e.g., pixel - space attacks ) in terms of their ability to induce misclassification. They also show that this method is capable of producing attacks that are both interpretable and can be transferred to the real world.    The paper further develops the method for generating “ copy / paste ” attacks, where one copy of a natural image is pasted into another to induce an attack on a class that is not present in the original image. This is motivated by a desire to improve the network ’s ability to discriminate between objects that are “ close ” to the target class and “ far ” from it, and also to “ fool ” the network into classifying objects of the same class using a different natural image. Experiments show that the proposed method outperforms several baselines and is competitive with state - of - the - art adversarial methods."
SP:873618263dc4246a39c44d0abfecfb5f688817e3,"The paper proposes a new approach to the simulated annealing ( SA ) problem. In contrast to existing literature, the authors propose to model the SA process in a reinforcement learning ( RL ) fashion, where the SA chain is a trajectory from an MDP, and view the proposal distribution as a policy that can be optimize for better quality solutions. The proposed model, called Neural SA, is trained using reinforcement learning with a learned proposal distribution. The authors demonstrate that Neural SA with such a learnt proposal distribution outperforms SA baselines with hand - selected parameters on Rosenbrock ’s function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. They also show that neural SA scales well to large problems while again outperforming popular off - the - shelf solvers in terms of solution quality and wall clock time."
SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"This paper studies the problem of non - stationarity in multi - agent reinforcement learning ( MARL ), in which policy divergence between agents changes during the learning process due to policy changes of the joint policies adopted by the agents during the training process. The authors propose a novel notion, the δ - divergence, to explicitly measure the non - Stationarity of a policy sequence, which can be further proved to be bounded by the KL divergence of consecutive joint policies. A straightforward but non - trivial way to measure the divergence is to decompose the joint policy and impose trust - region constraints on the factorized policies, but simple policy factorization like mean - field approximation will lead to more considerable policy divergence. Thus, the authors propose TRD - Net, which is based on message passing and mirror descent to establish a trust region decomposition network based on the learned policy through an end - to - end manner to adjust the trust -region of the local policies adaptively through message passing. The proposed algorithm MAMT, called Multi - agent mirror descent policy algorithm with trust - regional decomposition, is established by adjusting the trust-region of local policies adapted to each agent's individual policies through a message passing mechanism.   Theoretically, it is proved that MAMS is able to approximately satisfy the dual objective of controlling divergence and ensuring stationarity. In the experiments, the proposed algorithm brings noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity of different tasks."
SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"This paper proposes a self - supervised representation learning framework for audio - visual speech, called Audio - Visual Hidden Unit BERT ( AV - HuBERT ). The framework is based on the observation that the speaker's voice and the audience's body language are highly correlated with each other, which provides a strong signal for speech representation learning from the speaker ’s lip movements and the produced sound. To overcome the lack of multi - stream video data, the model learns to predict automatically discovered and iteratively refined multimodal hidden units.   Experiments on visual speech recognition show that AV -HuBERT achieves SOTA using 433 hours of text transcriptions, two orders of magnitude less than the ﬁrst state - of - the - art approach ( 33.6 K ) trained with a thousand times more transcribed video data ( 31,000 hours ). When only the visual modality is available, the lip - reading performance outperforms the prior SOTA by more than 10 % ( relative ). Similarly, the representation learned by AV - HuaweiBERT also improves the representation for the ASR downstream task."
SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,"The paper proposes ECORD, a state - of - the - art GNN - based approach for solving combinatorial maximum cut problems by preprocessing a large set of vertices before sending them to a recurrent unit for exploration. The novelty of ECORD lies in placing the expensive graph neural network ( GNN ) in a pre - processing stage, instead of in the exploration phase of traditional SOTA methods. ECORD preprocesses all of the vertices in an easy - to - evaluate way, before putting them to use in the fast - acting phase directed by the recurrent unit. In this way, ECORD circumvents the need for expensive GNNs in conventional SOTA algorithms, which can become prohibitive due to their large computational cost. To this end, the paper proposes to pre - process the vertice in a way that only requires the GNN to compute one action at a time, while allowing the rest of the actions to be interpreted as independent of each other. The proposed approach is evaluated on a set of graph minimum cut problems where the size of the problem is inversely proportional to the number of vertice. The results show that ECORD outperforms existing SOTA and RL algorithms on the problem, as well as showing scalability improvements."
SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"This paper proposes a new Variational Autoencoders ( VAEs ) model, i.e., a variational variational autoencoder ( VAE ) with discrete latents, by circumventing the limitations of standard VAE mechanisms such as sampling approximation, reparameterization trick and amortization. The proposed VAE model uses a truncated posterior method in conjunction with evolutionary algorithms to improve the variational lower bound of the latent variable, which is then used for training the decoder network to select the latent states for training. The main contributions of the paper are : 1. This paper shows that VAEs can be trained with direct variational optimization using evolutionary algorithms and can select latent states more efficiently than amortized training. 2. It shows direct optimization can improve the performance of VAEs in zero - shot denoising, where a single image is denoised without previous training on clean data and/or training on large image datasets. 3. It also shows VAEs are competitive with non - linear generative models ( DNNs ) when denoizing an image, e.g., VAE models trained on a single clean image and trained on large datasets."
SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,"This paper proposes an unsupervised method based on counterfactual measures of blame to identify effects on the environment controlled by the agent. The method is called Controlled Effect Network ( CEN ). CEN is evaluated in a wide range of environments showing that it can accurately identify controlled effects. The paper also demonstrates CEN ’s capabilities as intrinsic motivator by integrating it into the state - of - the - art exploration method, achieving substantially better performance than action - prediction models."
SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"This paper proposes a new model compression method, called structure - regularized pruning ( SRP ), for the lightweight image super - resolution ( SR ) networks. This method consists of two components : ( 1 ) L2 regularization of the pruned residual blocks and ( 2 ) pruning of the unimportant filters in the original network. The first part of the method aims to get rid of the weights in the residual blocks that are too large or too small relative to the other blocks in the network, i.e., those that are close to the ground truth. The second part aims to prune the weight values in the remaining blocks so that they are close enough to zero that their absence will cause minimal performance degradation. The method is applied to both lightweight and large image SR networks, resulting in a lightweight network SRPN - L and a very deep one SRPN. The experimental results show that the proposed method can achieve performance gains over previous methods both quantitatively and qualitatively."
SP:0dee45001ae9600f485614dfe6874a516ac01db5,This paper proposes a contrastive learning - based approach for few - shot classification. It differs from existing methods in two ways :   1 ) by learning a general representation followed by a feature selection mechanism while fine - tuning on the target domain ; 2 ) by training a feature masking module with appropriate constraints to select relevant features for the few - shots target samples. The proposed approach is evaluated on CDFSL ( a cross - domain classification benchmark ) and shows competitive performance with the state - of - the - art methods.
SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"This paper studies the generalization properties of neural networks trained with different Bayesian inference and gradient descent methods. It finds that gradient descent can further improve generalization by selecting networks with a large margin. This is done by using new tools ( Theorem 1, Theorem 2 ) to both analytically bound and empirically measure the average test error of the neural network - Gaussian process ( NNGP ) posterior. It is found that the average error is significantly worse than the holdout performance of Gaussian processes with large margins. Further, for finite width neural networks, gradient descent is shown to be able to select networks with vanishing posterior probability that nonetheless generalise significantly better than the posterior average. This highlights a curious fact : minimum a posteriori function can generalise best, and gradient ascent can select for those functions."
SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"This paper proposes a new cross - lingual manifold mixup ( X - Mixup ) method to enhance cross - lingual transfer ( CLT ) on the XTREME benchmark. The idea is to firstly ( 1 ) align the representation of source and target text representations and ( 2 ) perform a mixup on top of each representation ( i.e., Gumbel - Softmax - Gumble ) of the source representation and then ( 3 ) apply the same mixup to the representations of target representations. The proposed X - mixup is shown to improve CLT transfer on different tasks and languages, and ( 4 ) reduces the cross - linguistic representation discrepancy ( Figure 1 ). Experiments show that X -Mixup achieves 1.8% CLT gains on multiple tasks and language combinations, compared with strong baselines ( e.g., 2 ) and significantly ( Figure 2 ). The performance gain is especially noticeable for target languages."
SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"This paper studies the problem of federated learning in which a central server trains a machine learning model over data distributed across multiple workers. In this setting, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages to the server. While this problem has received significant attention recently, this paper focuses on realistic settings where the data across the workers are non - iid, i.e., the server does not have access to the identical data of all the workers. The authors propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. They also theoretically and experimentally validate their approach, showing that combining bucketing with existing robust algorithm is effective against challenging attacks.   This work is the first to establish guaranteed convergence for the non -iid Byzantine robust problem under realistic assumptions. The setting is assumed that the set of Byzantine workers B ⊂ [ n ] is fixed over time, with the remaining workers G being good, and the remaining data G = [ n-1, \sqrt{B } ]. We write $ \gamma_k$ as $ \tilde\gamma_{k-1}$ for the fraction of workers B, where B| = \mathbb{B| : q \leq q \dots, \nabla_{\text{B}}$. The Byzantine workers can be thought of as i.i.d. Therefore, the authors prove that there exists an attack model $ \Byzantine$ where $ \mathbf{B}$ is a class of attackers. Further, they show that the attacker can collude with other workers and know the states of all other workers. This attack model is then used to obtain data of fixed size $ n$, and train the model on this fixed data. The paper provides theoretical guarantees and experimental results to support the theoretical guarantees."
SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,"This paper studies the effect of hard parameter sharing in multi - task learning on the disentangled representation learned by neural networks trained on automatically generated supervised tasks. The paper first considers the question of whether disentanglement is beneficial when parameter sharing is encouraged through the use of hard - learning. The authors provide a set of metrics that indicate the degree to which a model learns representations that are least distanced from the ground truth. These metrics can be interpreted as measures of how well the model learns to disentangle itself. In general, it is known that disentangling is more difficult for models trained on different tasks and more difficult when it is applied to the same task ( e.g., when the model is only trained on one task and the data is available for reuse on other tasks ). Based on these metrics, the paper finds that the representations obtained by models trained with hard - parameter sharing tend to have nontrivial levels of disenanglement. However, the results are not conclusive and may not be indicative of a clear advantage when disentangler is introduced, as the obtained by them results vary for different datasets."
SP:9851adb72e2918780f661f83f7da06eb866787be,"The paper proposes CROP, a framework for certifying the robustness of policies and rewards under adversarial attacks in reinforcement learning ( RL ). The CROP framework has three main components : 1. A policy smoothing algorithm that uses Gaussian noise over each encountered state to ensure that the policy is at least as good as the policy derived from Q - functions smoothed with it. 2. A global smoothing method that applies this technique to cumulative rewards. 3. An adaptive search algorithm that applies it in local settings.   CROP is evaluated on three RL algorithms ( RegPGD, RegCVX, and RadialRL ) that are empirically shown to be empirically robust against adversarial training and regularization in three Atari games. The results show that the algorithms achieve high certified robustness compared to other algorithms not included in the CROP evaluation, and that the certifications are often tight. The authors also provide several interesting observations which would further inspire the development of robust RL algorithms."
SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"This paper proposes a new approach to conformal prediction ( CP ), in which a classifier is trained based on a set of candidates CP candidates. The key idea is to constrain the number of false positives in the candidate CP sets, such that there are only a finite number of correct answers. The proposed approach is based on the notion of set coverage, i.e. the true positive rate that allows for any number of true answers for a given query ( including zero ). To achieve this goal, the proposed approach first constructs a "" conformal set "", where the correct class is assumed to both exist and be unique, and the answer set is of size one. Once the predicted sets are constructed, the set coverage is used to train another classifier.   The experiments compare the proposed method with CP baselines ( including a naive, noisy candidate set ) on the following tasks : MNIST, CIFAR-10, and ASG. The experiments show that the proposed methods outperform the baselines on all of these tasks."
SP:b126d2f3c397633745c8833e22ace93a2470e963,"This paper analyzes the expected length distortion of ReLU networks with standard random initialization. The authors prove that the distortion does not grow exponentially with depth, as was previously assumed, and indeed shrinks slightly with depth. They also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher - dimensional volumes. The theoretical results are corroborated by empirical results on MNIST and CIFAR-10 data.   The main contributions of the paper are as follows :   - First, the authors show that length distortion is bounded by $ \ell_1 $, and decreases slightly with increasing depth. - They provide an exact expression for the mean length distortion, which is bounded above by $ 1 $. - The authors also prove that higher moments have well - controlled upper bounds. - In addition, they show empirically that their theoretical results closely match observations, unlike previous loose lower bounds, and empirically verify their lower bound. - Second, they provide preliminary results suggesting that expected distortion decreases modestly with depth in both convolutional networks and those with skip connections."
SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"This paper proposes SAFEty skill pRiors ( SAFER ), a behavioral prior learning algorithm that learns to extract a safety variable from offline data that encodes safety requirements, as well as the safe primitive skills over abstract actions in different scenarios. In the inference stage, SAFER composes a safe and successful policy according to the safety variable and abstract action, and demonstrates its effectiveness on several complex safety - critical robotic grasping tasks inspired by the game Operation, in which SAFER outperforms baseline methods in learning successful policies and enforcing safety."
SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"This paper proposes a multi - branch neural network architecture for image restoration. The proposed model is based on the multi - path neural network from Retinal Ganglion Cells ( RGCs ), which can achieve multiple restoration tasks in a general framework. Specifically, the proposed model, called CMFNet, has four different tasks : image dehazing, deraindrop, deblurring and image enhancement, which are common applications for self - driving. The source code and pretrained models of three restoration tasks are available on GitHub. The experiments demonstrate the competitive results of our CMF net in both synthetic and real - world datasets for three different types of restoration tasks. The improvements of SSIM values are also demonstrated. The performances of our model are closer to the human sense."
SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"This paper proposes a federated learning framework, Inference - time PFL ( IT - PFL ), where a model is initially trained on a set of clients and is then evaluated on novel clients ( termed novel clients in the paper ) at inference time. The novel clients are those who join a prediction service after the trained model has been deployed and want to obtain predictions for their own unlabeled data. To handle the data heterogeneity between clients, the authors propose a hypernetwork module and an encoder module to learn a representation for a client that is then fed to a hyper network to generate a personalized model for that client. Evaluated on four benchmark datasets, the proposed method generalizes better than current FL and PFL methods, especially when the novel client has a large domain shift. The paper also analyzes and bounds the generalization error for the novel clients."
SP:960d0a63a82593f6e72275b65f0501f0469d1924,"This paper analyzes representations learned with self - supervised learning ( SSL ) by using a conditional diffusion based generative model ( CRDM ). The objective is to measure the representation's invariance to various augmentation functions ( e.g. color, contrast, scale, grayscale, etc. ) without requiring any labelled data augmentation tasks. To do so, the authors train a representation conditioning model ( CFD ) on the original input and a projection model ( PS ) on a subset of the original inputs, and train a copy of the projection model conditioned on each of these inputs to generate a sample. The samples are then randomly sampled from the FCD representation and scores are used to evaluate the invariance of the PS and PS representations to reveal information that is not present in the unconditioned representation. The results show that : 1 ) PS representations are not as invariant as we would like them to be, 2 ) SSL representations tend to be more robust to adversarial noise than those of PS and SS, 3 ) there is an inherent structure learned with SSL models that can be exploited for image manipulation, as can be seen by ( i ) splitting representation in foreground/background components to allow background substitution and ( ii ) the discovery of exploitable structure in the representation ’s dimensions, and 4 ) the generation quality of the CRDM is evaluated."
SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,"The paper studies the differential privacy of Fp sketch, a well - known ( and highly - sought - after ) frequency - moments estimation ( Fp ) algorithm with polylogarithmic space complexity. It is claimed that Fp is differentially private as is when p = 0, 1, or when p \in ( 0, 1 ), and thus gives the first ( and so far only ) non - private FP protocol with space complexity of this kind with DP guarantee.   The main contributions are the following :   1. Proposing a new sensitivity definition for Fp : pure multiplicative sensitivity ( PMS ), which is defined as the maximal multiplicative change in the output when the inputs are neighboring datasets. The paper analyzes Fp and shows that it is exponentially better than existing DP baselines and only worse than the optimal not - private baseline by a logarithmically factor. 2. Providing a reasonable accuracy guarantee for FP with differential privacy. Theorem 3. Concretely, it is shown that F1 sketch preserves - DP if and only if p = 1, and for all other parameters $ \mathbb{P}$. Theorem 4. Further, numerical experiments are provided to prove the conjecture and conjecture is proved."
SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"This paper proposes Reward - Switching Policy Optimization ( RSPO ), a method to learn novel policies by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones. The motivation is to encourage the learning policy to consistently converge towards a previously undiscovered local optimum that can be discovered through novelty - constrained methods. The proposed method switches between extrinsic and intrinsic rewards via a trajectory - based novelty measurement during the optimization process, which is designed to encourage exploration. Experiments are conducted on single - agent particle - world tasks, MuJoCo continuous control, stag - hunt games, and StarCraft II challenges. The results demonstrate that the proposed method is both general and effective across a variety of single- agent and multi - agent domains."
SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high - fidelity sample. This paper introduces Differentiable Diffusion Sampler Search ( DDSS ), a method that optimizes fast samplers for any pre - trained diffusion model by differentiating through sample quality scores. The proposed method belongs to the Generalized Gaussian Diffusion Models ( GGDM ) family, a family of flexible non - Markovian sampler for diffusion models. The authors show that optimizing the degrees of freedom of GGDM - sampler by maximizing the sample quality score via gradient descent leads to improved sample quality. The optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization.   The experimental results on unconditional image generation across various datasets ( e.g. FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM / DDIM baselines ) are provided in the paper. The method is compatible with any pre-trained diffusion model without fine - tuning or re - training required."
SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,"The paper proposes P - adapters, lightweight models that sit between the embedding layer and first attention layer of Large Language Models ( LLMs ). They take LLM embeddings as input and output continuous prompts that are used to query the LLM. The authors compare them to MoE models that learn a set of continuous prompts ( “ experts ” ) and select one of them to query in LLM, while requiring a separate classifier trained on human - annotated data to map natural language prompts to the continuous ones. They show that P - adapter outperforms MoE and MoE while eliminating the need for additional annotations. The ablations are performed and the authors conclude that their method is helpful in reducing the impact of variable prompts."
SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"The paper proposes Continuous Classification of Time Series ( CCTS ), which is a continual learning task to achieve high - accuracy classification at every time. The idea is to model multiple distributions simultaneously, as time series always evolves dynamically, changing features introduce the multi - distribution form. Different from the existing one - shot classification, the key of CCTs is to models are independent identically distributed. However, most models are hard to achieve it due to their static assumption that all time series are identical. If a model learns a new distribution, it will likely forget old ones and if a model repeatedly learns similar data, it might be overfitted. The paper proposes a novel Adaptive model training policy ( ACCTS ) to overcome these two main problems. The key is to overcome the catastrophic forgetting and the overfitting. Instead of the fixed rules and the prior knowledge, ACCTS extracts data distributions adaptive to the time series evolution and the model change. In addition, it also proposes an Adaptive importance - based replay policy ( ARC ) to only replay the important samples. Experiments on four real - world datasets show that ACCTS can classify more accurately than all baselines at each time."
SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,"This paper proposes a k - nearest - neighbor ( kNN ) based memory approach to augment the memory of language models with external references to increase the length of the context that a language model can attend to by using k - kNN - lookup into a large external memory. The proposed kNN memory is not differentiable, but an approximate kNN lookup into the memory improves the performance of the model on various benchmarks and tasks, including generic webtext, math papers, books ( PG-19 ), code ( Github ), and formal theorems ( Isabelle ). They show that the performance steadily improves when we increase the size of memory up to 131k tokens. They also show that models can generalize to larger memory sizes than the one they were trained on."
SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"This paper proposes a method to sample directly from the masked language modeling ( MLM ) objective using a Metropolis - Hastings sampler based algorithm based on the Hastings - Metropolis algorithm. The motivation is that the MLM objective does not specify a principled probability distribution over the space of possible sequences, but rather an energy network. The paper proposes to interpret MLMs as energy - based sequence models and proposes two energy parametrizations derivable from the trained MLMs. Then, the authors propose a conditional generation algorithm that generates samples from the energy network using the proposed parameters. Experiments are conducted on two tasks : open - ended unconditional generation of sequences and conditional generation of machine translation. The experiments show that the proposed method generates higher quality samples than other recently proposed undirected generation approaches. The empirical analysis and success of the approach with the two proposed energy network strongly suggests that the optimization of the objective results in training of an implicit global energy network that induces probability distribution."
SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"This paper proposes a new augmentation policy learning method for NLP tasks. The main idea is to jointly train the augmentation policies to construct difficult but not too different samples, e.g., to provide informative training signals, while avoiding the risk of losing the semantics of original samples. To achieve this goal, the authors propose a novel reward function that rewards jointly training and selecting the difficult augmented samples. In addition, they introduce a sample re - weighting scheme to focus on difficult augmented examples after the original ones are learned confidently for more effective learning from the augmented ones. Experiments are conducted on various text classification datasets and GLUE benchmark ( Wang et al., 2019 ) where their method consistently improves over the recent state - of - the - art augmentation schemes by successfully discovering the effective augmentations for each task. They also found that their method is more effective on the challenging low - resource and class - imbalanced regimes, and the learned augmented policy can easily transfer to the different tasks and models."
SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"This paper proposes FOCAL++, a meta - learning framework for offline RL based on task representation learning. The authors propose two new intra - task learning mechanisms, i.e., self - attention and MoCo, to learn task representations. Self - attention consists of applying a batch - wise gated attention function to update the weights of transition samples based on the observed reward distribution. MoCo is an extension of the Momentum Contrast objective from He et al. ( 2020 ) and replaces the dictionary queue in MoCo with meta - batch samples generated on the fly.   The authors show that MoCo outperforms naive contrastive loss and model - free contrastive learning in terms of performance and robustness to reward and action distribution shift with the help of these two learning mechanisms. Theoretical analyses and experiments are presented to support the superiority of the proposed method over prior algorithms and compare it to baselines on four benchmark meta - RL benchmarks."
SP:ed86c60850d5c8302dcf1c2167db303e778fe681,"The paper proposes a parametric sequential generative modeling method for improving the accuracy of the belief model at inference time. The method is named "" belief fine - tuning ( BFT ) "", which leverages approximate dynamic programming to determine the model parameters at each time step of inference. It is claimed that BFT can improve the accuracy because it specializes the capacity of the model to the space of local observations. Furthermore, because decision - time planning is performed on top of BFT, it is able to approximate public belief state search in the Hanabi benchmark. This is the first instance of successful approximate belief state - based search in a multi - agent setting in which computing an exact belief state is intractable."
SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"The paper proposes a method for speeding up the training of sparse neural networks by optimizing over a continuous set of sparse matrices known as products of butterfly matrices. Specifically, searching for a sparsity mask over a discrete set of the chosen products is difficult and expensive, and the paper proposes to use a simple fixed sparsity pattern based on flat block butterfly and low - rank matrices to sparsify most network layers. The paper empirically shows that the proposed method, Pixelated Butterfly, is 3x faster than butterfly and speeds up training to achieve favorable accuracy - efficiency tradeoff without loss of accuracy. The method is applicable to popular models such as MLP - based and Transformer - based architectures, and it is claimed that it can speed up training of models ( Transformers, ViT, MLPMixer ) without loss in accuracy."
SP:136e31054a55abca840f6478491972023c2296cb,"This paper proposes a score - based generative model based on conditional diffusion probabilistic diffusion ( DDPM ), where the forward process of the model is the Markov chain, and the reverse process is the class clustering. The proposed method is named ST - DDPM, which stands for “ shift from sample to its corresponding class center ” during both the forward and reverse process, and is a modification to the original formulation of DDPM. It is claimed that this method enables controllable generation and gets interpretability, and provides another direction for faster sampling. Experiments are conducted on multiple tasks, and achieve competitive results compared with the state - of - the - art methods."
SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"This paper proposes a new method for domain generalization ( DG ) based on LASSO ( Learning to Exploit Sub - Space Orientation ) framework. In contrast to most existing DG methods, the proposed method explores diverse latent sub - spaces and learns individual hypotheses for each sub - space instead of constructing a single hypothesis shared among all training domains. The proposed method also uses different sampling strategies for source and target examples to project the target domain onto appropriate latent sub space. Moreover, the label - informative features of the source domain are preserved in the latent space to preserve crucial label - information for label prediction. Experiments on several well known DG benchmarks, where it achieves state - of - the - art results, are provided to demonstrate the effectiveness of the proposed approach."
SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"This paper improves upon the original kernel thinning algorithm of Dwivedi & Mackey ( 2021 ) by making it applicable to non - smooth analytic kernels, such as Laplace and Matérn, that do not have square roots. The improvements include : 1. Strengthening the MMD guarantees of target KT for analytic kernels such as Gaussian, multiquadric, and sinc, which are less affected by the presence or absence of a square root than square - root KT. 2. Showing that target KT admits maximum mean discrepancy ( MMD ) guarantees comparable to or better than Monte Carlo MMD for power KT, which uses fractional power kernels. 3. Providing a procedure called KT+ that inherits the guarantees of   TARGET KT and   KT+, which is a generalization of ROOT KT, for compact approximations of a probability distribution, by combining it with a sum of target and power kernels, to yield improved better - than - i.i.d. performance. 4. Experiments demonstrate significant improvements in integration error even in 100 dimensions and when compressing challenging differential equation posteriors."
SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"This paper presents an open - source benchmark suite for the MaxIMUM INDEPENDENT SET ( MIS ) benchmark, in both its weighted and unweighted variants. The suite unifies the evaluation of state - of - the - art traditional and machine learning - based solvers for the problem. It also conducts an in - depth analysis of the popular guided tree search algorithm by Li et al. ( 2018 ), testing various configurations on small and large synthetic and real - world graphs. The paper also extends the analysis to compare the tree search implementations to other classical algorithmic solvers, such as GUROBI and KAMIS, and a recent solver based on reinforcement learning, and shows that for this solver, the GNN is responsible for the competitive solution quality. Finally, it shows that LEARNING WHAT TO DEFER seems to be able to find good results very quickly, indicating that unsupervised reinforcement learning for combinatorial problems is a promising direction for future research."
SP:155ecd17d264a084b014abdfd0362146d8fb07e0,"This paper proposes Wavelet Compressed Convolution ( WCC ), a novel approach for activation maps compression for 1 × 1 convolutions ( the workhorse of modern CNNs ). WCC achieves compression ratios and computational savings that are equivalent to low bit quantization rates at a relatively minimal loss of accuracy. To this end, it use a hardware - friendly Haar - wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. The experiments show that WCC can be utilized with any 1× 1 convolution in an existing network architecture and achieves compression rates equal to 2 - bit and 1 - bit with minimal degradation in image - to -image tasks."
SP:004865e6affad32403b7965493a53c8a7ffdda0a,"This paper proposes a no - regret learning dynamics for learning in extensive - form correlated equilibrium ( EFCE ) in multiplayer general - sum imperfect - information games. More specifically, it proposes an $ O(T^3/4)$ approach to learn a correlated equilibrium $ \mathcal{O}(\sqrt{T})$ in T rounds, where T is the number of rounds in the game. This is achieved by ( 1 ) augmenting the learning dynamics of Nash equilibrium in normal - form games with a $ T$-step process, and ( 2 ) by ( 3 ) adding a $ \tilde{O }(\log T)$ step at each round to the end of each round. The proposed approach is shown to converge to EFCEs in 3/4 of the rounds, which is an improvement over the best prior estimate of $ O(\t^1/2)$.    The main contribution of this paper is the introduction of the concept of - regret in the context of learning in games, and the application of this concept to the setting of extensive form games. This setting is more challenging than normal form games, as it can capture sequential and simultaneous moves, and more importantly, imperfect information. The paper provides a method to characterize the stability of the fixed points associated with trigger deviation functions through a refined perturbation analysis, and experiments on standard benchmarks corroborate the theoretical findings."
SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"This paper proposes an approach for learning discrete action discretization techniques for reinforcement learning ( RL ) in the continuous control setting, where the agent is provided with a finite number of discrete actions to explore. This is in contrast to prior work ( Tang et al., 2017 ), which uses action - value functions in the dynamic programming setting. The main contribution of this paper is to propose a method ( AQuaDem ) to learn such discretizations by taking advantage of demonstrations. In particular, it leverages the fact that the actions in the demonstration space are plausible given the observation of the demonstrator ’s behavior. This allows to apply any discrete action - specific RL algorithm ( e.g., TD3 ) to the action space of the agent. The paper provides a neural architecture for such a framework, and empirically evaluates it on three different setups ( RL with demonstrations, RL with play data, and imitation learning ), where it outperforms state - of - the - art continuous control methods ( TD3, TD4 ) both in terms of performance and sample efficiency on a variety of robotics tasks."
SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,"This paper proposes AdvStyle, an adversarial style augmentation approach for the task of domain generalization ( DG ) in semantic segmentation. Specifically, AdvStyle dynamically generates hard stylized images by learning adversarial image - level style feature, which can encourage the model learning with more diverse samples. Experiments on two synthetic - to - real settings show that AdvStyle can largely improve the generalization performance and achieve state - of - the - art performance. In addition, Adv style is applied to domain generalized image classification and produces clear improvement on the considered datasets."
SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"The paper proposes a method to learn the similarity between real and synthetic mid - air gestures. The system consists of an event - based guided variational autoencoder ( i.e., a neural network that learns to encodes the high - dimensional visual data captured at the sensor into a latent space representation, and a learned classifier to classify the relative representations. The latent space representations are disentangled so that the target features can be learned. The classifier is jointly trained by two classifiers such that the latent space correctly represents target features.   The method is evaluated on the DVS - Gesture dataset and it achieves 87% classification accuracy. The method also implements the encoder component of the model on neuromorphic hardware. The paper also discusses the potential for the algorithm to enable real - time, self - supervised learning of natural mid - flight gestures."
SP:2e66468a6b94177e54b0052b97713ee63902c278,"This paper proposes a method to improve the training performance of Hierarchical Table Ensembles ( SHTEs ) by replacing neurons with ferns ( oblivious decision trees ). The method, named S - HTE, is based on annealing, where the weights of each fern are sparse at the beginning of the training process, and increases in depth as the training goes on. Annealing is used to encourage the internal representations of the fern to become more complex as the model size increases. The model is trained end - to - end with back - propagation, unlike previous methods ( e.g., NeurIPS ), which uses a single backpropagation step. Empirical results show that S -HTE improves the accuracy of SHTE over other methods, while having a much lower computational cost."
SP:b238db9252d83a13438bb747d70e635bb9945958,"This paper proposes an offline RL method, Latent Action Q - Learning ( LAQ ), to learn value functions from an undirected observation - only state - transition setting, by approximating a tabular Q - learning task in the latent space of discrete MDPs. They show that LAQ recovers value functions that have high correlation with value functions learned using ground truth actions, and in turn, learns value functions highly correlated with ground truth. Good value functions in - turn lead to sample efficient acquisition of behavior, leading to significant improvement over learning with only environment rewards. Their experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods."
SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"The paper proposes SWARM, a model parallelism algorithm for training neural networks on poorly connected, heterogeneous, preemptible devices that are not connected to each other in a way that would make it possible to parallelize training of very large models with pipeline parallelism. Specifically, the authors propose SWARM to overcome the challenges of parallelism on such devices under the Square Cube Law of distributed training, which states that if the number of parallel nodes in the pipeline is greater than some threshold value, then the parallelism should be increased. In order to achieve this goal, SWARM divides the nodes into randomized parallel streams that are restarted when one of them fails. The paper also proposes compression - aware techniques to further improve the performance of the SWARM algorithm.   The paper demonstrates that SWARM is capable of parallelizing a large Transformer language model with 1.1 B shared parameters ( before sharing ) on a swarm of preemptible low - power T4 GPUs with less than 400 Mb/s network throughput on a benchmark dataset."
SP:91d2f094d5481651b554f58aecc2a6207057a47c,"This paper proposes a method for bridging the gap between offline MARL and online MARL in decentralized MARL. The authors claim that the main contribution of this paper is the proposed Online Transition Correction ( OTC ), which aims to adjust the biased dynamics of the transition dynamics during online interaction in order to improve the value estimates of offline transitions. OTC consists of two measures to measure the similarity between transitions and an adaptive rank - based prioritization to sample transitions for updating the agent policy according to the transition similarity. Experimental results show that OTC outperforms baselines for online tuning in a variety of tasks."
SP:d0e650d568214481b07a0452ec606ccbf6d05410,"This paper proposes a method to quantize neural gradients in the forward and backward phase of neural networks to 4 - bit precision. The forward phase constitutes only a third of the training process, while the backward phase is the most computationally expensive. The proposed method is based on analyzing the difference between round - to - nearest and stochastic - rounding. The former has lower MSE and works better for the quantization of the forward phase ( weights and activations ), whereas the latter is an unbiased approximation of the original data and is better for quantizing the reverse phase. Based on these conclusions, the authors propose a logarithmic unbiased quantizer ( LUQ ) to quantise the gradients to format FP4 [ 1,3,0 ]. The authors also propose two more methods to improve the results, with overhead comparable to Sun et al. ( 2020 ), one of which is to reduce quantization variance by averaging several samples of LUQ. The second is a simple method for fine - tuning in high precision for one epoch. Finally, they suggest a method that exploits the low precision format of FP4 format by avoiding multiplications during two - third of training, thus reducing by 5x the area used by the multiplier."
SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"This paper studies the differences between monothetic and polythetic classifications of binary classifiers. In particular, the paper focuses on the application of meta - learning methods for few - shot classification of categorical sets of features. The paper shows that threshold classifiers such as Prototypical Networks ( PNs ) require an exponential number of active features, whereas attentional classifiers, such as Matching Networks ( MNNs ) are able to solve these problems with a linear embedding dimension. On the other hand, the proposed selfattention feature - selection mechanism proposed in this paper proposes to adaptively dilute non - discriminative features to the attention space, in order to reduce the risk of misclassification. The proposed self - attention mechanism is simple and can be used in conjunction with most few shot meta - learners. Experiments are conducted on several synthetic and real - world tasks, and the proposed method is shown to outperform the state - of - the - art."
SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"This paper proposes a multi - agent RL environment and training methodology to explore whether and how language can emerge when using RL to train agents that communicate via continuous acoustic signals. The paper introduces a simple messaging environment where a speaker ( agent ) needs to convey a concept to a listener ( listener ) over a lossy continuous channel, and the Listener needs to map the continuous signal to the concept by using a vocoder. The whole process is repeated until convergence, where the speaker learns to be able to successfully convey a combination of two concepts ( e.g., a speaker and a listener ). Experiments show that : 1 ) language does form when combining two concepts ; 2 ) channel noise affects generalisation ; and 3 ) a caregiver, predisposed to “hearing ” or “ speaking ” English, can ground the emergent communication."
SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"This paper proposes a novel backdoor method to attack pre - trained language models. The attacker first reconstructs the pre - training data by poisoning public corpus and fine - tuning a clean foundation model with the poisoned data. Then, the backdoored foundation model will be released to the public for users to train downstream models. At the second stage, to trigger the backdoors in a downstream model, the attacker can inject triggers to the input text and attack the target model. Different from ( Zhang et al. 2020 ), BadPre does not need any prior knowledge about the downstream tasks for embedding backdoors. Besides, it also design a trigger insertion strategy to evade backdoor detection. Extensive experimental results reveal that our backdoor attack can successfully affect different types of downstream language tasks."
SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"The paper proposes an unsupervised skill discovery method called incremental skills ( DISk ), which learns skills one after another in an incremental fashion. The idea is that traditional skill discovery methods assume a static and evolving environment during training, which makes it hard for skills to adapt quickly to changes in the environment or to not forget earlier skills after such adaptation. In contrast, DISk learns skills incrementally based on a fixed set of skills that are learned one at a time. The static skills are learned in a way that allows the skills to be easily transferred to new environments or changes in dynamics. The incremental skills ensure that the previously learned skills are not forgotten and are able to play a role in guiding the exploration of new environments.   Experiments are conducted in both dynamic and static environments, and the proposed method consistently outperforms state - of - the - art methods on both skill quality and the ability to solve downstream tasks."
SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"This paper proposes a novel convolution layer called log - polar space convolution ( LPSC ) that extends the local receptive field ( LRF ) of convolution kernels by adapting the convolution kernel's elliptical shape based on the direction and logarithmic distance of the input. The proposed LRF can be applied to increase the receptive field while reducing the number of parameters. This LRF is achieved by dividing the input pixels into different regions based on direction and distance and using a different receptive field for each region. The paper shows that this LRF increases the single - layer receptive field of conventional convolutions and increases the LRF of convolutions with Lipschitz regularization. In order to implement the LPsC in existing CNNs, the authors propose to pool the input features by using log - polarity space pooling and separate center - pixel convolution. Experiments on different CNNs and datasets demonstrate the effectiveness on different tasks and datasets to justify the advantages and drawbacks of the proposed LpsC."
SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"This paper studies the generalization of neural networks ( NNs ) based on information stored in weights ( IIW ). The main contribution is to build an approximation of IIW, i.e., PAC - Bayes Information Bottleneck ( PIB ), which is based on the trade - off between accuracy and complexity of NNs. Based on PIB, the authors derive a non - vacuous bound of the fitting - compressing phase transition in NNs and empirically identify the connection between fitting and generalization through IIW. Moreover, they propose an MCMC - based algorithm to sample from the optimal weight posterior characterized by PIB to provide an efficient approximation of it. Finally, they demonstrate IIW is able to explain the performance drop w.r.t. the degree of label noise and provide an insight into an optimal batch size."
SP:a733847ade77ffbf38760fc79da17893dea8d53f,"This paper investigates why indiscriminate data poisoning attacks, which add imperceptible perturbations to training data to maximize the test error, work in principle. The authors find that the perturbation generated by advanced poisoning attacks is almost linear separable when assigned with the target labels of the corresponding samples. This observation confirms that linear separability is indeed the workhorse for recent attacks. It also gives an explanation for why pre - trained feature extractors can be a powerful defense. Synthetic experiments are also conducted to demonstrate the effectiveness of such synthetic perturbing. The results demonstrate that synthetic pertubations are as powerful as the deliberately crafted attacks."
SP:7b50be406138ad01db3ee112899f622637896fe9,"This paper considers the importance - weighted return ( i.e., propensity weights ) for offline policy evaluation, and proposes an algorithm ( POELA ) to address an overfitting problem with the importance sampling in the optimization process. The paper provides a theoretical justification of the proposed algorithm through a better per - state - neighbor normalization condition, and shows the limitation of previous attempts to this approach through an illustrative example. They further test their proposed method in a healthcare - inspired simulator and a logged dataset collected from real hospitals, which show the proposed method with less overfitting and better test performance compared with state - of - the - art batch reinforcement learning algorithms. The proposed algorithm, named Policy Optimization with ELigible Actions, is shown to outperform the baselines of several offline RL algorithms in both domains."
SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,"This paper presents a method for continual learning based on CLIP. The idea is to train a language - image embedding model in a way that allows the model to adapt itself to changes in the target language over the course of training. For example, if a user starts speaking a new language after seeing a set of examples of the same language ( e.g., a new image ), the model first adapts its multimodal embedding to capture this new image. Then, the language embedding is also adjusted according to the needs of the learner. The method is evaluated on two continual learning benchmarks : ( 1 ) CLIP : a zero - shot image classification task where the user sees only a few examples, and the model is trained to predict a label for each example ; ( 2 ) Robust : a new Robust classifier is learned by sampling a subset of the input image during training that is similar enough to other examples. The evaluation shows that the method generalizes fairly well to new examples that are presented in a similar way ( i.e., with similar language )."
SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"The paper proposes VLAF2, a framework for learning visual - linguistic - linguistic features for novel object captioning ( NOC ). In particular, three aspects of the NOC model are quantified ( i.e., fluency, fidelity, and adequacy ), which are related to language fluency ( e.g., natural language expression of captions ), object interest, and the visual concept of the image. The paper first provides an analysis of how BERT and CLIP can be used to learn such features in NOC models. Then, the VLIF2 model is applied to the nocaps dataset, where it is shown to outperform the state - of - the - art on three metrics related to NOC, and also performs well on other NOC metrics, such as SPICE and SOTA. Finally, the paper provides ablation studies to analyze the effectiveness of the reinforcement learning component of the model."
SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"This paper studies neural collapse of the feature maps learned by overparametrized classification models. More precisely, this paper studies the phenomenon that the features of overparameterized models collapse when applied to new samples generated from the same distribution as the training samples. The authors make connections between this phenomenon ( called within - class variance collapse ) and the transfer learning strategy of learning feature maps to learn representations that are transferable to new, unseen classes. More specifically, they show that when neural collapse is observed in the new classes, then it requires very few samples to train a linear classifier on top of the learned feature representation that accurately predicts the new class. These results provide a justification to the recent successes of transfer learning in few - shot tasks, as observed by Tian et al. ( 2020 ) and Dhillon et al ( 2020.1 ). The inferior performance of their method compared to a similarly simple method, Distill - Simple, is most likely due to the choice of the final few - shots classifier, namely, our ridge regression classifier is inferior to their logistic regression solution, while it is superior compared to their nearest neighbor classifier."
SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"This paper proposes a method to jointly solve the tasks of densification and denoising of the point cloud obtained from 3D scanner data. The proposed method consists of a 3D sparse stacked - hourglass network, which is used for the initial densification, and a refinement via transformers converting the discrete voxels into 3D points. In particular, the proposed method further improves the performance of transformer by a newly proposed module called amplified positional encoding. This module has been designed to differently amplify the magnitude of positional encoding vectors based on the points ’ distances for adaptive refinements. Extensive experiments demonstrate that this network achieves state - of - the - art performance among the recent studies in the ScanNet, ICL - NUIM, and ShapeNetPart datasets. Qualitative and quantitative results demonstrate significant improvements in terms of reconstruction’s accuracy and generalization."
SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"This paper proposes a new method, PipeGCN, for efficient distributed full - graph GCN training. The key idea is to replace inter - partition communication of node features and gradients in distributed GCN with intra - partition computation to reduce the communication overhead. The paper shows that this method is non - trivial as without it, the existing pipeline methods such as GCN - GAN, GAT, and SOTA can get stale gradients and feature gradients, which can hurt the convergence. The main contribution of this paper is to provide a theoretical analysis of the convergence of the proposed method. The convergence rate is shown to be close to that of the vanilla method ( GCN ) without staleness. Also, a smoothing method is proposed for improving the convergence rate further. Experiments are conducted on CIFAR-10, Cifar-100, and ImageNet, and show improvements over the vanilla GCN and GAT methods."
SP:8302d49558ee0f16392d623d4e604e92db10d041,"This paper proposes marginal entropy minimization with ensembled augmentations ( MEME ), a method for improving test - time robustness by minimizing the entropy of the model ’s average, or marginal, output distribution across different data augmentations on the test example, and then adapt ( all of ) the model parameters on the data point. The authors evaluate this approach on two baseline ResNet models, two robust ResNet-50 models, and a robust vision transformer model, and demonstrate that this approach achieves accuracy gains of 1-8% over standard model evaluation and also generally outperforms prior augmentation and adaptation strategies. For the setting in which only one test point is available, MEME achieves state - of - the - art results on the ImageNet-C, ImageNet - R, and, among ResNet - 50 models, imageNet - A distribution shift benchmarks. The ablation study in Section 4 shows that an ablation of the neural network prediction error is important for maximal performance gains, and also an empirical study of the importance of invariance and confidence is provided."
SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"The paper proposes a model - based reinforcement learning ( MRL ) algorithm called '' Messaget No More '' ( MNM ). The main idea is to jointly train the model and the policy such that updates to either component increase a lower bound on expected return. This joint optimization mends the objective mismatch in prior work. The authors claim that there is a tight bound on the expected return which becomes tight at optimality under certain assumptions. Compared to previous work, MNM has the unique property of jointly optimizing the policy and model using the same objective. Across a range of tasks, they demonstrate that their method is competitive with prior state - of - the - art methods on benchmark tasks. On certain hard exploration tasks, their method outperforms prior methods based on maximum likelihood estimation."
SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"This paper proposes a simple but effective model combination approach to resolve the copycat problem. The proposed model combines BC - SO, BC - OH, and BC - C - C, which is referred to as the coarse - to - fine imitator. The coarse - action is computed based on the observation history, and the coarse action is used to refine the historical action.   The experiments show that this outperforms all baselines on CARLA autonomous driving from images and various MuJoCo continuous control tasks. Furthermore, extensive ablation studies are conducted to verify that the method outperforms other methods."
SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,"This paper proposes DyAd, a model - based meta - learning method for dynamics forecasting that generalizes well to different physical domains. The method partition the physical domain into several discrete tasks ( e.g., flow prediction, temperature prediction, and currents prediction ), and fine - tune the encoder and the prediction network based on the partitioned tasks. The encoder is trained with weak supervision to infer the hidden state of the task with the parameters of the target task under the assumption that the parameters are known. The prediction network is trained to capture the latent dynamics of the entire physical domain given the task description. DyAd has two parts : an encoder which infers the time - invariant hidden features of the subtasks with the weak supervision, and a forecaster which learns the shared dynamics of each task. In the experiments, DyAd outperforms state - of - the - art approaches on both turbulent flow and real - world ocean temperature forecasting tasks."
SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"This paper proposes a novel weakly supervised method to weakly supervise the object detection in 3D scene recognition using 2D box detection without 3D label annotation. The proposed method, called WeakM3D, first generate 2D boxes on the given dataset by generating a point cloud of object category labels using Gumbel Softmax, and then train a network to predict 3D box labels that can tightly align with corresponding RoI LiDAR points. This network is learned by minimizing a geometric alignment loss between the generated 3D boxes and their corresponding corresponding R_iid counterparts. Experiments are conducted on the KITTI benchmark and the proposed method is shown to achieve better performance than the state of the art methods."
SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"This paper proposes a soft - gradient - based subword tokenization module ( GBST ) that automatically learns latent subword representations from characters. Specifically, GBST enumerates candidate subword blocks and learns to score them in a position - wise fashion using a block scoring network. Then, a deep Transformer model, named CHARFORMER, is proposed that integrates GBST and operates on the byte level. Experiments are conducted on English GLUE, multilingual and noisy text datasets, and the proposed model consistently outperforms a series of competitive byte - level baselines while generally performing on par and sometimes outperforming subword - based models. The proposed model is also fast, improving the speed of both vanilla byte-level and sub - word - level Transformers by 28-100 % while maintaining competitive quality.   The authors believe this work paves the way for highly performant token - free models that are trained completely end - to - end without intermediate intermediate steps."
SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"This paper addresses the black - box hard - label backdoor detection problem, where only the output label of the black box neural network is available to observe. The authors propose the adversarial extreme value analysis ( AEVA ) algorithm, which is based on an adversarial map, computed from the monte - Carlo gradient estimation due to the hard - labels constraint. Experiments demonstrate the efficacy of AEVA across a set of popular tasks and state - of - the - art backdoor attacks.    Backdoor attacks inject a backdoor trigger into the target neural network, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Existing backdoor detection methods often require access to the original poisoned training data, which may not be practical for real - world applications. This paper proposes a method that directly observes the objective of backdoor detection as a function of adversarial objective. Theoretical and empirical studies reveal that this objective is bounded by a singularity, which leads to a solution with highly skewed distribution. Based on this observation, AEVA is shown to be effective in detecting backdoor attacks and has been shown to outperform existing methods."
SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"This paper proposes a new measure, KLoS, to measure the in - distribution and out - of - distribution ( OOD ) uncertainty in open - world classifiers. The key idea is to jointly measure the uncertainty in both cases, capturing both misclassification and lack of evidence. The proposed measure is based on a Kullback–leibler divergence between the model's predicted Dirichlet distribution and a specifically designed class - wise prototype Dirichlets. Prototype distributions are designed with concentration parameters that are shared with in-distribution training data, which enables to detect OOD samples without having access to auxiliary OOD data at train time. In addition, the paper proposes to learn an auxiliary model, KloSNet, to learn a refined criterion to regress the values of a refined objective for training samples and to improve uncertainty estimation. Experiments are conducted on several image datasets and model architectures, and the proposed measure achieves better performance than existing measures. In presence of OOD training data and / or the choice of a suitable OOD dataset, the proposed measures perform better than previous measures. When combined with ensembling or by ensemble, the performance of the different measures improves."
SP:8b4f3916dca4e627931558e14836749bd4a6792f,"This paper studies learning convolutional networks over unlabeled data with a structure that is similar to natural images. It proposes a semi - supervised learning algorithm, PAC, that learns a linear classifier over features, and shows that it efficiently learns CNNs. The main assumption is that the distribution of “ important ” patches in the images has a low - dimensional structure. Under this assumption, PAC learns the distribution using two steps :   1. First, a classifier is constructed over the features ; 2. It is assumed that the patches have a moderate covering number, such that each linear region has at least one representative patch. Then, PAC repeats this process until it reaches an output that is close to the output of a CNN. The paper shows that PAC is computationally efficient when the patches are of low - dimension. It also shows a lower bound that shows the dependence of PAC on the dimension of the patches is optimal."
SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"Graph Convolutional Networks ( GCN ) are a popular choice for face clustering due to their powerful representation capacity. However, existing GCN - based methods build face graphs mainly according to kNN relations in the feature space, which may lead to a lot of noise edges connecting two faces of different classes. The proposed Ada - NetS algorithm aims to tackle this problem by constructing clean graphs for GCNs by using an adaptive neighbour discovery strategy, which is proposed to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCN to cluster faces. Experiments on multiple public clustering datasets show that Ada - NETS significantly outperforms current state - of - the - art methods, proving its superiority and generalization."
SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"This paper proposes a distributionally robust optimization ( DRO ) method for solving the problem of generalizable person re - identification ( DG ReID ) in settings where domain information, camera ID, or other demographics are not available due to privacy or privacy concerns. The proposed method Unit DRO minimizes the loss over a reweighted dataset where important samples ( i.e. those which models perform poorly ) will be upweighted and others will be downweighted, while preserving a domain - invariant representation. The method is based on a combination of KL constraint DRO and a change - of - measure ( CofM ) approach. Empirical results and detailed analysis are provided to demonstrate the effectiveness of the proposed method on both large scale DG and cross - domain ReID tasks."
SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"This paper proposes a noise - based regulariser for GNNs to tackle the oversmoothing problem. The proposed noise regulariser is based on the idea of adding noise to graph convolutional networks by corrupting the input graph with noise and then adding a noise correcting node - level loss. The idea is that adding noise helps overfitting the graph and the noise correction loss helps ameliorate oversmoothhing by encouraging diverse node latents. The paper also proposes a simple regulariser that applies well - studied methods in simple ways which allows even generic architectures not designed for quantum chemistry to achieve state - of - the - art results. The experiments focus on 3D molecular property prediction tasks, and some generic GNN benchmark datasets. They show that using Noisy Nodes allows the GNS to achieve top performance on three 3D Molecular property Prediction tasks : the OC20 IS2RE direct task by 43% over previous work, 12% on OC20 Is2RS direct, and top results on 3 out of 12 of the QM9 tasks. They also test a non - spatial GNN on OGBN - Arxiv and test a MPNN ( Gilmer et al. 2021a ) and OGBG - PCQM4M ( Hu et al 2021 ) and again see significant improvements. These results suggest Noisy Node GNN can serve as a complementary GNN building block in the GNN toolkit."
SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"This paper proposes a differentiable EM model for the set2vec problem. The model is built from the perspective of fitting a Gaussian mixture model to the set data that are viewed as i.i.d. samples. The proposed model is also shown to generalize the recent set embedding models based on optimal transport and attention, leading to a computationally efficient model with superb performance on tasks in bioinformatics and NLP. The mixture fitting perspective can potentially solve other important problems in set representation learning."
SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,"This paper proposes contrastive feature selection for unsupervised learning in the contrastive analysis ( CA ) setting, where the goal is to select a small set of informative features for use in unknown downstream tasks. The proposed method CFS ( Contrastive Feature Selection ) is a method for performing feature selection in the CA setting. The authors apply CFS to a semi - synthetic dataset and four real - world biomedical datasets and find that CFS consistently outperforms previous state - of - the - art methods designed for standard un - supervised feature selection scenarios such as SCA, SCA - GA, SCI - GA and SCINet."
SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"This paper studies the problem of early stopping, which is a method to prevent over - training neural networks. Specifically, this paper focuses on early stopping of linear regression models where the dimension of the model often exceeds the number of features arising from data. The authors develop theoretical results to reveal the relationship between optimal early stopping time and model dimension as well as sample size of the dataset. They demonstrate theoretically that the early stopping process corresponds to the training process of deep neural network. Moreover, they study the effect of early stopped on generalization and demonstrate that optimal early stopped model can help mitigate “ double descent ” in various settings."
SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"This paper proposes a quasi - Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient ( NPG ) algorithm. For other entropy functions, this method results in brand new policy gradient algorithms. The authors provide a simple proof that all these algorithms enjoy the Newton - type quadratic convergence near the optimal policy. They demonstrate that the proposed quasi - Newton method typically converges in single - digit iterations, often orders of magnitude faster than other state - of - the - art algorithms."
SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"This paper proposes a reinforcement learning approach to generalize out of the training distribution for text - based games. In contrast to existing deep RL approaches, the proposed approach does n’t require a deep architecture and instead relies on a case - based reasoner which builds a collection of positive experiences from the agent ’s interaction with the world in the past and later reuses the collected experiences to act efficiently. The method can be applied in conjunction with any existing on - policy neural agent in the literature for TBGs. The proposed approach consistently improves existing methods, obtains good out - of - distribution generalization, and achieves new state-of - the - art results on widely used environments. The experiments show that CBR can be used to consistently boost the performance of existing RL agents and aid generalization in out -of - distribution settings."
SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,"This paper proposes a method to distill multiple senses from a pre - trained language model ( BERT ) by using attention over the senses of a word in a context and transferring this sense information to fit multi - sense embeddings in a skip - gram - like framework. The authors demonstrate an effective approach to training the sense disambiguation mechanism in their model with a distribution over word senses extracted from the output layer embed dings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state - of - the - art multi -sense embedding methods. Also, experiments with an embedding - based topic model ( ETM ) demonstrates the benefits of using this multi - senses embedding in a downstream application."
SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"This paper proposes to apply image - pretrained models to the point - cloud understanding task by inflating a 2D convolutional filter with a 3D filter and finetuning it to be able to understand 3D point - clouds. The main contribution of the paper is to empirically investigate the feasibility of this method of changing the convolution filters from 2D to 3D by first inflating the 2D filters and then fine - tuning them with the 3D filters. The paper shows that this method, when applied with a finetuned version of the model, can achieve competitive performance on the popular Point - CloudNet3D dataset. Moreover, it shows that when finetune all the parameters of the pretrained model ( FIP - All ), the performance is significantly improved on the common 3D classification tasks of point -cloud classification, indoor and outdoor scene segmentation."
SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"This paper proposes Energy - based training for autoregressive generative models ( A - GMs ), which is based on the observation that chain - style conditional models suffer from two intrinsic weaknesses : ( 1 ) exposure bias and ( 2 ) lack of temporal coherence. To address these two issues, the authors propose the Energy - Based Learning ( E - ARM ) framework, which consists of three components : 1 ) a joint distribution matching component for each time step of the generator ; 2 ) an energy function for estimating the energy of the joint distribution at each step ; 3 ) importance sampling to efficiently train the entire model without using MCMC.   Empirical results on language modeling, neural machine translation, and image generation illustrate the effectiveness of the proposed approach and compare favorably to existing methods."
SP:51e748c55bd4134047098559577fa3f37aa7433a,"This paper proposes a unified framework that connects Wasserstein distributional robustness with current state - of - the - art AT methods, such as PGD - AT, TRADES, MART, and AWP. The proposed framework encompasses the DR versions of the SOTA AT methods and they prove that these AT methods are special cases of their DR counterparts. Moreover, they develop a novel family of algorithms that generalize the AT methods in the standard robustness setting, which have better generalization capacity. Empirically, extensive experiments on benchmark datasets such as MNIST, CIFAR10 and CIFar100 show that the proposed algorithms are able to boost the model robustness against strong attacks with better generalisation capacity."
SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"This paper proposes a new unsupervised representation learning method for time series, namely Bilinear Temporal - Spectral Fusion ( BTSF ). The proposed method consists of two components. First, instance - level augmentation which uses the entire time series as input and applies dropout to generate different views for training. Second, iterative bilinear temporal - spectral fusion which iteratively fuse temporal - spectral information and refine the unified feature representation. Experiments are conducted on three major tasks such as classification, forecasting and anomaly detection downstream tasks and the results demonstrates the superior performance of our method over the state - of - the - art methods."
SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"This paper proposes an algorithm for automatically adjusting the learning rate during gradient descent ( GD ). The motivation is similar to line - search, where the goal is to find an appropriate hyperparameter that does not depend on the training parameters such as the dataset size, the architecture, the initial learning rate or the batch size. The key difference is that instead of using a fixed learning rate, GD treats learning rate as a parameter that can be automatically adjusted, as in line search. The paper further extends the line search idea to handle distinct learning rates at each layer, in order to provide finer control over the descent acceleration. The proposed algorithm is evaluated on MNIST, CIFAR-10, Tiny - ImageNet, and ImageNet-50, and converges to local minima."
SP:263c787361cd6d4443ce516d389c694d0fe44b28,"This paper proposes a meta - reinforcement learning algorithm for sequential multi - task learning, where the goal is to achieve high reward over any sequence of tasks quickly. In contrast to prior work, which transfers meta - RL learned from prior tasks to new tasks, this paper proposes to meta - train policies over each task in a sequence without revisiting prior tasks during training. The proposed algorithm, called CoMPS, consists of two steps : 1. learning a new task using RL and 2. using the experience from RL to perform offline meta - learning to prepare for subsequent task learning. The RL process keeps track of the set of trajectories that achieve the highest sum of rewards D_i, and all experience collected during RL training in the previous step is used to metaRL via training a model to learn how to reproduce the best policies achieved from previous tasks. The M step uses this experience to perform metaRL on new tasks and to learn a new policy.   The authors evaluate the proposed algorithm on a series of benchmarks that are commonly used metaRL benchmarks :   1. Continual control tasks. 2. Two - step continual RL benchmarks    3. Ant - continuous control tasks   4. A collection of held - out tasks that are solved one at a time."
SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"This paper introduces a new thread model of poisoned classifiers where a third party aims to gain control of the poisoned classifier without access to the original trigger. The authors propose a test - time, human - in - the - loop attack method to generate multiple alternative triggers that can be used as part of a robust classifier that is tested on different datasets. They construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images with human interaction. Experiments are conducted on high - resolution datasets : ImageNet and TrojAI. They compare their approach to previous work on modeling trigger distributions and find that our method are more scalable and efficient in generating effective triggers. The paper also conducts a user study which demonstrates that the method allows users to easily determine the existence of such backdoors in existing poisoned classesifiers. Thus, this paper empirically shows empirically that there is no such thing as a secret backdoor in poisoned classification models."
SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"This paper proposes a new method for distilling unconditional GAN, especially for the StyleGAN2 architecture. The main contribution is an analysis of the output discrepancy of the teacher and student model when distilling. The paper claims that standard knowledge distillation losses typically fail under heterogeneous distillation scenario. The analysis is conducted to identify the reason and effects of this discrepancy issue, and identify that the style module plays a vital role in determining semantic information of generated images. Based on this finding, they propose a novel initialization strategy for the student model, which can ensure output consistency to the maximum extent. Also, they present a latent - direction - based distillation loss that preserves the semantic relations in latent space. Experiments demonstrate the effectiveness of their approach in distilling StyleGAN^2, outperforming existing GAN distillation methods by a large margin. Code and models will be released."
SP:2c2231743fa33b95828c6615263954ce1c05f95d,"This paper proposes a method for generating online approximations of descriptive offline algorithms. An offline algorithm is defined as an algorithm whose output is always a hypothesis h ∈ F, where F is a class of functions. In this paper, the authors first introduce the notion of behavioral structures in characterizing offline algorithms and explain how to use these behavioral structures to generate descriptive training labels and then formulate the machine learning problem of approximating an offline algorithm in terms of the problem of generating an online approximation of a proper offline algorithm. Then, they describe how offline algorithms can be used to generate online labels and formulated the approximation as a multi - task learning problem. The paper demonstrates the proposed method on synthetic and real - world data, and shows that it is capable of learning up to the randomness inherent in the structural sequences, and that it can capture meaningful temporal relationships between behavioral structures even in the noisy price - action of commonly traded equities. Additionally, the model is demonstrated to match state - of - the - art one - step - ahead stock price prediction, with the caveat that our method did so using a one - decision - point - ahead approach."
SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"The paper proposes a new method, input - dependent sparse variational Gaussian process ( IDSGP ), which uses a neural network to compute the parameters of an approximate variational GP based on a sparse set of inducing points. The inducing points are simply given by a mapping from the observed data to a set of associated sparse points computed by a DNN. The DNN also outputs the corresponding parameters of the corresponding variational approximation on the inducing values associated to the inducing points, which are computed by an amortized version of VI. Experiments are performed on regression and binary classification problems and show that the proposed method improves the quality of the predictive distribution, reduces the training cost of sparse GP approximations, and increases the prediction time."
SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"This paper proposes a method to train distributed neural networks under the assumption that each client will have access to the training data of other clients. The method is called BTARD - SGD and it is designed to be Byzantine - tolerant, i.e., the client updates will not be affected by attacks that rely on the actions of a single attacker. This is in contrast to existing distributed training algorithms such as SGD, which rely on a trusted server to receive updates from clients and pass them through a second server. The main contribution of this paper is to propose a new algorithm for such distributed training that is meant to be zanet - tolerant and robust to attacks. Theoretical analysis is provided that proves that the proposed method has a marginal communication overhead in the presence of attackers. Experiments are conducted on image classification and language modeling on ResNet-18 and ALBERT- Large, pretrained in presence of malicious peers. Results show that the method outperforms baselines."
SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"This paper proposes a learning algorithm based on the smoothed particle hydrodynamics ( SPH ) method, which is a mesh - free Lagrangian method for approximating numerical solutions of the equations of fluid dynamics. This method has been applied to study weakly compressible turbulence in astrophysics and engineering applications. In this paper, the authors combine modern tools in CFD, machine learning, deep learning, automatic differentiation, and classical sensitivity analysis to develop a learnable hierarchy of SPH - informed models, which includes both physics based parameters and Neural Networks as universal function approximators. The learning algorithm develops a mixed mode approach, mixing forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization. Through experiments, it is shown that the learned SPH model can solve inverse problems over the physically interpretable parameter space, as well as over the space of Neural Network parameters. Further, it can fit underlying flow data using a combination of field based and statistical based loss functions, and be used to learn unknown functions embedded within SPH based models."
SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"This paper proposes Mix - MaxEnt, an approach to regularize a single neural network to be uncertain in regions of the data manifold that are unknown during training. The idea is to generate between - cluster samples by maximizing the entropy on these samples via a convex combination of two images from different classes, and then apply the MaxEnt regularizer corresponding to the predictive distribution in the region of the embedding space between the class clusters. The proposed approach is evaluated on two real - world datasets ( CIFAR-10 and CIFar-100 ) using ResNet and Wide - ResNet architectures, and consistently provides improved classification accuracy, better calibrated probabilities for in - distribution data, and reliable uncertainty estimates."
SP:365490b872464f00634dc7a50d024fceaf0a61ee,"This paper proposes a self - supervised auto - encoder - decoder model for image animation, called Latent Image Animator ( LIA ), which generates still images by directly manipulating the latent code in a deep generative model. The latent code consists of high - level transformations, such as zooming and rotation, that can be induced in the style of GANs. Specifically, the proposed method learns a linear displacement of codes in the latent space, such that motion in generated video is constructed by linear displacement   of images, and uses a set of orthogonal motion directions, and a linear combination of their linear combination, in order to represent any displacement in the   latent space. Extensive quantitative and qualitative analysis suggests that our model systematically and significantly outperforms state - of - the - art methods on VoxCeleb, Taichi and TED - Talk datasets w.r.t. generated quality."
SP:86f9f89f84e117c86478b9afaf087f65524f5472,"Meta - learning is a popular technique to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta - learning algorithms is the requirement of a large number of meta - training tasks, which may not be accessible in real - world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, the authors propose to augment the task set through interpolation, by meta-learning with task interpolation ( MLTI ). The proposed MLTI effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Theoretical analysis shows MLTI corresponds to a data - adaptive meta - regularization and further improves the generalization. Empirically, the proposed general MLTI framework is compatible with representative meta - learners and consistently outperforms other state - of - the - art strategies."
SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"This paper proposes a new method to enforce group fairness in the context of fair representation learning ( FRL ). In contrast to prior work, which only considers a restricted set of possible adversaries, this paper considers all possible combinations of adversarial examples. The key idea is to use an encoder based on normalizing flows which allows computing the exact likelihood in the latent space, given an estimate of the input density. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. This guarantee is stronger than prior work which only considered adversaries from a restricted model family. The experimental evaluation on several datasets showed that FNF effectively enforces fairness without significantly sacrificing utility, while simultaneously allowing interpretation of the representations and transfer learning."
SP:404d5643327f60f0f06f820033a56081f9e01900,"This paper proposes a GNN model named COUNT - GNN for subgraph isomorphism counting on labeled graphs. COUNT is equipped with two modules : edge - centric message passing and query - conditioned graph modulation, to improve structure matching between the query and input graphs. Specifically, at the edge level, the model uses a message passing scheme, where messages on edges are propagated and aggregated based on the edge adjacency. Then, the input graph representation is modulated in a way conditioned on the query, in order to adapt to each query individually to improve the matching with specific structures in each query. Experiments are conducted on a number of benchmark datasets to demonstrate the effectiveness and efficiency of COUNT-GNN. It achieves superior performance in comparison to the state - of - the - art baselines."
SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"This paper proposes a federated learning framework, called Agnostic Personalized Federated Learning ( APFL ), for the problem of label heterogeneity and domain heterogeneity in the federated setting, where each client has their own personalized label, which might not be compatible with others ( even for the same class ), and clients can be from a variety of different domains. APFL is loosely constrained in two ways : ( 1 ) labeling schemes are possibly not synchronized across all participants and ( 2 ) local clients may be from any multiple domains, which the authors named such challenges as Label Heterogeneity and Domain heterogeneity. To tackle these problems, APFL introduces a novel method, namely Similarity Matching and Kernel Factorization ( SimFed ), which aims to measure task - level similarity based on locally learned knowledge and matches the relevant ones for personalized knowledge reflection. SimFed uses a two basis vectors and the sparse masks to significantly reduce the dimensionlaity of parameter space for alleviating knowledge collapse and information loss when reflecting the heterogeneous knowledge. The authors extensively validate their method on both label - and domain - heterogeneous scenarios and show that their method outperforms the current state - of - the - art baselines."
SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"This paper presents Object Dynamics Distillation Network ( ODDN ), an unsupervised learning framework that distills explicit object dynamic representations ( e.g., velocity, acceleration ) from raw video to a learned relation module that calculates object - pair interactions and applies it to the corresponding dynamic representations of objects. The method is evaluated on tasks of video events reasoning and video prediction, which are two important evaluations for video understanding. The results show that visual representations of ODDNs perform better in answering reasoning questions around physical events in a video compared to representaions of the previous scene representation methods. And, ODDn could generate reasonable future frames given two input frames, considering occlusions and objects collision."
SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"This paper studies the problem of generalizing GNNs with positional encoding ( PE ) on unseen graphs, and proposes a class of GNN layers called Perplexity Equivariant Graph Layers ( PEG ) to address the issues. PEG consists of two components : rotation and permutation. The rotation component updates both node features and the positional features of the given nodes in a GNN. The node features are updated via a convolutional layer while the positional feature is updated via an embedding of the embedding. Theoretical analysis is performed to prove that the two components of PEG are equivariant under two conditions : ( a ) permutation equivariance w.r.t. the original node features, and ( b ) rotation and node equivariances. Experiments are conducted on link prediction tasks, and PEG is shown to generalize well to unseen graphs. In particular, experiments on domain shift link prediction show that PEG outperforms strong baselines such as DE."
SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,"This paper proposes a self - supervised text style transfer method built on large - scale language models. The proposed method, called LaMer, first mines the parallel expressions in the non - parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer ) and a newly proposed challenging task ( political stance transfer ), LaMer achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that LaMer generates more readable and diverse expressions than previous models."
SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"This paper proposes a hyper - relational model for multi - hop logical reasoning on knowledge graphs. It is motivated by the fact that existing algorithms for classical graphs operate on triple - based graphs, whereas modern KGs often employ hyper - relabeling, which is based on relation modeling. Qualified edges of graphs may have several key - value pairs known as qualifiers that provide fine - grained context for facts. In queries, this context modifies the meaning of relations, and usually reduces the answer set. This paper proposes to use these qualifier pairs in an approximate query answering ( QA ) framework to answer such queries. It builds upon recent advancements in Graph Neural Networks ( GNNs ) and proposes a method to answer conjunctive hyper - rational queries by embedding them in a query embedding. Then, it validates its approach by conducting experiments to see if qualifiers improve QA on a variety of query patterns."
SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"This paper proposes a multi - budget Bayesian optimization method for hyper - parameter optimization ( HPO ) in the gray - box setting. The proposed method is based on a surrogate for a Gaussian process in the form of a learnable deep kernel, which takes the learning curve as an input. The acquisition function of BO is motivated by this surrogate. The authors compare BO with Bayesian methods on 50 datasets across 3 domains ( image, caption, and nlp ) and 3 different types of neural networks ( MLP, CNN / NAS, RNN ). The main results show that BO outperforms the other methods on the tabular setting and multi - fidelity setting, and that DYHPO outperforms BO in the hyperparam optimization setting."
SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"This paper proposes to improve cross - platform consistency of learned image compression by introducing a post - training quantization to the model inference process. The proposed quantization is based on a well - developed technique for quantizing convolutional neural networks and aims to draw attention to the relation between the deterministic issue and model quantization techniques. The authors further improve the discretization of the entropy parameters and extend deterministic inference to fit Gaussian mixture models. They show that their proposed methods outperform the current state - of - the - art image compression models that can infer in a cross - platforms consistent manner, which makes the further development and practice of learning more promising."
SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"This paper proposes Noise2Noise, a noise loss function that is specifically designed for denoising FIB - SME images. It consists of two parts : ( 1 ) a noise reconstruction module that uses soft attention and signal boosting to reconstruct the original image, ( 2 ) a neural network architecture that uses the reconstructed image as input to a discriminator to remove the noise, and ( 3 ) a fully unsupervised training procedure to train the network. The proposed network is applied on FIB-SME images up to a resolution of 24 M pixels and trained on a set of 48 M pixels. The noise loss is applied to each of the 48 M images sequentially. The network is trained in a way similar to Gated Recurrent Neural Network ( GRN ), except that it is applied sequentially instead of using separate training sets for each training set. The results show that the proposed method achieves comparable or better performance compared to the baselines. The authors also provide a detailed analysis of their method."
SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"In this paper, the authors study a recently proposed method for learning graph neural networks ( GNN ) based on GNNs that share node embeddings but do not store any label information. In contrast, the proposed method uses label propagation ( LP ) to share label information to unlabeled nodes via a parameter - free diffusion process, but operates independently of the node features. Given that the material difference is merely whether features or labels are smoothed across the graph, it is natural to consider combinations of the two for improving performance - in this regard, a randomly - selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels, is commonly used. This so - called label trick accommodates the parallel use of features and labels, and is foundational to many of the top - ranking submissions on the Open Graph Benchmark ( OGB ) leaderboard.   The authors prove that under certain simplifying assumptions, the LP trick can be reduced to an interpretable, deterministic training objective composed of two factors : ( 1 ) a data - fitting term that resolves potential label leakage issues, and ( 2 ) a regularization factor conditioned on graph structure. The authors then leverage this perspective to motivate a broader range of label trick use cases, and provide experiments to verify efficacy of these extensions."
SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"Theory of mind is the ability to understand others'thoughts and desires, which is a cornerstone of human intelligence. Previous works have attempted to measure the ability of machines to develop a theory of mind, with one agent attempting to understand anothers’ internal “ mental state ” through a speaker - listener setup. This paper proposes a more flexible and symmetric scenario, called SymmToM, where all agents can speak, listen, see other agents, and move freely through a grid world. In order to develop theory of state, it is necessary to develop policies that maximize each agent ’s rewards, which in turn requires developing a theory that captures the other agents ’ mental state. The paper provides a framework to analyze machine theory ofMind in a multi - agent symmetric setting, a more realistic setup than the tasks currently used in the community. It provides a simplified setup on which to test the problem, and it provides a starting point to explore more complex models in this area. The main goal in this work is not to solve symmetric theory ofmind, but rather to give a starting line to explore deeper models.   The paper shows that even with this minimal set of rules, SymmTOM proves algorithmically difficult for current multi - agents deep reinforcement learning models, even when tailoring them to our specific task. It also provides metrics that show performance degradation of agents with access to the gold - standard mental state of other agents."
SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"This paper introduces zero - shot object detection ( zeroshot ), which is a subset of unsupervised learning, and it aims to detect novel objects in the image with the knowledge learned from and only from seen objects. For example, in the context of smart manufacturing, parts and products for robots to handle may change frequently and it is unrealistic to re - train the vision system for new products and tasks. Therefore, this paper proposes the necessity to introduce a zeroshot object detection algorithm that is based on the YOLOv5 neural network. The proposed method also proposed a novel splitting method for YCB Video dataset to split the dataset by seen and unseen objects. This splitting can be used for both gZSD and gZSL research that related to research related to daily objects. The detection of daily objects is actually more challenging since the knowledge that can be learned from each object is very limited. In this work, the authors explore the zero -shot detection of   daily objects in indoor scenes since the objects’ size and environment are closely related to the manufacturing setup. The YCB video dataset contains 21 objects in various categories. To the best of my knowledge, no previous work has explored   object detection at this object size level and on this dataset.   The contributions in this paper are in three folds : 1. A novel neural network structure that based on YOLov5 and able to perform generalized zero - shots object detection. The output bounding boxes can be further combined with other g - ZSL algorithm to achieve full zero - hit object detection and recognition. 2. An attribute labelling method to covert the class labels to 16 attributes that represents colour and shape information of an object for the neural network to learn. 3. A splitting method to create a split YCB dataset by 1. randomly partitioning the dataset into unseen and seen classes."
SP:aa1dcd9217270010f16a00004facede942efea17,"The paper proposes a new method to pre - train an autoregressive latent video model ( HARP ) for video prediction with minimal modification to existing models, and produces high - resolution ( 256x256 ) videos. The method scales up the prior models by employing a high - fidelity image generator ( VQ - GAN ) with a causal transformer model, and introduces additional techniques of top - k sampling and data augmentation to further improve video prediction quality. The proposed method achieves competitive performance to state - of - the - art approaches on standard video prediction benchmarks, with fewer parameters, and enables highresolution video prediction on complex and large - scale datasets."
SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,"This paper introduces a GAN model based on Vision Transformer layers, ViT - based generative adversarial networks, named ViTGAN. The proposed model is evaluated on CIFAR-10, CelebA, and LSUN bedroom and shows comparable performance to StyleGAN2 and ADA. The limitations of the proposed model are as follows.   1. Traditional GAN training methods for ViT discriminators interact poorly with self - attention, causing instability during training. To resolve this issue, the authors introduce several novel regularization techniques for training GANs with ViTs. 2. For ViT generators, they examine architectural choices for latent and pixel mapping layers to faciliate convergence. 3. To further improve training stability, they propose to use softmax and softmax - max regularization for GAN activations. 4. For discriminators, two new methods are proposed to ensure that discriminator output is unbiased."
SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"The paper proposes that the reason why VAEs with good likelihoods but poor sample quality is due to the presence of imperceptible information ( i.e., high - dimensional entropy ) in natural image distributions is because much of this entropy is attributable to the visually unobservable bits, which makes it easy for models to achieve competitive likelihoods without successfully modeling the visually perceptible bits. Based on this observation, the paper proposes to decompose the task of image generative modeling explicitly into two steps :   1. Prioritize the modeling of the perceptible information to achieve good sample quality, and 2. Model the high - rate ( high - entropy ) information that matters most in order to get good ELBOs.   Experiments demonstrate that only a small amount of information in natural images is pertinent to perceptual sample quality. However, conventional ELBO optimization does not distinguish the bits that matter perceptually from those that do not. And since the imperceptibly bits dominate the ELBO signal, the model does not dedicate enough modeling capacity to the perceptually relevant information. The paper proposes a simple two - stage training procedure that prioritizes the modeling the perceptibly relevant information and the prioritization of the low - rate information. By doing so, it shows that VAEs can have good likelihood and sample quality while achieving ELBO s   comparable to conventionally trained VAEs."
SP:bfed56018134ec66cde9a7e958df964d4cca3164,"This paper proposes Analytic - DPM, a training - free inference framework for Diffusion Probabilistic Models ( DPMs ) that uses Monte Carlo to estimate the reverse variance and KL divergence of a DPM using the Monte Carlo method and a pretrained score - based model. The authors claim that this approach leads to surprising results, such as that both the optimal reverse variance ( in terms of its score function ) and optimal KL divergence have analytic forms w.r.t their score function. Based on these findings, they propose AnalyticDPM, which is a $ \ell_2$-regularized version of the DPM and uses it to approximate the reverse and KL variance using a Monte Carlo model and a score model, respectively. They also derive lower and upper bounds of the variance of the score model to correct potential bias and reveal a relationship between the score function and the data covariance matrix. Experiments are conducted on a set of simple DPM models, and the proposed method is shown to improve the log - likelihood of these models compared to a baseline DPM."
SP:3f935ba5784c3e86db72421426bc479061af1a4b,"The paper investigates whether vision transformers ( ViTs ) can replace conventional convolutional neural networks ( CNNs ) as the de facto approach to automated medical image diagnosis, and investigates whether it is feasible to switch to transformer - based models for medical image classification as well, or if we should keep working with CNNs – can we trivially replace CNNs with transformers? The authors consider this question in a series of experiments on several standard medical image benchmark datasets, and perform experiments on ImageNet, CIFAR-10, and Imagenet, both in a supervised and self - supervised setting. They find that ViTs reach the same level of performance as CNNs in an array of medical classification and segmentation tasks, but they require transfer learning to do so. The best overall performance on medical imaging tasks is achieved using in - domain self - supervision - based pretraining, where ViTs show a small advantage over CNNs, but it is expected to grow as the data size grows."
SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"This paper analyzes a previously proposed self - attention - based self - training NLM by showing that it has a bias towards modeling dependencies between text segments that appear in the same training example, but are not seen in different training examples. This is in contrast to existing self - supervision - based NLMs, which have been shown to be able to learn to model dependencies between different text segments ( e.g., between pairs of words that were never seen in the training examples ). The paper shows that this bias is introduced by chunking the text into contiguous text segments of different sizes, which are then fed into the neural architecture. It is shown that this creates a separation rank of $ \mathcal{O}(\log(L)$, where $ L$ can be thought of as the number of hidden regions of the input that the model can learn to predict from. This separation rank is upper - bounded by comparing it to another separation rank $ \log(O)(\log{L}/\sqrt{L})$, and the authors prove that it is tight. On the other hand, it is not known how many hidden regions there are in the input, which the authors conjecture could be a fraction of a logarithmically large number. They also show that this separation rank can be used to upper - bound a "" depth deficit "", which is a measure of the distance between hidden regions in the network that is less than $ \leq L$."
SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"This paper proposes a symbolic regression approach to learnable learning to optimize ( L2O ) framework. In contrast to prior work, which learn the numerical representation of the optimization rule by a neural network, this paper introduces the use of symbolic regression in order to “ kill two birds by one stone ” : ( 1 ) scalability : the numerical rules represented by neural networks create extra memory overhead, which limits their applicability to optimizing larger tasks, and ( 2 ) interpretability : it is unclear what each L^2O model has learned in its black - box optimization rule, nor is it straightforward to compare different L ^ 2O models in an explainable way. With symbolic regression, the authors prove the concept that “ We can learn the optimization rules by distilling the optimization problem into a symbolic equation that enables the free room of optimization, then distill it into symbolic equation to link the optimization domain knowledge. ” The paper also proposes a lightweight re - parameterization that is lightweight and trainable to enable further fine - tuning of symbolic rules. Empirical results show that the proposed approach outperforms existing manually designed and tuned optimizers."
SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"This paper studies the problem of provable adversarial robustness for deep neural networks ( DNNs ). In contrast to prior works that studied DNN robustness in the static setting such as image classification, this paper considers the dynamic setting of reinforcement learning ( RL ), where an RL adversary can adapt itself to produce stronger attacks in future steps ( e.g., by focusing more on states critical to the agent ’s performance ). This paper proposes a method, called policy smoothing, which is designed specifically to defend against this adaptive RL adversary that can directly certify the total reward without requiring the policy to be robust at each time - step. The main theoretical contribution is to prove an adaptive version of the Neyman - Pearson Lemma, a key lemma for smoothing - based robustness certificates, which shows that one can defend a policy by adding Gaussian smoothing noise to the input of the policy, one can certifiably defend it against norm - bounded adversarial perturbations of its input. The method is evaluated in four RL games : Cartpole, Pong, Freeway and Mountain Car. The results show that the method can yield provable lower bounds on the average performances of the defended agents, against any adversary, that exceed the observed average performance of an undefended agent under a practical attack."
SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"This paper studies methods for accuracy estimation on target data by using unlabeled data from underlying target distribution. The proposed method, ATC, is based on the idea of “ average thresholded confidence ” ( ATC ) that learns a threshold on the model ’s confidence, predicting accuracy as the fraction of examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts ( e.g. due to synthetic corruptions, dataset reproduction, or novel subpopulations ), and datasets ( including WILDS, ImageNet, BREEDS, CIFAR, and MNIST ). In the experiments, A TC estimates target performance 2 – 4 % more accurately than prior methods.   The authors also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor. Thus, the efficacy of any method rests upon ( perhaps unstated ) assumptions on the nature of the shift. Also, analyzing our method on some toy distributions, the authors provide insights concerning when it works. As a starting point for theory development, they investigate ATC on a simple toy model that models distribution shift with varying proportions of the population with spurious features, as in Nagarajan et al. ( 2020 )."
SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"This paper proposes a method to learn transformation matching ( PDM ) based on the partial Wasserstein - 1 ( PW ) discrepancy, which it claims can be derived from the dual of the Kantorovich - rubinstein dual. Based on this theory, it proposes to use a neural network to approximate the discrepancy by running a gradient descent on a set of transformations corresponding to the point set on which the transformation from one set to the other is to be recovered. The neural network is trained using a loss function that penalizes the distance between the output of the network and the input point set. The method also uses a coherence regularizer to ensure that the non - rigid transformations do not deform too much due to the presence of outliers. Experiments are conducted on a point set registration task and show that the proposed method, called PwAN, is robust, scalable and performs favorably compared to existing methods."
SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"This paper proposes a transfer learning method for hyperparameter optimization ( HPO ) based on landmark meta - features ( KL - features ). The proposed method, called Landmark Meta - Features ( DKLM ), is an end - to - end meta - feature network that learns contextualized dataset - specific similarity representations for hyper - parameter configurations. To this end, DKLM is trained on a set of source tasks and a landmark set of hyperparameters, and the corresponding responses to the hyper - parameters are extracted from the set and transferred to the new task. In this way, the proposed method reduces the sample inefficiency of HPO. Experiments are conducted on OpenML datasets and compare DKLM with a series of state - of - the - art HPO baselines. Results show that DKLM outperforms the other methods on both non - transfer and transfer learning settings."
SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"The paper proposes a GAN - based fingerprinting method to identify the source of a given sample. The key idea is to generate a set of fingerprints for each instance of a generative model, such that the generated samples containing a fingerprint of a model can be identified and can thus be attributed to a source. The paper claims that this method is scalable, as only one generic GAN model is needed to train a large population of 1038 fingerprints, which can be then used to generate more than 10 different instances of each model. Each instance of each fingerprint can be used to train another 10 different instance of the same model. The authors propose a 128 - bit fingerprint per instance, which is scalable to many different fingerprints. They also claim that their method is'saturated'- that is, the number of fingerprints generated by the method is large enough to saturate the training data and therefore is effective at detecting deep fakes."
SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"This paper proposes two methods to provide model - agnostic post - hoc explanations for black box similarity learners that are applicable to both tabular and text data. The first method provides a feature attribution to explain the similarity between a pair of inputs as determined by a blackbox similarity learner. The second method proposes analogies as a new form of explanation in machine learning. The goal is to identify analogous pairs of examples that share the same level of similarity as the input pair and provide insight into ( latent ) factors underlying the model ’s prediction. The selection of analogies can leverage feature attributions, thus connecting the two forms of explanation. The authors prove that the analogy objective function is submodular, making the search for good analogies efficient. They apply the proposed approaches to explain similarities between sentences as predicted by a state - of - the - art sentence encoder, and between patients in a healthcare utilization application. The proposed methods outperform featureand exemplar - based baselines, showing high fidelity to the black box similarities learner, and providing reasons that users find sensible."
SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"This paper analyzes and provides certifiably robustness for ensemble ML models under the model - smoothness assumption. The authors first prove that diversified gradient and large confidence margin are sufficient and necessary conditions for the robustness of ML ensembles to be certified. Then, based on these theoretical results, they propose a regularization - based training approach, called Diversity Regularized Training ( DRT ), to train ensembled ML models to achieve higher certified robustness than a single base model under mild attack on ImageNet and CIFAR-10 under L2 - norm attack. Experiments show that DRT - trained ML models achieve the highest certified L2 robustness compared with single ML models."
SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"This paper analyzes a recently proposed method, local neighborhood pooling, to increase the expressiveness of graph neural networks ( GNNs ) with message passing, while maintaining computational cost similar to that of graph convolutional networks ( GCNs ). The proposed method is based on the following components : First, each node in a neighborhood pool is associated with a node in another GNN of the same order. Then, the nodes in each neighborhood are associated with nodes in the other GNN's orders, such that the total number of subgraphs of size k can be expressed as a function of the number of orders of the GCN of the corresponding node in the neighborhood. The authors prove two upper bounds : ( 1 ) the method is guaranteed to learn a subgraph of size d, which overcomes a known limitation of the low - order GNN ; and ( 2 ) it exploits the sparsity property of the local neighborhood network to reduce the computational cost compared to using higher - order GCNs. In addition, the authors provide a matching lower bound for counting sub - graphs with graph representations that pool over representations of derived ( sub-)graphs."
SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,"This paper studies the problem of knowledge integration ( KI ) in pre - trained language models, and particularly in the context of knowledge - enhanced language models ( KLMs ) which incorporate external knowledge into the pretrained language models. This paper proposes a simple probe model called Graph Convolution Simulator ( GCS ) for interpreting the KI process in order to expose what kind of knowledge is successfully integrated into these models. The GCS model is validated on two models : K - adapter and ERNIE. GCS shows that KI can capture a small amount of factual knowledge. While KI is better at integrating simple relational knowledge, complex relational knowledge is integrated better in ERNie. The paper also finds that while K - Adapter struggles to integrate time - related knowledge, it successfully integrates knowledge of unpopular entities and relations. The qualitative study, which breaks down the analysis of KI in terms of the type of relations, the popularity of entities and the catastrophic forgetting, finds that catastrophic remembering often happens to simple relations, while complex relations are often catastrophically forgotten."
SP:7e73948421e98307fceb69a316d8a4e7c4926cda,"This paper studies the effect of the adaptation learning rate in meta - learning with mixed linear regression on the population risk minimization in Model - Agnostic Meta - Learning ( MAML ). In particular, the authors provide a principled way to estimate an optimal learning rate $ \alpha$ that minimizes the empirical risk minimizer ( ERM ) with respect to the input data $ \tilde{O}(\epsilon^{-1,2})$. They show that ERM reduces the initialization distance to the task optima by a larger margin compared to MAMl. They also extend their result about the choice of α$ to a more practical regime, including deep learning."
SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"This paper proposes a new approach for source - free domain adaptation ( SFDA ) which aims to adapt a model trained on labelled data in a source domain to unlabeled data in the target domain without access to the source - domain data during adaptation. The proposed approach stores a lightweight approximation of the feature distribution under the source data ( e.g. Gumbel Softmax ) in the source domain and uses it to adapt the feature - extractor to the target data in order to realign with that saved on the source. They call this approach Feature Restoration ( FR ), as it seeks to extract features from the target that have the same semantics as those extracted from the source, rather than extracting new ones.   The authors also propose a new training scheme, called Bottom - Up Feature Restoration, which boosts performance by preserving learnt structure in the later layers of a network. They demonstrate that BUFR outperforms existing SFDA methods on real and synthetic data in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in thetarget domain."
SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"This paper studies the problem of federated learning ( FL ) with non - iid users, i.e., users who have different computation budgets and limited access to adversarial training ( AT ) during the training of a model from a set of users. The authors propose FedRBN, which is a method to federate propagating adversarial robustness among FL users by using batch normalization statistics ( BN ). The idea is to weight and average multiple AT users ’ statistics as BN for every ST user in order to efficiently propagate robustness. Experiments are conducted on image classification and semantic segmentation tasks, and compare with other state - of - the - art FL methods. The proposed method yields results competitive with the best all - AT - user baseline ( FATBN ) on robust accuracy."
SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"This paper proposes a novel method to infer the network structure of a game from observed game outcomes ( equilibrium actions ) without the knowledge of the utility function. The proposed method is based on a novel permutation - invariant transformer architecture, which is applied to three different types of network games, and evaluated on both synthetic and real - world data, to demonstrate its effectiveness in network structure inference and shows a superior performance over existing methods. The main contributions of the paper are :   1. A unified parameterization of the game is introduced, which helps reveal the different nature of these games and interpret the strategic interactions they represent. 2. This paper is one of the first to infer network structure behind the game, and to my knowledge,   a first one to propose a first and efficient data - driven learning framework, which learns a mapping from the equilibrium actions to the structure without explicit utility function and prior knowledge. 3. Experiments on three different networks games, using synthetic data ( Yelp, Indian Villages, and CIFAR-10 ), and comparing their proposed method to existing methods, demonstrate their effectiveness and superior performance."
SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"This paper proposes a novel method for relation prediction in heterogeneous graphs, such as knowledge graphs, by checking whether the subgraphs containing the considered relation are similar to other sub - graphs containing other nodes in the dataset. The method, named GraphANGEL, is based on the idea that logical rules in graphs ( e.g., triangle, quadrangle ) can be represented by graph patterns, which are searched for when searching for node pairs in a subgraph that satisfy certain logical rules. Each graph pattern is associated with a specific logical rule, which is used to train a classifier that predicts relations between each node pair. The classifier is trained by comparing the attention scores of attention matrices across the discovered logics. Experiments are conducted on heterogeneous graph based recommendation and knowledge graph completion tasks with the state - of - the - art methods from [ 1 ], [ 2 ], and [ 3 ]. The inductive capabilities of GraphAngEL are evaluated on both transductive and inductive benchmarks."
SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"The paper proposes to study few - shot learning of contrastive learning with latent augmentation ( LA ) in the context of histology images, which is of high clinical value as well as well - labeled datasets since rare abnormal samples are expensive to collect. The proposed approach consists of three components : ( 1 ) CL pretraining, ( 2 ) LA pretraining and ( 3 ) contrastive pretraining.    In the pretraining stage, the training is done in an unsupervised manner, where only the label - efficient part is done and the image encoder and decoder are trained on the unlabeled training set, and only the contrastive and the latent decoder do not receive any supervision. The experiments show that models trained with CL outperform those trained with LA in terms of generalizability, especially in rare and difficult - to - identify classes. The paper also shows that CL outperforms supervised learning in the rarely - encountered - class and rare - abnormal - image classes, and the generalization gap between the two is larger than that of ImageNet."
SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"The paper proposes a new recurrent neural network architecture, called mmRNNs, which extends the RNN with ODE - RNNs for learning long - term dependencies of irregularly - sampled time - series. The authors theoretically prove that the gradient of the ODE RNN suffers from vanishing and vanishing, respectively, in the training of recurrent neural networks. They then propose a solution to this problem by introducing a continuous - time dynamical flow within the Rnn, which allows it to respond to inputs arriving at arbitrary time - lags while ensuring a constant error propagation through the memory path. Experiments are conducted on CIFAR-10 and Fashion - MNIST, where mmRnns outperform other RNN - based and RNN-based baselines."
SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,"The paper proposes BiBERT, a method to improve the performance of BERT by fully binarizing the pre - trained language model. The paper provides a theoretical justification of the effect of 1 - bit binarization on model performance and proposes two strategies to improve it : Bi - Attention and Direction Matching Distillation. Through numerical experiments, the paper shows that BERT with BERT - like weights and activation can achieve better performance compared to the BERT baseline without BERT.    The main contributions of the paper are as follows :   1 ) A theoretical analysis of the impact of the weight, activation and representation of binarized BERT models. The results show that the performance suffers from the information degradation due to the information mismatch in the forward propagation of the representations and the direction matching in the backward propagation. Therefore, the authors propose a two - layer bi - attention structure and a direction matching distillation scheme to improve performance. 2 ) A new activation strategy based on bi - matching to maximize the distance between representations of different representations. Experiments shows that the proposed method outperforms the baseline and the SOTA BERT model with ultra - low bit activation on the NLP benchmark."
SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"This paper proposes a transformer - based method to supervise self - attention for keypoint detection and instance association. The key idea is to use the similarity measure between the features of two locations ( e.g. keypoint locations ) and the feature similarity of the corresponding pixels ( instance masks ) to assign the keypoint to instances based on the pairwise attention scores. The proposed method leverages a loss function to explicitly supervise the attention area of each person instance by the mask of the instance. Experiments are conducted on the COCO key point detection and person instance segmentation tasks. The results show that the proposed method achieves comparable performances to the state - of - the - art CNN - based bottom - up pose estimation systems. However, the current approach still has not yet beaten the CNN - baselines. In future, the reliance on the instance mask annotations may be removed in future works."
SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"This paper proposes a new reinforcement learning algorithm, called Pareto - efficient Q - learning algorithm ( EQUMRL ), for the mean - variance reduction ( MVRL ) problem in sequential decision making in RL under uncertainty. The main idea is to train an agent to maximize the expected quadratic utility function ( QF ) by maximizing the maximizer of the expected QF, which corresponds to the Pare - to - efficient policy ( eq.2 ). The method does not require gradient estimation of the variance due to the fact that the QF does n’t depend on it. The proposed method is computationally friendly compared to the existing MVRL methods ( e.g., SQRL, SQMIRL ) and experiments shows its utility on synthetic and real - world datasets."
SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"This paper proposes a method for adapting a generative mixture density network ( MDN ), based on Gaussian mixtures, to the test - time domain adaptation problem of adapting an autoencoder system whose channel is generatively - modeled using a Gaussian MDN. The authors propose a fast and sample - efficient method ( Table 1 ) for adapting the MDN without modifying the encoder and decoder neural networks, and adapting only the generative MDN channel model. The proposed approach is based on transforming the inputs to the decoder such that their class - conditional distributions are close to that of the source domain. The results are evaluated on simulated datasets and real mmWave wireless channels. The experiments demonstrate that the proposed method can adapt the   MDN with very limited number of samples, and improve or maintain the error rate of the autoenCoder under changing channel conditions."
SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"This paper proposes a new approach for the abduction task of natural language inference ( αNLI ), where the goal is to infer the most plausible explanation between the cause and the event. The proposed approach is based on the observation that previous works have focused on the relation between each candidate hypothesis separately and penalize the inference network uniformly. The authors argue that it is unnecessary to distinguish the reasoning abilities among correct hypotheses and similarly, all wrong hypotheses contribute the same when explaining the reasons of the observations. Therefore, they propose to group instead of ranking the hypotheses and design a structural loss called “ joint softmax focal loss ” in this paper. Based on this idea, they have designed a novel interactive language model aiming at exploiting the rich interaction among competing hypotheses. They name this new model as Interactive Model with Structural Loss ( IMSL ). The experimental results show that their IMSS has achieved the highest performance on the RoBERTa - large pretrained model, with ACC and AUC results increased by about 1% and 5% respectively. Impressive abductive reasoning performance is achieved by IMSSL when tested using RoberTa as the pretrained language model. The best language model DeBERTa ( He et al., 2021 ) is not tested due to the constraint of the limited GPU resources. In this experiment, compared with all recent algorithms whose codes have been made publicly available, the I MSL method has achieved state - of - the - art results on both the validation set and test set."
SP:17cd72df5fc19398f582d27516fd742b073f79e3,"This paper proposes ProoD, a method to train a classifier that is certifiable adversarially robust to out - of - distribution ( OOD ) samples, without loss in prediction accuracy or detection performance for OOD samples close to the in - distribution. The key technical contribution of the paper is to combine a provably robust binary discriminator between in - and out - distribution with a standard classifier in order to simultaneously achieve high accuracy, high OOD detection performance, and worst - case OOD guarantees that are comparable to previous works. Thus, it has combined the best properties of previous work with only a small increase in total model size and only a single hyperparameter. The authors also showed how in their model simply enforcing negativity in the final weights of the discriminator fixes the problem of asymptotic overconfidence in ReLU classifiers."
SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"This paper proposes Image Classification Eraser ( ICE ), a method to erase classification information for any encountered image from an arbitrary dataset. The authors define a new Generalized Transferable Attack ( GTA ) problem where the attacker has a set of surrogate models trained on different datasets ( with different label sets and image sizes ), and none of them is equal to the dataset used by the victim model. They then propose a novel method called ICE which builds a generalized attacker by a meta - learning framework. Experiments on several datasets demonstrate that the proposed ICE outperforms the existing transfer attack methods on the GTA problem. Furthermore, the authors show that existing transfer attacks methods can be modified to tackle the GTA   problem, but with significantly worse performance compared with ICE."
SP:2e0447c741a3f09be1095633d870200355211260,"This paper proposes a new approach to tackle the problem of false negative in discriminative pre - trained language models ( PrLMs ). In this paper, the authors firstly highlight the issue of misconceived false negative predictions of the discriminator ( e.g., false positive samples generated by training on wrong data ), and then propose two methods to counter this issue : ( 1 ) soft regularization to minimize the semantic distance between the prediction and the original one to smooth the rough cross - entropy, and ( 2 ) hard correction to shield the gradient propagation of the false negative samples to avoid training with them. The authors pre - train their methods on top of the ELECTRA architecture ( Clark et al. 2019 ) and fine - tune it on widely - used down - streaming benchmark tasks, including GLUE ( Wang et. al. 2018, 2018 ) and SQuAD ( Rajpurkar et. al. 2016 ). Experiments on GLUE show that their counter - false - negative pre - training methods indeed bring about better performance together with stronger robustness, and case studies show the effectiveness of their methods."
SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"This paper proposes ORCA, an end - to - end semi - supervised learning ( SSL ) method for open - world SSL in which novel classes can appear in the unlabeled test data. In this setting, the model needs to assign instances either to classes previously encountered in the labeled data, or form novel classes by grouping similar instances. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. Experiments on image classification datasets and a single - cell dataset demonstrate that ORCA consistently outperforms alternative baselines, achieving 25% improvement on seen and novel classes of the ImageNet dataset."
SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"This paper proposes a second - order method for training large - scale deep neural networks ( DNNs ) called SLIM - QN. It is based on the BFGS update rule, which directly approximates the Hessian inverse using past parameters and gradients, without explicitly constructing the Hessians matrix and computing its inverse. In order to counter the effect of stochastic training, SLIM-QN introduces momentum and damping into the update, which obviates the need for estimating the Hessan with high costs. Empirical analyses on CV models, such as ResNet-50 and ViT, show that the method achieves faster convergence compared to SGD, especially for large datasets such as ImageNet. Moreover, the method also reaches comparable accuracy with faster wall - clock time."
SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"This paper proposes a systematic method called LocalitySensitive Pruning ( LSP ) for graph pruning based on Locality - Sensitive Hashing. In LSP, nodes and edges are pruned based on the locality sensitive hashing ( LSH ). LSP is shown to achieve superior performance on synthetic and real - world datasets, which removes a significant amount of edges from large graphs without compromising the performance. Moreover, the reduction in the number of edges translates to a significant acceleration of the used models, which is linear in the size of edges."
SP:c5e024f4e2079586298519ca868630efd7579eca,"This paper proposes IDAA, a simple method to improve the augmentation quality of contrastive self - supervised learning ( SSL ). In contrastive SSL, the goal is to learn a set of augmentations that distinguish positives ( from negatives ) from easy positives ( without distorting the original identities ). The paper notes that strong augmentation may change the sample - identity of the positives, while weak augmentation produces easy positives/negatives leading to nearly zero loss and ineffective learning. The idea of IDAA is to add adversarial noise to an auto - encoder ( VAE ) whose output is conditioned on an identity - disentangled space learned by the pretrained VAE, and combine the perturbed VAE outputs with an intact identity - relevant part to produce augmentations to produce hard positives / negatives. To achieve this goal, the paper first perturbs the bottleneck dimension of the VAE by setting $ G = x - G(x)$ and adds it back to the original $ R(x ) as an augmentation, which is challenging for contrastive learning and meanwhile preserves the sample identity intact. The authors apply this “ IDAA ” to different SSL methods and show that IDAA consistently improves both the efficiency and generalization of SSL methods on multiple benchmark datasets."
SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"The paper proposes a warning function that is designed to detect changes in the distribution of a risk function of interest, such as accuracy or calibration. The key idea is that the warning function should be able to detect harmful shifts while ignoring benign ones, and ( b ) allow continuous monitoring of the model performance without increasing the false alarm rate. The proposed loss function is based on the Brier score, which is defined as the difference between the difference in the mean and the covariance matrix of a training and test distribution if it is significantly different between the two, under the assumption that the training distribution is representative of the true label distribution and the test distribution is not. The paper provides a sequence of tests that can detect such harmful shifts as well as benign ones. Experiments are conducted on both simulated and real - world data."
SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,"This paper proposes to learn a physical model from a single observation of a physical phenomenon, such as a pendulum moving in a monocular video. The physical model is composed of an encoder - decoder architecture and a neural ODE, where the encoder encodes the 2D transformation and the decoder maps the output to a 3D latent space. The decoder uses a neural network to map this latent space to a 2D latent representation, which is then used to compute a set of parameters for the model. The parameters of the model are modelled by a neural oscillator that modifies the output of the ODE in a way that allows for long - term prediction. The method is evaluated on a synthetic data and on a few experiments with pendulums. The results show that the method recovers the true length of the pendulum accurately."
SP:51efd1451343f4994d857daa5490e299b812bc2d,"The paper considers a context - dependent RL setting where there are an unknown finite number of not directly observable contexts, abrupt context changes occurring during an episode, and Markovian context evolution taking place in the episode. This setting is represented by a hybrid discrete - continuous variable process, where an unobserved discrete variable represents the context, and observed continuous variables represent the dynamics state. The paper proposes a variational inference algorithm for model learning using a sticky HDP prior, which is then coupled with a context distillation procedure to identify and remove spurious contexts. The resulting policy is then found by finding the representation of the optimal policy enabling efficient policy learning using off - the - shelf RL algorithms. Finally, the paper demonstrates empirically ( in cartpole swing - up, drone, intersection ) that the proposed method, where state - of - the art methods of other frameworks such as continual RL and POMDPs fail to solve C - MDPs, and gives the reasons why this might be."
SP:ea167b126212b2092bc1190d7f8376bf7c54a888,"This paper proposes a framework to pre - train a knowledge - based multilingual language model, i.e., GPT-2, on knowledge - intensive tasks such as entity recognition, factual knowledge retrieval, relation classification, and logic reasoning. The proposed approach firstly creates a synthetic multilingual corpus from the existing knowledge graph, then tailor - makes two pretraining tasks : multilingual knowledge oriented pre - training and logical reasoning oriented pretraining, to consolidate the knowledge modeling and multilingual pretraining and boost the capability of implicit knowledge modeling.   The proposed framework is evaluated on a series of knowledge-intensive cross - lingual benchmarks and the comparison results consistently demonstrate its effectiveness."
SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"The paper proposes an approach to train an RL agent to act altruistically towards other agents by maximizing the number of states another agent can reach in its future. This is motivated by theoretical work on instrumental convergence, which shows that for a large proportion of rational agents,   instrumental convergence is a useful subgoal, and thus can be leveraged to design generally altruistic agents. In particular, the authors propose an altruistic proxy objective that motivates the agent to increase the choices another agent has by preference - maximizing, i.e., maximize the total number of possible states the other agent could reach. The paper evaluates the proposed approach in three multi - agent environments and is able to match and sometimes outperform supervised baselines."
SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"This paper studies the phenomenon of ‘ double descent ’, which refers to the generalization behavior of models depending on the regime they belong to : under or over - parametrized. The main contribution of this paper is derived by leveraging influence functions in order to derive suitable expressions of the population loss and its lower bound, which is then used to define the upper bound of the Hessian spectrum of the Maximum - Likelihood estimator ( MNLI ). The authors then focus on understanding how double descent can occur in finite - width neural networks trained with maximum - likelihood - type models, which includes the class of neural networks but also applies to other settings such as linear and kernel regression. Their derived bounds bear an intimate connection with the spectrum of Hessian at the optimum, and importantly, exhibit a double descent behaviour, exhibit the double descent behavior at the interpolation threshold, and thus further investigate how the loss function affects double descent.    The use of influence functions makes this analysis applicable to a broad class of parametric estimators and makes it applicable to neural networks. However, it also provides insights into the effect of the loss functions used to train neural networks, which, as will be seen, clearly influences the nature of double descent and reveals novel insights."
SP:b485114712055f39a7afb951dbc3db482ff523fd,"This paper studies the asymptotic behavior of the Graph Neural Tangent Kernel ( GNTK ), which governs the optimization trajectory under gradient descent for wide GCNs and deep GCNs. The authors characterize the behavior and show that it reveals the dropping trainability of wide anddeep GCNs at an exponential rate in the optimization process. Additionally, they extend their theoretical framework to analyze residual connection - based techniques, which are found to be only able to mildly mitigate the exponential decay of trainability. To overcome exponential decay problem more fundamentally, they propose Critical DropEdge, a connectivity - aware and graph - adaptive sampling method, inspired by their theoretical insights. Experimental evaluation shows using the proposed method can outperform counterparts in the large depth."
SP:25a92b3583afdc6892e59f1e769125d52c8011af,"This paper studies the importance of incorporating higher - order dynamics ( i.e., acceleration ) into the loss function of neural networks for the task of video - based cardiac measurement. Specifically, the authors show that when training a neural network to measure left ventricle ejection time ( LVET ), it is better when incorporating a second - order derivative of both the input frames and the target vital sign signal into the training procedure. In particular, by explicitly incorporating the second derivative of the second derivatives of the input frame and target signal, the model is better able to estimate the estimated LVET. The authors also show that adding the second - derivative information also improves the accuracy of the waveform morphology.    While this work mainly focuses on the application of this type of model to video-based vital sign measurement, this paradigm is generally applicable to many applications. While this paper specifically focuses on this particular application, the paper is certainly not alone. It is important to understand the trade - off between optimizing for targets that capture different dynamic properties and optimizing for derivatives of interest such as acceleration. In cardiac measurement in particular, the LVET is one of the more important clinical parameters and can be better estimated using higher - ordered information."
SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"This paper proposes a method to help agents learn a symmetric language in complex settings like dialog games. Inspired by the theory that human language originated from simple interactions, the authors hypothesize that language may evolve from simple tasks to difficult tasks. They propose a novel architecture called symbolic mapping as a basic component of the communication system of agent. They find that symbolic mapping learned in simple referential games can notably promote language learning in difficult tasks ( dialog games ). Further, they explore vocabulary expansion, and show that with the help of symbolic mapping, agents can easily learn to use new symbols when the environment becomes more complex ( vocabulary expansion ). All in all, they probe into how symbolic mapping helps language learning and find that a process from simplicity to complexity can serve as a natural way to help multi agent language learning."
SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,"This paper proposes a modular approach to learn agents that navigate and interact with an environment using language instruction and a policy that operates at three levels : ( 1 ) a sequence of instructions to be generated by a high - level policy composition controller ( PCC ) ; ( 2 ) a master navigation policy to control the agent ’s navigation by alternating between navigation policy and various independent interaction policies ; ( 3 ) a module that executes the corresponding manipulation actions using the appropriate interaction policy. The proposed method, named HACR ( Hierarchical Approach for Compositional Reasoning ), achieves the state - of - the - art performance on the challenging ALFRED benchmark. It outperforms prior works by significant margins."
SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"This paper proposes a new method, Nuisance - Randomized Distillation ( NURD ), to untrain a model trained under one form of the nuisance - randomized family where the nuisance is independent of the label under the assumption that the covariates of the model are also independent of each other. The family is defined as distributions that only differ in terms of the relationship between the nuisance and the label. The paper proposes two ways of defining this family : the first is to use the generative models trained under the original family to generate data that satisfy the nuisance randomization, and the second is to define an uncorrelating set that is optimal on every member of the family simultaneously. The main contribution of the paper is that it proposes a method that finds the representation that maximizes the information a representation has with the label on the nusiance - randomised data under the nuisance distribution that is the most informative. This method is evaluated on class - conditional Gaussians, labeling colored MNIST images, detecting waterbirds, and classifying chest X - rays."
SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"This paper proposes a zero - shot classification method called OTTER ( Optimal TransporT distillation for Efficient Zero - Shot Recognition ). OTTER is motivated by the observation that natural language ( text ) descriptions of visual concepts are often better than supervised "" gold "" labels ( image - caption pairs ), and the fact that the image - text datasets collected from the Internet are noisy. Previous works, such as CLIP, use InfoNCE loss to train a model to predict the pairing between images and text captions, which can be inefficient because of the large amount of unlabeled pairs. To address this, OTTER uses online entropic optimal transport to find a soft image -text match as labels for contrastive learning. Based on pretrained image and text encoders, models trained with OTTER achieve strong performance with only 3 M image text pairs. Compared with InfoN CE loss, label smoothing, and knowledge distillation, OTter consistently outperforms these baselines in zero - shots evaluation on Google Open Images ( GOI ) ( 19,958 classes ) and multi - labeled ImageNet 10K ( 10032 classes ) from Tencent ML -Images ( Hu et al., 2019a ). Over 42 evaluations on 7 different dataset / architecture settings ( 6 metrics ) and 2 benchmark datasets ( YFCC and Conceptual Captions, with a total of 15 M text - image pairs, the method achieves SOTA performance in 37 out of 42 comparisons. It also shows comparable results to CLIP in the appendix."
SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,"This paper proposes Pix2Seq, a method for object detection that casts object detection as a language modeling task. Specifically, bounding boxes and class labels are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. The method is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. The proposed method achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms."
SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"This paper proposes distilling the learned policy from a CNN policy network into a symbolic policy that dwells on geometric and numerical symbols and operators. The method is based on a teacher - student framework, where the policy network is trained using a loss function called RoundTourMix, which progressively transforms the policy parameters into the interpretable symbolic policy through a distillation process that involves evaluating the policy in different environments and learning how to distill the resulting symbolic policy. The paper evaluates the method on six environments and finds that the distilled symbolic policy achieves comparable or higher scores with interpretable actions and generalizes better on the new environments than the CNN policy trained by the teacher policy network."
SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"The paper proposes a GAN - based image - to - image ( image2i ) model named PIVQGAN that can independently control the coarse - level object arrangements ( pose ) and the fine - grained level styling ( identity ) of the generated image from two exemplar sources. The method is based on StyleGAN2 and consists of a Vector - Quantized Spatial Normalization ( VQSN ) module for the generator and a joint - training scheme with self - supervision methods for the GANInversion encoder and the generator. Specifically, the encoderE and generator reconstruct images from two different augmented versions of the original ones, one defining the pose and the other for identity. The VQ SNN module automatically learns to encode the shaping and composition information from the commonly shared objects inside the training - set images. The training scheme also includes a self - supervised training scheme to train the segmentation network in the GNNE layer to learn meaningful segmentation embeddings that provide useful posture information. The resulting model, which the authors call PivqaGAN, outperforms existing baselines in terms of visual quality and translation accuracy. Experiments conducted on various datasets show better synthesis image quality and disentangling scores of PIVqaGAN. The authors also present model applications beyond image-to - image translation and semantic - aware image - editing."
SP:e51a7f45493064972585109f203a867e9828eb15,"This paper proposes a simple MLP architecture, called speech - MLP, for the tasks of speech recognition, speech synthesis, and speech enhancement. The proposed model consists of two components : split & glue, which is used to extract speech features by splitting them into non - overlapping chunks and processes each one individually, and then merge them together to form the final feature extractor. Similarly, the model also learns multiscale local temporal dependency by using different number of chunks and focusing on different contextual window sizes. Experiments are performed on two tasks : keyword spotting on the LibriWords dataset and VoiceBank dataset for speech enhancement, and show that the proposed model can achieve comparable performance to or better than transformers with less parameters and inference time. The paper argues that more complex models, such as transformers, are oftentimes not necessary for speech processing tasks, and simpler and more compact models should be considered as an alternative."
SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"This paper studies the generalization error bound of transfer learning ( TT ) for binary classification as a function of the amount of source and target data sets, and derives a lower bound that depends on a natural notion of distance that can be easily computed on real world data sets.   The key features of this lower bound are that it applies to any arbitrary source / target data distributions and requires minimal assumptions that enables it application to a broad range of problems. More specifically, TT can be applied to binary classification problems in which there are more than one source domains for knowledge transfer to the target task and develop new bounds on generalisation error. In addition, the paper considers a more general setting where there is data transfer between source domains and target domains and develops new bounds that also extend to non - transfer learning. The lower bound is verified on real image classification and action recognition data sets to demonstrate the sharpness of our lower bounds and demonstrate their utility."
SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"This paper proposes a probabilistic framework for shape completion from incomplete 3D scans. The proposed model is based on the GCA framework from ( Zhang et al., 2020 ). In particular, the model compresses the implicit field into sparse voxels associated with their latent code, and incrementally generates implicit surfaces. The training objective is proven to maximize the variational lower bound of the likelihood of sparse   voxel embeddings, which indicates that cGCA is a theoretically valid generative model. Extensive experiments show that the model is able to faithfully generate multiple surfaces from partial observation."
SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"This paper proposes a non - Markovian flow - based prior method for exploration in RL, i.e., a prior that does not depend on the state of the underlying reward function. This is in contrast to prior methods such as behavioral priors, which are state - conditioned methods that transfer across domain gaps ( e.g., GRACE ), but are less general. The proposed method is based on using a mixture of policy and prior, where policy is a Gaussian mixture of actions sampled from the prior, while temporal prior is a flow function. The paper shows that this mixture of prior and policy can be integrated into an off - policy RL framework, and that it can improve exploration in continuous control tasks with sparse rewards. It is shown to accelerate learning in unseen longhorizon tasks, and it improves upon prior methods that do n’t have a temporal prior."
SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,"This paper proposes a method for learning rate scheduling in training deep neural networks. The method is based on constructing a directed graph for the underlying neural network of the target problem, and then using a graph message passing network to train an agent to control the learning rate via reinforcement learning. The proposed scheduler can capture the intermediate layer information while being able to generalize to problems of varying scales. The authors evaluate the framework on benchmarking datasets, Fashion - MNIST and CIFAR10 for image classification, and GLUE for language understanding and show consistent improvement over popular baselines. Moreover, GNS demonstrates great generalization to different datasets and network structures."
SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"This paper proposes a generative unsupervised object - centric learning model, named SPAIR3D, for the task of scene decomposition from a 3D point cloud. In particular, the proposed model decomposes a scene into a spatial mixture model, where each spatial component corresponds to one object. The method uses the Chamfer Mixture Loss to model the spatial mixture loss, which fits naturally into the variational training pipeline. Experiments are conducted on the Unsupervised Object Recognition ( UOR ) and Scene decomposition ( UOT ) datasets to demonstrate the effectiveness of the proposed method."
SP:3c57e921c1bf23e482551ceb71702931a7f07439,"This paper investigates the possibility of using large language models ( LLMs ) to help execute high - level tasks, expressed in natural language, to a chosen set of actionable steps ( i.e. “ open fridge ” ). While prior work focused on learning from explicit step - by - step examples of how to act, this paper surprisingly finds that if pre - trained LMs are large enough and prompted appropriately, they can effectively decompose high -level tasks into low - level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. This paper proposes a procedure that conditions on existing demonstrations and semantically translates the plans to actions. The resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade -off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models1."
SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,"This paper provides a geometric interpretation of the Variational Autoencoder ( VAE ) framework, showing that the learned latent space of a VAE is a Riemannian manifold, and proposes a sampling method to generate samples from this manifold to improve the generation performance of the vanilla vanilla VAE. The method relies on sampling from a uniform distribution that is derived intrinsically from the manifold learned by the VAE, and is claimed to be able to compete with more advanced VAE models on four benchmark datasets, such as CIFAR-10, FIFO, Imagenet and OASIS. The experimental results show that the proposed method improves generation performance over VAEs trained with more complex priors such as ex - post density estimation, flow regularization and other regularization techniques. The robustness properties of the proposed methods are also investigated."
SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"This paper proposes Transformer with a Mixture of Gaussian Keys ( Transformer - MGK ), a novel transformer architecture that replaces redundant heads in transformers with a Gaussian mixture of keys at each head. Compared to its conventional transformer counterpart, the proposed model has fewer parameters, accelerates training and inference, and requires less FLOPs to compute. Experiments on the LRA benchmark, WikiText -103 language modeling, and IWSLT ’14 machine translation show the effectiveness of the model. The proposed model can be easily extended to use linear attentions.  Ethics Statement : Given the nature of the work, the ethical impact of the proposed work and the experimental settings are not given in the appendix."
SP:82731dcce233e748f63382e09b6224a513fe9689,"This paper proposes a novel approach to integrate proprioceptive and idiothetic inputs to build a cognitive map of a continuous spatial environment, by fusing them through the task of path integration. The authors propose a recurrent recurrent architecture, called Resetting Path Integrator ( RPI ), that can be trained to keep track of its position relative to its starting point during a sequence of movements. It updates its internal state using the ( possibly noisy ) self - motion signal, and occasionally resets it when the image signal is present. The internal state of this minimal model exhibits strong correlation with position in the environment due to the direct - inverse models, is stable across long trajectories through resetting, and allows for disambiguation of visually confusing positions through integration of past movement. This model is compared to off - the - shelf LSTM networks on identical tasks, and consistently shows better performance while also offering interpretable internal dynamics and higher - quality representations."
SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"This paper studies the role of structure in the feature learning of neural networks. Specifically, the authors consider a practical learning problem where the labels are determined by a set of class relevant patterns and the inputs are generated from these along with some background patterns. They show that neural networks trained by gradient descent can succeed on these problems. However, the success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data ( in particular, the structure of the input distribution ). In contrast, no linear models on data - independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no polynomorphic algorithm in the Statistical Query model can learn even weakly. These results provide theoretical evidence showing that feature learning in neural networks depends strongly on the input structure and leads to the superior performance."
SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,"This paper provides a methodology to analyze the robustness of fixed feature extractors, which in turn provides bounds on the robusts of any classifier trained on top of it. The key insight is to identify the layers of robustly trained models that contribute the most to a lack of robustness, as well as compare the same layer across different training methods to provide a quantitative comparison of their relative robustness. The paper also provides classifier - agnostic lower bounds, which are used to determine the plausibility of robust classification for a given adversary and dataset, and highlight practically relevant perturbation budgets."
SP:874b5fa51924cbcceed490d98a0ea80f74586b32,"This paper proposes Value - based Episodic Memory ( VEM ), a novel offline reinforcement learning ( offline RL ) framework that learns the V - function instead of the Q - function to naturally keep the learning procedure within the offline dataset. The authors propose Expectile V - Learning ( EVL ), which smoothly interpolates between the optimal value learning and behavior cloning. Also, they introduce implicit planning along offline trajectories to enhance learned V - values and accelerate convergence of EVL. Together, they present a new offline method called VEM. They provide theoretical analysis for the convergence of their proposed VEM method, and empirical results in the D4RL benchmark show that VEM achieves superior performance in most tasks, particularly in sparse - reward tasks."
SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"This paper proposes a novel adversarial training framework that learns to reweight the loss associated with individual training samples based on a notion of class - conditioned margin, with the goal of improving robust generalization. Inspired by MAML - based approaches, the authors formulate weighted adversary training as a bilevel optimization problem where the upper - level task corresponds to learning a robust classifier, and the lower - level tasks corresponds to a parametric function that maps from a sample ’s multi - class margin to an importance weight. The authors provide extensive experiments that demonstrate that their approach improves both clean and robust accuracy compared to related techniques and state - of - the - art baselines."
SP:3ad36be6b6900aabe43da043461cf178ce977082,"This paper proposes a non - linear non - equivariant message passing framework, called Steerable E(3 ) Equivariant Graph Neural Networks ( SEGNNs ), which leverage geometric and physical information, such as angular and angular momentum, in addition to position, position, and edge attributes, to improve message passing in graph neural networks.   Compared to prior work ( e.g., GAT ) on node attributes ( node attributes, edge embeddings ), the proposed framework addresses the question of how to incorporate such information in the message and update functions. The authors propose to use steerable MLPs ( defined by the spherical harmonic embedding of node attributes ), and argue that this provides a new class of activation functions, which can be used in combination with MLP - based methods such as vector or tensor - valued message passing. They demonstrate the effectiveness of their method on several tasks in computational physics and chemistry, and provide extensive ablation studies."
SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"This paper proposes a differentiable physics model for differentiable fabrics, where it dives into the granularity of yarns and model individual yarn physics and yarn - to - yarn interactions. To this end, it proposes several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient - based learning. These forces, albeit applied to cloths, are ubiquitous in various physical systems : friction, shear, elasticity, etc. The authors argue that these forces are familiar and can be used to develop a fine - grained DPM capable of incorporating domain knowledge. Through comprehensive evaluation, the authors demonstrate that their model can effectively solve inverse problems, provides high data efficiency, and has high - fidelity in capturing subtle dynamics."
SP:2c8358c095b10981d3015b9f6c75765419a9480d,"This paper proposes a method for efficient transfer learning in reinforcement learning. The method, called SOPGOL, leverages the logical composition in RL to help the agent decide whether a new task can be solved using its existing abilities, or whether a task - specific skill should be learned. In addition, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy.   The authors provide two main theoretical results :   1. A bound on the performance of the transferred policy on new task, and 2. A sufficient number of tasks that need to be learned throughout an agent’s lifetime to generalize over a distribution. The authors verify their approach in a series of experiments, where they perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks. They also demonstrate that as a side effect of their transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task."
SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"This paper presents LightWaveS, a new method for solving the multivariate time series classification ( MTSC ) problem. It builds on the recent state - of - the - art method ( ROCKET ) based on random convolutional kernels, which uses a large number of features and is known for its fast training speed. However, it has been observed that this method can be detrimental in terms of inference speed due to the large convolution kernel used. To address this issue, this paper proposes a distributed version of ROCKET that uses only 2,5 % of its features. The method is based on a wavelet scattering transformation of the time series, where each channel is divided into wavelets and each wavelet is used to generate a different scattering coefficient, which is then used as a feature vector to select the corresponding corresponding corresponding wavelet in the corresponding time series. The paper presents three versions of the method and their results on training time, accuracy, inference speedup and scalability, which are compared to ( 1 ) MINI - ROCKET, ( 2 ) ROCKET and ( 3 ) LightWave, with the goal to build a model that can be deployed on an edge device, with comparable accuracy to that of the original ( Mini - ROCKet ). Results show that the new method can achieve speedup ranging from 9x to 65x compared to ROCKET during inference during training, and can achieve accuracy up to 92x up to the original with LightWave."
SP:db43614ca016280a79448f44a97c81c8ff5ba981,"This paper proposes a new approach to ELECTRA - style pretraining, where auxiliary masked language models ( MLMs ) are trained alongside the main text encoder, which is called AMOS, for text encoders with an adversarial learning curriculum via a Mixture Of Signals ( MOS ) approach. The adversarial loss is calculated by backpropagating the loss of the discriminator through Gumbel - Softmax. Different from ELECTRA which trains one MLM as the generator, AMOS jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. Their experiments on the GLUE and SQuAD benchmarks demonstrate the empirical advantages of AMOS. AMOS outperforms ELECTRA and recent state - of - the - art pretrained models by about 1 point on GLUE    for BERT base - sized models, and the authors plan to release their pre - trained models."
SP:db3825633ab5d0671340390b23ab655838cc38b2,"This paper proposes to fine - tune a pre - trained language model for the task of fact extraction by using a small training dataset of existing facts from a knowledge graph. The fine - tuning is performed by modifying the parameters of the language model on a subset of the facts that are included in the training dataset. The method is based on the idea of prompt - based prompting, where a query of a few words is converted into a binary prompt and then used to improve the precision of the model. The paper evaluates the method on a number of benchmark datasets for the LAMA benchmark and shows that it can outperforms several baselines that have been previously evaluated for fact extraction from a language model. Furthermore, the method is evaluated with respect to the transfer learning capabilities of the adapted language model by training on a restricted set of relations to show that even fewer training relations are needed to achieve high knowledge extraction quality."
SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"This paper proposes a framework for learning representation learning for knowledge bases based on hyperbolic geometries. The main motivation is to allow different geometric spaces to model the various properties of relations as well as the various local structures of the knowledge base. The proposed method is based on learning the embedding of knowledge bases in different spaces and applying manifold alignment across the geometric spaces in order to build the projection. Experiments are conducted on two datasets : YAGO3 and MNIST. The empirical results demonstrate the superiority of the approach, especially in low dimensions and on small training rates."
SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,"This paper proposes a one - shot learning framework for link prediction in temporal knowledge graphs ( TKGs ) based on a self - attention mechanism to effectively encode temporal interactions between entities, and a similarity network to compute a similarity score between a given query and a ( one - shots ) example. The proposed algorithm outperforms the state - of - the - art baselines on two well - studied benchmarks while achieving significantly better performance for sparse relations.    The contribution of this paper is as follows :   1 ) A new method for learning low - shot techniques for temporal settings ( e.g., when only a small fraction of relations occur only a handful of times ) is proposed. The existing approaches are tailored to static knowledge graphs and do not easily generalize to temporal settings, where data scarcity poses even bigger problems due to occurrence of new relations. 2 ) This paper introduces a temporal neighborhood encoder that effectively extracts the temporally - resolved neighborhood information for each entity. 3 ) It conduct experiments on two real - world datasets and demonstrates the superiority of the proposed model over the baselines."
SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"This paper proposes Progressive Module Networks ( PMN ), which is a compositional neural network that learns how to combine previously learned lower - level modules ( "" task modules "" ) for a new task by exploiting previously learned modules. Specifically, the authors propose to represent a solver for each task as a neural module that calls existing modules ( solvers for simpler tasks ) in a functional program - like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. The proposed PMN for each new task learns to query existing modules and composes their outputs in order to produce its own output. Their model effectively combines previous skill - sets, does not suffer from forgetting, and is fully differentiable. They test their model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. They further analyze the interpretability of the reasoning process with human judges."
SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"This paper proposes a method to improve parameter efficiency of CNNs with bottleneck architectures by introducing channel - selectiveivity as an architectural feature. The bottleneck structure of such an architecture is composed of a convolutional layer and a residual ( e.g., a "" residual "" ) connection to each of the input channels. The idea is that the residual connection preserves the information preserving nature of the identity connection and can be leveraged to introduce channel - selectivity, i.e., redistribute computations to important channels. To achieve this, the authors propose two methods : ( 1 ) pruning unimportant channels, and ( 2 ) rewiring the parameters of the network to be more sensitive to the selected channel. Theoretical results show that the SCU - based models without any postprocessing generally achieve both model compression and accuracy improvement compared to the baselines, consistently for all tested architectures."
SP:2d80fa4bc440061be2234b5070503d3fa056baed,"This paper considers the problem of learning a binary classifier only from positive data and unlabeled data ( PU learning ). It considers the following setting :   1. The labeled positive data are identically distributed 2. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. Because it is difficult to learn the Bayes optimal classifier when the data has selection bias, the authors propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. 3. The experimental results show that the method outperforms previous methods for PU learning on various real - world datasets."
SP:5f312626b0613d2e07c59214c5f00db208a98717,"This paper proposes a simple yet effective way to measure the similarity between an auxiliary task and the main task of interest, given the value of their parameters. The authors propose to use the cosine similarity between gradients of tasks as an adaptive weight to detect when an auxiliary loss is helpful to the main loss and when it could start hurting. They show that their approach is guaranteed to converge to critical points of the main problem. They demonstrate the usefulness of the proposed algorithm in a few domains, including multi - task supervised learning on subsets of ImageNet, reinforcement learning on gridworld, and reinforcement learning   on Atari games."
SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"The paper proposes a geometric analysis of the high - dimensional geometry of adversarial examples drawn from the manifold reconstruction literature to study the robustness of classifiers in the presence of perturbations to the input that could lead to misclassification of otherwise statistically accurate models. The analysis relies on a geometric framework drawing from manifold reconstruction tools to draw connections between the adversarial example geometry and the geometry of decision boundaries learned by deep neural networks.   The analysis highlights the importance of codimension of the classifier on the high dimensional space, highlighting the fact that there are many directions off the low - dimensional data manifold in which to construct adversarial attacks. The paper shows that there exists a tradeoff between robustness under different norms of the norm of the adversary. It also shows that adversarial training in balls around the data is sample inefficient. Finally, it shows that nearest neighbor classifiers are robust to attacks under the $ \tilde{\mathcal{O}^-\infty$ norm."
SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"This paper proposes SOM - VAE, a state representation learning framework for time series that can recover interpretable state representations on time series and static data that are difficult to interpret. It combines ideas from interpretable discrete dimensionality reduction and deep generative modeling to learn a smooth and interpretable embedding with superior clustering performance. Besides, it introduces a new way to overcome the non - differentiability in discrete representation learning and present a gradient - based version of the traditional self - organizing map algorithm that is more performant than the original. Also, to allow for a probabilistic interpretation of the method, it integrates a Markov model in the representation space, which provides additional explanatory insights as well as a natural representation of uncertainty. The learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data."
SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"This paper investigates the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. The authors claim that regions of the learned manifold that are in close proximity to the origin of the space are oversampled, which limits the usefulness of linear interpolation as a tool to analyse the latent space. They show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non - linear interpolations. They provide a general method of creating non -linear interpolations, that is easily applicable to a large family of commonly used latent distributions, such as the Cauchy and normal distributions.   The main contributions of the paper are :   1 ) They prove that there is a trade - off between the invariance property of the nonlinear interpolation being linear, and the latent distribution having even the most basic properties required for stable training such as finite mean, e.g., finite distribution. 2 ) They also provide a method of providing a general framework to define non - Linear interpolations that is general enough to accommodate other desirable properties such as manifold convexity."
SP:19b63ca635712f1509ca6e0141303c192f2709e0,"This paper proposes a new method to learn the parameters of deep neural networks in hyperbolic space. The motivation is to increase the capacity of the network without increasing the number of trainable parameters so as to match the complexity of the data. The key idea is to use the attention mechanism, i.e. the weight applied to the current token, to compute the ubiquitous attention mechanism in different neural networks architectures, such as deep Relation Networks ( DNNs ) and Transformer ( RLNs ). The main advantage of this approach is that it does not change the geometry of embedding of object representations, only the embedding space, which allows to use it more efficiently. The method shows improvements in terms of generalization on neural machine translation, learning on graphs, and learning on visual question answering tasks while keeping the neural representations compact."
SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"This paper presents the first in - depth security analysis of DNN fingerprinting attacks that exploit cache side - channels. The attacker runs a co - located process on the victim machine where the victim ’s deep learning ( DL ) system is running and passively monitors the accesses of the target functions in the shared framework.   The paper introduces DeepRecon, an attack that reconstructs the architecture of the victim network using the internal information extracted via Flush+Reload, a cache side channel technique. Once the attacker observes function invocations that map directly to architecture attributes, the attacker can reconstruct the entire network architecture. The paper also provides a meta - model that accurately fingerprints the architecture and family of the pretrained model in a transfer learning setting. With the extracted architecture, the authors identified the essential attributes for these attacks. Finally, the paper proposes and evaluates new framework - level defense techniques that obfuscate our attacker’s observations."
SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"This paper proposes a hierarchical network model, called Hierarchical Prediction Network ( HPNet ), to understand how spatiotemporal memories might be learned and encoded in a representational hierarchy for predicting future video frames. The model is inspired by the feedforward, feedback and lateral recurrent circuits in the mammalian hierarchical visual system. Within each level, the feed - forward path and the feedback path intersect in a recurrent gated circuit that integrates their signals, integrating their signals as well as the circuit’s internal memory states to generate a prediction of the incoming signals. The network learns by comparing the incoming signal with its prediction, updating its internal model of the world by minimizing the prediction errors at each level of the hierarchy, in the style of predictive self - supervised learning. Experiments are conducted on short - range video sequence prediction datasets, yielding state - of - the - art performance. However, the hierarchical interaction in HPNet introduces sensitivity to global movement patterns in the representational units of the earliest module in the network, and that real cortical neurons in the early visual cortex of awake monkeys exhibit very similar sensitivity to memories of global moving patterns, despite their very local receptive fields. These findings support predictive Self - Supervised learning as an important principle for representation learning in the visual cortex and suggest that HPNet might be a viable computational model for understanding the cortical circuits in    the hierarchical visual systems at the functional level."
SP:fb74e57f35666742caf651e6da33b5defcf259a8,"This paper proposes a method to compute continuous embeddings for kmers from raw RNA - Seq data, in a reference - free fashion. The authors consider the problem of including the rich patient - specific sequence information from RNA -Seq data via a continuous representation that will account for both gene expression as well as mutations and chromosomal rearrangements. They propose a model which learns a gene - like representation from the raw patient specific sequence RNA -Segq data. They show that this approach does not rely on the reference genome used in standard pipelines and instead learns a common representation by looking at multiple patients. They study how this model handles situations in cancer which are standard in cancer genomics but considered edge case in standard pipeline. The extracted information may provide a foundation for future methods to build on and better understand the underlying biology."
SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"This paper proposes Architecture Compression ( AP ), a method to compress CNNs by first training a continuous embedding on a representation of the architecture and then performing gradient descent to determine an optimal architecture for the given task. The authors compare AP to conventional compression methods such as pruning, distillation and reinforcement learning and show that our architectures are smaller and faster than those produced by the baseline methods. They demonstrate the effectiveness of their approach on several visual learning tasks of varying difficulty ( FMNIST, SVHN, CIFAR-10 / 100 ) and show 5 - 20x compression on architectures."
SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"This paper proposes POLO, a framework that combines local trajectory optimization, value function optimization, and exploration in order to improve the performance of global value function learning in the online and offline settings. Specifically, the paper considers the problem of learning an internal dynamics model ( i.e., an agent that lives in a simulation world and needs to interact with the real world ), where it is possible to have access to a local model of the dynamics model. In addition, it considers how to handle uncertainty in the value function, which can affect the ability of value function to learn well. The paper shows how POLO can benefit from both local and global optimization. In particular, POLO considers the following three components :    1. Local trajectory optimization. This is when the agent needs to learn a local value function that, when given a good local model and computational resources, can provide faster generalization of learned value functions. 2. Exploration. It is when it is necessary to explore the environment to learn skills that are not present in the simulation world. 3. Value function optimization. When the uncertainty in value function is large, exploration is used to reduce the planning horizon. POLO shows how to combine these three components and achieves state - of - the - art performance on two tasks : dexterous hand and humanoid locomotion."
SP:771494fda4702cd8c7efbf225b19028f91b449b9,"This paper proposes a zero - shot model for neural machine translation ( NMT ) that is based on reinforcement learning, by training a model using only zero - shared parallel data between the target language and the source language. The model is trained using a multilingual NMT on English - French and Spanish - Spanish, and a semi - supervised model based on LSTM on the UN corpus, and fine - tuned using reinforcement learning on the monolingual corpora. Experiments show that this model outperforms the multilingual baseline model for unsupervised language pairs, on in - domain evaluations in the UN dataset. In out - of - domain evaluation, the proposed model performs as well or better than the state of the art for un - supervised MT, for comparable neural architectures, including Transformers. Finally, it is shown to be competitive with recent unsuper supervised methods such as ( Lample et al., 2018b )."
SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"This paper criticizes the training setup and loss formulation of the original IRGAN paper. The authors point out some inaccuracies in the proposed training strategy of IRGAN, namely the use of the minimax loss function, which they claim will result in a generator which can learn the conditional probability distribution p(d|q ) over the documents ( d ) given the query ( q ). They also show that IRGAN is not state - of - the - art on the required datasets ( Web - Search and Question Answering ). Further, the performance does not justify the large training time of 4 hours per generator epoch and 1 hour per discriminator epoch compared to the co - training of 2 hours per epoch of Co - training model ( GAN ).    The main contributions of the paper are :   1 ) They criticise IRGAN for not learning end - to - end trainable models like GANs, where two models are trained in a co -operative fashion rather than an adversarial fashion 2 ) They propose a model motivated by Co - Training which outperforms IRGANs and 3 ) They substantiate the same by drawing conclusions from the loss curves of the GAN."
SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"The paper proposes a variational auto - encoder ( VAE ) that merges ideas from two VAE techniques : variational coding and sparse coding, to generate more interpretable and efficient variational approximations of the latent codes of VAEs. The key idea is to use a Spike and Slab prior over the entire latent space of a VAE, conditioned on the number of latent dimensions, to induce sparsity in the latent code. The method relies on a discrete mixture recognition function to approximate the ELBO of a standard VAE. It is shown that this sparsity results in more efficient approximate variational inference with probabilistic sparse coding models. Experiments on MNIST and Fashion - MNIST show that the proposed method yields more informative and interpretable latent codes than standard VAEs, and that many sparse elements in retrieved codes control recognisable features."
SP:06a22143186fa2948fbe324ccae96a62ff12064e,"This paper proposes Generative Feature Matching Networks ( GFMN ), a non - adversarial training method to train GNNs. GFMNs are trained by using a mixture of self - supervised, auto - encoders, and deep convolutional neural networks ( CNNs ). Specifically, the encoder encodes the features and the decoder maps the features to a latent vector, which is then matched with a feature matching network that uses the latent vector as the input to the generator. The generator is trained with ADAM - based moving average averaging. Experimental results on CIFAR-10, STL-10 and STL - K demonstrate that the proposed method outperforms the state - of - the - art training methods ( DCNN, GAN, and GAN++ ) on ImageNet."
SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,"Graph Neural Networks ( GNNs ) follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and achieved state - of - the - art results on both node and graph classification tasks. However, there is limited understanding of their representational properties and limitations.   This paper presents a theoretical framework for analyzing the expressive power of GNNS to capture different graph structures. Theoretical results characterize the discriminative power of popular GNN   variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structure. Then, this paper develops a simple architecture that is provably the most expressive among the class of Gnns and is as powerful as the WeisfeilerLehman graph isomorphism test."
SP:51126f2dd37ce57d2614c9044ede1e43627f0829,"The paper proposes a framework for interpretable continual learning ( ICL ) based on variational continual learning. The idea is to generate an explanation of a previously learnt task, which is then used to enhance the attention of the learner during future tasks. The paper proposes to measure the quality of saliency maps generated by ICL and proposes a new metric for measuring it. Experiments show that ICL achieves state - of - the - art results in terms of overall continual learning performance as measured by average classification accuracy. Also, explanations of previously learnt tasks are evaluated qualitatively and quantitatively using the proposed metric."
SP:27a565b3e5442b93d208652784051e640b0c1bfe,"This paper proposes a new evaluation framework for adversarial attacks on neural sequence - to - sequence ( seq2seq ) models, taking into account the importance of meaning preservation. Based on previous work, adversarial examples have been shown to be an effective way of assessing the robustness of the model. However, these perturbations are only indicative of a weakness in the model if they do not change the semantics of the input in a way that would change the expected output. The authors propose a new constraints for attacks on word - based MT systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs that would still be beneficial to the model in terms of adversarial robustness without hurting test performance."
SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,"This paper proposes a new type of reinforcement learning algorithm that combines the policies using original rewards and inverse ( negative ) rewards. The inverse ( positive ) rewards are competitive with the original rewards, and help the original policies correct their mis - actions. The authors have proved the convergence of the inverse policy, and developed a hybrid policy based on deep Q - learning, double q - learning and on - policy actor - critic algorithms to obtain the rewards up to 63.8 % higher than the original algorithms. The improved polices are also shown to be more stable. In the experiments, the authors show that the rewards obtained by their hybrid policy reach the same or higher in some games."
SP:89a732b57934d08b937c93560f391b7758e54f8a,"This paper proposes a new model for learning object parts, their hierarchical structure, and system dynamics from unlabeled videos. The model is called Parts, Structure, and Dynamics ( PSD ) and it consists of three components : 1 ) recognize object parts via a layered image representation ; 2 ) predict hierarchy via a structural descriptor that composes low - level concepts into a hierarchical structure ; 3 ) model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that the PSD model works well on all three tasks. It is particularly noted that PSD works very well on complex RGB images and requires fewer input frames."
SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,"This paper proposes Deep Determinantal Generative Classifier ( DDGC ), which can obtain a more robust decision boundary under any softmax neural classifier pre - trained on noisy datasets. The main idea is inducing a generative classifier on top of hidden feature spaces of the discriminative deep model, i.e., the minimum covariance determinant estimator, to significantly improve the classification accuracy. Experiments are conducted on different learning models optimized by various training techniques to handle noisy labels or adversarial samples. The experimental results demonstrate the superiority of DDGC over softmax classifiers pretrained on CIFAR-10 and SVHN. In addition, DDGC is shown to be robust against various adversarial attacks."
SP:0fa525cc708470b757a60117cb608bb2feaa2c50,"This paper proposes a model - free hierarchical reinforcement learning approach to the problem of automatic subgoal discovery in the sparse reward environment of a Hierarchical Reinforcement Learning ( HRL ) setting. This approach is based on incremental unsupervised learning over a small memory of the most recent experiences of the agent. It consists of three components : ( 1 ) automatic state representation generation, ( 2 ) intrinsic motivation learning of skills to achieve subgoal, and ( 3 ) meta controller learning of subgoal selection. The meta controller learns the state representations and subgoal values by incremental gradient descent. The latent state representations are learned by sampling actions from the state space using the learned meta controller. The skills policy is learned by using a reward signal corresponding to the subgoal. Experiments are conducted on two environments ( a rooms environment and ATARI 2600 ) and two RL problems ( sparse reward and Montezuma ’s Revenge )."
SP:e5861538bc8bb9165cb33299bbf12dd875abf976,"This paper proposes a neural framework that can learn to solve the Circuit - SAT problem, which is an NP - hard problem in computer science. The paper proposes an end - to - end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out - of - sample generalization performance of the proposed model compared to the recently developed NeuroSAT method.   The proposed model has two contributions :   1. A rich embedding architecture that encodes the problem structure. Specifically, given a Boolean expression consists of { And, Or, Not, Variable}., find an assignment to the variables such that it would satisfy the original expression, aka a solution. If the expression is not satisfiable, it will be labeled as UNSAT. Moreover, when represented in the circuit format, Boolean expressions can aggregate the repetitions of the same Boolean sub - expression into one node in theCircuit format, which can be modeled as a DAG. The proposed DAG can be trained to generate a solution if the input circuit is indeed SAT. 2. A differentiable prediction procedure that is similar to that used in Selsam et al. ( 2018 ) to learn a Circuit SAT solver merely from data. In particular, it is used to predict the output of the DAG function from the previous section."
SP:ff3e5d44619df3825632b0b1a943add081364861,"This paper proposes combining evolutionary and deep RL methods in order to get the best of both worlds : the former is widely applicable and rather stable, but suffers from low sample efficiency while the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper - parameter setting. The authors evaluate the resulting method, CEM - RL, on a set of benchmarks classically used in deep RL, such as the SWIMMER - V2 benchmark, and show that it outperforms some existing evolutionary strategies, some sample efficient off - policy deep RL algorithms, and another combination, the ERL algorithm. Importantly, even though it is mainly an evolutionary method, the proposed method is competitive to the state - of - the - art even when considering sample efficiency, which is not the case with other deep neuroevolution methods."
SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,"This paper proposes an interpretable multi - variable LSTM, called IMV - LSTMs, which is equipped with a hidden state matrix and update process, as well as a mixture attention mechanism and summarization methods to quantify the temporal and variable importance in data. Experiments are conducted on synthetic and real - world datasets to demonstrate the performance and interpretability of the proposed method. The idea of the method is to build an end - to - end framework for both forecasting and knowledge extraction over multi - variate data. The proposed method could be applied to GRU and stacked RNN."
SP:1c26660569b579f060f7b4a31e321c6d2356b928,"This paper proposes a simple data augmentation method for neural networks called feature smoothing. The idea is to generate virtual data points as close as possible to the training data to reduce the computational cost of generating training data during adversarial defense. The proposed method is evaluated on MNIST and CIFAR-10 against label smoothing, logit squeeze, weight decay, mix up, and some other “ efficient ” methods. The paper provides a unified framework to understand the connections and differences among different efficient methods by analyzing the biases and variances of decision boundary. Theoretical analysis and experiments show that all the proposed methods are unbiased under some symmetrical assumptions, and all methods except weight decay produce an unbiased estimation of the decision boundary with smaller estimated variance."
SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"This paper proposes a theoretical framework for deep neural networks with ReLU nonlinearity. The framework bridges data distribution with gradient descent rules, favors disentangled representations and is compatible with common regularization techniques such as Batch Norm, after a novel discovery of its projection nature.   The framework is built upon teacher - student setting, by projecting the student ’s forward / backward pass onto the teacher’s computational graph. The paper does not impose unrealistic assumptions, such as independence of activations, and explicitly deals with back - propagation, which is the dominant approach used in practice. Besides, from the theoretical framework, it also provides general comment on multiple issues, e.g. overfitting, GD versus SGD in deep learning."
SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"This paper presents a neural network module called Behavioral Module ( MB ), which consists of two modules : Strategy Module and Behavior Module. The idea of separating the modules is inspired by the Pre - Frontal Cortex ( PFC ) of the human brain, as well as human behavior formation process. Specifically, the PFC acts as a pre - activated region in the MLP, while the MB corresponds to the output of the strategy module. The PFC is connected to each of the four walls of the main propositional layer of the network. The two modules are used to learn the preferences and strategy, respectively. The strategy module is trained end - to - end with adversarial training, where it learns the objective and behavior of the current state. The preferences and behavior module are then transferred to the main task via gradient descent. The behavior module is also trained to adapt the learned behavior module to the new task. Experiments are conducted on two video games : PUBG and Starcraft. The experiments show that Behavioral Module outperforms the other modules in terms of performance when applied to both main task and BPTT."
SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,"This paper extends the differentiable plasticity framework of Miconi et al. ( 2017 ) to train neural networks with differentiable neuromodulated plasticity using backpropagation. Specifically, the authors propose a differentiable formulation of the Hebbian - like effect of plasticity on the synaptic connectivity and use it to train differentiable LSTMs with gradient descent. This allows the weight - shifting mechanism to be mimicked by a neural network. The resulting LSTM, called backpropamine, outperforms its non - plastic counterparts on a variety of reinforcement learning and supervised learning tasks. The authors conclude that this differentiable version of the plasticity provides a powerful new framework for training neural networks."
SP:1ab5d94d31e99351433436c026799c8aa597bf73,"This paper proposes a non - intrusive quantization method based on re - training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. A new loss function, LKM, is proposed to regularize the weights, resulting in reduced quantization error. Experiments on CIFAR and WikiText-2 dataset with quantized models, and 1.5 bit and 2 bit quantization, are presented. Comparable results are also shown for ImageNet."
SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"This paper proposes STOC, Style Transfer onto Open - Ended Content ( STOC ), a method for style - transfer on open - ended content ( OCE ), which is general enough to be applied to any classification task. STOC uses a style encoder trained with a variational autoencoder ( VAE ) that combines the latent representation of the style embedding with the content embedding from another VAE. The VAE loss is decomposed into a loss that separates the content representation and the style representation. The loss includes an auxiliary loss, leakage filtering, to ensure that no style information is retained in the OCE that can be used for reconstruction of the content representations. The method is evaluated on the VGG-Face classification task and achieves SOTA performance on few - shot learning."
SP:d37e15cde7765fca87595a242f0a4511b3346d46,"This paper proposes a method for training agents in RL for problems that have the property of state - action permissibility ( SAP ). There are two types of states that an agent can decide whether an action in one of them is permissible in another : ( 1 ) An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried in that state. ( 2 ) A binary predictive model is proposed to help the agent make such a decision. This paper mainly deals with a class of RL problems with the SAP property : navigation problems, planning for solving a task, some games, etc. It proposes to incorporate the proposed SAP property into two state - of - the - art deep RL algorithms to guide their exploration. Experimental results show that the proposed approach can result in a huge speedup in RL training"
SP:20015d8b60e13300586b67c281858cbe28825c48,"This paper studies the behavior of weight - tied multilayer vanilla autoencoders under the assumption of random weights. The analysis reveals interesting phase transition phenomena when the depth becomes large. This analysis provides quantitative answers and insights to three questions that were yet fully understood in the literature. The first question is how the random deep weight - tied autoencoder model performs “ approximate inference ” as posed by Scellier et al. ( 2018 ), and its connection to reversibility is considered by several theoretical studies. The second question is whether deep auto - encoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts. And thirdly, the paper provides insights on pitfalls in training initialization practice, and demonstrates experimentally that it is possible to train a deep autoen coder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer - wise pre - training or batch normalization."
SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"This paper proposes a simple method for generating adversarial images within a small region around a target model. The goal is to find an image that is imperceptibly modified that is misclassified by the target model, while the feedback is through model evaluations. The proposed method can be used for both targeted and untargeted attacks. The search strategy is based on an intriguingly simple iterative principle : 1. randomly pick a low frequency component of the discrete cosine transform ( DCT ) and either add or subtract it to the target image. 2. Model evaluations are only required to identify whether an operation decreases the adversarial loss. 3. provide theoretical analysis on the efficacy of the proposed method and evaluate various orthogonal search subspaces. The authors argue that the proposed algorithm should serve as a strong baseline for future adversarial black - box attacks, in particular because it is extremely fast and can be implemented in PyTorch in less than 20 lines of code."
SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"This paper proposes a new approach to option discovery in hierarchical RL, inspired by the idea of discovering bottleneck states, or landmark states, in the state space. The key idea is to identify a set of "" core "" states that are prototypical of a well connected set of similar states, from which a collection of "" successor options "", or "" successor policies "", can be generated. These options are then clustered according to the similarity of these core states in the successor options. To this end, the authors propose a Hierarchical Successor Options ( SYO ) framework, which takes as input a sequence of successor options, each of which takes an initial state as input, and outputs a successor state. Each option is associated with a value function, which is learned by comparing the current state with the original successor state and the value function of the previous state. The originality of the SYO is that it does not require a reward function. The authors then propose a pseudo - reward for learning the intra - option policies, and an incremental approach to learning the options. They demonstrate the efficacy of their approach on a grid world and on complex high dimensional environments like Deepmind - Lab."
SP:12a172c1e2892d016b37932acfc48dcb56874a89,"This paper proposes a method for zero - shot classification tasks such as Open Set Learning ( OSL ) and Generalized Zero - Shot Learning ( G - ZSL ) that aims to separate the instances drawn from different probabilistic distributions. The proposed method divides the instances into known, unknown and uncertain domains, and then conducts recognition tasks in each domain. The uncertain domain is newly introduced in this framework to adopt those instances whose domain labels can not be predicted with high confidence. Two statistical tools, namely, bootstrapping and Kolmogorov - Smirnov ( K - S ) Test, are introduced to uncover and fine - tune the decision boundary of each domain in the proposed method. This method achieves state - of - the - art performance on the benchmark datasets."
SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"This paper proposes a neural network for classification and regression, without the need to learn layout structures in the output space. The proposed network, called polar prototype networks, explicitly states the layout of the output, i.e., polar prototypes, points on the hypersphere. For classification, each class is described by a single polar prototype and they are a priori distributed with maximal margin separation and equal shares. For regression, the training is performed as a polar interpolation between two prototypes, arriving at a regression with higher - dimensional outputs. While the structure is simple, the performance is on par with ( classification ) or better than ( regression ) standard network methods. Moreover, the regression outperforms standard squared error optimization, highlighting its expressive abilities."
SP:d1034342785d133cf8372b8624897963cc2ee83a,"The paper proposes Reward Learning by Simulating the Past ( RLSP ), a method to infer rewards from state in an IRL agent, assuming that the state of the world at initialization is similar to human preferences. The idea is that when a robot is deployed in an environment that humans act in, the state that it observes has already been optimized to satisfy human preferences, which explains the preference for a policy that generally avoids side effects. The paper formalizes this by assuming that Alice has been acting in the environment prior to the robot’s deployment, and derives an algorithm, RLSP, that computes a reward function based on MAP ( Maximum Causal Entropy ). The robot then acts according to a tradeoff between Alice ’s reward function and the specified reward function. Experiments on a suite of proof - of - concept environments show that RLSP can successfully infer side effects to avoid as well as tasks to complete, though there are cases in which it cannot infer the relevant preferences."
SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,"The paper proposes a method to learn the dependency structure between latent variables in deep latent variable models. It combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, it expresses the latent variable space of a VAE in terms of a Bayesian network with a learned, flexible dependency structure. The whole model ( including its structure ) is jointly optimized with a single stochastic variational inference objective, which is formulated via a sampling procedure that produces expectations over latent variable structures. Experiments are conducted on MNIST, Omniglot, and CIFAR-10. Comparisons are made with state - of - the - art structured variational autoencoder baselines."
SP:976dedab53e69610692a563382ada1dbb82c1e9d,"The paper proposes a dynamical neural network ( DNN ) formulation for dictionary learning. The authors show that the DNN can be used to solve the $ \ell_1$-minimizing dictionary learning problem by ( 1 ) learning a sparse representation to approximate the objective function of a dictionary, and ( 2 ) learning the weights of connected neurons to compute the gradient of the objective. The DNN adopts a different activation function compared to Hopfield NNs or Boltzmann MBMs, and uses spiking neurons instead of activation functions of those of DNNs. The paper shows that it is possible to obtain a finite - sum of the gradients of the connected neurons if each neuron maintains a $ L_1 $-norm over its local memory. It is shown that the network converges to a finite set of states after sufficient training. These finite states correspond to the limiting states of the dictionary learning objective, and the difference in states after a long enough evolution is called the gradient information.   The paper then applies this formulation to various dictionary learning problems, and shows that the proposed DNN obtains a 1 - minimizes solution to the problem of estimating the objective of a sparse dictionary when the weight of each neuron in the network is $ L(s, a)$. The paper also provides a learning process, a rigorous mathematical analysis, and numerical results on several Dictionary Learning problems."
SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,This paper presents a new convolutional neural network model for semantic image segmentation and lane detection task based on CU - U dataset. The proposed model uses multiple encoder - decoders module in end - to - end way to capture spatial information and pinpoint the localization of the object in the image. The authors also propose a small quantity of channel augmentation to reduce overfitting by considering interdependencies among channels. Experiments are conducted on the dataset and the proposed model achieves good results.   The main contributions are as follows. 1. Showing the effectiveness of the proposed network by using it as a model - based lane detection model. 2. Conducting an analysis of different configurations of multiple encoded modules. 3. Redesigning the evaluation methods of lane detection based on IoU to better understand the limitations of current methods.
SP:68b0a10ca06df74612d0753cc3f3ddddde806035,"This paper introduces a method for learning from logged feedback in batch contextual bandit learning, where one only has access to a collection of logged feedback from the actions taken by a historical policy and expects to learn a policy that takes good actions in possibly unseen contexts. This setting is ubiquitous in online and interactive systems, such as ad platforms and recommendation systems. The proposed method is called Maximum Likelihood Inverse Propensity Scoring ( MLIPS ) and it aims to improve the mean squared error of two existing methods based on inverse propensity scoring ( IPS ) and policy optimizer for exponential models ( POEM ). Theoretical analysis shows that the proposed MLIPS estimator is asymptotically unbiased, and moreover, has a smaller nonasymptotic means squared error than the classical IPS estimator. Such an error reduction phenomenon is somewhat surprising as the estimated surrogate policy is somewhat less accurate than the given historical policy. Experimental results on multi - label classification problems and a large - scale ad placement dataset demonstrate the empirical effectiveness of MLIPS. Furthermore, the proposed surrogate policy technique is complementary to existing error reduction techniques, and when combined with existing approaches, further improves the performance."
SP:8e0ed65c5dded23b34798499b2436b24422fd729,"This paper proposes a meta - learning framework for few - shot classification task that learns how to create individualized feature embedding specific to a given query image for better classifying. Given a query image, the proposed framework first learns to generate convolutional kernels based on the specific features in the query image to create an individualised feature space for that query image. The proposed framework uses a kernel generator as meta - learner to learn how to generate the convolution kernels for different query images. The kernel generator acquires meta - knowledge of generating the correct convolution kernel for a specific query image during training. In the experiments, two data sets, i.e. Omniglot and mini - ImageNet, are used to evaluate the proposed method. The experimental results demonstrate the superiority of the proposed approach over the baselines."
SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,"This paper proposes a population - based Genetic Algorithm ( GA ) for training deep neural networks, which is trained via gradient - based evolutionary methods, such as backpropagation. The authors show that the GA outperforms ES, DQN, A3C, and ES on Atari. The GA is faster and more efficient than ES, and enables the creation of a network with over 4 M free parameters, the largest ever evolveable network trained with a traditional evolutionary algorithm. The paper also shows that a combination of the GA and a novelty - based algorithm ( novelty search ) can outperform the other reward - maximizing algorithms in a high - dimensional maze environment."
SP:dfdbe3267a8160f24746884cdf5297993e424231,"This paper proposes a curiosity method inspired by the curious behaviour in animals that observes something novel and observes how it can be rewarded with a task reward that is concatenated with the real task reward. This is similar to ICM ( Pathak et al., 2017 ) that rewards the agent to create rewards for itself — thus making rewards dense and more suitable for learning. The novelty bonus is computed by comparing the observation of the current state with the observations in memory — which is based on how many environment steps it takes to reach the current observation from those in memory. This method is compared with other methods — including the state - of - the - art curiosity method ICM — under the same budget of environment interactions in VizDoom, DMLab and MuJoCo. The results show that the proposed method outperforms the baseline ICM in some navigational tasks and outperforms PPO in some non - reward - based tasks. However, the method performs poorly when the reward is not very dense."
SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,"This paper proposes a method for learning a sparse relational transition model, i.e., a template for describing transition models in complex uncertain domains, for example, when a robot pushes stacks of objects on a table, by selecting a set of relevant objects and a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct an iterative set of deictic references that determine which objects are relevant in any given state. Feed - forward neural networks are used to learn the transition distribution on the relevant objects ’ properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain."
SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"This paper proposes a new instance - wise feature selection method, which they call INVASE. The method consists of 3 neural networks, a selector network, a predictor network, and a baseline network. Each of these networks are trained iteratively, with the selector network being trained to minimize a Kullback - Leibler ( KL ) divergence between the full conditional distribution and the selected - features - only conditional distribution of the outcome. The proposed method can flexibly discover feature subsets of a different size for each instance, which is a key limitation of existing state - of - the - art methods. The authors demonstrate through a mixture of synthetic and real data experiments that INVASE significantly outperforms state -    benchmarks, including both static and dynamic benchmarks."
SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"This paper proposes a method for semantic segmentation ( STS ) that combines global and patch - level alignment for structured output prediction. The global alignment is achieved by learning a discriminative feature extractor module that is trained on unlabeled source patches and target patches. The classifier is trained by maximising the distance of the target patches from the source ones in a disentangled space, conditioned on the label histogram of source patches. Then, the adversarial learning scheme is used to align the classifier feature representations in the global space with those of the source patches in a patch - wise clustering. Experiments are conducted on several benchmark datasets for STS in synthetic and real world settings to show that the proposed STS method outperforms several baselines and state - of - the - art methods on STS. Ablation study and experiments are conducted to validate the effectiveness of the proposed method under various settings."
SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"This paper proposes two new optimistic algorithms for training neural networks, namely AMSGrad and Adam, respectively. The new algorithms combine the idea of momentum method, adaptive gradient method, and algorithms in OPTIMISTIC ONLINE LEARNING, which leads to speed up in training deep neural nets in practice. The experiments show that the proposed algorithms are faster than the baselines. A potential direction based on this work is to improve the method for predicting next gradient."
SP:52228b48f2776d57dd422edb33b82e247f056b75,"This paper studies the question of image classifier corruption and perturbation robustness. To this end, it introduces two datasets, IMAGENET - C and IMAGenet - P, which are the first comprehensive benchmarks for these two metrics. The former one measures the model's performance on relative corruption only, while the latter one also measures the network's prediction performance on perturbed images. The paper finds that AlexNet and ResNet classifiers exhibit negligible changes in relative corruption robustness from AlexNet - C to ResNet - P. Then, it establishes a new metric, perturbational stability, which measures the stability of the classifiers'performance on common perturbations such as common corruption and not worst - case adversarial attacks. It shows that even with the absence of adversarial defense, a bypassed adversarial example defense such as logit pairs can yield substantial robustness gains. Through experiments, it also introduces several methods to increase the robustness of the model, e.g. histogram equalization, multiscale architectures, and larger featureaggregating models."
SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"This paper shows that dropout training can be understood as performing MAP estimation for an entire family of conditional models whose objectives are lower bounded by the usual dropout objective. This discovery allows them to pick any model from this family after training, which leads to a substantial improvement on regularisation - heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully stochstic drop out objective. The deterministic subvariant ’s bound is equal to its objective, and the highest amongst these models. It also exhibits the best model fit in their experiments. Together, these results suggest that the results of previous work that deterministic dropout is a good approximation to MC averaging is misleading. The best of this family in terms of generalisation to improve language modelling is now possible thanks to this discovery."
SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,This paper proposes a method to prune redundant convolutional filters in CNNs by using a combination of global pruning and soft pruning. Global pruning measures the redundancy of the filter in the whole network by using the prune rate applied to each layer in the network based on the global average. Soft pruning is also used in the model recovery training process to improve the accuracy. The main contribution of this paper is the introduction of the cumulative saliency based global soft filter pruning ( GSFP ) method. Experiments shows that GSFP is effective on several classic CNN architectures and different data sets. GSFP achieves a much higher compression ratio compared with prior work while maintaining the same test accuracy.
SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,"This paper proposes a CLDC framework, CACO, that trains a document classifier with labeled training documents from a related language and optionally a small dictionary, pre - trained embeddings, or parallel text. The key idea is to exploit subword similarities between related languages to generalize from source language data. This technique is particularly useful in low - resource settings where unlabeled monolingual or parallel texts are limited. The authors provide empirical evaluation on multiple related language pairs, and CacO matches high resource CLWE - based methods."
SP:544e421f9c747640d949f433e3091763508b7237,"This paper proposes a new method for the localization of dense and integral regions in the THUMOS14 and ActivityNet1.3 videos. The proposed method is based on the marginalized average aggregation ( MAA ) algorithm. MAA aggregates multiple clips from the video according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. The authors prove that MAA reduces the difference in responses between the most salient regions and the others, and thus MAAN is able to generate better class activation sequences. Experiments on two large - scale video datasets show that MAAN achieves a superior performance on the two benchmark datasets."
SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"This paper proposes a Holographic Reduced Representation ( HRR ) framework for learning disentangled language models, inspired by VSA and holographic reduced representation ( HOLR ). The main idea of the framework is to use the reduced representation to learn a principled decomposition of representation. The proposed models are designed to work on both the word - level and chunk - level representations. The first part of the paper provides a background on HOLR and VSA. The second part introduces a language model with two main components, the inductive bias and the role - based model. The inductive part is based on the idea that by learning the role function as a function of the input, the learned representation can uncover information about the target language's role structure. The role function is learned by taking the input as input, splitting it into chunks of the same length, and then taking chunks of that length as input. The model is trained with two types of losses : ( 1 ) an anti - inductive loss that encourages the learned role function to have a fixed length and ( 2 ) an inductive regularization that encourages chunks to have different lengths.   Experiments are conducted on synthetic data and results are presented showing that the proposed model outperforms the baselines. However, there are some issues with the empirical results."
SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"This paper studies joint active perception and planning in Per - Observable Markov Decision Processes ( POMDPs ). In particular, it considers the problem of picking a cardinality - constrained subset of observations, in addition to original planning action. To tackle the computational challenge of adding combinatorial actions, the authors propose a greedy scheme for observation selection. The greedy scheme aims to minimize the conditional entropy of belief which is a metric of uncertainty about the state. Furthermore, founded upon the theoretical guarantee of greedy active perception, the paper develops a point - based value iteration solver for AP2 - POMP solver. The idea introduced in the solver to address active perception is general and can be applied on state - of - the - art Point - based solvers. Lastly, implemented and evaluated the proposed solver on a variety of robotic navigation scenarios."
SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"This paper studies the relationship between data complexity and network training, and investigates why hard example might hurt performance at beginning but helps at end. The paper proposes a curriculum loss that consists of two parts : a ) an adaptive weight that mitigates large early punishment ; b ) an additional representation loss for low - weighted samples. The intuition of the loss is very simple : We train top layers on “ good ” samples to reduce covariate shift, and encourage “ bad “ samples to learn from ” good “ sample. The adaptive weight assigns small values to hard examples, reducing the influence of noisy gradients. On the other hand, the less - weightsed hard sample receives the proposed representation loss. The proposed curriculum loss aims to encourage their training. Experimental results showed that the proposed algorithm have an consistent performance improvement on all datasets and accelerate training."
SP:8b555b9f24044bc68c204169d6a37e262361d706,"This paper presents a method to learn heuristic heuristics for vehicle routing problems of up to 100 nodes with the same set of hyperparameters. It builds upon the attention layer approach in Concorde ( Applegate et al., 2006 ) and proposes a new attention layer based on the attention convolution layer of the Interpreter Network ( IU ). The attention layer is trained with Reinforce ( Reinforce ), a powerful attention layer model. To train this model, the authors propose to use a simple but effective greedy rollout baseline : deterministic greedy rollout, which they find to be more efficient than using a value function. They show that their method outperforms Concorde and other heuristic approaches for two variants of the vehicle routing problem ( VRP, OVP, and PCTSP ), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms."
SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"This paper proposes a differentiable neural architecture search ( DNAS ) framework for quantizing different layers with different bit - widths. The authors formulate the mixed precision quantization of a ConvNet as an architecture search problem, by constructing a super net whose macro architecture is the same as the target network. Each layer of the super net contains several parallel edges representing convolution operators with quantized weights and activations with different precisions. They use DNAS to search for layer - wise precision assignment for ResNet on CIFAR-10 and ImageNet, and surpass the state - of - the - art compression. Their quantized models with 21.1x smaller model size or 103.9x smaller computational cost can still outperform baseline quantized or even full precision models."
SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"This paper studies the problem of combining the attention and output distributions of neural networks in order to model the dependence of the output and attention along the length of the predicted sequence. The authors propose a form of attention distribution called posterior attention that is conditioned on the output. The output token distribution is obtained by aggregating predictions from all the previous attention generators. Experiments on sequence prediction tasks such as translation, morphological inflection, and BLEU show that posterior attention outperforms soft - attention and vanilla attention in terms of entropy, running time overhead, and alignment accuracy."
SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"This paper proposes a new method, called HarmonicGAN, for the unpaired image - to - image translation task. The main idea is to introduce a smoothness term over the sample graph to enforce similarity - consistency between the source and target samples. The paper compares it to the existing methods, such as GANs and GAT, under an identical setting as CycleGAN, but with a few changes. The key difference is that instead of using labels to learn bi - directional translations, they use bi - directional ones to learn smoothness - consistent ones. The authors claim that this leads to improved accuracy and interpretability compared to CycleGAN. The method is evaluated on a variety of image - related tasks, including medical imaging, object transfiguration, and semantic labeling."
SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"Gradient clipping in recurrent neural networks can prevent important gradient components from being back - propagated adequately over a large number of steps, which is known as the exploding and vanishing gradient problem ( EVGP ). This paper proposes a stochastic algorithm, h - detach, that is specific to LSTM optimization and targeted towards addressing this problem. The algorithm is motivated by the hypothesis that the gradient components through the linear path ( cell state ) in the LSTMs computational graph get suppressed because they carry information about long term dependencies. The paper then proposes h -detach, which prevents gradients flowing through this path from getting suppressed, thus allowing the L STM to capture such dependencies better. The experimental results show that h - Detach achieves state - of - the - art convergence asymptotically on the task of image captioning."
SP:9aaff3777321347d1194884af5690b0b5185eff9,"This paper proposes SnapQuant, a probabilistic method for training binary weight neural networks from scratch under the Bayesian deep learning perspective. The objective is to approximate the posterior distribution of binary weights rather than reach a point estimation. The posterior distribution is parameterized as a policy network trained with a reinforcement learning scheme. The policy network has a nested parameter structure consisting of layer - wise, filter - wise and kernel - wise components. This nested structure is applicable to any neural network architecture and hierarchically models the joint posterior distribution. The performance of SnapQuant is evaluated with several visual recognition tasks including ImageNet."
SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,"This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to train local neural network weights, which are modeled through the framework. Then, the proposed method proposes an inference approach that allows to synthesize a more expressive global network without additional supervision or data pooling. The proposed method is tested on two image classification datasets simulated from the popular ImageNet and CIFAR-10."
SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"This paper introduces Stable Opponent Shaping ( SOS ), a method that interpolates between LOLA and a stable variant of LOLA, called LookAhead, to provide convergence guarantees for all differentiable games with n - player, non - convex objectives. Theoretically, the paper shows that LOLA converges locally to equilibria and avoids strict saddles in all equivariant games, and that SOS inherits these essential guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally. On the experimental side, the authors demonstrate that SOS plays tit - for - tat in the IPD on par with LOLA while all other methods mostly defect. The practical consequences are also demonstrated in the tandem game, where SOS always outperforms LOLA. Finally, in a GAN setup, testing for mode collapse and mode hopping when learning Gaussian mixture distributions, SOS successfully spreads mass across all Gaussians, at least matching dedicated algorithms like CO and LA, while LA is significantly slower."
SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"This paper proposes an alarm system for segmentation algorithms. The idea is to use a variational auto - encoder ( VAE ) that is trained on ground truth to detect out - of - distribution shapes in the data. The paper then collects the segmentation results from the training data and uses the VAE to extract the shape feature of a segmentation result. Finally, the representation in the one - dimensional feature space is used to train a classifier to estimate the regression parameters. The VAE is trained using only the ground truth masks, therefore the bad segmentations results with bad shapes become the rare events for VAE and will result in large loss value. Experiments are performed on several segmentations algorithms from the medical segmentation task. The system consistently provides reliable prediction on the qualities of segmentationResults."
SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"This paper proposes a simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoders has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. The underparameters are applied to compress images into a concise set of network weights, which the authors show is on par with wavelet - based thresholding. Further, underparameterization provides a barrier to overfitting, which enables them to show state - of - the - art performance for denoising.   Each layer of the DeepDecoder has an identical structure that consists of only one upsampling unit, pixel - wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis."
SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"This paper presents an end - to - end neural network capable of mapping relatively complex, multi - sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi - directional multi - layer LSTM for processing of word sequences. The decoder features a doubly - recurrent L STM, for which they propose novel signal propagation schemes and soft attention mechanism. When applied to a large dataset of problems proposed in a previous study, SAPS performs on par with or better than the method proposed in the previous paper, producing correct programs in over 92 % of cases. The authors find it likely that methods like SAPS may be successfully applied for practical synthesis of short code snippets, for instance in end - user programming. In future works, the plan to devise means to address the composite character of both specifications and program code."
SP:d2ec231bb6153a303e5110e671dea14c2721e636,"The paper presents a new adversarial defense strategy for MNIST against adversarial perturbations of the form $ L_0, L_2 $, L_\theta$, and $ \L_\infty$. The key insight is that even the most robust ( and widely recognized ) L∞ - based defense, Madry et al. ( 2018 ), has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbation, the paper shows. The authors then propose a new approach based on analysis by synthesis that seeks to explain its inference by means of the actual image features. They derive bounds on the robustness and go to great length to empirically evaluate their model using maximally effective adversarial attacks by ( a ) applying a combination of decision - based, score - based and gradient - based attacks for several different Lp norms, ( b ) by designing a new attack that exploits the structure of our defended model and ( c ) by devising a novel attack that tries to minimize the number of perturbed pixels ( L0 ). The results suggest that their approach yields state - of - the - art robustness on MNIST, which is surprising since most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class. However, preliminary experiments on two class CIFAR datasets indicate that the proposed model is not robust on the required test set."
SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"This paper proposes a reparameterization approach for controlling the spectra of weight matrices in the discriminator of GANs through various regularizers and constraints, inspired by recent work ( e.g., Miyato et al., 2016, 2017 ). The main contributions are :   ( 1 ) This paper introduces a new SVD - type reparametrization, which is based on the existing one ( SVD, 2018 ), to control the spectral normalization of the weight matrix of discriminator. ( 2 ) This method allows to use regularizers that encourage the slow singular value decay of the corresponding weight matrix without computing the singular value decomposition ( which is computationally expensive ). ( 3 ) Theoretically, this paper shows that the spectrum control improves the generalization ability of the GAN. ( 4 ) The experiments on CIFAR-10, STL-10 and ImgaeNet datasets support the theoretical results and demonstrate the effectiveness of the proposed method."
SP:8115fd9b681198d62100c36794926fb57dc0a4f5,"This paper proposes an accelerated value iteration method based on the Anderson acceleration technique. The proposed method is called Anderson Accelerated Value Iteration ( A2VI ). It is further extended to apply to deep Q - learning by extending the standard DA2Q algorithm. The convergence analysis of the proposed method, which is based on Anderson accelerated methods, is provided. Empirical results on toy problems and Atari games are provided to demonstrate the performance gain."
SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,"This paper proposes a novel method, SupportNet, to efficiently and effectively solve the catastrophic forgetting problem in the class incremental learning scenario. SupportNet combines the strength of deep learning and support vector machine ( SVM ), where SVM is used to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training, to prevent catastrophic forgetting. Two powerful consolidation regularizers are applied to stabilize the learned representation and ensure the robustness of the learned model. The experimental results validate the proposed method with comprehensive experiments on various tasks, which show that SupportNet outperforms the state - of - the - art incremental learning methods."
SP:d228d213f79716774043cea253305fecece659ec,"This paper studies various ways of measuring the selectivity of hidden units in the recurrent neural network ( RNN ) AlexNet. It compares various measures of selectivity, namely localist selectivity ( Bowers et al, 2014 ; Bowers, 2014, 2015 ; Zhou et al., 2015, and 2015 ; class - conditional mean activity selectivity CCMAS ; Morcos et al., 2018 ; and a new measure called top - class selectivity ; and generates activation maximization ( AM ) images that maximally activated individual units and generates interpretable images. It finds that the localist measures fail to capture highly selective units in AlexNet ; the most selective hidden units are found in layers fc6 and fc8, but not in fc9 or fc10. Furthermore, interpretable AM images generated with low - level selectivity scores associated with low selectivity. The findings highlight the problem with current selectivity measures and show that new measures are required in order to provide a better assessment of learned representations in NNs. The paper also considers why localist representations are learned in RNNs and not AlexNet, and provides an explanation."
SP:b9deae0392e0160b400d76c549d382e235196f8c,"This paper proposes a new class of graph neural networks, called Graph Neural Networks ( GNN ), that can solve community detection problems in a supervised learning setting. In particular, GNN can be trained without access to the underlying generative models, such as those used in stochastic block models, to solve the problem of community detection of binary and multiclass SBM on graphs. The GNN is trained by augmenting the non - backtracking operator defined on the line graph of edge adjacencies with the power graph adjacency matrix. The proposed GNN and LGNN are empirically shown to achieve good performance on real - world datasets. The authors also provide a theoretical analysis of the optimization landscape of using ( linear ) GNNs for community detection, showing that the loss value at any local minimum is close to that at the global minimum."
SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"This paper proposes a dictionary learning algorithm that recovers both the dictionary and the coefficient, under provable assumptions. The decomposition of the problem is given by a regularized least squares function, where the nonlinear function S is enforced to encourage sparsity. The proposed algorithm, NOODL, alternates between an IHT - based update for the coefficients and an update of the dictionary using a gradient descent - based estimator. Theoretical guarantees on the recovery of both factors and the coefficients are provided. The online nature of the algorithm, along with its linear convergence, makes it suitable for evaluation on synthetic data, and compares favorably with state - of - the - art DL techniques."
SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"This paper proposes a similarity search method for binary hash codes with any differentiable model and similarity function. The similarity function is a log likelihood loss applied to the probability that two inputs fall within a Hamming distance target. The paper also proposes a training scheme to obtain a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch. To leverage the resulting hashes, they use multi - indexing. They report the best results to date on competitive information retrieval tasks for ImageNet and SIFT 1 M, improving MAP from 73% to 85% and reducing query cost by a factor of 2-8."
SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,"This paper proposes Graph HyperNetwork ( GHN ) for neural architecture search. Given an architecture, the GHN directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks. To perform NAS, they randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. The GHN is fast – they can search nearly 10x faster than other random search methods on CIFAR-10 and ImageNet. GHN can be further extended to the anytime prediction setting, where they have found networks with better speed - accuracy tradeoff than the state - of - the - art manual designs."
SP:65ccf43cd4e033d22239069057f5200d49f33724,"This paper proposes M - GAIL, a method to improve GAIL by using non - expert demonstrations as an extra class in discriminator learning. It builds upon the approach of SSIRL and IRLF by performing a multiclass classification to learn discriminator functions with non - experts demonstrations regarded as being drawn from an "" extra class "" of demonstrations. Compared to related methods that use an additional dataset for IL, M -GAIL relies on a less restrictive assumption on the dataset and can efficiently train a deep neural network. It also avoids learning a mixture policy by only using expert demonstrations and does not learn the expert policy. Experiments on benchmark continuous control tasks show that our method performs better than GAIL especially when only a small number of expert demonstrations is available."
SP:e8427949a98effbd37ce7604fa11f240e2342196,"The paper proposes an Invertible Neural Network ( INN ) to solve the inverse problem. In inverse problem, multiple measurements may be obtained from the same set of observations. In order to handle this ambiguity, the posterior over measurement space has to be determined. An INN solves this problem by learning the inverse through a latent variable distribution. The distribution of the latent variable can then be used to determine the full posterior. The paper provides theoretical guarantees and empirically validates the INN on several applications : medicine and astronomy."
SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"This paper proposes a new type of neural network architecture, called compound density networks ( CDN ), which is a continuous extension of mixture density nets ( MDN ). MDN corresponds to a compound distribution with uncountable components, where both the component distribution and the mixing distribution are parametrized by NNs. The proposed CDN is the continuous counterpart to mixture density networks and can be implemented by using a hypernetwork to map the input to a distribution over the parameters of the target NN, that models a predictive distribution. An extensive experimental analysis shows that the proposed model results in better uncertainty quantification and is more robust to adversarial examples than previous approaches."
SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"This paper studies the problem of reducing the memory footprint of deep neural networks in order to reduce the computational cost of training neural networks. The authors propose a method called MIRACLE, which compresses the weights of neural networks according to a variational distribution over weights, following the bits - back argument. Specifically, the variational family of weights is replaced by the full Gaussian family, where each weight is sampled from this full family. The compression rate is controlled via a Monte Carlo method. The method is evaluated on MNIST, LeNet-5, and VGG-16 / CIFAR-10 and shows state - of - the - art performance compared to previous methods."
SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,"This paper proposes ProxylessNAS, a differentiable NAS method that directly learns the neural network architecture on target task and hardware instead of with pretrained models. The main contributions are two - fold :    ( a ) Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue ; ( b ) it can only grow linearly w.r.t the candidate set size. Therefore, it may use proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. ( c ) Benefiting from directness and specialization, Proxyless NAS can achieve better results than previous proxy - based approaches such as MnasNet, with 200x fewer GPU hours. ( Tan et al., 2018 ), and achieves the same accuracy level of top-1 accuracy of MobileNetV2 while being 1.8x faster by a factor of 1.4.   In addition, it can also be applied to specialize neural architectures for hardware with direct hardware metrics ( e.g. latency ). And it provides insights for efficient CNN architecture design."
SP:e5b70d43d301d1980fae02623ea711976b429c14,"This paper proposes a method for learning non - convex classifiers using second - order penalties in lieu of the usual additive penalty structure commonly used. Second - order penalty structure allows training the objective with a fixed value of the penalty coefficient, thus avoiding instability issues of two - player min - max games. The authors also derive a method to efficiently compute the gradients associated with the second order penalty in stochastic mini - batch settings. The resulting algorithm performs well empirically, learning an appropriately fair classifier on a number of standard benchmarks. In each setting, the algorithm adequately optimize the desired constraints."
SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"The paper proposes a reweighted wake - sleep ( RWS ) algorithm for learning deep generative models with discrete latent variables. The main contributions of the paper are two - fold : 1 ) addressing the issue of branching in the samples within the model, and 2 ) providing a path - wise derivative for the samples. The proposed RWS outperforms current state - of - the - art methods in learning discrete latent - variable models, such as the continuous - relaxation - based methods for the former and IWAE for the latter. The paper also shows that RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent variable models as well."
SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"This paper proposes a method for training structured prediction energy networks ( SPENs ) based on reward functions, such as scalar reward functions that can be assembled from human knowledge or non - differentiable pipelines. The authors propose to use truncated randomized search in this reward function to train a SPEN network using gradient - based search on a learned representation of the score landscape. The proposed method is called SG - SPEN and it is used to train SPEN models using supervised learning. The key ingredient of the training algorithm is sampling from the scalar rewards function through truncated random search, which generates informative constraints that guide gradient - descent to find better prediction according to the reward function. Through experiments, the authors show that the proposed method achieves better performance than previous methods such as learning from a reward function using policy gradient and SPEN."
SP:638c1bc09992029b78bd83f0127594dcccb96c06,"This paper proposes an active learning based framework, EffAcTS, to enable robust policy search, i.e. collecting only as much data as necessary to select a subset of a large set of trajectories, such as the ones that result in the worst performance, for transfer from simulation to the real world. The proposed framework is instantiated on top of EPOpt, a state - of - the - art policy search method. Experiments on environments from standard continuous control benchmarks are provided that illustrate the effectiveness of the proposed active learning approach. The paper also draws connections between the active learning and multi - task learning, as well as discusses the similarities and differences between Robust RL and Multi - Task Learning."
SP:491c239713a6489f0b1790ca26db54a1813c67ae,"This paper proposes a two - timescale network ( TTN ) architecture for nonlinear value function approximation that combines a slow learning process for adapting features and a fast process for learning a linear value function, both of which are straightforward to train. By leveraging the two timescales, the authors are able to prove convergence guarantees for a broad class of choices for both the fast and slow learning components. They highlight several cases where the decoupled architecture in TTN can improve learning, particularly enabling the use of linear methods, such as least - squares methods and eligibility traces. They also empirically demonstrate the benefits of TTN, both for policy evaluation and control."
SP:327d606cf3813b00a009a7785e08ef9e11f89493,"This paper proposes LEArning and Planning with Semantics ( LEAPS ), which is a hybrid model - based approach that combines a multi - target policy that acts on visual inputs, and a Bayesian model over semantic structures, to adapt to unseen and man - made environments that are visually diverse but contain semantic regularities. The framework consists of three components : 1. A high - level policy that, when placed in an unseen environment, takes the current semantic state, the next sub - policy to execute, and updates the semantic model based on new observations. 2. A policy that focuses on a set of targets that are drawn from a pre - trained set. 3. A search policy that aims to find sub - targets that correspond to the current target state.   Experiments are conducted on the navigation task of House3D, a 3D environment that contains diverse human - designed indoor scenes with real - world objects. The proposed LEAPS outperforms strong baselines that do not explicitly plan using the semantic content, and model - free baselines."
SP:d7c26f43bc68d160095b1f50447528843d79edbd,"This paper proposes a new model that consists of two modules : perception module and driving module. The perception module is used for learning easier driving - related perception knowledge, which the authors refer to as pixel level understanding of input. Specifically, they trained a segmentation map and depth map first, while the former serves as what & where knowledge and the latter serves as how far knowledge. The results of experiments demonstrated the effectiveness of the multitask perception knowledge for better generalization and accident explanation ability. The average sucess rate of finishing most difficult navigation tasks in the City of CoRL test surpassed current benchmark method for 15 percent in trained weather and 20 percent in untrained weathers"
SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,"This paper studies the trade - off between standard accuracy and adversarial robustness in adversarially robust generalization. The authors show that training robust models may reduce standard accuracy ( i.e., the robustness to adversarial perturbations ), but also yield better feature quality. This is because the features learned by robust models tend to align better with salient data characteristics and human perception. However, the authors also uncovers an unexpected benefit : the feature quality may be better because robust models learn invariances that are similar to human vision. This hints at a stronger connection between GANs and robustness. Finally, the paper also highlights the need to develop robust training methods, since robustness is unlikely to arise as a consequence of standard training."
SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"The authors propose Initialized Equilibrium Propagation ( i.e., an extension of Scellier & Bengio, 2017 ), a method to train biological neural networks without backpropagation. The main novelty of the method is that it does not rely on a mechanism for back - propagating an error gradient to the neurons. Equilibrium propagation requires an iterative search through neural networks to find a fixed point, and the number of steps required scales with the depth of the network. The paper proposes to train a feed - forward network to approximate this fixed point using a local learning rule, and then uses this initializing network for inference. Experiments are conducted on MNIST and CIFAR-10 and compare the proposed method with the original Equilibrium method. The experiments show that Initialized equilibration outperforms the original method."
SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,"This paper studies a zeroth - order ( ZO ) stochastic optimization method, ZO - signSGD, which is gradient - free, and has the dual advantage of being able to use the sign of the gradient estimate to guide the optimization. The main contribution of this paper is to provide convergence guarantees for this method under mild assumptions on the number of iterations and the solution size $ d$. This method is contrasted with the more commonly used SGD - type method, which has convergence guarantees on the order of O(d/\sqrt{d } ). The authors also study the effect of different estimators of the full gradient, and propose several variants with slightly improved convergence guarantees. Empirical evaluations on image classification datasets MNIST and CIFAR-10 demonstrate the superior performance of the ZO method over the SGD method on adversarial examples generation from black - box models."
SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"This paper proposes a method to reduce the computation efforts of convolutional neural networks by applying a new dynamic optimization method, i.e., MAC Check, to the multiply - accumulate ( MAC ) process in order to determine whether a filter should terminate early according to an intermediate result during the inference phase. The authors also propose a fine - tuning process to recover the accuracy drop due to the applied checkpoints. The experimental results show that the proposed method can save approximately 50 % MAC operations with less than 1 % accuracy drop for CIFAR-10 and CIFar-100 datasets. Compared to the state - of - the - art method, the proposed algorithm is more effective on the CIFARE-10 dataset and is competitive on the   CIFARS-100 dataset."
SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"This paper proposes a method to improve the robustness of ASR systems by exploiting the temporal dependency in audio data to gain discriminative power against adversarial examples. The authors consider three speech recognition ( ASR ) tasks and three recent audio adversarial attacks : speech - labeling, speech - to - text, speech2text, and speech3ac.    The proposed method is compatible with any ASR model and does not require adversarial training or data augmentation. The experimental results show that while four input transformations developed for image adversarial defense fail to withstand adaptive attacks, temporal dependency is shown to be resistant to these attacks. The paper also demonstrates the power of temporal dependency for characterizing adversarial example generated by three state - of - the - art audio attacks."
SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"This paper proposes a composition - based generative model ( GAN ) that learns about individual objects and background of a scene, without prior access to this information. The key idea is to generate images by means of composition ( i.e., by considering objects and their relations explicitly ), and generate images that are more faithful to the reference distribution. The paper evaluates this approach on several multi - object image datasets, and finds that the generator learns to identify and disentangle information corresponding to different objects at a representational level, and a human study reveals that the resulting GAN generates better images."
SP:fb59990b8da0e95d8202383478a456667de60449,"This paper proposes VAE with reference - based variational autoencoders, a novel deep generative model designed to exploit the weak supervisory signal provided by the reference set. Given a pool of unlabeled images, the goal is to learn a representation where a set of target factors are disentangled from others, and the only supervision comes from an auxiliary “ reference set ” that contains images where the factors of interest are constant. The proposed model uses the variational inference framework where adversarial learning is used to minimize the objective function. By addressing tasks such as feature learning, conditional image generation or attribute transfer, the proposed model is able to demonstrate its ability to learn disentangling representations from minimal supervision."
SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"This paper aims to develop a method for continual online learning from an incoming stream of data using deep neural network models. It proposes an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non - stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. The paper also shows that meta - learning can be used to meta - train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In the experiments, this method is applied to model - based RL, where adapting the predictive model is critical for control, and it is shown that MOLe outperforms alternative prior methods, and enables effective continuous adaptation to handle different distributions such as varying terrains, motor failures, and unexpected disturbances."
SP:5665e5f006f84927beb0440e145f476e02538077,"This paper investigates the effect of parameter lag and BPTPTT updates on the training of RNNs with experience replay in Distributed Reinforcement Learning ( RDNNs ). The authors show that parameter lag can lead to representational drift and recurrent state staleness, which can be exacerbated in the distributed setting. They also provide an empirical study into the effects of several approaches to RNN training with   experience replay, mitigating the aforementioned effects. Finally, they present an agent that integrates these findings to achieve significant advances in the state of the art on Atari-57 ( Bellemare et al., 2013 ) and matches the state - of - the - art on DMLab - 30 ( Beattie et al, 2016 ). To the best of their knowledge, this agent, Recurrent Replay Distributed DQN ( R2D2 ) is the first to achieve this using a single network architecture and fixed set of hyperparameters."
SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,"This paper studies the problem of training sequential generative models for capturing coordinated multi - agent trajectory behavior, such as offensive basketball gameplay. The authors propose a hierarchical framework that can capture long - term coordination using intermediate variables. The intermediate variables should capture interesting high - level behavioral semantics in an interpretable and manipulatable way. The approach is inspired by recent work on leveraging programmatically produced weak labels, which they extend to the spatiotemporal regime. In addition to synthetic settings, they show how to instantiate their framework to effectively model complex interactions between basketball players and generate realistic multi - agents trajectories of basketball gameplay over long time periods. The validate their approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts. They show significant improvements over standard baselines."
SP:1a90cdf028068528b0559e7d44bf26dda20310bd,"This paper proposes a method to integrate a learned dynamics model and a vision model in a graph - structured variational recurrent neural network ( Graph - VRNN ), which is trained end - to - end to infer the current state of the ( partially observed ) world, as well as to forecast future states. The authors show that their method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine. The main contribution is a unified way to do state estimation and future forecasting at the level of objects and relations directly from pixels using Graph -VRNN. They believe their technique will have applications beyond analysing sports videos, such as self -driving cars, self - driving cars, and human - robot interaction."
SP:8392f04b7265f665ba6d44d297bca245d44b4708,"The paper proposes a method for learning black - box functions by back - propagating through a differentiable neural network ( i.e., estimating a function before applying it to the data ) that can be realized by a policy gradient method ( e.g., backpropagation ). In particular, the differentiable estimator is defined so that it is compatible with the output of a blackbox function that is specified by the task. The black - boxes are then used to compute the output function of the neural network that was estimated by the estimator. Finally, the model is used in an end - to - end manner to generate the corresponding labels that are used during policy gradient optimization. The method is evaluated on a set of grid - world tasks and compared to differentiable policy gradient methods and fully neural networks. The results show that the integrated model generalizes better than fully differentiable models and learns more efficiently compared to RL - based methods."
SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,This paper proposes a method for generalizing the model - agnostic metalearning ( MAML ) algorithm by using a mixture of hierarchical Bayesian models over the parameters of an arbitrary function approximator such as a neural network. The authors propose to use a stochastic expectation maximization procedure to jointly estimate parameter initializations for gradient descent as well as a latent assignment of tasks to initialize the parameters in the model. This approach is claimed to better captures the diversity of training tasks as opposed to consolidating inductive biases into a single set of hyperparameters. The experiments demonstrate better generalization performance on the standard miniImageNet benchmark for 1 - shot classification. They also derive a novel and scalable non - parametric variant of their method that captures the evolution of a task distribution over time as demonstrated on a set of few - shot regression tasks.
SP:a410144dbe19713a06c63da87d9fb58b999a7492,"This paper proposes Meta Auxiliary Learning ( MAXL ), an unsupervised hierarchical auxiliary learning method for the classification task of CIFAR-10 where the auxiliary task is to predict sub - class hierarchical labels for an image. The proposed method is evaluated on three image datasets and shows that it outperforms other auxiliary learning baselines, such as Auxiliary GANs, as well as a human - defined auxiliary task using domain knowledge. The main contribution of the paper is that MAXL is able to generate auxiliary tasks which, when trained alongside a principal task in a multi - task setup, maximise the generalisation of the principal task across a validation dataset. This is achieved by using a self - supervised method, where a meta learner ( i.e., a neural network that is trained to generate the auxiliary tasks ) is used to determine the target labels for each sub class in the dataset to train a multi _ task evaluator. The performance of the proposed method, which is evaluated in three datasets, on three different image datasets, is shown to improve generalisation over existing methods for generalising auxiliary learning methods such as ASG, ASG - GAN, and Hausdorff - Stern."
SP:76248e1c914c60ce69de244fe7ec62488d01e161,"This paper proposes an approach for learning a neural network based representation for open set recognition ( OSS ), i.e., instances of the same class are projected closer together while instances of different classes are projected further apart. The motivation is that there are two properties which should lead to larger spaces among classes for instances of unknown classes to occupy, thereby facilitating open set Recognition. The paper proposes a loss function that enables to use the same distance function both when training and when computing an outlier score. The authors compare their proposed approach with a baseline network trained to minimize a cross entropy loss and with Openmax ( a state - of - the - art neural network - based open - set recognition approach ). The evaluated the approaches on datasets of malware samples and images and observed that the proposed approach achieves statistically significant improvement compared to previous research work on three datasets."
SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"This paper studies the problem of energy and area efficiency of embedded low - precision neural networks. Low - precision networks have been proposed as a way to reduce the computational and/or resource cost of computing deep neural networks for embedded or large - scale applications. One of the challenges that such networks face is that the loss in accuracy goes quadratically down as the precision goes down, while the gain in energy goes up as the number of bits goes up. One way to mitigate this issue is to train the network with fp32 precision, but this introduces a lot of computational and resource cost overhead. Another way is to scale up the size of the neural network to match the accuracy of the full - precision baseline, but at the cost of increased complexity. This paper tackles the issue of quantization of neural network weights and activations during training, and finds that it is not necessary to retrain the network at the beginning of each epoch because of the similarity of the weights to the baseline. Instead, it leverages the availability of pretrained models and fine - tune the model at each epoch to improve the performance. The authors demonstrate 8 - bit and 4 - bit models on the ImageNet benchmark, and show that their methods can match or exceed that of a pretrained baseline ( ResNet-18, ResNet - 34, and more ) at 8 bit precision. They also show that ResNet models at 4 bit precision are able to match that of the pretrained model. Surprisingly, the authors find that the weights of their models are very close to the corresponding baseline."
SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,"This paper proposes an end - to - end model to model surface properties governing bounces in everyday scenes. The model, dubbed Bounce and Learn, comprises two modules – a Physics Inference Module ( PIM ) and a Visual Inference module ( VIM ). VIM learns to infer physical parameters for locations in a scene given a single still image, while PIM is trained to model physical interactions for the prediction task given physical parameters and observed pre - collision 3D trajectories. The proposed model learns from a large dataset of real - world bounces and is bootstrapped with additional information from simple physics simulations. They show on the newly collected dataset that their model out - performs baselines, including trajectory fitting with Newtonian physics, in predicting post - bounce trajectories and inferring physical properties of a scene."
SP:010bd055310c363d3cb0fbe0e11546de58220e15,"This paper studies the relationship between adversarial vulnerability and gradient norms of differentiable classifiers with respect to the square root of the input dimension in terms of their sensitivity to adversarial noise. The authors provide theoretical and empirical support for their theoretical results through simple experiments on MNIST, CIFAR10, and Fashion - MNIST. They show that the dependence of the adversarial sensitivity on the gradient norm of the classifier is monotonic when viewed as a function of the image dimension, and that it increases as the image size increases ( Figure 1 ). They also provide strong support for the existence of a monotonicity in the loss function of such classifiers ( Figure 2 ), which is shown to also increase with increasing the number of losses ( Figure 3 ). Extensive experiments verify their results on various models ( CNN, fully - connected, and convolutional ) as well as on a slightly larger set of test images ( Figure 4 ). Experiments confirm that the results hold even after usual training on the test images."
SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"This paper proposes a curiosity - driven imitation learning method for agent modeling. The probing agent learns to interact with the environment and with a target agent ( i.e., a demonstrator ) to maximize the change in the observed behaviors of that agent. Through probing, rich behaviors can be observed and are used to enhance the agent modeling to learn a more accurate mind model of the target agent. The learning process consists of two learning processes :   imitation learning for an approximated agent model and ii ) pure curiosity -driven reinforcement learning to develop an efficient probing policy to discover new behaviors that can not be observed in the environment. The experimental results suggest that the agent model learned by this approach generalizes better in novel scenarios than the ones learned by passive observation, random probing, and other curiositydriven approaches do, and ii) can be used for enhancing performance in multiple applications including distilling optimal planning to a policy net, collaboration, and competition."
SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"This paper proposes a modification to traditional Artificial Neural Networks (ANNs ) which provides the ANNs with new aptitudes motivated by biological neurons. The proposed modification connects a new type of ANN nodes, which mimic the function of biological neuromodulators and are termed modulators, to enable other traditional ANN nodes to adjust their activation sensitivities in run - time based on their input patterns. In this manner, the slope of the activation function to be context dependent can be introduced. This modification produces statistically significant improvements in comparison with traditionalANN nodes in the context of Convolutional Neural Networks and Long Short - Term Memory networks. The experiment results show that the modulated models consistently outperform their original versions."
SP:287a577834fd2820a939a1113b39146a22727491,"This paper proposes a controllable generative model, NANSY, that can manipulate various aspects of the speech signal, e.g., pitch, voice, and time - scale. The proposed model can be trained in a fully self - supervised manner, i.e., it is trained without any labeled speech data such as text or speaker information. Instead, it uses a new set of analysis features, namely wav2vec and Yingram. Experiments are conducted on voice conversion ( zero shot voice conversion ), pitch shift ( pitch and frequency response ), and timestep - based modification ( adaptation ). The results show that the proposed model outperforms other state - of - the - art speech synthesis models.    Contributions :   1. By proposing a novel training strategy based on information perturbation, which can be used to selectively take essential attributes to reconstruct the input signal, the proposed NANSy enjoys both high reconstruction quality and controllability. 2. The effectiveness of the proposed TSA method by testing it on a small test - time speech sample in order to improve its performance. 3. By using Discriminator, a byproduct network, to discriminate real speech samples from fake speech samples, the authors demonstrate the detection performance of NansY."
SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"The paper studies generalization properties of approximate bilevel programs for hyperparameter optimization based on gradient descent. In particular, it establishes an expectation bound for the unrolled differentiation algorithm based on a notion of uniform stability on validation.   Theorem 1 establishes a bound on the learning rate in the outer validation set w.r.t. the validation set based on uniform stability. Theorem 2 establishes a lower bound on a term in the inner validation set that induces a regularization term in both the outer and inner levels that can alleviate the overfitting problem in gradient - based algorithms. The paper also shows that the classic cross - validation algorithm ( CV ) is also bound by an expectation of this type. Experiments on feature learning and data reweighting for noisy labels corroborate the theoretical findings."
SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,This paper proposes a method to transfer knowledge from a teacher to a student by training the teacher models on student unlabeled data. The teacher models are learned jointly by minimizing a loss function that penalizes the difference between the student model and the teacher model. The student model is then trained to be the student - friendly representation. The method is called Student - friendly teacher network ( SFTN ). The authors claim that it improves the transfer of knowledge from dark knowledge to light knowledge. The main advantage of the method is that it can be easily incorporated into existing knowledge distillation algorithms. The experimental results show that the proposed method outperforms the state - of - the - art in several distillation techniques with heterogeneous teacher networks.
SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"This paper studies the problem of out - of - distribution ( OOD ) generalization of deep learning models to datasets whose labels are out of distribution, under the assumption that invariance of the features extracted from the training set is guaranteed. This paper makes three contributions :   1. It provides a mathematical characterization of the OOD generalization problem, based on which it can be shown that generalization to OOD problems is learnable. 2. It defines and studies a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore gives a quantitative meaning of invariant features. 3. It establishes generalization bounds for the expansion function and gives guarantees for the generalization error.   Using these insights, it establishes a model selection criterion and conducts extensive experiments on benchmark OOD datasets to demonstrate that the proposed model selection method has a significant advantage over baselines."
SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"This paper proposes Variational Continual Bayesian Meta - Learning ( VC - BML ), a method that tackles the issues of negative knowledge transfer and catastrophic forgetting for streaming low - resource tasks that follow non - stationary distributions. The proposed method assumes that meta - parameters follow Gaussian distributions, and uses a Dynamic Gaussian Mixture Model ( GMM ) to model the whole mixture, which is a mixture of dynamically updated distributions, each component associated with a cluster of similar tasks. To approximate the intractable posterior of interest, a structured variational inference method is proposed. Experiments on tasks from non - stationary distributions show that the proposed method outperforms state - of - the - art baselines when adapting to diverse tasks and alleviating knowledge forgetting."
SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"This paper proposes a method for solving the probabilistic solution of boundary value problems ( BVPs ), which are ODEs that are ordinary differential equations subject to boundary conditions. The main contribution of this paper is the introduction of a Gauss - Markovian prior ( GMA ) that is specifically tailored to solve the BVP problem. This allows computing a posterior distribution over the solution of a BVP in linear time, at a cost that is comparable to that of well established, non - probabalistic methods. The proposed method also provides the same linear time complexity as traditional BVP solver - off - the - shelf solvers with high - quality estimates and calibrated uncertainty point estimates that can be set automatically by the user. Finally, it provides the mesh refinement, which improves the efficiency of the scheme.   The proposed model also provides uncertainty quantification, mesh refinement and hyperparameter adaptation steps to improve the solver's utility. Theoretically, it is shown that the proposed method is compatible with other parts of the statistical modelling tool-chain, such as hyperparametrized parameters, and it is arguable that it can be practically usable as well for BVP problems. In the experiments, the authors demonstrate how these practical considerations positively impact the efficiency."
SP:86aac0c6b75fdc12f84bba342934865616f866d4,"This paper considers a reward - mixing mixing process ( RM - MDP ), where a reward function is sampled from a set of mixing reward models, but only one of these reward models is available to the agent. The identity of the chosen reward model is kept a secret from the agent, and exploration is performed by picking the best one based on the best known policy from episode history. The paper proposes an exploration method based on a polynomial - time algorithm that finds an - optimal policy after exploration poly - poly(poly(H, S, A ) episodes, where H is the time horizon, S is the number of states and actions, and A and O(H - A ) are number of actions respectively. This is the first efficient algorithm for RL that does not require any assumptions on the dynamics of the system in this partially observable MDP, such that the observation space is smaller than the latent state space. The main contributions of the paper are the following :   1. An efficient method for learning a near optimal policy for two reward mixing MDPs with no assumptions on rewards or dynamics. 2. A method to recover an empirical model obtained by solving an LP for all mixing weights and pick the one that minimizes the dropout at the end of each episode. 3. An extension to non - uniform mixing weights in the form of sweep - learning to handle all possible mixing weights with any known weight is discussed."
SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"This paper proposes Single - cause perturbation ( SCP ), a method for conditional average treatment effect estimation ( CATE ) in multi - cause settings, where more than one single intervention can be performed at a time. CATE is defined as the average of the cumulative effects of interventions on variables that affect a group of patients of different treatment groups under the assumption that the intervention occurred at each of the six ( or more ) different “ causes ” listed in the paper. The paper shows that CATE can be used to estimate the treatment effect of interventions that occurred on at least one of the ( potentially multiple ) underlying causes. The proposed method consists of two steps : ( 1 ) augmenting the observational data with the ( estimated ) single - cause CATE ; ( 2 ) performing covariate adjustment on the augmented data to obtain the estimated treatment effect ; and ( 3 ) estimating the true treatment effect under the covariate assumption.    The paper validates the existence of the method under standard assumptions in causal inference. It is shown that the proposed method performs better than the baselines under the same set of assumptions ( i.e., only observation of one out of many possible outcomes and no confounding ). Extensive experiments demonstrate the performance gain achieved by the proposed CATE estimator."
SP:247bc6675cce89d51558537daf63dadb0c4307f8,"This paper proposes a multi - wavelet - based neural operator learning method for learning the inverse of the partial differential equation ( PDE ) corresponding to a partial differential operator ( differential operator ), which maps the input to the solution space and compresses the associated operator ’s kernel using wavelets. The proposed method learns the projection of the kernel onto fixed multiwavelet polynomial bases, which are learned at multiple scales derived from using repeated computation of multi wavelet transform. Compared with the existing neural operator approaches, the proposed model shows significantly higher accuracy and achieves state - of - the - art performance on a range of datasets. The experiments are conducted on the Korteweg - de Vries ( KdV ) equation, Burgers ’ equation, Darcy Flow, and Navier - Stokes equation."
SP:1153785e6a016cfee2644952a772aa08927299b6,"This paper proposes a new method for training binary neural networks ( BNNs ) by using the gradient of the original sign function in the Fourier frequency domain using the combination of a series of sine functions. The frequency domain approximation ( FDA ) does not affect the low - frequency part of the sign function which occupies most of the overall energy, and high - frequency coefficients will be ignored to avoid the huge computational overhead. Besides, it embeds a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using FDA achieves the state - of - the - art accuracy."
SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,"This paper studies distributed computation in the brain by training multi - area recurrent neural networks ( RNNs ) on a perceptual decision - making task. The authors propose to use neuroscience - inspired constraints inspired by Dale's law to motivate the learning of biologically plausible solutions by the networks. In particular, the authors show that incorporating multiple areas and Dale’s Law is critical for biasing the networks to learn biologically - plausible solutions. They also leverage the full observability of the RNN to show that output - relevant information is preferentially propagated between areas. The results suggest that cortex uses modular computation to generate minimal sufficient representations of task information. More broadly, the results also suggest that constrained multi - areas RNN can produce experimentally testable hypotheses for computations that occur within and across multiple brain areas."
SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,"This paper proposes a new visual representation, Structured Attention Graph ( SAG ), to help users gain a deeper understanding of CNN ’s decision making. The SAG is based on a new method, beam search, which uses a beam search algorithm to systematically search for multiple explanations for each image. It is shown that there are indeed multiple relatively localized explanations for many images. However, naively showing multiple explanations to users can be overwhelming and does not reveal their common structure. To compactly represent the different sets of attention maps for an image, the paper proposes to visualize SAGs by visualizing how different combinations of image regions impact the confidence of a classifier. An approach to computing a compact and representative SAG for visualization is proposed via diverse sampling. The paper conduct a user study comparing the use of SAG to traditional saliency maps and I - GOS for answering counterfactual questions about image classifications. The results show that user accuracy is increased significantly when presented with SAG and compared to standard saliency map baselines."
SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,"This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. The study finds that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross - entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks. The choice of loss has little effect when networks are fully fine - tuned on the new tasks. Using centered kernel alignment, the authors find that differences among loss functions are apparent only in the last few layers of the network. They delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. The results suggest there exists a trade - off between learning invariant features for the original task and features relevant for transfer tasks."
SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"This paper introduces selective backpropagation through time ( SBTT ), a method to obtain spatio - temporal super - resolution in neuronal time series by exploiting relationships among neurons, embedded in a latent low - dimensional population dynamics. The method is based on using a recurrent neural network to learn a latent dynamics model of observed variables, conditioned only on the observed variables at each time step, from which a neural network can infer the missing samples by combining observations with learned latent dynamics. They test the method on neural networks and sequential autoencoders, and demonstrate more efficient and higher - fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data using SBTT. The proposed method could lead to significant power savings for implanted neuroelectronic interfaces, and could pave the way to substantially decreased power consumption for fully - implantable brain - machine interfaces."
SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"This paper proposes a new model for sequence - to - sequence learning ( STS ) by modeling source and target trees as quasi - synchronous tree - copying neural grammars, where each node in the target tree is transduced by a node from the source tree, and both trees are treated as latent, i.e., they are not used in the training of STS. The motivation is that traditional STS models, while flexible and performant in many ways, can fail on benchmarks that measure compositional generalization such as SCAN, style transfer, and small - scale machine translation. The proposed model shares the parameterization of the grammar over the combinatorial space of the derivation rules, which enables parameter sharing without the need for manual feature engineering. The authors apply this model to various STS tasks and find that it performs reasonably well compared to baseline models."
SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,"This paper proposes a novel algorithm for the Group Elastic Net ( GNET ) formulation of a solution to the function - on - scalar feature selection problem, where a group of scalar $ k$ functions $ f_k$ are selected from a set of $ k-1,000,000 scalar features $ k$. The proposed method, called fgen, is based on the recently proposed SsNAL algorithm, which is then extended to the case when $ k=1,2,3 $ using the Augmented Lagrangian decomposition. The proposed fgen is evaluated on simulated data and applied to a GWAS study detecting a SNP that may affect obesity risk in children."
SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,"The paper proposes a mixture model of multi - level marked point processes to cluster repeatedly observed marked event sequences. A novel and efficient learning algorithm is developed based on a semi - parametric ES algorithm. The proposed method is demonstrated to significantly outperform other competing methods in simulation experiments and real data analyses. The current model only focuses on events over temporal domains. However, clustering of spatial patterns on 2 - or 3 - dimensional domains has also attracted much research interest. It will be an interesting research topic to extend the current model to such settings."
SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"This paper presents Online Meta - Adaptive - Control ( OMAC ), a meta - algorithm for adaptive nonlinear control in a sequence of environments. The goal is to control a nonlinear system subject to adversarial disturbance and unknown environment - dependent nonlinear dynamics, under the assumption that the environment dependent dynamics can be well captured with some shared representation. The approach is motivated by robot control, where a robotic system encounters a sequences of new environmental conditions that it must quickly adapt to. A key emphasis is to integrate online representation learning with established methods from control theory, in order to arrive at a unified framework that yields both control - theoretic and learning - theoretical guarantees. They provide instantiations of OMAC under varying conditions, leading to the first non - asymptotic convergence guarantee for multi - task adaptive non linear control, and integration with deep learning. They also validate OMAC empirically on inverted pendulum and 6 - DoF drone control tasks under varying wind conditions."
SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"This paper proposes three ways to improve IBP - based certified robust training methods such as CROWN - IBP and bound propagation based methods for training deep neural networks with certifiable robustness guarantees. The authors identify two important issues in the existing methods, namely exploded bounds at initialization and imbalanced ReLU activation states, and propose to add batch normalization ( BN ) to each layer in the model to reduce imbalance. They also propose IBP initialization and warmup regularization to mitigate the issues. Through these methods, they are able to efficiently train certifiably robust models that outperform previous SOTA performance in significantly shorter training epochs, and achieve a verified error of 65.03% verified error on CIFAR-10 ( $ \mathcal{CIFAR10}/8255 $ ) and 82.36% on TinyImageNet ( $ mathcal{TinyImagenet}/1, based on efficient IBP training )."
SP:18ffeb199a670fb2b1f4417b8653479001944dab,"This paper considers the problem of change point detection under the Huber - ε contamination framework, which allows the contamination distribution to be different at each time point. Under this framework, the adversary may deploy certain attacking strategies, e.g., create spurious change points or misleadingly mask the existence of true change points. The authors propose an adversarial algorithm based on ARC, which combines the idea of robust statistics and ARC - like approaches in the robust statistics literature to study the problem under this framework. They show that ARC is nearly - optimal in terms of both the signal - to - noise ratio condition and the localisation rate when the contamination ratio is small. The detection boundary is a function of the contamination proportion and is derived for the first time. The optimality results are first time shown in the literature, but are still somewhat restrictive. In order to achieve optimality in the whole parameter space, novel robust estimation and testing techniques are necessary and are on the agenda. The future plan includes both considering different contamination models and extending the methodology to more challenging data types."
SP:d03617b5fc446768809cf015c9234b0c9386a690,"This paper studies the relationship between PAC and Poly - batch Gradient Descent ( PAC ) learning in terms of the minibatch size b ( for SGD ) and sample size m ( for GD ), as well as the precision of the gradient calculations relative to the mini - batch size b in both cases. It shows that with enough precision, SGD and GD can always simulate learning with statistical queries ( SQ ) up to some limit, but their ability to go beyond that depends on the precision ρ ( the population loss ), and the trade - off between the precision and mini -batch size b. When b = 1 / n^2, for example, the power of SGD is equivalent to that of SQ learning. Similarly, GD can also simulate any sample - based learning algorithm based on m samples. Finally, the paper also studies the relation between bSQ and fbGD ( the so - called full batch variant ). In particular, it is shown that fbSQ is not able to simulate PAC or SQ with a sample size larger than m due to the difficulties in constructing a differentiable model capable of capturing the empirical loss."
SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"The paper studies the problem of constructing a uniform quantization over a set of N points that minimizes the Wasserstein distance to the model distribution. This minimization problem, where the unknowns are the positions of the atoms, is non - convex. However, there is a well - known algorithm for solving this problem, namely the "" Power "" algorithm by replacing Voronoi cells with Power cells. This algorithm is derived from the classic Lloyd - type algorithm, which is parameterized by barycentric projections of points onto a discretized version of the space. The paper shows that this Power algorithm converges to a point - by - point solution if and only if a sufficient number of Power cells dot the input distribution. It also provides error estimates for one step of this algorithm, starting from a cloud of points that are sufficiently far away from each other to guarantee tight convergence. Similar bounds can be deduced for the corresponding gradient descent."
SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"This paper proposes a new dynamic self - attention architecture for video understanding. The proposed RSA architecture is based on the Transformer architecture, which is a transformer - based architecture. RSA consists of two parts : ( 1 ) a kernel generator and ( 2 ) a context generator. The kernel generator generates a mapping from time and space to the kernel of a relational context. The context generator is trained by taking a look at the embeddings in a given video, and aggregating all the embedding contexts into a single embedding space. Then, the context generator generates the corresponding action kernels. The embedding in each context takes the form of a 2D embedding mapping from a given embedding to another embedding. This embedding is then appended to the output layer of the transformer and outputting to the feature encoder. The output of the kernel generator is the representation of the current state of the relation in the context. This representation is then fed to the decoder to output the corresponding corresponding action in the given context.   Experiments are conducted on standard video action recognition benchmarks ( SS - V1&V2, Diving48, and FineGym ), and RSA is shown to outperform the state - of - the - art methods on all three. Ablation studies are also conducted to validate the performance."
SP:2c2530069d5cab485629090243da464d107feadd,"This paper studies the learning dynamics of multilayer neural networks closely related to the mean field theory of finite - width neural networks. In particular, the authors derive a new dynamical equation, the second - order mean field limit, that captures the limiting fluctuation of the network dynamics. The authors then demonstrate through the framework the complex interaction among neurons in this second - orders mean field limits, the stochasticity with cross - layer dependency and the nonlinear time evolution inherent in the limit's fluctuation. A limit theorem is proven to relate quantitatively this limit to the fluctuation realized by large - width networks. The result is then used to show a stability property of gradient descent mean field training."
SP:a3d927854d9d7fd39b8d05a79666810d585d5062,"This paper proposes a method for learning irreversible dynamics with unknown model form from a metriplectic dynamical system. Specifically, energy and entropy are modeled by learning generalized Casimirs that satisfy the first and second laws of thermodynamics and the FFT for closed - form stochastic systems. The method is similar to the way in which NODEs and UDEs are learned, in that it modifies the inductive bias of the learned model form so that it is neither black - box nor penalty - based. However, instead of deriving a model from first principles, as in NODE, the authors instead use a training strategy similar to that of NeurIPS 2021, deriving the energy of a dissipative system by finding a latent space of the dynamics latent space, and then learning a Gaussian process to approximate the latent dynamics. This process is called "" dissipative dissipative calculus "".   The authors provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either "" black box "" or penalty based approaches. In particular, long - term forecasting with energy and dissipative space learned by the process is demonstrated to be more accurate than those learned by penalty based methods."
SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"This paper proposes a sample selection - based algorithm for fair and robust training. The key formulation is the combinatorial optimization problem of unbiased sampling in the presence of data corruption. The authors show that solving this optimization problem is strongly NP - hard and propose a greedy algorithm that is efficient and effective in practice. Experiments show that the proposed algorithm obtains fairness and robustness that are better than or comparable to the state - of - the - art technique, such as FR - Train, both on synthetic and benchmark real datasets. Moreover, the proposed greedy algorithm can be used without modifying the sampling step in batch selection without changing the training algorithm."
SP:991127729bf067fe27fdd7ed360aab39e4df5921,"This paper studies the role of periodic activation functions in Bayesian neural networks ( BNNs ). They show that using such activation functions induces a translation - invariant prior on the network weights that corresponds to the spectral density of the limiting stationary Gaussian process in the function space. They also show that this connection goes beyond sinusoidal ( Fourier ) activations by also covering triangular wave and periodic ReLU activation functions. They leverage this correspondence and show that placing a Student - t distribution on the weights of the hidden layer corresponds to a prior with Matérn covariance structure. Finally, they show in a range of experiments that periodic activation function obtain comparable performance for in - domain data, do not result in overconfident predictions, and enable robust out - of - domain detection."
SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,"This paper proposes a cooperative learning framework, called Play to Grade ( P2G ), that learns a classifier to automatically classify Markov Decision Processes ( MDPs ) of student code. Each student specifies an MDP that describes a dynamics and reward model for a given sequence of steps, and the authors propose to learn a PPO agent that can play the MDP and provide feedback to the classifier. They provide a dataset of 711,274 student submissions to a single assignment with hand - coded bug labels to support future research. They demonstrate that their PPO can generalize to autoregressive agents that can learn to sample trajectories from an input MDP. They also show that their method is compatible with a Gym - Gym compatible environment."
SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"The paper proposes a method for the interpretation of Deep Reinforcement Learning ( DRL ) models by using object representations derived from disentangled latent representations. To represent the object representations, the paper proposes an IMONet and a Mimic Tree Search ( MCRTS ), both of which are based on the IB - MDL objective, to jointly optimize both the fidelity and the simplicity of a mimic tree. The paper also proposes a new objective for the search for the optimal mimic tree, motivated by the information bottleneck principle. Experiments show that the proposed method achieves similar mimic performance to baseline DRL models with significantly fewer nodes than baseline models. It also demonstrates the interpretability of the mimic tree by showing latent traversals, decision rules, causal impacts, and human evaluation results."
SP:84560de78af979354fff83d1370d8675c1e9191f,"The paper proposes a new Bayesian framework, called Gaussian latent information martingale ( GLIM ), for understanding the structure of probabilistic predictions that evolve over time. In doing so, it introduces the Gaussian ( latent ) information flow model, which is a latent process inferred from historical data. The proposed model assumes that there are a set of continuous probability distribution $ \phi$ that follows a pre - defined distribution $ p(y|x)$ over time $ y$.   The authors then propose to estimate the posterior probability path of $ y$ along one of these continuous distribution via a Gaussian process.   To model this latent process, they use a new parameterization of the distribution, which they call a "" Gaussian Latent Information Martingale "" ( GIME ), from the definition of a latent variable : $ \mathbb{R}^2 $ ( where R is the number of latent variables in the GIME, $ \alpha$ is the expected future value of a variable, and $ \eta$ is a function of the expected values of previous variables ). The authors show that this GIME model preserves important properties of probability paths such as the martingales structure and appropriate amount of volatility. They also show that GIME outperforms three popular baseline methods, producing better estimated posterior probability paths distributions measured by three different metrics."
SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"This paper studies the problem of active pure exploration in stochastic bandits, where the goal is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance - specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply, which may be instrumental in the design of efficient learning algorithms. The authors devise Frank - Wolfe - based sampling ( FW - SAM ), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. The algorithm is computationally efficient as, to learn and track the optimal proportion of arm draw, it relies on a single iteration of FW algorithm applied to the lower - bound optimization problem.   The authors apply FWS to various pure exploration tasks, including best arm identification in unstructured bandits as predicted by [ 13 ], and linear, and Lipschitz bandits. Despite its simplicity, FWS is competitive compared to state - of - art algorithms."
SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"This paper proposes LADDER, a sample - efficient Bayesian optimization method for combinatorial spaces. The method is based on a novel structure - coupled kernel, which explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. The key idea is to define a Gaussian process based surrogate model that combines the complementary strengths of latent space representations with rich information about decoded outputs using structured kernels. Experiments are conducted on real - world benchmarks to show the efficacy of the proposed method. The BO performance is better or similar to state - of - the - art methods and significantly better than the Naïve latent space BO method."
SP:37adabdc6615c5199a481553c8ccc06d57363614,"This paper studies the regret minimization problem in finite horizon Markov Decision Processes ( MDPs ) with linear reward function. It first derive a necessary condition on the representation, called universally spanning optimal features ( UN ISOFT ), to achieve constant regret in any MDP. This condition generalizes to linear contextual bandits and it requires that the features observed along trajectories generated by the optimal actions provide information on the whole feature space. The authors then demonstrate that this condition is sufficient for these classes of problems by deriving a constant regret bound for two optimistic algorithms, namely LSVI - UCB and ELEANOR. Finally, they propose an algorithm for representation selection and prove that it achieves constant regret when one of the given representations, or a suitable combination of them, satisfies the UNISOFT condition."
SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"This paper proposes a differentiable physics - based neural network model for learning dynamics of rigid body systems with different coefficients of friction and elasticity. Differentiating through convex optimization, the differentiable model is shown to learn coefficients of restitution and friction associated with two differentiable body systems in 2D and 3D. The learned dynamics can then be used for downstream gradient - based optimization tasks, such as planning and control. The proposed model extends the applicability of Lagrangian / Hamiltonianinspired neural networks to enable the learning of hybrid dynamics in rigid body system and offer interpretability about system and contact properties."
SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"This paper studies the Lipschitz constant of a standard deep neural network ( NN ) in terms of its sensitivity to perturbations to the training signal, in order to support the hypothesis of the "" Benevolent Training Hypothesis "" ( BTH ), which states that the complexity of a neural network can be directly measured by its training dynamics, i.e., how quickly the network is trained to fit the training set. The paper finds that networks that have a small Lipschitz constant will have a tighter bias trajectory, and those with a larger constant will tend to deviate further from the training data, as measured by the variance of their perturbed signal. In addition, networks with a low dropout bias ( trained slowly and uniformly ) tend to have a bounded complexity.   The paper also shows that networks whose 1st layer bias is large ( but trained with dropout ) are less likely to generalize poorly. Finally, the paper shows that steady training with Dropout implies a training - and - datadifferent bound that grows poly - logarithmically with the number of parameters."
SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"The paper considers PAC learning of halfspaces in the Massart noise model, and proposes a new algorithm based on Forster decompositions to obtain a sample complexity bound that does not depend on the bit complexity of the examples, as was the case in the previous algorithms.   The main contribution of the paper is the main application of [ DGT19 ] as the main result : it is shown that there exists an algorithm with sample complexity that is polynomial in $ n$ samples, independent of $ b$ in terms of bit complexity, that achieves a misclassification error of $ \�+$ in time in poly(n, b ), as opposed to $ n/\sqrt(b)$ where $ b=n^2 $ and $ d=1/2 $ are information - theoretically necessary in order to learn well. The main idea of the algorithm is to make use of a Forster transform that decomposes a distribution into a disjoint mixture of few distributions that satisfy certain anticoncentration properties, such that there exist exist for a given distribution a set of Forster transforms that can efficiently be computed. The proof of this result is given by the following :    1. Given a homogeneous halfspace V, a classifier $ h : V - sample(x, y ) = \tilde{O}(\log(V, Y ) ) with probability at least 1 / \log(log(Y ) ), and a sample size $ d$, compute a partial classifier h : R(x,Y ) = O(k, d ) $ where $ k$ is the size of the outlier in the classifier's output distribution and $ D$ the accuracy. 2. Given an example set $ x_0 $ \in [ \mathbb{Rw}(x_0, x_\in V_\outlister)$ and some examples $ y_1 $, compute $ h$-contraction on the examples $ \text{K}$ such that $ k=0.1 $ with probability $ \delta(k)$, $ k-1 $ is close enough to the ground truth. 3. Given all of these points $ \mathbf{X}$ and their corresponding probabilities, compute an outlier $ \gamma_t$ that is at most $ t$ times smaller than some threshold value $ t$. 4. For all $ V$ and $ X_0$ that have high probability, output $ h$.   In order to make the algorithm work, the first two terms need to be defined in a different way : a first - order classifier is defined, and the second term needs to be a function that maps a fraction of the input to a lower - degree classifier and output on that fraction. In particular, the key of the first part of the second part is to define an error of order of magnitude for $ k$. The second part depends on two factors : the accuracy and the distance between the output of h and the true classifier. The second factor depends on the probability of the remaining fraction of inputs for which h is a “ Partial Classifier ”, which is defined as a function mapping some input values to a probability of $ 1/\infty$ ( or equivalently, the error of the full classifier ) of size $ \frac1}{\text{1/\delta}$ to some other input values. The running time of this algorithm depends on how many samples it takes to compute the partial classifiers output, which has polynomially large sample size. The paper shows that this algorithm is equivalent to a naive extension of [ KD19 ], called KD19, to [ KD20 ], based on the fact that KD19 ’s guarantees are qualitatively similar to that of KD19. Finally, it shows how to adapt the algorithm to the setting where the classifiers are defined using the   characteristics of the “ outlier ” defined in [ KD18 ], and derive sample complexity guarantees similar to [ CD20 ]."
SP:e5229305af00067ae2dbabd903e585964aec8928,"This paper proposes GRABNEL1, a new method to perform black - box adversarial attacks on graph neural networks ( GNNs ). The key advantage of this approach is that it is based on Bayesian optimisation, i.e., it doesn't require access to the internal state of the victim model, which makes the proposed method query efficient. The proposed method is also adaptable to various attack modes, e.g. deleting or rewiring edges, or injecting nodes, as well as hybrid attacks such as node injection.    The authors validate the effectiveness and flexibility of their approach on a variety of graph datasets, and provide interpretable adversarial examples to shed light on the adversarial robustness of graph classification models. They also analyze the topological properties of the obtained adversarial samples to provide insights into the connection between the graph topology change and the model robustness. An open - source implementation of their method is available."
SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"This paper studies the problem of online label shift adaptation. In this setting, the test - time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. The authors propose an adaptation algorithm inspired by classical online learning techniques such as Follow The Leader ( FTH ) and Online Gradient Descent ( OGD ) and derive their regret bounds. They empirically verify their findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of label shift scenarios."
SP:806515ae07fb1c9d02773592005d53d4158ef102,"This paper proposes a nonparametric method for detecting and localizing gradual changes ( CPD ). The proposed method requires no prior domain knowledge, and it offers theoretical guarantees on both detection ( false positive rate, power ) and localization ( consistency ). Different from most machine learning tasks, there are currently no benchmarking datasets with human annotations for gradual CPD. Thus, this paper considers the applications introduced in Section 1, and compare its result with known external events and/or other CPD estimators."
SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"The paper proposes an online blind source separation ( BSS ) algorithm based on ICA that mimics a biological neural network ( NN ), where data is streamed in one source at a time and the NN computes the sources on the fly, without storing any data in memory. The paper also proposes a novel objective function for ICA called LSTM that can be used to derive a biologically plausible NN, from which the paper derives both the neural architecture and the synaptic learning rules. Interestingly, the paper's algorithm relies on modulating synaptic plasticity by the total activity of the output neurons. In the brain, this could be accomplished by neuromodulators, extracellular calcium, local field potential, or nitric oxide.   The online algorithm maps the data onto a single - layer NN that can separate independent sources without pre - processing. The synaptic weights in our NN are updated using local learning rules, extending more conventional Hebbian learning rules by a time - varying modulating factor, which is a function of the total output activity."
SP:22f8b517a3df65144412938f5891c463d7bae0ab,"This paper studies the role of recurrent neural networks ( RNNs ) in understanding the space of solutions in neuroscience. The authors first study a simple two - neuron network trained on a task that leads to multiple solutions. They trace the nature of the final solution back to the network ’s initial connectivity and identify discrete dynamical regimes that underlie this diversity. They then examine three neuroscience - inspired tasks : Delayed discrimination, Interval discrimination, and Time reproduction. For each task, they find a rich set of solutions for each layer of the network. One layer of variability can be found directly in the neural activity of the networks. An additional layer is uncovered by testing the trained networks ’ ability to extrapolate, as a perturbation to a system often reveals hidden structure. Furthermore, extrapolation patterns are connected to specific dynamical objects and effective algorithms found by the networks by relating them to specific objects.   The paper also provides a tool to derive the reduced dynamics of networks by generating a compact directed graph describing the essence of the dynamics."
SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"This paper proposes a method, called Arbitrary Conditioning with Energy ( ACE ), to estimate the density of p(xu | xo ) over all possible subsets of unobserved features xu and observed features xo using an energy function. The motivation is to avoid unnecessary bias and complexity — we specify densities with a highly expressive energy function and reduce the problem to only learning one - dimensional conditionals. This results in an approach that is both simpler and higher - performing than prior methods, such as energy - based methods. Empirically, the authors show that ACE achieves state - of - the - art performance for arbitrary conditional likelihood estimation and data imputation on standard benchmarks, e.g., CIFAR10, MNIST, Fashion - MNIST."
SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"This paper proposes a new adaptive weighted loss LUDL for Single Image Super - Resolution ( SISR ). Specifically, variance estimation is introduced into SIsR so that the high - resolution images ( mean ) and their corresponding uncertainty ( variance ) can be learned simultaneously. Also, the uncertainty estimation is modeled under Bayesian framework, and sparsity prior is used for regularizing the PSNR function.   Textured and edge pixels with high uncertainty are then prioritized according to their visual quality. For the first time, this uncertainty - driven loss achieves better PSNR performance than traditional MSE or L1 loss."
SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"The paper proposes the first general PAC - Bayesian bound on the risk of adversarial robustness to adversarial perturbations that is upper bounded by a majority vote. The risk is defined as the average of the perturbation risk over all the hypotheses that a majority of the voters could adopt in a class of hypotheses. The majority vote is defined to be the set of all the possible hypotheses for a given classifier. Then the risk is estimated as the difference of the average risk of each of the hypotheses over the majority of voters.    The main contribution of this paper is to formalize the risk estimate as a function of the majority vote in the framework of the PACBayesian framework. The paper shows that this new risk estimate can be used to derive general bounds that are valid for any kind of attacks, that are tight thanks to the PAC -Bayesian   framework, and that can be directly minimized during the learning phase to obtain a robust model on different attacks at test time. The authors also empirically show the soundness of their bound in the case of differentiable decision trees."
SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,"This paper proposes a probabilistic entity representation model ( PERM ) for logical reasoning over Knowledge Graphs ( KGs ). Specifically, the model is based on a multivariate Gaussian density to capture the semantic position of the entities and a smooth decision boundary. Perm also defines the closed logical operations of projection, intersection, and union that can be aggregated using an end - to - end objective function. On the logical query reasoning problem, the proposed PERM significantly outperforms the state - of - the - art methods on various public benchmark KGs and drug recommendation on the DRKG dataset. Furthermore, the performance is evaluated on a COVID-19 drug case study."
SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"Hyperparameter optimization ( HPO ) is a popular technique for few - shot meta - learning, but it is generally impractical for tasks with long horizons due to memory scaling and gradient degradation issues. A common workaround is to learn hyperparameters online, but this introduces greediness which comes with a significant performance drop. This paper proposes forward - mode differentiation with sharing ( FDS ), a simple and efficient algorithm which tackles the memory scaling issues with forward mode differentiation, and the gradient degradation issue by sharing hyperparameter that are contiguous in time. The authors provide theoretical guarantees about the noise reduction properties of their algorithm, and demonstrate its efficiency empirically by differentiating through $ \nabla_t $ steps of unrolled optimization. They consider large hyper parameter search ranges on CIFAR-10 where they significantly outperform greedy gradient - based alternatives, while achieving +20x speedup compared to the state - of - the - art black - box methods."
SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"This paper proposes a neural - symbolic dual - system model to improve consistency and robustness of neural sequence models. The model consists of two modules : a logical system ( called System 2 ) and an intuitive system ( "" System 1 "" ). The logical system is responsible for accepting or rejecting generations from System 1. The intuitive system, on the other hand, acts as a decoder to generate sequences that are acceptable to the logical system. The intuition of the paper is that humans operate in two brains : one that is fast and learns patterns from data, while the other one is more deliberative and logical. The paper proposes to mediate between the two by learning a neural system that first generates sequences from the data, and then reasoning over them using a symbolic system that can accept or reject the generated sequences. The neural system in the first brain is called System 1, and the symbolic system, called 2, is the "" logical system "".    The proposed model is tested on two tasks : robust story generation ( robust CLUTR ) and robust instruction following ( gSCAN grounded compositional challenge ). It is shown that the proposed model generalizes better than prior work on both tasks. The main contributions are :   1. This paper proposes an efficient, training - free, dual process model of System 2. This requires no additional training or fine - tuning of the neural system. 2. It improves the robustness and consistency of System 1 by integrating the logical reasoning."
SP:d77d046095e4c8336c0c76ac48cb046923230753,"This paper proposes a method for off - policy evaluation ( OPE ) for continuous treatment settings, such as personalized dose - finding, where one aims to estimate the mean outcome under a new treatment decision rule using historical data generated by a different decision rule. Most existing works on OPE for continuous settings focus on kernel - based methods. To handle continuous settings, this paper proposes an estimation method for OPE using deep jump learning ( DJL ). The key ingredient of the method lies in adaptively discretizing the treatment space using deep discretization, by leveraging deep learning and multiscale change point detection. This allows the DJL method to apply existing OPE methods in discrete treatments to handle continuous treatments. The method is further justified by theoretical results, simulations, and a real application to Warfarin Dosing."
SP:4d085e57286fdd36143108a002d16914222c239a,"The paper proposes a variational inference framework for Markov jump processes ( MJPs ) for continuous - time hybrid systems, where discrete - event systems can be described as switching dynamical systems that remain in distinct regimes for extended periods of time.   The model is based on a Markov - jump process modulating a subordinated diffusion process. The authors provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are computationally intractable. Therefore, they propose a new variational method that combines a Gaussian process approximation on the diffusion level with posterior inference for the MJP. The key assumption is that the true discrete posterior is peaked at any z^\infty, which allows for a straightforward mixture of GPs to be used as approximation. They have evaluated their framework on various benchmark tasks including real - world biological data and demonstrated its ability to faithfully reconstruct complex latent dynamics and to learn the unknown system parameters, in particular in applications to meta - stable systems. While they implemented parameter learning via point estimates, they aim to extend this to a fully Bayesian framework in the future, enabling the integration of prior knowledge and the associated uncertainty about the system at hand."
SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"This paper studies the effect of the spectrum of the nonlinear transformation of the sensing matrix A on the performance of the expectation propagation algorithm GLM - EP, i.e., the generalized linear inverse problem of recovering an x - signal from a y - signal matrix A under the influence of A nonlinear mapping f, where A is a known linear mapping. The setting is suitable for many signal processing problems, e.g. compressed sensing and phase retrieval.   The paper provides a measure of the “ spikiness ” of A based on the Lorenz partial order and shows that it can be used to quantify the impact of certain quantities f, namely, the monotonicity of the function f, on the class of functions on which A depends on whether it helps or hurts the recovery of functions x, y under the condition that f(x ) is monotonically different from the mapping A. It is shown that in phase retrieval problems, for instance, phase - retrieval matrices with spiky spectrums are better for EP, while in 1 - bit compressed sensing problems, less spiky ( flatter ) spectrums offer better recoveries. The results unify and substantially generalize the existing results that compare sub -Gaussian and orthogonal matrices, and provide a platform toward designing optimal sensing systems."
SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,"This paper addresses the problem of generalized zero - shot learning ( GZSL ) with new semantic information, e.g., new categories and confusion between seen and unseen categories by progressively improving cross - domain transferability and category discriminability. The proposed approach, named Dual Progressive Prototype Network ( DPPN ) constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively : 1 ) for attribute prototypes : an attribute locator maps attribute - related features to target visual regions and updates corresponding attribute prototypes to progressively explore both attribute - region correspondence and category discrimination. 2 ) for category prototypes : the category representations are concatenated by concatenating image representations of related attributes and vice versa, and 3 ) for multi - view representations, the visual representations are uniformly distributed across multiple images to progressively repel visual representations from different categories. Experiments on four benchmarks prove that DPPP helps significantly in alleviating the domain shift problem.   The main contributions of this paper are as follows :   1 ) A novel dual progressive prototype network that constructs prototypes for both attributes and category : instead of using a fixed set of prototypes for attributes, DPPPN obtains prototypes by alternating between the current state - of - the - art and a target image, which is then used to progressively adjust the corresponding prototypes to capture the target image. This method is referred to as “ progressive prototypical learning ” ( PPO ). The second contribution is a method of combining PPO and PPO in order to progressively expand the number of prototypical instances of each type. The visual representations of PPO are learned in an unifed framework. This is done by projecting the proposed visual representations into multiple images and then combining them with the corresponding perceptual representations of different categories to progressively distill certain visual representations in each image. 3 ) An alternation updating strategy is also proposed to dynamically adjust the progressive attribute prototypes according to the target images, which results in more accurate attribute localization. 4 ) A new approach is proposed to project category prototypes into multiple spaces to progressively enhance category discrimination by distilling the corresponding visual representations."
SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"This paper presents an end - to - end deep learning approach for removing defocus blur from a single image, so as to have an all - in - focus image for consequent vision tasks. The proposed method first proposes a pixel - wise Gaussian kernel mixture ( GKM ) model for representing spatially variant defocus blurring kernels in an efficient linear parametric form, with higher accuracy than existing models. Then, a deep neural network ( DNN ) is developed by unrolling a fixed - point iteration of the GkM - based deblurring, using a scale - recurrent implementation, with a module for estimating the mixing coefficients. Extensive experiments show that the proposed method significantly outperforms existing defocus - blur - removal methods, with state - of - the - art performance."
SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"This paper proposes a self - supervised video representation learning ( SSVRL ) framework that simultaneously captures both storage and computation efficiency. In contrast to existing works, which only consider one of the two objectives simultaneously, the authors propose to simultaneously capture both objectives in a single model by exploiting the information from RGB and contrastive loss. Motion vectors are decomposed on - the - fly to capture the mutual information between RGB frames and motion vectors, and cross guidance is used to enhance the representation ability of the motion vectors. The proposed method is evaluated on two downstream tasks : action recognition and action retrieval, and it is shown to outperform its competitors on both tasks by a large margin."
SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"This paper studies the problem of overconfidence of Bayesian neural networks with ReLU neural networks ( BNNs ) with finite number of ReLU features. The authors first show that Gaussian processes with infinite ReLU feature converge to a GP with a cubically different variance that does not scale cubically with the distance from the training data. They then extend this result to finite BNN with infinite number of features to fix asymptotically overconfidentness. The way they do so is to replace the classic cubic spline kernel in the GP with the one obtained by solving the cubic - spline problem. They show that the resulting model is maximally uncertain far from the data while the BNN’s predictive power is unaffected near the data. Finally, they empirically confirm the analysis and show effectiveness in the non -asymptotic regime. They extend the result to the binary classification setting."
SP:e77276f61626e896f6a985296f1d832129242cdf,"This paper considers the problem of finding the best estimate of a causal quantity among a set of known estimators of the effect of perturbed variables. The problem is formulated using the multi - arm bandit framework, in which the goal is to identify the estimate with the lowest asymptotic variance in as few samples as possible. The paper makes use of the best - arm - identification algorithms of LUCB and SEL to adapt them to the setting of this paper. The main contributions include :   1. Introducing the idea of finite - sample confidence intervals for estimating the asymetrical variance of estimators if and only if the estimator satisfies certain conditions. This is done by assuming a class of consistent estimators $ \sum_1/\sqrt{n}$, where $ n \leq n$ is a population of known influence functions, and using the leading constant error rate $ \gamma_\theta(n)$ to define a confidence interval that holds over all possible estimators. This assumption is made for the purpose of estimating the estimate of the causal effect function $ \tau$, which has a rate of convergence of $ O(n−1/2 ) $.   2. Achieving such a result is challenging because of the high - dimensional nature of the estimators and the fact that they are often derived from complex functions such as the influence function of a neural network, which may not have classical Donsker smoothness, and therefore may not converge as quickly as one would like under normal assumptions. 3. Providing a lower bound on the error rate of such estimators, the paper proposes to use the asymptic variance as a metric to evaluate the resulting estimate. 4. Experiments are given on simulated data to demonstrate the effectiveness of the method, and sample complexity upper bounds are also provided."
SP:471361588bfc6c6033631509d1e43e77fd9721ce,"This paper proposes ErrorCompensatedX, which utilize the compression error from the last two steps of the original ErrorCompressed algorithm in order to fully compensate the history compression error. This method is used for variance reduced optimization algorithms such as Momentum SGD and SGD, and it is shown to achieve the same asymptotic convergence rate as the original error - compensation - aware algorithm. The paper also provides a unified theoretical analysis framework for this class of variance reduced algorithms, with or without error compensation. Numerical experiments are implemented to show the convergence and its better performance compared to other implementations."
SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"This paper presents a method for generating multi - granularity explainable explanations of graph neural networks ( GNNs ). The main motivation is that existing GNN explainability methods typically only consider one aspect of the input graph ( e.g., local explainability ) and fail to consider the global aspect ( i.e., the fraction of the graph that influenced the decision - making process ). This dichotomy limits the flexibility and effectiveness of existing explainers, which can not account for the globally important patterns that might be trivial in the local context. To address this limitation, this paper proposes a method that first pre - trains the GNN with a class - aware module and distills the class knowledge to the class - level knowledge via contrastive learning, and then fine - tunes the model on a specific instance to reproduce the global and local examples with high fidelity. The proposed method, ReFine, achieves state - of - the - art performance on various datasets w.r.t. predictive accuracy on explaining graph neural network. Quantitative and qualitative results verify the multi -granularity explainability of ReFine."
SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"Graph Neural Networks ( GNNs ) generate graphs that predict a class of a given graph, and are then used to generate counterfactual explanations that are robust to noise. It is important to generate explanations that align well with human intuition and do not overfit the noise. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction, independently optimizing the correlation for a single input can easily overfit noise. In this paper, the authors propose a novel method to generate robust explanations by explicitly modelling the common decision logic of a Graph Neural Network on similar input graphs. The authors develop a novel way to extract decision boundaries from the given GNN model to formulate an intuitive and effective counterfactually loss function, and optimize this loss to train a neural network to produce explanations. Since the decision boundaries are shared by multiple samples of the same predicted class, explanations produced by our method are intuitively robust to the noise, and the explanations generated by our methods are also robust to background noise. Extensive experiments on many public datasets demonstrate the superior performance of our method, last, conducts comprehensive experimental study to compare our method with the state - of - the - art methods on fidelity, robustness, accuracy and efficiency. All the results solidly demonstrate the advantage of our approach.  "
SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"This paper presents a method for voice - to - speech ( VCTK ) transfer via self - supervision and adversarial feedback. The method relies on self - supervised representation learning to learn a style and content encoder that can effectively transfer voice style through the information bottleneck. The discriminator is decomposed into a content discriminator and a style discriminator that are trained using supervised learning. The style encoder is trained using a loss with a loss proportional to the loss of content information. The adversarial network is trained based on a cross - entropy loss that penalizes the loss on the discriminator.   The method is evaluated on the VCTk dataset and compared against several baselines including VCTConvExpert, V - ConvoConv, and V - Transformer. The experimental results show the superiority of the proposed method over the baselines in terms of disentanglement and transfer performance."
SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,This paper proposes a voxel - to - BEV ( V2B ) method for 3D single tracking on sparse point clouds. It consists of a shape - aware feature learning network and a vbox - to BEV target localization network. The shape - based feature learning uses a template embedding to embed the template into the potential target and then generates a dense 3D shape to characterize the shape information of the possible target. The vbox-to-BEV localization network regresses the target ’s 2D center and the z - axis center from the dense bird’s eye view ( BEV ) feature map in an anchor - free manner. This paper has extensive experiments on the KITTI and nuScenes datasets and shows that it outperforms the current state - of - the - art methods.
SP:8b788c78680a54c453a04f4551436763ee57585e,"This paper proposes a method for learning positional encodings in Transformer based on learnable Fourier features. The idea is that instead of encoding each position in the sequence as a vector, the Transformer encoder instead encodes it as a learnable feature mapping, modulated by a multi - layer perceptron. The perceptron encodes the multi - dimensional position matrix as a set of learnable features, such that the inner products of the positional encoding approximate Euclidean distances. The method is evaluated on image generation, object detection, and classification tasks and shows improvements over existing positional encoding methods."
SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"This paper considers the problem of learning the causal structure of a system from observational data in the presence of latent variables and selection bias. The main motivation is to learn the structure efficiently and recursively, as this allows us to reduce both the number of required conditional independence ( CI ) tests and the size of the conditioning sets. The key idea of the proposed method is to remove one type of variable at each iteration, such that the total number of variables in the system becomes smaller as the order of the graph becomes smaller over the iterations. Also, the authors propose a recursive technique to handle latent and selection variables. The proposed method, called L - MARVEL, is aimed at developing a constraint - based method that is sound and complete.   The authors first give a lower bound on the complexity of the problem, which they claim to be the tightest in the literature. Then, they provide lower bounds for the total amount of CI tests that are required by any constraint based method. Finally, they compare the performance of the method with several state - of - the - art approaches on both synthetic and real - world structures."
SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,"This paper proposes a Thompson Sampling Batch variant for stochastic multi - arm bandit and linear contextual bandit with finitely many arms. The proposed method iteratively iterates through a fixed number of iterations, called Batch, sampling samples from each arm using a policy that selects an arm at each iteration. Each arm is sampled from a fixed batch of sampled samples so that the total amount of samples is proportional to the sum of the regret upperbound and lowerbound. The sampling process is iterated in a way that ensures that each arm sampled in each iteration is sampled enough times to achieve the upper bound on the total number of sampled arms. In order to ensure that the sampling process does not overfit, the authors propose a dynamic allocation mechanism that changes the batch size and batch length based on an online regret estimate. Experiments on synthetic and real data show that the proposed method achieves the same upper and lower bound as a fully sequential approach, but with exponentially fewer number of interactions."
SP:653a519e3c799c25e0d0b4240322642040b121a3,"This paper studies domain adaptation ( DA ) and domain generalization ( DG ) by analyzing the trade - off between invariant classifiers and learning domain - specific representations ( DI ). In particular, it considers two settings of DA, MSDA and DG, where each class is divided into target and source domains, and two kinds of representations are learned : general DI representation ( GDI ) which works on all source domains and compressed DI representation motivated from reducing inter - domain representation discrepancy, and derive a lower bound on the target general loss. In addition, it conducts experiments on Colored MNIST dataset and real dataset to illustrate their theoretical claims and compare against existing works."
SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"This paper proposes a lightweight image super - resolution ( SR ) method based on structured sparsity learning. This method consists of two components : ( 1 ) a weight normalization layer and ( 2 ) a sparsity structure alignment penalty term. The weight normalisation layer acts as a weight estimator for the sparsity, while the L2 regularization applies to the scale parameters. The sparsity penalty term is used to align the location of the soft mask matrices when computing the distance between the output and input of the network. The paper shows that the proposed weight normalizer leads to a network with smaller model size and FLOPs compared to previous SR methods ( SOTA, ICLR, IIT - FSN ). The second component is a network pruning layer that applies the L1 regularization and the distance regularizer to the parameters of the model. This paper shows how the two components fit together and performs an analysis of the final network architecture. Finally, the paper compares the proposed method with other SR methods on CIFAR10/100 and Tiny Imagenet to establish the superiority of the proposed approach."
SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"This paper introduces Episodic Multi - agent Reinforcement Learning ( EMC ), a novel method for efficient exploration in deep cooperative multi - agent reinforcement learning ( MARL ). The authors leverage an insight of popular factorized MARL algorithms that the “induced ” individual Q - values, i.e., the individual utility functions used for local execution, are the embeddings of local actionobservation histories, and can capture the interaction between agents due to reward backpropagation during centralized training. Therefore, they use prediction errors of individual Q-values as intrinsic rewards for coordinated exploration and utilize episodic memory to exploit explored informative experience to boost policy training. The didactic examples and the empirical results in the StarCraft II benchmark demonstrate its significant outperformance over state - of - the - art MARL baselines on challenging tasks."
SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"The paper studies the problem of list - decodable linear regression, where an adversary can corrupt a majority of the examples. The authors prove a statistical query ( SQ ) lower bound of d for this problem, which they claim matches the performance of previously developed algorithms, and provides evidence that current upper bounds for this task are nearly the best possible.   The SQ model was introduced by Kearns [ Kea98 ] in the context of supervised learning as a natural restriction of the PAC model [ Val84 ]. Subsequently, the SQ model has been extensively studied in a plethora of contexts, capturing a range of known supervised learning algorithms, including spectral techniques, moment and tensor methods, local search, and many others."
SP:7b258252a9063514348f5fa8d9c85afd85748747,"This paper proposes a latent hybridisation model ( LHM ) that integrates a system of expert - designed ODEs with machine - learned Neural ODE to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. The authors evaluated LHM on synthetic data as well as real - world intensive care data of COVID-19 patients. LHM consistently outperforms previous works, especially when few training samples are available such as at the beginning of the pandemic. The machine learning component provides links between the expert variables and the clinical measurements, the underlying pharmacological model improves sample efficiency, and the expert variable provides additional insights to the clinicians."
SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,"This paper studies the problem of fine - tuning - based meta - learning based on representation learning, in which a task - specific representation is frozen ( i.e., learned only for that task ), and compares it to prior work based on “ frozen representation ” or similar methods ( e.g., MAML ). The authors provide a theoretical framework for analyzing such methods, assuming that there are approximately equal number of tasks with approximately the same representation. They show risk - upperbound guarantees for finetuning - based methods that ensure that the learned representation generalizes to all the remaining tasks in the set. In contrast, they show that such methods fail to learn one representation for all the tasks ( or a frozen representation with no other information ) under the weaker assumption that all tasks share the same structure. They also provide risk bounds on predictors for logistic regression, neural network, and linear hard case, which extend to non - linear cases."
SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"This paper presents Grammar - Based Grounded Lexicon Learning ( G2L2 ), a method to learn a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. The method relies on a lexicalist approach, i.e., it first looks up the lexicon entries associated with each token in a given sentence, then derives the meaning of the sentence as an executable neuro - symbolic program by composing lexical meanings based on syntax. The recovered meaning programs can then be executed on grounded inputs. To facilitate learning in an exponentially growing compositional space, the model introduces a joint parsing and expected execution algorithm, which does local marginalization over derivations to reduce the training time. The authors evaluate the model on two domains : visual reasoning and language - driven navigation. Results show that G 2L2 can generalize from small amounts of data to novel compositions of words."
SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,"This paper proposes FedSN - LITE, a stochastic Newton algorithm for convex quasi - self - concordant objectives, where each machine has access to a set of independent Hessian - vector products of the population objective, and can compute a gradient for each vector. The objective is to maximize the sum of the variance of the Hessian of the objective over all possible values of the variables in the set, under the condition that the objective is a quasi - selfish function ( Bach, 2010 ). The paper analyzes the convergence guarantees of FedSN-LITE, that is, the main guarantees of the main algorithm are Theorem 1, Theorem 2, and Proposition 3. In Section 4, the paper shows how, in some regimes, the proposed method may improve upon the rates of previous first - order methods, FedAC ( Yuan, 2020 ), such as FedAC. In Sections 5 - 6, it shows how to reduce the required communication rounds of the proposed algorithm, by showing that FedSN LITE ( Algorithm 6 ), by leveraging parallel methods, can significantly reduce the number of communication rounds. Finally, in Section 5, it compares a practical version of the method, FEDSN - SNLite, against the other methods, showing how to significantly reduce communication compared to other first - orders methods."
SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"This paper proposes a new Chamfer Distance ( CD ) based on Earth Mover's Distance ( EMD ) and Point Cloud Similarity ( PCS ) metrics to measure the similarity between two point sets. The authors claim that EMD and CD are less sensitive to mismatched local densities and more sensitive to the overall structure of the point set compared to EMD, and that the proposed metric, called Density - aware Chamfer' Distance ( DCD ), overcomes these issues. They also propose a new DCD as the training loss function, which outperforms the same model trained with CD / EMD / PCS on all three metrics. Finally, a new point discriminator module is proposed, which achieves noticeable improvements under the new metric."
SP:e4b302009520770814ff2c096020b779a9fc38fe,"This paper studies the problem of knowledge distillation ( KD ), which is a popular technique to train a small student network to resemble a larger teacher model, such as an ensemble of networks, in general improving student generalization. The authors find that KD does not typically work as well as it is commonly understood : there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases where the student has the capacity to perfectly match the teacher. They identify difficulties in optimization as a key reason for why the student is unable to match the teachers. The paper also shows how the details of the dataset used for KD play a role in how closely the student matches the teacher — and that more closely matching the teacher paradoxically does not always lead to better student generalisation."
SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"This paper introduces a coreset for k - trees, a class of decision trees based on 2D matrices, called k - coresets, that can be used to build a k - tree that optimally matches the loss function of an optimal k - decision tree. More precisely, given an error parameter $ \epsilon$, the coreset $ C$ of a given matrix D is the sum of squared differences between every label in D and its assigned label by t. The authors provide the first algorithm that outputs such a ( k, \�)-coreset for every such matrix D, and its construction takes O(Nk ) time. The coreset is polynomial in k log(N)/ε and its size $ C |C |, which can be computed in O(klog(nm)/eu ) times.   This paper forge a link between decision trees from machine learning – to partition trees in computational geometry, as it is doing in the case of graph convolutional networks ( GRUs ). This is by forging a link from the decision trees used in graph convolutions ( e.g., TreeGlow ) to the partitioning matrices in GRUs. It is also doing this in case of 2D signal, where each row in the graph is composed of axis - parallel rectangles, and each axis is assigned a real label. Experiments on sklearn and lightGBM show that applying the k - coreset on real - world data-sets boosts the computation time of random forests and their parameter tuning by up to x10, while keeping similar accuracy."
SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"This paper studies the problem of Top - M identification, i.e. the identification of the arms with the largest means under a fixed error rate $ \delta$ under a misspecified linear bandit model, under the assumption that the model does not deviate significantly from linearity. This is motivated by practical applications in medicine and recommendation systems, where linear models are popular due to their simplicity and the existence of efficient algorithms to implement them. The paper first provides a lower bound on the sample complexity of any $ \tilde\gamma$-correct algorithm for the general $ \top^2 $-confidence problem based on the observation that it is always possible to find an instance where the regret is necessarily linear, under certain assumptions. Then, this paper describes a new algorithm for this setting, which is both practical and adapts to the setting, and derives an upper bound to its sample complexity that matches the lower bound that matches that of an existing lower bound for the best - arm - identification algorithm. Finally, the paper evaluates its algorithm on both synthetic and real - world data, showing competitive performance with respect to existing baselines."
SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"This paper proposes a new self - supervised GNN method called Disentangled Graph Contrastive Learning ( DGCL ) to learn disentangled graph representations with self - supervision. The main idea is to first identify the latent factors of the input graph and derive its factorized representations, which are then used to learn a feature extractor and a message passing layer. Each of these two feature extractors is used to extract a latent representation pertinent to a specific latent factor of the graph. Then, a contrastive learning objective is added to each latent representation to encourage the learned representations to independently capture the expressive information from different latent factors. The experiments are conducted on both synthetic and real - world datasets and demonstrate the superiority of DGCL over existing SOTA GNN methods."
SP:0a7edbbdabab11273689c40c517001eb46491113,"This paper proposes a statistical simulation to make assessment on corruption robustness. It looks at this problem from a hypothesis testing ( false positive / false negative ) and from a certification ( complete / soundness ) point of view. The proposed procedure is scalable, efficient, complete and comes with theoretical guarantees on the lack of soundness.   The robustness assessment is cast as a statistical hypothesis test : the network is deemed as locally robust if the estimated probability of failure is lower than a critical level. The procedure is based on an Importance Splitting simulation generating samples of rare events. The theoretical guarantees are nonasymptotic w.r.t. sample size. Experiments tackling large scale networks outline the efficiency of the method making a low number of calls to the network function."
SP:c1db485ff1ff9573daa421e167225654babb55ac,"This paper proposes Polynomial Neural Networks ( CoPE ) for conditional data generation. CoPE expresses a polynomial expansion of two input variables, i.e., a noise vector and a conditional variable, and the authors show how SPADE and sBN can be considered as special forms of this two - variable polynomials expansion. Notably, CoPE can be augmented to accept an arbitrary number of conditional variables as inputs. The empirical evaluation shows that CoPE generates realistic images in five diverse tasks, including inverse problems and class - conditional generation. Inverse problems, such as super - resolution, can benefit from the proposed framework."
SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,"This paper proposes a new approach to compute the Maximum Mean Discrepancy ( MMD ) statistic of a neural network by adapting a connection between the neural tangent kernel ( NTK ) and MMD. The motivation is to address the challenge of memory and computational complexity of computing MMD with the long - standing challenge of online implementation of MMD, which is essential for online implementation to assimilating new samples. The proposed approach is based on adapting existing theories for kernel MMD and neural network MMD by developing a computationally efficient and memory - efficient approach for computing the MMD statistic and performing NTK based two - sample tests. However, the paper is aware of the limitations of NTK in explaining deep network optimization and expressiveness power, and so on. The last section of the paper discusses limitations and extensions in the last section to improve the proposed approach."
SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"The paper proposes a new objective called CD - VAE ( Class - disentangled variational autoencoder + classifier ) to learn the minimum information required by a neural net D(· ) from an image x to accurately predict its class. The objective consists in a trade - off between reconstructing x by G(x ) and classifying x by D(x − G( x ), where the former competes with the latter in decomposing x so the latter retains only necessary information for classification in x − G('x ). They apply it to both clean and adversarial images and discover that the perturbations generated by adversarial attacks mainly lie in the class - dependent part x - G's rather than the clean image. The decomposition results also provide novel interpretations to classification and attack models. Inspired by these observations,   they propose to conduct adversarial detection and defense respectively on x + G and G respectively, which consistently outperform the results on the original x in experiments."
SP:2789874561620ba7894c4672f935056bb911e919,"This paper proposes a federated Thompson sampling ( FTS ) extension called DP - FTS - DE ( differentially private federated exploration with DE ), which extends the generalized differential privacy ( DP ) framework for adding DP to FTS to improve its utility. The proposed approach is motivated by the fact that FTS currently does not provide a privacy guarantee for user - level information. They leverage the ability of this general DP framework to handle different parameter vectors, as well as the technique of local modeling for BO, to further improve the utility of their algorithm through distributed exploration ( DE ). The resulting differential private FTS with DE algorithm is endowed with theoretical guarantees for both the privacy and utility, and is amenable to interesting theoretical insights about the privacy - utility trade - off. They also use real - world experiments to show that DP -FTS -DE achieves high utility ( competitive performance ) with a strong privacy guarantee ( small privacy loss ), and induces a favorable trade -off between privacy & utility."
SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"The paper proposes a multi - label active learning ( AL ) approach for training models for active data annotation. The proposed model is based on a Gaussian Process - Bayesian Bernoulli Mixture model ( BM ), where each mixture component corresponds to a global pattern of label correlations. The BM is integrated with a predictive Gaussian process ( GP ) that connects the features to the label through an effective inductive bias and achieve a feature - component - label mapping. The GP predicts coefficients of mixture components that help to recover the final set of labels of a data sample. A novel auxiliary variable based variational inference algorithm is developed to tackle the non - conjugacy introduced along with the mapping process for efficient end - to - end inference. The model also outputs a predictive distribution that provides both the label prediction and their correlations in the form of a label covariance matrix. Experiments on both synthetic and real - world datasets demonstrate the state - of - the - art AL performance of the proposed model."
SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"This paper proposes a streaming model for 3D object detection, 3D segmentation, and panoptic segmentation based on the nuScenes dataset. The key contribution of this paper is the use of a cartesian coordinate system for representing the sectors instead of the full point cloud. The proposed model uses multi - scale padding from neighboring sectors : preceding sector from the current scan, followed by bidirectional padding and single edge padding. The model also uses feature undistortion and range stratified convolutions to address the problem of applying convolutions on a polar grid. Experimental results show that the proposed model outperforms existing non - streaming methods as well as existing streaming methods with lower latencies."
SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"The paper proposes a method for learning structured latent variables that is based on score - function - based gradient estimators. This is in contrast to most existing approaches that consider a differentiable surrogate for the structured variable, e.g., Gumbel - Softmax trick. The paper argues that the surrogate imposes additional constraints on the model and biased gradients, while the score function based estimators alleviate the bias of the relaxed counterparts. The main contribution of the paper is to propose a family of recursive algorithms that have the property that their score function estimators are stochastic invariant. This allows them to construct reliable gradient estimates and control variates without additional constraints. In the experiments, the paper considers various structured latent variable models and achieves results that are competitive with relaxation - based counterparts."
SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"This paper proposes GainTuning, a method to adapt CNNs pre - trained on large datasets to a single test image by fine - tuning individual convolutional layers of the encoder / decoder. To avoid overfitting, the method optimizes a single multiplicative scaling parameter ( $ G$ ) of each convolution layer in the CNN. The method is evaluated on Imagenet and CIFAR denoising benchmarks, where it outperforms existing methods ( Self2Self, Denoising - MNIST, and ImageNet ). In addition, it is also tested on a synthetic data and a real scientific - imaging application where adaptivity is crucial. Empirical results show that the method outperforms the baselines by a large margin."
SP:90afa1102683b456bc72a54abef466326827546a,"This paper proposes a new architecture for semantic and instance segmentation ( a.k.a panoptic segmentation ), which is a fundamental task in computer vision used in down - stream tasks. The proposed model combines a convolutional neural network and an asymmetric multi - way cut problem solver. The former solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a pan - optic labeling. The latter is solved by backpropagating the gradient through the optimization problem w.r.t. a smooth surrogate of the quality metric. Experiments on Cityscapes and COCO datasets show improvement by back - propagation - based optimization."
SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"This paper proposes Recursive Bayesian Networks ( RBNs ), which generalize and unify Probabilistic context - free grammars ( PCFGs ) and dynamic Bayesian networks ( DBNs ), generalising both model classes and defining a joint distribution over tree - structured bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables.    The authors provide two solutions :   1 ) For arbitrary PCFG, they generalise inside and outside probabilities from PCFG to the mixed discrete - continuous case, which allows for maximum posterior estimates of the continuous latent variable via gradient descent, while marginalising over network structures. 2 ) For Gaussian DBNs, they additionally derive an analytic approximation of the marginal data likelihood ( evidence ) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. 3 ) They provide a quantitative evaluation on two data sets, namely, real - valued time series and musical data."
SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"The paper proposes a constrained backpropagation ( CBP ) algorithm based on the pseudo - lagrange multiplier method to obtain the optimal set of weights that satisfy a given set of constraints. The defining characteristic of the proposed CBP algorithm is the utilization of a Lagrangian function ( loss function plus constraint function ) as its objective function. The authors considered various types of constraints — binary, ternary, one - bit shift, and two - bit - shift weight constraints. As a posttraining method, CBP applied to AlexNet, ResNet-18, and ResNet50 on ImageNet, which were pre - trained using the conventional back Propagation. For most cases, the proposed algorithm outperforms the state - of - the - art methods on Image net, e.g., with different constraint functions."
SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"This paper studies Gaussian process classification ( GPC ) active learning for querying and active learning with GPC. The authors propose efficient methods to optimize the estimated error reduction ( EER ) for estimating the error reduction by querying instances based on the joint distribution of label pairs. The computation of EER - based EER is computationally expensive as it requires retraining the GPC with every new query. As a result, the authors propose methods that leverage a smooth approximation of the mean objective cost of uncertainty ( MOCU ) to compute EER and leverage a chain rule for efficient gradient computation of the SMOCU reduction by deriving the corresponding chain rule. In addition, they propose methods for efficient active learning of GPC for query synthesis by optimizing the EER-based acquisition function. This is the first algorithm for GPC active learning based on EER. The proposed methods outperform existing methods on both synthetic and real - world datasets."
SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"The paper studies the effect of unbounded gradients on the regularization of canonical autoencoder ( CAE ) architectures, such as VAE, on the reconstruction of sparse latent representations. The main result is that if the objective of training the CAE is to simultaneously avoid over - regularization ( i.e. high reconstruction error ) and under - regularize the latent representations, then an energy function with infinite gradients around optimal representations is provably required. The paper provides a number of technical reasons why this is so, in a very comprehensive and thoughtful way. In particular, it provides : 1. compelling evidence that by learning $ \gamma$-infinite gradients away from global minimizers, at least some bad local minimizers can be mitigated or smoothed within the VAE loss surface. This provides a possible explanation for why adaptive heuristics ( e.g., reduced step size, checks for oscillating gradient sign patterns, etc ) may sometimes be ill - advised. 2. compelling empirical evidence that VAE energy functions with bounded gradients are necessary for ensuring the convergence of CAEs to optimal sparse representations, and that large gradients should be accommodated to the extent possible."
SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"This paper studies the min - max regret of bandits with graph feedback for a directed graph G = ( V, E ), where V is the collection of bandit arms, and E is the observation distance to all incident arms. The authors propose the notions of the fractional weak domination number ( FVP ) and k - packing independence number ( KI ) capturing upper and lower bounds for the regret respectively. They show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual — the fractionic vertex packing set respectively, and utilize the strong duality theorem to prove a general regret upper bound O ( ( ( \�∗ log | V |) 1 3 T 2 3 ) ) and a lower bound O(\text{( \delta / \alpha)1 3 T\alpha ), respectively, where α is the integrality gap of the dual linear program. They also show that for some families of graphs, one can get rid of the ( \log |V | ) 1 3 factor and establish optimal regret."
SP:e50dec57af337839cbde4b65fb7b431785fda44d,"This paper studies the role of neighbourhood neighbourhood reference distributions ( SHAP ) in understanding the interpretability of the standard Shapley values. The authors argue that the use of a global population under the assumption of feature absence can lead to misleading results when local model behaviour is of interest. They propose to use a neighbourhood reference distribution formulation that improves interpretability by expressing the Nadaraya - Watson estimator as a self - normalised importance sampling estimator ( SIS ). The SIS formulation is motivated by the observation that the kernel of the well - studied kernel regressor, the Natarai - Watson, can be expressed as an SIS estimate of the importance of each node in a neighbourhood. They show that the SIS parameterizes the importance sampling as a weighted sum of the features that are absent from the global population. They demonstrate that the resulting SIS estimator is robust to the loss incurred by the Lipschitz continuity of the smoothing procedure. They also show that neighbourhood SHAP values provide meaningful sparse feature relevance attributions, complementing conventional Shapley analysis."
SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"This paper proposes PlayVirtual, a method to generate cycle - consistent virtual state - action trajectories to improve the data inefficiency of deep reinforcement learning ( DRL ). Specifically, PlayVirtual predicts future states in a latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, a sequence of sampled actions is generated, which is used to enforce a trajectory to meet the cycle consistency constraint, which can significantly enhance the data efficiency. The experimental results on the Atari and DMControl benchmark demonstrate the effectiveness of the PlayVirtual on the DRL setting, where it achieves the state - of - the - art performance."
SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,"This paper investigates how neural networks with different alignments with different ( target and noise ) functions affect the network’s robustness to noisy labels, in particular, how an architecture ’s alignments ( i.e., the one that is more aligned with the target function than the noise function ) affects the network's predictive power in the face of noisy labels. The authors provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. They also find that when the network is well - aligned ( aligned as defined in Theorem 2 ), it could outperform state - of - the - art ( SOTA ) noisy - label - training methods in terms of test accuracy and could even outperform sophisticated methods that use clean labels. Besides, this finding further leads to improvements over SOTA noisy - labeling methods on various datasets, and under various noisy settings."
SP:903727fe028684623a8ccadec210e641ecffc685,"This paper proposes an end - to - end imitation learning algorithm for solving off - policy example based control problems. The key idea is to directly learn to predict whether the task will be solved in the future via recursive classification, without using separate reward learning and reward function. The method estimates the probability of reaching a success example in a future and optimizes a policy to maximize a probability of success. They show that their method satisfies a new Bellman equation, where examples take the place of the typical reward function term. Experiments show that the proposed algorithm outperforms prior methods that learn explicit reward functions."
SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"This paper studies differentially private stochastic optimization in convex and non - convex settings. For the convex case, the authors focus on the family of non - smooth generalized linear losses ( GLLs ).   In particular, the proposed algorithm for the l2 setting achieves optimal excess population risk in near - linear time, while the best known differential private algorithms for general convex losses run in super -linear time. The proposed algorithm in the l1 setting has nearly - optimal excessPopulation risk, and circumvents the dimension - dependent lower bound of AFKT21. In addition, this paper provides several new algorithms for approximating stationary points of the population risk, which are claimed to be obtainable in the linear time setting."
SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"This paper studies cooperative bandit learning under three typical real - world imperfect communication scenarios : message - passing over stochastic time - varying networks, instantaneous reward sharing over a network with random delays, and message - sharing with adversarially corrupted rewards. For each of these environments, the authors propose decentralized algorithms that achieve competitive empirical performance, along with near - optimal guarantees on the incurred group regret. Furthermore, in the setting with perfect communication, an improved delayed - update algorithm outperforms the existing state - of - the - art on various network topologies. Finally, a tighter network - dependent minimax lower bound on the group regret is provided.   I believe that this paper can be of immediate utility in applications, and future inquiry can be pursued in several different directions, including multi - agent reinforcement learning and contextual bandit - learning."
SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"This paper studies post - training quantization of vision transformers with mixed precision for better compression and speed - up. The quantized process in the transformer is formulated as an optimization problem for finding the optimal low - bit intervals. To preserve the functionality of the attention mechanism, the paper introduces a ranking loss into the conventional quantization objective that aims to keep the relative order of the self - attention results after quantization. Moreover, the nuclear norm of the quantization loss of different layers is analyzed and explored to explore a mixed - precision quantization scheme by exploiting the nuclear norms of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state - of - the - art post - treatment quantization algorithms."
SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"This paper analyzes the convergence rate of double Q - learning, under certain assumptions ( e.g., the cardinality of the state - action space, discount factor, and parameterization of the function approximation ). In particular, the authors assume a constant learning rate of order of magnitude $ \tilde{L}$ for both synchronous and asynchronous sampling ( using the Makovian algorithm ), and use it to derive finite - time bounds of the global optimum of the two algorithms. The results obtained so far ( Xiong et al., 2020 ) are in terms of sample complexity, but do not provide a convergence rate. The main contributions of this paper are :   ( 1 ) This paper presents a systematic approach to bounding two nested stochastic approximation recursions by using a constant - time learning rate and a series of novel techniques to bound them ; ( 2 ) it provides a new analytical tool that improves the dependence of each of the important factors in the existing results ; ( 3 ) this paper provides results that improve upon the best known results of the vanilla Q learning that apply a polynomial learning rate. However, it still does not achieve the same convergence rate as the best available results from the literature that apply polynomials."
SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"This paper proposes a semi - supervised learning ( SSL ) method for OOD detection, called Structure - keep unzipping ( STEP ). It learns a new representation space in which OOD samples can be separated well, and an efficient optimization algorithm is derived to solve the objective. The paper evaluates the proposed method on various benchmarks and shows that it outperforms other methods by a large margin.    The main contribution of this paper is the introduction of a new SSL setting, called “ Semi - supervised OOD Detection ”. In this setting, only limited ID labeled data and large amount of mixed unlabeled data can be used. This is a novel and practical setting commonly appearing in real - world applications because under this setting it is possible to obtain a good detection performance. The main idea of the STEP approach is to detect OODs in a detection - specific space where we maintain the same local topological structures as the original feature space. Meanwhile, the paper also conduct comprehensive experiments to verify the robustness and generalization of STEP approach."
SP:6bf8b94483b26033795b0eda9649518027f5e1c2,"The paper proposes a Transformer - based model for the tasks of referring expression comprehension ( REC ) and segmentation ( RES ). Specifically, the model consists of a lingual query generator ( the decoder ) and a segmentation model ( the regression model ). The decoder generates lingual queries that are then translated into corresponding image segmentation masks. The regression model predicts the corresponding segmentation mask for each referred entity. The paper shows that the proposed model can achieve better performance than the previous two state - of - the - art methods ( ViLBEERT and ResNet ) on both the benchmarks of REC and RES by a large margin. Extensive experiments and ablations illustrate that the model benefits greatly from contextualized information, multi - task training and pre - training."
SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"This paper studies the problem of boosting, which is an algorithmic approach based on the idea of combining weak and moderately inaccurate hypotheses to form a strong and accurate one. In particular, this paper focuses on the setting where the weak learner is assumed to belong to an easy - to - learn base class, while the booster is an agnostic PAC learner for that class. The goal of the overall boosting algorithm is then to learn a combination of weak hypotheses by repeatedly calling the weak learners. This paper extends Adaboost ( Moran et al., 2017 ) to the multiclass boosting setting, by assuming that the base class is a simple classifier with a simple weak learning oracle. It then shows that there is a prominent gap between the sample complexities of the learner O(k2 ) and that of the booster O(log k ), depending on the number of classes k, that are needed for learning."
SP:f63b050773871338c48b778c362172e4b72477a4,"This paper proposes a new method for unsupervised object segmentation and object - centric scene generation based on a differentiable embedding - based model called GENESIS - V2. The embedding is learned using a stochastic stick - breaking process, which is similar to iterative refinement in that object embeddings are clustered according to a predefined distribution, but unlike refinement, this method does not require a fixed number of clusters to be generated. The clustering is then used to generate a set of randomly - selected object representations, which are then used in an autoregressive manner to generate scenes. The method is validated on two synthetic datasets ( ObjectsRoom and ShapeStacks ), as well as more challenging real datasets ( Sketchy and APC ), where it achieves promising results compared to several recent baselines. Experiments are also conducted on the Sketchy dataset to evaluate the quality of the reconstructed images."
SP:408deb9e5577ee7118b836fee77135df641fe545,"This paper proposes an adaptive method, called adaptive conformal inference ( ACI ), for forming prediction sets that are robust to changes in the marginal distribution of the data. The method builds on the idea of "" conformal "" inference, which is a type of black box predictive inference built on top of a conformal regression model, e.g., fitted to a point prediction set conditioned on a label function and a quantile function. In this setting, the data generating distribution of data may vary over time in an unknown fashion. The authors propose to model this distribution shift as a learning problem in a single parameter whose optimal value is varying over time and which must be continuously re - estimated. ACI is shown to achieve the desired coverage frequency over long - time intervals by various authors to demonstrate that it achieves the coverage regardless of the assumed exchange of data. In particular, ACI can be used to improve the consistency of the conformal prediction sets by obtaining smaller intervals by adapting the parameter value of the optimal parameter, and also obtain marginal coverage at most time steps. Experiments on two real world datasets, CIFAR-10 and Yahoo Finance, show that ACI achieves the target coverage frequency without any assumptions on the data - generating distribution."
SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"This paper proposes a Pose - level Inference Network ( PINet ) for multi - person pose estimation in crowded scenes. The main idea is to directly infer the pose from the body parts of each person without bounding boxes and individual keypoint detection. PINet first applies the Part - based Pose Generation ( PPG ) to infer multiple coarse poses for each person from his/her body parts, refined by the Pose Refinement module through incorporating pose priors, and finally fused in the Pose Fusion module. The superior performance of PINet is validated by extensive experiments on several crowded scenes pose estimation benchmarks.   For instance, PINet achieves 59.8% AP on the OCHuman dataset, outperforming the recent works by a large margin."
SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,"This paper studies the problem of computing the Bellman operator for S - rectangular robust Markov decision processes with L - restricted rectangular ambiguity sets in quasi - linear time in the number of states and actions. The authors propose a homotopy - based method to solve an S - triangular ambiguity set using a bisection method, which improves on the cubic time required by leading general linear programming methods. They also show that the worst - case time complexity of their algorithms is O(SA log(S ) ), which is faster than a general - purpose LP solver. In terms of future work, the authors believe that it is important to understand whether similar algorithms can be developed for RMDPs defined using Wasserstein distance, L2 - norm, or KL - divergence."
SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,"This paper studies the online knapsack problem with frequency predictions in the form of an upper and lower bound for the number of items of each value. The authors derive an online algorithm with the best possible competitive ratio for any fixed prediction that is obtainable in this setting. They also extend the results to more general settings such as generalized one - way trading and two - stage online knapack, and show that even seemingly weak predictions can be utilized effectively to improve the performance of online algorithms.    In brief, the problem is as follows :   ( 1 ) For any value v, let sv = P i|vi=v, where i.e., the total size of items with value v is the size of the set of all possible values that items may take. ( 2 ) For each value v 2 V, the frequency predictions P = { `v;uv}v2V provided to the algorithm are a lower bound `v and an upper bound uv such that `v   uv `, respectively. ( 3 ) A competitive ratio is a ratio that ensures that the algorithm ’s profit is at least ↵ times the optimum for all inputs respecting the given frequency prediction P. In this paper, the algorithm has a competitive ratio of ↵ for KNAPSACK - FP with prediction P if and only if P is the upper bound. ( 4 ) The goal of the paper is to design an online algorithms with the competitive ratio that is inversely proportionate to P in terms of the lower bound and upper bound such that P is a constant factor in \theta \in \mathbb{P}(P. ( 5 ) The paper also extends the results in the above to the case of one - step trading, by considering the following two - step approach : 1 ) for each value A, assume A is a set of equal numbers of equal values, where A and B are inputs of the form a convex optimization problem with the form A = A - A(V, R, O(V ),..., R ), 2 ) for all B = O(log(vmax / vmin ), where O(k ) is log(V / O(K ) where K is the logarithmically constant and R is the error in computing the ratio between the expected profit of the algorithm and that of the optimal algorithm."
SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"This paper proposes MBEC++, a model - based episodic episodic memory for trajectories. The memory estimates trajectory values, guides the agent towards good policies, and is built upon a complementary learning model which is called dynamic hybrid control. The learning model is composed of three modules : episodic control module, memory - based planning module, and trajectory model module. Experiments are conducted on grid - world, classical control, Atari games, and navigation tasks. The hybrid module is shown to outperform the baselines on classical control and Markovian RL. The performance gain is attributed to ( a ) storing distributed trajectories produced by a trajectory model, ( b ) memory based planning with fast value - propagation memory writing, and ( c ) dynamic consolidation of episodic values to parametric value function."
SP:551174c1266b5f4b6aaf5432a4c713386f90898c,"This paper proposes a semi - supervised learning ( SSL ) method called DP - SSL for unlabeled data. The proposed method is based on a multi - choice learning ( MCL ) based approach to automatically generate LFs from scratch in an SSL style. The noisy labels are then used to design a label model to resolve the conflict and overlap among the noisy labels, and finally probabilistic labels are inferred using a data programming scheme. Experiments are conducted on four standard SSL benchmarks and the proposed method achieves better classification performance on test sets than existing SSL methods, especially when only a small number of labeled samples are available."
SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"The paper proposes a Transformer - based model for estimating multi - person 3D poses from multi - view images. The proposed model is called Multi - view Pose Transformer ( MVP ) and it is based on the Transformer principle. In particular, MVP casts the pose estimation as a direct regression problem, i.e., instead of estimating 3D joint locations from volumetric representation or reconstructing per - person pose from multiple detected 2D poses as in previous methods. In contrast, in MVP, the multi - viewed 3D pose estimate is directly regressed from the 3D skeleton joints directly, without any intermediate task. The skeleton joints are represented as learnable query embeddings, which are exploited by a two - stage process. The first stage ( stage 1 ) is the projection stage, where the model is trained to estimate the per - view information from the input images. In stage 2, the model uses an input - dependent adaptation strategy ( stage 2 ) to learn the query embedding. In addition, the authors propose a novel geometrical guided attention mechanism, called projective attention, to more precisely fuse the cross - views information for each joint. The authors also introduce a RayConv operation to integrate the view - dependent camera geometry into the feature representations for augmenting the projective Attention.   The authors show that MVP outperforms the state - of - the - art methods on several benchmarks while being much more efficient. Notably, it achieves 92.3% AP25 on the challenging Panoptic dataset, improving upon the previous best approach [ 40 ] by 9.8%."
SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"This paper studies the problem of recovering from a mixture of sign responses the supports of unknown sparse vectors from a family of known but possibly non - existent, but potentially recoverable, support matrices of a known family of sparse vectors, under the assumption that each vector in the family has at most one non - zero element. The authors propose a learning algorithm based on tensor decomposition techniques and constructions of union - free families, which they claim to efficiently learn the supports with high probability the all unknown vectors from the family without any assumptions.   The main contribution of the paper is to prove the existence of learning algorithms for the first problem which work without any assumption of the family structure under a mild structural assumption on the unknown vectors, and to give a detailed analysis of the query complexity of the proposed algorithm for the second problem. The main contributions are :   ( 1 ) Under the first assumption that the family of the unknown vector is known, and the query vector is sparse, the authors provide a single - stage and two - stage reconstruction algorithms working under Assumption 1 for the approximate recovery problem. ( 2 ) For the support recovery problem, they provide a completely non - adaptive solution that does not use any adaptive strategies and is resilient to noisy measurements. ( 3 ) They introduce the notion of a robust resolvable matrix and derive an existential result for such matrices."
SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"The paper considers the problem of time - varying changepoint detection, i.e., the application of GCD - based continuous monitoring of a suite of sensors to detect abrupt temporal behavior patterns. The problem is formulated as a sequential choice - of - sensor problem, where only a subset of the correct set of sensors is used to continuously measure a given changepoint and the remaining sensors are used to detect changes in the sensor response. The goal is to ensure that only a small set of informative sensors ( of a fixed size ) is used at all times. The authors propose a method for doing this by iteratively sampling a sampling set of samples at each time step based on a known probability distribution. They show that this sampling process is computationally efficient, and derive lower bounds on the detection delay for this sampling procedure for a class of finite - parameterized probability distributions. Then, they propose an online version of their sampling procedure, called - GCD, which is based on sampling from the sampling distribution at time $ t$ and sampling from a probability distribution $ \tilde{T}$ of a set of fixed size $ \mathbb R^n$-sensors. They derive expected delay bounds for the proposed scheme, and show that these bounds match an information - theoretic lower bound. They then perform a number of experiments on synthetic and real datasets to demonstrate the effectiveness of their proposed method."
SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"This paper studies simple SGD - type updates for stochastic nested optimization of bilevel, min - max, and compositional optimization problems. The main contribution is the proposed ALTERNATE STochastic gradient dEscenT ( ALSET ) method for nested optimization, which is based on the alternating SGD approach in Section 1. The method is claimed to converge faster than the non - alternating approach under certain regularity conditions. The proposed method is validated on three nested optimization problems : 1 ) Compositional optimization problem, 2 ) Min - max problem, and 3 ) Reinforcement learning problem. The results show that ALSET converges faster than vanilla SGD on all three problems, and in some cases even matches the best - known sample complexity."
SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"This paper proposes a method to learn the soft labels for VideoQA task by learning from inside on the video question answering task. The method consists of a siamese sampling mechanism to generate sparse but similar clips from the same video, and a reasoning strategy to distill the inter - related knowledge between clips into the network inference. The reasoning strategy consists of two modules :    1. Siamese knowledge generation : generate some samples from the video that are similar to other samples so that they capture similar visual and semantic information ; and 2. siamesese knowledge reasoning to produce the refined soft label by propagating the weights of inter - relationship to the predicted candidates of all clips. Finally, the soft label guidance and ground truth can be jointly evolved by the proposed SiaSamRea.   Experiments demonstrate that the proposed method achieves state - of - the - art performance on five videoQA benchmarks, e.g., a significant +2.1% gain on MSRVTT - QA, +.9% on MSVD -QA,   +1.0% on ActivityNet   and +.8% on How2QA and +4.3% ( action ) on TGIF-QA. The main contributions of the paper are as follows : 1. Demonstrating the potential power of multiple clips in same video to learn rich inter - dependent knowledge which is ignored by current advanced methods 2. Providing the learned knowledge distill in the network training with the help of evolving the network with the learning during training during the inference during inference 3."
SP:160022e2cd61159da92f92e85520b7062a337a8d,"This paper proposes a simple method to reduce the computational and memory complexity of learning large structured models, such as latent probabilistic models, by viewing the central inference step as a matrix - vector product, and using a low - rank constraint to trade off model expressivity and speed via the rank. The method compares with the existing methods in several tasks, including language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling. It is able to achieve results on HSMMs, LHMMs, and some instances of Probabilistic Context - Free Grammars ( PCFGs ), and demonstrates to match the accuracy of standard models at large state spaces while providing practical practical speedups."
SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"This paper proposes a new uncertainty measure for contextual bandits, called Sample Average Uncertainty ( SAU ), which directly measures the uncertainty of the outcome based on the value predictions. In contrast to existing methods based on Thompson Sampling that instead require an estimate of the variability of the model parameters, SAU is immune to the negative effects that neural network parameterizations and optimization have on the quality of uncertainty estimation, resulting in reliable and robust exploration. The authors also provide theoretical guarantees that the uncertainty measure estimated by SAU asymptotically matches the uncertainty measured by Thompson Sampled, as well as its regret bounds. Because of its simplicity SAU can be seamlessly applied to deep contextual bandits as a very scalable drop - in replacement for epsilongreedy exploration. Finally, empirically the authors empirically show that SAU - based exploration outperforms current state - of - the - art deep Bayesian bandit methods on several real - world datasets at modest computation cost."
SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"This paper proposes a method for unsupervised behavior representation learning from multi - view videos. The method consists of two components : ( 1 ) Disentangled Behavior Embedding ( DBE ), which learns a latent representation of the dynamics of a pose by disentangling the dynamic behavioral factors ( pose ) from time - invariant, non - behavioral nuisance factors ( context ) in a deep autoencoder, and exploit the temporal structures of pose dynamics in a stochastic temporal model. Then, this model is combined with VDBE, which is an end - to - end approach that learns discrete behavior embedding and generates interpretable behavioral videos. DBE, DBE+VDBE achieve superior performance on downstream tasks such as fine - grained behavioral motif generation and behavior decoding."
SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"This paper proposes a deep 3D conditional generative model ( DMTET ) that can generate high - resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation, which is composed of a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Experiments are conducted on a dataset of complex 3D animal shapes, trained on a mixture of coarse and fine voxel inputs, and compared to existing implicit generative models, which are trained to regress the signed distance values, and directly optimizes for the reconstructed surface, which enables to synthesize finer geometric details. The proposed model achieves better reconstruction quality than state - of - the - art methods on challenging 3D shape synthesis tasks, while requiring a lower computation cost."
SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"This paper proposes sliced mutual information ( SMI ) as a surrogate measure of dependence. SMI is defined as an average of MI terms between one - dimensional random projections. The authors show that SMI preserves many of the structural properties of classic MI, while gaining scalable computation and efficient empirical estimation from samples. Furthermore, and in contrast to classicMI, SMI can grow as a result of deterministic transformations. This enables leveraging SMI for feature extraction by optimizing it over processing functions of raw data to identify useful representations thereof."
SP:e220b348901b476c2afd95f97630fb5400582f40,"This paper proposes a non - myopic constrained BO method, called 2 - OPT - C, which is a two - step method for constrained BO with a Monte Carlo rollout acquisition function, and it is suitable for both sequential and batch settings. The key idea is to use a likelihood ratio method to estimate the gradient of the acquisition function based on a reparameterization of a measure used in importance sampling, in contrast to prior work ( e.g., [ 1 ], [ 2 ] ), which uses a measure based on the variance reduction technique. The proposed method is computationally efficient ( in terms of expected return ), but it requires a sample average approximation of the gradient, unlike the prior work. In numerical experiments, the proposed method achieves improved query efficiency by a factor of 2x or more over previous methods, and in some cases by as much as 10x."
SP:51fbd861422647912f275b48861ea3c4812afdc8,The paper proposes a distributional RL method called MD3QN ( Multi - Dimensional Distributional DQN ) for learning multi - dimensional joint return distributions in environments with richly correlated reward functions. The authors claim that it is a good choice for learning joint distributions as it can capture not only the return distribution but also the correlation of rewards across the sources. The proposed algorithm is based on the convergence of the joint distributional Bellman operator and builds the empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distribution and its Bellman target. The effectiveness of the proposed method is verified on pixel - input environments in terms of the quality of modeled joint distribution and the performance of learnt policies.  
SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"This paper proposes a method for the reconstruction of high - resolution, accurate, and regular triangular meshes from volumetric images. The method relies on a new Diffeomorphic Mesh Deformation ( DMD ) module, which is parameterized by a set of diffeomorphic mappings. This module is used to efficiently reconstruct the template mesh by learning to deform it towards a target object. To train this model, the authors implement a flow ODE framework, which benefits from a small GPU memory footprint, allowing the generation of surfaces with several hundred thousand vertices. To reduce topological errors introduced by its discrete resolution, they derive numerical conditions which improve the manifoldness of the predicted triangle mesh. Finally, the proposed model achieves state - of - the - art performance in the challenging brain cortical surface reconstruction problem. This benchmark reveals that CorticalFlow is more accurate and, by construction, more robust to image artifacts providing anatomically plausible surfaces."
SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"This paper studies the problem of data deletion, which aims to remove the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. Most prior work in the non - convex setting gives valid deletion guarantees only for sequences that are chosen independently of the models that are published. In this paper, the authors show that if people choose to delete their data as a function of the published models ( because they don’t like what the models reveal about them, for example ), then the update sequence is adaptive. They provide a general reduction from deletion guarantees against adaptive sequences to deletion guarantees for non - adaptive sequences, using differential privacy and its connection to max information. This leads to a very flexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletion guarantees. The authors also provide a model and training algorithm agnostic methodology that allows for deletion of arbitrary sequences of adaptively chosen points while giving rigorous guarantees."
SP:7150006590e268ab732c9be6c9048f67a377f956,"This paper proposes a method to optimise the conditional value at risk ( CVaR ) of the total return in Bayes - adaptive Markov decision processes ( MDPs ) based on model - based Bayesian RL. This is motivated by the aim of mitigating risk due to both epistemic uncertainty over the model, and aleatoric uncertainty due to the environment stochasticity. The authors propose an approximate algorithm based on Monte Carlo tree search ( MCTS ) and Bayesian optimisation ( BAMCP ). They reformulate the problem as a two - player stochstic game, and derive a Monte Carlo algorithm for policy optimization based on a Bayesian optimization algorithm and an approximate gradient descent algorithm. They evaluate the proposed method on two domains : BAMDP and sequential decision making. The empirical results show that the proposed algorithm significantly outperforms baseline methods on both domains."
SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"This paper studies the problem of learning deep neural networks with the logistic loss via gradient descent on binary classification problems where the underlying data distribution is general and the ( optimal ) Bayes risk is not necessarily zero. In this setting, the proposed method ( gradient descent with early stopping ) achieves population risk arbitrarily close to optimal, both in terms of logistic and misclassification losses. The sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. The necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain measure of the complexity of the true conditional model. The paper shows that as data, width, and training time increase, the logistically loss measured over the population converges to optimality over all measurable functions, and moreover implies that the induced conditional model ( defined as a sigmoids mapping ) converges   to the true model, and the population misclassified rate also converges. This is in contrast with prior analyses of gradient descent which either only consider the training risk, or can only handle restricted conditional models.    Main contribution of this work is to provide a mathematical basis for this good performance on arbitrary binary classification problem, considering the simplest possible networks : shallow ReLU networks, where only the inner ( input - facing ) weights are trained via vanilla gradient descent. Instead, it shows that the simple network can achieve population risk approximately converge to optimal one. The main result is to show that early stopping is necessary, as any univariate classifier satisfying a local interpolation property is inconsistent."
SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"This paper proposes VigDet, a method for social media coordinated group detection based on neural temporal point process. The method uses a Gibbs distribution of group assignment based on how consistent an assignment is to ( 1 ) the account embedding space and ( 2 ) the prior knowledge. To address the challenge that the Gibbs distribution is hard to compute and sample from, the authors use a theoretically guaranteed variational inference approach to learn a mean - field approximation for it. Experiments on a real - world dataset show the effectiveness of the proposed method compared to state - of - the - art model in both unsupervised and semi - supervised settings. Furthermore, the model is applied to uncover suspicious misinformation campaign in COVID-19 vaccine related tweet dataset. The detection result suggests presence of suspicious coordinated efforts on spreading misinformation."
SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"The paper studies a binary classification task that uses a deep neural network to classify two disjoint smooth curves drawn from the unit sphere. The paper claims that when the network depth is large relative to certain geometric properties that set the difficulty of the problem, randomly - initialized gradient descent quickly learns to correctly classify all points on the two curves with high probability. This is the first generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties. The analysis proceeds by a reduction to dynamics in the neural tangent kernel ( NTK ) regime, where the networkdepth plays the role of a fitting resource in solving the classification problem. In particular, via fine - grained control of the decay properties of the NTK, the paper shows that the deep network guarantees the guarantee of a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization."
SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"This paper proposes two new methods to tackle the training instability issue of Conditional Generative Adversarial Networks ( cGANs ). The authors first identify that gradient exploding in the classifier can cause an undesirable collapse in early training, and project input vectors onto a unit hypersphere can resolve the problem. Second, the authors propose the Data - to - Data Cross - entropy loss ( D2D - CE ) to exploit the relational information of the class - labeled dataset. On this foundation, this paper proposes the Rebooted Auxiliary Classifier Generative adversarial network ( ReACGAN ), which achieves state - of - the - art generation results on CIFAR10, Tiny - ImageNet, CUB200, and ImageNet datasets. Extensive analyses are performed to show the superiority of the proposed method over existing classifier - based and projection - based GANs on five benchmark image datasets. Moreover, exhaustive analyses are conducted to demonstrate the robustness of the model on hyperparameters selection and differentiable augmentations. Experiments are conducted on four datasets."
SP:080e80746a87228b156408ff649ab7a17f44e92d,"This paper studies reinforcement learning for policy space response oracle ( PSRO ), a RL algorithm for two - player zero - sum games that has been empirically shown to find Nash equilibria in large games. The authors propose XDO, an extensive - form version of PSRO that is guaranteed to converge to an approximate Nash equilibrium linearly in the number of infostates. They also introduce Neural XDO ( NXDO ), where the best response is learned through deep RL. In tabular experiments on Leduc poker, XDO achieves an Nash equilibrium that is an order of magnitude smaller than PSRO. On Oshi - Zumo, they show that NXDO outperforms PSRO and NFSP on a sequential multidimensional continuous - action game. They hypothesize that games with these properties are prevalent in many domains such as large board games, video games, and robotics applications."
SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"This paper proposes a permutation - invariant variational autoencoder for graph - level unsupervised representation learning, where the objective is to learn a graph representation that is by design invariant to the order of nodes in a graph. The proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. In the experiments, the authors demonstrate the effectiveness of their proposed model for graph reconstruction, generation and interpolation, and evaluate the expressive power of extracted representations for downstream graph-level classification and regression."
SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"This paper proposes a method to improve expressivity and accuracy of Graph Neural Networks ( GNN ) on large graphs by using a “ decoupling ” method. The key idea is to first extract a subgraph within a fixed scope ( e.g. a node or edge ) within the scope of a given GNN, and then apply a GNN of arbitrary depth on top of that subgraph. The idea is that the GNN will smooth the local neighborhood into informative representation rather than oversmoothing the global graph into “ white noise ”. The paper provides theoretical analysis on expressivity from three different perspectives, and also rich design components to implement such design principle in different GNN architectures : GCN, function approximation ( GIN ), and topological learning. Experiments show significant performance improvement on seven benchmarks ( including the largest ogbn - 100 M graph with 111 M nodes ) and across two graph learning tasks. In addition, the computation and hardware costs are reduced by orders of magnitude."
SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"Normalizing flows are a widely used class of latent - variable generative models with a tractable likelihood. Affine - coupling networks are a particularly common type of normalizing flows, for which the Jacobian of the latent - to - observable - variable transformation is triangular, allowing the likelihood to be computed in linear time. The question of universal approximation was recently resolved by three parallel papers who showed reasonably regular distributions can be approximated arbitrarily well using affine couplings ( albeit with networks with a nearly -singular Jacobian ), but the fundamental question remains : which distributions can we hope to fit via training using an affine coupling flow? In this paper, the authors provide the first guarantees on universal approximation with well - conditioned affine-coupling networks, and provide theoretical evidence to support the benefits of Gaussian padding ( i.e. approximate the log - concave distribution with iid Gaussians ), a strategy which has been empirically observed to result in better - conditioned flows, but which was previously untried and unprovoked. Theoretically, their techniques uncover some deep connections between affine coupled networks and underdamped Langevin dynamics and Hénon maps, a family of dynamical systems."
SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"This paper proposes a generalization method based on the Lagrangian multiplier, called \lambda, for the problem of coupons allocation within a fixed budget in the online e -commerce setting. The proposed method can help enterprises develop a coupons allocation policy which greatly improves users’ retention rate on the platform while ensuring the cost does not exceed the budget. Specifically, \lambda - generalizes the policy learning process according to different \lambda values adaptively, avoiding re - learning new polices from scratch. Also, a budget constrained offline reinforcement learning ( BCORLE ) method and an off - policy evaluation method ( REME ) are proposed for policy learning and policy evaluation, respectively. Experiments on a simulation platform and a real mobile shopping app validate the effectiveness of the proposed methods."
SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"This paper proposes a new method for source - free domain adaptation ( SFDA ), where the source pretrained model is adapted to the target domain in the absence of source data. The proposed approach is based on the observation that the target data might no longer align with the source domain classifier, but still forms clear clusters that can be easily detected by a classifier. To achieve the adaptation by encouraging label consistency among local target features, the SFDA method distinguishes between nearest neighbors, reciprocal neighbors and expanded neighborhood. The experimental results on both 2D image and 3D point cloud datasets testify the efficacy of the method."
SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,"This paper proposes a pooling method for aggregating a set of features into a fixed - dimensional representation, called pooling by sliced - Wasserstein embedding ( PSWE ). The key insight of the method is that it uses a permutation - invariant pooling mechanism, i.e., the sliced - wasserstein distance ( SW ), to compute a constant - size representation for the entire set, whose elements are aggregated in a pool using the learned SW distance. PSWE is motivated by existing pooling methods, such as mean pooling and featurewise sort pool, to learn the SW distance, and derive an exact Euclidean embedding, which is geometrically interpretable. The authors evaluate PSWE on a variety of data modalities, including point cloud classification, graph classification, and image classification tasks, and show that their method provides superior performance over existing set representation learning approaches."
SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"This paper proposes a new family of RNNs, namely SBO - RNN, that can be formulated using stochastic bilevel optimization ( SBO ), where the lower - level optimization is mapped to the sub - network for computing hidden features, and the upper -level optimization defines the predictor based on the computed features. The authors prove that under mild conditions there is no vanishing / exploding gradient in the training of our SBO RNN. Empirically, the proposed approach achieves superior performance on several benchmark datasets, with fewer parameters, less training data, and much faster convergence."
SP:d3a4300e21ca215334f256f0467a428470548fe4,"The authors study the problem of minimizing power consumption in systems with multiple power - saving states, where an algorithm has to choose between different energy consumption and wake - up costs. They develop a learning - augmented online algorithm that makes decisions based on ( potentially inaccurate ) predicted lengths of the idle periods. The algorithm ’s performance is near - optimal when predictions are accurate and degrades gracefully with increasing prediction error, with a worst - case guarantee almost identical to the optimal classical online algorithm for the problem. The key ingredient in their approach is a new algorithm for online ski rental problem in the learning augmented setting with tight dependence on the prediction error. They support their theoretical findings with experiments."
SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,"This paper introduces a mathematical framework for quantifying the transferability in multi - source transfer learning problems, with both the task similarities and the sample complexity of learning models taken into account. In particular, the authors consider the setup where the models learned from different tasks are linearly combined for learning the target task, and use the optimal combining coefficients to measure the transferable capacity. Then, they demonstrate the analytical expression of this transferability measure, characterized by the sample sizes, model complexity, and the similarities between source and target tasks, which provides fundamental insights of the knowledge transferring mechanism and the guidance for algorithm designs. Furthermore, they apply their analyses for practical learning tasks, and establish a quantifiable transferable measure by exploiting a parameterized model. In addition, they develop an alternating iterative algorithm to implement their theoretical results and implement a practical algorithm, which is evaluated on image classification tasks."
SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"The paper proposes a neural network model, called eccNET, that learns to discriminate between a target image and a search image by following a sequence of eye movements until the target is found. The eye movements follow a trajectory based eccentricity distribution that combines the eccentricity of the target with the location of the object in the search space. The model is trained using an ImageNet, but it is not trained with the target or search images, nor is it trained with data showing asymmetry in human search behavior. The authors hypothesized that search asymmetry could be explained by a statistical property of the developmental diet fed to the model. They tested the model by training the model on augmented versions of ImageNet where the biases of natural images were either removed or reversed. The results show that the polarity of search asymmetries disappeared or was altered depending on the training protocol.   This study highlights how classical perceptual properties can emerge in neural network models, without the need for task - specific training, but rather as a consequence of the statistical properties of the dataset the model is fed. The findings are interesting and build bridges between psychophysics observations in visual search studies and analyses of behavioral biases of deep networks."
SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"This paper studies the role of loss landscape in training certifiable robust models. The authors claim that linear relaxation - based methods often have a landscape that is favorable to optimization, and that the current state - of - the - arts method often has a landscape with favorable optimization properties, but Interval Bound Propagation ( IBP ) uses much looser bounds but outperforms other models that use tighter bounds. Also, the authors identify another key factor that influences the performance of certifiable training : smoothness of the loss landscape. The main contribution of the paper is to propose a method that satisfies the two criteria : tightness of   the upper bound on the worst - case loss and smoothness   of the landscape. With the two factors, the proposed method achieves a decent performance under a wide range of perturbations, while others with only one or the other can perform well only for a specific range. The paper also proposes a test method with the desired properties."
SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,The paper considers the problem of online linear regression in the stochastic setting. It derives high probability regret bounds for online ridge regression and the forward algorithm. This enables to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. The study advocates for the use of   forward algorithm in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. The paper explains how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. The numerical experiments are provided to illustrate   intuitions and endorse   the intuitions.
SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"This paper proposes a two - time - scale extension of the extragradient ( EG ) method with an anchoring technique named Fast Expresourced Gradient ( FEG ), which is based on the EAG method. The proposed FEG is designed for smooth structured nonconvex - nonconcave problems. It is shown to have an accelerated $ O(1/k2 ) rate, with respect to the squared gradient norm, for the Lipschitz continuous and negative comonotone operators for the first time. The FEG also has value for smooth convex - concave problems, compared to existing works. This paper further develops its backtracking line - search version, named FEG - A, for smoother problems. Finally, this paper analyzes a stochastic version of FEG, named S - FEG."
SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"This paper studies the problem of uniformity testing for statistical data that consists of rankings over m items, where the alternative class is restricted to Mallows models. The ranking data is factored into a binary distribution whose mean and covariance matrix are the binary statistics extracted from the ranking data. Simple pairwise statistics are used to compute the expected values, which allows to test uniformity using only two samples, if m is large enough. The paper also considers uniforming testing with central and local differential privacy ( DP ) constraints. It proposes a central DP algorithm that requires O(max{1/✏0, 1/ p m}, where ✏0 is the privacy budget parameter. Interestingly, the algorithm is straightforward to apply to the local DP scenario, since it works with binary statistics. The proposed algorithms are scalable, since they could handle large m including m = 10000, and are able to detect non - uniformity with very small error."
SP:99a835191a3ba8372e391b6d3316e9b68e543295,"This paper studies a general greedy score - based algorithm for learning directed acyclic graphs ( DAGs ) called BNSL, which is based on the duality between Bregman divergences and exponential families. The key insight is that the proposed algorithm is in fact a general case of existing edge - greedy algorithms such as GES, hill climbing, and A* search, all of which can be interpreted as special cases of the proposed greedy algorithm. The paper then shows how polynomial - time algorithms for learning DAG models are a special case of this general greedy algorithm, and proposes new score functions and optimality conditions based on this generalization. Finally, extensive experiments are provided suggesting that this algorithm indeed optimizes the score in a variety of settings."
SP:b60989706296b963b6671c01f22384978a334be1,This paper aims to improve the adversarial robustness of the backbone CNNs that have a satisfactory accuracy over the natural data. The authors propose a dilation method for improving the robustness while maintaining accuracy. The framework is called neural architecture dilation for adversarially robustness ( NADAR ). It is motivated by analyzing the standard and adversarial error bounds of the network under a constrained optimization objective. The FLOPs - aware approach to optimize the architecture is also introduced to reduce the computational overhead. Experiments on real - world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm to balance the accuracy and robustness.
SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"This paper studies model - based reward - free reinforcement learning with linear function approximation for episodic Markov decision processes ( MDPs ) under the linear mixture MDP assumption, where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. The authors propose a new provably efficient algorithm, UCRL - RFE, based on this assumption, which they call UCRL-RFE+. They show that to obtain an $ \tilde{H}^k$-optimal policy for arbitrary reward function, the algorithm needs to sample at most $ Ú¬(H5d2d2ε−2) $ episodes during the exploration phase, where H is the length of the episode, d is the dimension of the feature mapping, and O(H^d ) is the number of times the mapping can be used. They also propose a Bernstein - type bonus and show that it needs to samples at most Ú(Hd(H + d)^k \leq H^d + \epsilon^2 ) for optimal policy. Finally, they also give a sample complexity lower bound   for any reward - based RL algorithm   under this assumption and give a Bernstein bonus variant.    The main contributions of this paper are the following :   1. Provably efficient algorithms and theoretical guarantees for model - free RL with function approximation under the assumption of Linear Mixture MDP. Theorem 1. 2. Strong theoretical guarantees in terms of dependence on accuracy ϵ and dependence on the dimension d if H < d."
SP:28563ba0975f56ddb662cd46e85de78bb6024d36,"This paper proposes a method for effectively and efficiently forecasting future events given a data stream of events with seasonal patterns that innovate over time. The key idea is a Shifting Seasonal Matrix Factorization approach ( SSMF ) that can adaptively learn multiple seasonal patterns ( called regimes ), as well as switching between them. The proposed method has the following properties : ( a ) it accurately forecasts future events by detecting regime shifts in seasonal patterns as the data stream evolves ; ( b ) it works in an online setting, i.e., processes each observation in constant time and memory ; ( c ) it effectively realizes regime shifts without human intervention by using a lossless data compression scheme. The authors demonstrate that their algorithm outperforms state - of - the - art baseline methods by accurately forecasting upcoming events on three real - world data streams."
SP:e4bb07033001be4d04695ef058f426d49fe440be,"This paper proposes WeaveNet, a neural network architecture for solving the combinatorial problem of assignment, i.e., finding a task to match a limited number of elements. The authors consider one of the most popular non - linear assignment problem : stable matching with two strongly NP - hard settings, and apply their architecture based learning - based method. The learning layer, called feature weaving layer, is stacked to model frequent communication between elements in a parameter - efficient way to solve the assignment problem. The experimental results showed its impressive performance among the baselines and compared to the state - of - the - art algorithmic method, depending on the size of the problem instances."
SP:8a559e21d45661eef427b310e5fe8488d5749137,"This paper studies the impact of various self - supervised learning proxy tasks on the adversarial robustness of 3D deep learning models for 3D point clouds with adversarial training. Specifically, they study MLP - based PointNet, Convolution - based DGCNN, and Transformer - based PCT, as well as the jigsaw proxy task. They find that fine - tuned models from different pre - training tasks have different vulnerabilities, and adversarial examples generated by attacking them do not transfer well among them. Thus, they further leverage two simple yet powerful ensemble methods, namely, PointEnsemble and JigsawEnsemble, to boost the robustness by a substantial margin. The best ensemble models achieve robust accuracy of 53.5% (+15.6%), 69.4% (+7.4%) and 57.9% (+8.8%) with PointNet, DGCnn, and PCT on the representative dataset ModelNet40 [ 30 ]."
SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"The paper considers the problem of convex optimization over base base polytopes, where a parametric submodular function can be expressed as a convex function of degree $ f$ over a set of base subtopes $ \mathbb{R}$, and optimizers such as FISTA, mirror descent, and projected Newton ’s method enjoy near - optimal regret bounds and high convergence rates. Concretely, the problem is formulated as an optimization problem over the set of cardinality - based sub - topes $ B(f)$. The paper proposes a toolkit to compute projections of close - by points $ n$ using both discrete and continuous perspectives, and applies the away - step Frank - Wolfe algorithm to improve the convergence of this iterative method. The theoretical results show orders of magnitude reduction in runtime.   The main contribution of the paper is the adapt - step version of the Frank - Wolfe algorithm to consider the combinatorial structure of previous projections, and accordingly obtain improvements over the basic AFW algorithm. This algorithm is most effective for computing projections ( since it can invoke all of the tools in the proposed toolkit ), but it also serves as a standalone algorithm for convex optimisation over $ B$-topes. Specifically, for cardinality based sub modular function minimization, the proposed A2FW improves the runtime of computing certain Bregman projections by a factor of $ \�(n ) / log(n$ ( theoretical results )."
SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"This paper considers learning the natural parameters of a minimal truncated k - parameter exponential family from i.i.d. samples in a statistically and computationally efficient manner. The authors propose a novel estimator via minimizing a convex loss function and obtain consistency and asymptotic normality of the same. They provide finite sample guarantees to achieve an α - approximate error of $ \alpha$ in the parameter estimation with sample complexity O(poly(k/alpha ) ) and computational complexity O ( poly(k / α ) ). They show that, at the population level, the method can be viewed as the maximum likelihood estimation of a re - parameterized distribution belonging to the same class of exponential family. The natural parameter $ \theta$ specifies a particular distribution in the exponential family, and if the natural statistic $ \tilde{x}$ and the support of $ x^2 $ are known, then learning a distribution in exponential family is equivalent to learning the corresponding natural parameter    of the corresponding exponential family $ \mathbb{K}$.   The main contributions of the paper are :   ( 1 ) The authors provide a novel and statistically efficient estimator that is consistent and normal under mild conditions. Under mild conditions, the traditional maximum likelihood estimator for this class is computationally inefficient. They focus on the setting where the support as well as the natural parameter are appropriately bounded. In particular, under mild assumptions ( e.g., a singular condition on the parameter distribution ), their estimator is as follows : The loss function minimizes the convex function $ \phi(x^2)$, and the sample complexity is a function of the degree of each of the parameters of the class corresponding to each node in the class. This loss function can be used to obtain an estimate of the true natural parameter with a sample complexity of O(log(k ) / ( k / α) ). The sample complexity depends logarithmically on the dimension of the parameter dimension i.e., $ O(k^2 ) $.   - ( 2 ) The main contribution of this paper is an interpretation of the estimator in terms of maximum likelihood. This interpretation enables the reader to understand the computation and statistical guarantees of our estimator more clearly."
SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"The paper proposes a method to infer 3D geometry, reflectance, and lighting from a single image containing strong specular transport using a convolutional neural network and a differentiable renderer. The method is based on the DIB - R framework [ 10 ]. The main difference is that instead of using a rasterization - based renderer, the model is divided into two parts : a geometry head and a reflectance head. The geometry head predicts the geometry and reflectance of the mesh, while the reflectance is estimated using a spherical basis function. The lighting is modeled using a spatially - varying material model, which is used to control the amount of scattering of diffusions. The path tracing is done using path - tracing - based differentiability. The renderer is differentiable in order to preserve the consistency of the differentiable part of the model. The approach is validated on both synthetic and real images. It is shown to reconstruct realistic materials BRDFs and lighting configurations over prior rasterisation - based methods."
SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"This paper proposes a differentiable training method, called sampling - argmax, to replace the conventional soft - max in the detection - based localization operation in neural networks. Soft - max decomposes the probability distribution in the output distribution into a set of samples, which are then used to minimize an expectation of the localization error. To achieve this, the paper develops a continuous formulation of the sampling process, which is differentiable, and a sampling process which is continuous. Samples are sampled so that the average error of all samples drawn from the continuous sampling process can be approximated by computing the weighted average of the samples. The paper shows that this sampling process is effective and flexible on various localization tasks. It is shown that sampling - max can be used in the same way as the conventional hard - max operation, and it can be trained to be differentiable like the naive implementation of Soft - Max."
SP:478c05c90090f9d80b72ac352c488073b45a5d8b,This paper introduces a directed graph data augmentation method called Laplacian perturbation ( LPT ) and a new directed graph contrastive learning ( DiGCL ) framework. The proposed LPT generates contrastive views for directed graphs using a pre - trained contrastive generator and a curriculum learning - based method to progressively learn from different combinations of easy - to - learn and difficult contrastive points. They show that the proposed method retains more structural features of directed graphs than other GCL models while providing enough contrastive information. They also show that their method outperforms the state - of - the - art approaches.
SP:85b383d2f722f7bff438840e423f5cb4c67d5980,"This paper proposes Symbolic Interactive Language Grounding Benchmark ( SILG ), which unifies a collection of diverse grounded language learning environments under a common interface. SILG consists of grid - world environments that require generalization to new dynamics, entities, and partially observed worlds, as well as symbolic counterparts of visual worlds that require interpreting rich natural language with respect to complex scenes ( such as ALFWorld and Touchdown ). Together, these environments provide diverse grounding challenges in richness of observation space, action space, language specification, and plan complexity. In addition, the authors propose the first shared model architecture for RL on these environments, and evaluate recent advances such as egocentric local convolution, recurrent state - tracking, entity - centric attention, and pretrained LM using SILG. The shared architecture achieves comparable performance to environment - specific architectures. Moreover, the most models significantly trail human performance on SILG, which suggests that there is plenty of room for future work."
SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"The paper proposes a sparse version of the Vision Transformer ( ViT ) that is scalable and competitive with the largest dense networks. The proposed method, dubbed Vision MoE ( V - MoE ), can be applied to image recognition and matches the performance of state - of - the - art networks, while requiring as little as half of the compute at inference time. The paper also proposes an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per - image compute. Finally, the paper demonstrates the potential of V -MoE to scale vision models, and train a 15 B parameter model that attains 90.35% on ImageNet when fine - tuned."
SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"This paper studies the expressivity and training of neural networks with very narrow 1 - hidden layers on the condition that the input dimension of the network is at least twice as large as the output dimension. The authors show that as long as the network's width is greater than 2^{n/d }, there exists at least one global minimizer with zero training loss. Moreover, they identify a nice local region with no local - min or saddle points that is likely to hold for a very long time, and prove that every KKT point in this region is a nearly global optimizer. Next, they propose a constrained optimization formulation where the feasible region is the nice local area, and show that a projected gradient method will converge to KKT points under mild technical conditions. Finally, numerical results show that projected gradient methods on this constrained formulation significantly outperform SGD for training narrow neural nets."
SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"This paper proposes a continuous mean - median - covariance bandit model, CMCB, which is a risk - aware multi - armed bandit that explicitly takes into account option correlation in the form of the covariance of the weight vector of each action taken by the learner. The paper proposes three different feedback settings for the bandit, i.e. ( 1 ) Full Bandit, ( 2 ) Semi - Bandit and ( 3 ) Fully Bandit. Under the full bandit setting, the objective of the model is to achieve the best trade - off between reward and risk, measured with the option covariance. The authors provide a lower bound for the search space of the optimal regret within logarithmic factors, and provide matching lower bounds to validate their optimality. The experimental results also demonstrate the superiority of their algorithms.    To the best of my knowledge, this is the first work that considers option correlation as a factor in the learning objective of bandit models and quantifies how it affects the learning performance. The techniques they develop for exploiting the estimated covariance to build concentration and bounding the risk based on the sampling strategy properties are very interesting and can find applications in other bandit analysis."
SP:472a90bb175b0286765c5a47b040e1a58f594a05,"This paper proposes an extension to the Multiplicative Update ( MMR ) algorithm for nonnegative matrix factorization ( NMF ) of PSD factorization of positive semidefinite matrices ( PSD ) based on the fact that the squared loss minimization objective is non - decreasing and fixed points correspond to critical points. The authors show that the MMU algorithm can be used as a primitive to calculate PSD or tensor factorizations and provide numerical results supporting the expressive power of the method. The method is based on a modification of the multiplicative update algorithm of Lee - Seung ( Lee and Seung, 2017 ) for the PSD update of matrix { Ai } and { Bj } satisfying the condition Xij = tr(AiBj ) for all i, j ( for all m,..., n ).   In this paper, the authors propose a non - commutative extension of the MMR algorithm to compute PSD updates using diagonal PSD matrices, and show that it preserves the non - negativeness property of the update by congruence scaling with the matrix geometric mean. They also show that this method is a good baseline for computing block - diagonal or PSD matrix factorizations of diagonal matrices. Experiments on real and synthetic data demonstrate that the method outperforms MMR on a variety of tasks."
SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"The paper proposes a meta - learning framework for domain generalization ( DG ) that combines the idea of domain invariance and the importance of domain - specific information ( i.e., the label information that is specific to a particular domain ) in order to generalize to unseen target domains. The key idea is to disentangle features in the latent space while jointly learning both domain - invariant and domainspecific features in a unified framework, which is called meta - Domain Specific - Domain Invariant ( mDSDI ).   The theoretical analysis and experimental results on several DG benchmark datasets are provided to demonstrate the competitive performance of the proposed method. The ablation study with Background - Colored - MNIST also supports the hypothesis that domain -specific information is essential, leading to better results when compared with only using domain -invariant. The main contributions of the paper are as follows :   1. The paper provides the first theoretical analysis to understand and realize the efficiency of domain-specific information in domain - generalization 2. It provides a training scheme to support the meta - training scheme    3. Empirically, the proposed approach demonstrates competitive results with state - of - the - art techniques in DG."
SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"The paper studies the generative modeling of diffusion - based generative models based on Ho et al [ 31 ] and the improvements from Nichol and Dhariwal [ 49 ] and Song et al. [ 64 ] and compares them to the state - of - the - art GAN - based models such as DeepGAN and BigGAN - Deep. The diffusion models achieve better FID scores ( FID of 2.97 on ImageNet 128 ) compared to DeepGan - based GANs ( 2.59 on   ImageNet 256 and 512 ) on unconditional image synthesis tasks. The authors further improve FID by using classifier guidance ( i.e., using the gradients from a classifier to guide the diffusion model ) and upsampling the diffusion models, which further improves FID. However, the proposed method is not yet competitive with the SOTA GAN models due to the use of multiple denoising steps and the length of the forward passes."
SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"This paper proposes to leverage out - of - distribution ( OOD ) samples for improving few - shot learning. The key idea is to drive the classifier to avoid irrelevant features by maximizing the distance from prototypes to OOD samples while minimizing the distance to in - distribution samples, e.g., support data. The proposed approach is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre - training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures ( cross - domain FSL, InvariNet, etc. )."
SP:b1f65724926f136979829b7a6c870bc31f38f591,"This paper studies the problem of prioritized sampling in reinforcement learning, in which the goal is to obtain an optimal Bellman update that directly maximizes the return of the policy. The paper first points out that the objectives of prioritization are different from the objective of RL, which can lead to suboptimal training process. To solve this issue, the paper proposes two methods, ReMERN and ReMERT, that directly aims to improve the policy through prioritization. Theoretical analysis suggests that data with higher hindsight TD error, better on - police and more accurate Q value should be assigned with higher weights during sampling, thus most previous criteria only consider this strategy partially. The authors provide theoretical justifications for previous criteria, and propose two new methods to compute the prioritization weight, namely Re - MERT and Re -ERN. Both methods outperform previous prioritization sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta - World."
SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,"This paper considers the sequential prediction problem with expert advice in a nonstationary environment with long - term memory. The authors consider a model based on the Bousquet - Warmuth algorithm [ 4 ], where memory is assumed to be available for all possible values of the variables. They consider two versions of a linear - time algorithm based on Gaussians, one of which improves on the best known regret bounds [ 27 ] by a factor of sqrt(W ) for all non - uniform lower - box constraints. The other algorithm they consider is based on a relative entropy projection step, where the lower bound of the entropy is computed by taking the mean and covariance of a vector. They derive a regret bound for both of them.    The main contribution of this paper is the introduction and derivation of related results related to the previous work. In particular, they give the following main results :   ( 1 ) A regret bound that holds for all variants of the two algorithms. ( 2 ) An algorithm to compute the projection step in linear time. ( 3 ) A new “ geometric - decayay ” method for estimating the relative entropy. ( 4 ) A proof of the correspondence between the new algorithm and the existing best known algorithm. ( 5 ) And finally, a few concluding remarks."
SP:b2439973063e827b3cbe92306a2fdee3286b6b44,"This paper considers a variant of contextual linear bandits motivated by routing applications in navigational engines and recommendation systems. In this variant, the learner is presented with a hidden d - dimensional value w and a set of actions A, R, where A is a subset of actions on the set R. The goal is to learn a hidden utility function w which, when multiplied with the set A, yields the hidden value w. The paper provides two algorithms for this problem which achieve regret O(d log T ) and exp(O(log d ) ), respectively. It also considers another variant where the user is allowed to make a list of several actions and the learned utility function is the best one on the list. In the first variant, regret is given by computing the log of the distance between the true value and the hyperplane the oracle returns on if the recommendation to take the action is lower than the utility value. The second algorithm is designed for the second variant, where the list size is limited to poly(d ).   The main contributions of the paper are the following :"
SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"This paper introduces Lale, an open - source sklearn - compatible AutoML library, and evaluates it with a user study. The goal of Lale is to simplify gradual AutoML by providing a way for data scientists to control certain choices and AutoML explores the rest. Lale uses combinators for modularity, and uses schemas both for search spaces and for type - checking. It is more expressive than the high - level interfaces of prior AutoML tools. While Lale achieves a high level of sklearn compatibility, Python code that explicitly reflects over dynamic types can still expose differences. The authors mitigate this by testing that Lale interoperates well with a broad set of features."
SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"This paper studies the effect of meta - learning in terms of the amount of information meta - learners ( i.e., how many parameters are meta - learned ) on the generalization error of neural network weight initialization. It shows that per - parametrized learning rates for the look - ahead version of MAML exhibit patterned sparsity, where a large fraction of learning rates drops to zero. This phenomenon is observed for both continuous learning problems ( few - shot and continual learning ), and it is also observed for a more expressive model ( the Bayesian neural network ). The paper further shows that the pattern of sparsity is dependent on the problem and can be enhanced or eliminated depending on the setting. For example, in the case of continual learning, the paper finds that it is better for the model to learn only a small fraction of parameters, while for continual learning it is best to learn all of the parameters. Moreover, it is shown that learning rates increase as the number of parameters increases in proportion to the size of the reward function, which indicates that the meta - learner is beginning to adapt to the task at hand. Taken together with previous work, these findings suggest that learning by sparse gradient descent is a powerful inductive bias for meta learning systems."
SP:05037e1850003a725a466b64d3e32aa2aed458fb,"This paper proposes a new model for multi - view learning, called ShICA, that models each view as a linear transform of shared independent components contaminated by additive Gaussian noise. Theoretically, the authors show that this model is identifiable if the components are either non - Gaussian or have enough diversity in noise variances. They also show that even a small amount of sampling noise makes Multiset CCA, which is used to fit the ShICA model, to have large error in the estimation of unmixing matrices. To address this issue, they propose to apply joint diagonalization to the result of multiset correlation analysis, leading to a new method called Shica - J. While ShICA - J is based on second - order statistics, they further propose to leverage non -Gaussianity of the components using a maximum - likelihood method, ShICA-ML, that is both more accurate and more costly. Experiments on fMRI and MEG data demonstrate that the method outperforms existing GroupICA and IVA methods."
SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"This paper proposes a novel multi - agent reinforcement learning method, called Figurative Co - Play ( FCP ), to learn agents that can collaborate well with humans without using human data. The method is based on the idea that self - play ( SP ) or population play ( PP ) agents overfit themselves to their training partners and do not generalize well to humans, while Behavioural Cloning Play ( BCP ) agents are more generalizable to new human co - players. The key idea is to produce a diverse set of training partners by selecting a population of agents that are good at playing with each other and their checkpoints. The goal is to select the agent that is the best response to the current checkpoint taken by the population. To do this, the authors propose to introduce a new state - of - the - art cooperative learning agent, which learns to play with a partner that responds to the checkpoint of the current partner. The proposed approach is shown to outperform BCP agents in a number of experiments in a simulated kitchen environment, and it outperforms SP and PP agents as well."
SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"This paper presents FACMAC, a multi - agent actor - critic method that learns decentralised policies with a centralised but factored critic, working for both discrete and continuous cooperative tasks. The centralised policy gradient estimator is a non - linear monotonic function that optimises over the entire joint action space, rather than optimising over each agent’s action space separately as in MADDPG. This method also employs a nonmonotonic factorisation to solve tasks that can not be solved with monolithic or monotonically factored critics. The paper evaluates FACMAC on three different domains : particle environment, MuJoCo benchmark, and a challenging set of StarCraft II micromanagement tasks. Empirical results demonstrate FACMAC ’s superior performance over MADDPGs and other baselines."
SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,"This paper proposes a biologically plausible plasticity rule for the storage of long - term memories. The proposed model is based on the classic Hopfield network, which relies on Hebbian plasticity for dynamics and attractor dynamics for recall. The authors propose to instead use a three - factor rule for storing and reading out the keys to the memory. The key - value rule consists of three components : ( 1 ) storing the current state, ( 2 ) the previous state, and ( 3 ) the current hidden state. The three components are applied sequentially at each time step of the memory process.   The authors evaluate their model on auto - associative and continual recall tasks, as well as sequence learning and heteroassociative memory tasks. They find that their model performs on par with the classical Hopfield networks. Moreover, they show how the feedforward structure of their model is compatible with slot - based networks, which is more biologically plausible."
SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"This paper proposes simple stochastic and online gradient descent methods for pairwise learning. The main difference from the existing studies is that they only pair the current instance with the previous one in building a gradient direction, which is efficient in both the storage and computational complexity. The authors develop novel stability results, optimization results, and generalization error bounds for both convex and nonconvex problems. They introduce novel techniques to decouple the dependency of models and the previous instance in optimization, and they perform excess generalization bounds, stability analysis, and optimization error of their algorithms. The proposed algorithm, Algorithm 3, is based on the iterative localization technique, and it provides utility bound, which matches the lower bound in the previous work."
SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"This paper proposes REDO, a class - agnostic framework to reconstruct dynamic objects from videos. Dynamic objects are defined as those that can be divided into three categories : rigid, non - rigid, and articulation. To handle the different types of object dynamics, REDO introduces a canonical 4D implicit function, which is pixel - aligned with aggregated temporal visual cues, and a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. The paper provides extensive experiments on synthetic RGBD video datasets SAIL - VOS 3D and DeformingThings4D++, and on real - world video data 3DPW, and shows that REDO outperforms state - of - the - art dynamic reconstruction methods by a margin. In addition, ablation studies are performed to validate that each developed component, including the canonical function, outperforms previous methods on various benchmarks."
SP:8ae97752e74b4395774575009031abcb6ba5cea7,"This paper provides a non - asymptotic analysis of linear stochastic approximation ( LSA ) algorithms with fixed stepsize. This family of methods arises in many machine learning tasks and is used to obtain approximate solutions of a linear system Ā✓ = b̄ for which Ā and b can only be accessed through random estimates { (An,bn ) : n 2 N 2 N⇤ }. The analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight and of high probability. In particular, it is shown that the bounds with order depending on the stepsize and the number of iterations can not be improved without additional assumptions on the sequence of random matrices.   The main contributions of the paper are :   1. High probability bounds on the performance of LSA under weaker conditions on the predicted $ An : n^2 $ sequence { ( An,bn) : n ^ 2 N \leq n - 1 $ with probability at least 1 $ that hold for any 2 ( 0, 1 ). The bounds are proven to be polynomial in order, and   2. The high probability bound for LSA necessarily has Polynomial dependency in, leading to a ‘ heavy - tail ’ phenomena. 3. The leading terms of the bound which contain the covariance matrices appears in the central limit theorems. 4. The results do not require the matrices An to be symmetric."
SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"This paper extends the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes ( MDPs ) to average - reward processes ( ARPs ). The main contributions include general convergent off - policy inter - option learning algorithms, intra - option algorithms for learning values and models, as well as sample - based variants of learning algorithms. The convergence of the proposed algorithms and convergence proofs extend those recently developed by Wan, Naik, and Sutton ( 2021 ). They also extend the notion of option - interrupting behavior from the discounted to ARP formulation, and thus extend the Shu et al. ( 1999 ) algorithm to the ARP setting. Finally, the algorithms are validated on a continuing version of the Four - Room domain."
SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"This paper considers the visual transformers ( VTs ) as an architectural alternative to Convolutional Neural Networks ( CNNs ). Different from CNNs, the design of the VT can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of typical convolutional inductive bias makes these models more data hungry than common CNNs. Some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. The authors propose an auxiliary self - supervised task ( Ldrloc ) which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data is scarce. They empirically analyse different VTs, comparing their robustness in a small training set regime, and show that their performance largely varies when trained with small - medium datasets, and that CvT is usually much more effective in generalizing with less data. Moreover, they propose a self - supervision auxiliary task to regularize VT training, which is inspired by the localization task in [ 12 ]. They show that this task can improve the corresponding baseline accuracy, usually by a significant margin, and sometimes dramatically ( up to +45 points )"
SP:0132ef17585e293b23e9dc45189c0989d829b52a,"Hyperbolic Procrustes Analysis ( HPA ) is a new method for label - free alignment of hyperbolic data in the Lorentz model. HPA consists of three components : translation, scaling, and rotation. The proposed scaling and translation operations align the first and second Riemannian moments as well as a wrapped rotation that aligns the orientation in the hyperboloid model. The theoretical analysis of HPA provides further insight and highlights properties that may be useful for practitioners. Empirically, HPA achieves good performance on three batch correction tasks involving gene expression and mass cytometry data."
SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"This paper studies the tradeoff between accuracy for a single query ( “ sum query ” ) and the accuracy for its component sub - populations ( ‘ point query ’ ) when generating microdata. The paper shows that for pure differential privacy, for some population of interest, the accuracy of a query can degrade by a logarithmic factor for a subset of the population. This is because of an uncertainty principle that controls the trade - off between accuracy in favor of accuracy for the sum query vs. accuracy for each of the subpopulation under the consideration of the microdata requirement.   The paper provides lower bounds for pure, approximate, and concentrated differential privacy. The authors also propose some algorithms inspired by their lower and upper bound proofs, for mitigating the effects of this uncertainty principle. They create a collection of benchmark datasets that can be used for public study of this problem."
SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"The paper proposes a goal - conditioned reinforcement learning ( RL ) framework called CO - Pilot ( “ Learning to Plan and Learning to Teacher ” ) to learn from each other on long - horizon tasks. The goal is to solve the problem of how to train a RL policy ( planner ) and an RL agent ( agent ) to improve each other ’s performance in order to overcome their own drawbacks : ( a ) planning policy needs to find the shortest path to a distant goal that provides dense reward/guidance ; ( b ) without a precise environment model, RL policy exploration is inefficient ; and ( c ) planning is inaccurate without a good environment model. The framework accomplishes this by training the planner and the RL agent to interact with each other via a curriculum of tree - structured tasks that the planner learns to sample from ; ( i ) the planner decomposes a long horizon task into a few sub - tasks at first and then gradually increases the interpolated sub - tasks, forming an easy - to - hard curriculum to train the planning policy ; ( ii ) the RL policy is trained to complete finer sequences of bottom layers and then progressively move to harder ones in the top layers ; ( iii ) each layer of the curriculum is repeated until both policies are optimized for each other. The method is evaluated on navigation and continuous control tasks and compared with other RL - based methods ( SAC, HER, PPO, RRT, SGT, SoRB )."
SP:9911693a04a300b5a93634fb0267ef83e5489d77,This paper proposes a new Bayesian approach to generate reliable and consistent explanations for black box models. It builds on top of existing methods such as LIME and SHAP by first obtaining a pointwise estimator of the feature's importance and then using it to estimate the associated uncertainty. The uncertainty is modeled using a mixture of logistic regression and Bayesian inference. The authors then propose a Bayesian estimator for the number of perturbations to be used in estimating the uncertainty and a new sampling method based on BayesLIME and BayesSHAP. Experiments on several benchmark datasets show that the proposed approach yields explanations that are both stable and more consistent than those of the state - of - the - art methods.
SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"The paper studies the potential problem of heavy tails in dermed neural networks ( ANNs ) and finds that it could be the key component which prevents ANNs from achieving superior classification performance since fatter tails tend to overlap in feature space. The authors propose to pre - define the feature distributions in order to model the heavy - tails and embed them in the loss function of the classifier head by using a mixture of multivariate Skew Laplace distributions, which is shown to have higher - order moment skewness. Experiments conducted on several benchmarks and comparison with other distributions demonstrate the effectiveness of proposed approach for boosting the performance of ANNs. The proposed SLAC - ANN consistently achieves superior performance."
SP:cbccb65457564992d534504c0d060da44cafce8c,"This paper formalizes Gradient Starvation ( GS ) as a phenomenon that arises when training with cross - entropy loss in neural networks. It is shown that GS can slow down the learning of certain features, even if they are present in the training set. The authors derive spectral decoupling ( SD ) regularization as a possible remedy to GS, which is a type of regularization that aims to decouples feature learning dynamics. Experiments are conducted on simple out - of - distribution ( OOD ) classification tasks."
SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"This paper evaluates human - AI teaming in the cooperative card game Hanabi, with both rule - based and learning - based agents. In addition to the game score, which is used as an objective metric of the human -AI team performance, they also quantify subjective measures such as human’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. They find that humans have a clear preference toward a rule -based AI teammate ( SmartBot ) over a state - of - the - art Learning - based AI teammate, ( Other - Play ) across nearly all subjective metrics, and generally view the learning based agent negatively. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human - artificial teaming rather than a singular focus on objective task performance."
SP:2a05e333fc1a14057515ef3addde9a40152373db,"The paper proposes a GAN - based method for the task of visual question generation ( VQG ), which is a task that aims to generate human - like neural questions from an image and some other information ( e.g., the answer ). The paper proposes to solve the task by using a novel approach called Double - Hints guided Generative Adversarial Networks ( DH - GAN ). It consists of a visual hints predictor with a cross - modal reasoning module to determine the salient visual regions of interest for each question and a question generation module with predicted visual hints and textual answer hints. The discriminator is to distinguish whether a sample triplet ( i.e. image, answer, and question ) is generated from the generator or ground truth. Moreover, it design a novel hybrid reward function that combines the generator and discriminator so that it encourages the model to perform both question sample and the visual hints via policy gradient. Experiments are conducted on two benchmark datasets and the proposed method outperforms the state - of - the - art approaches by a large margin."
SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"This paper proposes Generalized Data Weighting ( GDW ) to simultaneously mitigate label noise and class imbalance by manipulating gradients at the class level. GDW unrolls the loss gradient to class - level gradients by the chain rule and reweights the flow of each gradient separately. In this way, GDW achieves remarkable performance improvement on both issues.   GDW efficiently obtains class-level weights without introducing any extra computational cost compared with instance weighting methods. Specifically, GD W performs a gradient descent step on class -level weights, which only relies on intermediate gradients. Extensive experiments in various settings verify the effectiveness of GDW."
SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"The paper proposes a language - based language grounding task for learning grounded language for embodied agents. Specifically, the task is to learn a truth function that can predict if a given sentence is true of temporally - extended observations of an agent interacting with a collection of objects. This task is formulated using a synthetic language that includes time - extended predicates in past and present tense as well as spatio - temporal references to objects in the scene. Various transformer architectures are considered, including Transformer, LSTM, and LSTK. The latter implement different attention computations between words and objects across space and time. The authors test models on two classes of generalization : 1 ) generalization to randomly held - out sentences, and 2 ) generalisation to grammar primitives. They observe that maintaining object identity in the attention computation of the Transformer is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance."
SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"This paper proposes Prototypical Cross - Attention Network ( PCAN ), a method for multi - object tracking and segmentation ( MOTS ) in videos. PCAN first distills a space - time memory into a set of prototypes and then employs cross - attention to retrieve rich information from the past frames. Experiments on the popular BDD100 K dataset and the semantically diverse YouTube - VIS dataset show that PCAN outperforms existing video instance tracking and video segmentation algorithms on both the popular datasets.   The main contribution of this paper is the prototypical appearance module for efficiently utilizing long - term spatio - temporal video information. In contrast to most previous MOTS methods with limited temporal consideration, PCAN efficiently performs long -term temporal propagation and aggregation, and achieves large performance gain on the two largest MOTS benchmarks with low computation and memory cost."
SP:1175ad16382b349ab1a39895150172d266abe571,"The paper studies the role of gradient flow in the optimization of deep neural networks. Gradient flow is a formalism that allows one to view optimization as an approximate numerical solution to the initial value problem of gradient descent. The paper shows that the degree of approximation depends on the curvature around the gradient flow trajectory. It is shown that over simple networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent ( e.g. gradient descent ). This finding allows the paper to translate gradient flow into a guarantee that gradient descent efficiently converges to global minimum almost surely under random initialization. Experiments suggest that gradient flow with conventional step size is indeed close to gradient descent, and the paper hypothesizes that the theory of gradient flows will unravel mysteries behind deep learning."
SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"This paper considers a stochastic multi - armed bandit ( MAB ) problem with delayed impacts. In particular, actions taken in the past impact the arm rewards in the subsequent future. This delayed impact of actions is prevalent in the real world, and this paper generalizes the bandit setting to encode the dependency of this “ bias "" due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. The proposed algorithm achieves a regret of $ \tilde{KT^2/3 } $, where K is the number of arms and T is the learning horizon.   The learning formulation builds on the rich bandit learning literature [ 42, 3 ] and is related to non - stationary bandits. Its techniques share similar insights with Lipschitz bandits   and combinatorial bandits. However, the setting and formulation of this paper are different from the ones studied in the previous works. In the first half, the paper studies the one - step delayed impacts or sequential learning with full information. The second half studies the long - term impacts of actions in sequential decision making under uncertainty. The Markovian learning formulation bears similarity to reinforcement learning since our impact function encodes memory. The reward structure is similar, although the focus is on the exploration - exploitation tradeoff in bandit formulation."
SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"This paper proposes a new model for video instance segmentation ( VIs ) based on transformers. It is based on the per - clip approach which leverages the information from multiple frames to construct a frame - to - frame summary. The main difference from the previous approaches is that the proposed model does not require full space - time attention for information passing between frames. Instead, it uses a memory encoder which generates tokens representing the context of each frame. Each token represents a unique embedding of the features of a frame, which is correlated with other frames through exchange of information between the precisely encoded memory tokens. The proposed Inter - frame Communication Transformers ( IFC ) model achieves state - of - the - art performance ( AP of $ 42.6 $ on YouTube - VIS 2019 val set using the offline inference ) while having a considerably fast runtime ( $ 89.4 $ ). The code is available at GitHub.  "
SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"This paper proposes a general graph embedding method, called residual2vec, that can be used to replace the bias induced by random walks on graph embeddings. Specifically, the proposed method replaces the random walk with skip - gram negative sampling ( SGNS ), which happens to negate the bias caused by the friendship paradox. The authors also propose a more general framework, called word2vec that can also compensate for other systematic biases in random walks, such as the degree of each node. The experimental results are conducted on two tasks : link prediction and community detection. Using a citation graph of 260k journals, the authors demonstrate that the biases from random walks overshadow the salient features of graphs. By removing the bias, residual1vec better captures the characteristics of journals such as impact factor and journal subject."
SP:851eac96135b577a5014166edcb43db6a190cf4b,"This paper studies estimating non - linear functionals of discrete distributions under differential privacy constraints in the setting of local differential privacy ( LDP ), where the initial data x, a, xn are supposed to belong to an unknown discrete distribution p = ( p1,..., pK ), and only α - locally - differentially private ( locally - differingially private ) samples are publicly available. The paper provides lower bounds on the quadratic risk for estimating the power sum functional Fγ, defined as a function of K, n, α, and the dimension K. The paper considers two plug - in type estimators of F.^k, for all $ \mathbb{R}^k$, $ \gamma$, and $ k$-1/\sqrt{K }$, which are similar to the MLE analyzed by Jiao et al. [ 18 ] in the multinomial model. However, due to the privacy constraint, the rates obtained by the authors are slower and similar to those obtained in the Gaussian model by Collier et al [ 9 ]. In the non - interactive case, the paper introduces a two - step procedure which attains parametric rate ( nα2)−1/2 when $ \� < 1 $, and a rate of $ \leq \frac{\log n-1}{\log k/2}$ for $ \log n/\sim pk$ where $ pk=1$."
SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"This paper proposes GAPPLETRON, a surrogate regret - based algorithm for online multi - class classification. In this setting, the learner ’s feedback graph is an arbitrary directed graph. The main contribution of this paper is to provide surrogate regret bounds that hold for a large class of loss functions, such that there is a constant surrogate regret of order BK. The paper also provides a general lower bound of order max { BK, \sqrt{B }, \tilde O(T, B ) }, which is not significantly improvable. Experiments on synthetic data show that for various feedback graphs, GAPPLERON is competitive against known baselines, such as GAT and GCN, in terms of its surrogate regret."
SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"This paper studies the problem of explainable clustering of k - means under the ICLR 2020 setting, where the objective is to find k - mean weighted sum of the weighted average of the k - median of all samples sampled from a reference clustering. The authors give a lower bound that matches the previous upper bounds of O(k log k ) and O(log k ), respectively, for the optimization of k-means. This is in contrast to prior work ( Dasgupta et al., [ 6 ], Laber and Murtinho. [ 10 ] ), which gave an upper bound in the $ \mathcal{O}(\log k)$ range for explainable K - median and K - means, respectively. The main contributions of the paper are as follows :   - Given an unconstrained ( non - explainable ) reference cluster, find an explainable one that, when applied to the data, is equivalent to an optimal one ( i.e., does n n n not matter how many data points there are, and is oblivious to the number of data points n ), for a query $ n=0 $, the authors give an algorithm that achieves this result. The algorithm is remarkably simple, in that, given an initial not necessarily explainable reference cluster of size $ k$, it is the same as and equal to the initial value of $ \text{k}$ of the reference cluster ( in the naive way ), but when scaled by $ k$. The main difference is that when $ k=\k$, the original $ n$ is replaced by $ \dots$ where $ d$ is the size of the query and $ \theta_k$ is a threshold of the original reference cluster.   The authors show that this explainsable one can be applied to instances where $ k \leq k$ is not too large ( for some reason, e.g. when the query is a simple one - dimensional vector, such as when the size is small enough that $ n < 1 $, and the query does n’t matter too much in other dimensions. They also give lower bounds that match those of the previous work, in particular in terms of the ratio of K(n)$ and K(k )$.   In particular, the upper bound they give is in the limit of a low - dimensional setting, in which $ d=\dots \mapsto n$ and the scaling factor is linear in the dimension of the sample size."
SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"This paper proposes a multilingual language model that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre - training objective but also learned representation in the model. This model brings unprecedented PrLM interpretability and convenience for downstream task use. The proposed model outperforms two popular multilingual PrLM, multilingual - BERT and XLM - R, on cross - lingual natural language understanding ( NLU ) benchmarks and linguistic structure parsing datasets, demonstrating the effectiveness and stronger cross - linguistic modeling capabilities of the proposed model."
SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"This paper presents a novel model based on transformer architecture for solving vehicle routing problems. The proposed model is called Dual - Aspect Collaborative Transformer ( DACT ). DACT learns separate embeddings for node and positional features, and is equipped with cyclic positional encoding ( CPE ) to capture the symmetry of VRP solutions. The authors train DACT using proximal policy optimization ( PPO ) and curriculum learning ( CL ). They apply DACT to solve the traveling salesman problem ( TSP ) and capacitated vehicle routing problem ( CVRP ). Results show that DACT outperforms existing Transformer based improvement models, and exhibits much better generalization performance across different problem sizes."
SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"The paper considers the importance of Bayes error in assessing the inherent difficulty of a given data - driven classification problem to establish absolute benchmarks and evaluate progress in the field. The paper shows that it is possible to learn generative models with exact Bayes errors via normalizing flows. The key insight is that the error of a generative model is invariant under invertible transformations of the underlying data distribution. Therefore, the authors can directly compute the exact error of the learned flow model by computing it for Gaussian base distributions. The method generates synthetic datasets that closely resemble standard benchmark datasets, but with exactly controlled Bayeserror. The authors then conduct a thorough investigation of state - of - the - art classification models, and find that in some — but not all — cases, these models are capable of obtaining accuracy very near optimal. Finally, the method also uses the method to evaluate the intrinsic "" hardness "" of standard benchmark dataset, and ii) hardness caused by the internal data distribution p. The focus of this work is about the latter : the hardness of p is measured by the relationship between EBayes error and that of the test distribution."
SP:2896679f0472522bc3334178cd7574494cf12b7b,"This paper proposes GradInit, an initialization method for neural networks that is applicable to any convolutional or post - lN transformer architectures. GradInit modifies the scaling factor of each parameter in a given network by learning a scale factor for each randomly initialized parameter block of a network, so that the training loss evaluated on a different minibatch after one gradient step of a specific stochastic optimizer such as SGD or Adam with prescribed hyperparameters is minimized. The initialization learned by GradInit often decreases the gradient variance for most of the parameter blocks. The experiments demonstrate that GradInit accelerates the convergence and improves the test performance of a variety of architectures on image classification tasks. It also enables training the Post - LN Transformer without any form of learning rate warmup, even for SGD. Finally, the experiments also provide a useful tool for analyzing the learned scaling coefficients and their impact on gradient variance."
SP:f69731403592fa5bdd4ca327708582d615aa131c,"This paper proposes a new approach for predicting disease progression through linear mixed - effect models. The proposed method embeds the data in a Riemannian manifold and learns patient - specific trajectories distributed around a central geodesic. A few interpretable parameters characterize the subject trajectories at the cost of a prior choice of the metric, which determines the shape of the trajectories. The authors extend this approach by learning the metric from the data, which allows more flexibility while keeping the interpretability. Specifically, they learn the metric as the push - forward of the Euclidean metric by a diffeomorphism that is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The metric update allows the metric to be used to improve the forecasting of imaging and clinical biomarkers in the Alzheimer’s Disease Neuroimaging Initiative ( ADNI ) cohort. The results compare favorably to the 56 methods benchmarked in the TADPOLE challenge."
SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"This paper proposes a routing - by - memory mechanism for CNNs. In each stage of the network, it introduces parallel Procedural Units ( PUs ). A PU consists of a memory head and a procedure. The memory head maintains a summary of a type of features. For an intermediate feature, we search its closest memory and forward it to the corresponding procedure. In this way, different procedures are tailored to different features and therefore tackle them better. The authors also employed memories to forward different features to their expert PUs. According to the results on Tiny ImageNet, ImageNet and CIFAR - 100, our RMN significantly improves VGG - 16, ResNet - 18, ResNets - 50, and EfficientNet - B0 while not increasing the computational cost."
SP:d240173080cd3647dbaa5173a6422396f226775b,"This paper analyzes the equivariance of certain symmetries in classical physics. The authors show that it is possible to parameterize a nonlinear O(d)-equivariant polynomial function as a composition of scalar products and scalar contractions. More specifically, given a set of symmetry - invariant functions $ \mathcal{O}(\delta)$ and a symmetry - equivariant group $ G(x)$, one can express the nonlinear function as $ \tilde\Omega(d)$ where $ d$ is a scalar field and $ x \in \mathbb R^\mathbb{R}$ a vector, $ z$ a subspace vector, and $ \nabla_t $ are scalar vectors that are locally invariant to $ \text{x}$ and $ z^\infty$ functions. Equivariant functions can then be expressed as a collection of scalars $ x^*^^^ *, where $ x$ is the scalar product of one scalar and one of the subspace vectors. The paper shows that this scalar - based representation is equivalent to the Euclidean, Lorentz, and Poincaré group of functions. It also shows that the vector - based method is simple, efficient, and scalable. Numerical experiments are provided that illustrate some of the results."
SP:72c0f47566904deb27d8157da30807ec1d6b5685,"This paper proposes a new generalization formula for the popular intersection over union ( IoU ) loss, α - IoU, to replace the IoU loss in bounding box ( bbox ) regression. The new formula has the following properties :   1. It preserves the order of the objects by only changing the parameters of the power function $ \alpha$ of the intersection between two IoU objects when computing the loss function. 2. It introduces a new power regularization term $ p(a, r)$, which can be used to adjust the power of a given IoU term by changing its radius $ r$. 3. It defines a new class of bbox regression loss function based on this new regularization, which is called α - AoU loss. 4. It provides a way to up - weight the gradient and loss of a particular IoU object by changing the radius of its power vector $ r(a,r)$.   The main contributions of the paper are the following : 1. Introducing a new family of loss functions based on the power terms of IoU losses. This new line of losses can be formulated as a weighted sum - product of two terms of power terms, where each term represents an IoU bound and a power parameter. The second term is the result of an application of these two terms. The authors define the third term as a product of the first and second IoU terms.   2. They analyze the effect of reweighting the resulting loss and gradient of changing the parameter $ r$ on the object bounding boxes. 3. They show that this can improve the performance of a benchmark bbox model on high - AP data."
SP:397125177d7007316d67194ec00d5dc57b44ac79,"The paper considers an imitation learning problem where the reward function is not given, but demonstrations from experts are available. In this setting, distributionally robust robust imitaiton learning ( DROIL ) is defined as learning a policy that is robust to noisy demonstrations based on an adversarial construction that avoids optimistic generalizations of the demonstrated data. The authors show that DROIL can be seen as a framework for maximizing a general entropy function that is defined based on a particular loss of interest. They then develop a novel approach to transform DROIL ’s objective into a convex optimization problem over a polynomial number of variables for a class of loss functions that are additive over state and action spaces. Their approach lets us optimize both stationary and non - stationary policies and, unlike prevalent previous methods, it does not require repeatedly solving an inner reinforcement learning problem. They experimentally show the significant benefits of DROIL’s new optimization method on synthetic data and a highway driving environment ( discrete / low - dimensional discrete state - action space )."
SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"This paper proposes general post - processing algorithms for individual fairness ( IF ). In this setting, the learner only has access to the predictions of the original model and a similarity graph between individuals guiding the desired fairness constraints. The authors cast the IF post - Processing problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired “treat similar individuals similarly ” interpretation. Theoretical results and empirical results theoretically and empirically demonstrate the connection of the new objective function to a local relaxation of the originally individual fairness. Empirically, the post - processed algorithms correct individual biases in large - scale NLP models such as BERT, while preserving accuracy."
SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"This paper proposes a structure - aware dual graph aggregation network ( SADGA ) for the cross - domain Text - to - SQL ( T2C ) task.    The main idea is to combine the dual graph encoding for both the natural language question and the database schema, and the question - schema linking method to learn the mapping between words in the question and tables /columns in the database schemas. The proposed network has three components : global graph linking, local graph linkinging, and DualGraph Aggregation Mechanism. Global graph encoding is used to represent the graph structure of the question graph. Local graph linking and Dual graph aggregation are used to capture the structure information of the questions graph and schemas graph. The last part of DualGraph aggregation is based on the fact that relational databases use embeddings that are unique to each row / column in the query."
SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"This paper studies the problem of learning stochastic computation graphs, that is, graphs that have more than one discrete probability distribution in each execution path. The main contribution of this paper is to propose two tricks to improve the training behavior of such graphs, namely the Gumbel - softmax trick and the dropout residual connection. The key insight of the paper is that it is challenging to optimize the parameters of these models, mainly due to small gradients and poor local minima. The authors then propose two new strategies to overcome these challenges, i.e., increasing the scale parameter $ \beta$ and increasing the size of $ \gamma_k$ Gumbels, respectively. Experiments on several benchmark datasets demonstrate that the proposed methods enable the training of complex discrete - continuous models, which could not be learned before. These models beat prior state - of - art models without discrete components on several benchmarks and show a remarkable gain in generalization behavior."
SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"This paper studies approximate Bayesian inference with Hamiltonian Monte Carlo ( HMC ) with a Bayesian neural network ( BNN ). It shows that HMC with the typical Bayesian average can suffer from covariate shift and generalizes poorly when the covariate is orthogonal to the weights. The authors propose three methods to mitigate this issue : ( 1 ) train the BNN via approximate least squares method, ( 2 ) generalize the approximate inference via MAP method, and ( 3 ) propose novel priors to improve the generalization."
SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"This paper categorizes meta - few - shot learning evaluation as a two settings : in - distribution ( ID ) and out - of distribution ( OOD ). While most meta - learning theory and some FSL applications follow the ID setting, the paper finds that most common FSL benchmarks instead reflect OOD evaluation, as they use disjoint sets of train ( base ) and test ( novel ) classes for task generation. This discrepancy is problematic because, as the paper shows on numerous benchmarks, meta - learned methods that perform better on OOD datasets may perform significantly worse in the IDs setting. The paper also highlights concerns in 1 ) reliably performing model selection for a given meta - learner method, and 2 ) consistently comparing the performance of different methods. To address these concerns, the authors propose new benchmarks for ID FSL and OOD FSL as well as suggestions on how to modify existing OOD benchmarks to allow for ID evaluation."
SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"This paper proposes a new method for rule induction based on language model ( LM ) instead of rule induction from a knowledge base ( KB ). The motivation is that the current KB - based methods “ learn rules ” while the current LM - based method is “ learning rules from data ”, which limits the expressive power of the proposed method to only produce “ canned “ rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power   of LMs for free text. Therefore, in this paper, the open rule induction problem is proposed, which aims to induce open rules utilizing the knowledge in LMs. Besides, this paper proposes Orion, which automatically mine open rules from LMs without supervision of annotated rule. Experiments are conducted to verify the quality and quantity of the inducted open rules. Surprisingly, when applying the open rules to downstream tasks ( i.e. relation extraction, these automatically inducted rules even outperformed the manually annotate rules, even when they were manually annotated )."
SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,"This paper proposes Implicit Constraint Q - learning ( ICQ ), a multi - agent offline learning method that learns from a dataset of state - action - actions taken by all the agents but with extrapolation error due to the large number of agents. The main idea of ICQ is to only use the action - value pairs from the dataset as input when performing value inference in the offline RL setting, i.e., joint policy. To achieve this, the authors propose implicit constraints on the joint action taken by each agent to enforce a constraint on the action value to be maximized over the entire state space. The proposed method is evaluated on a set of offline RL problems based on the cooperative offline learning of agents in StarCraft II. The results show that ICQ outperforms the single agent equivalent of D4RL, DQNRL, and TDRL across a variety of offline settings, from discrete control to continuous control."
SP:1939b24b68970c33ca16ce238deed257f76d009e,"This paper proposes a method to generate non - uniform perturbation sets to improve the adversarial robustness of deep neural networks ( DNNs ) with respect to evasion attacks. It is well known that uniform norm - ball constraint results in non - recognisable perturbations that are difficult to train. This paper aims to address this problem by proposing a methodology to generate such a set that takes into account the characteristics of the empirical data distribution, both on the correlation between the features and the importance of the features themselves. The proposed method is validated on three applications : Credit card default prediction, malware classification, and spam detection. On all three applications, it is shown that the proposed method improves the robustness over the baselines that were trained by using uniform norm bounds. On the final application, it further provides robustness certification using non - Uniform bounds, and shows that non - uniform bounds achieve better certification."
SP:417b30930b245667d777e5d90ee80dd41546760e,"This paper extends the Tikhonov regularization scheme to generalized self concordant loss functions ( GSC ), which contain logistic loss. In particular, the authors show that, when applied to the least square setting, the iterated Tikhonev regularization ( IT ) scheme ( i.e. proximal point iterations ), corresponding to minimizing the residuals, achieves better learning rates than the classical $ \tilde{T}(\log n)$ for GSCs under certain assumptions ( e.g., the capacity and source of the problem ). They also show that IT overcomes a limitation of the original IT scheme ( which is unable to adapt well to different kinds of learning problems ). The main result is a probabilistic upper bound on the excess risk, which is optimal given usual assumptions on the learning task.  "
SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"This paper introduces a new kind of linear transform called deformable butterfly factor ( DeBut ) that generalizes the square PoT butterfly factor matrices [ 5, 6 ] and aims to replace CONV and FC layers in deep neural networks ( DNNs ) with this new layer. The proposed DeBut factor inherits the fine - to - coarse grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. The authors apply DeBut as a drop - in replacement of standard fully connected and convolutional layers and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy. Experiments have shown the superiority of DeBut linear transform over competing algorithms, especially for large networks, especially where the training and inference complexities matter."
SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"This paper proposes MetA Reusable Knowledge ( MARK ), a method to address the problem of catastrophic forgetting in continual learning. Specifically, the authors propose a method that maintains a set of shared weights across tasks to prevent catastrophic forgetting. The authors propose to use these weights as a Knowledge Base ( KB ) that is queried periodically to update with new task - specific weights. The weights that are queried from the KB are the same ones that are currently being used in the task. The idea of using a mask function to select the relevant weights for each task is the key to the success of the method. Experiments are conducted on CIFAR-100, Mini - ImageNet, and MNLI datasets and the proposed method achieves state - of - the - art results.  "
SP:722c52467e384058f8fdffa254d0e8db47440a64,"This paper proposes a data - driven framework for using primal heuristics in exact MIP solvers such that the primal performance is optimized. Central to the approach is a novel formulation of the learning task as a scheduling problem, consisting of an efficient data collection procedure, and a fast and effective heuristic for solving the learning problem on a training dataset. The main idea is to learn from data describing the performance of primal heuristic, such that a specific schedule of heuristic that collectively find many solutions at minimal cost can be computed. The authors then propose an efficient algorithm for computing such a schedule. Experiments are conducted on two classes of challenging MIP instances, and the proposed method is able to reduce the average primal integral of SCIP by up to 49 % w.r.t. the default setting.  "
SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"This paper studies the problem of learning when the only feedback provided to the learner is a binary label at the end of an episode. This setting is chosen because it is easier to evaluate whether a trajectory was either “ good ” or “ bad ”, but harder to provide a reward signal at each step. The goal of the paper is to prove that learning is possible in this more challenging setting. To do so, the authors study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sublinear regret. The main results are as follows :   ( 1 ) Under an explorability assumption, the paper shows that the proposed algorithm is statistically significant and efficient ( Section 2 ). ( 2 ) In Section 3, it presents a generalization of the algorithm and main results. ( 3 ) In section 4, it provides a comparison with existing work. ( 4 ) Section 4 points to other related work, ( 5 ) and a discussion in Section 5. ( 6 ) Experiments are provided in section 5."
SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"This paper proposes a novel edge representation learning framework based on Dual Hypergraph Transformation ( DHT ), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows the DHT to apply message - passing techniques for node representations to edges. After obtaining edge representations from the hypergraph, the authors then cluster or drop edges to obtain holistic graph - level edge representations. The authors validate DHT on hypergraphs and graph datasets for graph representation and generation performance, on which the proposed DHT method largely outperforms existing graph representation learning methods. Moreover, the proposed method also outperforms state - of - the - art graph pooling methods on graph classification, which is due to its accurate edge representations learning and lossless compression."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the problem of estimating the sufficiency of a state representation for learning and representing the optimal policy in the context of Mutual Information ( MI ) minimization, a common objective of reinforcement learning ( RL ). The paper provides a theoretical result that, under mild assumptions on the structure of the MDP, two commonly used MI objectives, mutual information maximization ( MIM ) and mutual information discounting ( MD ), can yield representations that are not sufficient for RL from a theoretical perspective. Empirical results on a simulated high - dimensional environment show that ( 1 ) MIM and ( 2 ) MD do not provide sufficient representations for learning in general, and ( 3 ) MD and MIM can not be used as an alternative to MIM in certain settings ( e.g., when the environment is difficult to learn and the agent needs to maintain a certain state in order to control the environment )."
SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"This paper introduces Sparse Steerable Convolution ( SS - Conv ), an extension of steerable convolution with sparse tensors, for the task of 3D object pose estimation. The main idea is to combine the steerer convolution layer with SE(3)-equivariant deep feature learning in order to make the feature learning process smoother and more efficient, while preserving the equivariance property. The implementation consists of two components : ( 1 ) a feature encoder and decoder module, and ( 2 ) a steerer - based feature refinement module. Experiments are conducted on three tasks of pose estimation, namely instance - level 6D pose estimation ( category level and full - size estimation ), and 6D category level pose and size estimation, and show clear improvements over the state - of - the - art methods.  "
SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"This paper aims to address the observation that the final prediction in vision transformers is only based on a subset of the most informative tokens, which is sufficient for image recognition. Based on this observation, the authors propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, they devise a lightweight prediction module to estimate the importance score of each token given the current features ; the module is added to different layers to pruning redundant tokens hierarchically. To optimize the prediction module in an end - to - end manner, they propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. The DynamicViT models achieve very competitive complexity / accuracy trade - offs compared to state - of - the - art CNNs and VIT transformers on ImageNet. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31% ∼ 37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5 %."
SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"This paper studies confidence intervals for conditional mean regression ( CMRL ) in settings where the underlying regression function E [ Y | X ] is continuous and the features X are finite ( in terms of their number of possible values ). More precisely, this paper studies the confidence intervals that can be obtained if and only if the effective support size of the distribution of X is smaller than the square of the sample size.    The main contributions of this paper are :   ( 1 ) This paper provides a new regime - finding for confidence intervals in CMRL under certain assumptions on the feature space. Specifically, it shows that any confidence interval that can guarantee a non - vanishing confidence interval for the mean under the continuous assumption that X is not too large ( even if the features are finite ), must have a sample - level vanishing - width guarantee. This is in contrast to previous work, such as [ 1 ], [ 2 ], and [ 3 ], which established that confidence intervals can be guaranteed for any sample size greater than some threshold, even if that sample size isn't necessarily large enough. The main result of this work is to show that there are regimes that exist for both finite and continuous - set CMRLs, and that there is a certain threshold value for which confidence intervals are guaranteed, and a distribution - wise upper bound for the lower bound, which is lower than some other threshold value. In particular, the authors show that for a finite - set of samples, there exists a regime that guarantees a confidence interval of size $ \epsilon$ such that the confidence interval is $ \leq \sqrtf$-1 $, where $ x_1 / y_1 = \sum_0 $ \hat{x_0,x_1}$ with probability greater than $ \epsilon$ for all samples that have value $ \in [ 0,1,0,0]$. For a continuously - distributed feature space $ X \in \mathbb{R}^n$, there is an upper bound that $ \ell_2 $ for all $ \psilon$ samples with value $ Y_0, \ldots, \sum_{0,1 } $ such that $ z_1 $ \theta \geq_0.\nabla_{\delta_0}$ is a lower bound. In other words, for a sample with a fixed number of values $ X : $ \eta_0 < \emph{X}$ and $ \cdots \in x_0 / \in\mathbf{Y}\leq_{1,\ldots}$ where $ \en\lambda_{0,\text{x})$ is the expected value of a sample that is drawn from a fixed distribution $ \lambda_{1 } / \cdot \en_0$.   [ 1] This paper shows that for any fixed - set feature space, there exist regimes that guarantee confidence intervals with a certain ( finite ) upper bound of $ \cup{Y}$ or $ \gamma_0 } $, depending on $ \frac { \ell_{1}{\mid}$, $ \text{y}_1 } - \sum { x_2 } $ and $ z _ 2 $, respectively. These regimes are referred to as the "" finite CMRL regime "" and "" continuous mean regime "". The authors also study methods for performing conformal prediction ( cross - validation ) and hold - out methods ( conformal holdout ), which are variants of the conformal method for estimating the covariance matrix of a CMRL bound. They compare and contrast the performance of these methods with respect to other existing methods ( e.g., cross validate, holdout, conformal ), and holdout variants of CMRL. They show that under certain conditions, the probability distribution - free methods are more or less guaranteed than the other methods."
SP:123952325765c040c3078fc7dca2b6d370e55590,"This paper proposes Representation Neutralization for Fairness ( RNF ), a new method to learn biased representations that discourage the classification head from capturing undesirable correlations between fairness sensitive information in encoder representations with specific class labels and the biased representations obtained by using biased representations as inputs. The proposed approach is based on the GCE framework. In contrast to GCE, which leverages instance - level annotations for sensitive attributes, in RNF, the sensitive attributes are generated by training a bias - amplified model that is trained to generate proxy annotations based on its confidence level. The confidence level is obtained by training the model on ground truth labels and then training the classifier on labeled samples that have the same ground truth label but with different sensitive attribute annotations. The sensitive class labels are learned by sampling from a distribution that is similar to that of the sensitive class label distribution. The classifier is trained on the ground truth labeled samples and then debiased according to the class label. Experiments are conducted on MNIST, CIFAR10, ImageNet, and Fashion MNIST. RNF is shown to significantly reduce discrimination of DNN models with minimal degradation in task - specific performance."
SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,"The paper proposes a new convolutional layer called Bessel - CNN that ensures rotational invariance in CNNs. The key property that ensures the invariance is the Bessel function. The Bessel functions are defined on a continuous hypersphere, and are parameterized by Lagrangian functions. These functions are then multiplied by a convolution, such that the resulting sum maximizes the sum of the original convolution and the multiplication. The result is a CNN layer that is said to be invariant to all possible rotations. The method is described in detail in the paper. Experiments are conducted to verify the effectiveness of the method."
SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,"This paper introduces ParK, a method that combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. In particular, constructing suitable partitions directly in the feature space rather than in the input space, for kernel ridge regression, the authors promote orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. The authors provide a simple analysis that characterizes the statistical-computational trade - off of a partitioned kernel estimator by the interplay of intuitive quantities. Moreover, they demonstrate the effectiveness of their method by numerical experiments on large - scale datasets. The main theoretical limitation of the work is the lack of a result connecting the proposed partitioning algorithm to the properties of the resulting partition."
SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"The paper proposes a method to learn discrete communication among emergent agents by using one - hot tokens instead of hot one hot vectors. One - hot is hot because it represents a hot set of tokens in a learned, continuous space, whereas discrete tokens are hot because they can be embedded in the learned space in a semantically meaningful way ( e.g. by embedding them with embedding techniques from NLP ). The paper argues that using discrete tokens in emergent communication reduces the need for zero - shot understanding, which is harmful in many real - world scenarios. Instead, the paper proposes an architecture and implementation for learning such discrete communication using a learned discrete communication architecture, which employ discrete tokens derived from a learned, continuous space.   The paper provides a decision theoretic analysis of the proposed architecture and shows that it outperforms existing methods that only consider one hot tokens. It also provides insights into self - play experiments that validate that the proposed method can learn to cluster the hot vectors in a meaningful way. Lastly, it demonstrates that the method can be used to learn human - understandable communication in a reference game and that humans can interpret unlabeled emergent communications."
SP:8630ccc627534f9033bced04e2137a897ffef701,"This paper proposes a new family of hybrid models called CoAtNets ( pronounced “coat ” nets ) that combines the strengths of depthwise convolution and self - attention in order to improve generalization and efficiency. The key insights of the paper are : 1 ) depthwise Convolution and Self - Attention can be unified via simple relative attention ; 2 ) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency, as it can be seen by looking at the image classification task. Experiments show that the proposed CoAtNet achieves state - of - the - art performance under different resource constraints across various datasets. For example, when pre - trained with ImageNet-21K with about 10 M images, CoAt net achieves 88.56% top - 1 accuracy when finetuned on ImageNet - 1K, while ViT - Huge achieves 90.88% with 300 M images from JFT - 300 M while using 23x less data."
SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,"This paper proposes a second - order oracle bound for the expected risk of a weighted majority vote. The bound is based on a parametric form of the ChebyshevCantelli inequality, which is amenable to efficient minimization. The new form resolves the optimization challenge faced by prior oracle bounds based on   the Chebysshev - Cantelli inequality and the C - bounds [ Germain et al., 2015 ], and at the same time, improves on the oraclebound based on second order Markov ’s inequality introduced by Masegosa et al [ 2020 ]. The authors also derive a new concentration of measure inequality,   which they name PAC - Bayes - Bennett, since it combines PAC-Bayesian bounding with Bennett ‘s inequality. Finally, the authors provide an empirical evaluation demonstrating that the new bounds can improve on the work of   Seldin et al   [ 2012 ]."
SP:5bac542a6532d43cf100e085398b4a4783719814,"This paper proposes a weakly - supervised framework to learn cross - modality representations for audio - visual video parsing. The framework consists of two modules : ( 1 ) Cross - Modality Semantic Representation Module, which learns the event semantics across videos to identify audio or visual events, and ( 2 ) Event Co - occurrence Module that explores the event co - occurrence across audio, visual and audio - audio - video streams. The key idea of the event module is to jointly attend to events of different modalities by performing self - attention and crossmodality co - attention on class - wise features, respectively. This module is trained in a similar way to the one used in ( Xie et al., 2020 ), which is supervised. The experimental results show that the proposed method performs favorably against existing methods on the benchmark dataset."
SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"This paper proposes a federated learning ( FL ) algorithm called QuPeD for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. Different clients may have different requirements for the compressed model ( both in model dimension and precision ), so the authors propose a compressed personalization framework by introducing knowledge distillation loss for local client objectives collaborating through a global model. The authors develop an alternating proximal gradient update for solving this compressed personalisation problem, and analyze its convergence properties. Numerically, the proposed algorithm outperforms competing personalized FL methods, FedAvg, and local training of clients in various heterogeneous settings."
SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"The paper proposes a Variational Auto - Encoder ( VAE ) based framework for constrained clustering that incorporates clustering preferences, with varying degrees of certainty, within the stochastic gradient variational inference ( SGI ) framework. In contrast to existing deep clustering approaches, the proposed VAE - based method DC - GMM uncovers the underlying distribution of the data conditioned on the prior clustering preference, expressed as pairwise constraints. These constraints guide the clustering process towards a desirable partition by indicating which samples should or should not belong to the same cluster. Extensive experiments are provided to demonstrate the superior clustering performances and robustness of the proposed method compared to unsupervised variational clustering methods on a variety of real - world datasets."
SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,"The Neural Tangent Kernel ( NTK ) is a non - linear approximation method for learning neural networks that characterizes the behavior of infinitely - wide neural networks trained under least squares loss by gradient descent. The computational complexity of kernel methods has limited their use in large - scale learning tasks, which motivates this paper to design a near - input - sparse approximation algorithm for NTK, by sketching the polynomial expansions of arc - cosine kernels. Similarly, the proposed sketch for the convolutional counterpart of NTK ( CNTK ) can transform any image using a linear runtime in the number of pixels, and the authors prove a spectral approximation guarantee for the NTK matrix, by combining random features ( based on leverage score sampling ) and sketching algorithm. The performance of the proposed methods has been benchmarked on various tasks, including regression and classification."
SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"This paper proposes a Transformer - based method for long - term 3D motion prediction for multi - person 3D trajectories. The proposed method is based on the observation that a human ’s actions and behaviors may depend on the other persons around, and the Transformer model considers each person by taking a corresponding pose as a query, which attends to both local - range and global - range features. The encoder operates on a two - stage process : ( 1 ) It considers individual motion and ( 2 ) global interaction at each step, which outputs a long - range motion with a correspond pose as the decoder. The decoder is trained to output a set of predictions for each individual person. ( 3 ) It is also trained to automatically group the persons into different social interaction clusters using the attention mechanism. ( 4 ) It performs experiments on multiple datasets including CMU - Mocap, MuPoTS - 3D [ 48 ], 3DPW [ 64 ], and Panoptic [ 24 ]. It outperforms the state - of - the - art methods on 3D ( with 2 - 3 persons ) and also generates diverse social interactions. More interestingly, it can even predict 15 persons simultaneously by automatically splitting them into different interaction groups."
SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"The paper proposes a method, Model Predictive Program Synthesis ( MPPS ), that uses program synthesis to automatically generate guiding programs for RL in partially observed environments. The method leverages a generative model to predict the unobserved parts of the world, and then generates a program based on samples from this model that is robust to its uncertainty. In the experiments, the authors show that MPPS outperforms non - program - guided approaches on a set of challenging benchmarks, including a 2D Minecraft - inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent.   The results demonstrate that   MPPS can obtain the benefits of program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task."
SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"This paper investigates the problem of causal imitation learning ( i.e., when the imposter has access to information unavailable to the demonstrator, e.g., a different set of sensors, in sequential settings, where the imitator has to make multiple decisions in an episode.   The paper proposes a graphical criterion that is necessary and sufficient for determining the feasibility of. causal imitation, providing conditions when an imitators can match a demonstrator ’s performance despite differing. capabilities. The paper also proposes an efficient algorithm for determining imitability and to find the policy for each action that leads to proper imitation. Finally, it corroborates the theory with simulations."
SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,"This paper proposes a slot - wise, object - based transition model ( OAT ) that is trained end - to - end without supervision using a transition loss at the level of the object - structured representation rather than pixels. The key contributions of the OAT model are : 1 ) it introduces an object - wise alignment module that ensures that objects are placed in the same slots in the memory regardless of their current positions in the scene ; 2 ) it provides a method to predict the reappearance of objects that have been occluded from the view of the observer for a given sequence of frames. The model is evaluated on a real world dataset that consists of robot trajectories where it is able to outperform a state - of - the - art baseline, and demonstrates better roll - out performance when unrolling the model for significantly longer periods than seen during training."
SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"The paper considers the generic importance - weighted ERM ( ISWERM ) algorithm for regression and policy learning. The main contributions are :   ( 1 ) generic guarantees for generalization under Donsker - like conditions and ( 2 ) generalization guarantees for fast and slow rates when a variance bound is available. The algorithm is based on a new maximal inequality that carefully leverages the importance sampling structure to obtain rates with the good dependence on the exploration rate in the data. ( 3 ) For regression, the paper provides fast rates leveraging the strong convexity of squared - error loss. ( 4 ) For policy learning, it provides regret guarantees that close an open gap in the existing literature whenever exploration decays to zero, as is the case for bandit - collected data. The empirical investigation validates the theory."
SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,"This paper proposes a novel and coherent scheme for kernel - reweighted regression by reparametrizing the sample weights using a doubly non - negative matrix. When the weighting matrix is confined in an uncertainty set using either the log - determinant divergence or the Bures - Wasserstein distance, the authors show that the adversarially re - weighted estimate can be solved efficiently using first - order methods. Numerical experiments show that this reweighting strategy delivers promising results on several datasets.    The main contributions of the paper are as follows :   1. A new method for approximating the Frobenius inner product between the inner products of two regression models A and B by means of first order methods ( Tr [ A > B ] ). 2. An extension of the Bure-Wasserstein - distance method ( Theorem 2 ) to the uncertainty set ( Section 3 ). 3. A method for estimating the mean and covariance of the kernel of a log - d - divergence estimate when the input is a mixture of two unlabeled samples ( Section 4 )."
SP:fe12e13602925b9400fd596a987755beb10aa3d1,"This paper aims to find unbiased gradient estimators for the discrete latent variable setting, where low - variance reparameterization gradients of a continuous relaxation can provide an effective solution. In contrast, continuous relaxations are not always available or tractable. Previous work ( Yin et al., DisARM ) is limited to binary random variables and hence this paper derives a novel derivation of their estimator based on importance sampling and statistical couplings, which are then extended to the categorical setting. Motivated by the construction of a stick - breaking coupling, two gradient estimator - based methods are introduced, namely DisARM - SB and DisArmed - Tree, by reparametrizing the problem with a sequence of binary variables and performing Rao - Blackwellization. In systematic experiments, the proposed estimators provide state - of - the - art performance, which can sometimes outperform RLOO."
SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"This paper proposes a new predictor - based architecture search framework, called WeakNAS, that progressively shrinks the search space by learning a series of weak predictors that can connect towards the best architectures. By co - evolving the prediction stage and the learning stage, WeakNAS can progressively evolve to sample towards the subspace of best architectures, thus greatly simplifying the learning task of each predictor. The proposed WeakNAS is shown to be more sample - efficient and robust to various combinations of predictors and architecture encoding means."
SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"This paper proposes a new intrinsic control method based on Entropic Desired Dynamics for Intrinsic ConTrol ( EDDICT ). It is an extension of Entropics [ 1 ], which is a type of unsupervised control method that uses latent codes to determine the state - action probability distribution. The key idea is to place the latent codes in a globally consistent coordinate system, e.g., a globally additive latent dynamics, such that it is tractable to learn and can provide an interpretable latent space. The authors then show that this can be used to train an intrinsic controller that can reach more states in the long term, while still optimizing a local objective. They build on top of this by defining a set of latent dynamics parameters that bound the additive noise in the latent space, and show that these parameters are enough to provide a tractable learning method. They also show that EDDCT is more interpretable than previous intrinsic control methods, and that it encourages exploratory behavior compared to prior methods. They test this by comparing it to state coverage in two hard exploration games ( Montezuma's Revenge and Pong ), and compare it to one of two baselines ( MCTRL and MICE )."
SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"The paper proposes a framework, FREED, that combines fragment - based generative RL with Explorative Experience replay for Drug design ( EXPERT ) to generate pharmacochemically acceptable molecules with large docking scores. The docking program is a physical simulation that estimates protein - small molecule binding affinity and is an ideal reward function for RL, as it is a straightforward proxy of the therapeutic potential. The proposed method constrains the generated molecules to a realistic and qualified chemical space and effectively explores the space to find drugs by coupling fragment-based generation method and a novel error - prioritized experience replay ( PER ). The paper shows that FREED model performs well on both de - novo and scaffold - based schemes. It achieves state - of - the - art performance on two of three targets in terms of docking scores and ablation studies. It further shows that predictive error - PER significantly improves the model performance."
SP:b938bca513e7de1231212064caf8877a78d8b612,"This paper studies the problem of learning directed acyclic graphical models ( DAGs ) from observational data without any specific distributional assumptions. It uses an information - theoretic approach that uses a local Markov boundary search procedure to recursively construct ancestral sets in the underlying graphical model, which is called ancestral sets construction, in order to construct identifiable ancestral sets that can be used to learn a DAG in polynomial time. The paper shows that for certain graph ensembles, a simple forward greedy search algorithm ( i.e. without a backward pruning phase ) suffices to recover the Markov boundaries of each node, which substantially improves the sample complexity. This is then applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature ( e.g., a finite - sample guarantee ). Finally, the paper applies the results to the special case of polytrees, for which the assumptions simplify, and provides explicit conditions under which polytree DAG are identifiable and learnable in Polynomial Time.   The main contributions of the paper are as follows. First, it shows that when the underlying DAG is a polytree ( or more generally, a polyforest ), condition 2 of Theorem 5.2 is always satisfied, and therefore an efficient algorithm for learning identifiable polyforests that satisfy Condition 1 is given. Second, it follows immediately ( by combining Theorems 5.1 and 5. 2 with Lemma G.1 ) that any polytree satisfying ( C1 ) is learnable. Third, a simulation study is provided to illustrate the performance of the algorithm."
SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"This paper studies learning with differential privacy ( DP ) in the setting where each user holds m samples and the privacy protection is enforced at the level of each user’s data. They show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an DP algorithm using only O(log(1 / )/"") users. For example, for - DP algorithms, they show that we can solve the global - stability problem using a correlated sampling strategy to show that the global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the number of samples per user.    The paper also considers learning with DP in the local model, where d is the probabilistic representation dimension. They give a nearly - matching lower bound on the   number of users required in both cases, but only in the case of ( global ) - -DP algorithms. In particular, their algorithms make extensive use of shared randomness — in the form of correlated sampling — to boost the stability."
SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"This paper studies the effect of implicit parameterization of value functions on the convergence of end - to - end model - based reinforcement learning approaches such as VLIAs ( e.g. VLI as in VLI - GCN ) on the loss function of a Bellman optimal policy under a non - linear and non - convex optimization problem. The paper first provides a simple instance of such an implicit formulation as an implicit reward chain model, then analyzes the dynamics of gradient descent under this parameterization. It shows that implicitly parameterized linear weights still converge to a unique global optimal. Furthermore, it quantifies the convergence rate of the residual under the implicit model, showing that it has surprisingly fast convergence rate along the 1 - direction. Finally, it provides empirical results in some simple domains to illustrate some of the phenomena identified in the theoretical results."
SP:992aa07d4f815d1c81f967374590eece933833b1,"This paper proposes an embedding framework, IterefinE, that combines PSL - KGI and type - supervised KG embeddings through iterative feedback. The proposed framework operates in a co - training mode where PSL is used to refine KG using inference rules and the learned embedding is co - trained with the refined KG under the guidance of type supervision. The experiments over a range of KG benchmarks show that the embedding that we produce are able to reject noisy facts from KG and at the same time infer higher quality new facts."
SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"This paper proposes a novel evaluation paradigm for knowledge base completion ( KB ), namely that models do not actually decide whether a new fact should be accepted or not but are solely judged on the position of true facts in a likelihood ranking with other candidates. The authors argue that consideration of binary predictions is essential to reflect the actual KBC quality, and propose a novel   evaluation paradigm, designed to provide more transparent model selection criteria for a realistic scenario. They construct a data set FB14k - QAQ with an alternative evaluation data structure, where instead of single facts, we use KB queries, i.e. facts where one entity is replaced with a variable, and construct corresponding sets of entities that are correct answers. They randomly remove some of these correct answers from the data set, simulating the realistic scenario of real - world entities missing from a KB. This way, they can measure a model’s ability to handle queries that have more correct answers in the real world than in the KB, including the special case of queries without any valid answer. The latter especially contrasts the ranking setting. They evaluate a number of state - of - the - art KB embeddings models on our new benchmark. The differences in relative performance between ranking - based and classification - based evaluation that they observe in their experiments confirm their hypothesis that good performance on the ranking task does not necessarily translate to good performance in the actual completion task. The results motivate future work on KB embedding models with better prediction separability and, as a first step, propose a simple variant of TransE that encourages thresholding and achieves a significant improvement in classification F1 score relative to the original TransE."
SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,"This paper proposes a new model for dialog generation, Alternating Roles Dialog Model ( ARDM ). ARDM models each speaker separately and takes advantage of the large pre - trained language model. It requires no supervision from human annotations such as belief states or dialog acts. Experiments shows that ARDM outperforms or is on par with state - of - the - art methods on two popular task - oriented dialog datasets : CamRest676 and MultiWOZ. Moreover, it can generalize ARDM to more challenging, non - collaborative tasks such as persuasion. In this paper, ARDM is capable of generating human - like responses to persuade people to donate to a charity."
SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,"The paper proposes a new method to measure confidence in model predictions ( e.g., predicting the probability of a top 5 classification on the test set ). The key idea is to use the notion of implied loss and prove that if an uncertainty measure is an implied loss, then high confidence means high probability of correct ( or top k ) classification. The authors propose to use model entropy as a measure of confidence since it is a "" large "" random variable which is expected to be large when the loss is large.   The method is simple and can be applied to existing models such as ResNet and DenseNet. It is shown that the proposed confidence measures for Top k can be evaluated by binning values which can be obtained by computing log - pmax of a confidence measure ( such as model entropy or log pmax ) and binning the values. The expected Bayes factor is then used to measure the confidence. The Bayes factors were used to compare existing confidence measures on different tasks : detection of off manifold data, detection of adversarial examples, and detection of mislabelled images."
SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"This paper studies the relationship between trainability and generalization of neural networks as a function of their architecture and hyperparameters. In particular, this paper focuses on training neural networks at large depths, where it will see that the situation simplifies considerably. The authors leverage recent advances that have previously shown :   ( 1 ) that in the wide network limit, random networks before training are Gaussian Processes governed by a kernel known as the Neural Network Gaussian process ( NNGP ), ( 2 ) that at larger depths the spectrum of the NNGC kernel simplifies and becomes “ weakly data - dependent ”, and ( 3 ) that gradient descent training of wide neural networks is described by a   kernel called the Neural Tangent Kernel ( NTK ) that is related to the N NGP. Here, the authors show that   in the large depth limit, the spectrum   of the NTK simplifies in much the same way as that of the NeuralNGP kernel. By analyzing this spectrum,   the authors arrive at a precise characterization of trainability, which is a necessary condition for generalization across a range of architectures, including Fully Connected Networks ( FCNs ) and Convolutional Neural Networks ( CNNs )."
SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"This paper proposes Graph Convolutional Network ( GCN ) for protein quality assessment ( QA ). GCN is based on the fact that proteins are represented as graphs, which allows it to benefit from several advantages of previous QA methods including representation learning, geometric invariance, explicit modeling of sequential and 3D structure, and computational efficiency. Through extensive experiments, the authors show significant improvements over the state - of - the - art results on various metrics and datasets and further analyze the results via thorough ablation and qualitative studies."
SP:5188280131b58a35d3deda126a0754aea8fa6e58,"This paper revisits and extends the literature on the loss landscape of linear neural networks with different loss functions and different parameterizations. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. It is well known that Baldi & Hornik ( 1989 ) proved the absence of non - global ( “bad ” ) local minima for linear networks with one hidden layer. Their result was generalized to the case of deep linear networks by Kawaguchi ( 2016 ). Many papers have since then studied the loss landscapes of linear networks under different assumptions. This paper extends the general results from the previous subsection to study the critical locus Crit(L ) with L = ` \mathbb{R}^d, where ` is any smooth function. It firstly shows that pure and spurious critical points exist for arbitrary smooth convex losses under certain conditions. Then, it extends the analysis from the linear networks to the loss of quadratic functions and isotropic ones, and shows that the same phenomenon holds for both. Finally, it provides a unified framework to explain all these results."
SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"This paper proposes a general framework SEED ( Sampling, Encoding, and Embedding distribution ) for unsupervised and inductive graph learning. Given an input graph, the proposed SEED framework first samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of sub - graph vectors, and uses the embedding of the sub graph vector distribution as the output vector representation for the input graph. By theoretical analysis, the authors demonstrate the close connection between SEED and graph isomorphism. The experimental results suggest the proposed framework is effective, and achieves state - of - the - art predictive performance on public benchmark datasets."
SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,"This paper introduces Lazy - CFR, a counterfactual regret minimization ( CFR ) method that minimizes the regret of a minimizer over a set of nodes in a twoplayer zero - sum extensive games. The paper shows that the vanilla CFR traverses the whole game tree in each round, which is time - consuming, and proposes a lazy update strategy to instead visit only a small part of the nodes. The analysis is provided in Section 3.1. The main contribution of the paper is then to show that the proposed lazy - CFR is equivalent to vanilla CFR, and that it is faster than CFR, when regret is minimised over a small portion of the game tree. Based on the analysis, the paper then proposes MC - CFR ( a variant of CFR with lazy update ), and a follow - up version called Lazy + CFR ( an extension ). Theoretical and empirical results on Leduc Hold 'em and heads - up flop hold 'em show that Lazy+ CFR is fast in practice. In section 3.2, the same idea is also proposed for CFR+, but with a different laziness strategy : instead of visiting all the nodes in the tree, the update only visits a few of them. The resulting algorithm is evaluated on the benchmark games and compared to the vanilla and lazy CFR."
SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"This paper proposes a new method for unsupervised domain adaptation ( UDA ), called Distribution Matching Prototypical Network ( DMPN ), to model and match the deep feature distribution of the source and target data as Gaussian mixture distributions. This is done by minimizing the cross - entropy loss on the labeled source data and the domain discrepancy losses together, which are called Gaussian Component Mean Matching ( GCMM ) and Pseudo distribution mean matching ( PPM ). The first one minimizes the distance between the corresponding Gaussian component means of source vs. target data, while the second one minimises the pseudo negative log likelihood of generating the target features from source feature distribution. Experiments are conducted over two UDA tasks, namely Digits Image Transfer ( Digits ) and Synthetic Image Transfusion ( ViT ), where the proposed method achieves a mean accuracy of 81.4 % on the Digits dataset. Moreover, a post - hoc sensitivity analysis shows that the approach is robust w.r.t. hyper - parameter changes."
SP:40be996e8bb86e887077b762b87c7c34a786ac98,"The paper proposes InfoCNF, an extension of Conditional Normalizing Flow ( CNF ) that uses gating to learn the error tolerances of its ODE solvers. The proposed method is inspired by LatentODE, a gating - based conditional generative model that partitions the latent space into a supervised code and an unsupervised code. The supervised code is shared among all classes and is optimized by a supervised component for each class. The unlabeled code is partitioned among classes according to their similarity in dimension. The authors argue that this inefficient method of partitioning the space slightly increases the number of function evaluations ( NFEs ). To cope with this, they use gating networks to train a tolerance network that encourages the solver to overfit to a lower tolerance range. Experiments are conducted on CIFAR-10 with and without gating, and on time - series data. The gating network is shown to reduce the NFE by a large margin on small - batch training while increasing the NLL by a smaller margin on larger batches. The learned tolerance network is also shown to improve the extrapolation performance when extrapolated from time to time."
SP:97764e3393216106ff2ac3f550845acf4636119f,"This paper studies nonlinear value function approximations of Markov decision processes ( MDPs ) learned with the Temporal - Difference ( TD ) learning algorithm in the lazy training regime, when the model is a nonlinear function of its parameters. In this regime, the algorithm behaves essentially like a linear approximator spanning the tangential space of the approximating manifold ( in function space ). The paper provides convergence results in both the under - and over - parametrized regime to local and global minima of a natural, weighted error function, respectively, of the projected TD error, and illustrates such convergence properties through numerical examples. The results obtained emphasize the interest of this regime in the framework of deep reinforcement learning, where models often suffer from divergent behavior, especially during the early stages of training.   The main contributions of the paper are as follows :   1. This paper proves that on - policy TD learning for policy evaluation, a widely used algorithm for value function approximation in the reinforcement learning setting, is convergent,    2. In particular, it shows with probability one to the global minimum or a local fixed point depending on the codimension of the approximate manifold in the search space, that the lazy TD learning algorithm converges to either global minimizer $ \mathcal{W}$ or $ \delta$ $ ( \tilde{W } + \epsilon$ ), depending on $ \frac{\Omega}{W } \log(W)$. 3. It also gives examples of such convergence results for models that diverge if trained with non - lazy TD learning, and in the case of neural networks, under the lazy regime."
SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"This paper proposes a method to train a hypothesis verifier, that is, an agent that given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. The method is based on the observation that agents trained end - to - end with the reward fail to learn how to solve the problem of hypothesis verification. The authors propose to train the agent by using a reinforcement learning approach where they formulate the problem as a triplets ( pre - condition, action sequence, post - condition ), and show that the majority of the time the triplets can be formulated as triplets. Once the agents have been pretrained to verify hypotheses with this structure, they can be fine - tuned to verify more general hypotheses.   This work takes a step towards a “scientist agent ’s “ goal ” that develops an understanding of the the world by generating and testing hypotheses about its environment."
SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"This paper studies neural networks and the ability of GNNs to approximate reasoning in a fixed - dimensional latent space. Specifically, the authors consider embedding mathematical formulas into a latent space of fixed dimension, and train a neural network to predict the latent embedding of a given formula by predicting whether a rewrite of the formula will succeed based on the predicted latent embeddings. The set of rewrites that can be successfully performed on a statement represents the essential semantic features of the statement, and the vector associated with a statement is used to predict whether a statement can be rewritten by other theorems. The paper proposes to measure the effectiveness of this reasoning by performing sequences of rewrite steps both in formula space and in latent space, and compare the quality of the resulting formulas to their predicted latent representations. The experiments show that graph neural networks can make non - trivial predictions about the rewrite - success of statements, even when they are trained to only predict the predictions of a predicted latent representation for several steps."
SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"This paper proposes a new method to learn depth from images and very sparse ground truth. It is inspired by the way natural agents interact with the environment via visual and haptic feedback to introduce an inductive bias by designing a specialized global - local network architecture. Experiments are conducted on several datasets and the proposed model can learn monocular dense depth estimation that can be used for training with very sparse data, even a single pixel per image. The global parameters extracted by the network are predictive of the metric agent motion. The method is compared against generic deep networks, classic geometry methods, and unsupervised learning approaches. It outperforms all baselines thanks to its ability to train with sparse labels and its robustness to variations in the camera parameters."
SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,"This paper proposes a Transformer - based model, called Superbloom, for the task of hashed prediction on hashed vocabularies. The model is trained using BERT's masked language model task, i.e., predicting hash tokens masked out from the input. The key observation is that it is important to use a multi - layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. It is claimed that this provides an alternative method to solve problems with large vocabulary size. Experiments are conducted on simple hashed image dataset, represented using Bloom filter. They show that the proposed model outperforms models of a similar size without hashing and trained using sampled softmax with the same computational budget."
SP:745dd86d7f7bba79a02d27922003b764b620f83e,"This paper proposes a learning based agglomerative clustering framework for zero - shot 3D shape part discovery. The core idea is to learn part - level features within part local contexts, in order to generalize the part discovery process to unseen novel categories. The proposed method is evaluated and compared against four shape segmentation algorithms on the PartNet dataset. The evaluation shows that the proposed method achieves the state - of - the - art performance."
SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"The paper proposes a method for extrapolation from GANs to datasets outside of the manifold of the training set. The method uses an autoencoder to decompose the variation within the dataset into activations of different neurons and generates transformed data by defining an editing transformation on those neurons. By performing the transformation in a latent trained space, the authors encode fairly complex and non - linear transformations to the data with much simpler distribution shifts to the neuron ’s activations. The technique has the advantage of being generally applicable to a wide variety of data domains, modalities, and applications. The authors first demonstrate it on image transformations and then move to two main applications in biology : removal of batch artifacts representing unwanted noise and modeling the effect of drug treatments to predict synergy between drugs."
SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"This paper studies first - order meta - learning of initializations for deep neural networks that must produce structured predictions given an arbitrary amount of training data for a new task. The authors extend FOMAML and Reptile to the few - shot setting and develop a new architecture, EfficientLab, for this task. They also provide a formalization of the generalization error of meta - learner algorithms, which they leverage to decrease error on unseen tasks. Experiments are conducted on the FSS - 1000 dataset and the FP - k dataset. They show that their network, with an empirically estimated optimal update procedure yields state - of - the - art results, while only requiring one forward pass through a single model at evaluation time."
SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"This paper proposes a graph - based method for semi - supervised few - shot learning ( SS - FSL ) based on Prototypical Networks ( PN ). The main difference is that the graph is added to the prototypical network by adding a random walk component to the loss function, in contrast to PN, which is applied on top of the graph representation. The resulting network is claimed to be more compact and well - separated than PN. Experiments are conducted on standard benchmarks such as mini - imagenet and Omniglot. The results show that the proposed method outperforms the state - of - the - art on most benchmarks."
SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"This paper proposes a new self - supervised learning objective, Contrastive Sensor Fusion ( CSF ), which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. The proposed objective is trained on a dataset of 47 million unlabeled cotermineous image triplets, which is used to train an encoder to produce semantically meaningful representations from any combination of channels from the input sensors. Experiments show that CSF outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused."
SP:4d8e054f07006b4f896721b5c24da805727d2c22,"This paper presents a network - agnostic pruning algorithm that matches the accuracy and compression ratio of several more network - specific state - of - the - art techniques for retraining after pruning. The considered retraining technique is fine - tuning, which removes the unwanted structure from the network and retrain the remaining structure with a small learning rate to recover the accuracy. This paper proposes weight rewinding, which rewinds the weights that were pruned earlier in the training process to their values from earlier in training and retrains them from there using the original training schedule, to replace the fine - tuned weights with the one that was pruned. The same learning rate is applied to the parameters that control the learning rate of the network. The paper evaluates this algorithm against two other retraining algorithms from the literature that are considered to be the current state of the art : Ortiz et al. ( 2020 ) and Carreira - Perpin PIN ( 2019 ). These algorithms are complex to use, and require learning hyperparameters that are intractable for the proposed method. On the other hand, this paper proposes an iterative algorithm that iteratively prunes the network to completion, removes unwanted structure to compress the network, and retires the weights based on the values that have been learned from the final values. This method, called unstructured pruning, can be combined with two retraining techniques : ( 1 ) learning rate re - training and ( 2 ) weight re - warming. When the two methods are combined, the resulting network retraining algorithm matches the performance of the one evaluated in Ortiz et a."
SP:3bb1c79f9482e09828eda45fbb2e654f37219365,"This paper studies the all - layer margin ( margin ) of deep neural networks. It is well known that a large output margin implies good generalization for linear classifiers, but it is less clear for deep networks. This paper proposes to analyze this metric by analyzing a new notion of margin, which is called the "" all - Layer Margin "". The analysis is based on Danskin's Theorem, which states that any continuous differentiable margin $ G(δ,Θ;x, y) $ is guaranteed to have a descent direction in $ \epsilon $ in the objective $ L(Θ)$ for optimization. Based on this result, the paper proposes an algorithm to encourage the optimization of this metric. The main idea is to use the Lagrange multiplier method to relax the constraint maxy′ F ( f(x, y ) ), which can be expressed as a softmax relaxation of the constraint $ F(x,y ). The paper then applies this objective to a neural network of the form WideResNet ( Zagoruyko and Komodakis, 2016 ) trained on CIFAR-10 and demonstrates improved generalization performance for both clean and adversarially robust classification on the datasets. In addition, based on theoretically inspired training algorithm in Section F1.1, it is shown that increasing the all layer margin leads to improved generalisation over the strong baseline in addition to the Wide ResNet architecture ( Z.Simonyan and Z.Konstantinou, 2014 ) in the single validation run of WRLN-1 with VGG."
SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"This paper proposes a disentangled response decoder for knowledge - grounded dialogue generation ( GDC ) with a limited set of training examples, assuming that only a small number of grounded dialogues and documents are available. The decoder is decomposed into an encoder and a decoder module, where the decoder output is sampled from a large set of grounded and ungrounded dialogues, along with a document embedding, to be used as a basis for parameter estimation. The authors claim that this decomposition helps to isolate the parameters that depend on the GDC from the rest of the parameters in the entire model, while the remaining parameters can be well fitted with the limited training examples. The proposed approach is evaluated on two GDC benchmarks, where it is able to achieve state - of - the - art performance with only 1.5 % of the training data, and exhibits a good generalization ability on out of domain knowledge."
SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"This paper proposes a new architecture for neural machine translation models ( NMT ) that makes use of the non - parallel corpus generated by previous NMT models. The proposed model, called MGNMT, is a mirror - generative NMT, which simultaneously learns bidirectional translation models as well as source and target language models in a latent space of the shared bilingual semantics. Experiments show that the proposed model consistently outperforms existing approaches in a variety of language pairs and scenarios, including resource - rich and low - resource situations. Moreover, it shows that translation quality indeed becomes better when the jointly learned translation model and language model of M GNMT work together."
SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,"This paper studies the role of the entropy term in Soft Actor Critic ( SAC ) for the Mujoco benchmark. It shows that SAC is best when there are bounded action spaces, and proposes a simple non - uniform sampling method for selecting transitions from the replay buffer during training that can improve the performance of SAC without using any entropy regularization. The paper also proposes a new SAC - like algorithm that does not use any entropy maximization but nevertheless matches the sampling efficiency and robustness of current SAC. The experimental results demonstrate a need to revisit the benefits of entropy regularisation in DRL and further improve SAC by using it as an additional input to the mixer."
SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. The authors discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks. These vulnerabilities are especially apparent for neural network based systems. As proof of concept, the authors describe a well - known music identification method and implement this system in the form of a neural net, and then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag music recognition service ( AudioTag, 2009 ) and YouTube’s Content ID system(Google, 2019 )."
SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"The paper proposes a new activation decomposition method to explain the visual similarity between two input images by decomposing their final activation functions. The proposed decomposition could be used for cross - view pattern discovery ( CWS ) and interactive retrieval ( RED ), where the idea is to generate a point - to - point activation map of two images so that the relationship between different regions of the two images can be uncovered, and the proposed method is validated on face recognition and person re - ID applications.   The contributions of the paper are as follows :   1. The paper proposes an interesting new decomposition of the final activation maps of CWS, which can be used as an explanation for the visual aspect of deep metric learning. The decomposition can be decomposed into two components : the overall activation map and the partial activation map. The partial activation maps are used to analyze the visual aspects of the CWS training data. The intuition behind the partial similarity is that if two images have similar features but have different activation maps, then the features from similar parts of the images should be clustered together in the activation maps. The probability that an image from the same set of images has higher activation is that it is more similar to the query image than it is to the other images. Therefore, the high activation of an image in a map with a map of similar features will be clustered with the more similar images. The method is evaluated on CWS and RED for face recognition, and it is shown to retrieve images with similar partial features and to be able to interactively retrieve images having similar parts with the query images."
SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"This paper proposes an approach to tackle the problem of lifelong continual reinforcement learning ( LTL ), where a policy network is designed over an extended period of time ( possibly indefinitely ), and the goal is to avoid failure modes that can occur early in the policy network's lifetime that are typically associated with traditional model - free policy learning methods ( e.g., catastrophic mistakes ). To tackle this challenge, the paper proposes Adaptive Online Planning ( AOP ), which combines model - based planning ( i.e., planning to use the current state of the environment to determine when to change the policy ) with model - Free Learning ( model- free ). The idea is that the planner learns to estimate the uncertainty of the model by taking into account both the planner's performance and that of the models that are left - unexplored, such as the ones that are learning in the absence of a planner. The hope is that this will inform the planner to use more planning only when necessary, resulting in reduced computation times. Empirical results show that AOP is able to successfully reduce computation while achieving high performance in difficult tasks, often competitive with a much more powerful MPC procedure."
SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"This paper proposes to replace the traditional softmax attention mechanism in image captioning models by two alternative sparsity - promoting transformations : sparsemax and TVMAX. The first TVMAX introduces sparsemax to obtain sparse attention weights, selecting relevant features. The second TVMAX improves the interpretability of the generated captions by selecting relevant groups of features. They compare the performance of the proposed methods on the Microsoft COCO and Flickr30k datasets, obtaining gains in comparison to softmax. They also carry out a human evaluation experiment."
SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,"This paper proposes a model to predict the evolution of dynamic graphs by using a graph neural network and recurrent architecture. Specifically, the authors combine the temporal evolution patterns of static graphs with a generative model that predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology. The authors evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real - world datasets following similar dynamics. Results demonstrate the effectiveness on several tasks that are commonly used in graph mining."
SP:ff722957a1765c0568426ed88dd910a6b74054ef,"This paper proposes Generative Imputation and Stochastic Prediction ( GI ) for estimating the distribution of missing features and uncertainty over target class assignments given incomplete data. The key idea of GI is to use neural networks trained using an adversarial objective function to generate imputations and measure uncertainties over the class assignments based on incomplete feature vectors. Specifically, a generator network generates imputations by generating features that a discriminator network is tasked to distinguish. Then, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions based on the imputations. The proposed approach is evaluated on CIFAR-10 image dataset as well as three real - world tabular classification datasets, under different missingness rates and structures. The experimental results show the effectiveness of the proposed method in generating imputations as well providing estimates for the class uncertainties."
SP:c051b0fe779d9e4131016970b7ba469b596f3009,"This paper proposes an importance - sampling - based estimator for long - horizon off - policy estimation in RL, where the goal is to estimate the importance ratio of a given indicator function over a set of states ( called states in the paper ) that are not sampled from a known behavior policy. The paper considers the long horizon problem in Reproducing Kernel Hilbert spaces ( RKHS ) setting, which is equivalent to solving for the fixed point of a certain operator. The proposed estimator computes importance ratios of stationary distributions of the indicator function, without any knowledge of how the data for the indicator is collected. The authors analyze its asymptotic consistency and finite - sample generalization. Experiments on benchmarks verify the effectiveness of the approach."
SP:065c900843011a71b70ed35357a2f71fe83872a7,"This paper studies the problem of generating mixed images on an unsupervised Gaussian mixture model ( GMM ), which assumes the existence of a random index k that indicates which Gaussian distribution the data belongs to. The authors propose a GAN - based method to compute the conditional likelihood p(x|k|k, \� ) and responsibility probability p(k|x, k ) of this index using GAN, where k is the random index of the dataset and k-1 is the number of Gaussians associated with the data. They also propose a modified GAN to allow them to define the distribution p(z|k, \�), where z is the corresponding latent representation of x, through an additional classification network which is trained with the GAN. In experiments, they demonstrate that the proposed method surpasses previous baselines in terms of image generation performance with only minor growth on the size of the network."
SP:2da1608209058d214f8671062cc9eb0833ba4831,"This paper proposes a method to train large neural networks with improved accuracy and lower dynamic computational cost. The method trains the network by gating the deep - learning architecture on a fine - grained level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, the authors introduce a new residual block architecture that gates Convolutional channels in a fine-grained manner. The authors also introduce a generally applicable tool batch - shaped loss that matches the marginal aggregate posteriors of features in a neural network to a pre - specified prior distribution. They use this novel technique to force gates to be more conditional on the data. They present results on CIFAR-10 and ImageNet datasets for image classification and Cityscapes for semantic segmentation. Their results show that their method can slim down large architectures conditionally such that the average computational cost is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, their ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top - 1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity."
SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"This paper proposes a lossless pruning method for deep neural networks ( DNNs ) based on a probabilistic importance inference approach. The main idea is to score all connections of a DNN and its output using a non - paremetric measure, and only keep the significant connections. The remaining connections are kept as is. The idea is that by keeping only significant connections, one can infer how likely it is that a given DNN's output depends on a certain set of connections.    The main theoretical contribution of this paper is a proof that the proposed approach achieves better lossless compression rate than existing techniques, namely PCII ( Kolmogorov et al., 2019 ), the PCII method and the Gumbel - Softmax algorithm. The experimental results are presented in Table 5."
SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"This paper proposes a method for hierarchical reinforcement learning based on identifying behavioral ‘motifs ’ — repeated action sequences that can be compressed to yield a compact code of action trajectories. The method iteratively compresses trajectories by iteratively convolutional sparse coding, with structure similar to classic string compression methods such as the Nevill - Manning algorithm. Using this method, the paper is able to extract compact, hierarchically nested representations of actions, with time - extended actions of arbitrary length. The learned temporally extended actions provide new action primitives that can participate in deeper hierarchies as the agent learns. The authors demonstrate the relevance of this approach for tasks with non - trivial hierarchical structure and show that the approach can be used to accelerate learning in recursively more complex tasks through transfer."
SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"The paper proposes a generative model called Hierarchical Bayes Autoencoder ( HBAE ), which consists of two components : 1. A latent code for the input is sampled, and then this code is passed to a conditional generator to output a stochastic reconstruction. 2. An energy based model ( EBM ) is used to decode the latent code from the input to a set of examples, and samples are generated using a multimodal EBM decoder that is trained using an adversarial approximation. The paper also proposes a variant called Set - H BAE, which is a generalization of the above model to generate sets of examples. The advantages of using an EBM over a Gaussian decoder is demonstrated on a single image, and on ShapeNet for the classification task."
SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"This paper studies normalization techniques for off - policy ( TD ) RL algorithms. It first shows that batch normalization is not effective in normalizing the feature differences of TD algorithms. Then, it shows that normalization based on mixture of on - and off - policies is effective. Finally, it proposes a normalization method called cross - normalization that can be applied to DDPG and TD3. Experiments show that the proposed normalization improves the performance of these algorithms across MuJoCo tasks."
SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"This paper proposes a bias - resilient neural network ( BR - Net ) that learns discriminative features unbiased to the confounder(s ). This is achieved by training the model with an adversarial loss function that encourages a vanished correlation between the bias and learned features. The proposed bias - sensitive network was evaluated on a synthetic dataset, a medical dataset, and a gender prediction task. Experiments show that the learned features by the proposed network are uncorrelated with the bias or confounding variables. In addition, the feature embedding space by the biased network was agnostic to the bias in the data."
SP:783049ff463edd1283c058c6106a3e1f9a033df4,"This paper proposes a lightweight transformer - based model called Group - Transformer, which is based on group - wise operators to reduce the number of parameters and complexity of the Transformer. The main contribution of the paper is the proposed inter - group linear operators to prevent performance degradation from the group strategy. The paper also provides ablation studies and qualitative analysis to analyze the contributions of the proposed work to demonstrate the effectiveness of the lightweight model for practical application. The implementation code of the Group - transformer model is provided. Empirical results on enwik8 and text8 datasets show that Group - Transformers with 6 M parameters outperform all LSTM - based models with under 35 M parameters and compatible parameter size of Transformers with comparable number of parameter."
SP:946c26d371297c88d0ac246257104099b4585edc,"The paper proposes stacking WAEs based on Optimal Transport ( OT ) to train generative models with hierarchical latent structures. This is achieved by recursively applying the Wasserstein distance as the regularisation divergence, which is extended to arbitrarily deep latent hierarchies in the paper. The paper shows that this approach is more effective at learning smooth hierarchical latents than the standard WAE. It also introduces a novel objective function based on OT, called STACKEDWAE, which can be used to combine different stacking strategies. The main contributions of the paper are two - fold :   1. It introduces a stacking strategy based on optimal transport that can be applied to train hierarchical models, and shows that it is able to fully leverage its hierarchy of latents. 2. It shows that the approach performs significantly better when training hierarchical - latent models than the original WAE framework regularising the latent with the Maximum Mean Discrepancy (MMD )."
SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"The paper proposes an autoregressive model of videos based on a three - dimensional self - attention mechanism for generating continuations. The proposed model is based on the block - local attention mechanism, which is a variant of attention applied to 3D blocks. The authors argue that this three - dimensionality allows the model to capture longer - range dependencies while being agnostic to the position of the objects in the videos.   The authors show that the proposed model achieves state - of - the - art results across a range of video generation benchmarks, while the scalability of their model enables them to make an initial attempt at modeling videos of unusually high complexity and diversity as found in the Kinetics dataset. While the models generate encouraging continuations, especially on a subset of cooking videos, the authors find modeling the full range of videos clearly remains a major challenge."
SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"This paper proposes an adversarial generative model for generalized zero - shot ICD classification. The proposed method, called AGMC - HTS, aims to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. It generates semantically meaningful features by exploiting ICD code hierarchical structure and a novel cycle architecture that reconstructs the relevant keywords. Experiments are conducted on the public MIMIC - III dataset, which shows that the proposed method can improve the F1 score from nearly 0 to 20.91% for the zero - shots codes and AUC score by 3% ( absolute improvement ) from previous state of the art."
SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,"This paper proposes a method for self - supervised representation learning to improve sample efficiency in reinforcement learning ( RL ). The authors propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. The state embedding captures the state dynamics of the environment, while the action embedding is used to capture the action dynamics. Together, the two can be combined to form a representation of a state and an action. The representation of an action can then be used to learn a policy in a model - free RL setting. This representation can be learned in a few million environment steps ( or pixels ) from a control task. The method is evaluated on goal - conditioned continuous control tasks and shows improved sample efficiency compared to state - of - the - art RL methods on control tasks, with larger gains on more complex environments."
SP:11ce1616e721340eea9e80dad7460c77355ac7d1,"This paper proposes a meta - learning framework, called ARML, for heterogeneous tasks. ARML is motivated by the way knowledge organization in knowledge bases and aims to address the challenge of task heterogeneity by a learned meta - knowledge graph, which captures the relationship among tasks and improves the interpretability of meta - learner algorithms. The proposed framework automatically constructs the meta -knowledge graph to facilitate learning a new task, and empirically outperforms the state - of - the - art meta learning algorithms.   The experiments are conducted on 2D toy regression and few - shot image classification and the results demonstrate the superiority of ARML over the baselines."
SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"This paper proposes a method for controllable language generation that combines a pre - trained language model ( LM ) with one or more simple attribute classifiers to guide text generation without any further training of the LM. The authors propose a forward and backward pass in which gradients from the attribute model push the LM ’s hidden activations and thus guide the generation. The proposed method is called Plug and Play Language Model ( PPLM ). In the canonical scenario, the attribute models are simple classifiers consisting of a user - specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM, and they are sampled through a simple gradient - based mechanism. Experiments demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications."
SP:12d0980bfea2de880905a0b87b40856969bb1c58,"This paper proposes a denoising autoencoder for unsupervised representation learning ( un - supervised learning ), where the noisy input data is generated by corrupting clean data in the gradient domain with a Laplacian pyramid. The proposed method is claimed to be superior to the conventional DAE and achieves state - of - the - art performance on several benchmarks. Furthermore, the learned representations perform well when transferring to other vision tasks. The core idea of performing both local and non - local learning that is consistent with the hierarchical nature of ConvNets, is the LaplACian pyramid space, which is a promising direction for future research."
SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"This paper proposes a novel method to verify the robustness of the Decomposable Attention Model ( DA ) using Interval Bound Propagation ( IBP ). The authors propose to formalise the problem of under - sensitivity by using IBP to test whether samples from a model are free from the under - sensitive setting. They use IBP for the DA model on the SNLI and MNLI datasets, and show that IBP - trained DA leads to significantly improved verified accuracy."
SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"The paper studies model - free off - policy deep reinforcement learning ( MBRL ), where a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions. The authors represent these transitions in a data graph and link its structure to soft divergence by selecting a subgraph with a favorable structure, to construct a simple Markov Decision Process ( MDP ) for which exact Q - values can be computed efficiently as more data comes in – resulting in a QGRAPH. They show that the Q - value for each transition in the simplified MDP is a lower bound of the Q values in the original continuous Q - learning problem. By using these lower bounds in TD learning, their method is shown to be less prone to hard soft divergence and exhibits increased sample efficiency."
SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,"This paper analyzes the effect of embedding complexity on the generalization of unsupervised domain adaptation ( UDA ), i.e., the ability to generalize the hypothesis trained in a source domain to an unlabeled target domain. A commonly used technique for UDA is to learn domain - invariant embeddings for both the source and target domains. This paper shows that there is a complexity tradeoff between learning domain invariant representations and generalization. More specifically, it shows that the complexity of the representations in a multilayer neural network ( layer - wise ) with respect to the source domain is magnified in terms of the loss on the target domain, and vice versa. The paper further develops a simple yet effective algorithm to approximately optimize the tradeoff, achieving performance across tasks that matches the best network division."
SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"This paper proposes a new framework, Bayes - Stability, for proving generalization error bounds for learning general non - convex objectives, which has attracted significant attention in recent years. The new framework combines ideas from both the PAC - Bayesian theory and the notion of algorithmic stability to obtain new data - dependent generalization bounds for stochastic gradient Langevin dynamics ( SGLD ) and several other noisy gradient methods ( e.g. with momentum, mini - batch and acceleration, Entropy - SGD ). Their result recovers ( and is typically tighter than ) a recent result in Mou et al ( 2018 ) and improves upon the results in Pensia et al. (2018 ). The experiments demonstrate that their new bounds can distinguish randomly labelled data from normal data, which provides an explanation to the intriguing phenomena observed in Zhang et al! ( 2017a ). They also study the setting where the total loss is the sum of a bounded loss and an additional `2 regularization term."
SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"This paper studies the role of the hippocampus in continual learning in the context of two different continual learning problems, i.e., learning to navigate between two different environments ( an allocentric and an egocentric environment ). The authors first analyze the population - level activity of the hippocampal CA1 neurons of a Rodent hippocampus using the parametrized principal component analysis ( dPCA ) method. They find that the firing patterns reveal that the hippocampus encodes relevant task variables such as decisions, navigational strategies and reward location. They also compare this hippocampal features with standard reinforcement learning algorithms, highlighting similarities and differences. Finally, they show that a standard deep reinforcement learning model achieves similar average performance when compared to animal learning, but fails to mimic animals during task switching. Overall, this results gives insights into how the hippocampus solves reinforced spatial continual learning, and puts forward a framework to explicitly compare biological and machine learning during spatial continuallearning"
SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"The paper proposes a new Monte Carlo tree search method, bootstrapping tree search with a pre - trained policy, in order to improve the performance in continuous environments. The proposed method is based on two components : ( 1 ) limiting tree search branching factor by only taking action samples from the policy distribution and ( 2 ) a new loss function based on the trajectories ’ mean and standard deviations. The method is evaluated on continuous versions of several popular continuous action - space games, such as Humanoid. The empirical results show that the bootstrapped method improves the performance of the baseline policy optimization algorithm ( PPO ) significantly over the baseline algorithm ( MCTS ) in most of these environments."
SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,"This paper aims to answer the question : can we find winning tickets with few data samples or few labels? Specifically, the authors combine self - supervised learning with winning tickets generation, showing that good winning tickets can be found without labels and that it is possible to generate winning tickets without labels on ImageNet. The authors find that winning tickets found in these scenarios are competitive to winning tickets generated on the full ImageNet dataset, which is quite surprising since deep networks trained on CIFAR-10 are natually sparse, making conclusions potentially misleading.   The authors also show that using a larger dataset is important to study winning tickets."
SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"This paper studies the problem of prediction undersensitivity in neural reading comprehension models ( NPs ) with adversarially selected questions. This problem is when the question is posed to a NPs that are semantically different from the one they are trained on ( e.g., text perturbations on the question ), but the model ’s prediction of the answer does not change. The paper proposes to address this problem by training NPs with noisy adversarial attacks that randomly perturb the answers to NPs, such that the model still produces the same answer as the original one. This paper focuses on two NPs : - SQuAD2.0 and NewsQA models are found to be particularly vulnerable to the adversarial attack, as they produce questions that are provably unanswerable ( i.e., contain words such as “ what if ”, “ when ” and “ how ” ). They show that even if they do n’t contain any answer, they still produce the same ( possibly incorrect ) prediction as if the question were an answer to some other question. This suggests that the NPs are not necessarily aware of all the information provided in a given comprehension question. - A combination of data augmentation and adversarial training is applied to improve the performance of these NPs by making them more robust. - Adversarially robust NPs outperform naive models in a biased data setting, where they are less likely to rely on predictive cues only present in the training set and outperform a conventional model."
SP:5da870060778de460c1abe91562d6f3e707efef4,"This paper proposes a model - based approach to ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives because they require knowing apriori all the possible unsafe scenarios an agent could encounter. The proposed approach is to learn the transition dynamics of the environment and generate a directed graph called the imaginative module that encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. The baseline state, which can either represent a safe or an unsafe state, is taken as a human input, and the Imagination module is used to predict whether the current actions of the policy can cause it to end up in dangerous states in the future. This approach is compatible with any existing RL algorithm and any task with discrete action space. The method induces the agent   to act safely while learning to solve the task. The authors validate their approach by experimenting it on two gridworld environments and a self - driving car simulator, demonstrating that their approach to safety visits unsafe states significantly less frequently than a baseline state."
SP:c2796f28fb067138303df8d424d646f4ada31558,"This paper proposes a neural network architecture, Physics - aware Difference Graph Networks ( PA - DGN ), which leverages data - driven end - to - end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. The authors claim that PDEs have a prominent role in physics - aware modeling and that they can be used to discover hidden physics describing interactions between temporal and spatial derivatives. To this end, they build a DGN that approximates spatial derivatives to use them as approximators and to also learn the derivatives. Then, they conduct exhaustive experiments to predict climate observations from land - based weather stations and demonstrate that PA - dgn outperforms other baselines."
SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"This paper considers training structured neural networks with nonsmooth regularization ( e.g. $ \ell_1$-norm ) and constraints and proposes a proximal - type stochastic gradient descent ( ProxSGD ) algorithm to solve the problem. The authors formulate training as an optimization problem where the smooth loss function is augmented by a nonsmoot and convex regularization. They show that under a certain learning rate with probability 1, every limit point of the sequence generated by their algorithm is a stationary point. Extensive numerical tests are provided to demonstrate the performance of the proposed algorithm.    The main contributions of the paper are as follows :   1. Showing that under certain assumptions on the training loss $ \mathbb{R}^n$, the optimal aj will be exactly $ \leq \sqrt{0,\ldots,1}$ if $ \nabla_t$ is chosen correctly. 2. Proposing a $ \tilde{O}$-constraint on the interval $ t$ where $ t_0, \ldots, \in [ -1,\theta]$ is a sequence of $ n_t(t ) -1 $ steps. 3. Proving via numerical experiments that $ \kappa$-regularized networks trained with such constraints outperform their unconstrained counterparts on various benchmarks."
SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"This paper proposes a non - deterministic compression method based on relative entropy coding ( REC ), which maps an image to a discrete code using an encoder from which the original image can be reconstructed through a decoder. The decoder maps the input image into a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e., it is bits - back efficient. The method is trained end - to - end using a gradient - based optimizer. The authors then apply it to lossy image compression by training Probabilistic Ladder Networks ( PLNs ) on the CLIC 2018 dataset and show that their rate - distortion curves on the Kodak dataset are competitive with the state - of - the - art on low bitrates."
SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"This paper proposes a lossless SISR method for the super resolution ( SR ) of compressed JPG ( C - JPG ) images, which is widely used on the Internet. The proposed method consists of two components : ( 1 ) a functional sub - model to recover information from the compressed image, and ( 2 ) a cycle loss to ensure consistency across iterations. The paper also proposes an integrated SR model training pipeline with two - level data, i.e. CJPG LR and LR images, as well as a new integrated loss function, cycle loss. The experimental results demonstrate that the proposed method can surpass traditional SR models."
SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"This paper presents a neural network architecture for estimating the probability of a given event from a set of high - frequency spatio - temporal labels of events in a soccer match. The model consists of a feature extractor layer and a feature merge layer, where features at different levels of the extractor are merged to form a "" coarse "" feature representation of the event, and a prediction layer at each level of the input to produce a "" full surface "" of the probability distribution. This feature representation is then used in an MLP to estimate the full probability distribution over an event's possible outcomes. The proposed model is an extreme case of weakly supervised learning, where only the highest level features are available, and the task of the model is to learn to directly predict the probability for an event. The goal of the paper is to provide an accurate visual representation of game situations that can be interpreted directly by soccer coaches. The same model is also used to learn the pass - selection likelihood, and is shown to achieve considerably better results for single - pass probability estimation, while also correctly estimating the complete probability map."
SP:1ae31baf383fc520687b255d9cac14c3b040e253,"This paper proposes an inductive matrix completion model without using side information. The proposed model ( IGMC ) trains a graph neural network ( GNN ) based purely on 1 - hop subgraphs around ( user, item ) pairs generated from the rating matrix and maps these sub - graphs to their corresponding ratings. It achieves highly competitive performance with state - of - the - art transductive baselines. In addition, IGMC is inductive – it can generalize to users / items unseen during the training ( given that their interactions exist ) and can even transfer to new tasks. The transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Finally, a visualization confirms that local enclosing sub - graph is indeed strong predictors of ratings."
SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,"The paper considers the unconstrained minimization of a smooth objective function in R under the setting where only function evaluations are possible. The authors propose and analyze stochastic zeroth - order method with heavy ball momentum for convex objectives ( SMTP ). In particular, SMTP is a momentum version of Bergou et al. ( 2019 )'s three - point method ( STP ). They show new complexity results for non - convex, convex and strongly convex functions. They test their method on a collection of learning to continuous control tasks on several MuJoCo environments with varying difficulty. They compare against STP, other state - of - the - art derivative - free optimization algorithms and against policy gradient methods.   SMTP significantly outperforms STP and all other methods that they considered in their numerical experiments."
SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"This paper proposes a neural network, called Action Semantics Network ( ASN ), to characterize the action influence between two agents in multi - agent systems ( MAS ). Specifically, the authors show that actions taken by one agent can have different impacts on other agents by considering the action semantics between the agents. The main difference between ASN and previous works is that, whereas previous works borrow various multiagent coordination mechanisms into deep learning architecture to facilitate multi agent coordination, this paper explicitly considers action semantics and proposes a novel network to extract it to facilitate learning in MASs. Moreover, ASN can be easily combined with existing deep reinforcement learning ( DRL ) algorithms to boost their performance. Experiments on StarCraft II micromanagement and Neural MMO show ASN significantly improves the performance of state - of - the - art DRL methods compared with a number of network architectures."
SP:efaf3a440dc17e05177832083ffbc23760ed7c97,"This paper proposes exploiting the structure of the state - action value function, i.e., Q function, to improve planning and reinforcement learning ( RL ) algorithms. Specifically, the authors leverage Matrix Estimation ( ME ) techniques to leverage the low - rank structure in Q function. To this end, SVP and SV - RL algorithms are proposed, which leverage the existing low rank structures in the Q function to improve the planning process of classical control and deep RL. Experiments on control and Atari games demonstrate the effectiveness of the proposed methods."
SP:430336893b247b7bd45687d78b0d0511a7369e87,"This paper proposes Best - Action Imitation Learning ( BAIL ), an algorithm for learning policy in a batch setting. It is compared to Batch Constrained Q - Learning ( BCQ ) and BCQ - Batch, an off - policy RL algorithm. BCQ is different from BAIL in that it maximizes the Q function over the action space, while BAIL does not. BAIL first selects from the batch the actions it believes to be high - performing actions for their corresponding states, and then uses those state - action pairs to train a policy network using imitation learning. Although BAIL is simple, it achieves state - of - the - art performance on the Mujoco benchmark."
SP:94078964876667e8a5d9ae7728d779d5b91a576e,"This paper proposes a new extreme multi - label learning framework for short text documents. The proposed DeepXML tackles the issues of scalability and low accuracy ( especially on tail labels and very short documents ) with existing approaches such as Slice, AttentionXML, and XML - CNN, and improves on them substantively. The main contributions are word embeddings on head labels and transferring them through a novel residual connection to data - impoverished tail labels, increasing the amount of negative training data available by extending state - of - the - art negative sub - sampling techniques, and re - ranking the set of predicted labels to eliminate the hardest negatives for the original classifier. All of these contributions are implemented efficiently by extending the highly scalable Slice algorithm for embedding into the learned representations. The training algorithm is designed to be an order of magnitude more scalable than leading deep extreme classifiers. In addition, the method was also empirically determined to be up to 19% more accurate than leading techniques for matching search engine queries to advertiser bid phrases."
SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"The paper proposes a variant of hash - based collaborative filtering ( HBC ) with the Hamming distance. The main difference from standard HBC is that the proposed method considers user - specific bits ( i.e., the ones encoded by the user ) when computing the distance, as opposed to the underlying property of the item. The proposed method, called self - masking, first creates a modified item hash code by applying an AND operation between the user and item hash codes, before computing the Hammed distance. Intuitively, this can be seen as ignoring user - specified bits when computing Hammers distance. Experiments are conducted on 4 datasets, and obtain significant gains of up to 12 % in NDCG. The method is trainable and yields less than 4 % runtime overhead."
SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"This paper investigates the quantitatively quantitatively measuring the mode collapse of GANs. It proposes a set of statistical tools that are broadly applicable to measure mode collapse and discusses possible causes. The paper also proposes two simple yet effective “ black box ” methods to calibrate the GAN learned distribution, i.e., latent space reshaping via Gaussian mixture models, and importance sampling. They are observed to alleviate mode collapse without re - training data, nor re - calibrating the model parameters.   This paper is intended as a pilot study. It uses face generation as a study subject. Using face generation, it quantify the general mode collapse via statistical tools, discuss and verify possible causes, as well as propose two black - box calibration approaches for the first time to alleviate the mode collapses."
SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"This paper studies neural networks that are beyond the linear NTK regime but still governed by the Taylor expansion of the network. The authors propose to randomize the neural networks, which allows them to escape their NTK and couple with quadratic models. They show that the optimization landscape of randomized two - layer networks is nice and amenable to escaping - saddle algorithms. They prove concrete generalization and expressivity results on these randomized networks. This leads to sample complexity bounds for learning certain simple functions that matches the NTK without distributional assumptions and are advantageous when mild isotropic assumptions on the feature are present. Finally, they demonstrate that the randomization technique can be generalized systematically beyond the quadratically case, by using it to find neural nets that are coupled with higher - order terms in their Taylor series. These models also have the same expressivity and generalization guarantees as linearized models but in addition can generalize better by a dimension factor."
SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"This paper proposes a novel tool, Graph Filter Discriminant Score ( GFD ), to evaluate the effectiveness of graph convolutional filters for a given graph in terms of node classification. The GFD shows that there is no single filter as a “ silver bullet ” that performs the best on all possible graphs, and graphs with different properties are in favor of different graph convolutionsal filters. Based on these findings, the authors develop Adaptive Filter Graph Neural Network ( AFGNN ), a simple but powerful model that can adaptively learn data - specific filters for each graph. AFGnn leverages graph filter assessment as an extra loss term and learns to combine a set of base filters. Experiments on both synthetic and real - world benchmark datasets have demonstrated that AFN has the flexibility in learning an appropriate filter and consistently provides state - of - the - art performance across all the datasets."
SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"This paper studies the problem of worst - case generalization of overparameterized neural networks, in which a model can be perfectly fit the training data but will perform poorly on atypical groups of the data, by learning spurious correlations that hold on average but not in certain groups. The authors propose distributionally robust optimization ( DRO ) methods to learn a group DRO model that minimizes the worst case training loss over a set of groups, pre - defined in advance. The main idea is to firstly remove any connection between the data and the group loss, such that any model with vanishing average training loss also has vanishing worst case loss. Then, the model that performs poorly on some groups ( those for which the correlation does not hold ) is assumed to have poor generalization on others ( by assuming that the average loss also vanishes ). The group loss can then be optimized through a combination of two methods : ( 1 ) a stronger - than - typical ` 2 penalty or ( 2 ) early stopping, and ( 3 ) a stochastic optimizer. Experiments are conducted on a natural language inference task and two image datasets, and the authors show that their group DRo model outperforms the naive baselines on both tasks while maintaining high average accuracies. The experiments also show that regularization is important for worst - group generalization in the overparametrized regime, even if it is not needed for average generalization. Finally, the authors propose an efficient and stable convex optimization algorithm, with convergence guarantees, to efficiently train group DR O models."
SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"This paper proposes a simple but effective mask predictor for explaining the behavior of black - box classifiers. The key idea is to use a distribution controller ( i.e. a neural network ) to directly control the distribution of relevance scores and introduce a classification loss ( KL loss ) to regularize these relevance scores. The paper provides two practical implementations of the distribution controller, which can be used to enforce scores towards the desired right - skewed distributions, and where the involved hyper - parameters can be easily set. In addition, the paper proposes to train a mask generator using a loss that mimics the classifier. Experiments on image classification tasks show that the proposed method outperforms existing local explanation methods."
SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"The paper proposes a non - differentiable multi - stage learning framework that can be used for image reconstruction and classification tasks that involve detection of multiple instances of an object, without any supervision regarding their whereabouts. The proposed framework is based on the multi - instance image reconstruction ( MNIST ) and object localization ( MIST ) frameworks. In particular, instead of training an auto - encoder or a classifier for each instance, the authors propose to train an end - to - end network that first learns how to reconstruct patches of the image, and then feeds these patches to a task - specific network, e.g., auto - decoder for image classification. The idea is to lift the training optimization problem by treating the result of top - K selection as a slack variable, and training the end - stage network by lifting the stage training. The authors demonstrate the effectiveness of their framework by training it on a variant of the MNIST dataset, and demonstrate compelling performance in both reconstruction and classification."
SP:da1c5f6351d531482e90b86c3cceb52850c520de,"This paper proposes AutoAssemblet, a neural program synthesis ( NPT ) method that uses reinforcement learning to learn a policy to guide the search through the large code space to generate a piece of code that can execute stage changes in the CPU and RAM. The key idea of the method is to use a mixture of policy networks and value networks to reduce the Monte Carlo Tree Search, as well as a multi - entropy policy sampling technique to alleviate online update correlations. The method is applied to basic NPT tasks and shows higher success rates compared to several competing baselines. In the experiments, a sequence of assembly codes can be successfully generated, which points to a promising direction for program synthesis, if properly formulated."
SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"This paper studies the impact of model architecture on the speed of training in the context of gradient descent optimization. It builds upon the ideas from prior work that shows gradient descent can be modeled as a first - order ODE and use ODE ’s coefficient matrix H to characterize the convergence rate. The authors introduce a simple analysis technique that enumerates H in terms of all possible ""paths "" in the network. They show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed   convergence. They believe their analysis technique is useful in reasoning about more complex model architecture modifications."
SP:3e3bc8f617df742a395e7d315ec3810a42071294,"This paper makes explicit the bias of initialization on strongly overparametrized neural networks towards kernel methods in generalization under gradient descent. They prove that fully connected wide ReLU - NNs trained with squared loss are essentially a sum of two parts : The first is the minimum complexity solution of an interpolating kernel method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences : ( 1 ) the second part becomes negligible in the regime of small initialization variance, which allows to transfer generalization bounds from minimum complexity interpolating kernels to NNs ; ( 2 ) in the opposite regime, test error of wide NNs increases significantly with the initialization variance and thus provides a novel criterion to identify good initialization strategies."
SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,This paper proposes a new depth - based object detection framework based on pseudo - LiDAR that uses depth information from depth - aware stereo cameras to replace the expensive LIDAR in semi - radar - based methods. The proposed method is based on two components : 1 ) depth propagation algorithm that learns depth directly end - to - end instead of through disparity estimation ; and 2 ) GDC - based sparse GNN that combines object detection and depth estimation. The pseudo - lidar framework is evaluated on the KITTI object detection benchmark and shows slight improvements over the state - of - the - art methods.
SP:983d84502264633f3385d426c1d4601a0744ea9a,"This paper proposes a method for adversarial example detection and classification based on generative modeling. The proposed approach is inspired from the one - against - the - rest framework, where adversarial examples are perturbed from other classes and the goal is to find a class label that can identify whether the input is of class k under the assumption that the label is likely to belong to k-1 or k-2 classes. The authors propose to train K detectors in a way similar to one - face adversarial training, where the i - th detector is trained to discriminate natural data of class i from adversarial samples of class ii under a norm - constrained white - box attack, and at inference time, the k detectors are trained based on the class label prediction. Then, the proposed approach uses a generative model to predict the label of the input given the predicted label, and the k - detectors are used at inference to classify whether it belongs to k - 1 class or k - 2 class. The paper provides a comprehensive evaluation of the proposed adversarial detection and classification methods, and demonstrates their competitive performances and compelling properties."
SP:461e9308d050bc3dc7b35233452668bb31f5d491,"This paper proposes Rewarding Impact - driven Exploration ( RIDE ), a novel intrinsic reward for exploration in RL that encourages the agent to take actions which result in impactful changes to its representation of the environment state. The authors evaluate their method on multiple procedurally - generated tasks in MiniGrid, as well as on tasks with high - dimensional observations used in prior work. Their experiments demonstrate that RIDE is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, they analyze the learned behavior and intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control."
SP:c002c20b5e8696588e029c0f65e88860418826c4,"This paper studies the problem of large - scale document retrieval, i.e., given a query, retrieve the set of relevant documents ( e.g., paragraphs containing the answer ) from a large document corpus. The retrieval process consists of two steps : retrieval phase first reduces the solution space, returning a subset of documents to the problem solution, and scoring phase which re - ranks the documents. Critically, the retrieval algorithm which is desired to be highly efficient, returning candidates in time sublinear to the number of documents. This paper studies Transformer - based retrieval models with and without pre - training tasks, which are paragraph - level tasks such as Inverse Cloze Task ( ICT ), Body First Selection ( BFS ), Wiki Link Prediction ( WLP ), and the combination of all three. They show that without these tasks, the two - tower Transformer model with random initialization ( No Pretraining ) and MLM ( Unsuitable token - level pretraining ) are no better than the robust information retrieval baseline BM - 25. However, with these pre - trained models, retrieval algorithms such as TF - IDF, ICT, BFS, and WLP can improve over the baselines and vice versa."
SP:4e161e08a624f87633dfb49dfd46bd1665e15189,"This paper proposes a novel method to replace graph convolution operations such as graph skip connections and graph pooling in hierarchical graph neural networks with a single parametric bipartite Graph Convolution operation. The proposed method is called BiGraphNet and it aims to answer two questions : 1.    - Is it necessary to use pooling? - Can we accelerate computations and lower memory requirements by eliminating pooling layers? - How do we select the optimal node to aggregate all the vertices in a graph?   The main contribution of the paper is to propose a method to split the input graph into two parts : an input and an out graph. This innovation allows for development of computational layers for the graph that are analogues to layers used in lattice convolutional networks such as strided convolutions, deconvolution and skip connections. Such modules are critical components for building hierarchical representations of graph based data sets. The authors compare BiGraphNets with several baselines and show that they can generate comparable or better performance."
SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"This paper proposes a few - shot classification framework that can generalize to unseen domain due to the large discrepancy of the feature distribution across domains. The proposed framework uses the feature - wise transformation layer to simulate various feature distributions extracted from the tasks in different domains. Then, the proposed method applies a learning - to - learn approach to search for the hyper - parameters of the features in the affine transform layer to further improve the performance. Experiments are conducted on three metric - based methods, i.e. MatchingNet, RelationNet and Graph Neural Networks, with extensive experiments on five datasets ( mini - ImageNet, CUB, Cars, Places, and Plantae ). Comparisons are performed against baselines and show consistent improvement over the baselines under the domain generalization setting."
SP:df46627cb984a56bba36d510bfc52e00751e9107,"This paper proposes a convolutional neural network for Lagrangian fluid simulation based on N - D convolutions in the continuous domain. Different from prior work, the authors do not use a graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. They show that their network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. The authors also demonstrate that their continuous convolutions outperform prior formulations in terms of accuracy and speed."
SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"This paper proposes BatchEnsemble, an ensemble method whose computational and memory costs are significantly lower than typical ensembles. The key idea is to define each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank - one matrix per member. The paper shows that this method is parallelizable across devices, where one device trains one member, and parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini - batch. They also apply it to lifelong learning, where it yields comparable performance to progressive neural networks while having a much lower computational andmemory costs. The speedup at test time is 3X faster and memory reduction is 3 X at an ensemble of size 4."
SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"This paper proposes a neural network - based unsupervised solver for solving nonlinear PDEs. This method can be unified under the forward and inverse problems, where the optimized loss function consists of few elements : the L2 and L∞ - norms that unlike previous methods promote a strong solution. The proposed method is grid free, mesh free and shape free, and the solution is approximated by a network. The loss function is an explicit smooth differentiable function with known analytical form. This framework enables the solution of high order non - linear PDE. The authors demonstrate their method on several free shape 2D second order systems with application to EIT, diffusion and wave equations."
SP:973d0ad0faadcf7298300f2758de9154205e7113,"This paper proposes a new method to train binarized neural networks ( BNNs ), a class of neural networks that allows equivalent representation in Boolean logic and can be analyzed formally with SAT solvers. The main bottleneck for existing methods is their ability to reason about large BNN efficiently - the main bottleneck is the architecture of the BNN.   The authors propose changes to BNN architecture and the training procedure to get a simpler network for SAT solver without sacrificing accuracy on the primary task of SAT verification. Specifically, they propose a modified training procedure that makes the resulting network easier for logic - based verification tools, like SAT solver or approximate model counting solvers, to understand about. The proposed method scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets."
SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"This paper studies the expressive power of graph neural networks ( GNNs ), mainly focusing on the message - passing framework. The main takeaway is two - fold :   1. Graph neural networks are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. 2. It is discovered that GNNnmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Specifically, several of these problems are deemed impossible unless the product of a GNNmp ’s depth   and width exceeds a polynomial of the graph size ; this dependence remains significant even for tasks that appear simple or when considering approximation.   Overall, the proposed results demonstrate that the power ofGraph neural networks depends critically on their capacity and illustrate the importance of using discriminative node attributes."
SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"This paper proposes a generative flow framework, called localised generative flows ( LGF ), which is composed of continuous mixtures of bijections to learn a local distribution for each target bijection. The authors argue that traditional flow - based density models are limited in their ability to learn target distributions with complicated topologies, and propose an LGF to address this problem. The proposed LGF is a generalization of the normalising flow method, which can be used without modification as the basis for an LGFs. Unlike normalising flows, the proposed LGFs do not permit exact computation of log likelihoods, but a simple variational scheme is proposed. Empirically, the authors show that the proposed method yields improved performance across a variety of density estimation tasks, and some extensions to the LGF are discussed."
SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"This paper studies the role of environment bias in vision - language navigation ( VLN ) and tries to explain where and why this environment bias exists. To this end, it performs two experiments : environment re - splitting and feature replacement. Through these experiments, it finds the source of the environment bias to be in the low - level visual appearance, and proposes three ways to eliminate it : 1 ) detect object labels, 2 ) ground truth semantic views, and 3 ) learned semantic view features. Each of these semantic features significantly reduce the performance gap between seen and unseen environments on multiple datasets ( R2R, R4R, CVDN ), and achieve competitive unseen results to previous state - of - art models."
SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"This paper proposes an approach to integrate human feedback ( ErrP ) into the training of deep reinforcement learning ( DRL ) algorithms without requiring any explicit human interaction ( expert labeling, demonstrations ). The ErrP consists of electrodes placed on the scalp of a human, which measure an individual human ’s event - related potential ( EEG ) corresponding to an individual state in a discrete grid environment, which is then converted to an error potential ( EPM ) by encoding and decoding. This EPM is then used as an auxiliary reward function to train an RL algorithm.    The main contribution of this paper is the method developed to encode and decode the EPM of an adult human for training state - action pairs in an Atari environment. The method is shown to generalize the error - potential of the human to other Atari - like environments and to be able to provide error potentials that are general enough to be used as auxiliary reward functions in other environments. The paper also proposes two ways to incorporate ErrP : ( 1 ) integrate ErrP in the training loop of RL agent based on active learning ; ( 2 ) learn a reward function from imperfect demonstrations labeled by ErrP ; and ( 3 ) scale the ErrP based RL based RL to reasonably complex environments."
SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"The paper proposes a laconic classification framework that aims to minimize the amount of information ( aka entropy ) required in individual test images to maintain correct classification. Given a classifier and a test image, they compute an approximate minimal - entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The notion of entropy offers a unifying metric that allows to combine and compare the effects of various reductions, e.g., crop, colour, and resolution reduction on classification performance, in turn generalising similar methods explored in previous works. The paper proposes two complementary frameworks, namely : - L - entropy ( L ), which combines the notion of minimal entropy positive images of both human and machine classifiers, in experiments over the ILSVRC test-set - L.A.L.C.R. ( Laconic Classification of Neural Nets ), where a neural network ( DNN ) pretrained on the test images is trained to output a classification for each instance, and this pretrained classifier outputs an entropy estimate for that instance. - MLE ( MLE ) decomposes the latent space of a latent representation to a set of latent representations, and computes a classification error bound for each latent representation.   The paper evaluates the proposed approach on the CIFAR-10 dataset, and finds that the proposed framework outperforms previous methods in terms of accuracy, sensitivity to reduced resolution, and texture bias. Specifically, the paper finds that MLE leads to higher accuracy for MLE - trained with reduced resolution ( i.e., less cropping or reduced colour ) for machines, as well as reduced resolution for humans, than it does for humans."
SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,"This paper provides a detailed analysis of the robustness properties of perturbation based neural networks based on the observation that the adversarial examples are not robust and small perturbations to the attacking input often recover the desired prediction. The paper identifies a family of defense techniques that are based the instability assumption and empirically shows that all of them lead to similar gains in robustness. The defense techniques include deterministic lossy compression algorithms and randomized perturbational attacks to the input. The results are consistent across both deterministic and stochastic attacks that degrade the fidelity of the input example. The analysis is motivated by two main questions : ( 1 ) How do we determine the recovery window of the defense? The paper shows that adaptive attacks designed on a particularly strong lossy channel, such as the additive Laplace noise, are often successful against other defenses. And ( 2 ) how do we know which kind of transformation is used to defend the network? The attacker does not need to know the specifics of the transformation used by the network to design adaptive attacks."
SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"This paper proposes a neural 3D mapping network that takes as input 2.5D video streams captured by a moving camera, and lifts them to a 3D feature map by disentangling the scene content from the motion of the camera. The proposed model also projects its feature map to novel viewpoints, to predict and match against target views. The authors propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data. The experiments show that the proposed model learns visual representations useful for ( 1 ) semi - supervised learning of 3D object detectors, and ( 2 ) unsupervised learning for 3D moving object detectors. The model also learns dense 3D motion fields that allow objects to emerge without any human supervision."
SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"This paper studies the problem of unsupervised domain translation ( UDT ), which consists in finding meaningful correspondences between two domains without access to pairings between them. Inspired by the CycleGAN model, many variants and extensions of this model have been applied successfully to a wide range of applications, but these methods remain poorly understood and lack convincing theoretical guarantees. In this paper, the authors define UDT in a rigorous and non - ambiguous manner, explore the implicit biases present in the approach and demonstrate the limits of theses approaches. Specifically, they show that mappings produced by these methods are biased towards low energy transformations, leading them to cast UDT into an Optimal Transport ( OT ) framework by making this implicit bias explicit. This allows them to provide theoretical guarantees for existing methods, and also to solve UDT problems where previous methods fail. Finally, making the link between the dynamic formulation of OT and CycleGAN, they propose a simple approach to solve the UDT problem, and illustrate its properties in two distinct settings :    1. Given pairs of elements from two different domains, domain translation consists in learning a mapping from one domain to another, linking these paired elements together.   2. For some tasks, obtaining paired examples can be difficult thus motivating the unpaired setting where only samples from both domains are available, which allows us to tackle a wider range of problems, and is used to tackle image - toimage, video - to - video, image captioning, natural language translation (Bahdanau et al., Zhang et al ), etc...   3. A dynamical formulation allows to build a model which overcomes the shortcomings of CycleGAN - like models, and has been applied to many applications, such as translation, language translation, semantic translation, etc."
SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,"This paper proposes a new dropout method, RotationOut, as an alternative to standard dropout for neural networks. In contrast to dropout, which treats each neuron in a neural network as an individual channel individually, rotationOut considers the input of each neuron as a vector, and introduces regularization by randomly rotating the vector. The proposed method is mainly aimed at convolutional layers, but it can also be applied to recurrent layers and some layers of fully - connected networks. The paper provides noise analysis to show how co - adaptation is reduced by rotation out, and shows how dropout is more effective than dropout. Extensive experiments are conducted to show the effectiveness of the proposed method in vision and language tasks."
SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"This paper proposes a method to generate Universal Adversarial Perturbations ( UAP ) for a given CNN in a data - free manner. This is achieved by approximating the adversary generation with full training data under certain conditions. To do so, the authors propose a sequential optimization of the adversarial perturbation with the proposed dilate loss. Dilate loss basically maximizes the Euclidean norm of the output before nonlinearity at any layer. By doing so, it constrains the ReLU activation function at every layer to act roughly linear for data points and thus eliminate the dependency on data for crafting UAPs. The authors perform extensive experiments and ablations to demonstrate that their method achieves higher fooling rate than the existing UAP baselines."
SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"This paper proposes a new method for transfer learning for Neural Architecture Search ( NAS ), which is based on meta - learning. The main idea is to learn a meta - architecture to be able to adapt to a new task easily and quickly through a few gradient steps, whereas the existing NAS methods are inefficient or not optimal for adapting to new tasks. To achieve this, the proposed method, called T - NAS, learns a meta architecture that can be transferred easily between tasks. In addition, to optimize the whole search network, the authors propose an efficient first - order approximation algorithm. Experiments are conducted on few - shot learning and supervised learning. In supervised learning setting, the method achieves comparable performance with other baselines but the searching cost is decreased by 50x."
SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"This paper proposes a stochastic neural network ( SNN ) called SE - SNN. Compared to existing SNNs, the proposed SNN is simpler to implement and faster to train, and produces state - of - the art results on network compression by pruning, adversarial defense and learning with label noise. The proposed model is related to VIB and variational dropout, but provides a simpler and more direct realization via neuron regularization by a non - informative activation prior."
SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"This paper proposes a meta - learning algorithm to generate curiosity mechanisms to guide an agent to behave provocatively in a search space. The motivation is that curiosity is a learned mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. The paper proposes to formulate the problem of generating curious behavior as one of meta learning : an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent’s reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta -RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, the authors propose to meta - learn algorithms : pieces of code similar to those designed by humans in ML papers that combine neural networks with other building blocks such as buffers, nearest - neighbor modules and custom loss functions. They demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human - designed published curiosity algorithms in various benchmarks."
SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"This paper proposes a structured language modeling ( SLM ) approach for code generation under the Any - Code - to - Code Generation ( AnyC2C ) framework. The goal is to solve the problem of generating code given its surrounding code without any restriction on the vocabulary or structure. The proposed approach is based on the seq2seq approach, which is the state - of - the - art in code generation. In contrast, the proposed approach does not treat code as a sequence and leverages the strict syntax of the programming languages to model a code snippet as a tree. The main idea of the proposed SLM is to compute the probability of the program ’s abstract syntax tree ( AST ) by decomposing it into a product of conditional probabilities over its nodes. The authors present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Theoretical analysis is provided to show that the proposed model generalizes most previous work in this area, while reaching sine - state performance on of challenging benchmarks."
SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"This paper provides theoretical analysis of non - convex optimization of large - scale neural networks ( NNs ) based on canonical space, a tool introduced for training neural networks. The authors show that the objective functions in learning NNs are convex in the canonical model space, under certain conditions. They further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so - called disparity matrix. Finally, they show that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank.   The theoretical results have well explained common practices widely followed by deep learning practitioners, such as random initialization and gradient descent."
SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,This paper presents two graph - based multi - label segmentation algorithms for ground truth image classification on PASCAL VOC and Cityscapes validation sets with pixel - by - pixel pixel ( i.e. pixels of the same label that have global connectivity.   The first algorithm is based on a discrete instance - aware Potts model with a class - aware heuristic and an ILP formulation. The second algorithm is also based on the Potts heuristic but with a graph - aware formulation. Both algorithms can be trained with DCNNs trained on the target dataset or any DCNN can be used as input. The paper provides competitive semantic ( and panoptic ) segmentation results on the VOC dataset given initial scribbles. They also demonstrate that their interactive approach can reach 90.6% mIoU on VOC validation set with an overhead of just 3 correction scribble. They are suitable for interactive annotation on new or existing datasets. They can also be used inside any weakly supervised learning framework.
SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"This paper proposes a novel method to detect adversarial examples generated by adversarial perturbations by using a learned saliency model. Previous work has shown that a gradient - based saliency map is not effective as an adversarial defense due to a shift in the salient features that is captured by the perturbation. This paper proposes to use a learnt ( by learning a pixel - by - pixel ) model to capture the shifts in the saliency while keeping the gradient of the classifier constant. The proposed method is tested on MNIST, CIFAR-10, and ASSIRA against adversarial attacks such as C&W and DeepFool. It is shown to outperform a simple baseline that uses the same model but without the sigmoid function. Lastly, even when trained on weak defenses, the method is able to detect strong attacks."
SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"This paper considers the problem of computing the global adversarial robustness of a given machine learning model, i.e., the probability that its prediction at any point sampled from the ( unknown ) input distribution is susceptible to adversarial attacks. The authors derive concentration inequalities that bound the error bound of global robustness with respect to any $ \tilde{O}(\epsilon)^2 $ estimation error upper - bounded by, for any $ t$ selected a priori. They then apply the methods to provide bounds on the generalization error of PAC for the evaluation of the generalisation error of learning models ( McAllester, 1999 ; Vapnik, 2013 ; Xu & Mannor, 2012 ; Gourdeau et al., 2019 ) and their robustness, similarly to those used in the previous work. However, whereas PAC aims at bounding the generalizability of a family of models, whereas our method yields bounds on a specific trained model. Additionally, when computing global   robustness  , they focus on analyzing the probability    that a perturbation applied to a test point causes a prediction change, independently of the prediction ground truth. They use the presented techniques to obtain statistically sound estimation of the robustness profile for an array of NN architectures and training methods ( e.g., stochastic gradient descent, iterative pruning ), using FGSM to approximate computation of local robustness. They empirically observe that robustness and accuracy are negatively correlated for networks trained via SGD and SGRD, while a positive trend is observed between them in Bayesian networks."
SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"This paper studies the problem of robust reinforcement learning ( BR ), which aims to find an optimal policy with some degree of robustness to environmental dynamics. The authors leverage the Wasserstein distance to measure the disturbance to the reference transition kernel. The derived theoretical framework can be reformulated into a tractable iterated - risk aware problem, and the corresponding risk - aware optimal Bellman equation is derived. Then, the authors show the existence of optimal robust policies, provide a sensitivity analysis for the perturbations, and then design a novel robust learning algorithm ( WRAAC ). Experiments on the Cart - Pole environment verified the effectiveness and robustness of the proposed approaches."
SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"This paper proposes a method to approximate mixed strategy Nash equilibria in multi - player continuous games, which always exist and include the pure ones as a special case. The proposed method adopts the Pushforward measure technique to represent a mixed strategy in continuous games. It generalizes the Gradient - based Nikaido - Isoda function to measure the distance between the players ’ joint strategy profile and a Nash equilibrium. Applying the gradient descent algorithm, the approach is shown to converge to a stationary Nash equilibrium under the convexity assumption on payoff functions. In numerical experiments, the proposed method consistently and significantly outperforms recent works on approximating Nash equilibrium for quadratic games,   including Delta - Dirac GAN and GAMUT games."
SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"This paper proposes Neural Execution Tree ( NExT ), a method to utilize natural language ( NL ) explanations for augmenting training data for text classification using NLP tasks with unlabeled examples. The paper tackles two challenges in NL explanations : ( 1 ) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics ; ( 2 ) Often, NL explanations have large numbers of linguistic variants, resulting in low recall and limited generalization ability. To address these issues, the authors propose four modules to generalize the different types of actions in logical forms, which substantially increase the coverage of NL explanations. Experiments are conducted on two tasks ( correlation extraction and sentiment analysis ) to demonstrate the superiority of the proposed method over baseline methods, and an extension to multi - hop question answering achieves performance gain with light annotation effort."
SP:a9b5f7257dedd719cfe341fca275776734af1d98,"This paper extends the formal verification of machine learning models to recurrent neural networks ( RNNs ). The authors first verify the robustness of Reinforcement Learning ( RL ) models trained on ImageNet under the supervision of Pertsch et al. ( 2018 ) and then apply it to DNNs trained via verified training. In particular, temporal properties are commonly desired, particularly in settings where the outputs of the model are not sequential. For example, a robot that never leaves its charging station, or a language model that produces bounded length sentences, are temporal properties that are not satisfied by standard RL models. In addition to the RNN setting, the authors also consider auto - regressive RNN settings, where the output of a model conditioned on a prior model is a sequential concatenation of several previous examples. They verify that the trained model is robust to perturbations of the input features under each of two conditions : ( 1 ) the model's output is a fixed length and ( 2 ) the temporal dependence of the inputs is bounded. In the experiments, they show that trained with verified training outperforms training with standard training in terms of test error and test failure, and also outperforms models trained with adversarially perturbed training."
SP:3903680e07b676409e3cf6a1044b67291fe38630,"This paper proposes a method for performing visual domain randomization ( VDN ) on reinforcement learning agents that is meant to improve generalization across visual domains. The authors argue that standard VDN is inefficient as it only randomly perturbs one visual domain at a time, and propose to apply a form of regularization during training that tries to minimize the Lipschitz constant associated with each visual domain, i.e., the expected value of a state in a visual domain that is sampled from the learned state space when applying VDN. The paper shows that minimizing this constant can lead to a low variance in the learned policies. To achieve this, the authors propose a regularization method where the agent is only trained on one variation of the environment, and its learned state representations are regularized during training to minimize this constant. Experiments are conducted on a toy gridworld environment and compare the proposed method with VDN and other regularization techniques. The results show that the proposed regularization technique is more efficient and generalizes better compared to other methods of VDN in this setting."
SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"This paper proposes a DRO framework for Deep metric learning ( DML ) based on the pairwise binary classification problem. The authors cast DML as a simple pairwise classification problem that classifies a pair of examples as similar or dissimilar. They identify the most critical issue in this problem -- imbalanced data pairs -- and propose a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini -batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the uncertainty decision set of the dual variable allows us to recover state - of - the - art complicated losses and also to induce novel variants for DML. Empirical studies on several benchmark datasets demonstrate that the proposed variants outperform the baselines and general DML methods on benchmark datasets."
SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"This paper proposes two stochastic trust region ( STR ) algorithms to solve the non - convex finite - sum minimization problem. The first method, called STR1, is a mix of inexact gradient and Hessian estimation. It is shown to converge to a local minimum of $ \mathcal{O}(\sqrt{n})$ within $ \tilde{\sum_\infty}/\mathbb S^{n/\omega}$, where $ S^{(, \infty ) } \leq \sqrt { \mathbf{n}\infty } \log n/ \omega }. The second method, named STR2, is based on replacing the gradient estimator in STR1 with a Hessian estimator, which is claimed to be more accurate. Theoretically, it is shown that under certain assumptions, the second - order oracle queries of STR1 achieve a complexity of $ O(\frac{O}{\text{m}}/2})$, while the first - order query of STR2 attains $ \Omega(n^3/4/1.5)$. Hessian - free versions of the algorithms are also proposed.  "
SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"This paper introduces Farkas layers, a method that ensures at least one neuron is active at a given layer using elementary results from linear programming. The authors focus on residual networks with ReLU activation and demonstrate a significant improvement in training capacity in the absence of batch normalization or methods of initialization across a broad range of network sizes on benchmark datasets.   The key contribution of the paper is the introduction of a non - dead component of the network, i.e., the green hyperplane, which is defined by the ratio of the distance between each coordinate in the hyperplane matrix to each neuron in the network and is determined by the interaction of the geometry of the weights with the data. By simply adding one linearly dependent row to the weight matrix, the authors ensure that no neurons are dead at any layer. In this work, they have shown that training with larger learning rates is possible even with default initialization."
SP:1d325b148e3efe407241c1f1cbe8d17400499741,"The paper considers the task of computing exact robustness certificates for deep neural networks under adversarial attacks. In this setting, the authors aim to improve the robustness of the classifier in the presence of perturbations that are at least as large as the perturbation value that would be expected from a trained classifier. This requires solving a nonconvex optimization problem. To this end, the paper proposes two methods : ( 1 ) a curvature - based robustness certificate ( CROWN ) and ( 2 ) curvature regularization ( CRT ). The former is based on obtaining a differentiable upper bound on the curvature of the activation function of a deep neural network, and ( 3 ) using the upper bound as a regularizer in the training of the network to boost its certified accuracy against adversarial examples. Experiments on the MNIST dataset show that the proposed CROWN and CRT methods outperform CROWN in terms of certified accuracy and robustness, respectively."
SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"This paper proposes a compressed sensing recovery method based on the recently proposed Deep Image Prior ( DIP ) algorithm. In DIP, convolutional weights of the model are optimized to match the observed measurements in a linear inverse problem. This paper shows that DIP can be generalized to solve any differentiable linear inverse problems and outperforms previous unlearned methods. The authors also propose a learned regularization method, which incorporates a small amount of prior information on the network weights to reduce reconstruction error, especially for noisy measurements. Finally, the authors prove that moderately overparameterized single - layer networks trained with DIP optimization can perfectly fit any signal despite the nonconvex nature of the fitting problem."
SP:23c0f621e6041003b59bf0532130760694cf6a4a,"This paper proposes a hierarchical reinforcement learning ( HRL ) framework TAIC for learning temporal abstraction from action sequences. The authors formulate the temporal abstraction problem as learning latent representations ( called options ) over action sequences, and derive theoretically on how to regularize the option space and give an applicable solution of adding constraints to option space. In the experiments, the authors try to reveal the underlying structure of the options space by visualizing the correlation between options and state changes. They show that the learned abstraction allows them to learn new tasks on higher level more efficiently. The results also demonstrate that learning temporal abstractions is an effective technique in increasing the convergence rate and sample efficiency of RL algorithms."
SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"This paper proposes a new layer - wise sampling method for training graph convolutional networks ( GCNs ) based on the bi - directional diffusion between the nodes at each layer. Conditionally, the proposed method samples the nodes layer by layer conditionally according to the factors of the diffusion between layers, conditionally based on two factors : ( a ) the influence of parent nodes of upper layer on the candidate nodes of lower layer and ( b ) the reverse influence of lower candidates on the upper parents of lower parents, and ( c ) the self - attention mechanism to learn suitable weights for different - hop neighbors during the training. The proposed method is shown to be able to address both the “neighbor explosion ” and “ over - smoothing ” problem, as well as the sparsity issue of current layerwise sampling algorithms. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model for training GCNs on graph classification tasks."
SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,"This paper presents STOVE, a state - space model for unsupervised video modeling and planning. It combines an image model and a dynamics model in a compositional manner, and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. The authors show how to perform joint inference and training, as well as how to extend the model to the RL setting. They then present their experimental evaluation, before touching on further related work and concluding. Lastly, they further demonstrate the strength of their model as a simulator for sample efficient model - based control in a task with heavily interacting objects."
SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"The paper proposes a variational autoencoder ( VAE ) that combines the best features of VAE and GAN, i.e. sharpness, coherent samples, and ability to encode observations into low - dimensional representations. The objective of the VAE consists of two parts : ( 1 ) generate samples that are linearly independent of each other and ( 2 ) maximize the divergence between the model distribution and the true data distribution. The first part can be optimized with the standard adversarial training, and the second one is the objective of a VAE. To achieve the former one, the authors propose to use an implicit likelihood by an adversarially trained discriminator. In order to achieve the latter one, an explicit likelihood such as Gaussian or Laplace is required, which are not suitable for modeling images in the space of pixels. To address this, the paper proposes to use the implicit likelihood of a generative adversarial network ( GAN ) trained with the loss function \� - Jeffreys divergence. The authors theoretically analyze the introduced loss function and show that it minimizes the corresponding divergence under optimal discriminators. They call their method as Implicit \lambda - JAE. In experiments, they demonstrate that their model achieves the state - of - the - art trade - off between generation and reconstruction quality on CIFAR10 and TinyImagenet datasets."
SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"This paper shows that adversarial attacks on CNN classifiers can be made to the Bayes - optimal classifier for certain class distributions, while for others optimal classifiers are robust to such attacks. The authors provide analytical results showing conditions on the data distribution under which all the class distributions can be attacked such that the classifier is made arbitrarily close to the optimal decision boundary. They show this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. They introduce new datasets of realistic images of faces and digits on which they show that for some classes the optimal classifer is robust and for others it is vulnerable to adversarial examples. In systematic experiments with many such datasets, they find that standard CNN training consistently finds a vulnerable classifier while large - margin methods often find a robust classifier with the exact same training data. The results suggest that the adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may be the result of suboptimal training methods used in current practice."
SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"This paper studies the impact of pruning a neural network with respect to top - 1 test set accuracy. It finds that certain examples, which they term pruning identified exemplars ( PIEs ) and classes are systematically more impacted by the introduction of sparsity. Removing PIE images from the test-set greatly improves top-1 accuracy for both sparse and non - sparse models. These hard - to - generalize - to images tend to be mislabelled, of lower image quality, entail abstract representations, atypical examples or require fine - grained classification. The findings provide important insights about when pruned models are qualified to make decisions on real world inputs."
SP:4b17edaa7ec6201891433320d85f9a415656b763,"This paper presents KG - A2C1, a RL agent trained for interactive fiction games ( IF ) that generates actions in the combinatorially large natural language action space of ( potentially hundreds of millions of ) actions. The agent is composed of two components : knowledge graph ( DK ) and template - based action space ( KS ). The DK serves as a means for the agent to understand its surroundings, gather information about the game, and disambiguate similar textual observations, while the KS acts as a measure of structure that enables to exploit the knowledge graph for language generation. Together they constrain the vast space of possible actions into the compact space of sensible ones. Experiments on a diverse set of 28 IF games shows wide improvement over TDQN, the current state - of - the - art templatebased agent. Finally, an ablation study replicates the performance on Zork1 using an action space six orders of magnitude larger than previous agents."
SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"This paper proposes a data - dependent Gaussian prior ( D2GPo ) to be added to the training of maximum likelihood estimation ( MLE ) for sequence prediction problems such as language generation. The idea is to counter negative diversity ignorance ( NDI ) of MLE, which is commonly used in such problems. NDI is caused by the fact that MLE assumes a one - to - all matching between the predicted sequence and gold - standard, consequently treating all incorrect predictions as equally incorrect. To counter this, the authors propose to add an extra Kullback - Leibler divergence term derived from comparing the data - independent Gussian prior and the one obtained from the detailed model prediction. The D2 GPo objective is then injected into the MLE loss. Experiments have been conducted on a variety of tasks, including supervised and unsupervised machine translation, text summarization, and image captioning. The proposed method consistently outperformed the baseline MASS."
SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"This paper studies the problem of confidence miscalibration in deep neural networks ( DNNs ), which is a mismatch between a model ’s confidence and its correctness. The authors propose to address this problem by replacing the widely used cross - entropy loss with a new loss, i.e., F focal loss. Focal loss replaces the cross entropy loss, which causes the model to become overly confident in its predictions, with the expectation that it will also be less confident about its predictions. The paper shows that this loss can be combined with temperature scaling in order to avoid the temperature scaling effect that can be caused by applying temperature scaling to the DNN during training. Experiments on a variety of computer vision ( CIFAR-10 / 100 ) and NLP ( SST, 20 Newsgroups / Newsgroups ) datasets, and with a wide variety of different network architectures, show that focal loss achieves state - of - the - art accuracy and calibration in almost all cases. In addition, the authors provide a principled way of getting sample - dependent f focal loss and show that it produces good results across datasets and models."
SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,"This paper proposes a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boils down to either linear ( LP ) or semidefinite ( SDP ) programming, where sparse connectivity of a network can significantly reduce the complexity of computation. The proposed LiPopt framework generalizes some existing methods available in the literature. Empirically, the authors have empirically demonstrated that it can tightly upper bound such constant. The resulting optimization problems are computationally expensive but the sparsity of the network can reduce this burden. The authors conduct experiments on networks with random weights as well as networks trained on MNIST."
SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,"This paper proposes a self - supervised learning approach for video features that results in significantly improved performance on downstream tasks, such as video classification, captioning and segmentation compared to existing methods. The method extends the BERT model for text sequences to the case of sequences of real - valued feature vectors, by replacing the softmax loss with noise contrastive estimation ( NCE ). The authors also show how to learn representations from sequences of visual features and sequences of words derived from ASR ( automatic speech recognition ), and show that such cross - modal training ( when possible ) helps even more"
SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"This paper proposes a simple yet effective framework that allows to select certain parts of the input data needed for the subsequent application of a given neural network in order to achieve a comparable model performance. The transfer of data between servers and clients can become a major bottleneck during the inference phase of a neural network, and the proposed approach aims to automatically select those parts that are needed by the network to perform well, while, at the same time, minimizing the amount of selected data. The approach resorts to various types of selection masks that are jointly optimized together with the corresponding neural network during the training phase. The individual selection criteria can be adapted to the specific needs of the task at hand, as well as the transfer capabilities between the server and the client. The experiments indicate that it is often possible to achieve good accuracy with significantly less data needed to be transferred, as shown by the fact that the model performance is not negatively affected by the selection criteria."
SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"This paper proposes a method to train a deep neural network ( DNN ) to detect out - of - distribution ( OOD ) samples without losing much of its classification accuracy on the test examples from known classes. Based on the Outlier Exposure ( OE ) technique, the authors propose a novel loss function that achieves SOTA results in OOD detection with OE both on image and text classification tasks. The proposed loss function includes two regularization terms where the first minimizes the l1 norm between the output distribution of the softmax layer of a DNN and the uniform distribution, while the second minimizes Euclidean distance. Experiments were conducted on CIFAR-10 and MNIST, MNITS, and compared with Mahalanobis distance - based classifier ( Lee et al., 2018b ) and DBPT. The combination of the two methods outperforms the original Mahalanoibis method in all of the experiments and to the best of the knowledge, achieves state-of - the - art results in the OO detection task."
SP:89bc528ef801182365ac279e8963803afccb391d,"This paper proposes an end - to - end deep learning model, called E2Efold, for RNA secondary structure prediction, which can effectively take into account the inherent constraints in the problem, such as asymmetric matrix distribution, that is much smaller than the space of all binary matrices {0,1}L×L. The paper also proposes to use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. The proposed model achieves superior performance on benchmark datasets, as compared to the previous SOTA models."
SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"This paper proposes a setting where each agent internallyises their own predictive model of the environment and forms a virtual simulation within which the agent plays trials of the episodes in entirety. The goal is to develop a collective policy trained solely inside the virtual simulation that can then be transferred to the real - world environment. The idea is to have each agent take turns to host virtual episodes within which all agents participate and interact with their own biased representations. Since agents’ biases vary, the collective policy developed while sequentially visiting the internal simulations will complement one another ’s shortcomings. The experimental results show that collectively trained policies achieve significantly higher returns than the best individually trained policies."
SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"This paper proposes a new model for open - domain dialogue generation, which tackles the generic response problem. The proposed model learns a semantic latent space, on which representations of semantically related sentences are close to each other. This latent space is learned by maximizing the correlation between the features extracted from prompt and responses, and learning the pair relationship between the prompts and responses as a regression task on the latent space. Experiments are conducted on two dialog datasets, and show that the proposed model generates coherent and specific responses."
SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"This paper proposes Gaussian light and shadow ( GLAS ) to explain the spatial impact of neural networks on fine - grained classification by perturbing the features in an image with a Gaussian distribution. The method is based on the observation that the effects of lighting and shadowing in a black - box model are strongly influenced by its relative magnitudes in the presence and absence of light, as observed in nature. The main advantage of this method over existing methods is that it is fast to compute ( in ~0.5 sec ) and generates a visual explanation, which is faster than existing methods. In experiments, GLAS shows state - of - the - art performance and efficient computing time. It is also shown the broad applicability of GLAS to various tasks."
SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"Convolution is a central operation in Convolutional Neural Networks ( CNNs ) which applies a kernel to overlapping regions shifted across the image. However, because of the strong correlations in real - world image data, convolutional kernels are in effect re - learning redundant data. This paper proposes a procedure called network deconvolution, which optimally removes pixel - wise and channel - wise correlations before the data is fed into each layer of CNNs to remove them. The paper shows that this procedure can be efficiently calculated at a fraction of the computational cost of a convolution layer. Network deconvolutions are evaluated on 10 modern neural network models by replacing batch normalization within each of them with the proposed method, and show performance improvement."
SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"This paper studies the problem of quantizing weights in generative adversarial neural networks ( GANs ). The authors first conduct an extensive study on the effectiveness of typical quantization methods which are widely used in CNNs. Their observation is that none of them generates samples with reasonable quality because of the underrepresentation of quantized weights in the models, and the generator and discriminator networks show different sensitivities upon the quantization precision. Motivated by these observations, they develop a novel quantization method for Gans based on EM algorithms, named as QGAN. Also, they propose a multi - precision algorithm to help find an appropriate quantisation precision of GAN given image qualities requirements. Experiments on CIFAR-10 and CelebA show that QGAN can quantize weights in GAN to even 1 - bit or 2 - bit representations with results of quality comparable to original models."
SP:58c4905f59f04a50b30d27c99521126a6455d38a,"This paper studies global last - iterate convergence of Hamiltonian gradient descent ( HGD ) in settings that are not linear or strongly convex in either input. In particular, the authors introduce a novel “ sufficient ” condition on the second - order derivatives of the objective g and show that this condition is sufficient for HGD to achieve linear convergence in convex - concave settings. They also show convergence rates for stochastic HGD and for some parameter settings of Consensus Optimization algorithm of Mescheder et al. ( 2017 ), which is a perturbation of HGD. In the last part of the paper, they discuss how their results can be applied to various settings, including a simple Dirac - GAN. Theoretically, they show that HGD will converge globally to the Nash Equilibrium ( NE ) of this Dirac GAN, as shown in Balduzzi et al ( 2018 ). Moreover, Lemma 2.2 shows that the blocks of the NE are 0 at any arbitrary f with a non -zero derivative. In this paper, the convergence rates are also shown to be within a region around the $ \tilde{O}(\sqrt{k})$ region for arbitrary Jacobian diagonal."
SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"This paper studies the stability of ResNet block hl = φ(hl−1 + τ ) where φ is ReLU activation and $ \tau$ is a scalar. The authors find that for standard initialization used in practice, $ tau = 1/\�( \� L ) $ is a sharp value in characterizing the stability, where L is the number of residual blocks. They also provide the global convergence result for learning ResNet with $ T$ :   for $ t \leq 1 / \�( L / L ) \log(L)$. Moreover, they show that deep ResNet can be effectively trained without normalization layer, and deep Res net with normalization does not perform well in practice. On the other hand, adding $ Tau$ on top of normalization can improve the performance."
SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,"This paper proposes a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense - to - sparse training methods. The proposed method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. They show that this approach requires fewer floating - point operations ( FLOPs ) to achieve a given level of accuracy compared to prior techniques. The authors demonstrate state - of - the - art sparse training results with ResNet-50, MobileNet v1 and MobileNet   v2 on the ImageNet-2012 dataset, WideResNets on the CIFAR-10 dataset and RNNs on the WikiText-103 dataset."
SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"The paper proposes a new method to control specific properties of the generated image like the position or scale of the object in the image by simply looking in the latent space of any generative model. The proposed method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations of thegenerated image, such as translation, zoom or color variations. The authors demonstrate the effectiveness of their method qualitatively and quantitatively, both for GANs and variational auto - encoders."
SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"This paper proposes a physics - as - inverse - graphics - based unsupervised system dynamics model that is able to infer physical parameters from a video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object - or state - supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. This paper proposes to address this problem through a physics-as - inverse graphics approach that brings together differentiable and learnable physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows to perform long - term extrapolative video prediction, as well as vision - based model - based predictive control. The results show that the proposed model significantly outperforms related un - supervised methods in long - horizon future frame prediction of systems with interacting objects, due to its ability to build dynamics into the model as an inductive bias. It is also shown to be able to learn via data - efficient learning of vision - acting - model - predictive - control for an under - acting pendulum system, which is used to exploit goal - parameterized policies and physical reasoning for zero - shot adaptation."
SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"This paper proposes a novel method to assign class relevance to noisy examples obtained by textual queries with class names. The noisy examples are partitioned into two groups : clean and noisy, and each group is given a few labeled examples. Clean and noisy data are modeled by a graph per class, and graph convolutional networks ( GCN ) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross - entropy loss function, and then the “ clean ” probability is exploited as a relevance measure. Each noisy example is weighted by its relevance when learning a classifier for the end task. The proposed GCN - based method outperforms the transductive approach ( Douze et al., 2018 ) that is using the same additional data without labels without labels but large - scale dataset."
SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"This paper proposes a new GNN model called Edge Information Maximized Graph Neural Network ( EIGNN ) that maximizes the Mutual Information ( MI ) between edge features and message passing channels. The existing GNNs have been studied extensively in various settings including regression on molecular graphs and relation prediction in knowledge graphs. In this work, the mutual information is reformulated as a differentiable objective via a variational approach. The authors theoretically show that the newly introduced objective enables the model to preserve edge information, and empirically corroborate the enhanced performance of MI - maximized models that are applied to learning tasks.   The main contributions of the paper are as follows :   1. The introduction of a new mutual information objective that enables the message passing channel of a GNN to be differentiable. 2. The application of the new objective to GNN models. 3. The use of the variational objective to learn the edge attribution. 4. The combination of 3 layers of message passing and 3 steps of set2set."
SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"This paper presents APPROXLINE, a method for verification of non - convex properties of neural networks. The key property of the method is that it is scalable to infinite number of inputs and output samples. The method is capable of performing both deterministic and probabilistic interpretation. It is shown to be able to verify the robustness of classifiers to different head rotations and different amounts of “ moustache ”, and it is faster and more precise than existing verification methods. It also shows that it can verify interesting interpolations in the latent space."
SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"This paper studies the problem of suspended animation of graph neural networks ( GCN ) based on the spectral graph convolutional operator. Such a problem occurs when the model depth reaches a certain limit, where the model no longer responds to the training data and becomes not learnable. In this paper, the authors introduce GRESNET ( Graph Residual Network ) framework to address this problem, which creates extensively connected highways to involve nodes ’ raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work, and hence, the introduction of Graph residual networks is introduced. Experiments on several existing GCN models, including GCN, GAT, and LOOPYNET, are provided to demonstrate the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid drastic changes to the node ’s representations between sequential layers."
SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"This paper tackles the problem of face reconstruction from an unlabeled set of 2D images of a single face, by learning a 3D model with identity, expression, pose, and lighting representations that can be transferred to another 2D image and used for reconstructing the original 2D face. The model is trained using a semi - supervised manner, where the training data is split into two batches : one with labeled face images only, and one containing all the other face images but not labeled in the labeled batch. The training is done in a way that ensures that the reconstructed face has the same identity shape, texture, and albedo, and that the pose, lighting and expression are similar to the original image. The paper also introduces a novel adversarial loss that aims to ensure that the reconstruction results from the same set have the same 3D geometry shape and texture. The proposed model also uses a novel center loss to ensure the identity and texture representations are close to each other. Experiments are conducted on a set of 5 face images ( 2D and 3D ) taken from a test subject of the same age, same year, same city, same age and same city ID. Results show that the proposed model can achieve state - of - the - art performance in face reconstruction, e.g., face replacement, face reenactment, expression transfer."
SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"This paper proposes a method for solving imitation problems using reinforcement learning ( RL ), when only partial knowledge about the transition kernel is available. The proposed method, dubbed Expert Induced Markov Decision Process ( eMDP ), aims to augment a training set by simulating the components of the state space for which the transition model is known ( i.e., { SR, Su } ), and extract from demonstrations the next state components for which ( kernel is unknown ), { s, su }. The key idea is to replace the unknown transition kernel with a synthetic kernel that can 1 ) simulate the transition of state components with known transition kernel, and 2 ) extract ( from demonstrations ) the next states with kernel that is unknown. The paper provides the recipe for building an eM DP and analyzes the errors caused by its synthetic kernel. The experiments include imitation tasks in multiplayer games, where the agent has to imitate one expert in the presence of other experts for whom we cannot provide a transition model. They show that combining a policy gradient algorithm with their model achieves superior performance compared to simulation - free alternative, and that solving it amounts to solving an imitation problem that seeks to match the state densities at each step. Finally, the paper shows empirical results that stress the benefits of using e MDP when the transition kernels are partially available and model - based approaches are not applicable."
SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"This paper proposes a self - supervised reinforcement learning ( SRL ) method called Mutual Information - based State - Control ( MISC ). MISC trains a policy to maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, MISC enables the agent to self - learn manipulation behaviors, such as reaching, pushing, picking up, and sliding an object. In contrast, in the navigation task, the MISC - trained agent learns to navigate to the target. The authors evaluate three variants of MISC in different tasks and demonstrate a substantial improvement in learning efficiency compared to state - of - the - art methods. A video showing experimental results is available."
SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"This paper proposes a novel and programmable trojaning method for training neural networks with large - scale models. The proposed method is based on the recently proposed Tensorflow - MXNet ( TRF, MXNet, Englebert et al., 2017 ) retraining tutorial from TensorFlow and ImageNet, which uses pre - trained ImageNet ( Deng et al, 2009 ) models to replace FC layers for the FC layers in the Flower dataset and Caltech - 256 dataset. The authors insert a trojan into the ImageNet model and attack the victim model for the two smaller datasets. The trojan remains effective for both cases.   The authors demonstrate their attack method under the scenario described in the retraining   tutorial described in Figure 1. The same trojaned model can affect victims using any other dataset. In addition, the trojan shows no biased behavior for different target classes, which makes it more difficult to defend."
SP:35ea626ee4dd1a7a368a660eb852192924966b7f,This paper proposes a few - shot regression algorithm for drug discovery tasks based on biological assays. It is motivated by the fact that there are relatively few FSR methods available for such tasks and that few of them are suitable for real - world applications such as drug discovery. The proposed algorithm is based on learning a deep network in combination with a kernel function and a differentiable kernel algorithm. The choice of kernel is critical and the proposed algorithm learns to find the appropriate kernel for each task through inference. It performs better than the state - of - the - art FSR algorithms on both toy and novel benchmarks that the authors introduce in the paper.
SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"This paper studies the problem of understanding why and when neural networks generalize better than others, although they have equal expressive power. The goal is to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. The paper formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, e.g., Graph Neural Networks ( GNNs ), but less structured networks fail to generalize. Theoretically, there is limited understanding of when and when not to use such structured networks. In this paper, the authors focus on unifying seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming ( DP ), which unifies thematically and analyzes their ability to solve these tasks. The theoretical results and the resulting framework are shown to work well on several reasoning tasks ( 1 ), 2 ), 3 ), 4 ), 5 ), and 6 ), on four categories of reasoning tasks with increasingly complex structure, applying to analyze which tasks some popular networks can learn and generalize well ( and vice versa ).   Theoretical results :   1. align well with algorithms that compute summary statistics over individual objects and thus are expected to solve intuitive physics tasks    2. GNN aligns well with DP and thus is expected to be good at intuitive physics because summary statistics are a special case of dynamic programming, which GNN can learn as well as compute shortest paths. 3. Similarly, GNN also has a good generalization performance on shortest path tasks."
SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,"This paper proposes a new metric, Fréchet Joint Distance ( FJD ), to measure image quality, conditional consistency, and intra - conditioning diversity in conditional generative adversarial networks ( cGANs ). The proposed metric is based on the joint distribution of the conditional generator and discriminator, and is defined as the ratio of the distances between the joint distributions of the discriminator and the generator. The authors claim that FJD implicitly captures the aforementioned properties in a single metric, and conduct proof - of - concept experiments on a controllable synthetic dataset that demonstrates the benefits of FJD when compared to currently established metrics, such as Image Quality Indicator ( FID ) and Image Quality Comparisons ( ICC ). Moreover, the authors use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities ( e.g. class labels, object masks, bounding boxes, images, and text captions ), and show that it can be used as a promising single metric for cGAN benchmarking and model selection."
SP:fa822e8472efae17c7dfde8258057898383ecbbb,"The paper proposes a method to learn to identify ‘ decision states ’, namely the parsimonious set of states where decisions meaningfully affect the future states an agent can reach in an environment. The method is based on the VIC framework ( Gregor et al., 2016 ), which maximizes an agent’s ‘empowerment ’ ( i.e., the ability to reliably reach a diverse set of state - relationships ), and formulate a sandwich bound on the empowerment objective that allows identification of decision states. The decision states are discovered without extrinsic rewards – simply by interacting with the world. The proposed method is evaluated on a goal - driven navigation task, and shows that it is able to identify decision states that align with human intuition and leads to better exploration."
SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"This paper proposes a framework for classifying time series with irregularly - sampled and unaligned measurements, that is non -synchronized observations. The proposed framework, called SEFT ( Set Functions for Time Series ), is based on recent advances in differentiable set function learning, is extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. The authors extensively compare their method to competitors on multiple healthcare time series datasets and show that it performs competitively while significantly reducing runtime. This is particularly relevant for medical and healthcare applications."
SP:4ae89d64460b08749acc192004545c1fa8b7553b,"This paper proposes a method to explicitly utilize the harmonic structure in the kernels of convolutional neural networks to better model the priors in audio signals. The authors claim that the current audio processing architectures do not model the audio signal priors naturally. They propose Harmonic Convolution, an operation that helps the convolution kernels to be supported by sets of harmonic series, instead of by local neighborhoods as convolutionsal kernels. They show that fitting a randomly initialized network equipped with such a method achieves high performance on unsupervised audio restoration tasks. They also show that with such method, they also achieve better generalization performance on supervised musical source separation tasks."
SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"This paper introduces “ data echoing ”, which reduces the total computation used by earlier stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses ( or “echoes ” ) intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. The authors investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. They find that in all settings, at least one data echoing algorithm can match the baseline’s predictive performance using less upstream computation. They measured a factor of 3.25 decrease in wall - clock time for ResNet-50 on ImageNet when reading training data over a network."
SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"This paper proposes Variational Intrinsic Successor FeatuRes ( VISR ), a variant of successor features ( SF ) designed to transfer learned policies to different tasks that differ only in their reward function, which is assumed to be linear in some features. The key idea of the paper is to use Behavioral Mutual Information ( BMI ) maximization to learn the features that maximize BMI in order to enable SF to be transferred to other tasks. The paper shows that the GI objective can be adapted to learn precisely the features required by SF. Together, these methods give rise to an algorithm, which significantly improves performance in the RL with unsupervised pre - training scenario, and outperforms all baselines, including methods that operate in three regimes :   1. Behavioral mutual information, which learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework. 2. A curiosity - based intrinsic reward inside of VISR, which encourages the agent to explore the space of controLLable policies."
SP:83500230586a9134f910ad067b7233dc563dc1ba,"The paper studies generalization of over - parametrized functional approximations of ReLu neural networks. It shows that generalization results from smoothness of the functional approximation, combined with a flat initial approximation. This smoothness increases with the number of units, which explains why massively overparameterized networks generalize well. Generalization is due to Flat Initialization in the Over - Parameterized Regime, the paper finds. The paper also investigates the effect of the curvature - based parametrization of the approximating function ( breakpoints ) and the role of gradient descent ( GD ) in preserving ( i ) and regularizing ( ii ). In particular, the global, rather than local, impact of breakpoints and delta - slopes helps regularize the approximate function in the large gaps between training data, resulting in their smoothness."
SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"This paper proposes a GAN - based method for image - to - image translation by incorporating an attention mechanism from the discriminator. Specifically, the attention mechanism consists of two components : ( 1 ) a probabilistic feature extractor and ( 2 ) attention map generator. The key idea of the attention generator is to concatenate the features of target images with the attention map of the auxiliary network to create an attention map to guide the generator to generate more plausible and realistic images. Experiments are conducted on a number of image transfer tasks, and qualitative results and quantitative comparison demonstrate the superiority of the proposed approach over the baselines."
SP:41c089ba65393174dae1dc136f79030a0a4fc532,"This paper proposes to re - examine the role of multiplicative interaction layers in various neural network architectures, such as hypernetworks, multplicative LSTMs, and multiplicative RNNs. They show that such layers strictly enrich the representable function classes of neural networks, and conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. They therefore argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft -used concatenation operation. Experiments on large - scale RL and sequence modeling tasks, where their use allows them to deliver state - of - the - art results, and thereby provides new evidence in support of multiplication interactions playing a more prominent role when designing new neural networks."
SP:5144391584e6d3825e12684b7c053e4e282cff2b,"This paper proposes a new algorithm for batch active learning, named Batch Active Learning by Diverse Gradient Embeddings ( BADGE ), which samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand - tuned hyperparameters. The authors show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well as or better than the best baseline across all experiments, which vary all of the aforementioned environmental conditions."
SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"This paper proposes a self - explanation method to make deep neural networks ( DNNs ) that are hard to interpret because each hidden layer has a mix of low level features and high level features. The authors propose to use a feature - level - based "" leveling "" method to separate the high - level features from the hidden layer features in order to better utilize the GLM layer in the proposed architecture for interpretation. To achieve this goal, the authors propose a per - layer feature leveler that learns to extract different levels of features from a DNN. The proposed method is evaluated on a toy dataset and compared with a standard DNN on MNIST and Fashion - MNIST. The results show that the proposed model is able to achieve competitive performance on both datasets while being more self - explainable."
SP:b70ceead1bf6c7dc684c74501716e7012b891022,"This paper proposes a method to improve the signal - to - noise ratio of the update of the stochastic gradient of the loss function when training a classifier over a large set of labels. The proposed method is based on a scalable approximation to the softmax loss function via a generalized form of negative sampling. By generating adversarial negative samples from an auxiliary model, the authors proved that they maximize the signal to noise ratio. While the auxiliary model introduces a bias, they can remove it at test time. The main contribution of this paper is the proof that this adversarial sampling minimizes the gradient variance while minimizing the bias. The authors also provide experimental results on large scale data sets that show a reduction of the training time relative to several competitive baselines."
SP:29b52fee83309268d9864f3b1fc3617948577d41,"This paper proposes a novel exploration method that leverages novelty measures in a model - free exploration method. The novelty measures are calculated by using a distance metric that relates the distance of the nearest neighbors of the agent in low dimensional state space to measure novelty. The state space representation is learned by performing gradient descent in - between each exploration step and the next environment step. This is done in a greedy manner where the agent takes the same number of steps but increases the distance at each step by a factor of sqrt(d / n ), where d is the distance between the agent and its nearest neighbors in state space. The agent is also trained to learn a representation of its state space using a planning routine in representational space. This representation is then used to explore the state space in a sample - efficient manner. The method is tested on a number of maze variants as well as a control problem. Results show that the proposed method is better than baselines in terms of sample - efficiency when compared to exploration methods that don't perform gradient descent between steps."
SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,"This paper proposes few - shot classification and out - of - distribution ( OOD ) detection as special cases of OOD, where only a few labeled examples per class are available to classify. The authors propose two new methods, -MinDist and LCBO, for OOD and OOD detection respectively. They establish new benchmark datasets, based on four popular OOD classification datasets, and compare their methods against existing methods and the baselines. They show that confidence scores developed in the supervised setting are not suitable when used with popular few -shot classifiers such as CIFAR-10 and ImageNet. They also show improved results with their proposed methods compared to existing methods on the new benchmarks.  "
SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,"This paper proposes a generalized model of sequence generation that unifies decoding in directed and undirected neural sequence models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, various neural sequences models are treated as special cases of the autoregressive, semi - autorgressive, and refinement - based non - auto - regressive models. This unification enables the authors to adapt decoding algorithms originally developed for directed sequence models to the unsupervised decoding of undirectED models. They demonstrate this by evaluating various decoding strategies for a cross - lingual masked translation model ( Lample and Conneau, 2019 ), such as position selection, symbol replacement, and constant - time variants of the proposed generation strategies, by testing them on WMT '14 En - De machine translation using a recently proposed masking model. Their experiments show that generation from the proposed unstructured neural sequence model, under their framework, is competitive with the state - of - the - art on the WMT ’14 English - German translation."
SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"This paper proposes a two - stage approach to generate mathematical expressions ( MEs ) from a printed mathematical expression image as input and generates LaTeX sequence as output. In the first stage, this method locates and recognizes the math symbols of the input image by object detection algorithm and in the second stage, it translates math symbols with position information into LaTeX sequences by seq2seq model equipped with attention mechanism. The authors claim that the detection of mathematical symbols and the structural analysis of mathematical formulas are carried out separately in two steps, which effectively improves the recognition accuracy and enhances the generalization ability. The experiment demonstrates that the two - step method significantly outperforms the end - to - end method. Especially, the ExpRate(expression recognition rate ) of the proposed model is 74.1 %, 20.3 % higher than that of the baseline model."
SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"This paper proposes a quantization method based on Product Quantization for reducing the memory footprint of convolutional neural networks with ResNet-50, Yalniz et al. ( 2019 ) and Mask R - CNN ( He et a., 2017 ). The key idea of the method is to minimize the loss reconstruction error for in - domain inputs by vector quantization. The method only requires a set of unlabeled data at quantization time and allows for efficient inference on CPU by using bytealigned codebooks to store the compressed weights. The authors validate their approach by quantizing a high performing ResNet - 50 model to a memory size of 5 MB ( 20x compression factor ) while preserving a top - 1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26x factor."
SP:74850ad70241948f93fed95ba1f0ac11360437c1,"This paper proposes a transformer - based attention mechanism, called TP - Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. The paper also proposes to incorporate Tensor - Product Representations within the Transformer in order to better support the explicit representation of relation structure. The TP - Attention goes beyond linear combination of retrieved values, strengthening representation - building and resolving ambiguities introduced by multiple layers of standard attention.   The paper sets a new state of the art on the recently introduced Mathematics Dataset containing 56 categories of free - form math wordproblems. The essential component of the model is a novel attention mechanism that encodes relation structure between each transformer cell and other cells. It beats the previously published state - of - art by 8.24 %."
SP:d319df820c6630c409fab32097652a083e8f53ea,"This paper provides a theoretical and experimental framework for understanding generalization in deep learning, defined as the difference between training and inference errors. The theoretical findings and experimental results show that a learned classification function must be sufficiently complex for a classification task in order to be closer to the true classification function. Another insight from this study is that concatenating encodings of input features to the original input features might help to achieve generalization    by enabling the classifier to learn relations between features not captured by the original inputs. Experiments demonstrate that a model trained on arbitrarily encoded input features is more robust to common corruptions and adversarial perturbations and that using more encoding may be beneficial to minimize the generalization error. The paper uses channel coding to improve the degree to which the training and test sets are representative of the empirical sample set."
SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,"This paper introduces a new graph pooling method based on compressive Haar transforms for deep graph neural networks ( GNNs ), called HaarPooling. The pooling process consists of a sequence of clusterings of nodes in the graph, followed by a Haar transform applied to each cluster. The final pooling layer is the concatenation of all the clusterings in the chain, with the corresponding Haar basis computed on the corresponding node. The proposed Haar pooling operation operates in the frequency domain by the synthesis of node in the same cluster, and filters out fine detail information by applying the compressive transform. It is shown that the proposed method achieves state - of - the - art performance on graph classification tasks on several benchmark datasets."
SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"This paper studies adapting PointNet ’s encoder network to match the semantics of the input point clouds, which has been shown in previous work to improve the effectiveness of point - cloud decoders over naive feedforward architectures. The main contribution of this paper is to study how to apply the lessons learned from PointNet’s semantic congruence with point clouds to a point cloud decoder design. Specifically, the paper develops three sample - based decoder architectures and compare their performance to each other and show their improved effectiveness over feedforward networks. In addition, it investigates the learned distributions to gain insight into the output transformation. The paper is available as an extensible software platform to reproduce these results and serve as a baseline for future work."
SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"This paper studies the generalization ability of deep neural networks ( DNNs ) on real - world noisy data. The main contributions of this paper are as follows :   1. The paper establishes a benchmark of real world noisy labels at 10 controlled noise levels to establish a benchmark for DNN performance on real world noise. This is done by comparing six robust DNN methods, i.e., ResNet, CNNs, ResNets, GNNs, DGGs, GATs, and LSTMs across a variety of noise levels and settings ( e.g., 1 - M, 5 - M M, 20 - M ).   2. This paper shows that deep networks generalize much better on synthetic data than real data. 3. When networks are fine - tuned, ImageNet architectures generalize well on noisy data, 4 ) Real - world noise appears to be less harmful, yet it is more difficult for robust networks to improve. 5 ) Robust learning methods that work well on synthetic noise may not work as well on real data, and vice versa. The authors hope this benchmark, as well as the findings, will facilitate future research on noisyData."
SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"This paper proposes a method for learning from a mix of clean and noisy supervision. The clean supervision is composed of rule - level and instance - level supervision, where the rule is implemented as a set of generalization rules and the instance is a collection of labeled examples under the supervision of a teacher. The teacher enforces a hard - implication loss over the coverage and label variables of the examples. The supervision is done by using a rule - exemplar network, where each rule is represented by a subset of the classifier's latent coverage, and the instances are represented by instances of a specific classifier under a specific set of teacher rules. The classifier is trained using a loss that penalizes the distance between the true label and the assumed label. Inference is performed using the jointly denoised classifier and the teacher rule. Experiments are performed on 5 tasks : question classification, spam detection, sequence labeling, and record classification. They compare the proposed method to several existing methods for learning with noisy supervision and constraints, and show much better results with our method."
SP:6f2c656dbb7629f652a4291d6971625184d8118b,"This paper proposes a memory layer for graph neural networks ( GNNs ) that can jointly learn node representations and coarsen the graph. The authors also introduce two networks based on the memory layer : memory - based GNN ( MemGNN ) and graph memory network ( GMN ). The former consists of a GNN that learns the initial node representations, and a stack of memory layers that learn hierarchical representations up to the global graph representation. GMN, on the other hand, learns the hierarchical representations purely based on memory layers and hence does not require message passing. The experimental results show that the proposed models achieve state - of - the - art results in 8 out of 9 graph classification and regression benchmarks. The learned representations could correspond to chemical features in the molecule data."
SP:81bc52d734c86975d741b6482d65ca71a9d81620,"This paper studies the convergence time of iid Gaussian and orthogonal initialization of deep neural networks. It shows for the first time that convergence time increases exponentially in the depth unless the iid weight matrix width is at least as large as the depth. On the other hand, convergence time scales linearly in the number of layers.    Further, it provides a proof that   iid gaussian initialization with iid weights speeds up convergence relative to standard Gaussian initialization. Finally, experiments are provided to support the theoretical and empirical findings."
SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"This paper proposes a uniform scalar quantization method to compress the bit - complexity of quantization of weights and activations in deep CNNs. Previous works have shown that 2 - bit bit compression can reduce the bit complexity of CNNs to 4 bit, but the accuracy drops to 2 bit. One of the issues with these methods is that they assign equal bit rate to quantization in all layers, which is not reasonable in the case of high - rate compression, such as 2-bit quantization, when some layers are sensitive to the quantization and performing coarse quantization on these layers can hurt the accuracy. This paper proposes an efficient method to solve the optimization problem via Lagrangian Formulation. It first explore the additivity of output error caused by quantized neural networks and find that additivity property holds for deep neural networks which are continuously differentiable in the layers. Based on this observation, they formulate the optimal bit allocation problem of weight and activation in a joint framework and propose a very efficient method, which they call the joint formulation method, as follows :   1. Quantize weights and weights $ \in [ \mathcal{L_1,\ldots]^n$ where $ L_1 = \sum_i \leq \sqrt{i \times \log n+\frac{t}{k\times t+1 } \log t-k$, 2. quantize activations $ \text{i,\text{a } \times t-1}$ which is a weighted sum of the output errors of each layer $ \times k$, 3. Then quantize the quantized activations by linearly computing $ t - 1 $, where $ t_1 $ is the number of quantized levels of the quantizer. 4. Quantized output errors for each layer are then entropy - coded according to $ \theta(i,t)$. The proposed method is shown to compress deep CNN ResNet-50 down to 2 bits with only 0.7 % accuracy loss."
SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"This paper proposes a new inference WGAN model, iWGAN, which is a principled way to fuse auto - encoders and GANs. The iWAGN jointly learns an encoder network and a generative network using an iterative primal dual optimization process. The authors establish a generalization error bound for iWANs and provide a rigorous probabilistic interpretation of the model. The proposed iwGAN has a clear stopping point and a metric for individual sample quality checking. The empirical results on both synthetic and benchmark datasets are state - of - the - art."
SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,"This paper extends the mention pair model of Paun et al. ( 2018b ) ( anaphora ), a crowdsourced model of anaphoric annotation for coreference, to a more structured setting where the number of annotators is hierarchical. The authors propose to use a Dirichlet process mixed with a randomly distributed pool of workers to create a non - parametric community model. The proposed model is evaluated on the Phrase Detectives 2 crowdsourced dataset, and compared to an unpooled version of the same dataset ( on par with the state of the art ) in terms of performance under conditions of sparsity, and on par when enough observations are available. The model is shown to be more resilient to sparsity in different crowdsourcing setups, and provides insights into the community of workers."
SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"This paper proposes successor feature control ( SFC ), a hierarchical approach to learning intrinsic reward signals for exploration in sparse reward reinforcement learning. In contrast to prior work, SFC learns intrinsic and extrinsic reward signals separately and uses them to schedule exploration. This paper proposes to use SFC for hierarchical exploration planning. In particular, it introduces a new type of intrinsic motivation, called statistics driven intrinsic motivation ( SID ), which takes into account statistics over complete trajectories. This is different from previous methods that only use local information to evaluate intrinsic motivation. Moreover, it proposes a new intrinsic reward SFC that is general and evaluates intrinsic motivation based on longer horizons. Experiments are conducted in three environments with pure visual inputs : VizDoom, DeepMind Lab and DeepMind Control Suite. The results show a substantially improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives."
SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"This paper proposes a method for weakly - supervised video moment retrieval based on a frame - by - word interaction module and a Word - conditioned Visual Graph ( WCVG ). Specifically, given a video and a sentence, the goal of the model is to locate the video segment which is described by the sentence without having access to temporal annotations during training. Thus, the model must learn how to identify the correct segment ( i.e. moment ) when only being provided with video - sentence pairs. The proposed method aims to automatically infer the latent correspondence between visual and language representations by exploiting a multi - level attention mechanism to learn richer multimodal representations. The mechanism is comprised of a Frame - By - Word interaction module as well as a novel Word - Conditioned Visual Graph. Moreover, the method also incorporates a novel application of positional encodings, commonly used in Transformers, to learn a visual - semantic representations that contain contextual information of their relative positions in the temporal sequence through iterative message - passing. Experiments on the DiDeMo and Charades - STA datasets demonstrate the effectiveness of the learned representations of the wMAN model."
SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"This paper proposes a method to generate photo - realistic re - rendering of reconstructed objects for virtual and augmented reality applications. The method is a hybrid between image - based rendering and machine learning, where the key contribution is the explicit handling of view - dependent effects in the source and the target views using EffectsNet that can be learned in a self - supervised fashion. The composition of the reprojected views to a final output image without the need of hand - crafted blending schemes is enabled using a network called CompositionNet. The paper demonstrates the effectiveness of the approach both qualitatively and quantitatively on synthetic as well as on real data."
SP:257d124367b1da9a595dc11a9df750d6bade298e,"This paper proposes a method for estimating the model uncertainty of deep neural networks ( DNNs ) that relies on an inverse formulation of Multivariate Normal Distribution ( MND ), which is an information form. The authors show that the uncertainty can be estimated in this form using a scalable Laplace Approximation scheme, which involves a diagonal correction of the Kronecker - factor of the eigenbasis of MND. Since this correction is intractable, the authors devise a novel low rank approximation, based on spectral sparsification, to approximate it. The method is evaluated on various benchmarks and shows the superiority of their approach over existing methods."
SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"This paper proposes a load - balanced hashing algorithm, called AHash, to improve the accuracy of One Permutation Hashing ( OPH ) and densification strategies, and to reduce the number of empty bins in OPH. The main idea of the algorithm is to balance the load so as to generate as few empty bins as possible, which leads to more accurate hashing. The proposed AHash outperforms the state - of - the - art OPH in similarity estimating, large - scale learning and fast near neighbour searching. The experiments on real datasets validate the claim."
SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,This paper proposes a method for extracting features from phase shift data by adding a graph structure to each data point and constructing a machine learning model according to the characters of the graph structure and the original data. The proposed method performs a cyclic permutation to a graph neural network. This method assures that the results account for phase shift of the periodic measurements. The use of this method in Section 3 shows that it offers predictions with sufficient accuracies for idealized data and the experimental data obtained from a test setup Gest et al. ( 2019 ).
SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,"This paper proposes a confidence - based decoder for neural conditional text generation. The decoder learns a confidence score for each position in the input that is used as a warning when the decoder is less confident than the target position. The confidence score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on a structured data - to - text dataset – WikiBio ( Leebret et al., 2016 ) – show that the proposed method is more faithful than existing state - of - the - art approaches, according to both automatic metrics and human evaluation."
SP:03307deac29173b2968fbd08f95fc77eb1f82410,"This paper proposes a new lookahead pruning method based on the observation that magnitude - based pruning ( MP ) minimizes the Frobenius distortion of a linear operator corresponding to a single layer. Based on this observation, the proposed LAP method proposes to apply MP by extending the single layer optimization to a multi - layer optimization by introducing a functional approximation perspective. Experiments on neural networks trained with VGG, ResNet, and Wide ResNet show that LAP consistently outperforms MP and other baseline pruning methods."
SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"This paper introduces Moniqua, a decentralized graph - based communication method for decentralized stochastic gradient descent ( SGD ). The method is based on the idea of graph neural networks ( GNNs ), where parallel workers are connected to form a graph and communicate via full - precision message passing. The main novelty of the proposed method is that it replaces the memory - consuming graph neural network used in standard SGD with a more memory - efficient, quantized version that uses only a small number of bits per iteration. The paper provides theoretical proofs that under certain assumptions the method is guaranteed to converge under certain conditions. Empirically, the paper also presents convergence results for the method on CIFAR-10 for training VGG16."
SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"This paper studies the problem of learning a "" partial model "", i.e. a model that only models part of an environment, in contrast to a fully - connected model that includes all the information about the environment. In reinforcement learning, a partial model is used to learn a model of future observations and rewards, and use it to plan the agent’s next actions. However, jointly modeling future observations can be computationally expensive or intractable if the observations are high - dimensional, e.g., images. This paper proposes a general family of partial models that are provably correct, yet remain fast because they do not need to fully model future observations.    The main contributions of this paper are : 1. The authors prove that a class of "" partial models "" that is correct when the environment stays the same but is wrong when the behavior policy changes can still make the same predictions. 2. They propose a modification to such a class that makes it possible for such a model to remain correct even if the policy changes ( called "" policy update "" in the paper ). 3. They empirically investigate the effect of this modification on models learned in illustrative environments ( simple MDPs and 3D environments )."
SP:c70479b2096a52584b242de58272ca8d8565feea,"This paper proposes a new variational autoencoder ( VAE ) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is based on two information theoretic problems : Distributed simulation and channel synthesis, in which Wyner’s common information arises as the fundamental limit of the succinctness of the common representation. The WynerVAE decomposes a pair of correlated data variable into their common representation and local representations that capture the remaining randomness by imposing the mutual information between the data variables and the common representations as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images. Experiments show that the proposed model outperforms existing VAE variants and the variational information bottleneck method."
