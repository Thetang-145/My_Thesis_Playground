,paper_id,input,output_bestRouge1,output_bestRougeAvg
0,SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"Role - based learning USED-FOR scalable multi - agent learning. Role - based learning USED-FOR decomposing complex tasks. decomposing complex tasks USED-FOR scalable multi - agent learning. roles USED-FOR decomposing complex tasks. role selector USED-FOR role discovery. role selector USED-FOR role space. role space CONJUNCTION temporal resolution. temporal resolution CONJUNCTION role space. it USED-FOR bi - level learning hierarchy. primitive action - observation spaces FEATURE-OF role policies. action effects USED-FOR role selector. learning efficiency CONJUNCTION policy generalization. policy generalization CONJUNCTION learning efficiency. method COMPARE MARL algorithms. MARL algorithms COMPARE method. OtherScientificTerm are joint action spaces, and restricted role action spaces. Material is StarCraft II micromanagement benchmark. ","This paper proposes a new role-based learning for scalable multi-agent learning with decomposing complex tasks into roles. The role selector is used to learn the role space and temporal resolution, and the action effects are used to guide the role discovery. The proposed method achieves better learning efficiency and policy generalization than MARL algorithms, and it can be applied to the bi-level learning hierarchy. The paper also proposes a StarCraft II micromanagement benchmark.",This paper proposes a new approach for multi-agent multi-task learning. The proposed approach is based on the role-based learning framework. The key idea is to use a role selector to learn the role space and temporal resolution. The role selector is then used for role discovery. The paper shows that the proposed method can achieve better learning efficiency and policy generalization compared to MARL algorithms. 
9,SP:7deb61890d97422a0fe141ca807f968c70ab239a,"stochastic subgradient descent ( SSGD ) method USED-FOR over - parameterized nonsmooth optimization problems. interpolation condition FEATURE-OF over - parameterized nonsmooth optimization problems. composite structure USED-FOR SSGD. composite structure USED-FOR empirical risk minimization problems. stochastic gradient descent ( SGD ) method USED-FOR smooth problems. rates COMPARE rates. rates COMPARE rates. rates USED-FOR stochastic gradient descent ( SGD ) method. SGD USED-FOR smooth and nonsmooth machine learning models. SSGD USED-FOR smooth and nonsmooth machine learning models. SGD CONJUNCTION SSGD. SSGD CONJUNCTION SGD. subgradient method USED-FOR convex and interpolation setting. OtherScientificTerm are convex and strongly - convex objectives, and interpolation. ",This paper proposes a new stochastic subgradient descent (SSGD) method for over-parameterized nonsmooth optimization problems with interpolation condition. The authors show that the composite structure of SSGD can be used to solve empirical risk minimization problems with a composite structure. The main contribution of the paper is to show that SGD can be combined with SGD and SSGD to improve the performance of smooth and nonsmoothed machine learning models. The paper also shows that the proposed rates are better than existing rates for smooth problems and strongly-convex objectives. ,"This paper proposes a stochastic subgradient descent (SSGD) method for over-parameterized nonsmooth optimization problems with an interpolation condition. The authors show that the composite structure of SSGD can be used for empirical risk minimization problems with convex and strongly-convex objectives. They also show that SGD and SSGD outperform other rates for smooth problems with the same interpolation setting. Finally, the authors propose a subgradient method for the convex-and-interpolation setting and show that it outperforms other rates. "
18,SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"non - linear “ reservoir ” layers CONJUNCTION regular transformer layers. regular transformer layers CONJUNCTION non - linear “ reservoir ” layers. Method are transformers, and machine learning. OtherScientificTerm is layers. Metric is wall - clock compute time. Task is machine translation. ",This paper studies the problem of learning transformers with non-linear “reservoir” layers and regular transformer layers. The authors show that the wall-clock compute time of these layers can be used to compute the number of layers in a machine translation. They also provide a theoretical analysis of this problem.    The paper is well written and easy to follow. The main contribution of the paper is the theoretical analysis. The theoretical analysis is clear and the empirical results are convincing. ,This paper proposes a new method for learning transformers. The main idea is to use non-linear “reservoir” layers and regular transformer layers. The authors show that these layers can be used to reduce the wall-clock compute time. They also show that the proposed method can be applied to machine learning. The paper is well-written and easy to follow. 
27,SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"transformation invariance CONJUNCTION equivariance. equivariance CONJUNCTION transformation invariance. transformation invariance FEATURE-OF network architecture. equivariance FEATURE-OF network architecture. geometry transformation of data FEATURE-OF network robustness. Filter transform USED-FOR steerable CNN. group representation theory USED-FOR steerable CNN. group representation theory USED-FOR function space structure. group representation theory USED-FOR steerable kernel function. function space structure FEATURE-OF steerable kernel function. theory COMPARE filter transform technique. filter transform technique COMPARE theory. group representation theory USED-FOR kernel. filter transform USED-FOR kernel. filter transformed kernels USED-FOR group representation. approach USED-FOR steerable convolution operators. Method are Steerable CNN, and steerable CNN theory. OtherScientificTerm is overfitting. ","This paper studies the problem of steerable CNN. The authors propose a new group representation theory for the function space structure of a steerable kernel function. The kernel is represented by filter transformed kernels, and the group representation can be used to represent the kernel as a group of kernels. The group representation is then used to train the steerable convolution operators. The proposed method is evaluated on a variety of tasks, and it is shown that the proposed method outperforms existing methods.","This paper proposes a new approach to train steerable convolution operators. The key idea is to use group representation theory to model the function space structure of the steerable kernel function. The authors show that the transformation invariance of the network architecture and equivariance of network architecture are related to the geometry transformation of data, which is a key factor in the network robustness to overfitting. The paper also shows that the proposed Steerable CNN is more robust than the original steerable CNN theory. The main contribution of the paper is the use of filter transformed kernels to represent the group representation of the kernel. The proposed theory is shown to outperform the original filter transform technique."
36,SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"Multimodal program synthesis USED-FOR program. user input USED-FOR program. Multimodal program synthesis USED-FOR program synthesis. user input USED-FOR Multimodal program synthesis. noisy signals USED-FOR it. natural language HYPONYM-OF noisy signals. neural model USED-FOR program ’s score. natural language ( NL ) CONJUNCTION input - output examples. input - output examples CONJUNCTION natural language ( NL ). user intent FEATURE-OF multimodal synthesis tasks. input - output examples USED-FOR multimodal synthesis tasks. input - output examples USED-FOR user intent. top - down recurrent neural model PART-OF method. automated program analysis techniques USED-FOR search space. user ’s constraints FEATURE-OF infeasibility of partial programs. automated program analysis techniques USED-FOR it. multimodal synthesis dataset ( STRUCTUREDREGEX ) EVALUATE-FOR method. method COMPARE techniques. techniques COMPARE method. multimodal synthesis dataset ( STRUCTUREDREGEX ) EVALUATE-FOR techniques. accuracy EVALUATE-FOR techniques. accuracy EVALUATE-FOR method. Method is optimal neural synthesis approach. OtherScientificTerm are user - provided constraints, abstract syntax trees, NL input, and syntactically valid programs. Generic is model. ","This paper proposes a new optimal neural synthesis approach for program synthesis. Multimodal program synthesis uses user input to synthesize a program from user input, where the user-provided constraints are a set of abstract syntax trees. The proposed method uses a top-down recurrent neural model to predict the program’s score using noisy signals (e.g., natural language or input-output examples). The authors show that the proposed method achieves better performance on the multimodal synthesis dataset (STRUCTUREDREGEX) than existing techniques. ","This paper proposes an optimal neural synthesis approach for program synthesis. Multimodal program synthesis uses user input to generate a program from user input, and the program’s score is computed using a top-down recurrent neural model with noisy signals (e.g., natural language or input-output examples). The authors propose to use user-provided constraints on the abstract syntax trees, and then use a neural model to generate syntactically valid programs. The authors evaluate the proposed method on a multimodal synthesis dataset (STRUCTUREDREGEX) and show that it outperforms existing techniques in terms of accuracy, and also outperforms other automated program analysis techniques in the search space. "
45,SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"protease enzymes HYPONYM-OF proteins. substrate specificity landscape FEATURE-OF protease enzyme. sequence motifs PART-OF substrate specificity landscape. methods USED-FOR predicting protease specificity landscapes. sequence patterns USED-FOR methods. protein graph convolutional neural network ( PGCN ) USED-FOR substrate specificity. Rosetta energy function USED-FOR topology and energetic features. structure - based molecular interaction graph USED-FOR substrate specificity. Rosetta energy function USED-FOR structure - based molecular interaction graph. structure - based molecular interaction graph USED-FOR protein graph convolutional neural network ( PGCN ). PGCN USED-FOR specificity. specificity FEATURE-OF NS3/4 protease. Hepatitic C virus FEATURE-OF NS3/4 protease. PGCN COMPARE machine learning models. machine learning models COMPARE PGCN. classification tasks EVALUATE-FOR PGCN. feature importance USED-FOR sub - graph patterns. sub - graph patterns USED-FOR molecular recognition. physical interactions USED-FOR PGCN. PGCN model USED-FOR enzymes. Task is robustness of key life processes. OtherScientificTerm are protease specificity landscapes, mutational changes, and molecular interactions. ","This paper studies the robustness of key life processes, i.e. proteins such as Protease enzymes. The authors propose two methods for predicting protease specificity landscapes based on sequence patterns. The first is a protein graph convolutional neural network (PGCN) that predicts the substrate specificity landscape of a protease enzyme based on the sequence motifs. The second is a structure-based molecular interaction graph based on a Rosetta energy function that predicts topology and energetic features. The PGCN model is able to predict the specificity of the NS3/4 protease (Hepatitic C virus) in the presence of mutational changes. The experiments show that PGCNs can achieve better performance on classification tasks than other machine learning models. ","This paper proposes a novel method for predicting the specificity landscape of proteins, i.e., the sequence motifs of a protease enzyme. The authors propose a protein graph convolutional neural network (PGCN) to predict the substrate specificity landscape, which is based on a structure-based molecular interaction graph with a Rosetta energy function that predicts the topology and energetic features of the protein. The PGCN model predicts the specificity of the enzymes, and is trained on a set of protein sequences. The proposed method is evaluated on a variety of classification tasks, where it outperforms state-of-the-art machine learning models in terms of feature importance and sub-graph patterns for molecular recognition. "
54,SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"method USED-FOR overestimation bias. Double Q - learning USED-FOR overestimation bias. Double Q - learning HYPONYM-OF method. approximate Bellman operator USED-FOR non - optimal fixed points. underestimation bias PART-OF double Q - learning. approximate dynamic programming USED-FOR approach. method COMPARE baseline algorithms. baseline algorithms COMPARE method. Atari benchmark tasks EVALUATE-FOR baseline algorithms. Atari benchmark tasks EVALUATE-FOR method. Method are Bellman operation, and deep Q - learning paradigm. Task are value prediction, and learning. OtherScientificTerm is non - optimal stationary solutions. ","This paper proposes a method to reduce the overestimation bias in Double Q-learning by using approximate Bellman operator for the non-optimal fixed points. The approach is based on approximate dynamic programming, where the Bellman operation is used to learn the optimal stationary solutions. The authors show that the proposed method outperforms baseline algorithms on a variety of Atari benchmark tasks. ","The paper proposes a method for minimizing the overestimation bias in Double Q-learning. The method is based on the Bellman operation, which is an extension of the deep Q-Learning paradigm. The authors propose an approximate Bellman operator for non-optimal fixed points, which can be used for value prediction. The proposed approach is built on approximate dynamic programming, and the authors show that the proposed method outperforms baseline algorithms on Atari benchmark tasks. The underestimation bias is reduced by the proposed approach. The paper also shows that the approach is more robust to non-optimistic stationary solutions. "
63,SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"models USED-FOR high - resolution image generation. BigGAN CONJUNCTION VQVAE-2. VQVAE-2 CONJUNCTION BigGAN. compute resources USED-FOR models. VQVAE-2 HYPONYM-OF models. BigGAN HYPONYM-OF models. ESRGAN HYPONYM-OF GAN - based image super - resolution models. two - step training framework USED-FOR deep generative models ( DGMs ). high - dimensional natural images USED-FOR deep generative models ( DGMs ). wavelet domain USED-FOR sampler. wavelet super - resolution decoder network USED-FOR images. Wavelet - based down - sampling method COMPARE pixel - based methods. pixel - based methods COMPARE Wavelet - based down - sampling method. generative quality EVALUATE-FOR low - resolution sampler. Wavelet - based down - sampling method USED-FOR structural information. generative quality EVALUATE-FOR Wavelet - based down - sampling method. generative quality EVALUATE-FOR pixel - based methods. sampler CONJUNCTION decoder. decoder CONJUNCTION sampler. ImageNet EVALUATE-FOR model. model COMPARE BigGAN model. BigGAN model COMPARE model. Fréchet Inception Distance ( FID ) EVALUATE-FOR BigGAN model. Fréchet Inception Distance ( FID ) EVALUATE-FOR model. OtherScientificTerm are low - frequency bands, pixel - space, and dimensional spaces. Method is end - to - end models. Metric is training cost. ","This paper proposes a two-step training framework for deep generative models (DGMs) trained on high-dimensional natural images. The proposed models are based on BigGAN and VQVAE-2, both GAN-based image super-resolution models (e.g., ESRGAN). The authors use compute resources to train the models, and use Wavelet-based down-sampling method to extract structural information from low-frequency bands. The sampler is trained on the wavelet domain, and the decoder is trained using a wavelet super-resolution decoder network to generate images from the pixel-space. The model is evaluated on ImageNet, and compared to the BigGAN model on Fréchet Inception Distance (FID) and on a few other datasets. The authors show that the proposed model has better generative quality than pixel-based methods, and that the training cost is lower than end-to-end models.","This paper proposes a two-step training framework for deep generative models (DGMs) trained on high-dimensional natural images. The authors propose two models, BigGAN and VQVAE-2, which are both GAN-based image super-resolution models (e.g., ESRGAN). The models are trained using compute resources, where the low-frequency bands are sampled from the pixel-space. The sampler is trained in the wavelet domain, and the decoder is trained on images generated by a wavelet super-resolution decoder network. Experiments show that the Wavelet-based down-sampling method improves the generative quality of a low-resolution sampler and the structural information of a decoder. The model is evaluated on ImageNet and compared to a BigGAN model with Fréchet Inception Distance (FID) and other end-to-end models. "
72,SP:b943a73b1ec34867371325748dc3a91ff4011947,"self - supervised learning ( SSL ) algorithms USED-FOR Fewshot learning(FSL ). pre - trained embedding network USED-FOR downstream FSL tasks. self - supervised training USED-FOR pre - trained embedding network. SSL USED-FOR FSL. self - supervised training USED-FOR FSL. supervised training USED-FOR FSL. self - supervised loss CONJUNCTION supervised loss. supervised loss CONJUNCTION self - supervised loss. supervised training CONJUNCTION self - supervised training. self - supervised training CONJUNCTION supervised training. test accuracy EVALUATE-FOR self - supervised FSL. Material are large - scale labeled data, and labeled data. Method are embedding network, and supervised FSL methods. ","This paper studies self-supervised learning (SSL) algorithms for Fewshot learning(FSL). The authors propose a pre-trained embedding network for downstream FSL tasks, which is a combination of SSL and supervised training for FSL. The authors show that the embedding networks can be used to improve the test accuracy of FSL with supervised training. They also show that supervised FSL methods can achieve better test accuracy than supervised training in terms of test accuracy. ",This paper proposes self-supervised learning (SSL) algorithms for Fewshot learning(FSL). SSL is used to train a pre-trained embedding network for downstream FSL tasks. The authors show that SSL improves the test accuracy of FSL with supervised training and FSL without supervised training on large-scale labeled data. They also show that supervised FSL methods do not improve test accuracy. 
81,SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"first order methods USED-FOR ultra - wide neural networks. finite width FEATURE-OF neural networks. OtherScientificTerm are global minima, initialization, teacher neurons, local minima, student neurons, and Angular Distance ( AD ) function. Method is two - layer teacher - student networks. Generic is methodology. ","This paper proposes a new method for training two-layer teacher-student networks. The main idea is to use first order methods to train ultra-wide neural networks with finite width with a finite width. The authors show that the global minima of the initialization of the teacher neurons are the same as the local minima for the student neurons. They then propose a new Angular Distance (AD) function, which can be used to train the student networks. Experimental results show the effectiveness of the proposed methodology.","This paper proposes a new method for training two-layer teacher-student networks with finite width. The main idea is to use first order methods to train ultra-wide neural networks of finite width with global minima, where the initialization is performed by the teacher neurons and the local minima are computed by the student neurons. The proposed methodology is based on the Angular Distance (AD) function."
90,SP:0f62846913ec10b44ed32845770da0565479dc75,"framework USED-FOR deep neural networks. user - provided formal knowledge USED-FOR learning from data. Deep Adaptive Semantic Logic ( DASL ) USED-FOR deep neural networks. Deep Adaptive Semantic Logic ( DASL ) HYPONYM-OF framework. knowledge representation USED-FOR first order logic. finite sampling USED-FOR truth values. infinite domains FEATURE-OF finite sampling. prior neuro - symbolic work USED-FOR DASL ’s representation. structure PART-OF image classification task. DASL USED-FOR visual relationship detection task. OtherScientificTerm are formal semantics, vanishing gradients, deeper logical structure, data requirements, commonsense knowledge, and data scarcity. ","This paper proposes a framework for learning deep neural networks with user-provided formal knowledge from data. The framework, Deep Adaptive Semantic Logic (DASL) is based on the Deepadaptive Semantics Logic (DSL) framework. DASL learns a knowledge representation of the first order logic, which is then used to learn the formal semantics. The authors show that finite sampling of the truth values in infinite domains can lead to vanishing gradients, and that the representation learned by prior neuro-symbolic work can be used to improve the performance of the DASCL’s representation. The paper also shows that the structure of the image classification task can be learned from the structure learned by DASl, which can be applied to the visual relationship detection task. ","This paper proposes a framework for learning deep neural networks with user-provided formal knowledge. The framework is called Deep Adaptive Semantic Logic (DASL) and is based on the idea that the formal semantics of the data can be used to guide learning from data. The key idea of DASL is to use finite sampling to learn truth values in infinite domains with vanishing gradients. The authors propose a knowledge representation for first order logic, which can be represented as a prior neuro-symbolic work. The deeper logical structure of the representation is then used for learning from commonsense knowledge, and the authors show that finite sampling can be applied to infinite domains without data scarcity. The proposed framework is evaluated on a visual relationship detection task where the structure of an image classification task is used."
99,SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"feedforward residual neural networks ( ResNets ) USED-FOR iterative recurrent computations. they USED-FOR neural networks. regularization approach USED-FOR learning of iterative solutions. ResNets USED-FOR iterative solutions. iteration CONJUNCTION convergence. convergence CONJUNCTION iteration. ResNets USED-FOR iterative solutions. regularizations USED-FOR iterative convergent computation. this USED-FOR inductive bias. regularizations USED-FOR inductive bias. ResNet CONJUNCTION recurrent ” ResNet. recurrent ” ResNet CONJUNCTION ResNet. method USED-FOR recurrence regularization. recurrent network USED-FOR one. one HYPONYM-OF recurrent ” ResNet. Lipschitz constraint FEATURE-OF residual functions. spectral normalization USED-FOR Lipschitz constraint. gradient coupling CONJUNCTION Lipschitz constraint. Lipschitz constraint CONJUNCTION gradient coupling. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. recurrence regularization CONJUNCTION spectral normalization. spectral normalization CONJUNCTION recurrence regularization. visual recognition tasks EVALUATE-FOR classification accuracy. Digitclutter HYPONYM-OF recognition tasks. MNIST HYPONYM-OF visual recognition tasks. classification accuracy EVALUATE-FOR spectral normalization. classification accuracy EVALUATE-FOR recurrence regularization. CIFAR-10 HYPONYM-OF visual recognition tasks. Iterative convergent computation USED-FOR tasks. inductive bias FEATURE-OF ResNets. Task are Iterative computations, and computer vision tasks. Method are Iterative methods, and soft gradient coupling. Metric is iterative convergence. Generic are them, and networks. ","This paper proposes a new regularization approach for the learning of iterative solutions in feedforward residual neural networks (ResNets) for iterative recurrent computations. Iterative computations are computations where the goal is to find a solution to a sequence of problems. ResNets are trained to find iterative solution to these problems by iterative convergence and iteration. The authors show that the Lipschitz constraint on residual functions of the residual functions can be used as a regularization for the iterative convergent computation. They also show that this regularizations can reduce the inductive bias of the ResNs.  The authors propose a new method for recurrence regularization and spectral normalization. The proposed method is based on soft gradient coupling between the ResNet and the recurrent “ResNet” ResNet, where the recurrent network is a “resnet” and the one trained by the recurrent ResNet is the “reward” one. They show that their method can improve the classification accuracy of the RecNets on a variety of visual recognition tasks such as MNIST, CIFAR-10, Cifar-100, and Digitclutter.","This paper proposes a regularization approach for learning of iterative solutions in iterative recurrent computations. Iterative computations are commonly used in the literature for learning neural networks. ResNets are used to solve iterative problems, and they can be used to train neural networks to solve them. The authors propose two regularizations for iterative convergent computation: Lipschitz constraint on the residual functions, gradient coupling, and iterative convergence. The first regularization is soft gradient coupling and the second one is a recurrent network. This regularization reduces the inductive bias of ResNs, and the authors show that the proposed method can be combined with recurrence regularization and spectral normalization to improve the classification accuracy. Experiments are conducted on several tasks, including CIFAR-10, MNIST, and Digitclutter. "
108,SP:6c14506b8b2b06043409d912e6bf877651aaa665,"Normalization techniques USED-FOR deep neural networks. they USED-FOR independent and identically distributed ( IID ) data. normalization methods USED-FOR OOD generalization. SelfNorm CONJUNCTION CrossNorm. CrossNorm CONJUNCTION SelfNorm. CrossNorm USED-FOR OOD generalization. SelfNorm USED-FOR OOD generalization. SelfNorm HYPONYM-OF normalization methods. CrossNorm HYPONYM-OF normalization methods. SelfNorm COMPARE CrossNorm. CrossNorm COMPARE SelfNorm. attention USED-FOR SelfNorm. SelfNorm USED-FOR OOD generalization. CrossNorm USED-FOR OOD generalization. SelfNorm CONJUNCTION CrossNorm. CrossNorm CONJUNCTION SelfNorm. OtherScientificTerm are channel - wise mean and variance, feature maps, and statistics usage. Task is classification and segmentation. ","This paper studies the problem of normalization techniques for deep neural networks. The authors show that they can be used to generalize independent and identically distributed (IID) data. The main contribution of the paper is a new normalization method called SelfNorm, which is a combination of SelfNorm and CrossNorm to improve OOD generalization. SelfNorm is based on attention, where attention is used to learn the channel-wise mean and variance of the feature maps. The paper also shows that SelfNorm can be combined with CrossNorm in order to improve the OOD performance. ","This paper proposes two new normalization techniques for deep neural networks. The authors show that they can be applied to independent and identically distributed (IID) data. The main idea is to use channel-wise mean and variance to learn the feature maps. They show that SelfNorm and CrossNorm can achieve better OOD generalization than other normalization methods such as SelfNorm, CrossNorm, and attention. They also show that CrossNorm is better than SelfNorm in terms of attention. "
117,SP:2774abdc11917321dd4994af0f0da1ff824bea03,language CONJUNCTION speech. speech CONJUNCTION language. vision CONJUNCTION language. language CONJUNCTION vision. unsupervised pre - training CONJUNCTION generative modeling. generative modeling CONJUNCTION unsupervised pre - training. supervised learning CONJUNCTION unsupervised pre - training. unsupervised pre - training CONJUNCTION supervised learning. generative modeling USED-FOR multiple domains. Attention mechanisms HYPONYM-OF inductive biases. unsupervised pre - training USED-FOR multiple domains. vision HYPONYM-OF multiple domains. speech HYPONYM-OF multiple domains. language HYPONYM-OF multiple domains. neural network architectures USED-FOR reinforcement learning ( RL ). they USED-FOR neural network architectures. high dimensional inputs USED-FOR neural network architectures. pixels HYPONYM-OF high dimensional inputs. attention module PART-OF convolutional encoder. attention module PART-OF RL agent. convolutional encoder PART-OF RL agent. data augmentations CONJUNCTION contrastive losses. contrastive losses CONJUNCTION data augmentations. module USED-FOR interpretable task - relevant information. DeepMind Control Suite environments EVALUATE-FOR module. sampleefficiency EVALUATE-FOR agents. module USED-FOR agents. sampleefficiency EVALUATE-FOR module. attention mechanisms USED-FOR reinforcement learning and control. Generic is approach. ,"This paper proposes a new attention module for reinforcement learning (RL) agents. Attention mechanisms are inductive biases that can be used in reinforcement learning and control. The authors show that they can be applied to neural network architectures with high dimensional inputs (e.g., pixels) and unsupervised pre-training (e., language, speech, vision). The authors also show that the proposed approach can be combined with supervised learning and/or generative modeling for multiple domains, including vision, language, and speech. The proposed module is evaluated on the DeepMind Control Suite environments and shows that it improves the sampleefficiency of agents trained with the proposed attention module. ","This paper proposes a novel approach to unsupervised pre-training for multiple domains such as vision, language, speech, and generative modeling. The authors propose to use neural network architectures for reinforcement learning (RL) with high dimensional inputs (e.g., pixels) and to use attention mechanisms to mitigate inductive biases in reinforcement learning and control. The attention module consists of a convolutional encoder and an RL agent. The proposed module is evaluated on DeepMind Control Suite environments, and the authors show that the proposed module can capture interpretable task-relevant information. "
126,SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"gradient - based approach USED-FOR multitask networks. GradNorm HYPONYM-OF gradient - based approach. extension USED-FOR GradNorm. game theory USED-FOR Rotograd. Rotograd COMPARE approaches. approaches COMPARE Rotograd. approaches USED-FOR multitask learning. Rotograd USED-FOR multitask learning. real - world datasets EVALUATE-FOR Rotograd. real - world datasets CONJUNCTION network architectures. network architectures CONJUNCTION real - world datasets. network architectures EVALUATE-FOR Rotograd. Task is learning. OtherScientificTerm are network parameters, gradient magnitude, gradient magnitudes, and task gradients. Generic is it. Metric is convergence. ","This paper proposes a gradient-based approach for multitask networks, called Rotograd, which is based on GradNorm. The authors propose an extension of GradNorm to the problem of learning the network parameters. The main idea is to use game theory to learn the gradient magnitude of the gradient magnitudes of the task gradients. Theoretically, the authors show that it is possible to converge to the optimal gradient magnitude in practice. They also show that the proposed approaches perform better than existing approaches in multitask learning. The experiments on real-world datasets and network architectures demonstrate the effectiveness of rotograd.","This paper proposes a gradient-based approach for multitask networks, called Rotograd, which is a variant of GradNorm. The authors propose an extension to GradNorm that is based on game theory. The main idea is to learn a set of network parameters that are independent of the task parameters. This is done by minimizing the gradient magnitude of the gradient magnitudes of the network parameters. The paper shows that it converges to a convergence rate of 1/\sqrt{n}^n. The results are shown on real-world datasets and several network architectures. The proposed approaches are shown to outperform other approaches in multitask learning. "
135,SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"geometry distortion problem FEATURE-OF methods. randomness of color transformation FEATURE-OF translation process. unwanted distortions FEATURE-OF translation. Minimal Geometry - Distortion Constraint ( MGC ) HYPONYM-OF I2I translation constraint. approximate representation of mutual information USED-FOR estimation and maximization of MGC. MGC COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MGC. benchmark datasets EVALUATE-FOR state - of - the - art methods. benchmark datasets EVALUATE-FOR MGC. OtherScientificTerm are domain mapping function, mapping function, geometry structure, and consistency of geometry structures. Material are paired data, and translated images. Generic is function. ","This paper studies the geometry distortion problem of the I2I translation problem. The authors consider the randomness of color transformation in the translation process, where the domain mapping function is a mapping function, and the goal is to minimize the distance between the input and the target domain. The paper proposes a new mapping function called Minimal Geometry-Distortion Constraint (MGC), which is a I2i translation constraint. MGC is based on the approximate representation of mutual information between the source and target domains, and is used for the estimation and maximization of MGC.  The authors show that MGC outperforms state-of-the-art methods on several benchmark datasets. ","This paper studies the geometry distortion problem in the translation process. The authors propose the Minimal Geometry-Distortion Constraint (MGC) which is an I2I translation constraint, which is based on the domain mapping function. The mapping function is defined as the mapping function between two pairs of images, where one is a pair of paired data, and the other is a set of translated images. The paper shows that the MGC can be approximated by an approximate representation of mutual information between the two pairs, which leads to the estimation and maximization of MGC. Experiments on several benchmark datasets show that MGC outperforms state-of-the-art methods. "
144,SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,point sampling patterns USED-FOR point cloud GANs. DGCNN CONJUNCTION PointConv. PointConv CONJUNCTION DGCNN. PointConv CONJUNCTION KPConv. KPConv CONJUNCTION PointConv. sampling - oversensitive discriminators USED-FOR valid shape generation. PointNet++ CONJUNCTION DGCNN. DGCNN CONJUNCTION PointNet++. sampling - insensitive discriminators USED-FOR shape point clouds. point clustering artifacts FEATURE-OF shape point clouds. KPConv HYPONYM-OF sampling - oversensitive discriminators. PointNet - Max HYPONYM-OF sampling - insensitive discriminators. PointNet++ HYPONYM-OF sampling - oversensitive discriminators. PointConv HYPONYM-OF sampling - oversensitive discriminators. DGCNN HYPONYM-OF sampling - oversensitive discriminators. evaluation metrics EVALUATE-FOR sampling pattern. perceptual metrics PART-OF sampling spectrum of metrics. sampling pattern COMPARE geometry. geometry COMPARE sampling pattern. sampling spectrum USED-FOR middle - point sampling - aware baseline discriminator. PointNet - Mix HYPONYM-OF point cloud generators. sampling - related metrics EVALUATE-FOR point cloud generators. PointNet - Mix HYPONYM-OF middle - point sampling - aware baseline discriminator. Task is generator design. Method is discriminator design. Generic is discriminators. ,"This paper studies the problem of point sampling patterns in point cloud GANs. The authors propose two sampling-oversensitive discriminators, PointNet-Max and PointConv, to generate valid shape point clouds with point clustering artifacts. PointNet++ and DGCNN are the two main sampling-oversensitive discriminators. The sampling spectrum of metrics in the sampling spectrum includes perceptual metrics as well as the sampling pattern of the discriminator design. The empirical results show that the sampling-related metrics of the two point cloud generators outperform the middle-point sampling-aware baseline discriminator (PointNet-Mix).","The paper proposes a new sampling patterns for point cloud GANs. The sampling-oversensitive discriminators are used for valid shape generation. The authors show that sampling-sensitive discriminators can be used to generate shape point clouds with point clustering artifacts. The paper also shows that the sampling pattern is more robust to geometry than the other discriminator design. Experiments are conducted on PointNet++, DGCNN, PointConv, PointNet-Max, and KPConv. Results show that the proposed sampling pattern outperforms the other evaluation metrics in the sampling spectrum of metrics. "
153,SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"images USED-FOR Convolutional Neural Networks ( CNNs ). small quasi - imperceptible artificial perturbations USED-FOR Convolutional Neural Networks ( CNNs ). Capsule Networks ( CapsNets ) COMPARE CNNs. CNNs COMPARE Capsule Networks ( CapsNets ). CNNs COMPARE Capsule Networks ( CapsNets ). Capsule Networks ( CapsNets ) COMPARE CNNs. Capsule Networks ( CapsNets ) USED-FOR white - box attacks. attack protocols USED-FOR CNNs. CapsNets USED-FOR adversarial examples. adversarial robustness FEATURE-OF CapsNets. multi - step attack methods USED-FOR CapsNets. multi - step attack methods USED-FOR CNNs. routing process USED-FOR vote attack. vote attack PART-OF detection - aware attack paradigm. vote attack USED-FOR CapsNets. OtherScientificTerm are votes, computationally expensive routing mechanism, and votes of CapsNets. Metric is computational cost. Method is class - conditional reconstruction based detection method. ","This paper proposes a novel detection-aware attack paradigm based on the vote attack. The proposed method is based on a class-conditional reconstruction based detection method. The key idea is to train a CNN with small quasi-imperceptible artificial perturbations on the input images, and then use these images to train the CNNs. The authors show that the proposed method outperforms the state-of-the-art CNNs in terms of adversarial robustness.",This paper proposes a novel detection-aware attack paradigm based on the vote attack. The proposed method is based on a class-conditional reconstruction based detection method. The authors propose a computationally expensive routing mechanism to learn the votes of CapsNets. They show that the proposed attack protocols are more robust to white-box attacks than CNNs. They also show that multi-step attack methods can be used to improve the robustness of CNNs against adversarial examples.
162,SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"Meta - reinforcement learning USED-FOR policy. recurrent neural networks USED-FOR policies. algorithm USED-FOR learning of recurrent policies. privileged information USED-FOR learning of recurrent policies. task descriptor FEATURE-OF privileged information. privileged information USED-FOR algorithm. parameters sharing CONJUNCTION auxiliary objective. auxiliary objective CONJUNCTION parameters sharing. method USED-FOR informed policy. informed policy USED-FOR task embeddings. policy HYPONYM-OF informed policy. parameters sharing USED-FOR recurrent policy. descriptors USED-FOR task embeddings. auxiliary objective USED-FOR recurrent policy. learning sample complexity EVALUATE-FOR approach. task - inference approaches USED-FOR meta - reinforcement learning. Thompson sampling CONJUNCTION task - inference approaches. task - inference approaches CONJUNCTION Thompson sampling. vanilla RNNs CONJUNCTION Thompson sampling. Thompson sampling CONJUNCTION vanilla RNNs. it COMPARE vanilla RNNs. vanilla RNNs COMPARE it. it COMPARE Thompson sampling. Thompson sampling COMPARE it. it COMPARE task - inference approaches. task - inference approaches COMPARE it. it USED-FOR meta - reinforcement learning. Thompson sampling USED-FOR meta - reinforcement learning. exploration / exploitation strategies USED-FOR algorithm. Generic are information, them, and they. Task is online adaptation setting. OtherScientificTerm is behaviour. Method is RNNs. ","This paper proposes a new algorithm for the learning of recurrent policies in meta-reinforcement learning, where the goal is to learn a policy that is able to adapt to new tasks in an online adaptation setting. The proposed algorithm is based on privileged information from the task descriptor of the policy, which is used as a proxy for the information about the behaviour of the agent. The authors propose a method to learn an informed policy that can adapt to different task embeddings by using the parameters sharing and an auxiliary objective. They show that the proposed approach reduces the learning sample complexity by a factor of 2.5, and it outperforms Thompson sampling and other task-inference approaches. They also provide exploration/exploration strategies to improve the performance of their algorithm.","This paper proposes a new algorithm for learning of recurrent policies in the online adaptation setting. The key idea is to use privileged information of the task descriptor to guide the learning of the policy. The algorithm is based on recurrent neural networks, where the goal is to learn a set of policies that can be used to learn new tasks. The idea is that the privileged information can be shared across different tasks, and that the learned policy can be applied to any task embeddings (e.g. a new task descriptor). The authors show that the proposed algorithm can be combined with Thompson sampling and an auxiliary objective that combines parameters sharing and the auxiliary objective to learn the recurrent policy. Experiments show that it outperforms vanilla RNNs and Thompson sampling in meta-reinforcement learning, and outperforms other task-inference approaches. The authors also provide some exploration/exploration strategies for the algorithm. "
171,SP:bd89d254fbf31db61db237d08ab42981e27c52df,"trial - and - errors USED-FOR realworld applications. trial - and - errors USED-FOR RL. simulator USED-FOR optimal policies. dataset USED-FOR simulator. offline dataset USED-FOR policy. paradigm USED-FOR RL policy. model learning technique USED-FOR paradigm. offline data USED-FOR paradigm. offline data USED-FOR RL policy. models USED-FOR policy learning. adaptive policy USED-FOR real - world environments. stochasticity FEATURE-OF dynamics. synthetic environments CONJUNCTION real - world ride - hailing platform. real - world ride - hailing platform CONJUNCTION synthetic environments. method USED-FOR robust recommendations. method USED-FOR distortion problem. Generic is approach. OtherScientificTerm are fidelity of the simulator, and online sampling. Method is learning. ","This paper proposes a new approach to improve the fidelity of the simulator in RL by reducing the number of trial-and-errors in realworld applications. The authors propose a new paradigm for learning an RL policy from offline data using a model learning technique. The simulator is used to learn optimal policies from a dataset, and then the policy is learned from the offline dataset using an online dataset. The paper shows that the adaptive policy can be used in both synthetic environments and a real-world ride-hailing platform, and that the dynamics of the dynamics can be learned from stochasticity. The proposed method is shown to achieve robust recommendations for the distortion problem. ",This paper proposes a new paradigm for learning an RL policy from offline data. The proposed approach is based on the idea that the fidelity of the simulator to the optimal policies in the simulator is a function of the number of trial-and-errors in realworld applications. The authors propose a model learning technique to learn the paradigm from the offline dataset. The method is evaluated on synthetic environments and a real-world ride-hailing platform. The paper shows that the proposed method is able to learn robust recommendations that are robust to distortion problem. 
180,SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"sparse rewards USED-FOR goal - reaching behaviors. expert demonstrations CONJUNCTION value function. value function CONJUNCTION expert demonstrations. RL algorithms USED-FOR goal reaching policies. imitation learning USED-FOR goal reaching policies. imitation learning USED-FOR RL algorithms. algorithm USED-FOR goal - reaching behaviors. goal - reaching performance CONJUNCTION robustness. robustness CONJUNCTION goal - reaching performance. robustness EVALUATE-FOR RL algorithms. goal - reaching performance EVALUATE-FOR RL algorithms. iterated supervised learning procedure USED-FOR RL objective. benchmark tasks EVALUATE-FOR RL algorithms. Method are reinforcement learning ( RL ) algorithms, and supervised imitation learning. Generic is it. OtherScientificTerm are demonstrations, policy, and performance bounds. ","This paper studies the problem of reinforcement learning (RL) algorithms with sparse rewards for goal-reaching behaviors. The authors propose a new algorithm that uses imitation learning to learn goal reaching policies using expert demonstrations and value function. The algorithm is based on supervised imitation learning, and it is shown that it is able to learn a policy that achieves the goal with high performance bounds. The RL objective is learned using an iterated supervised learning procedure. The paper also shows that the proposed RL algorithms are able to achieve high performance on a variety of benchmark tasks. ","This paper proposes a new reinforcement learning (RL) algorithms based on supervised imitation learning. The key idea is to use sparse rewards to encourage goal-reaching behaviors in RL algorithms that can be learned from expert demonstrations and value function. The RL objective is formulated as an iterated supervised learning procedure, and it is evaluated on two benchmark tasks, where it is shown that the proposed algorithm is able to learn goal-reaching behaviors that are more robust to adversarial attacks. The authors also show that the policy is more robust when the number of demonstrations is small, and that the performance bounds are tighter. "
189,SP:c306530164d677e670554eeba8203c66bb3d9f7a,autoregressive models USED-FOR speech. autoregressive teacher model USED-FOR duration prediction. one - to - many mapping problem PART-OF TTS. knowledge distillation USED-FOR one - to - many mapping problem. autoregressive teacher model USED-FOR FastSpeech model. teacher model USED-FOR mel - spectrograms. teacher model USED-FOR duration. information loss FEATURE-OF mel - spectrograms. pitch CONJUNCTION energy. energy CONJUNCTION pitch. energy CONJUNCTION duration. duration CONJUNCTION energy. FastSpeech 2 USED-FOR FastSpeech. FastSpeech 2 USED-FOR one - to - many mapping problem. variation information of speech USED-FOR conditional inputs. one - to - many mapping problem PART-OF TTS. duration HYPONYM-OF variation information of speech. energy HYPONYM-OF variation information of speech. pitch HYPONYM-OF variation information of speech. pitch CONJUNCTION energy. energy CONJUNCTION pitch. duration CONJUNCTION pitch. pitch CONJUNCTION duration. predicted values USED-FOR inference. conditional inputs USED-FOR training. speech waveform USED-FOR pitch. speech waveform USED-FOR energy. FastSpeech 2s USED-FOR speech waveform. end - to - end inference USED-FOR FastSpeech 2s. FastSpeech 2 COMPARE FastSpeech. FastSpeech COMPARE FastSpeech 2. FastSpeech 2 COMPARE FastSpeech. FastSpeech COMPARE FastSpeech 2. FastSpeech 2 COMPARE autoregressive models. autoregressive models COMPARE FastSpeech 2. FastSpeech 2 COMPARE FastSpeech 2s. FastSpeech 2s COMPARE FastSpeech 2. training speed - up EVALUATE-FOR FastSpeech. training speed - up EVALUATE-FOR FastSpeech 2. voice quality EVALUATE-FOR FastSpeech 2. voice quality EVALUATE-FOR FastSpeech. Method is teacher - student distillation pipeline. Task is data simplification. Generic is model. ,"This paper proposes a teacher-student distillation pipeline to improve the performance of autoregressive models for speech. The teacher model is trained to predict the duration of a speech waveform using the speech waveforms, and the student model is used to predict a sequence of conditional inputs, which are the variation information of speech (e.g., pitch, energy, duration, etc.). The teacher is then used to train a FastSpeech model, which is a combination of the autoregression teacher model and the Fastspeech model trained with the teacher model. The authors show that the proposed model is able to achieve better performance than the state-of-the-art on the one-to-many mapping problem in TTS, a variant of knowledge distillation. The paper also shows that the training speed-up of the proposed method is comparable to that of the state of the art. ","This paper proposes a teacher-student distillation pipeline to improve the performance of autoregressive models for speech. The teacher model predicts mel-spectrograms with information loss, and the FastSpeech model predicts the duration prediction based on the autorgressive teacher model. The authors propose a one-to-many mapping problem based on knowledge distillation, which is an extension of TTS. The proposed model is evaluated on a speech waveform, where the teacher model is used to predict the duration and the duration of the speech. Experiments show that the proposed model achieves better performance than the state-of-the-art in terms of training speed, training speed-up, and voice quality. "
198,SP:79e9fb20d383816f54738ce70d137131ebc10290,"k - dimensional subspace FEATURE-OF tempered distribution q(x ). tempered distributions USED-FOR unsupervised dimension reduction problem ( UDR ). tempered distribution q(x ) USED-FOR empirical probability density function. q CONJUNCTION pemp. pemp CONJUNCTION q. minimization of the distance USED-FOR problem. generalized functions USED-FOR minimization of the distance. sufficient dimension reduction problem ( SDR ) HYPONYM-OF data science. algorithm USED-FOR problem. algorithm USED-FOR second. algorithm USED-FOR problem. optimization problem USED-FOR optimization problem. distributions USED-FOR optimization problem. ordinary functions USED-FOR optimization problem. algorithm USED-FOR minimization of I(f ) + λR(f ). two - step iterative computation USED-FOR algorithm. two - step iterative computation USED-FOR minimization of I(f ) + λR(f ). synthetic data CONJUNCTION datasets. datasets CONJUNCTION synthetic data. examples EVALUATE-FOR method. datasets USED-FOR method. synthetic data USED-FOR method. datasets USED-FOR examples. synthetic data USED-FOR examples. UDR HYPONYM-OF examples. Method is infinite - dimensional formulation. OtherScientificTerm are nonnegative penalty function R(f ), and λR(f ). Material is real data. ","This paper studies the unsupervised dimension reduction problem (UDR) with tempered distributions in a k-dimensional subspace, where the tempered distribution q(x) is the empirical probability density function. The problem is formulated as the minimization of the distance between the two distributions of a nonnegative penalty function R(f) and the true distribution Q(x). The authors propose a new infinite-dimensional formulation, which is based on generalized functions. The optimization problem is then formulated as an optimization problem with ordinary functions, and a second algorithm is proposed to solve the second problem. The authors show that the proposed algorithm is able to achieve the minimisation of I(f ) + λR(f). The proposed method is evaluated on synthetic data and two datasets with synthetic data as well as synthetic data with real data. ","This paper studies the unsupervised dimension reduction problem (UDR) with tempered distributions in the k-dimensional subspace, where the empirical probability density function q(x) is defined as a nonnegative penalty function R(f). The authors propose an infinite-dimensional formulation of the problem. The problem is solved by minimizing the minimization of the distance between q and p(x), which is a generalized function of generalized functions. The authors provide an algorithm to solve the first part of this problem and a second algorithm for solving the second part. The optimization problem is formulated as an optimization problem with ordinary functions, and the authors show that the optimization problem can be solved with distributions that are similar to q, p, and q. The proposed algorithm is based on two-step iterative computation for minimizing I(f) + λR(f), and experiments are conducted on synthetic data and synthetic data for examples of UDR. "
207,SP:93e54522e6c2b805905d21fc968fc40866f2898b,methods USED-FOR model. methods USED-FOR robustness. rare or underrepresented patterns FEATURE-OF model. contextual feature utility CONJUNCTION contextual feature sensitivity. contextual feature sensitivity CONJUNCTION contextual feature utility. robustness CONJUNCTION sensitivity. sensitivity CONJUNCTION robustness. Feature Contrastive Learning ( FCL ) USED-FOR model. contextual utility FEATURE-OF features. robustness CONJUNCTION sensitivity. sensitivity CONJUNCTION robustness. noise FEATURE-OF generalization. sensitivity EVALUATE-FOR models. robustness EVALUATE-FOR models. generalization EVALUATE-FOR models. FCL USED-FOR models. Task is real - world applications. ,This paper proposes a method to improve the robustness of a model trained on rare or underrepresented patterns. Feature Contrastive Learning (FCL) is used to train a model on features with contextual feature utility and contextual feature sensitivity. The authors show that FCL improves the sensitivity and robustness for models trained on the same dataset. They also show that the generalization performance of models trained with FCL is improved when the noise is reduced. ,"This paper proposes two methods for improving the robustness of a model trained on rare or underrepresented patterns. The first method is Feature Contrastive Learning (FCL), where the model is trained on a set of features with contextual feature utility, contextual feature sensitivity, and robustness. The second method is FCL, where the models are trained using FCL. The authors show that FCL improves the generalization of the models with respect to noise, robustness, and sensitivity. They also show that the proposed methods improve generalization in real-world applications."
216,SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"algorithm USED-FOR autonomous agents. latent representation PART-OF discriminator network. latent representation USED-FOR adversarial learning. adversarial learning USED-FOR algorithm. high dimensional observations USED-FOR autonomous agents. adversarial learning USED-FOR autonomous agents. mutual information constraints USED-FOR latent representation. shared feature space USED-FOR imitation. environment appearance CONJUNCTION agent embodiment. agent embodiment CONJUNCTION environment appearance. balancing CONJUNCTION manipulation and locomotive tasks. manipulation and locomotive tasks CONJUNCTION balancing. algorithm USED-FOR control problems. agent embodiment FEATURE-OF domain differences. environment appearance FEATURE-OF domain differences. manipulation and locomotive tasks HYPONYM-OF control problems. balancing HYPONYM-OF control problems. Method is Imitation learning methods. Generic are they, and constraints. OtherScientificTerm are optimal states, and features. ","This paper proposes a new algorithm for training autonomous agents from high dimensional observations. The proposed algorithm is based on adversarial learning with a latent representation in the discriminator network. The authors propose mutual information constraints on the latent representation to encourage imitation in the shared feature space. They show that they can achieve state-of-the-art performance on a variety of control problems (e.g. balancing, manipulation and locomotive tasks). They also show that the agent embodiment and environment appearance are important factors in domain differences. ","This paper proposes an algorithm for learning autonomous agents from high dimensional observations. The proposed algorithm is based on adversarial learning, where the latent representation of the discriminator network is learned from mutual information constraints. The authors show that they are able to learn the optimal states in the shared feature space for imitation. They also show that the proposed algorithm can be applied to control problems such as balancing, manipulation and locomotive tasks, and show that it can learn domain differences such as environment appearance and agent embodiment.  "
225,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"test accuracy EVALUATE-FOR unpruned network. pruned network COMPARE unpruned network. unpruned network COMPARE pruned network. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep neural network ( DNN ) USED-FOR applications. LTH PART-OF deep neural network ( DNN ). natural language processing HYPONYM-OF applications. computer vision HYPONYM-OF applications. objective function CONJUNCTION sample complexity. sample complexity CONJUNCTION objective function. guaranteed generalization EVALUATE-FOR model. algorithm USED-FOR pruned neural network. non - pruned weights PART-OF hidden layer. accelerated ) stochastic gradient descent algorithm USED-FOR algorithm. pruned neural network USED-FOR model. pruned neural network COMPARE unpruned one. unpruned one COMPARE pruned neural network. Task are lottery ticket hypothesis ( LTH ), and pruning multi - layer neural networks. Metric are generalization, and generalization of the winning ticket. OtherScientificTerm are zero generalization error, and convex region. Method is neural network model. ",This paper studies the lottery ticket hypothesis (LTH) in the context of pruning multi-layer neural networks. The authors propose a new algorithm for pruning a pruned neural network with non-pruned weights in the hidden layer. The algorithm is based on (an accelerated) stochastic gradient descent algorithm. The paper shows that the pruned network performs better than an unpruned network in terms of test accuracy and sample complexity compared to the prune network.  The authors also show that the proposed algorithm achieves a guaranteed generalization of the winning ticket. ,"This paper studies the lottery ticket hypothesis (LTH) for pruning multi-layer neural networks. The authors show that a pruned network outperforms an unpruned network in terms of test accuracy and generalization error. The paper also shows that the LTH is applicable to applications such as computer vision and natural language processing. The proposed algorithm is based on an accelerated (stochastic gradient descent algorithm, where non-pruned weights are added to the hidden layer of a deep neural network (DNN). The authors also show that the pruned neural network is more robust to zero-generalization error compared to the original neural network model. The main contribution of the paper is the generalization of the winning ticket to the convex region. The model achieves guaranteed generalization with respect to the objective function and sample complexity."
234,SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,generalization EVALUATE-FOR neural networks. accuracy CONJUNCTION generalization. generalization CONJUNCTION accuracy. data augmentation approaches USED-FOR neural networks. accuracy EVALUATE-FOR neural networks. generalization EVALUATE-FOR data augmentation approaches. accuracy EVALUATE-FOR data augmentation approaches. augmented data COMPARE clean data. clean data COMPARE augmented data. AutoLabel USED-FOR augmented data. clean distribution CONJUNCTION augmented distribution. augmented distribution CONJUNCTION clean distribution. hold - out validation set USED-FOR calibration - performance. calibration - performance USED-FOR AutoLabel. hold - out validation set USED-FOR AutoLabel. label smoothing USED-FOR AutoLabel. mixup CONJUNCTION adversarial training. adversarial training CONJUNCTION mixup. AugMix CONJUNCTION mixup. mixup CONJUNCTION AugMix. AutoLabel USED-FOR data augmentation methods. adversarial training HYPONYM-OF data augmentation methods. AugMix HYPONYM-OF data augmentation methods. mixup HYPONYM-OF data augmentation methods. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. AutoLabel USED-FOR models. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR AutoLabel. calibration EVALUATE-FOR AutoLabel. accuracy EVALUATE-FOR AutoLabel. AutoLabel USED-FOR adversarial training. clean accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION clean accuracy. OtherScientificTerm is distributional shift. ,"This paper studies the relationship between accuracy and generalization of data augmentation approaches for neural networks. The authors propose AutoLabel, a new dataset-augmented version of AugMix and mixup for training models. AutoLabel is based on a hold-out validation set, which is used to evaluate the calibration-performance of AutoLabel. The paper shows that AutoLabel outperforms AugMix on CIFAR-10 and ImageNet. ","This paper studies the generalization properties of data augmentation approaches for neural networks. The authors propose AutoLabel, an extension of AugMix and AugMix, which improves the accuracy and generalization of neural networks on augmented data compared to clean data. AutoLabel is based on a hold-out validation set for calibration-performance and a calibration-perception set for generalization. The paper also proposes label smoothing to improve the accuracy of AutoLabel. Experiments on CIFAR-10 and ImageNet show that AutoLabel outperforms other models on both clean accuracy and adversarial robustness."
243,SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"heuristic proxy classification tasks CONJUNCTION data augmentations. data augmentations CONJUNCTION heuristic proxy classification tasks. methods USED-FOR heuristic proxy classification tasks. data augmentations USED-FOR methods. causal framework USED-FOR self - supervised representation learning. proxy classifiers USED-FOR pretraining. invariance constraints USED-FOR proxy classifiers. invariance constraints USED-FOR data augmentations. selfsupervised objective, Representation Learning USED-FOR invariant prediction of proxy targets. invariance regularizer USED-FOR generalization guarantees. invariance regularizer USED-FOR invariant prediction of proxy targets. Invariant Causal Mechanisms ( RELIC ) USED-FOR selfsupervised objective, Representation Learning. causality USED-FOR contrastive learning. contrastive learning HYPONYM-OF self - supervised method. RELIC COMPARE methods. methods COMPARE RELIC. robustness CONJUNCTION out - of - distribution generalization. out - of - distribution generalization CONJUNCTION robustness. RELIC COMPARE methods. methods COMPARE RELIC. out - of - distribution generalization FEATURE-OF ImageNet. human - level performance EVALUATE-FOR methods. out - of - distribution generalization EVALUATE-FOR RELIC. Atari EVALUATE-FOR methods. robustness EVALUATE-FOR RELIC. out - of - distribution generalization EVALUATE-FOR methods. robustness EVALUATE-FOR methods. Method is Self - supervised learning. OtherScientificTerm are supervised signals, and augmentations. Material is unlabeled data. ","This paper proposes a causal framework for self-supervised representation learning based on the Invariant Causal Mechanisms (RELIC) for selfsupervised objective, Representation Learning. The authors propose two methods for heuristic proxy classification tasks and data augmentations. The invariance constraints on proxy classifiers are used for pretraining the proxy classifier, and the invariance regularizer is used for generalization guarantees for the invariant prediction of proxy targets. The two methods are evaluated on ImageNet with human-level performance and out-of-distribution generalization on Atari. RELIC outperforms the other methods in terms of robustness, out of-sample generalization, and robustness on unlabeled data. ","This paper proposes a causal framework for self-supervised representation learning. The authors propose Invariant Causal Mechanisms (RELIC) for selfsupervised objective, Representation Learning, which provides invariant prediction of proxy targets and invariance constraints for data augmentations for heuristic proxy classification tasks. The invariance regularizer provides generalization guarantees for the training of proxy classifiers for pretraining. They show that RELIC outperforms existing methods in terms of robustness, out-of-distribution generalization, and human-level performance on ImageNet, Atari, and CIFAR-10. They also show that contrastive learning with causality can be used to improve the performance of RELIC. "
252,SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,visual representations of the observed scene USED-FOR navigation actions. Visual Transformer Network ( VTNet ) USED-FOR informative visual representation in navigation. spatial locations of objects CONJUNCTION image regions. image regions CONJUNCTION spatial locations of objects. VTNet HYPONYM-OF structure. structure USED-FOR visual representations. pre - training scheme USED-FOR navigation policy learning. pre - training scheme USED-FOR visual representations. navigation signals USED-FOR visual representations. informative representation USED-FOR navigation. attention operations USED-FOR informative representation. descriptors USED-FOR informative representation. VTNet USED-FOR informative representation. object and region features USED-FOR spatial - aware descriptors. spatial - aware descriptors USED-FOR VTNet. object and region features USED-FOR VTNet. location cues FEATURE-OF object and region features. attention operations USED-FOR descriptors. artificial environment AI2 - Thor EVALUATE-FOR VTNet. VTNet COMPARE methods. methods COMPARE VTNet. artificial environment AI2 - Thor EVALUATE-FOR methods. Task is Object goal navigation. OtherScientificTerm is directional navigation signals. Method is visual representation. ,"This paper proposes a visual Transformer Network (VTNet) for informative visual representation in navigation. The structure of VTNet is based on the VTNet HYPONYM-OF structure, where the visual representations of the observed scene are used to guide navigation actions. The visual representations are learned using navigation signals from the navigation signals. Object goal navigation signals are used as directional navigation signals to guide the visual representation. The informative representation of the informative representation is learned by using attention operations on the object and region features in VTNet. The spatial-aware descriptors learned by VTNet are also used to learn the spatial-specific descriptors for the object/region features. Experiments on the artificial environment AI2-Thor show that VTNet performs better than other methods. ",This paper proposes a Visual Transformer Network (VTNet) for learning an informative visual representation in navigation. The structure of the visual representations of the observed scene can be used to guide navigation actions. The authors propose a pre-training scheme for learning visual representations for navigation policy learning. The informative representation is learned by using attention operations on the object and region features of the scene and the spatial locations of objects and image regions. Object goal navigation signals are used as directional navigation signals to guide the visual representation. Experiments on an artificial environment AI2-Thor show that VTNet outperforms other methods. 
261,SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,Federated learning USED-FOR neural network models. model parameters USED-FOR federated learning. solution USED-FOR privacy - preserving federated learning. secure aggregation primitive USED-FOR privacy - preserving federated learning. secure aggregation primitive USED-FOR solution. communication - computation efficient secure aggregation COMPARE secure solution. secure solution COMPARE communication - computation efficient secure aggregation. communication - computation efficient secure aggregation USED-FOR communication / computational resources. scheme USED-FOR topology. sparse random graphs COMPARE complete graph. complete graph COMPARE sparse random graphs. topology FEATURE-OF secret - sharing nodes. sparse random graphs USED-FOR topology. Erdős - Rényi graph USED-FOR G. reliability / privacy EVALUATE-FOR scheme. reliability CONJUNCTION data privacy. data privacy CONJUNCTION reliability. data privacy FEATURE-OF federated learning systems. scheme USED-FOR scheme. federated learning systems EVALUATE-FOR scheme. data privacy EVALUATE-FOR scheme. reliability EVALUATE-FOR scheme. OtherScientificTerm is local data. ,This paper proposes a secure aggregation primitive for privacy-preserving federated learning for neural network models. The proposed solution is based on the communication-computation efficient secure aggregation for communication/computational resources. The key idea is to use a G. Erdős-Rényi graph as the topology instead of a complete graph. The topology of the secret-sharing nodes can be computed by sparse random graphs. The authors show that the proposed scheme improves the reliability/privacy of the scheme compared to other recent work in the literature. ,This paper proposes a new solution for privacy-preserving federated learning for neural network models. The proposed solution is based on a secure aggregation primitive that is more efficient than the communication-computation efficient secure aggregation for communication/computational resources. The key idea of the proposed G.R.E.G. is to use the Erdős-Rényi graph as the topology of the secret-sharing nodes. The authors show that the proposed scheme achieves better reliability and data privacy compared to the complete graph and sparse random graphs. 
270,SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"incentive compatible auction PART-OF Auction Design. theoretical approaches USED-FOR problem. neural network architectures USED-FOR optimal auctions. theoretical auction design USED-FOR time - independent Lagrangian. inner maximization loop USED-FOR optimal misreports. inner maximization loop USED-FOR optimization procedure. stationary utility functions FEATURE-OF two - player game. two - player game USED-FOR Auction Design. Generic is approach. Method are hyper - parameter search, and neural network. OtherScientificTerm is auctions. ","This paper studies the problem of finding anincentive compatible auction in Auction Design. The authors propose a theoretical approach to solve this problem by using neural network architectures to find optimal auctions. The proposed approach is based on hyper-parameter search, where the optimal misreports are computed using an inner maximization loop. The optimization procedure is then applied to a two-player game with stationary utility functions. The theoretical auction design is applied to the time-independent Lagrangian. The experimental results show that the proposed neural network achieves competitive performance.",This paper proposes a novel approach to solve the problem of finding an incentive compatible auction in Auction Design. The problem is formulated as a time-independent Lagrangian with theoretical approaches to solving the problem. The main idea is to use neural network architectures to find optimal auctions with hyper-parameter search. The optimization procedure is based on an inner maximization loop that maximizes the optimal misreports of the neural network. The proposed approach is evaluated on a two-player game with stationary utility functions. 
279,SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,pre - trained model USED-FOR downstream task. large - scale dataset USED-FOR deep neural network. supervised and unsupervised pre - training approaches USED-FOR learning representations. discriminative knowledge of labels CONJUNCTION intrinsic structure of data. intrinsic structure of data CONJUNCTION discriminative knowledge of labels. discriminative knowledge USED-FOR fine - tuning. former USED-FOR fine - tuning methods. intrinsic structure of data USED-FOR boosting fine - tuning. general learning approach USED-FOR supervised and unsupervised pre - trained representations. Bi - tuning HYPONYM-OF general learning approach. supervised and unsupervised pre - trained representations USED-FOR downstream tasks. classifier head CONJUNCTION projector head. projector head CONJUNCTION classifier head. projector head USED-FOR intrinsic structure of data. contrastive cross - entropy loss USED-FOR label information. classifier head USED-FOR label information. Bi - tuning USED-FOR vanilla fine - tuning. instancecontrast way FEATURE-OF label information. contrastive cross - entropy loss FEATURE-OF classifier head. categorical contrastive learning loss USED-FOR projector head. Bi - tuning USED-FOR fine - tuning tasks. fine - tuning tasks EVALUATE-FOR supervised and unsupervised pre - trained models. low - data regime FEATURE-OF accuracy. Generic is latter. OtherScientificTerm is pre - trained representations. ,"This paper studies the problem of fine-tuning a pre-trained model for a downstream task on a large-scale dataset. The authors propose a general learning approach called Bi-tunning, which is a combination of supervised and unsupervised pre-training approaches for learning representations for downstream tasks. The main idea is to use discriminative knowledge of labels and intrinsic structure of data to guide the training of the classifier head and the projector head. The former can be used to fine-tune methods that rely on the former, while the latter is used for boosting the performance of the latter. The proposed method is evaluated on a variety of fine -tuning tasks and shows that the proposed method can improve the accuracy in a low-data regime. ","This paper proposes a general learning approach, Bi-tuning, to improve the performance of supervised and unsupervised pre-training approaches for learning representations for a downstream task on a large-scale dataset. The main idea is to use discriminative knowledge of labels and the intrinsic structure of data for boosting fine-tuned methods. The authors show that the classifier head, the projector head, and the contrastive cross-entropy loss improve the label information in an instancecontrast way. The latter improves the accuracy in the low-data regime. The proposed method is evaluated on a variety of downstream tasks, and compared to vanilla fine -tuning. "
288,SP:87e5b552c13d73bd85249062a152c6c140e594a9,"adversarial accuracy CONJUNCTION adversarial training. adversarial training CONJUNCTION adversarial accuracy. robustness EVALUATE-FOR classifiers. adversarial accuracy USED-FOR robustness. adversarial accuracy EVALUATE-FOR classifiers. measure USED-FOR robustness. measure USED-FOR classifiers. robustness EVALUATE-FOR classifiers. accuracy CONJUNCTION accuracy. accuracy CONJUNCTION accuracy. It USED-FOR adversarial robustness. adversarial robustness EVALUATE-FOR classifiers. It USED-FOR classifiers. accuracy FEATURE-OF adversarially perturbed samples. invariance - based adversarial examples USED-FOR model. genuine adversarial accuracy EVALUATE-FOR classifier. test accuracy CONJUNCTION lp norm - based test adversarial robustness. lp norm - based test adversarial robustness CONJUNCTION test accuracy. OtherScientificTerm are generalization, predicted classes, and perceptual classes. Material is clean data. Generic are it, and distance metrics. Method is norm - based distance metric. ","This paper studies the problem of robustness of classifiers against adversarial accuracy and adversarial training in the presence of clean data. The authors propose a new measure, the norm-based distance metric, to measure the robustness against classifiers. It is based on the fact that the accuracy of the classifier depends on the test accuracy as well as the accuracy in the adversarially perturbed samples. The distance metrics are based on invariance-based adversarial examples, and the authors show that it can be used to improve the performance of a classifier. The main contribution of the paper is to show that the distance metric is a good way to evaluate the classifiers' robustness to adversarial perturbations. ","This paper proposes a new measure for measuring robustness of classifiers to adversarial accuracy and adversarial training. It is based on the norm-based distance metric. The authors show that adversarial robustness is a measure of the generalization ability of a classifier to a set of predicted classes. They also show that it can be used to measure classifiers' accuracy on adversarially perturbed samples. They further show that classifiers can be trained on invariance-based adversarial examples, and show that the distance metrics can be applied to the classifier's accuracy and test accuracy. "
297,SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"fairness PART-OF algorithmic designs. graph - structured data USED-FOR disparate impact. fairness concept FEATURE-OF dyadic fairness. edges PART-OF graph. graph connections USED-FOR dyadic fairness. dyadic fairness FEATURE-OF link predictive scores. algorithm USED-FOR fair adjacency matrix. fair adjacency matrix USED-FOR fair link prediction. graph structural constraints USED-FOR fair link prediction. FairAdj USED-FOR fair adjacency matrix. graph structural constraints FEATURE-OF fair adjacency matrix. method USED-FOR dyadic fairness. fairness - utility tradeoff EVALUATE-FOR method. Task are Disparate impact, machine learning applications, and mitigating discrimination. OtherScientificTerm is predictive relationship. Method is graph neural networks. Metric is predictive accuracy. ","This paper studies the problem of fairness in algorithmic designs with graph-structured data. The authors propose a new fairness concept called dyadic fairness, which is based on the notion of edges in a graph. They show that the distance between the edges in the graph is a measure of the difference in the predictive relationship between the two nodes. They then propose an algorithm called FairAdj to learn a fair adjacency matrix for fair link prediction with graph structural constraints. The proposed method is evaluated on a fairness-utility tradeoff and shows that the proposed method achieves better dyadic unfair. ","This paper studies the problem of fairness in algorithmic designs. Disparate impact is an important problem in machine learning applications, where the goal is to mitigate disparate impact on graph-structured data. The authors propose a fairness concept of dyadic fairness, where edges in a graph are weighted according to the predictive relationship between the edges of the graph and the edges in the graph. The paper proposes an algorithm called FairAdj to learn a fair adjacency matrix for fair link prediction under graph structural constraints. The proposed method is evaluated on the fairness-utility tradeoff, and is shown to be effective in mitigating discrimination. "
306,SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"Autoencoders HYPONYM-OF information compression framework. generative ability FEATURE-OF it. generative ability FEATURE-OF autoencoder. Gaussian prior knowledge USED-FOR synthesis. Gaussian prior knowledge USED-FOR VAE. interpolation HYPONYM-OF exploration in latent space. disentangled representation CONJUNCTION regularization. regularization CONJUNCTION disentangled representation. regularization USED-FOR exploration in latent space. Disentangled Exploration Autoencoder ( DEAE ) USED-FOR controllable synthesis. regularization USED-FOR controllable synthesis. regularization USED-FOR Disentangled Exploration Autoencoder ( DEAE ). disentangled representation USED-FOR Disentangled Exploration Autoencoder ( DEAE ). encoder USED-FOR DEAE. encoder USED-FOR latent code space. directed interpolation USED-FOR encoder. directed interpolation USED-FOR latent code space. encoder USED-FOR latent representation. disentanglement FEATURE-OF latent representation. disentanglement CONJUNCTION exploration. exploration CONJUNCTION disentanglement. positive loop USED-FOR DEAE. exploration USED-FOR positive loop. disentanglement USED-FOR positive loop. DEAE USED-FOR attribute - controllable augmented samples. DEAE USED-FOR dataset bias. DEAE USED-FOR fairness problems. Method are GAN - based adversarial training, and decoder. OtherScientificTerm are latent code, disentangled latent code, and interpolated latent code. Generic is method. ","This paper proposes a new information compression framework called Autoencoders. The idea is to use GAN-based adversarial training to train an autoencoder with a generative ability similar to it. The authors use Gaussian prior knowledge from VAE to guide the synthesis and disentangle the latent code from the disentangled latent code. The encoder is then used to learn the latent representation of the encoder and the encoders in a latent code space, and directed interpolation is used to encourage exploration in latent space (e.g., interpolation) and regularization for controllable synthesis. The proposed method is evaluated on attribute-controllable augmented samples and shows that DEAE can achieve better dataset bias than DEAE for fairness problems. ","This paper proposes a new information compression framework, Autoencoders, which is based on GAN-based adversarial training. The key idea of the autoencoder is to use generative ability of the encoder and the disentangled representation of the latent code space. The authors propose a VAE based on Gaussian prior knowledge for synthesis. The encoder encodes the latent representation and disentangles it into two parts. The first part is the decoder, and the second part is a disentanglement of the interpolated latent code. Experiments are conducted on controllable synthesis with regularization and exploration in latent space with interpolation. The proposed method is evaluated on attribute-controllable augmented samples. DEAE is shown to improve the dataset bias in fairness problems. "
315,SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"Episodic and semantic memory PART-OF human memory model. serial event ( episodic memory ) USED-FOR compressed representation. Bayesian memory allocation scheme USED-FOR episodic and semantic memory. hierarchical latent variable model USED-FOR Bayesian memory allocation scheme. locally contiguous memory USED-FOR differentiable block allocated latent memory. locally contiguous memory USED-FOR Kanerva Machine. feed forward deterministic process USED-FOR it. binarized MNIST CONJUNCTION binarized Omniglot. binarized Omniglot CONJUNCTION binarized MNIST. allocation scheme USED-FOR memory conditional image generation. binarized MNIST FEATURE-OF conditional likelihood values. DMLab Mazes CONJUNCTION Celeb - A. Celeb - A CONJUNCTION DMLab Mazes. CIFAR10 CONJUNCTION DMLab Mazes. DMLab Mazes CONJUNCTION CIFAR10. Celeb - A CONJUNCTION ImageNet32×32. ImageNet32×32 CONJUNCTION Celeb - A. Method are complementary learning systems, and heap allocation. Task is memory writing. OtherScientificTerm is read key distribution. ","This paper proposes a Bayesian memory allocation scheme for episodic and semantic memory in a human memory model. The hierarchical latent variable model is used to learn the differentiable block allocated latent memory. The compressed representation is learned from the serial event (episodic memory) and the read key distribution. The allocation scheme is applied to memory conditional image generation with binarized MNIST and binarised Omniglot. The Kanerva Machine is trained with locally contiguous memory. Experiments on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32×32 demonstrate the effectiveness of the proposed allocation scheme.","This paper proposes a Bayesian memory allocation scheme for episodic and semantic memory in the human memory model. The authors propose a hierarchical latent variable model, where each event (episodic memory) is represented as a compressed representation, and the read key distribution (semantic memory) corresponds to a sequence of events. The proposed allocation scheme is applied to memory conditional image generation using binarized MNIST, binarised Omniglot, and a feed forward deterministic process. Experiments on CIFAR10, DMLab Mazes, Celeb-A, and ImageNet32×32 demonstrate the effectiveness of the allocation scheme. "
324,SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,deep learning models USED-FOR machine learning tasks. Attention mechanisms CONJUNCTION deep learning models. deep learning models CONJUNCTION Attention mechanisms. sample complexity CONJUNCTION loss landscape. loss landscape CONJUNCTION sample complexity. loss landscape FEATURE-OF attention - based neural networks. sample complexity FEATURE-OF attention - based neural networks. attention models COMPARE models. models COMPARE attention models. local minimum PART-OF attention model. sample complexity EVALUATE-FOR models. prediction error EVALUATE-FOR local minimum. sample complexity EVALUATE-FOR attention models. OtherScientificTerm is attention. Method is self - attention. ,"This paper studies the problem of self-attention in deep learning models for machine learning tasks. Attention mechanisms and the loss landscape of attention-based neural networks are well-studied in the literature. The authors show that the local minimum of an attention model is a local minimum in terms of the prediction error, and that the sample complexity of these models is lower than that of other attention models. ",This paper presents a theoretical analysis of attention mechanisms in deep learning models for machine learning tasks. Attention mechanisms and the loss landscape of attention-based neural networks are studied. The authors show that the local minimum of an attention model is a function of the prediction error of the attention model. They also show that self-attention does not improve the sample complexity of attention models compared to other models.
333,SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,Bayesian modeling USED-FOR Active inference. biologically plausible model USED-FOR Bayesian modeling. free energy principle CONJUNCTION prior preference. prior preference CONJUNCTION free energy principle. reinforcement learning ( RL ) algorithms USED-FOR active inference. negative value function USED-FOR EFE. method USED-FOR prior preference. prior preference CONJUNCTION theoretical connection. theoretical connection CONJUNCTION prior preference. theoretical connection USED-FOR method. active inference USED-FOR inverse RL. prior preference learning USED-FOR active inference. active inference USED-FOR inverse RL problem. prior preference learning USED-FOR inverse RL problem. EFE - based rewards USED-FOR active inference. OtherScientificTerm is expected free energy ( EFE ). ,This paper studies active inference in Bayesian modeling with a biologically plausible model. The authors propose two reinforcement learning (RL) algorithms for active inference with a negative value function for the expected free energy (EFE) and a free energy principle for the prior preference. The proposed method is based on prior preference learning with theoretical connection and active inference for inverse RL problem with EFE-based rewards.,"This paper proposes a biologically plausible model for Active inference in Bayesian modeling. Active inference is an extension of reinforcement learning (RL) algorithms for active inference, where the free energy principle and prior preference are introduced. The authors propose a method to learn the prior preference and the theoretical connection between EFE and the expected free energy (EFE). The EFE is defined as the negative value function of the EFE. The proposed active inference is based on prior preference learning for inverse RL problem with EFE-based rewards."
342,SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"data augmentation method USED-FOR generalization. data augmentation method USED-FOR adversarial and standard learning. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. OOD data USED-FOR learning scenario. method COMPARE data augmentation methods. data augmentation methods COMPARE method. method COMPARE adversarial training. adversarial training COMPARE method. Method is neural networks. Generic is methods. Material are UID data, and image data. OtherScientificTerm are pseudo - labels, and undesirable features. ","This paper proposes a data augmentation method for generalization between adversarial and standard learning. The proposed method is based on the observation that adversarial training does not generalize well to unseen data. The authors propose to use OOD data for the learning scenario, and show that the proposed method can generalize better than existing data augmentation methods. The method is evaluated on CIFAR-10 CONJUNCTION and ImageNet.",This paper proposes a data augmentation method for generalization in adversarial and standard learning. The authors show that the proposed method outperforms adversarial training and standard training on CIFAR-10 CONJUNCTION and ImageNet. The main difference between the two methods is that the authors propose to use UID data instead of OOD data for the learning scenario. They also show that their method is more robust to pseudo-labels than other methods. 
351,SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"Fast Linearized Adaptive Policy ( FLAP ) HYPONYM-OF metareinforcement learning ( meta - RL ) method. shared linear representation of the policy USED-FOR FLAP. adapter network USED-FOR linear weights. adapter network USED-FOR policy. MAML HYPONYM-OF prior meta - RL methods. gradient descent USED-FOR meta - policy. adaptation run - time EVALUATE-FOR separate feed - forward network. FLAP COMPARE prior methods. prior methods COMPARE FLAP. continuous - control meta - RL benchmarks EVALUATE-FOR FLAP. adaptation run - time speeds EVALUATE-FOR prior methods. average return CONJUNCTION adaptation run - time speeds. adaptation run - time speeds CONJUNCTION average return. out - of - distribution tasks EVALUATE-FOR FLAP. average return EVALUATE-FOR FLAP. adaptation run - time speeds EVALUATE-FOR FLAP. Task are outof - distribution tasks, and adaptation. Generic are task, and tasks. Method is prior MetaRL methods. ","This paper proposes a Fast Linearized Adaptive Policy (FLAP) method for metareinforcement learning (meta-RL) method. FLAP is based on a shared linear representation of the policy. The adapter network is used to learn the linear weights for the policy by gradient descent. The separate feed-forward network is then used for the adaptation run-time. The authors show that FLAP outperforms prior methods on continuous-control meta-RL benchmarks, and achieves better average return than prior methods in out-of-distribution tasks.","This paper proposes a Fast Linearized Adaptive Policy (FLAP) method for metareinforcement learning (meta-RL) method. FLAP uses a shared linear representation of the policy and a separate feed-forward network to compute the adaptation run-time. The policy is learned by gradient descent. The adapter network is used to learn the linear weights. The authors show that FLAP outperforms prior meta-RL methods such as MAML on out-of-distribution tasks, and the adaptation is faster than prior methods on continuous-control meta- RL benchmarks. "
360,SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"communication efficiency FEATURE-OF algorithm. kernel k - means USED-FOR optimization problem. federated settings FEATURE-OF kernel k - means. federated settings USED-FOR optimization problem. communication efficient mech anism ( CEM ) USED-FOR communication cost. feder ated kernelk - means USED-FOR privacy preservation. matrix operations USED-FOR local computational results. federated kernel k - means COMPARE kernel k - means. kernel k - means COMPARE federated kernel k - means. clustering quality EVALUATE-FOR federated kernel k - means. clustering quality EVALUATE-FOR kernel k - means. communication cost EVALUATE-FOR DSPGD. O(1 / T ) rate FEATURE-OF DSPGD. CEM USED-FOR DSPGD. CEM USED-FOR DSPGD. communication cost EVALUATE-FOR federated kerne l k - means. clustering quality EVALUATE-FOR federated kerne l k - means. Method are federated kernel k - means algorithm, and kernelk - means. OtherScientificTerm are approximate solution, and cloud server. ",This paper proposes a federated kernel k-means algorithm for the optimization problem in federated settings. The proposed algorithm is based on communication efficient mech anism (CEM) to reduce the communication cost of the algorithm. The authors provide an approximate solution to the approximate solution and provide a theoretical analysis of the communication efficiency of the proposed algorithm. They show that the proposed DSPGD has O(1/T) rate and CEM can be used to improve the communication costs of the federated kernels. They also provide some theoretical guarantees on the clustering quality of the kernel k - means compared to the standard kernels. ,This paper proposes a federated kernel k-means algorithm for the optimization problem in federated settings. The algorithm is based on communication efficient mech anism (CEM) to reduce the communication cost. The authors provide an approximate solution to the problem. The paper also provides privacy preservation by using federated ated kernelk-mean. The local computational results are obtained by using matrix operations. The proposed DSPGD has O(1/T) rate and CEM is shown to reduce communication cost for DDPGD. The results show that the proposed kernel k -means are competitive with other federated kernels in terms of clustering quality. 
369,SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"hardware & latency constraints FEATURE-OF architectures. accuracy EVALUATE-FOR architectures. approach USED-FOR models. approach USED-FOR resource - intensive tasks. deployment targets USED-FOR resource - intensive tasks. CompOFA HYPONYM-OF design space. model search / extraction time COMPARE state of the art. state of the art COMPARE model search / extraction time. heuristics COMPARE state of the art. state of the art COMPARE heuristics. design space USED-FOR models. diversity of hardware and latency targets FEATURE-OF models. Method is CNNs. Metric are constant training cost, and complexity. Generic is cost. OtherScientificTerm are combinatorial explosion of sub - optimal model configurations, search space, training budget, search, accuracy - latency Pareto frontier, model dimensions, and Pareto optimality. Material is ImageNet. ","This paper studies the problem of constant training cost for CNNs. The authors consider the combinatorial explosion of sub-optimal model configurations, and propose a new approach to train models with different deployment targets for resource-intensive tasks. They show that architectures with different hardware & latency constraints have different performance and accuracy. They also show that models with diversity of hardware and latency targets have different design space (e.g. CompOFA) and different search space (i.e. ImageNet). The authors show that in the search space, the search is more expensive than in the state of the art, and that the complexity of the search increases as the number of tasks increases. They then show that the search, the accuracy-latency Pareto frontier, and the heuristics are more efficient than the state-of-the-art. ","This paper proposes a new approach for training models on resource-intensive tasks with deployment targets. The approach is based on the combinatorial explosion of sub-optimal model configurations. The authors show that architectures with hardware & latency constraints are more likely to reach accuracy-latency Pareto frontier than architectures with constant training cost. They also show that models with diversity of hardware and latency targets achieve better Paretopo optimality. The paper also shows that heuristics are better than state of the art in terms of model search/extraction time, and that the search space is more efficient than the training budget. "
378,SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,Meta - learning USED-FOR model. limited data USED-FOR model. adversarial samples USED-FOR meta - learning. ADML ( ADversarial Meta - Learner ) USED-FOR initialization of a learning model. meta - learning algorithm USED-FOR initialization of a learning model. adversarial manner USED-FOR initialization of a learning model. ADML ( ADversarial Meta - Learner ) HYPONYM-OF meta - learning algorithm. clean and adversarial samples USED-FOR ADML ( ADversarial Meta - Learner ). meta - learning algorithms COMPARE it. it COMPARE meta - learning algorithms. it USED-FOR adversarial samples. it COMPARE meta - learning algorithms. meta - learning algorithms COMPARE it. MiniImageNet CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION MiniImageNet. accuracy CONJUNCTION robustness. robustness CONJUNCTION accuracy. ADML COMPARE representative meta - learning algorithms. representative meta - learning algorithms COMPARE ADML. attack mechanisms USED-FOR adversarial samples. image datasets EVALUATE-FOR ADML. MiniImageNet HYPONYM-OF image datasets. CIFAR100 HYPONYM-OF image datasets. Method is learning model. Material is clean samples. OtherScientificTerm is limited and even contaminated samples. ,This paper proposes a meta-learning algorithm for the initialization of a learning model in an adversarial manner. The authors propose ADML (ADversarial Meta-Learner) which uses both clean and adversarial samples for the training of a model on limited data. They show that ADML outperforms the state-of-the-art in terms of accuracy and robustness on image datasets such as MiniImageNet and CIFAR100. They also show that it can be used to train adversarial models with attack mechanisms. ,This paper proposes a meta-learning algorithm for the initialization of a learning model on limited data. The authors propose ADML (ADversarial Meta-Learner) which combines clean and adversarial samples in order to improve the performance of the model. They show that ADML outperforms other representative meta-Learning algorithms on image datasets such as MiniImageNet and CIFAR100. They also show that it can be used to learn adversarial targets that are more robust to attack mechanisms.
387,SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,Error correction codes PART-OF communication applications. maximum likelihood rule USED-FOR decoding of transmitted codewords. permutation USED-FOR permutation decoding. data - driven framework USED-FOR permutation selection. node embedding CONJUNCTION self - attention. self - attention CONJUNCTION node embedding. domain knowledge CONJUNCTION machine learning concepts. machine learning concepts CONJUNCTION domain knowledge. domain knowledge PART-OF data - driven framework. machine learning concepts PART-OF data - driven framework. self - attention HYPONYM-OF machine learning concepts. node embedding HYPONYM-OF machine learning concepts. simulated Bose Chaudhuri Hocquenghem ( BCH ) code COMPARE baseline decoders. baseline decoders COMPARE simulated Bose Chaudhuri Hocquenghem ( BCH ) code. bit error rate EVALUATE-FOR simulated Bose Chaudhuri Hocquenghem ( BCH ) code. bit error rate EVALUATE-FOR baseline decoders. self - attention networks USED-FOR physical layer communication systems. Method is suboptimal decoding algorithms. Generic is algorithms. ,"This paper studies the problem of suboptimal decoding algorithms in the context of communication applications with error correction codes. The authors propose a data-driven framework that uses permutation to perform permutation selection in the form of a maximum likelihood rule for decoding of transmitted codewords. The proposed framework combines domain knowledge and machine learning concepts such as node embedding, self-attention, and self-supervised learning. Experimental results show that the simulated Bose Chaudhuri Hocquenghem (BCH) code has a better bit error rate than the baseline decoders. ",This paper proposes a data-driven framework for permutation selection in the context of permutation decoding. The authors propose a novel algorithm that uses a maximum likelihood rule for decoding of transmitted codewords. The proposed algorithms are based on suboptimal decoding algorithms. The paper also proposes a set of error correction codes for communication applications. The experiments show that the simulated Bose Chaudhuri Hocquenghem (BCH) code outperforms the baseline decoders in terms of bit error rate and domain knowledge.  The authors also propose to use self-attention networks for physical layer communication systems.
396,SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"fine - tuning BERT USED-FOR text classification task. unsupervised classification task USED-FOR task. finetuning USED-FOR task. unsupervised classification task EVALUATE-FOR finetuning. unsupervised clustering USED-FOR intermediate task. labeled examples USED-FOR topical classification tasks. classification step USED-FOR topical classification tasks. classification step USED-FOR labeled examples. Material are labeled data, and data sets. Method is BERT. ",This paper proposes a fine-tuning BERT for the text classification task. The task is an unsupervised classification task with labeled data. The authors propose finetuning the task using the finetuned version of BERT. The main idea is to learn the classification step for the labeled examples in the topical classification tasks. The intermediate task is done by using unsupersupervised clustering. The experimental results show that the proposed BERT outperforms the state-of-the-art.,"This paper proposes a fine-tuning BERT for text classification task. The task is an unsupervised classification task, where the labeled data are generated from a set of data sets. The authors propose finetuning for the task, which is an extension of BERT. The finetuned task is a simple unsuper supervised classification task with a classification step to generate labeled examples for topical classification tasks. The intermediate task is done using unsupersupervised clustering. "
405,SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"fixed ( random shooting ) control agent USED-FOR generative models. mixture density nets COMPARE models. models COMPARE mixture density nets. they COMPARE probabilistic counterparts. probabilistic counterparts COMPARE they. deterministic models COMPARE probabilistic counterparts. probabilistic counterparts COMPARE deterministic models. heteroscedasticity USED-FOR regularizer. them USED-FOR control problem. sample complexity EVALUATE-FOR MBRL. framework USED-FOR MBRL. sample complexity EVALUATE-FOR framework. Acrobot EVALUATE-FOR MBRL. training schedule USED-FOR MBRL. OtherScientificTerm are multimodal posterior predictives, multimodality, and probabilistic posterior predictives. ","This paper proposes a fixed (random shooting) control agent for generative models with multimodal posterior predictives. The authors show that models trained with mixture density nets are more robust than probabilistic counterparts, and that they can be used to solve a control problem with heteroscedasticity. They also show that MBRL with the proposed framework can achieve better sample complexity than MBRL without a training schedule. ","This paper proposes a fixed (random shooting) control agent for generative models. The authors show that models trained with mixture density nets outperform their probabilistic counterparts. They also show that the multimodal posterior predictives are more robust to heteroscedasticity. They show that MBRL with a training schedule is more robust than MBRL on Acrobot. Finally, they show that they can solve the control problem with them."
414,SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"Affine Disentangled GAN ( ADIS - GAN ) HYPONYM-OF Generative Adversarial Network. affine regularizer USED-FOR inductive bias. affine transformation properties of images USED-FOR affine regularizer. transformation matrices PART-OF affine matrix. maximum likelihood estimation USED-FOR transformation parameters. horizontal and vertical zoom CONJUNCTION horizontal and vertical skew. horizontal and vertical skew CONJUNCTION horizontal and vertical zoom. horizontal and vertical skew CONJUNCTION horizontal and vertical translation. horizontal and vertical translation CONJUNCTION horizontal and vertical skew. rotation CONJUNCTION horizontal and vertical zoom. horizontal and vertical zoom CONJUNCTION rotation. rotation CONJUNCTION horizontal and vertical skew. horizontal and vertical skew CONJUNCTION rotation. disentangled representations COMPARE features. features COMPARE disentangled representations. ADIS - GAN USED-FOR features. approaches USED-FOR disentangled representations. horizontal and vertical translation HYPONYM-OF transformations. rotation HYPONYM-OF transformations. horizontal and vertical skew HYPONYM-OF transformations. horizontal and vertical zoom HYPONYM-OF transformations. ADIS - GAN USED-FOR features. MNIST, CelebA, and dSprites datasets EVALUATE-FOR ADIS - GAN. MNIST, CelebA, and dSprites datasets EVALUATE-FOR features. OtherScientificTerm is affine transformations. Method is InfoGAN. ","This paper proposes Affine Disentangled GAN (ADIS-GAN), a Generative Adversarial Network that uses the affine transformation properties of images as an affine regularizer to reduce inductive bias. The affine matrix is composed of transformation matrices, and the transformation parameters are computed by maximum likelihood estimation. The authors show that ADIS-GAN is able to disentangle features from disentangled representations, which is a significant improvement over previous approaches. The experiments on MNIST, CelebA, and dSprites datasets demonstrate the effectiveness of the proposed features. ","This paper proposes Affine Disentangled GAN (ADIS-GAN), an extension of Generative Adversarial Network (GAN) to disentangle the affine transformations of images. The affine regularizer is designed to reduce inductive bias by learning affine transformation properties of images, which are then used to train InfoGAN. The main idea is to use maximum likelihood estimation to estimate the transformation parameters of an affine matrix. Experiments on MNIST, CelebA, and dSprites datasets show that ADIS-GAN is able to learn disentangled representations that are more robust to adversarial attacks than other approaches. "
423,SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"contrastive learning methods COMPARE supervised learning counterparts. supervised learning counterparts COMPARE contrastive learning methods. contrastive learning methods USED-FOR Representation learning. data augmentations USED-FOR methods. augmentations USED-FOR instance discrimination - based contrastive learning. fully supervised upper bound USED-FOR unsupervised learning. distribution divergence USED-FOR retrieval of strongly augmented queries. augmentations USED-FOR contrastive loss. ResNet-50 architecture CONJUNCTION single - layer classifier fine - tuned. single - layer classifier fine - tuned CONJUNCTION ResNet-50 architecture. ImageNet EVALUATE-FOR ResNet-50 architecture. top-1 accuracy EVALUATE-FOR method. ImageNet EVALUATE-FOR method. fully supervised ResNet-50 USED-FOR top-1 accuracy. it COMPARE self - supervised and supervised methods. self - supervised and supervised methods COMPARE it. self - supervised and supervised methods USED-FOR transfer learning and object detection tasks. transfer learning and object detection tasks EVALUATE-FOR it. Metric is generalizability. OtherScientificTerm are distortions, image structures, representation bank, overoptimistic assumption, distorted visual structures, and distributions of weakly augmented counterparts. ","This paper studies the problem of representation learning with contrastive learning methods. Representation learning is an important problem in the context of unsupervised learning, where the goal is to learn representations that are robust to distortions in the input image. The authors propose a new contrastive loss based on data augmentations to improve the generalizability of the learned representations. The proposed method is based on a fully supervised upper bound on the unclassification error of the representation bank. The main contribution of the paper is to show that the proposed method improves the top-1 accuracy of the fully supervised ResNet-50 architecture and the single-layer classifier fine-tuned on ImageNet. ","This paper proposes a new contrastive learning methods for Representation learning. The main idea is to use data augmentations to improve the generalizability of the proposed methods. The augmentations are based on instance discrimination-based contrastive loss, which is based on a fully supervised upper bound for unsupervised learning with distribution divergence for retrieval of strongly augmented queries. The authors show that the proposed method outperforms the ResNet-50 architecture and the single-layer classifier fine-tuned on ImageNet with top-1 accuracy on both transfer learning and object detection tasks, and it outperforms self-supervised and supervised methods in terms of generalization. "
432,SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"magnetic resonance imagery ( MRI ) USED-FOR De - identification. de - identification methods USED-FOR task. MRI de - identification techniques USED-FOR privacy - sensitive facial features. removal - based techniques COMPARE deep learning framework. deep learning framework COMPARE removal - based techniques. segmentation CONJUNCTION age prediction. age prediction CONJUNCTION segmentation. deep learning framework USED-FOR medical analyses. medical analyses FEATURE-OF brain. age prediction HYPONYM-OF medical analyses. segmentation HYPONYM-OF medical analyses. Material are database, and patient ’s MRI scan. Generic are they, and them. OtherScientificTerm is 3D volume. ","This paper proposes a new method for de-identifying facial features from magnetic resonance imagery (MRI) for the task of De-identification. The proposed method is based on a combination of MRI de-imagenation techniques to identify privacy-sensitive facial features. The authors show that the proposed method can achieve better performance than removal-based techniques on a variety of medical analyses such as segmentation, age prediction, and medical analyses of the brain. They also show that they can be combined with a deep learning framework to improve the performance of their proposed method.","This paper proposes a new method for De-identification based on magnetic resonance imagery (MRI) for the task of privacy-sensitive facial features. The proposed method is based on de-identifying methods for this task. The authors show that they are able to identify the patient’s MRI scan from a database of 3D volume. They also show that the proposed deep learning framework outperforms removal-based techniques in medical analyses of the brain, including segmentation and age prediction."
441,SP:0ac3964bd2320341488476d60f57b75d2a79f92c,Graph neural networks USED-FOR modeling graph data. Graph neural networks USED-FOR node classification and link prediction tasks. representation USED-FOR graph. pooling function USED-FOR node representations. pooling function USED-FOR compact form. pooling function USED-FOR representation. task relevance CONJUNCTION structural dependencies. structural dependencies CONJUNCTION task relevance. hierarchical graph pooling methods USED-FOR representation. representation USED-FOR graphs. Weisfeiler - Lehman test FEATURE-OF graphs. Graph Multiset Transformer ( GMT ) HYPONYM-OF multi - head attention based global pooling layer. graph structure FEATURE-OF auxiliary information. auxiliary information FEATURE-OF multiset encoding problem. multiset encoding problem USED-FOR graph pooling problem. injectiveness CONJUNCTION permutation invariance. permutation invariance CONJUNCTION injectiveness. it COMPARE Weisfeiler - Lehman graph isomorphism test. Weisfeiler - Lehman graph isomorphism test COMPARE it. injectiveness FEATURE-OF GMT. permutation invariance FEATURE-OF GMT. node clustering approaches USED-FOR hierarchical graph pooling. methods USED-FOR hierarchical graph pooling. methods USED-FOR node clustering approaches. GMT COMPARE graph pooling methods. graph pooling methods COMPARE GMT. graph classification benchmarks EVALUATE-FOR graph pooling methods. memory and time efficiency EVALUATE-FOR graph pooling methods. graph classification benchmarks EVALUATE-FOR GMT. memory and time efficiency EVALUATE-FOR GMT. Material is graph data. OtherScientificTerm is node features. Generic is they. ,"This paper proposes a multi-head attention based global pooling layer, called Graph Multiset Transformer (GMT), which is based on the Weisfeiler-Lehman test for graph representations. The authors show that the pooling function can be used to learn the compact form of a graph by pooling the node representations. They also show that they can be combined with hierarchical graph pooling methods to learn a representation for graphs. They show that GMT improves the injectiveness, permutation invariance, and memory and time efficiency on several graph classification benchmarks. The paper also shows that the proposed methods are competitive with existing node clustering approaches in terms of the performance of hierarchical node pooling.","This paper proposes a multi-head attention based global pooling layer, called Graph Multiset Transformer (GMT), for modeling graph data. Graph neural networks are used for node classification and link prediction tasks. The authors propose a new representation for graphs, which is a compact form of the Weisfeiler-Lehman test of graphs. The representation is based on hierarchical graph pooling methods, where the pooling function is used to learn node representations of the graph. The paper also proposes a multiset encoding problem with auxiliary information based on the graph structure. The auxiliary information consists of the task relevance and structural dependencies of the node features. The proposed GMT is evaluated on a number of graph classification benchmarks and shows better memory and time efficiency than other node clustering approaches. It also shows better injectiveness and permutation invariance compared to other graph pooled methods."
450,SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"GNNs USED-FOR prediction task. long - range interaction USED-FOR prediction task. tuning CONJUNCTION weights. weights CONJUNCTION tuning. GCN CONJUNCTION GIN. GIN CONJUNCTION GCN. over - squashing FEATURE-OF GNNs. GNNs COMPARE GAT. GAT COMPARE GNNs. GAT CONJUNCTION GGNN. GGNN CONJUNCTION GAT. GNNs USED-FOR over - squashing. GNNs COMPARE GGNN. GGNN COMPARE GNNs. bottleneck USED-FOR GNNs. GIN HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. Method are graph neural network ( GNN ), and GNN models of long - range problems. OtherScientificTerm are graph, exponentially growing information, fixed - size vectors, long - range signals, and incoming edges. ","This paper studies graph neural network (GNN) models of long-range problems, where the graph is a set of nodes with exponentially growing information, and the goal is to predict the next node in the graph. The authors propose to use GNNs for this prediction task, which is based on the observation that the prediction task can be solved by a GNN with a long range interaction. The main idea is to use a bottleneck to train the GNN, which can be used as a proxy for the number of nodes in a graph.  The authors show that the bottleneck can be applied to a variety of GNN models, including GAT, GGNN, GIN, and GCN. ","This paper proposes a graph neural network (GNN) model for long-range problems. The authors show that GNNs with exponentially growing information are more robust to over-squashing than GAT, GCN, GIN, and GGNN. The main contribution of the paper is to propose a new GNN model for the long range problem. The paper also proposes a new bottleneck for the GNN models of long range problems. "
459,SP:90d8fa381446923902e42b259392e5e975e6caa1,"cross - domain generalizable classifiers USED-FOR methods. methods USED-FOR domain - agnostic representations. annotated data USED-FOR classifier. embedding space USED-FOR domain - agnostic. data distributions USED-FOR domain - agnostic. Task are Sentiment analysis, and marketing strategies. Method are cross - domain sentiment analysis methods, and domain adaptation method. OtherScientificTerm are data annotation, and prototypical distribution. Generic is method. ",This paper studies the problem of cross-domain generalizable classifiers in the context of Sentiment analysis. The authors propose two methods for learning domain-agnostic representations from annotated data. The first is a domain adaptation method that learns a set of data distributions for each domain. The second is a method that uses a prototypical distribution to learn the representations of the domains. The proposed method is evaluated on a variety of datasets.,"This paper proposes two methods for learning domain-agnostic representations from cross-domain generalizable classifiers. Sentiment analysis is a popular topic in the literature. The authors propose two methods to learn domain-agnostic representations from annotated data. The first method is a domain adaptation method, and the second method is an extension of the cross-domains sentiment analysis methods. The key idea is to use the data annotation as input to the classifier and the prototypical distribution of the embedding space to learn the domain-gnostic representations. The paper also proposes two marketing strategies to improve the performance of the proposed method. The proposed method is evaluated on a set of datasets with different data distributions."
468,SP:893fd7440b82f5da0d4c0944928810322eaee2f0,Gender - bias stereotypes PART-OF natural language processing. genderbias FEATURE-OF natural language understanding. evaluation of genderbias PART-OF natural language understanding. inference USED-FOR natural language understanding. inference USED-FOR evaluation of genderbias. gender neutral premise COMPARE gender - specific hypothesis. gender - specific hypothesis COMPARE gender neutral premise. NLI models USED-FOR gender stereotypes. challenge task USED-FOR NLI models. occupations USED-FOR NLI models. BERT CONJUNCTION RoBERTa. RoBERTa CONJUNCTION BERT. RoBERTa CONJUNCTION BART. BART CONJUNCTION RoBERTa. models USED-FOR genderinduced prediction errors. BERT CONJUNCTION BART. BART CONJUNCTION BERT. MNLI and SNLI data - sets USED-FOR models. BART HYPONYM-OF models. BERT HYPONYM-OF models. RoBERTa HYPONYM-OF models. Generic is evaluation methodology. Method is debiasing techniques. Material is gender - balanced dataset. ,"This paper proposes a new evaluation methodology for evaluating gender-biased stereotypes in natural language processing. The evaluation of genderbias is an important part of natural language understanding, and the authors propose to use inference for the evaluation of the performance of the NLI models on the task of evaluating gender stereotypes. The authors propose a challenge task to evaluate the gender bias of NLI model on a set of occupations, where the goal is to find a gender-balanced dataset. The debiasing techniques are evaluated on MNLI and SNLI data-sets. The results show that the models are able to identify genderinduced prediction errors in BERT, RoBERTa, BART, and BART.","This paper proposes a new evaluation methodology for evaluating gender-bias stereotypes in natural language processing. The evaluation of genderbias is an important part of natural language understanding, and the authors propose to use inference for the evaluation of the gender bias. The authors propose a gender neutral premise to evaluate the gender-specific hypothesis, which is based on debiasing techniques. The NLI models trained on MNLI and SNLI data-sets are evaluated on a challenge task, where the authors show that the gender stereotypes of the occupations of the NLI model can be used to improve the performance of the models in terms of genderinduced prediction errors. Experiments are conducted on two models, BERT and RoBERTa, on a gender-balanced dataset."
477,SP:a32ab755bd249c393b70938036ce8e810c0c439f,"variational intrinsic control ( VIC ) HYPONYM-OF unsupervised reinforcement learning method. other HYPONYM-OF VIC algorithms. one HYPONYM-OF VIC algorithms. intrinsic reward USED-FOR latter. transitional probability model CONJUNCTION Gaussian mixture model. Gaussian mixture model CONJUNCTION transitional probability model. transitional probability model USED-FOR methods. Gaussian mixture model USED-FOR methods. OtherScientificTerm are intrinsic options, and stochastic environments. ","This paper proposes a new unsupervised reinforcement learning method called Variational intrinsic control (VIC) that combines two existing VIC algorithms, one based on the transitional probability model and the other based on a Gaussian mixture model. The latter is motivated by the fact that the intrinsic reward in the former is not the same as the intrinsic options in the latter. The authors show that the two methods can be combined using the transitional probabilities of the two, and that they can be used in stochastic environments.","This paper proposes a variant of variational intrinsic control (VIC) which is an unsupervised reinforcement learning method. The authors propose two variants of VIC algorithms, one based on a transitional probability model and the other based on Gaussian mixture model. The latter is based on the intrinsic reward, where the intrinsic options are chosen from a set of intrinsic options, and the latter is trained in stochastic environments. "
486,SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,Deep neural networks USED-FOR image classification. low data regime FEATURE-OF sample efficiency. ensemble of relatively small deep networks USED-FOR image classification problems. neural ensembling USED-FOR small data domains. technique COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE technique. deep ensembling COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE deep ensembling. deep ensembling HYPONYM-OF technique. Generic is they. Material is small datasets. Method is ensemble configurations. OtherScientificTerm is losses. ,"This paper studies the problem of image classification with ensemble of relatively small deep networks in the low data regime. The authors propose a technique called deep ensembling, which can be applied to small data domains. They show that this technique can achieve better sample efficiency than state-of-the-art approaches, and they also show that they can be used to learn ensemble configurations. ","This paper proposes an ensemble of relatively small deep networks for image classification problems. The technique is compared to state-of-the-art approaches in terms of sample efficiency in the low data regime. The proposed technique, deep ensembling, can be applied to small data domains. The authors show that they are able to achieve better sample efficiency on small datasets. They also show that the ensemble configurations are more robust to losses."
495,SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,computational power and storage requirements CONJUNCTION processing speed. processing speed CONJUNCTION computational power and storage requirements. DNN - based applications USED-FOR InternetOf - Things ( IoT ) devices. them USED-FOR DNN - based applications. quantized networks COMPARE Binary Neural Networks ( BNNs ). Binary Neural Networks ( BNNs ) COMPARE quantized networks. speed - up EVALUATE-FOR Binary Neural Networks ( BNNs ). fixed and limited compression factor FEATURE-OF they. positive 0/1 binary weights COMPARE -1/+1 weights. -1/+1 weights COMPARE positive 0/1 binary weights. Sparse Binary Neural Networks HYPONYM-OF model and training scheme. -1/+1 weights COMPARE binary networks. binary networks COMPARE -1/+1 weights. sparsity FEATURE-OF BNNs. positive 0/1 binary weights USED-FOR sparsity. compression factor EVALUATE-FOR method. MNIST and CIFAR-10 datasets USED-FOR linear and convolutional networks. linear and convolutional networks EVALUATE-FOR method. it USED-FOR DNNs. compression rates CONJUNCTION generalization. generalization CONJUNCTION compression rates. generalization EVALUATE-FOR SBNNs. compression rates EVALUATE-FOR SBNNs. Method is Quantized neural networks. Metric is accuracy. Material is limited resources. ,This paper proposes a new model and training scheme called Sparse Binary Neural Networks (SBNN) for Internet of Things (IoT) devices. The main idea is to use quantized neural networks instead of binary neural networks (BNNs) in order to reduce the computational power and storage requirements and improve the processing speed of DNN-based applications for InternetOf-Things (IOT). The authors show that they have a fixed and limited compression factor of $\mathcal{O}(\sqrt{T})$ and that they can achieve speed-up of up to 1.5x faster than binary networks with positive 0/1 binary weights compared to -1/+1 weights. The authors also show that SBNNs can achieve better compression rates and generalization than BNNs with sparsity. The proposed method is evaluated on MNIST and CIFAR-10 datasets for linear and convolutional networks and shows that it achieves better performance than DNNs.,"This paper proposes a new model and training scheme, called Sparse Binary Neural Networks (SBNN), to improve the speed-up of Quantized neural networks (QNNs) in the context of DNN-based applications for InternetOf-Things (IoT) devices. The authors argue that quantized networks are more robust to sparsity and faster to the computational power and storage requirements compared to BNNs, and that they have a fixed and limited compression factor. They also show that positive 0/1 binary weights outperform-1/+1 weights and binary networks in terms of sparsity. The proposed method is evaluated on MNIST and CIFAR-10 datasets for linear and convolutional networks, and it is shown that it improves the compression rates and generalization of SBNNs. "
504,SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"OOD data USED-FOR model calibration. outlier exposure USED-FOR model probabilities. outlier exposure USED-FOR method. estimates of class membership probabilities USED-FOR model predictions. baseline method USED-FOR predictive uncertainty. softmax probabilities USED-FOR model. softmax probabilities USED-FOR baseline method. softmax probabilities USED-FOR predictive uncertainty. Stochastic Variational Bayesian Inference ( SVBI ) USED-FOR deep learning. model ensembles CONJUNCTION Stochastic Variational Bayesian Inference ( SVBI ). Stochastic Variational Bayesian Inference ( SVBI ) CONJUNCTION model ensembles. Stochastic Variational Bayesian Inference ( SVBI ) HYPONYM-OF approaches. temperature scaling HYPONYM-OF approaches. model ensembles HYPONYM-OF approaches. predicted error rates CONJUNCTION actual error rates. actual error rates CONJUNCTION predicted error rates. calibration error FEATURE-OF methods. Metric is accuracy. Task are Predictive uncertainty, and PREDICTIVE UNCERTAINTY. Generic are models, and measures. Method are post hoc calibration method, machine learning model, and Uncertainty estimates. Material is corrupted data. OtherScientificTerm are class membership probabilities, model outputs, pmax, and Brier score. ",This paper proposes a post hoc calibration method for model calibration on corrupted data. The method uses outlier exposure to estimate the model probabilities based on the class membership probabilities of the model outputs. The authors propose a baseline method based on softmax probabilities for predicting the predictive uncertainty of a model using softmax probability. The proposed method is based on Stochastic Variational Bayesian Inference (SVBI) for deep learning and model ensembles (e.g. temperature scaling). The authors show that the calibration error of the proposed methods is lower than the predicted error rates and actual error rates of the models. They also show that their calibration error is close to the true accuracy of the machine learning model. ,"This paper proposes a post hoc calibration method for model calibration on corrupted data. The method is based on outlier exposure to model probabilities. The authors propose a baseline method for predictive uncertainty based on softmax probabilities of the class membership probabilities for model predictions. The proposed approaches are Stochastic Variational Bayesian Inference (SVBI) for deep learning, model ensembles, and temperature scaling. Predictive uncertainty is defined as the difference between the predicted error rates and the actual error rates of the model outputs, and the accuracy is measured by the Brier score of the models. Uncertainty estimates are used to train the machine learning model. The results show that the proposed methods achieve better calibration error than other methods, and PREDICTIVE UNCERTAINTY."
513,SP:ea503f67e38fce7dee9cc4996b55b8959911f030,Graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION Graph neural networks. graph kernels USED-FOR machine learning problems. Graph neural networks USED-FOR machine learning problems. graphs USED-FOR machine learning problems. graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION graph neural networks. approaches USED-FOR graph properties. approaches USED-FOR non - isomorphic graphs. graph representations USED-FOR similarity / distance of graphs. graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION graph neural networks. expressive power EVALUATE-FOR graph kernels. expressive power FEATURE-OF graph neural networks. algorithms COMPARE those. those COMPARE algorithms. graph representations and similarities COMPARE those. those COMPARE graph representations and similarities. algorithms USED-FOR graph representations and similarities. models CONJUNCTION kernels. kernels CONJUNCTION models. node attributes USED-FOR kernels. node attributes USED-FOR models. graph kernels COMPARE graph neural networks. graph neural networks COMPARE graph kernels. ,This paper studies the expressive power of graph neural networks and graph kernels in the context of machine learning problems with graphs. The authors propose two approaches to learn the graph properties of non-isomorphic graphs by learning the graph representations and the similarity/distance of graphs. They show that these two algorithms achieve better expressive power than those based on graph kernels and graph models. They also show that models trained with these two types of graph attributes are more expressive than those trained with graph kernels.,This paper presents a new approach to learning non-isomorphic graphs from graph neural networks and graph kernels for machine learning problems. The key idea is to learn the graph properties of the graph representations and the similarity/distance of graphs. The authors show that the graph kernels have higher expressive power than the models and node attributes for learning the graphs. They also show that those algorithms are more expressive than those that learn the graphs and graph representations. 
522,SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"inpainting USED-FOR warping artifacts. data augmentation techniques USED-FOR regularizing non - warp - based image generation. them USED-FOR image animation. difficulty of inpainting FEATURE-OF warped image. CutMix HYPONYM-OF data augmentation techniques. PriorityCut USED-FOR image animation. augmentation approach USED-FOR image animation. PriorityCut HYPONYM-OF augmentation approach. low - level similarity CONJUNCTION keypoint distance. keypoint distance CONJUNCTION low - level similarity. keypoint distance CONJUNCTION feature embedding distance. feature embedding distance CONJUNCTION keypoint distance. pixel - wise difference CONJUNCTION low - level similarity. low - level similarity CONJUNCTION pixel - wise difference. PriorityCut COMPARE vanilla CutMix. vanilla CutMix COMPARE PriorityCut. PriorityCut COMPARE image animation models. image animation models COMPARE PriorityCut. PriorityCut USED-FOR identity. vanilla CutMix COMPARE image animation models. image animation models COMPARE vanilla CutMix. low - level similarity EVALUATE-FOR image animation models. pixel - wise difference EVALUATE-FOR image animation models. inpainting USED-FOR warping artifacts. PriorityCut USED-FOR regularize discriminator predictions. occlusion information USED-FOR regularize discriminator predictions. regularize discriminator predictions USED-FOR inpainting. occlusion information USED-FOR image animation. occlusion information USED-FOR PriorityCut. Method are Image animation, Self - supervised image animation approaches, self - supervised image animation approaches, and Warp - based image animation. OtherScientificTerm are motion of a driving video, pose references, motion of the driving video, pose differences, guidance, inpainted regions, motion of the driving image, smooth transitions, and mixture of context. Task is learning. ","This paper proposes a new data augmentation techniques for regularizing non-warp-based image generation, called CutMix. The main idea is to use them to improve the performance of image animation. The proposed augmentation approach, called PriorityCut, is based on the idea of using occlusion information to regularize discriminator predictions for inpainting the warping artifacts in a warped image. The key idea of the proposed method is to learn the identity of the inpainted regions of a driving video by learning the distance between the keypoint distance and the pixel-wise difference between the image and the target image, and then using these two keypoints to train a self-supervised image animation approaches.    The main contribution of the paper is to show that the proposed methods are able to achieve better performance than vanilla CutMix and other image animation models in terms of low-level similarity and feature embedding distance. ","This paper presents a new approach to regularize non-warp-based image generation using data augmentation techniques. CutMix and PriorityCut are two data augmentations techniques for regularizing non-warping-based images generation. The main idea is to use them for image animation. The authors show that their augmentation approach can improve the quality of image animation by reducing the pixel-wise difference, low-level similarity, keypoint distance, and feature embedding distance. They also show that using the occlusion information of PriorityCut improves the identity of an image, and the inpainting of a warped image.  "
531,SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"approaches USED-FOR disentangled representations. data generation process USED-FOR independent latent variables. independent causal mechanisms ( ICM ) COMPARE disentangled representations. disentangled representations COMPARE independent causal mechanisms ( ICM ). coarse granularity FEATURE-OF data generation processes ( mechanisms ). observational data USED-FOR groundtruth mechanisms. unconventional mixture prior USED-FOR self - supervised generative model. self - supervised generative model USED-FOR mechanisms. mechanisms PART-OF self - supervised scenario. intervention CONJUNCTION covariant shift. covariant shift CONJUNCTION intervention. covariant shift CONJUNCTION noise. noise CONJUNCTION covariant shift. downstream tasks EVALUATE-FOR approach. approach COMPARE disentangled representations. disentangled representations COMPARE approach. approach USED-FOR intervention. covariant shift USED-FOR approach. noise EVALUATE-FOR approach. downstream tasks EVALUATE-FOR disentangled representations. downstream tasks EVALUATE-FOR approach. Generic are model, and methods. OtherScientificTerm is disentanglement. ","This paper studies the problem of learning disentangled representations from observational data. The authors propose a self-supervised generative model that learns to disentangle the independent latent variables in a data generation process. The proposed model is based on an unconventional mixture prior, which allows to learn the groundtruth mechanisms in observational data without the need for disentanglement. The paper shows that the proposed approach can achieve better performance on downstream tasks than the state-of-the-art in terms of intervention, covariant shift, and noise.",This paper proposes two approaches to disentangle representations from independent causal mechanisms (ICM) and disentangled representations. The authors propose a self-supervised generative model with an unconventional mixture prior to learn the mechanisms of independent latent variables in the data generation process. The main contribution of the paper is to show that the coarse granularity of data generation processes ( mechanisms) is correlated with the groundtruth mechanisms in observational data. The proposed approach is evaluated on two downstream tasks: intervention and covariant shift. The experimental results show the effectiveness of the proposed approach on both intervention and noise. 
540,SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"2D image USED-FOR molecular graph structure ( W ). graph aligning approach USED-FOR rich or detailed labels. 2D images USED-FOR chemical compound graphs. domain adaptation COMPARE pretrained model. pretrained model COMPARE domain adaptation. domain adaptation USED-FOR model. Maybridge data set EVALUATE-FOR self - labeling approach. Task are machine learning, and predicting chemical compound graphs. Method are mediating representation V, and machine learning model. OtherScientificTerm are f, normal labels W, fully mediating layer, and mediating layer. ","This paper proposes a graph aligning approach for rich or detailed labels on a 2D image of molecular graph structure (W). The idea is to use a mediating representation V, where f is the normal labels W, and the f is a fully mediating layer. The model is trained by domain adaptation on the Maybridge data set, and is shown to perform better than a pretrained model. The paper also shows that the proposed self-labeling approach can be applied to predicting chemical compound graphs on 2D images.","This paper proposes a graph aligning approach to generate rich or detailed labels for molecular graph structure (W) from a 2D image. The idea is to use a mediating representation V, where f is the normal labels W, and f is a fully mediating layer of the machine learning model. The authors show that the proposed self-labeling approach on the Maybridge data set outperforms the domain adaptation of the pretrained model. "
549,SP:ad906dd9a176cffd283593321ff6b9ad19595528,domain knowledge based deep learning framework USED-FOR chiller plants energy optimization problems. image classification CONJUNCTION NLP. NLP CONJUNCTION image classification. deep network USED-FOR realworld physical systems. NLP HYPONYM-OF deep learning. image classification HYPONYM-OF deep learning. methods USED-FOR complex systems. methods USED-FOR linear model. linear model USED-FOR complex systems. deep network USED-FOR nonlinear model. domain knowledge USED-FOR deep network. domain knowledge USED-FOR nonlinear model. redundancy function space FEATURE-OF nonlinear model. domain knowledge USED-FOR small sample size problem. energy consumption estimation FEATURE-OF chillers. input - output monotonic problem USED-FOR energy consumption estimation. monotonic constraints FEATURE-OF Neural Network. framework COMPARE ones. ones COMPARE framework. framework USED-FOR energy optimization. method COMPARE ones. ones COMPARE method. data center FEATURE-OF cooling system. cooling system EVALUATE-FOR method. ,This paper proposes a domain knowledge based deep learning framework for chiller plants energy optimization problems. The authors propose to use domain knowledge from image classification and NLP to train a deep network to model realworld physical systems. The proposed framework is able to learn complex systems with a linear model and a nonlinear model in the redundancy function space. The main contribution of the paper is the use of domain knowledge for the small sample size problem in the input-output monotonic problem of energy consumption estimation for chillers. The method is evaluated on a cooling system with a data center and shows that the proposed framework can achieve better performance than existing ones for energy optimization.,"This paper proposes a domain knowledge based deep learning framework for chiller plants energy optimization problems. The authors propose to use domain knowledge for realworld physical systems such as image classification, NLP, and deep learning. The main idea is to use a deep network with domain knowledge to train a nonlinear model in the redundancy function space. The proposed framework is compared to existing methods for training a linear model for complex systems, and the authors show that the proposed framework outperforms the ones for energy optimization in terms of energy consumption estimation in the case of chillers with input-output monotonic problem. The method is evaluated on a cooling system with a data center and a small sample size problem. "
558,SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,retail forecasting CONJUNCTION urban traffic forecasting. urban traffic forecasting CONJUNCTION retail forecasting. weather forecasts CONJUNCTION retail forecasting. retail forecasting CONJUNCTION weather forecasts. spatio - temporal predictions USED-FOR real - world applications. spatio - temporal predictions USED-FOR large - scale systems. weather forecasts HYPONYM-OF real - world applications. urban traffic forecasting HYPONYM-OF real - world applications. retail forecasting HYPONYM-OF real - world applications. methods USED-FOR predicting variables. interpretability EVALUATE-FOR forecasting models. methods USED-FOR forecasting models. collaborative causal spatio - temporal fusion transformer USED-FOR collaborative causal effects of predictors. collaborative causal effects of predictors USED-FOR forecasting targets. CausalTrans HYPONYM-OF collaborative causal spatio - temporal fusion transformer. causal attention USED-FOR causal inference. nodes PART-OF graph. Taylor ’s expansion CONJUNCTION softmax. softmax CONJUNCTION Taylor ’s expansion. time complexity EVALUATE-FOR multi - head attention. softmax USED-FOR multi - head attention. Taylor ’s expansion USED-FOR multi - head attention. time efficiency EVALUATE-FOR CausalTrans. model components USED-FOR CausalTrans. time efficiency EVALUATE-FOR model components. error reduction EVALUATE-FOR baseline methods. CausalTrans framework COMPARE baseline methods. baseline methods COMPARE CausalTrans framework. error reduction EVALUATE-FOR CausalTrans framework. Material is ride - sharing platforms. Method is spatial graph fusion mechanism. ,This paper proposes a new framework for predicting the causal effects of a graph. The framework is based on the collaborative causal spatio-temporal fusion transformer (CausalTrans). The authors propose a new spatial graph fusion mechanism that is able to predict the causal effect of each node in the graph. They show that CausalTrans improves the time efficiency of the model components in terms of error reduction compared to the baseline methods. The authors also show that the multi-head attention with Taylor’s expansion and softmax can reduce the time complexity of the models. ,"This paper proposes a novel spatial graph fusion mechanism, called CausalTrans. The authors propose a collaborative causal spatio-temporal fusion transformer to learn the collaborative causal effects of predictors for different forecasting targets. The proposed methods can be applied to predicting variables in large-scale systems, including weather forecasts, retail forecasting, and urban traffic forecasting. Experiments show that the proposed methods improve the interpretability of forecasting models and improve the time efficiency of the model components compared to baseline methods. The paper also shows that multi-head attention with Taylor’s expansion and softmax can reduce the time complexity of the causal inference. "
567,SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"unsupervised framework USED-FOR problem. coupled mixture VAE ( cpl - mixVAE ) HYPONYM-OF unsupervised framework. interacting autoencoding agents USED-FOR unsupervised framework. variational inference problem USED-FOR it. categorical assignments EVALUATE-FOR approach. MNIST and dSprites EVALUATE-FOR approach. approach USED-FOR type - specific, activity - regulated genes. type - specific, activity - regulated genes PART-OF single - cell gene expression dataset. cortical neuron types FEATURE-OF single - cell gene expression dataset. single - cell gene expression dataset EVALUATE-FOR approach. OtherScientificTerm are mixture of discrete and continuous factors of variability, and continuous factors. Method are mixture representations, and multi - agent framework. ","This paper proposes a novel unsupervised framework called coupled mixture VAE (cpl-mixVAE) to solve the problem of learning a mixture of discrete and continuous factors of variability. The authors propose to use interacting autoencoding agents to learn the multi-agent framework, and then use it to solve a variational inference problem. The proposed approach is evaluated on a single-cell gene expression dataset with cortical neuron types and on MNIST and dSprites. The results show that the proposed approach can learn type-specific, activity-regulated genes.","This paper proposes a novel unsupervised framework for the problem of learning a mixture of discrete and continuous factors of variability. The proposed framework is called coupled mixture VAE (cpl-mixVAE) and is based on interacting autoencoding agents. The main idea is to learn mixture representations from a multi-agent framework. The approach is evaluated on a single-cell gene expression dataset with multiple cortical neuron types. It is shown that it is able to solve the variational inference problem with categorical assignments. Experiments on MNIST and dSprites show that the proposed approach can learn type-specific, activity-regulated genes."
576,SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"Group equivariant convolutional networks ( GCNNs ) USED-FOR convolutional networks. symmetry priors FEATURE-OF convolutional networks. convolutions USED-FOR models. equivariance constraint FEATURE-OF kernels. G - steerable kernels USED-FOR convolutions. G HYPONYM-OF compact group. constraints FEATURE-OF steerable kernels. constraints CONJUNCTION spherical tensor operators. spherical tensor operators CONJUNCTION constraints. steerable kernels CONJUNCTION spherical tensor operators. spherical tensor operators CONJUNCTION steerable kernels. quantum mechanics USED-FOR spherical tensor operators. generalized reduced matrix elements CONJUNCTION ClebschGordan coefficients. ClebschGordan coefficients CONJUNCTION generalized reduced matrix elements. ClebschGordan coefficients CONJUNCTION harmonic basis functions. harmonic basis functions CONJUNCTION ClebschGordan coefficients. Wigner - Eckart theorem USED-FOR spherical tensor operators. homogeneous spaces FEATURE-OF harmonic basis functions. generalized reduced matrix elements USED-FOR steerable kernel spaces. Method is GCNNs. OtherScientificTerm are G - steerability constraint, and Gsteerable kernel spaces. Generic is it. ","This paper studies group equivariant convolutional networks (GCNNs) with symmetry priors. The authors show that convolutions with G-steerable kernels can be used as convolutions for models with symmetric priors, and that the equivariance constraint on kernels is equivalent to the G-stakeability constraint on convolutions. They also show that the compact group G HYPONYM of a compact group can be approximated by a G-Steerable kernel spaces. Finally, the authors prove the Wigner-Eckart theorem for spherical tensor operators based on quantum mechanics, and show that these constraints can be applied to steerable kernels as well as other constraints such as generalized reduced matrix elements and ClebschGordan coefficients. ","This paper studies the group equivariant convolutional networks (GCNNs) for learning the symmetry priors of convolutionally networks. The authors show that the equivariance constraint of the kernels of G-steerable kernels can be applied to the convolutions of these models. They show that it is equivalent to a compact group of G. They also show that G-standerable kernels are equivalent to convolutions with quantum mechanics. Finally, the authors prove the Wigner-Eckart theorem for spherical tensor operators and steerable kernels.  The authors also show the equivalence of generalized reduced matrix elements and ClebschGordan coefficients for steerable kernel spaces in homogeneous spaces. "
585,SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"it USED-FOR accuracy disparities. average accuracies EVALUATE-FOR selective classification. accuracy EVALUATE-FOR selective classification. selective classification USED-FOR full - coverage accuracy disparities. models EVALUATE-FOR selective classification. full - coverage accuracies EVALUATE-FOR distributionally - robust models. Method is Selective classification. OtherScientificTerm are abstentions, spurious correlations, margin distribution, symmetric margin distributions, and left - log - concavity. Material is vision and NLP datasets. Metric is accuracies. Generic is distribution. ",This paper studies the problem of selective classification in the presence of abstentions and spurious correlations. It shows that the average accuracies for selective classification with respect to the margin distribution of the training data are highly correlated with the true margin distribution. The authors then propose a new metric for measuring the accuracies of the selective classification. The new metric is based on the left-log-concave concavity of the margin distributions. They show that the new metric can be used to measure the full-coverage accuracy disparities between the two models. They also show that distributionally-robust models have better full-cover coverage accuracies. ,"This paper proposes a new metric for evaluating the accuracy of a classifier. The metric is based on the margin distribution of the classifier, which is defined as the sum of the average accuracies of all classes. The authors show that this metric can be used to measure the accuracy disparities between classes. They also show that it can measure the full-coverage accuracy disparities. The paper also shows that it is able to detect abstentions and spurious correlations. "
594,SP:f1d57ee27e901daf7e4e2b84139019e945818911,multi - layer network analysis CONJUNCTION temporal document classification. temporal document classification CONJUNCTION multi - layer network analysis. temporal document classification CONJUNCTION video data analysis. video data analysis CONJUNCTION temporal document classification. topic modeling USED-FOR applications. complex multi - modal structure FEATURE-OF applications. complex multi - modal structure FEATURE-OF topic modeling. complex multi - modal structure FEATURE-OF large - scale data. latent hierarchical structure FEATURE-OF multi - modal data. large - scale data USED-FOR topic modeling. video data analysis HYPONYM-OF applications. multi - layer network analysis HYPONYM-OF applications. temporal document classification HYPONYM-OF applications. Neural NCPD USED-FOR hierarchical topic modeling. Neural NCPD HYPONYM-OF training method. multi - modal tensor data USED-FOR hierarchical topic modeling. neural network architecture CONJUNCTION backpropagation. backpropagation CONJUNCTION neural network architecture. backpropagation USED-FOR error propagation. hierarchical NCPD USED-FOR error propagation. neural network architecture USED-FOR Neural NCPD. backpropagation USED-FOR Neural NCPD. ,"This paper proposes a new training method, Neural NCPD, for topic modeling on large-scale data with complex multi-modal structure such as video data analysis, temporal document classification, and multi-layer network analysis. The authors propose a new neural network architecture and backpropagation for neural NCPD for hierarchical topic modeling. The latent hierarchical structure of multi- modal data can be used to learn the topic model. ","This paper proposes a new training method, Neural NCPD, for topic modeling on large-scale data with complex multi-modal structure. The main idea is to learn a latent hierarchical structure of multi- modal data, which can be used for applications such as multi-layer network analysis, temporal document classification, and video data analysis. The paper also proposes a neural network architecture and backpropagation to improve the error propagation in hierarchical NCPD. The authors conduct extensive experiments on hierarchical topic modeling using multi-Modal tensor data. "
603,SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"node classification CONJUNCTION image segmentation. image segmentation CONJUNCTION node classification. graph CONJUNCTION image. image CONJUNCTION graph. image segmentation CONJUNCTION named - entity recognition. named - entity recognition CONJUNCTION image segmentation. classifier USED-FOR tasks. named - entity recognition HYPONYM-OF tasks. node classification HYPONYM-OF tasks. image segmentation HYPONYM-OF tasks. adversarial robustness certificates USED-FOR tasks. locality property USED-FOR collective certificate. single - node certificates PART-OF collective certificate. locality property CONJUNCTION perturbations. perturbations CONJUNCTION locality property. collective certificate USED-FOR node classification. Citeseer dataset EVALUATE-FOR collective certificate. OtherScientificTerm are perturbed inputs, and perturbation. Method are collective robustness certificate, and Graph Neural Networks. ","This paper proposes a new adversarial robustness certificate for graph neural networks. The proposed certificate is based on the notion of locality property, which is defined as the distance between the nodes in the graph and the nodes of the node classifier. The authors show that the proposed certificate can be applied to a variety of tasks such as node classification, image segmentation, and named-entity recognition. They also show that their proposed collective certificate is more robust than single-node certificates. ",This paper proposes a new adversarial robustness certificate for graph neural networks. The proposed certificate is based on the locality property and the perturbations. The authors show that the proposed certificate outperforms single-node certificates on the Citeseer dataset. 
612,SP:cc93dd2f68e415e2457166e78627865dc1b44697,"generative models USED-FOR complex real - world data. Generative Adversarial Networks ( GANs ) USED-FOR complex real - world data. Generative Adversarial Networks ( GANs ) HYPONYM-OF generative models. generative and discriminative neural networks USED-FOR Learning high - dimensional probability distributions. non - convergence problem CONJUNCTION mode collapse. mode collapse CONJUNCTION non - convergence problem. mode collapse CONJUNCTION gradient explosion or vanishing. gradient explosion or vanishing CONJUNCTION mode collapse. non - convergence problem FEATURE-OF GANs. Least Squares GAN ( LSGANs ) CONJUNCTION Wasserstein GANs ( WGAN ). Wasserstein GANs ( WGAN ) CONJUNCTION Least Squares GAN ( LSGANs ). Wasserstein GANs ( WGAN ) HYPONYM-OF GANs. Least Squares GAN ( LSGANs ) HYPONYM-OF GANs. LSGANs USED-FOR mode collapse. quantile regression USED-FOR 1 - Wasserstein distance. modification of loss functions USED-FOR GANs. approach USED-FOR modification of loss functions. approach USED-FOR GANs. quantile regression USED-FOR Quantile Regression GAN ( QRGAN ). discriminator CONJUNCTION gradients. gradients CONJUNCTION discriminator. QRGAN USED-FOR mode collapse problem. robustness EVALUATE-FOR QRGAN. QRGAN COMPARE GANs. GANs COMPARE QRGAN. generation performance assessment EVALUATE-FOR GANs. evaluation EVALUATE-FOR Frechet Inception Distance ( FID ). Frechet Inception Distance ( FID ) USED-FOR generation performance assessment. Frechet Inception Distance ( FID ) EVALUATE-FOR GANs. Frechet Inception Distance ( FID ) EVALUATE-FOR QRGAN. generation performance assessment EVALUATE-FOR QRGAN. evaluation EVALUATE-FOR QRGAN. Method are modification methodology of loss functions, WGANs, and Wasserstein distance approximation. OtherScientificTerm are local minima, inefficient computation, and real and generated data distribution. ",This paper studies the problem of learning high-dimensional probability distributions with generative and discriminative neural networks. The authors propose a modification methodology of loss functions to improve the performance of GANs. The proposed method is based on the Wasserstein distance approximation. The main contribution of the paper is to show that the proposed method can achieve better performance than the state-of-the-art in terms of robustness. ,"This paper proposes a modification methodology of loss functions to improve the robustness of generative models for complex real-world data. Generative Adversarial Networks (GANs) and Least Squares GAN (LSGANs) are two generative model architectures for learning high-dimensional probability distributions. In particular, the authors propose to modify the modification of the loss functions of GANs and LSGANs to avoid the mode collapse problem and the non-convergence problem of the GAN with respect to the local minima. The WGANs are used to approximate the Wasserstein distance approximation, and the WGAN is used to compute the 1-Wasserstein-distance between the real and generated data distribution. Quantile regression is used for quantile regression in Quantile Regression GAN and QRGAN. The authors show that QRGAN is more robust to mode collapse than LSGAN and WGAN. QRGAN outperforms LSGAN in terms of robustness and generation performance assessment on Frechet Inception Distance (FID) and FID."
621,SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,relevance metrics USED-FOR similarity - based explanation. cosine similarity FEATURE-OF gradients. Method is machine learning models. Generic is metrics. ,"This paper studies the problem of similarity-based explanation in machine learning models. The authors propose a set of relevance metrics to measure the similarity between the gradients of the two gradients. These metrics are based on the cosine similarity between gradients, which is a well-studied property of the metrics. ",This paper proposes a new metric for similarity-based explanation based on the cosine similarity of the gradients of two machine learning models. The proposed metrics are based on existing relevance metrics. The authors show that the proposed metrics can be used to improve the performance of the model.
630,SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"generalization power EVALUATE-FOR Graph Neural Networks ( GNNs ). algorithmic alignment USED-FOR graph isomorphism test. LRGA module PART-OF GNNs. LRGA USED-FOR it. sample complexity EVALUATE-FOR kernel ’s feature map. 2 - FWL update step USED-FOR RGNN. LRGA USED-FOR RGNN. randomly initialized two - layer MLP USED-FOR RGNN. randomly initialized two - layer MLP USED-FOR kernel ’s feature map. polynomial kernels USED-FOR RGNN. polynomial kernels USED-FOR 2 - FWL update step. LRGA USED-FOR GNN layers. LRGA USED-FOR GNN architectures. Method are dot - product attention, and expressive GNNs. OtherScientificTerm is generalization properties. Generic is kernel. Material is GNN benchmarks. ","This paper studies the generalization power of Graph Neural Networks (GNNs) under algorithmic alignment in the graph isomorphism test. The authors propose a new LRGA module in GNNs that is able to learn the kernel’s feature map from a randomly initialized two-layer MLP, which is then used to update the RGNN using polynomial kernels in the 2-FWL update step. They show that LRGA can be used to learn GNN layers that are more expressive than existing GNN architectures using LRGA. They also show that dot-product attention can be added to the kernel to improve its generalization properties. ","This paper studies the generalization power of Graph Neural Networks (GNNs) under algorithmic alignment in the graph isomorphism test. The authors propose a new LRGA module for GNNs, and show that it improves the sample complexity of the kernel’s feature map under the randomly initialized two-layer MLP and the 2-FWL update step of the RGNN with polynomial kernels. They also show that LRGA can be applied to all GNN layers, and that it can be used to improve the dot-product attention of expressive GNN. The paper is well-written and well-motivated. The generalization properties of GNN benchmarks are interesting."
639,SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"objectness measures USED-FOR calibration. objectness measures USED-FOR Convolutional Neural Networks ( CNNs ). calibration EVALUATE-FOR Convolutional Neural Networks ( CNNs ). loss functions USED-FOR classification CNNs. CNNs USED-FOR classifiers. transformation USED-FOR CNN. random crops USED-FOR approaches. Context dependence FEATURE-OF safety - critical applications. objectness CONJUNCTION label smoothing. label smoothing CONJUNCTION objectness. approach USED-FOR classification. label smoothing USED-FOR training. objectness USED-FOR approach. label smoothing USED-FOR approach. relative object size USED-FOR smoothing factor. approach USED-FOR confidences. adaptive label smoothing USED-FOR CNNs. approach COMPARE baselines. baselines COMPARE approach. MS COCO COMPARE hard label approach. hard label approach COMPARE MS COCO. transfer learning USED-FOR MS COCO. transfer learning COMPARE hard label approach. hard label approach COMPARE transfer learning. Generic are they, and methods. Material are ImageNet-1 K, ImageNet, and context only images. Method is class activation maps. Task is classification and transfer learning tasks. ","This paper studies objectness measures for calibration of Convolutional Neural Networks (CNNs) in the context of classification and transfer learning tasks. The authors propose a new approach based on objectness and label smoothing to improve the calibration of classification CNNs. The proposed approach is based on the idea that the relative object size of the smoothing factor can be used as a proxy for the relative smoothness of the class activation maps of the CNN. They show that the proposed approach can improve the performance of CNNs for classification, as well as transfer learning.   ",This paper proposes a novel approach to improve the calibration of Convolutional Neural Networks (CNNs) by using objectness measures for calibration. The key idea of the approach is to use the relative object size as the smoothing factor for training the classifiers. The proposed approach is evaluated on classification and transfer learning tasks and compared to other baselines such as MS COCO and adaptive label smoothing for CNNs. The results show that the proposed approach can improve the confidences of the class activation maps. 
648,SP:5254658923e594294b69d124a8d004166852822a,"Neural networks USED-FOR inverse problems. convex dual network USED-FOR interpreting training and prediction. convex solvers USED-FOR convex dual network. neural networks USED-FOR path sparsity. weight decay regularization FEATURE-OF neural networks. piecewise linear filtering USED-FOR prediction. MNIST and fastMRI datasets EVALUATE-FOR dual network optimization problem. Task is medical imaging. Method are convex duality framework, and convex optimization. ","This paper studies the dual network optimization problem in medical imaging. The authors propose a new convex duality framework, which is based on convex solvers. The main idea is to use neural networks to solve inverse problems with weight decay regularization in neural networks for path sparsity.  The authors show that the proposed method is able to achieve state-of-the-art performance on MNIST and fastMRI datasets. ",This paper proposes a convex duality framework to solve the dual network optimization problem in medical imaging. The main idea is to use convex solvers in the convex pairwise dual network for interpreting training and prediction. The authors show that neural networks with weight decay regularization can be used to solve inverse problems with path sparsity. They also show that the proposed convex optimization can be combined with piecewise linear filtering for prediction. Experiments on MNIST and fastMRI datasets demonstrate the effectiveness of the proposed method.
657,SP:085cad6bc143c8713580bddfaa71f06496dac314,processing stages PART-OF text - to - speech synthesis pipelines. models USED-FOR raw speech audio outputs. character or phoneme input sequences USED-FOR models. generator USED-FOR inference. generator USED-FOR training. training CONJUNCTION inference. inference CONJUNCTION training. token length prediction USED-FOR differentiable alignment scheme. differentiable alignment scheme USED-FOR generator. adversarial feedback CONJUNCTION prediction losses. prediction losses CONJUNCTION adversarial feedback. total duration CONJUNCTION mel - spectrogram. mel - spectrogram CONJUNCTION total duration. prediction losses USED-FOR It. adversarial feedback USED-FOR It. soft dynamic time warping USED-FOR spectrogram - based prediction loss. soft dynamic time warping USED-FOR model. model COMPARE models. models COMPARE model. mean opinion score EVALUATE-FOR model. multi - stage training USED-FOR models. OtherScientificTerm is normalised text or phonemes. ,"This paper studies the problem of text-to-speech synthesis pipelines. The authors propose a differentiable alignment scheme based on token length prediction. It is based on adversarial feedback, prediction losses, and a mel-spectrogram. The model is trained with soft dynamic time warping to improve the performance of the spectrogram-based prediction loss. The paper shows that the proposed model achieves better mean opinion score than existing models. ","This paper proposes a novel way to train models for raw speech audio outputs from character or phoneme input sequences. The main idea is to use multiple processing stages in text-to-speech synthesis pipelines. The authors propose a differentiable alignment scheme based on token length prediction. It is based on adversarial feedback, prediction losses, and a mel-spectrogram. The generator is used for training and inference. The model is evaluated on the mean opinion score of normalised text or phonemes and on multi-stage training. The proposed model is shown to outperform other models in terms of total duration and prediction losses. The spectrogram-based prediction loss relies on soft dynamic time warping."
666,SP:01148cea55db606aa78d27e900818684a8bce9ab,"attributed graphs FEATURE-OF real - world graphs. non - topological features FEATURE-OF nodes. attributes PART-OF attributed graph. lower - dimensional space FEATURE-OF discrete distributions. Wasserstein metric USED-FOR lower - dimensional space. Wasserstein metric USED-FOR discrete distributions. Wasserstein graph diffusion USED-FOR distribution representations of nodes. topology structure CONJUNCTION attributes. attributes CONJUNCTION topology structure. point representations USED-FOR downstream tasks. it USED-FOR node classification. algorithms USED-FOR node classification. algorithms USED-FOR matrix completion. node classification CONJUNCTION matrix completion. matrix completion CONJUNCTION node classification. it USED-FOR matrix completion. algorithms EVALUATE-FOR representation method. missing attributes USED-FOR node classification. it USED-FOR algorithms. Method are node representation learning approaches, and non - parametric framework. OtherScientificTerm are incomplete information, decomposition of the attribute matrix, node features, Wasserstein space, and local neighborhoods. Metric is distortion. ","This paper proposes a new representation learning approach for real-world graphs. The proposed method is based on the Wasserstein graph diffusion (Wasserstein metric) to learn the distribution representations of nodes in a lower-dimensional space. The authors propose a non-parametric framework, where nodes are represented as a set of non-topological features, and the attributes of the attributed graph are represented by a decomposition of the attribute matrix. The nodes are then represented by point representations, which can be used for downstream tasks such as node classification, matrix completion, and node classification with missing attributes. The paper shows that the proposed representation method outperforms existing algorithms for node classification and matrix completion.","This paper proposes a novel non-parametric framework for node representation learning approaches. The key idea is to use Wasserstein graph diffusion to learn distribution representations of nodes with non-topological features. The attribute matrix is constructed by decomposition of the attribute matrix into two parts: the topology structure and the attributes of the attributed graph. The lower-dimensional space of the discrete distributions of the attributes in the attribute matrices is modeled as a function of the Wassersteins metric. The authors show that the distortion between the two parts can be reduced to zero. The proposed representation method is evaluated on three different algorithms for node classification, matrix completion, and point representations for downstream tasks. "
675,SP:aeeb5909f7123ef631f569b469af9715205c881f,"Adversarially Motivated Intrinsic GOals USED-FOR goal - conditioned “ student ” policy. AMIGO HYPONYM-OF agent. meta - learning USED-FOR agent. goal - generating teacher PART-OF agent. intrinsic motivation CONJUNCTION RL methods. RL methods CONJUNCTION intrinsic motivation. Task are reinforcement learning ( RL ), and procedurally - generated tasks. OtherScientificTerm are sparse extrinsic rewards, environment reward, and constructively adversarial ” objective. Generic is method. ","This paper studies the problem of reinforcement learning (RL), where the goal-conditioned “student” policy is trained with adversarially Motivated Intrinsic GOals. The agent is an AMIGO, which is an agent trained with meta-learning. The goal-generating teacher is the agent’s agent, and the agent is trained using goal-generated teacher. The teacher is trained to generate sparse extrinsic rewards for the agent. The environment reward is a “constructively adversarial” objective. The authors show that the intrinsic motivation of the agent and the RL methods can be improved by the proposed method. ","This paper proposes a method for adversarially Motivated Intrinsic GOals for learning goal-conditioned “student” policy with sparse extrinsic rewards. The agent is modeled as an AMIGO, where the agent is a goal-generating teacher and the environment reward is a “constructively adversarial” objective. This agent is trained using meta-learning. The method is evaluated on both procedurally-generated tasks and on reinforcement learning (RL), where it outperforms intrinsic motivation and RL methods."
684,SP:3d05bc7dca97681cb582298e318b9b973841eed3,"user distortion CONJUNCTION user privacy constraint. user privacy constraint CONJUNCTION user distortion. dataset of files USED-FOR information retrieval. distortion FEATURE-OF retrieval process. private information retrieval USED-FOR model. download rate CONJUNCTION distortion. distortion CONJUNCTION download rate. distortion CONJUNCTION user privacy leakage. user privacy leakage CONJUNCTION distortion. mutual information FEATURE-OF information - theoretical formulation. download rate EVALUATE-FOR schemes. generative adversarial models USED-FOR data - driven framework. constrained minimax game USED-FOR scheme. download rate CONJUNCTION distortion. distortion CONJUNCTION download rate. synthetic Gaussian dataset CONJUNCTION MNIST and CIFAR-10 datasets. MNIST and CIFAR-10 datasets CONJUNCTION synthetic Gaussian dataset. MNIST and CIFAR-10 datasets EVALUATE-FOR scheme. synthetic Gaussian dataset EVALUATE-FOR scheme. MNIST dataset EVALUATE-FOR data - driven approach. data - driven approach COMPARE achievable scheme. achievable scheme COMPARE data - driven approach. MNIST dataset EVALUATE-FOR achievable scheme. source coding USED-FOR achievable scheme. OtherScientificTerm are privacy level, perfect privacy requirement, and distortion constraint. Metric are rate - distortion - leakage tradeoff, and rate - distortion tradeoff curve. Material is CIFAR-10. ","This paper proposes a new data-driven framework based on generative adversarial models. The authors propose to use a dataset of files for information retrieval from which a model can be trained using private information retrieval. The privacy level is defined as the distance between the user distortion and the user privacy constraint. The paper also proposes a rate-distortion-leakage tradeoff between the two, which is based on the mutual information between the information-theoretical formulation and the actual retrieval process. The proposed scheme uses a constrained minimax game to learn the tradeoff curve between the download rate, the distortion, and user privacy leakage. Experiments on the synthetic Gaussian dataset, MNIST and CIFAR-10 datasets show that the proposed scheme performs better than a data-based approach on the MNIST dataset and achieves better performance than an achievable scheme based on source coding.","The paper proposes a data-driven framework based on generative adversarial models for information retrieval from a dataset of files. The model is based on private information retrieval, where the privacy level is enforced by a perfect privacy requirement. The paper proposes an information-theoretic formulation of the retrieval process, which is motivated by mutual information between the user distortion and the user privacy constraint. The authors propose a rate-distortion-leakage tradeoff between the distortion and user privacy leakage. The proposed scheme is evaluated on a constrained minimax game and on a synthetic Gaussian dataset and on MNIST and CIFAR-10 datasets. The experimental results show that the proposed scheme outperforms existing schemes in terms of both the download rate and the distortion. In addition, the authors propose an achievable scheme based on source coding."
693,SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"Graph Neural Networks ( GNNs ) USED-FOR graph - related applications. they USED-FOR large scale settings. fidelity FEATURE-OF model. fidelity EVALUATE-FOR sampling - based methods. decoupled greedy learning method USED-FOR GNNs ( DGL - GNN ). greedy auxiliary objectives USED-FOR module. method USED-FOR time or memory limited applications. efficiency EVALUATE-FOR method. sampling - based acceleration COMPARE model. model COMPARE sampling - based acceleration. efficiency CONJUNCTION accuracy. accuracy CONJUNCTION efficiency. decoupled approach COMPARE methods. methods COMPARE decoupled approach. sampling PART-OF GNN training. sampling HYPONYM-OF it. OtherScientificTerm are node embeddings, and GNN layers. Method are GNN, lazy - update scheme, and DGL - GNN model. Generic are modules, and approach. Task is parallel GNN training. ","This paper proposes a decoupled greedy learning method for GNNs (DGL-GNN) for graph-related applications where the node embeddings are very large and they are used in large scale settings. The main idea is to learn a GNN with a set of modules that are greedy auxiliary objectives, and then update each module according to the greedy auxiliary objective. The authors propose a lazy-update scheme, where each module is updated according to a greedy auxiliary goal. They show that their method can be used for time or memory limited applications, where the fidelity of the model is highly dependent on the number of modules. They also show that the proposed model achieves better efficiency and accuracy than sampling-based acceleration, and that it can be combined with sampling in GNN training. ","This paper proposes a decoupled greedy learning method for GNNs (DGL-GNN) for graph-related applications, where they can be used in large scale settings. The proposed GNN is based on a lazy-update scheme, where each module is trained with greedy auxiliary objectives to improve the fidelity of the model. The main idea of the proposed approach is to combine the sampling in GNN training with the GNN layers. The method is evaluated on time or memory limited applications, showing that the proposed model outperforms sampling-based acceleration in terms of efficiency and accuracy."
702,SP:5ecb1b288f7fc02aead4493f81640867bc349290,Neural link predictors USED-FOR missing edges. missing edges PART-OF large scale Knowledge Graphs. logical conjunctions ( ∧ ) CONJUNCTION disjunctions. disjunctions CONJUNCTION logical conjunctions ( ∧ ). disjunctions CONJUNCTION existential quantifiers. existential quantifiers CONJUNCTION disjunctions. framework USED-FOR complex queries. incomplete Knowledge Graphs USED-FOR complex queries. solutions USED-FOR optimisation problem. gradient - based and combinatorial search HYPONYM-OF optimisation problem. gradient - based and combinatorial search HYPONYM-OF solutions. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. state - of - the - art methods COMPARE neural models. neural models COMPARE state - of - the - art methods. approach COMPARE neural models. neural models COMPARE approach. Hits@3 EVALUATE-FOR knowledge graphs. factual information FEATURE-OF knowledge graphs. intermediate solutions USED-FOR complex query atoms. intermediate solutions USED-FOR model. Generic is models. OtherScientificTerm is end - to - end differentiable objective. Method is neural link predictor. ,"This paper proposes a neural link predictor for missing edges in large scale Knowledge Graphs with missing edges. The authors propose a framework to solve complex queries on incomplete knowledge Graphs. The proposed solutions are based on gradient-based and combinatorial search, where the optimisation problem is formulated as a combination of logical conjunctions (∧) and disjunctions such as existential quantifiers. The model is trained using intermediate solutions for complex query atoms, and the proposed approach is shown to outperform state-of-the-art methods on Hits@3.","This paper proposes a new framework for learning missing edges in large scale Knowledge Graphs. The missing edges are represented as logical conjunctions (∧) and disjunctions. The authors propose to use neural link predictors to predict missing edges. The proposed approach is compared to state-of-the-art methods on a number of tasks, and is shown to outperform neural models on Hits@3. The main contribution of the paper is to introduce an end-to-end differentiable objective. The paper also proposes two solutions to the optimisation problem, namely gradient-based and combinatorial search, which is a combination of two existing approaches to the optimization problem. Experiments show that the proposed framework is able to solve complex queries on incomplete knowledge Graphs, where the knowledge graphs contain factual information. The model is trained on intermediate solutions for complex query atoms, and the proposed approach outperforms neural models."
711,SP:f04a522fd04c503754fdb8c52da68646d31271a4,"procedure USED-FOR local robustness. procedure USED-FOR feed - forward neural networks. local robustness FEATURE-OF feed - forward neural networks. piecewise - linear activation functions FEATURE-OF feed - forward neural networks. decision boundaries USED-FOR assessing robustness. highly - parallel GPU implementation USED-FOR ` 2 norm. approach COMPARE approximate verification approaches. approximate verification approaches COMPARE approach. approximate verification approaches COMPARE verifiers. verifiers COMPARE approximate verification approaches. approach COMPARE verifiers. verifiers COMPARE approach. Task is Local robustness. Generic are model, networks, network, and algorithm. OtherScientificTerm are ` p - ball consistently, adversarial inputs, convex polyhedral regions, and geometric projections. Metric is robustness. ",This paper proposes a new procedure for evaluating the local robustness of feed-forward neural networks with piecewise-linear activation functions. The proposed approach is based on a highly-parallel GPU implementation of the `2 norm' of the model. The authors show that the `p-ball consistently converges to the ‘2 norm’ of the network when the number of adversarial inputs is small. They also show that their approach is more robust than existing approximate verification approaches and verifiers. ,This paper proposes a new procedure for evaluating local robustness of feed-forward neural networks with piecewise-linear activation functions. Local robustness is defined as the robustness to adversarial inputs in convex polyhedral regions. The proposed approach is compared to other approximate verification approaches and verifiers. The authors show that the `2 norm of the `p-ball consistently’ can be computed using a highly-parallel GPU implementation. The paper also shows that the proposed algorithm can be used to evaluate robustness on decision boundaries.
720,SP:5297651ff873f97c07b9c47ed3eff52251661844,"approach USED-FOR embedding of objects. affordance space FEATURE-OF embedding of objects. embedding COMPARE approaches. approaches COMPARE embedding. dimensions USED-FOR mental representation of objects. human judgements of object similarity USED-FOR mental representation of objects. Generic are knowledge, and they. OtherScientificTerm are object “ affordance ”, and human judgments of affordance. Material is text corpora. ",This paper proposes a new approach for embedding of objects in the affordance space of text corpora. The key idea is to use the knowledge of object “affordance” in the space of dimensions to learn a mental representation of objects based on human judgements of object similarity. The authors show that this embedding is better than existing approaches in terms of performance. ,This paper proposes a new approach for embedding of objects in the affordance space. The idea is to use knowledge of an object“ affordance” as a measure of how well an object can be represented in the “affordance space”. The authors show that the embedding is better than other approaches in terms of the dimensions of the space and the mental representation of objects based on human judgements of object similarity. They also show that they can be used in text corpora.
729,SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"Individuality USED-FOR human society. efficiency CONJUNCTION productivity. productivity CONJUNCTION efficiency. It USED-FOR division of labor. efficiency EVALUATE-FOR It. productivity EVALUATE-FOR It. it USED-FOR multi - agent cooperation. method USED-FOR emergence of individuality ( EOI ). emergence of individuality ( EOI ) PART-OF multi - agent reinforcement learning ( MARL ). probabilistic classifier USED-FOR probability distribution. EOI USED-FOR probabilistic classifier. regularizers USED-FOR classifier. intrinsic reward USED-FOR emergence of individuality. regularizers USED-FOR emergence of individuality. MARL algorithms USED-FOR EOI. EOI COMPARE methods. methods COMPARE EOI. multi - agent cooperative scenarios EVALUATE-FOR methods. multi - agent cooperative scenarios EVALUATE-FOR EOI. OtherScientificTerm are individuality, and intrinsic reward signals. ","This paper proposes a method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). It aims to improve the efficiency and productivity of the division of labor in human society. The authors propose a probabilistic classifier based on the EOI to learn the probability distribution of each agent’s intrinsic reward signals. It is shown that it can be used to improve multi-adversarial cooperation, and it can also be used for multi-agents cooperation. Empirically, the authors show that the proposed MARL algorithms are able to achieve better performance than existing methods in several multi- agent cooperative scenarios.","This paper proposes a method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). It improves efficiency and productivity in the division of labor, and it can be applied to multi-agents cooperation. The authors propose a probabilistic classifier that predicts the probability distribution of an agent’s intrinsic reward signals. They show that EOI can be learned with MARL algorithms, and they show that it can outperform other methods on multi-Agent cooperative scenarios. "
738,SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"certified robustness EVALUATE-FOR Randomized smoothing. base classifier USED-FOR randomized smoothing. Smoothed WEighted ENsembling ( SWEEN ) scheme USED-FOR randomized smoothed classifiers. SWEEN USED-FOR optimal certified robustness. adaptive prediction algorithm USED-FOR SWEEN models. adaptive prediction algorithm USED-FOR prediction and certification cost. SWEEN models COMPARE candidate models. candidate models COMPARE SWEEN models. training time EVALUATE-FOR SWEEN models. small models USED-FOR SWEEN models. OtherScientificTerm are l2 - norm adversarial attacks, and ensembling generality. Method is SWEEN model. ",This paper studies the problem of certified robustness of Randomized smoothing with a base classifier. The authors propose a Smoothed WEighted ENsembling (SWEEN) scheme for randomized smoothed classifiers. SWEEN is an adaptive prediction algorithm that can be used to improve the prediction and certification cost of the SWEen models. The paper also shows that SWEE models are more robust to l2-norm adversarial attacks than candidate models.  The authors also show that small models can be trained with SWEED models with small training time.   The main contribution of the paper is a theoretical analysis of the effect of ensembling generality on the performance of the proposed model. ,This paper studies the problem of certified robustness of Randomized smoothing with a base classifier. The authors propose a Smoothed WEighted ENsembling (SWEEN) scheme for randomized smoothed classifiers. They show that the SWEEN model is robust to l2-norm adversarial attacks. They also show that SWEen can be used to achieve optimal certified robusts. They propose an adaptive prediction algorithm to reduce the prediction and certification cost. They evaluate the SweEN models on small models and compare them to other candidate models.
747,SP:ea892e3d199ed6121279b20061a87f43afae8796,hierarchical structures USED-FOR learning process. hierarchical structures USED-FOR generalization. Ordered Memory Policy Network ( OMPN ) USED-FOR subtask hierarchy. subtask hierarchy USED-FOR task decomposition. subtask boundaries PART-OF unstructured demonstration. Craft CONJUNCTION Dial. Dial CONJUNCTION Craft. model COMPARE baselines. baselines COMPARE model. Craft EVALUATE-FOR model. Dial EVALUATE-FOR model. task decomposition EVALUATE-FOR model. unsupervised and weakly supervised settings EVALUATE-FOR model. OMPN USED-FOR partially observable environments. task decomposition EVALUATE-FOR OMPN. subtask hierarchy PART-OF model. Task is complex real - world tasks. OtherScientificTerm is inductive bias. ,This paper studies the problem of learning hierarchical structures for the learning process in complex real-world tasks. The authors propose an Ordered Memory Policy Network (OMPN) to learn a subtask hierarchy for the task decomposition in an unstructured demonstration with subtask boundaries. The OMPN is able to generalize well to partially observable environments. Experiments show that the proposed model performs better than existing baselines such as Craft and Dial in both unsupervised and weakly supervised settings.,"This paper proposes a hierarchical structure for the learning process. The main idea is to use hierarchical structures for generalization to complex real-world tasks. The authors propose an Ordered Memory Policy Network (OMPN) to learn the subtask hierarchy for the task decomposition in an unstructured demonstration. The OMPN is trained on partially observable environments. The model is evaluated on Craft, Dial, and unsupervised and weakly supervised settings. The results show that the model outperforms baselines. "
756,SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,semantic factor CONJUNCTION variation factor. variation factor CONJUNCTION semantic factor. deep ones HYPONYM-OF supervised learning methods. methods USED-FOR OOD prediction. causal reasoning USED-FOR Causal Semantic Generative model ( CSG ). learning CONJUNCTION prediction. prediction CONJUNCTION learning. variational Bayes USED-FOR learning. variational Bayes USED-FOR prediction. causal invariance principle USED-FOR methods. CSG USED-FOR semantic factor. semantic - identification USED-FOR adaptation. OOD EVALUATE-FOR baselines. OtherScientificTerm is domain - specific correlation. Metric is OOD generalization error. ,"This paper proposes a Causal Semantic Generative model (CSG) based on causal reasoning. The CSG learns a semantic factor, a variation factor, and a domain-specific correlation between the semantic factor and the variation factor. The proposed methods are applied to OOD prediction using variational Bayes for learning and prediction with the causal invariance principle. The authors show that the proposed baselines can achieve OOD generalization error of 0.5-1.5% when the semantic-identification is used for adaptation.","This paper proposes a Causal Semantic Generative model (CSG) based on causal reasoning. The authors propose two methods for OOD prediction based on variational Bayes. The methods are based on the causal invariance principle, which allows for domain-specific correlation between the semantic factor and the variation factor. The proposed methods are evaluated on standard supervised learning methods (e.g., deep ones) as well as on OOD. The experimental results show that the proposed baselines outperform the baselines on both learning and prediction. The main contribution of the paper is the use of semantic-identification for adaptation and the OOD generalization error."
765,SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"adversarially corrupted rewards FEATURE-OF online learning algorithms. small regret FEATURE-OF algorithms. algorithm USED-FOR corrupted rewards. uncorrupted reward distribution FEATURE-OF regret. robust estimation USED-FOR unsupervised learning problems. stochastic multi - armed bandits CONJUNCTION linear contextual bandits. linear contextual bandits CONJUNCTION stochastic multi - armed bandits. linear contextual bandits CONJUNCTION Markov Decision Processes ( MDPs ). Markov Decision Processes ( MDPs ) CONJUNCTION linear contextual bandits. robust estimation USED-FOR robust online algorithms. robust online algorithms USED-FOR scenarios. stochastic rewards and transitions FEATURE-OF Markov Decision Processes ( MDPs ). near optimal regret FEATURE-OF robust online algorithms. Markov Decision Processes ( MDPs ) HYPONYM-OF scenarios. stochastic multi - armed bandits HYPONYM-OF scenarios. linear contextual bandits HYPONYM-OF scenarios. synthetic and real datasets EVALUATE-FOR algorithms. Method are online algorithm, and online learning. OtherScientificTerm are stochastic reward, and noise rate. ","This paper studies the problem of robust estimation for unsupervised learning problems with adversarially corrupted rewards. The authors propose a new online algorithm to estimate the corrupted rewards of a stochastic reward. The algorithm is based on the idea that the uncorrupted reward distribution of the online algorithm can be used as a proxy for the regret of the algorithm. They show that the algorithm can estimate corrupted rewards with small regret. They also show that robust online algorithms can achieve near optimal regret in a variety of scenarios, including stoachastic multi-armed bandits, linear contextual bandits, and Markov Decision Processes (MDPs). ","This paper studies the problem of adversarially corrupted rewards in online learning algorithms with small regret. The authors propose a new algorithm to mitigate the corrupted rewards. The algorithm is based on robust estimation for unsupervised learning problems with uncorrupted reward distribution. The main idea is to learn an online algorithm that is robust to corrupted rewards, and then use this robust estimation to train robust online algorithms for different scenarios such as stochastic multi-armed bandits, linear contextual bandits, and Markov Decision Processes (MDPs). The authors show that the proposed algorithms can achieve near optimal regret on synthetic and real datasets. "
774,SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"Encoder - decoder architecture USED-FOR neural machine translation ( NMT ). rewriter CONJUNCTION evaluator. evaluator CONJUNCTION rewriter. rewriter PART-OF It. evaluator PART-OF It. evaluator USED-FOR translation quality. rewriter CONJUNCTION evaluator. evaluator CONJUNCTION rewriter. prioritized gradient descent ( PGD ) method USED-FOR rewriter. PGD method USED-FOR Rewriter - Evaluator. framework USED-FOR NMT models. Transformer HYPONYM-OF NMT models. framework COMPARE baselines. baselines COMPARE framework. translation tasks EVALUATE-FOR framework. NMT models COMPARE baselines. baselines COMPARE NMT models. framework USED-FOR NMT models. Chinese - English and English - German HYPONYM-OF translation tasks. Generic is it. OtherScientificTerm is termination policy. Method are RewriterEvaluator, decoding, and encoder - decoder models. Task is rewriting process. ","This paper proposes RewriterEvaluator, an encoder-decoder architecture for neural machine translation (NMT). It consists of a rewriter and an evaluator to improve the translation quality. The rewriter uses prioritized gradient descent (PGD) method to train the rewriter, and the evaluation is based on Rewriter-evaluator's termination policy. The authors show that the proposed framework can improve the performance of NMT models such as Transformer, Transformer-NMT, and Transformer+NMT on a variety of translation tasks such as Chinese-English and English-German. They also show that it can be applied to the rewriting process.","This paper proposes an Encoder-decoder architecture for neural machine translation (NMT). It consists of a rewriter, an evaluator, and a termination policy. The rewriter uses a prioritized gradient descent (PGD) method to improve the translation quality. The evaluation is performed using RewriterEvaluator. The authors show that the proposed framework outperforms other NMT models such as Transformer, Transformer-2, and Rewriter-Decoder models on translation tasks such as Chinese-English and English-German. "
783,SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"images CONJUNCTION unsystematic annotation. unsystematic annotation CONJUNCTION images. Ambiguities CONJUNCTION unsystematic annotation. unsystematic annotation CONJUNCTION Ambiguities. Ambiguities CONJUNCTION images. images CONJUNCTION Ambiguities. empirical frequency FEATURE-OF sampled predictions. two - stage, cascaded strategy USED-FOR calibrated adversarial refinement. adversarial network USED-FOR coherent predictions. black - box segmentation framework USED-FOR learning of calibrated stochastic mappings. model USED-FOR learning of calibrated stochastic mappings. model PART-OF black - box segmentation framework. multigrader LIDC dataset CONJUNCTION Cityscapes dataset. Cityscapes dataset CONJUNCTION multigrader LIDC dataset. multigrader LIDC dataset EVALUATE-FOR approach. Cityscapes dataset EVALUATE-FOR approach. core design USED-FOR tasks. framework USED-FOR semantic segmentation. core design USED-FOR calibrated predictive distribution. toy regression dataset EVALUATE-FOR framework. calibrated predictive distribution USED-FOR tasks. OtherScientificTerm are distribution over predictions, empirical distribution, multimodal predictive distribution, categorical likelihood, and calibrated stochastic mappings. Method is probabilistic networks. Generic is these. ","This paper proposes a new black-box segmentation framework for learning of calibrated stochastic mappings from images, Ambiguities, and unsystematic annotation. The proposed model consists of a two-stage, cascaded strategy for calibrated adversarial refinement. The core design of the proposed framework is to learn a calibrated predictive distribution for tasks such as semantic segmentation and unsupervised learning. Experiments on the multigrader LIDC dataset and Cityscapes dataset demonstrate the effectiveness of the approach.","This paper proposes a black-box segmentation framework for learning of calibrated stochastic mappings. The proposed model is based on a two-stage, cascaded strategy for calibrated adversarial refinement, where the distribution over predictions is sampled from a multimodal predictive distribution, and the empirical frequency of sampled predictions is computed based on the categorical likelihood. The adversarial network is trained to produce coherent predictions. The approach is evaluated on the multigrader LIDC dataset, Cityscapes dataset, and on a toy regression dataset. The core design of the proposed framework is applied to two tasks: semantic segmentation and unsystematic annotation. "
792,SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"distributed compute systems USED-FOR stochastic optimization algorithms. stochastic optimization algorithms USED-FOR large - scale machine learning applications. distributed compute systems USED-FOR large - scale machine learning applications. error feedback ( EF ) USED-FOR compressed communication. Top - K or PowerSGD HYPONYM-OF contractive compressors. EF USED-FOR contractive compressors. alternative USED-FOR contractive compressors. alternative USED-FOR EF. construction USED-FOR contractive compressor. construction USED-FOR induced unbiased compressor. contractive compressor CONJUNCTION induced unbiased compressor. induced unbiased compressor CONJUNCTION contractive compressor. approach COMPARE EF. EF COMPARE approach. reduced memory requirements CONJUNCTION communication complexity guarantees. communication complexity guarantees CONJUNCTION reduced memory requirements. communication complexity guarantees CONJUNCTION assumptions. assumptions CONJUNCTION communication complexity guarantees. partial participation FEATURE-OF federated learning. Generic are systems, and transformation. OtherScientificTerm are communication overhead, stochastic gradients, and unbiased compressors. ","This paper studies distributed compute systems for stochastic optimization algorithms for large-scale machine learning applications. The authors propose error feedback (EF) for compressed communication between two systems, and propose a new alternative to EF for contractive compressors such as Top-K or PowerSGD. The main contribution of the paper is a new construction for the induced unbiased compressor and the contractive compressor. The proposed approach is shown to outperform EF in terms of reduced memory requirements, communication complexity guarantees, and assumptions. The paper also shows that the partial participation in federated learning can be improved by using unbiased compressors.","This paper proposes a new approach to improve the communication efficiency of distributed compute systems for stochastic optimization algorithms for large-scale machine learning applications. The authors propose to use error feedback (EF) for compressed communication between two systems to reduce the communication overhead. The proposed approach is evaluated on Top-K or PowerSGD, and compared to an alternative to EF for contractive compressors (e.g. Top-k or PowerSGHD) and an alternative for an induced unbiased compressor (eq. 1). The authors show that the proposed approach outperforms EF in terms of reduced memory requirements, communication complexity guarantees, and assumptions on partial participation in federated learning. "
801,SP:4fd702490293e481c79614852ba27dd3ce9215a4,"hyperparameter optimization ( HPO ) USED-FOR HPO. HT - AA baseline algorithms CONJUNCTION benchmarks. benchmarks CONJUNCTION HT - AA baseline algorithms. baseline COMPARE HPO algorithm. HPO algorithm COMPARE baseline. HPO PART-OF ML development. python packages USED-FOR baselines. python packages USED-FOR benchmarks. baselines CONJUNCTION benchmarks. benchmarks CONJUNCTION baselines. python packages USED-FOR HT - AA. Method are machine learning ( ML ) algorithm, ML algorithms, and neural architectures. OtherScientificTerm are hyperparameter settings, hyperparameter search space, and hyperparameter search spaces. Generic are approaches, and research framework. ","This paper proposes a new machine learning (ML) algorithm called Hyperparameter Optimization (HPO) for hyperparameter search. HPO is an extension of hyperparameters optimization (HOO) in the context of HPO, where the goal is to find the optimal solution to a hyperparametrized problem. The authors show that HPO can be used to improve the performance of existing HT-AA baseline algorithms and benchmarks. The paper also shows that the HPO algorithm is more efficient than the baseline, and can be applied to a variety of tasks. ",This paper proposes a new hyperparameter optimization (HPO) algorithm for the machine learning (ML) algorithm. HPO is an extension of hyperparameters optimization (HOO) for the HPO setting. The authors show that HPO can be used in the context of ML development. They show that the proposed HPO algorithm outperforms the baseline and several other HT-AA baseline algorithms and benchmarks using python packages. They also show how HPO improves the performance of other approaches in the research framework. 
810,SP:e8f99bae5853de525450fcb8facd23cf973fc161,"audio labels COMPARE categorical probabilities. categorical probabilities COMPARE audio labels. image classifier USED-FOR classification. image classifier USED-FOR audio labels. audio labels COMPARE numerical probabilities. numerical probabilities COMPARE audio labels. numerical probabilities CONJUNCTION text. text CONJUNCTION numerical probabilities. audio labels COMPARE text. text COMPARE audio labels. they USED-FOR error signal. spectrograms CONJUNCTION shuffled spectrograms. shuffled spectrograms CONJUNCTION spectrograms. shuffled spectrograms CONJUNCTION Gaussian mixtures. Gaussian mixtures CONJUNCTION shuffled spectrograms. constant matrices CONJUNCTION spectrograms. spectrograms CONJUNCTION constant matrices. Gaussian mixtures CONJUNCTION uniform random matrices. uniform random matrices CONJUNCTION Gaussian mixtures. dimensionalities FEATURE-OF uniform random matrices. uniform random matrices HYPONYM-OF label representations. constant matrices HYPONYM-OF label representations. Gaussian mixtures HYPONYM-OF label representations. shuffled spectrograms HYPONYM-OF label representations. spectrograms HYPONYM-OF label representations. high dimensional, high entropy labels COMPARE text ( categorical ) labels. text ( categorical ) labels COMPARE high dimensional, high entropy labels. robustness EVALUATE-FOR features. image classification task EVALUATE-FOR high dimensional, high entropy labels. image classification task EVALUATE-FOR text ( categorical ) labels. accuracy EVALUATE-FOR high dimensional, high entropy labels. label representations USED-FOR features. OtherScientificTerm are data labels, and adversarial attacks. Generic is models. Method are high dimensional, high entropy label representations, and label representation. ","This paper studies the problem of learning high dimensional, high entropy label representations from data labels. The authors propose a new image classifier for audio labels that can be used in the context of classification. They show that audio labels are more robust to adversarial attacks than categorical probabilities and numerical probabilities. They also show that they are more sensitive to the error signal than text. Finally, they show that the label representations learned by the proposed label representations (e.g., spectrograms, Gaussian mixtures, uniform random matrices, and constant matrices) are better than those learned by standard label representations such as the standard text (categorical) labels.  ","This paper proposes a new method for learning high dimensional, high entropy label representations. The main idea is to use an image classifier to learn audio labels and categorical probabilities, and then use the audio labels to predict numerical probabilities and numerical probabilities for text. The authors show that these models are robust to adversarial attacks. They also show that they can be used to predict the error signal of the data labels. The paper also shows that the label representation can be learned with uniform random matrices, Gaussian mixtures, shuffled spectrograms, and constant matrices. The proposed method is evaluated on the image classification task, and the results show that the proposed features improve robustness and accuracy of the high dimensional, high entropy labels compared to text (categorical) labels."
819,SP:4e8d924cba7367af0999b30d79250b4dc40413e1,approaches USED-FOR ensemble neural networks. forward passes USED-FOR prediction. forward passes USED-FOR methods. single model ’s capacity USED-FOR subnetworks. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. accuracy CONJUNCTION calibration error. calibration error CONJUNCTION accuracy. negative log - likelihood CONJUNCTION accuracy. accuracy CONJUNCTION negative log - likelihood. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. out - of - distribution variants COMPARE methods. methods COMPARE out - of - distribution variants. ImageNet CONJUNCTION out - of - distribution variants. out - of - distribution variants CONJUNCTION ImageNet. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. Generic is network. OtherScientificTerm is forward pass. Metric is model robustness. ,"This paper proposes a new approach to train ensemble neural networks with forward passes. The proposed methods are based on forward passes to improve the prediction performance of the network. The key idea is to use a single model’s capacity to train subnetworks with a single forward pass. The authors show that the proposed methods improve the accuracy, the calibration error, and the negative log-likelihood of the prediction. They also show that their methods are more robust than out-of-distribution variants such as ImageNet, CIFAR10, and ImageNet.","This paper proposes two approaches for ensemble neural networks that use forward passes for prediction. The proposed methods are based on the forward passes, where a single model’s capacity is used to train subnetworks, and the network is trained with a single forward pass. The authors show that the proposed methods outperform out-of-distribution variants and other methods in terms of accuracy, calibration error, negative log-likelihood, and model robustness. "
828,SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"teacher network CONJUNCTION student network. student network CONJUNCTION teacher network. method USED-FOR intermediate knowledge. Sparse Representation Matching ( SRM ) USED-FOR intermediate knowledge. Convolutional Neural Network ( CNN ) USED-FOR intermediate knowledge. sparse representation learning USED-FOR Sparse Representation Matching ( SRM ). sparse representation learning USED-FOR method. pixellevel and image - level labels USED-FOR intermediate feature maps. intermediate feature maps PART-OF student network. sparse representations of the hidden features PART-OF teacher CNN. sparse representations of the hidden features USED-FOR SRM. neural processing block USED-FOR SRM. stochastic gradient descent USED-FOR neural processing block. stochastic gradient descent USED-FOR SRM. SRM COMPARE KD techniques. KD techniques COMPARE SRM. Task is Knowledge Distillation. Method are CNN, and teacher and student networks. ",This paper proposes a method for learning intermediate knowledge from sparse representation learning. The method is based on Sparse Representation Matching (SRM) which uses sparse representations of the hidden features in a teacher CNN and a student network. The intermediate feature maps in the student network are learned by pixellevel and image-level labels. The neural processing block in the SRM is trained by stochastic gradient descent. The authors show that SRM performs better than KD techniques. ,This paper proposes a method for learning intermediate knowledge from sparse representation learning. The method is based on Sparse Representation Matching (SRM) which is a variant of Sparse Gradient Descent (SGD). The authors propose to use a Convolutional Neural Network (CNN) to learn intermediate knowledge. The intermediate feature maps of the student network and the teacher network are learned from pixellevel and image-level labels. The teacher and student networks are trained with sparse representations of the hidden features of the teacher CNN. The neural processing block for SRM is a stochastic gradient descent. The authors show that SRM outperforms KD techniques. 
837,SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,Reinforcement learning methods USED-FOR policies. sequential structure PART-OF representation learning process. sequential structure PART-OF reinforcement learning. approach COMPARE approaches. approaches COMPARE approach. theoretically motivated policy similarity metric ( PSM ) USED-FOR behavioral similarity. contrastive representation learning procedure USED-FOR state similarity metric. contrastive representation learning procedure USED-FOR policy similarity embeddings ( PSEs1 ). PSM USED-FOR policy similarity embeddings ( PSEs1 ). LQR CONJUNCTION jumping task. jumping task CONJUNCTION LQR. jumping task CONJUNCTION Distracting DM Control Suite. Distracting DM Control Suite CONJUNCTION jumping task. generalization EVALUATE-FOR benchmarks. spurious correlations FEATURE-OF LQR. benchmarks EVALUATE-FOR PSEs. generalization EVALUATE-FOR PSEs. Distracting DM Control Suite HYPONYM-OF benchmarks. jumping task HYPONYM-OF benchmarks. LQR HYPONYM-OF benchmarks. Generic is structure. OtherScientificTerm is optimal policies. ,"This paper proposes a novel approach to learn policies with sequential structure in the representation learning process. The authors propose a theoretically motivated policy similarity metric (PSM) to measure behavioral similarity between two policies. The proposed approach is different from existing approaches in that it uses a contrastive representation learning procedure to learn the state similarity metric between two policy similarity embeddings (PSEs1). The authors show that the proposed PSEs can achieve better generalization than existing approaches on three benchmarks: LQR, jumping task, and Distracting DM Control Suite. ","This paper proposes a novel approach to learn policies that have sequential structure in the representation learning process. The proposed approach is different from existing approaches in the sense that the structure is learned in a sequential manner. The key idea is to use a theoretically motivated policy similarity metric (PSM) to measure behavioral similarity between two policies. The state similarity metric is learned using a contrastive representation learning procedure. The PSEs1 is a variant of the PSM for learning policy similarity embeddings (PSs1). Experiments on three benchmarks (LQR, jumping task, and Distracting DM Control Suite) show that the proposed approach achieves better generalization than existing approaches. "
846,SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"approach USED-FOR disentanglement. latent representation FEATURE-OF model. approach USED-FOR disentanglement. rotations CONJUNCTION translations. translations CONJUNCTION rotations. topological defects FEATURE-OF transformations. images FEATURE-OF transformations. affine transformations HYPONYM-OF transformations. translations HYPONYM-OF affine transformations. rotations HYPONYM-OF affine transformations. approach USED-FOR disentanglement. distributed equivariant operators USED-FOR approach. approach USED-FOR disentangle affine transformations. distributed operators USED-FOR disentanglement. distributed operators USED-FOR models. Task is Machine Learning. OtherScientificTerm are object shape, encoder, and latent space. Generic is factors. Method is group representation theory. ","This paper proposes a new approach for disentanglement based on distributed equivariant operators. The key idea is to use the latent representation of the model as a proxy for the object shape, and then use the encoder to disentangle the latent space between the two factors. The proposed approach is shown to achieve better disentangled representations than existing models using distributed operators. Empirical results show the effectiveness of the proposed approach.","This paper proposes a new approach to disentanglement for disentangle affine transformations, i.e., transformations with topological defects. The key idea is to use distributed equivariant operators to learn a model with a latent representation of the object shape. The encoder is trained in a group representation theory, and the latent space is represented as a set of factors. The authors show that the proposed approach can achieve disentangled disentangling of disentangles of the transformations (e.g., rotations, translations, etc.). The authors also show that models trained with distributed operators can achieve better performance than models trained without distributed operators. "
855,SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,statistical framework USED-FOR timedependent interaction of neuronal spiking activities. Hawkes process USED-FOR timedependent interaction of neuronal spiking activities. Hawkes process USED-FOR statistical framework. Hawkes process USED-FOR inhibitory interactions. nonlinear Hawkes process USED-FOR influence pattern. excitatory or inhibitory interactions FEATURE-OF influence pattern. latent marked Poisson processes CONJUNCTION sparsity variables. sparsity variables CONJUNCTION latent marked Poisson processes. Pólya - Gamma variables CONJUNCTION latent marked Poisson processes. latent marked Poisson processes CONJUNCTION Pólya - Gamma variables. auxiliary latent variables USED-FOR functional connection weights. Gaussian form FEATURE-OF functional connection weights. analytical updates USED-FOR iterative algorithm. sparsity variables HYPONYM-OF auxiliary latent variables. latent marked Poisson processes HYPONYM-OF auxiliary latent variables. Pólya - Gamma variables HYPONYM-OF auxiliary latent variables. expectationmaximization ( EM ) algorithm USED-FOR maximum a posteriori ( MAP ) estimate. accuracy EVALUATE-FOR algorithm. synthetic and real data EVALUATE-FOR algorithm. algorithm USED-FOR temporal dynamics of interaction. algorithm USED-FOR interpretable functional connectivity. interpretable functional connectivity FEATURE-OF neural spike trains. real neural recordings EVALUATE-FOR algorithm. ,"This paper proposes a statistical framework for measuring the timedependent interaction of neuronal spiking activities based on the Hawkes process. The influence pattern of the influence pattern is modeled by the nonlinear Hawkes processes, and the authors propose an iterative algorithm based on analytical updates. The functional connection weights are modeled in Gaussian form, with auxiliary latent variables such as sparsity variables, latent marked Poisson processes and Pólya-Gamma variables. The expectationmaximization (EM) algorithm is used to estimate the maximum a posteriori (MAP) estimate. The authors show that the proposed algorithm is able to capture the temporal dynamics of interaction in neural spike trains with interpretable functional connectivity. The algorithm is evaluated on synthetic and real data.","This paper proposes a statistical framework for measuring the timedependent interaction of neuronal spiking activities using the Hawkes process. The influence pattern is modeled by the nonlinear Hawkes Process, where the influence pattern depends on excitatory or inhibitory interactions. The authors propose an iterative algorithm that uses analytical updates to estimate the functional connection weights in the Gaussian form. The auxiliary latent variables are Pólya-Gamma variables, latent marked Poisson processes, and sparsity variables. The maximum a posteriori (MAP) estimate is computed using the expectationmaximization (EM) algorithm. The algorithm is evaluated on synthetic and real data, and is shown to provide interpretable functional connectivity for neural spike trains."
864,SP:1156d3deac022829bda930ffcb081947609d972b,"gradient descent ( GD ) algorithm USED-FOR two - layer neural network models. under - parameterized regime FEATURE-OF GD dynamics. quenched ” neurons USED-FOR continued activation and deactivation process. quenching - activation process USED-FOR GD. random featurelike behavior FEATURE-OF it. quenching process USED-FOR implicit regularization ”. mean - field ” scaling FEATURE-OF GD dynamics. OtherScientificTerm are parameter regimes, neural network - like behavior, and inner - layer parameters. Method is random feature model. Generic is dynamics. ","This paper studies the gradient descent (GD) algorithm for two-layer neural network models. The authors consider the under-parameterized regime of the GD dynamics in the random feature model. They show that it has random featurelike behavior, and that the quenching-activation process of GD with “quenched” neurons leads to a continued activation and deactivation process. They also show that the “implicit regularization” of GD dynamics with quenched process leads to “mean-field” scaling in the parameter regimes. ","This paper proposes a gradient descent (GD) algorithm for two-layer neural network models. The authors show that GD dynamics in the under-parameterized regime can be decomposed into two parameter regimes, where the parameter regimes have neural network-like behavior (i.e. random featurelike behavior) and the inner-layer parameters have a “quenched” behavior. The main contribution of the paper is to show that the quenching-activation process of GD is equivalent to the “implicit regularization” of “continued activation and deactivation process” with quenched ” neurons. The dynamics of GD dynamics are shown to be invariant to mean-field “scaling”, and the authors also provide a random feature model."
873,SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"constrained Markov decision process ( CMDP ) problems USED-FOR reinforcement learning problems. model USED-FOR CMDP problem. reconnaissance MDP ( R - MDP ) CONJUNCTION planning MDP ( P - MDP ). planning MDP ( P - MDP ) CONJUNCTION reconnaissance MDP ( R - MDP ). MDPs PART-OF CMDP. planning MDP ( P - MDP ) HYPONYM-OF MDPs. reconnaissance MDP ( R - MDP ) HYPONYM-OF MDPs. threat function CONJUNCTION Q - function analogue of danger. Q - function analogue of danger CONJUNCTION threat function. threat function USED-FOR R - MDP. reward - seeking policy USED-FOR P - MDP. fixed threat function USED-FOR reward - seeking policy. generative model USED-FOR threat function. reward CONJUNCTION danger - constraint. danger - constraint CONJUNCTION reward. threat function USED-FOR baseline policy. reward FEATURE-OF CMDP problems. danger - constraint FEATURE-OF CMDP problems. approximation method USED-FOR R - MDP. approximation method USED-FOR threat function. method COMPARE approaches. approaches COMPARE method. benchmark dataset CONJUNCTION complex collision - free navigation tasks. complex collision - free navigation tasks CONJUNCTION benchmark dataset. complex collision - free navigation tasks EVALUATE-FOR method. complex collision - free navigation tasks EVALUATE-FOR approaches. benchmark dataset EVALUATE-FOR method. benchmark dataset EVALUATE-FOR approaches. OtherScientificTerm are prescribed safety constraints, and state - action pair. ","This paper studies constrained Markov decision process (CMDP) problems in reinforcement learning problems. The authors propose a model to solve the CMDP problem with prescribed safety constraints. The MDPs in CMDP are reconnaissance MDP (R-MDP) and planning MDP [1], which are both MDP with MDP constraints. In R- MDP, the reward-seeking policy is a fixed threat function, and in P-MPD, the threat function is a Q-function analogue of danger. The paper proposes an approximation method to the R - MDP by using a generative model to predict the threat of the state-action pair. The proposed method is evaluated on a benchmark dataset and complex collision-free navigation tasks, and shows that the proposed method performs better than existing approaches.","This paper studies constrained Markov decision process (CMDP) problems for reinforcement learning problems. The authors propose a model to solve the CMDP problem. The CMDP consists of two MDPs: reconnaissance MDP (R-MDP) and planning MDP(P-DPP). The MDP of CMDPs are defined by prescribed safety constraints. The P-DP is a reward-seeking policy, and the R-DPM is a fixed threat function. The paper proposes a generative model to learn the threat function and the Q-function analogue of danger. The proposed method is evaluated on three CMDP problems: reward, danger-constraint, and complex collision-free navigation tasks. The results show that the proposed method outperforms other approaches on the benchmark dataset and on the complex collisions. The main contribution of the paper is to propose a new baseline policy that is based on the proposed threat function, and an approximation method for the R - MDP. "
882,SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,neural architectures USED-FOR classification tasks. crossentropy loss COMPARE square loss. square loss COMPARE crossentropy loss. crossentropy loss USED-FOR neural architectures. benchmark datasets USED-FOR NLP. automatic speech recognition ( ASR ) CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION automatic speech recognition ( ASR ). neural architectures CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION neural architectures. neural architectures USED-FOR NLP. benchmark datasets USED-FOR automatic speech recognition ( ASR ). NLP CONJUNCTION automatic speech recognition ( ASR ). automatic speech recognition ( ASR ) CONJUNCTION NLP. hyper - parameter settings USED-FOR architectures. square loss USED-FOR architectures. square loss USED-FOR NLP. Cross - entropy USED-FOR computer vision tasks. square loss USED-FOR classification. cross - entropy FEATURE-OF deep learning. equal footing FEATURE-OF deep learning. OtherScientificTerm is cross - entropy loss. Task is non - vision tasks. ,"This paper proposes a new cross-entropy loss for neural architectures for classification tasks. The authors show that the cross-Entropy loss is equivalent to the square loss in NLP and can be used to train neural architectures on benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks. They also show that these architectures can be trained in hyper-parameter settings. They show that this square loss can improve the performance of NLP on classification with cross-estentropy. ","This paper proposes a novel cross-entropy loss for classification tasks, which is a variant of the square loss used in neural architectures for NLP. The authors show that the proposed loss is equivalent to square loss in NLP in hyper-parameter settings. The paper also shows that the cross-Entropy loss can be applied to non-vision tasks as well. The experiments are conducted on a variety of benchmark datasets, including automatic speech recognition (ASR), NLP, and computer vision tasks. "
891,SP:915f1f0fc4850507c28c1d609239b41775863ebe,"self - supervised objectives FEATURE-OF reward maximization. exponential moving average USED-FOR encoder. encoder USED-FOR target representations. prior methods USED-FOR sample - efficient deep RL. future prediction objective COMPARE prior methods. prior methods COMPARE future prediction objective. data augmentation USED-FOR future prediction loss. future prediction CONJUNCTION data augmentation. data augmentation CONJUNCTION future prediction. median human - normalized score EVALUATE-FOR self - supervised objective. Atari EVALUATE-FOR self - supervised objective. future prediction PART-OF self - supervised objective. data augmentation PART-OF self - supervised objective. SPR COMPARE expert human scores. expert human scores COMPARE SPR. limited data regime EVALUATE-FOR SPR. Method are deep reinforcement learning, Self - Predictive Representations ( SPR ), and transition model. OtherScientificTerm are limited interaction, latent state representations, agent ’s representations, and environment interaction. Generic are method, and state - of - the - art. ","This paper studies the problem of self-supervised objectives for reward maximization in deep reinforcement learning. The authors propose Self-Predictive Representations (SPR), a method to learn the latent state representations of the agent’s representations. The encoder is trained using an exponential moving average, and the target representations are learned using a transition model. The paper shows that SPR outperforms prior methods in sample-efficient deep RL, and achieves a median human-normalized score on Atari. SPR also outperforms expert human scores in a limited data regime.","This paper proposes a new method for deep reinforcement learning, called Self-Predictive Representations (SPR), which is an extension of prior methods for sample-efficient deep RL. The key idea of the method is to learn a set of latent state representations of the agent’s representations, and then use an exponential moving average of the encoder to predict the target representations. The authors show that the proposed self-supervised objective can achieve a median human-normalized score on Atari, outperforming prior methods on the future prediction objective and data augmentation for future prediction loss. They also show that SPR outperforms expert human scores in the limited data regime. "
900,SP:983f01c170909c8c67fd3be25f121bd61bdd8307,method USED-FOR generating single - node representations. InstantEmbedding USED-FOR generating single - node representations. InstantEmbedding HYPONYM-OF method. local PageRank computations USED-FOR InstantEmbedding. local PageRank computations USED-FOR method. approach USED-FOR globally consistent representations. DeepWalk CONJUNCTION node2vec. node2vec CONJUNCTION DeepWalk. node2vec CONJUNCTION VERSE. VERSE CONJUNCTION node2vec. VERSE CONJUNCTION FastRP. FastRP CONJUNCTION VERSE. InstantEmbedding COMPARE methods. methods COMPARE InstantEmbedding. InstantEmbedding USED-FOR single node ’s embedding. computation time CONJUNCTION memory. memory CONJUNCTION computation time. FastRP HYPONYM-OF methods. DeepWalk HYPONYM-OF methods. VERSE HYPONYM-OF methods. node2vec HYPONYM-OF methods. computation time EVALUATE-FOR InstantEmbedding. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. unsupervised representation learning USED-FOR tasks. unsupervised representation learning USED-FOR node classification. unsupervised representation learning USED-FOR link prediction. method USED-FOR representations. method COMPARE unsupervised representation learning. unsupervised representation learning COMPARE method. link prediction HYPONYM-OF tasks. node classification HYPONYM-OF tasks. social networks CONJUNCTION chemical molecules. chemical molecules CONJUNCTION social networks. chemical molecules CONJUNCTION knowledge graphs. knowledge graphs CONJUNCTION chemical molecules. approach USED-FOR graphs. node PART-OF graph. d - dimensional embedding vector USED-FOR node. compact representations of graphs USED-FOR approach. d - dimensional embedding vector USED-FOR graph. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. visualization CONJUNCTION node classification. node classification CONJUNCTION visualization. Unsupervised embeddings USED-FOR machine learning tasks. visualization HYPONYM-OF machine learning tasks. link prediction HYPONYM-OF machine learning tasks. node classification HYPONYM-OF machine,"This paper proposes a method for generating single-node representations from a single node’s embedding. InstantEmbedding uses local PageRank computations to compute the embedding of each node in a graph. The proposed approach is based on compact representations of graphs, which can be used to generate globally consistent representations. The authors show that the proposed method is able to learn representations that are globally consistent across different nodes in a given graph. They also show that their method can be combined with existing unsupervised embeddings to achieve state-of-the-art performance on a variety of machine learning tasks such as node classification, link prediction, and visualization. ","This paper presents a method for generating single-node representations. InstantEmbedding is a method to learn a single node’s embedding. The proposed method is based on local PageRank computations. The authors propose a new approach to learn globally consistent representations for each node in a graph. The approach builds on compact representations of graphs, where each node is represented as a d-dimensional embedding vector, and each node PART-of the graph is represented by a different type of compact representations. The method is evaluated on a number of machine learning tasks, including unsupervised embeddings, node classification, link prediction, and visualization. The results show that the proposed method can learn representations that are globally consistent, and that the computation time and memory can be reduced significantly compared to other methods such as DeepWalk, node2vec, and FastRP."
909,SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"Graph coarsening USED-FOR graph. deep learning on graphs USED-FOR graph coarsening. Laplace operator CONJUNCTION projection / lift operators. projection / lift operators CONJUNCTION Laplace operator. Laplace operator USED-FOR coarse graph. framework USED-FOR coarsening algorithm. edge weight USED-FOR coarse graph. it USED-FOR coarsening quality. graph neural networks USED-FOR weight assignment map. reduction ratios CONJUNCTION graph sizes. graph sizes CONJUNCTION reduction ratios. graph sizes CONJUNCTION graph types. graph types CONJUNCTION graph sizes. method COMPARE graph coarsening methods. graph coarsening methods COMPARE method. metrics CONJUNCTION reduction ratios. reduction ratios CONJUNCTION metrics. synthetic and real networks EVALUATE-FOR method. metrics EVALUATE-FOR graph coarsening methods. metrics EVALUATE-FOR method. reduction ratios EVALUATE-FOR method. It USED-FOR graphs. OtherScientificTerm are large - scale graphs, and essential properties. Material is large graph data. Method is data - driven methods. ","This paper proposes a new graph coarsening algorithm based on deep learning on graphs. The main idea is to use the Laplace operator and the projection/lift operators to learn a coarse graph with edge weight. The authors propose a framework to learn the coarsens of the coarse graph using the framework. The proposed method is evaluated on both synthetic and real networks. The method is shown to achieve better performance than other popular graph-coarsening methods on several metrics including reduction ratios, graph sizes, and graph types. ","This paper proposes a novel framework for graph coarsening on graphs. The main idea is to use deep learning on graphs to improve the coarsens quality of the graph. The proposed framework is based on the Laplace operator and the projection/lift operators. The authors propose a coarse graph with edge weight that is defined as the sum of the edge weight of the coarse graph and the weight assignment map computed by graph neural networks. It can be applied to large-scale graphs. It is shown that the proposed method outperforms the existing data-driven methods on several metrics, including graph sizes, graph sizes and graph types. "
918,SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,localization CONJUNCTION acoustic scene analysis. acoustic scene analysis CONJUNCTION localization. environmental acoustic effects CONJUNCTION localization. localization CONJUNCTION environmental acoustic effects. Acoustic properties USED-FOR 3D audio content creation. 3D audio content creation CONJUNCTION environmental acoustic effects. environmental acoustic effects CONJUNCTION 3D audio content creation. environmental acoustic effects CONJUNCTION acoustic scene analysis. acoustic scene analysis CONJUNCTION environmental acoustic effects. Acoustic properties USED-FOR environmental acoustic effects. scattering characteristics FEATURE-OF Acoustic properties. numeric solvers USED-FOR acoustic properties. numeric solvers USED-FOR interactive applications. geometric deep learning algorithm USED-FOR characteristics. characteristics USED-FOR 3D objects. interactive rates FEATURE-OF 3D objects. discrete - laplacian and implicit encoders USED-FOR geometric deep learning algorithm. multi - layer network USED-FOR acoustic properties. multi - layer network USED-FOR arbitrary topologies. arbitrary topologies FEATURE-OF acoustic properties. NVIDIA GeForce RTX 2080 Ti GPU USED-FOR multi - layer network. accuracy EVALUATE-FOR learning method. dynamic environments FEATURE-OF generating environmental acoustic effects. Method is point cloud approximation. OtherScientificTerm is high - dimensional latent space. ,"This paper studies the problem of generating environmental acoustic effects in dynamic environments. Acoustic properties such as scattering characteristics of 3D audio content creation, acoustic scene analysis, and localization are of great interest in the field. The paper proposes a geometric deep learning algorithm that learns these characteristics by combining discrete-laplacian and implicit encoders, and then uses numeric solvers to learn acoustic properties for interactive applications. The proposed multi-layer network is able to learn arbitrary topologies, and the proposed learning method achieves state-of-the-art accuracy.","This paper proposes a geometric deep learning algorithm to learn acoustic properties for 3D audio content creation, localization, acoustic scene analysis, and environmental acoustic effects. Acoustic properties are defined as scattering characteristics of the acoustic acoustic effects in the high-dimensional latent space. These characteristics can be used for generating 3D objects with interactive rates at different interactive applications (e.g., point cloud approximation). The key idea is to use discrete-laplacian and implicit encoders to learn the characteristics. The multi-layer network is used to learn arbitrary topologies for generating acoustic properties. The proposed learning method is evaluated on NVIDIA GeForce RTX 2080 Ti GPU."
927,SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"model USED-FOR extreme distributional shifts. extreme distributional shifts FEATURE-OF model ’s sensitivity. model ’s sensitivity EVALUATE-FOR model. robust optimization USED-FOR Risk Extrapolation ( REx ). perturbation set of extrapolated domains ( MMREx ) USED-FOR robust optimization. REx USED-FOR causal mechanisms. causally induced distributional shifts CONJUNCTION covariate shift. covariate shift CONJUNCTION causally induced distributional shifts. REx COMPARE methods. methods COMPARE REx. robustness EVALUATE-FOR REx. covariate shift FEATURE-OF robustness. causally induced distributional shifts FEATURE-OF robustness. Invariant Risk Minimization HYPONYM-OF methods. Task is Distributional shift. Method is machine learning prediction systems. OtherScientificTerm is causal and anti - causal elements. Generic are approach, and variant. ","This paper studies the problem of Risk Extrapolation (REx) in machine learning prediction systems, where the model’s sensitivity to extreme distributional shifts can be affected by the number of causal and anti-causal elements. The authors propose a robust optimization based on the perturbation set of extrapolated domains (MMREx) for robust optimization. They show that REx improves the robustness of REx against causally induced distributional shift and covariate shift in terms of robustness. They also show that their approach is more robust than existing methods such as Invariant Risk Minimization.","The paper proposes a new model for extreme distributional shifts in machine learning prediction systems. The model’s sensitivity is measured by minimizing the model ‘s sensitivity to the model. The approach is based on Risk Extrapolation (REx), a robust optimization based on the perturbation set of extrapolated domains (MMREx). REx is able to capture causal mechanisms and covariate shift. The authors also propose a variant of REx, Invariant Risk Minimization (IRM), which can capture both causal and anti-causal elements. The proposed REx outperforms other methods in terms of robustness to causally induced distributionsal shifts and covariates."
936,SP:411d5bcf7698d534ad60f581d479ff74849ba4de,neural networks USED-FOR mappings between finite - dimensional Euclidean spaces. this USED-FOR neural operators. neural operators USED-FOR mappings between function spaces. neural operators USED-FOR mapping. neural operators USED-FOR partial differential equations ( PDEs ). they USED-FOR PDEs. Fourier space FEATURE-OF integral kernel. integral kernel USED-FOR neural operator. Burgers ’ equation CONJUNCTION Darcy flow. Darcy flow CONJUNCTION Burgers ’ equation. Darcy flow CONJUNCTION Navier - Stokes equation. Navier - Stokes equation CONJUNCTION Darcy flow. Fourier neural operator HYPONYM-OF ML - based method. Fourier neural operator USED-FOR turbulent flows. ML - based method USED-FOR turbulent flows. zero - shot super - resolution FEATURE-OF Fourier neural operator. zero - shot super - resolution FEATURE-OF turbulent flows. It COMPARE PDE solvers. PDE solvers COMPARE It. it COMPARE learning - based solvers. learning - based solvers COMPARE it. accuracy EVALUATE-FOR learning - based solvers. fixed resolution FEATURE-OF learning - based solvers. fixed resolution FEATURE-OF it. accuracy EVALUATE-FOR it. OtherScientificTerm is functional parametric dependence. Generic is architecture. ,"This paper proposes a new architecture for learning partial differential equations (PDEs) by using neural networks to learn mappings between finite-dimensional Euclidean spaces. The neural operators are used to learn the mapping between function spaces using neural operators, and they are then used to solve PDEs. The authors propose a new ML-based method, called Fourier neural operator, which is based on the integral kernel in the Fourier space of the neural operator. It achieves better accuracy than PDE solvers in terms of fixed resolution and zero-shot super-resolution for turbulent flows. ","This paper proposes a new architecture for learning partial differential equations (PDEs) in finite-dimensional Euclidean spaces. The key idea is to use neural operators to learn mappings between function spaces, and then use this mapping to train neural networks to solve PDEs. The authors show that the integral kernel of the neural operator in the Fourier space is a function of the functional parametric dependence. They also show that they can solve the PDE with a simple ML-based method that can be applied to turbulent flows with zero-shot super-resolution. It is shown to outperform PDE solvers with fixed resolution."
945,SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"gradient flow USED-FOR linear neural network training. infinitesimal step size FEATURE-OF gradient descent. gradient descent HYPONYM-OF gradient flow. convergence direction FEATURE-OF network parameters. formulation USED-FOR convergence direction. singular vectors PART-OF tensor. network USED-FOR tensor. singular vectors USED-FOR convergence direction. tensor USED-FOR convergence direction. gradient flow USED-FOR separable classification. gradient flow USED-FOR ` 2 / L max - margin problem. transformed ” input space FEATURE-OF ` 2 / L max - margin problem. network USED-FOR transformed ” input space. orthogonally decomposable FEATURE-OF L - layer linear tensor networks. gradient flow USED-FOR global minimum. transformed input space FEATURE-OF weighted ` 1 and ` 2 norms. norm - like function FEATURE-OF global minimum. transformed input space FEATURE-OF norm - like function. weighted ` 1 and ` 2 norms FEATURE-OF norm - like function. gradient flow USED-FOR underdetermined regression. Method are tensor formulation of neural networks, and linear tensor networks. OtherScientificTerm is convergence assumptions. ","This paper studies the tensor formulation of neural networks. The authors propose a gradient flow for linear neural network training with infinitesimal step size. The gradient flow is based on gradient descent with singular vectors in the gradient flow. The convergence direction of the network parameters depends on the singular vectors of a tensor, and the network is trained to predict the convergence direction using the singular vector. The paper shows that gradient flow can be used for separable classification in the `2/L max-margin problem in the “transformed” input space of a “linear tensor networks” with orthogonally decomposable input space. The global minimum of gradient flow in the transformed input space is a weighted `1 and `2 norms’ with a norm-like function.  The paper also provides convergence assumptions for underdetermined regression. ","This paper proposes a new tensor formulation of neural networks, which is based on gradient flow for linear neural network training with infinitesimal step size. The authors show that the convergence direction of the network parameters depends on the singular vectors of the tensor. The convergence direction is defined as the gradient descent of the gradient flow with respect to the number of steps. The paper also shows that gradient flow can be used to solve the `2/L max-margin problem in the “transformed” input space and in the transformed “input space” of linear tensor networks with orthogonally decomposable. The global minimum of the norm-like function of the transformed input space is defined in terms of the weighted `1 and `2 norms. "
954,SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"prediction error CONJUNCTION computational cost. computational cost CONJUNCTION prediction error. storage cost EVALUATE-FOR model. FLOPs HYPONYM-OF computational cost. They USED-FOR resource - constrained settings. mobile devices HYPONYM-OF resource - constrained settings. slimmable neural networks USED-FOR sub - networks. width - multiplier USED-FOR sub - networks. performance profiles FEATURE-OF sub - networks. prediction accuracy EVALUATE-FOR network. width - multiplier USED-FOR slimmable neural networks. approach USED-FOR slimmable networks. approach USED-FOR widthmultipliers. shared weights CONJUNCTION width - multipliers. width - multipliers CONJUNCTION shared weights. algorithm USED-FOR shared weights. algorithm USED-FOR width - multipliers. width - multipliers USED-FOR sub - networks. algorithm USED-FOR sub - networks. multiobjective optimization lens USED-FOR slimmable networks. FLOPs CONJUNCTION memory footprint. memory footprint CONJUNCTION FLOPs. method COMPARE alternatives. alternatives COMPARE method. network and dataset combinations CONJUNCTION cost objectives. cost objectives CONJUNCTION network and dataset combinations. FLOPs HYPONYM-OF cost objectives. memory footprint HYPONYM-OF cost objectives. FLOPs CONJUNCTION memory footprint. memory footprint CONJUNCTION FLOPs. ImageNet dataset EVALUATE-FOR MobileNetV2. channel counts USED-FOR layers. Method is Slimmable neural networks. OtherScientificTerm are FLOP requirements, and heterogeneous width - multipliers. Task is optimizing slimmable networks. Metric is top-1 accuracy. ","This paper studies the problem of optimizing slimmable networks. Slimmable neural networks have been a popular topic in recent years. They are particularly useful in resource-constrained settings such as mobile devices, where the number of devices is limited and the storage cost of the model is high. The authors propose a multiobjective optimization lens to optimize slimable networks with heterogeneous width-multipliers. The proposed approach is based on the idea that the width multipliers are shared weights and width-multiers can be used to optimize sub-networks with different performance profiles. The main contribution of the paper is to show that the proposed algorithm can be combined with width- multipliers to improve the performance of sub-neighboring networks. The method is evaluated on MobileNetV2 on the ImageNet dataset and compared to alternatives. ","This paper proposes Slimmable neural networks, a new approach for optimizing slimmable neural networks. They are designed for resource-constrained settings (e.g., mobile devices). The authors propose a multiobjective optimization lens to optimize sub-networks with different performance profiles. The main idea is to use the width-multipliers of the sub-network to optimize the shared weights and width- multipliers of shared weights. The authors show that the proposed method outperforms other alternatives in terms of prediction accuracy, computational cost, and storage cost. The proposed method is evaluated on MobileNetV2 on the ImageNet dataset. "
963,SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"Federated SemiSupervised Learning ( FSSL ) HYPONYM-OF federated learning problem. scenarios PART-OF FSSL. method USED-FOR problems. labeled and unlabeled data USED-FOR disjoint learning. inter - client consistency loss USED-FOR FedMatch. federated learning and semi - supervised learning approaches USED-FOR FedMatch. federated learning CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION federated learning. method COMPARE baselines. baselines COMPARE method. method COMPARE local semi - supervised learning. local semi - supervised learning COMPARE method. local semi - supervised learning CONJUNCTION baselines. baselines CONJUNCTION local semi - supervised learning. method COMPARE method. method COMPARE method. federated learning USED-FOR baselines. semi - supervised learning USED-FOR baselines. Method is federated learning approaches. Metric is labeling cost. Task are annotation, and Federated Matching ( FedMatch ). OtherScientificTerm is expert knowledge. Material are private data, and labeled data. Generic is scenario. ","This paper studies federated semi-supervised learning (FSL) in the federated learning setting, where each client has access to a subset of the server’s data. The goal is to learn a set of expert knowledge that can be used to train a federated model. The authors propose a method called Federated Semi-Supervised Matching (FedMatch) that uses the inter-client consistency loss to improve the performance of FedMatch. They show that FedMatch outperforms the baselines in terms of performance on a variety of tasks.",This paper proposes a federated semi-supervised learning (FSSL) framework for disjoint learning. The proposed framework is based on the Federated Matching (FedMatch) framework. FedMatch uses an inter-client consistency loss to ensure that the expert knowledge is shared across clients. The authors show that FedMatch outperforms other federated learning methods on a number of tasks. 
972,SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,real - world users FEATURE-OF discrete event sequences. discrete event sequences USED-FOR self - supervised learning. low - dimensional fixed - length vector representations USED-FOR downstream machine learning tasks. low - dimensional fixed - length vector representations USED-FOR Self - supervised learning. contrastive learning USED-FOR audio and computer vision domains. CoLES USED-FOR discrete event sequences domain. self - supervised setting USED-FOR discrete event sequences domain. contrastive learning USED-FOR CoLES. augmentation method USED-FOR discrete event sequences. augmentation method USED-FOR CoLES. CoLES USED-FOR discrete event sequences. CoLES representations COMPARE methods. methods COMPARE CoLES representations. downstream tasks EVALUATE-FOR CoLES representations. downstream tasks EVALUATE-FOR methods. public datasets EVALUATE-FOR CoLES. ,"This paper studies the problem of self-supervised learning with discrete event sequences for real-world users. The authors propose CoLES, a low-dimensional fixed-length vector representations for downstream machine learning tasks based on contrastive learning. CoLES can be used in the discrete event sequence domain in a self supervised setting, where the goal is to learn a sequence of discrete events from a set of events. The main contribution of the paper is a new augmentation method for learning discrete events sequences using CoLES. Experiments on public datasets show that CoLES representations perform better than existing methods on downstream tasks.",This paper proposes a novel approach to learning discrete event sequences for real-world users. Self-supervised learning uses low-dimensional fixed-length vector representations for downstream machine learning tasks. CoLES is an extension of contrastive learning for audio and computer vision domains. The proposed CoLES can be applied to the discrete event sequence domain in the self-supervision setting. The authors propose an augmentation method for learning discrete events sequences using CoLES. The experimental results show that CoLES representations perform better on downstream tasks than other methods on public datasets.
981,SP:385942a5bcee7384bb722a1669b541f2fac0cd36,constituency grammar USED-FOR assembly of one or several corresponded words. dependency grammar CONJUNCTION constituency grammar. constituency grammar CONJUNCTION dependency grammar. constituency grammar HYPONYM-OF natural language grammars. dependency grammar HYPONYM-OF natural language grammars. StructFormer USED-FOR dependency and constituency structure. model USED-FOR dependency and constituency structure. StructFormer HYPONYM-OF model. constituency tree CONJUNCTION dependency graph. dependency graph CONJUNCTION constituency tree. parsing framework USED-FOR constituency tree. parsing framework USED-FOR dependency graph. induced dependency relations PART-OF transformer. dependency - constrained self - attention mechanism USED-FOR transformer. unsupervised dependency parsing CONJUNCTION masked language modeling. masked language modeling CONJUNCTION unsupervised dependency parsing. unsupervised constituency parsing CONJUNCTION unsupervised dependency parsing. unsupervised dependency parsing CONJUNCTION unsupervised constituency parsing. model USED-FOR unsupervised constituency parsing. model USED-FOR unsupervised dependency parsing. model USED-FOR masked language modeling. Method is unsupervised parsing methods. ,"This paper proposes a new model, StructFormer, for unsupervised dependency parsing and constituency parsing in natural language grammars such as dependency grammar and constituency grammar. The proposed model is based on a parsing framework that constructs a constituency tree and a dependency graph using a dependency-constrained self-attention mechanism. The authors also propose a transformer based on induced dependency relations in the transformer. Experiments show the effectiveness of the proposed model in unstructured and masked language modeling, as well as in unclassified constituency parsing. ","This paper proposes a model for unsupervised dependency parsing and masked language modeling. The model is based on StructFormer, a parsing framework for the assembly of one or several corresponded words with dependency grammar and constituency grammar. The transformer consists of a dependency tree, a dependency graph, and a dependency-constrained self-attention mechanism. The induced dependency relations in the transformer are modeled by the parsing framework. Experiments show that the proposed model outperforms the state-of-the-art in both the case where the dependency and constituency structure are learned separately, and in the case when the dependency graph is learned together. "
990,SP:078966ff62775bba6031e47d374bda95f4a7dde3,images USED-FOR structured representations. nodes of scene graphs CONJUNCTION object bounding boxes. object bounding boxes CONJUNCTION nodes of scene graphs. annotated mapping USED-FOR nodes of scene graphs. annotated mapping USED-FOR methods. scene graph nodes CONJUNCTION visual objects. visual objects CONJUNCTION scene graph nodes. object features CONJUNCTION relational features. relational features CONJUNCTION object features. visual objects CONJUNCTION scene graph nodes. scene graph nodes CONJUNCTION visual objects. Visual Genome ( VG ) CONJUNCTION Visual Relation Detection ( VRD ) datasets. Visual Relation Detection ( VRD ) datasets CONJUNCTION Visual Genome ( VG ). model COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE model. scene graph grounding task EVALUATE-FOR state - of - the - art approaches. scene graph grounding task EVALUATE-FOR model. scene graph parsing task EVALUATE-FOR method. scene graph parsing task EVALUATE-FOR model. model COMPARE method. method COMPARE model. OtherScientificTerm is weak supervision. ,"This paper proposes a method for learning structured representations from images. The proposed method is based on an annotated mapping of the nodes of scene graphs and object bounding boxes. The authors show that the proposed method can be used to train a model on the scene graph grounding task, which is a popular scene graph parsing task. The method is evaluated on Visual Genome (VG) and Visual Relation Detection (VRD) datasets and shows that it outperforms state-of-the-art approaches.",This paper proposes a new way to learn structured representations from images. The authors propose an annotated mapping for the nodes of scene graphs and object bounding boxes. The proposed methods are based on the idea of weak supervision. The model is evaluated on Visual Genome (VG) and Visual Relation Detection (VRD) datasets and compared to state-of-the-art approaches on a scene graph grounding task. 
999,SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"Relational regularized autoencoder ( RAE ) USED-FOR distribution of data. framework USED-FOR distribution of data. reconstruction loss CONJUNCTION relational regularization. relational regularization CONJUNCTION reconstruction loss. Relational regularized autoencoder ( RAE ) HYPONYM-OF framework. relational regularization FEATURE-OF latent space. sliced fused GromovWasserstein ( SFG ) USED-FOR distributions. discrepancy CONJUNCTION relational regularization. relational regularization CONJUNCTION discrepancy. relational discrepancy USED-FOR discrepancy. spherical sliced fused Gromov Wasserstein ( SSFG ) HYPONYM-OF relational discrepancy. mixture of von Mises - Fisher distributions USED-FOR vMF distribution. power spherical distribution USED-FOR sampling time. high dimension settings FEATURE-OF sampling time. power spherical distribution USED-FOR vMF distribution. discrepancies USED-FOR variants. discrepancies USED-FOR RAE framework. image generation CONJUNCTION reconstruction. reconstruction CONJUNCTION image generation. learning latent manifold structure CONJUNCTION image generation. image generation CONJUNCTION learning latent manifold structure. autoencoders USED-FOR learning latent manifold structure. autoencoders USED-FOR image generation. autoencoders USED-FOR reconstruction. OtherScientificTerm are inner discrepancy, von Mises - Fisher distribution, and latent manifold structure. Generic are approach, it, and variant. Task is discriminative task. Method is SSFG. ","This paper proposes a new framework for learning the distribution of data using a Relational regularized autoencoder (RAE) that is able to learn a distribution over the latent space. The proposed approach is based on a spherical sliced fused GromovWasserstein (SFG) which is a mixture of von Mises-Fisher distributions. The main contribution of the paper is that it uses the inner discrepancy between the discrepancy and the relational discrepancy between a reconstruction loss and relational regularization. The authors also propose a variant of the SSFG, which uses the power spherical distribution to reduce the sampling time in high dimension settings. Experiments show that the proposed RAE framework outperforms the state-of-the-art in terms of discrepancies. ",This paper proposes a novel framework for learning the distribution of data using Relational regularized autoencoder (RAE) for a discriminative task. The key idea of the approach is to use sliced fused GromovWasserstein (SFG) to learn distributions of data with inner discrepancy and relational regularization in the latent space. The authors propose a variant of SSFG called spherical sliced fused gromov Wasserstein-Fisher (SSFG) where the inner discrepancy is a mixture of von Mises- Fisher distribution and relational discrepancy is the discrepancy between the two distributions. The sampling time of the vMF distribution in high dimension settings is computed using a power spherical distribution. The proposed RAE framework is evaluated on two datasets and two variants of the proposed discrepancies are compared. The results show that it is able to learn the latent manifold structure and learn autoencoders for image generation and reconstruction.
1008,SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"natural language processing CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION natural language processing. computational costs CONJUNCTION training time. training time CONJUNCTION computational costs. approach USED-FOR training. training USED-FOR deep networks. approach USED-FOR deep networks. repeated structures PART-OF deep networks. transformer module HYPONYM-OF repeated structures. deep linear networks USED-FOR theoretic analysis. theoretic analysis USED-FOR adaptive untying criterion. deep linear networks USED-FOR adaptive untying criterion. method USED-FOR BERT. training time EVALUATE-FOR BERT. training time EVALUATE-FOR method. OtherScientificTerm are deep learning model sizes, repeated layers, weight sharing, and monitoring gradient statistics. Method is deep network. ",This paper proposes a new approach for training deep networks with repeated structures in the transformer module. The key idea is to use deep linear networks as a theoretic analysis for the adaptive untying criterion. The authors show that the proposed method can reduce the training time of BERT and reduce the computational costs of training. They also provide a theoretical analysis of the effect of repeated layers on the performance of the deep learning model sizes. ,This paper proposes a new approach for training deep networks with repeated structures in the transformer module. The key idea is to reduce the computational costs and training time of the deep learning model sizes. The authors propose a new adaptive untying criterion based on the theoretic analysis of deep linear networks. The proposed method is evaluated on BERT and on natural language processing and computer vision tasks. 
1017,SP:a51710551142316b67e2fccd969fea1ece35ba39,interaction inside adversarial perturbations USED-FOR adversarial transferability. adversarial transferability CONJUNCTION interaction. interaction CONJUNCTION adversarial transferability. negative correlation FEATURE-OF adversarial transferability. DNNs USED-FOR negative correlation. negative correlation USED-FOR transferability - boosting methods. methods USED-FOR transferability. interactions USED-FOR attacking process. OtherScientificTerm is adversarial perturbations. ,This paper studies the interaction inside adversarial perturbations to improve adversarial transferability and interaction in the context of transferability-boosting methods. The authors show that the negative correlation between the transferability of adversarial transfers and the interaction between the two is a strong indicator of the effectiveness of DNNs for negative correlation. They then propose two methods to improve transferability in the attacking process by using interactions. ,"This paper studies adversarial transferability and interaction inside adversarial perturbations. The authors show that negative correlation is the most important measure of the transferability between adversarial and interaction. They also show that DNNs can be used as negative correlation for transferability-boosting methods. Finally, the authors propose two methods to improve transferability in the attacking process. "
1033,SP:f1565319075c1442c2cb52d96443facb492c06c2,"neural network ( hidden ) representations CONJUNCTION task semantics. task semantics CONJUNCTION neural network ( hidden ) representations. deeper layers USED-FOR forgetting. sequential training USED-FOR task representational subspaces. Methods USED-FOR forgetting. Methods USED-FOR deeper layers. maximal forgetting FEATURE-OF task sequences. forgetting CONJUNCTION task semantic similarity. task semantic similarity CONJUNCTION forgetting. intermediate similarity FEATURE-OF task sequences. Task is Catastrophic forgetting. Method are deep learning models, and neural representations. Generic are some, and others. OtherScientificTerm are feature reuse, task representations, and interference. ","This paper studies Catastrophic forgetting in deep learning models. The authors consider the problem of feature reuse in the context of neural representations and task semantics, where the task representations are learned by a neural network (hidden) representations and the task semantics are encoded in a set of deep layers. They show that the deeper layers can be used for forgetting by sequential training on task representational subspaces. They also show that for maximal forgetting of task sequences with intermediate similarity to the original task sequences, there is no interference between the two tasks.  ","This paper studies Catastrophic forgetting in deep learning models, where the task representations of a neural network (hidden) representations and the task semantics are not the same. The authors propose to use sequential training to learn task representational subspaces, and then use these deeper layers to prevent forgetting. They show that the maximal forgetting of task sequences with intermediate similarity to the original task sequences leads to feature reuse. They also show that some of the deeper layers are more susceptible to forgetting than others. They further show that there is interference between the learning of neural representations and task semantic similarity. "
1049,SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"XLNet CONJUNCTION T5. T5 CONJUNCTION XLNet. BERT CONJUNCTION XLNet. XLNet CONJUNCTION BERT. Deep, heavily overparameterized language models USED-FOR natural language processing ( NLP ) tasks. T5 HYPONYM-OF Deep, heavily overparameterized language models. XLNet HYPONYM-OF Deep, heavily overparameterized language models. BERT HYPONYM-OF Deep, heavily overparameterized language models. training time USED-FOR pre - training and fine - tuning. computation resources CONJUNCTION training time. training time CONJUNCTION computation resources. computation resources USED-FOR model complexity. model compression USED-FOR large NLP models. large batch sizes USED-FOR pre - training time. training algorithm USED-FOR pre - training and fine - tuning. Early - Bird Lottery Tickets USED-FOR computer vision tasks. training algorithm USED-FOR large - scale language models. EarlyBERT HYPONYM-OF training algorithm. pre - training and fine - tuning USED-FOR large - scale language models. self - attention CONJUNCTION fully - connected sub - layers. fully - connected sub - layers CONJUNCTION self - attention. fully - connected sub - layers PART-OF transformer. structured winning tickets PART-OF BERT training. tickets USED-FOR BERT training. GLUE and SQuAD downstream tasks EVALUATE-FOR pre - training and fine - tuning. EarlyBERT COMPARE BERT. BERT COMPARE EarlyBERT. training time EVALUATE-FOR EarlyBERT. training time EVALUATE-FOR BERT. Metric is inference time. OtherScientificTerm are training process, and computational resource demands. ","This paper proposes a new training algorithm for pre-training and fine-tuning large-scale language models such as BERT, XLNet, T5, and Deep, heavily overparameterized language models for natural language processing (NLP) tasks. The proposed training algorithm, called EarlyBERT, is based on the idea of Early-Bird Lottery Tickets for computer vision tasks, where the goal is to train a BERT model with structured winning tickets. The authors show that the training process is computationally efficient, and that the computation resources and the training time can be used to reduce the model complexity and reduce the inference time. The paper also shows that the model compression can improve the performance of large NLP models by reducing the computational resource demands. ","This paper proposes a new training algorithm for large-scale language models, namely BERT, XLNet, T5, and Deep, heavily overparameterized language models for natural language processing (NLP) tasks. The training algorithm is based on the idea of Early-Bird Lottery Tickets for computer vision tasks, where the goal is to maximize the number of tickets for BERT training. The authors propose a transformer that combines self-attention and fully-connected sub-layers in the transformer. The main idea is to reduce the inference time by reducing the computation resources and training time for pre-training and fine-tuning. This is done by minimizing the computational resource demands of the training process. The paper shows that the model compression can reduce the model complexity of large NLP models with large batch sizes. Experiments on GLUE and SQuAD downstream tasks show that the proposed training algorithm improves the performance of BERT and XLNet on both the training time and the final performance of the model. "
1065,SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"classifier ’s predictions CONJUNCTION supervised labels. supervised labels CONJUNCTION classifier ’s predictions. f -divergence measure USED-FOR classifier ’s predictions. variational form USED-FOR decoupling property. variational difference CONJUNCTION bias term. bias term CONJUNCTION variational difference. decoupling property USED-FOR f -divergence measures. clean distribution FEATURE-OF variational difference. variational difference PART-OF divergence. bias term PART-OF divergence. robustness EVALUATE-FOR f -divergence functions. robustness FEATURE-OF f -divergence functions. f -divergence functions USED-FOR metrics. OtherScientificTerm are label noise, noise, and labels ’ noise rate. Task is learning with noisy labels. Generic is they. Material is UCSC - REAL. Metric is Robust - f - divergence - measures. ","This paper studies the problem of learning with noisy labels. The authors propose a new f-divergence measure for classifier’s predictions and supervised labels, which is based on a variational form of the decoupling property of the variational difference between the classifier and the supervised labels.  The authors show that under certain assumptions on the clean distribution and the bias term of the divergence, the f-derivergence measures are robust to label noise. They also show that they are more robust than UCSC-REAL. Finally, the authors provide some theoretical guarantees on the robustness of these metrics.   ",The paper proposes a new f-divergence measure for classifier’s predictions and supervised labels. The main idea is to use a variational form of the decoupling property of the variational difference and the bias term to measure the divergence between the clean distribution and the one with label noise. The authors show that they can achieve UCSC-REINFORCE on learning with noisy labels. Robust-f-d-differential-measure is also proposed. The proposed metrics are evaluated on the robustness of the f-Divergence functions. 
1081,SP:841888179dcdac901889c8d62cb5234311fe28f1,"Q - ensemble USED-FOR uncertainty estimates. uncertainty estimates USED-FOR ensemble - based weighted Bellman backups. method USED-FOR learning. continuous and discrete control benchmarks EVALUATE-FOR method. weighted Bellman backups COMPARE Bellman backups. Bellman backups COMPARE weighted Bellman backups. weighted Bellman backups CONJUNCTION UCB Exploration. UCB Exploration CONJUNCTION weighted Bellman backups. ensemble USED-FOR weighted Bellman backups. off - policy RL algorithms USED-FOR continuous and discrete control tasks. lowdimensional and high - dimensional environments FEATURE-OF continuous and discrete control tasks. Bootstrap USED-FOR diversity. Soft Actor - Critic and Rainbow DQN HYPONYM-OF off - policy RL algorithms. Material is challenging domains. Task is Q - learning. OtherScientificTerm are Q - estimates, and noisy rewards. Metric is signal - to - noise aspect. ",This paper proposes a new method for learning the uncertainty estimates for ensemble-based weighted Bellman backups with Q-ensemble. The proposed method is evaluated on continuous and discrete control benchmarks on both lowdimensional and high-dimensional environments. The authors show that the proposed method can achieve better performance than Bellman backbones on both challenging domains. They also show that their ensemble can outperform weighted bellman backups on UCB Exploration and Soft Actor-Critic and Rainbow DQN. ,"This paper proposes an ensemble-based weighted Bellman backups for estimating uncertainty estimates for Q-learning. The method is evaluated on continuous and discrete control benchmarks on both challenging domains. The authors show that the ensemble outperforms Bellman backup and UCB exploration. They also show that their ensemble is more diverse than Bellman backbones, and that the diversity is due to the signal-to-noise aspect of the Q-estimate. They further show that using Bootstrap improves diversity in terms of the number of samples and the diversity of the training data. The paper also shows that off-policy RL algorithms are able to perform well on both lowdimensional and high-dimensional environments. "
1097,SP:afc08f203562b841180811aef943bfb63a1659ea,"meta - learning algorithms USED-FOR fewshot classification problems. few - shot classification framework USED-FOR modeling uncertainty. meta - training USED-FOR model. class - wise similarities USED-FOR distributional mismatch. meta - learning models PART-OF method. training strategy USED-FOR model. training strategy USED-FOR calibrated classification. Task are prediction of uncertainty, and random sampling of tasks. OtherScientificTerm is dataset shift. Metric is accuracy. ",This paper proposes a new few-shot classification framework for modeling uncertainty in fewshot classification problems. The proposed method uses meta-learning models to train a model using meta-training to learn the distributional mismatch between the class-wise similarities between the training data and the test data. The training strategy is then used to train the model for calibrated classification. The prediction of uncertainty is based on the random sampling of tasks and the dataset shift. The authors show that the proposed method is able to achieve state-of-the-art accuracy.,"This paper proposes a few-shot classification framework for modeling uncertainty in the context of meta-learning algorithms for fewshot classification problems. The key idea is to use class-wise similarities in the dataset to reduce the distributional mismatch between the prediction of uncertainty and the actual accuracy. The proposed method is based on meta-training, where the model is trained using meta-learned models and the training strategy is used to train the model for calibrated classification. The authors show that the proposed method can be applied to a wide range of few shot classification problems, and that the dataset shift can be controlled by random sampling of tasks."
1113,SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"dominant paradigm USED-FOR video - text representations. generative model USED-FOR method. VATEX CONJUNCTION ActivityNet. ActivityNet CONJUNCTION VATEX. ActivityNet CONJUNCTION MSVD. MSVD CONJUNCTION ActivityNet. MSR - VTT CONJUNCTION VATEX. VATEX CONJUNCTION MSR - VTT. method COMPARE others. others COMPARE method. MSVD EVALUATE-FOR method. ActivityNet EVALUATE-FOR method. VATEX EVALUATE-FOR method. MSR - VTT EVALUATE-FOR others. MSR - VTT EVALUATE-FOR method. Method are noise contrastive learning, and dissimilar representations. Generic is representations. OtherScientificTerm are visually similar videos, and depicted action. ","This paper proposes a new method to learn video-text representations using a dominant paradigm. The proposed method is based on a generative model, where the representations are generated from a set of visually similar videos. The idea is to use noise contrastive learning to learn dissimilar representations. The method is evaluated on VATEX, MSVD, and ActivityNet. The results show that the proposed method performs better than others such as MSR-VTT and VATE.","This paper proposes a novel paradigm for learning video-text representations from visually similar videos. The method is based on noise contrastive learning. The authors propose to use a generative model to learn the representations. The proposed method is evaluated on VATEX, MSVD, ActivityNet, and MSR-VTT. The results show that the proposed method outperforms others. "
1129,SP:8a71d8fad25a126aff01431cacf348c05de75667,pre - trained language models ( PLMs ) USED-FOR Chinese natural language processing ( NLP ) tasks. single vocabulary USED-FOR masked language model pre - training. Chinese word segmentation ( CWS ) CONJUNCTION subword tokenization. subword tokenization CONJUNCTION Chinese word segmentation ( CWS ). seg tok USED-FOR Chinese BERT. Chinese word segmentation ( CWS ) USED-FOR method. subword tokenization USED-FOR method. multi - vocabulary pretraining ( MVP ) USED-FOR models expressiveness. char based vocabulary COMPARE seg tok. seg tok COMPARE char based vocabulary. MVP USED-FOR PLMs. seg tok USED-FOR Chinese PLMs. it USED-FOR seg tok. char based vocabulary USED-FOR Chinese PLMs. seg tok COMPARE it. it COMPARE seg tok. sequence labeling tasks EVALUATE-FOR it. sentence level tasks EVALUATE-FOR Chinese PLMs. sequence labeling tasks EVALUATE-FOR seg tok. sentence level tasks EVALUATE-FOR seg tok. OtherScientificTerm is Chinese characters. ,"This paper proposes a method for pre-trained language models (PLMs) for Chinese natural language processing (NLP) tasks with a single vocabulary. The method is based on Chinese word segmentation (CWS) and subword tokenization. The authors propose a masked language model pre-training where the single vocabulary is used to train the models expressiveness by multi-vocabulary pretraining (MVP). The proposed method, seg tok, is used in Chinese BERT to improve the expressiveness of Chinese PLMs. Experiments on sequence labeling tasks and sentence level tasks show that it is able to achieve better performance than seg-tok and char based vocabulary for Chinese PLM.",This paper proposes a method to improve the expressiveness of pre-trained language models (PLMs) for Chinese natural language processing (NLP) tasks using a single vocabulary. The method is based on Chinese word segmentation (CWS) and subword tokenization. The authors also propose multi-vocabulary pretraining (MVP) to improve models expressiveness. Experiments on sequence labeling tasks and sentence level tasks show that MVP outperforms seg tok on Chinese PLMs and a char based vocabulary. 
1145,SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"Graph Convolutional Networks ( GCNs ) USED-FOR graph - based learning tasks. graph partition CONJUNCTION distributed training. distributed training CONJUNCTION graph partition. memory CONJUNCTION communications. communications CONJUNCTION memory. boundary nodes PART-OF partitioned subgraph. BDS - GCN USED-FOR distributed GCN training. method USED-FOR distributed GCN training. BDS - GCN HYPONYM-OF method. unbiased boundary sampling strategy USED-FOR BDS - GCN. full - graph accuracy EVALUATE-FOR method. full - graph accuracy EVALUATE-FOR BDS - GCN. unbiased boundary sampling strategy USED-FOR method. accuracy EVALUATE-FOR state - of - the - art methods. throughput EVALUATE-FOR BDS - GCN. BDS - GCN USED-FOR GCN training. Method are GCNs, and GCN architectures. Material is real - world large graphs. OtherScientificTerm is GCN structures. Metric is memory usage. ","Graph Convolutional Networks (GCNs) are widely used in graph-based learning tasks. However, GCNs are not well-suited for real-world large graphs. The authors propose a method called BDS-GCN, which combines graph partition and distributed GCN training with an unbiased boundary sampling strategy to improve the full-graph accuracy of the proposed method. The proposed method is based on the idea that boundary nodes in a partitioned subgraph can be represented as GCN structures, and that the memory and the communications between the boundary nodes can be used to learn GCN architectures. Experiments show that the performance of the method is comparable to state-of-the-art methods in terms of throughput and accuracy.","Graph Convolutional Networks (GCNs) are used for graph-based learning tasks. The authors propose a new method, BDS-GCN, for distributed GCN training with graph partition and distributed training with GCN structures. The proposed method is based on an unbiased boundary sampling strategy, where the partitioned subgraph consists of boundary nodes, and the GCN architectures are partitioned into two subgraphs, one for each subgraph, and one for the other for the whole subgraph. The paper shows that the proposed method achieves better full-graph accuracy than state-of-the-art methods in terms of throughput and memory usage."
1161,SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"Machine Learning ( ML ) USED-FOR large - scale physics - based simulations. models USED-FOR real large - scale and complex problems. quantum chemistry simulations USED-FOR catalyst discovery. model USED-FOR quantum chemistry simulations. catalyst discovery USED-FOR renewable energy applications. model USED-FOR catalyst discovery. ForceNet USED-FOR quantum chemistry simulations. ForceNet HYPONYM-OF graph neural network. graph neural network USED-FOR per - atom forces. surrounding 3D molecular structure USED-FOR per - atom forces. surrounding 3D molecular structure USED-FOR graph neural network. model scaling USED-FOR ForceNet. expressive message passing architecture USED-FOR ForceNet. basis and non - linear activation functions USED-FOR ForceNet. ForceNet COMPARE ML models. ML models COMPARE ForceNet. ForceNet USED-FOR atomic forces. ForceNet USED-FOR large - scale catalyst dataset. OC20 HYPONYM-OF large - scale catalyst dataset. ForceNet USED-FOR quantum chemistry simulations. ForceNet COMPARE ML models. ML models COMPARE ForceNet. success rate EVALUATE-FOR ML models. success rate EVALUATE-FOR ForceNet. ML - based simulations COMPARE physics - based simulations. physics - based simulations COMPARE ML - based simulations. Task is atomic simulations. OtherScientificTerm are 3D space, forces, and out - of - distribution structures. ","This paper proposes a new model for quantum chemistry simulations for catalyst discovery in the context of renewable energy applications. The proposed model, ForceNet, is a graph neural network with a surrounding 3D molecular structure to model per-atom forces in the 3D space. The authors propose an expressive message passing architecture to improve the model scaling of ForceNet. They show that ForceNet is able to learn both basis and non-linear activation functions, and can achieve better success rate than existing ML models for atomic forces. ","This paper proposes a new model for quantum chemistry simulations for catalyst discovery for renewable energy applications. The model is based on a graph neural network with a surrounding 3D molecular structure. The authors propose a new expressive message passing architecture for ForceNet, which uses both basis and non-linear activation functions to learn the per-atom forces in the 3D space. Experiments are conducted on a large-scale catalyst dataset, OC20, where the authors show that ForceNet achieves better success rate than other ML models on atomic simulations, and outperforms other models in many real large -scale and complex problems."
1177,SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,regularisation USED-FOR fine - tuning. approaches USED-FOR regularisation. approaches USED-FOR fine - tuning. regularisation USED-FOR deep neural networks. Rademacher complexity USED-FOR neural network generalisation bound. bound COMPARE bounds. bounds COMPARE bound. bounds USED-FOR convolutional networks. bound USED-FOR fine - tuning. learning USED-FOR generalisation. initialisation FEATURE-OF network. transfer learning USED-FOR initialisation. transfer learning USED-FOR network. fine - tuning algorithm USED-FOR hypothesis class. generalisation EVALUATE-FOR transfer learning. It COMPARE fine - tuning competitors. fine - tuning competitors COMPARE It. It COMPARE penalty - based alternatives. penalty - based alternatives COMPARE It. Generic is algorithm. OtherScientificTerm is radius of the search space. ,"This paper studies the problem of fine-tuning deep neural networks with Rademacher complexity. The authors propose a new regularisation method for the problem. The proposed method is based on transfer learning, where the parameters of the network are learned from the initialisation of the neural network. The main contribution of the paper is to show that the proposed method can be used to improve the generalization performance of neural networks. ",This paper proposes a new regularisation for fine-tuning deep neural networks. The authors propose two approaches for regularisation. The first approach is based on the Rademacher complexity of the neural network generalisation bound. The second approach uses transfer learning for the initialisation of the network. The main contribution of the proposed algorithm is to reduce the radius of the search space. It is shown to outperform the bounds for convolutional networks. It also outperforms the fine-tuning competitors in terms of generalisation. 
1193,SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"model evaluation EVALUATE-FOR mask discovery. training configuration USED-FOR mask. mask discovery ( Hfind ) CONJUNCTION mask evaluation ( Heval ). mask evaluation ( Heval ) CONJUNCTION mask discovery ( Hfind ). hyperparameters USED-FOR mask evaluation ( Heval ). hyperparameters USED-FOR mask discovery ( Hfind ). unstructured magnitude pruning USED-FOR vision classification tasks. hyperparameters USED-FOR stages. unstructured magnitude pruning USED-FOR decoupled find - eval phenomenon. hyperparameters USED-FOR masks. Hfind values USED-FOR masks. layerwise pruning ratios FEATURE-OF masks. ratios USED-FOR decoupled find - eval phenomenon. Task is model pruning. Generic are model, and models. Method are lottery ticket framework, and one - shot structured pruning. OtherScientificTerm is decoupling hyperparameters. ","This paper studies the problem of model pruning in the context of mask discovery and mask evaluation. The authors propose a lottery ticket framework, where the model is trained with a set of hyperparameters for mask discovery (Hfind), mask evaluation (Heval), and unstructured magnitude pruning for vision classification tasks. The training configuration of the mask is then used as a training configuration for the final mask. The paper shows that the Hfind values of the masks can be decoupled from the masks by using layerwise pruning ratios, and that these ratios can lead to a decoupling find-eval phenomenon when the masks are trained with different hyperparameter for different stages.  The paper also shows that one-shot structured pruning can be used to improve the performance of the model. ","This paper proposes a new model evaluation method for mask discovery and mask evaluation. The authors propose a lottery ticket framework, where each layer of the model is randomly selected from the training configuration. The paper proposes two stages: mask discovery (Hfind) and Heval (Heval). The first stage uses unstructured magnitude pruning for vision classification tasks. The second stage uses one-shot structured pruning. The decoupling hyperparameters of the masks is done using layerwise pruning ratios of the Hfind values. "
1209,SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"training USED-FOR alignment of per - example gradients. metrics COMPARE m - coherence. m - coherence COMPARE metrics. m - coherence COMPARE O(m ). O(m ) COMPARE m - coherence. memorization CONJUNCTION generalization. generalization CONJUNCTION memorization. ResNet CONJUNCTION EfficientNet models. EfficientNet models CONJUNCTION ResNet. m - coherence USED-FOR evolution of alignment of per - example gradients. label noise FEATURE-OF variants. ImageNet EVALUATE-FOR EfficientNet models. m - coherence COMPARE real labels. real labels COMPARE m - coherence. neural networks USED-FOR generalization. OtherScientificTerm are gradient, and gradient diversity. Method are Coherent Gradients ( CG ) theory, over - parameterized neural networks, and CG. ","This paper studies the Coherent Gradients (CG) theory and proposes a new metric called Coherent Coherence (CoCo) to measure the alignment of per-example gradients. CoCo is an extension of the O(m) metric, which measures the coherence between the gradients of a pair of gradients in the training process. The authors show that CoCo can be used as a metric for measuring the generalization performance of neural networks trained with neural networks. The paper also provides a theoretical analysis of CoCo.","This paper proposes Coherent Gradients (CG) theory, which proposes to use over-parameterized neural networks to improve the alignment of per-example gradients during training. The main idea is to use m-coherence instead of O(m) as the metric to measure the similarity between the gradient and the gradient diversity. The authors show that the proposed metrics are more robust to label noise and generalization compared to other metrics. They show that ResNet and EfficientNet models with label noise are better than other variants of the same gradient. They also show that real labels are more stable than real labels. Finally, they show that neural networks can be used for generalization. "
1225,SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"summary statistics USED-FOR implicit generative models. approach USED-FOR approximate Bayesian computation. approximate Bayesian computation CONJUNCTION neural likelihood methods. neural likelihood methods CONJUNCTION approximate Bayesian computation. approach USED-FOR neural likelihood methods. tasks EVALUATE-FOR approach. Task are evaluation of the likelihood function, and constructing sufficient statistics. OtherScientificTerm are likelihood function, sufficient statistics, and density or density ratio. Generic is model. Method are deep neural networks, and infomax learning procedure. ","This paper proposes a new approach to approximate Bayesian computation and neural likelihood methods based on summary statistics for implicit generative models. The proposed approach is based on the evaluation of the likelihood function of the model. The authors propose an infomax learning procedure where the model is trained on a set of sufficient statistics, and the goal is to learn a likelihood function that is close to the density or density ratio. The approach is evaluated on three different tasks.","The paper proposes a new approach to approximate Bayesian computation and neural likelihood methods for implicit generative models with summary statistics. The approach is based on the evaluation of the likelihood function, and constructing sufficient statistics for the model. The authors show that the proposed approach can be applied to a variety of tasks. The proposed approach is evaluated on several datasets, and is shown to outperform existing approaches for deep neural networks. "
1241,SP:c5997bf2348e94949684f45fbd418661e85220c1,"set - level supervision USED-FOR data collection. paired images CONJUNCTION domain labels. domain labels CONJUNCTION paired images. model COMPARE set - level supervised model. set - level supervised model COMPARE model. pseudo domains HYPONYM-OF hyperparameters. full labels USED-FOR set - level supervised model. TUNIT USED-FOR semi - supervised scenario. Method is image - to - image translation model. Task is image - to - image translation. OtherScientificTerm are image domains, and estimated domains. ","This paper proposes a new image-to-image translation model, called TUNIT. The proposed model is based on set-level supervision for data collection, where the goal is to learn a set of hyperparameters (i.e., pseudo domains) for each of the image domains. The model is trained on paired images and domain labels. The authors show that the proposed model performs better than a set -level supervised model on data collection with full labels. They also provide a semi-supervised scenario, where TunIT is used to learn the estimated domains. ","This paper proposes a new image-to-image translation model. The proposed model is based on set-level supervision for data collection. The authors show that the proposed model outperforms the standard set - level supervised model on two image domains, paired images and domain labels. The hyperparameters are pseudo domains, where pseudo domains are a subset of the image domains and the estimated domains are the full domains. The semi-supervised scenario is modeled by TUNIT."
1257,SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"probability distribution USED-FOR network parameters. it USED-FOR initialization procedures. probability distribution USED-FOR curvature penalty function. asymmetric initialization USED-FOR constant curvature penalty. natural cubic spline interpolation USED-FOR solution function. uniform distribution USED-FOR asymmetric initialization. multivariate regression CONJUNCTION activation functions. activation functions CONJUNCTION multivariate regression. regularization strength FEATURE-OF spatially adaptive smoothing splines. spatially adaptive smoothing splines USED-FOR training trajectories. Method are wide neural networks, and width - n shallow ReLU network. OtherScientificTerm are implicit bias in function space, and weighted second derivative. Task is 1D regression. ","This paper studies the problem of estimating the curvature penalty function of wide neural networks. The main idea is to use the probability distribution of the network parameters as the parameter of the initialization procedures, and to use it as the bias in function space. The solution function is approximated by a natural cubic spline interpolation, which is a weighted second derivative of the original solution function. The authors show that using the uniform distribution allows for asymmetric initialization of the constant curvature of the solution function, and that it can be used for initialization procedures. They also show that the regularization strength of spatially adaptive smoothing splines for training trajectories with multivariate regression and activation functions can be improved by using a width-n shallow ReLU network. ","This paper proposes a new method for learning the curvature penalty function of wide neural networks. The main idea is to use the probability distribution of the network parameters as a regularization of the initialization procedures. The authors show that it can be used to improve the generalization performance of initialization procedures in the presence of implicit bias in function space. They show that the solution function can be approximated by natural cubic spline interpolation. They also provide a weighted second derivative, which is a width-n shallow ReLU network. Experiments are performed on multivariate regression and activation functions. The results show that spatially adaptive smoothing splines with regularization strength are able to approximate the training trajectories. "
1273,SP:8b885142facbb3b8db41ec9d83822cee81324694,Weight decay HYPONYM-OF regularization technique. regularization technique USED-FOR deep neural networks. L2 regularization USED-FOR weight decay. L2 regularization USED-FOR deep learning libraries. L2 regularization COMPARE weight decay. weight decay COMPARE L2 regularization. weight decay USED-FOR adaptive gradient methods. L2 regularization USED-FOR adaptive gradient methods. Decoupled Weight Decay ( AdamW ) USED-FOR Adam. Adaptive Momentum Estimation ( Adam ) HYPONYM-OF adaptive gradient methods. L2 regularization CONJUNCTION decoupled weight decay. decoupled weight decay CONJUNCTION L2 regularization. decoupled weight decay PART-OF deep learning libraries. decoupled weight decay HYPONYM-OF weight decay. L2 regularization HYPONYM-OF weight decay. unstable weight decay USED-FOR optimizers. stochastic gradient descent ( SGD ) HYPONYM-OF Momentum. stochastic gradient descent ( SGD ) HYPONYM-OF optimizers. Momentum USED-FOR optimizers. decoupled weight decay USED-FOR adaptive gradient methods. Stable Weight Decay ( SWD ) method USED-FOR unstable weight decay problem. L2 regularization CONJUNCTION decoupled weight decay. decoupled weight decay CONJUNCTION L2 regularization. SWD method COMPARE decoupled weight decay. decoupled weight decay COMPARE SWD method. SWD method COMPARE L2 regularization. L2 regularization COMPARE SWD method. weight decay PART-OF Adam. hyperparameters FEATURE-OF Adam variants. SWD USED-FOR Adam. SWD USED-FOR weight decay. OtherScientificTerm is hyperparameter. ,"This paper proposes a new regularization technique called Weight decay for deep neural networks called Adaptive Momentum Estimation (AdamW). The authors show that weight decay outperforms L2 regularization and decoupled weight decay in adaptive gradient methods such as L1 regularization, decoupling weight decay, and L2-based regularization in deep learning libraries. The authors also show that the Stable Weight Decay (SWD) method can be used to solve the unstable weight decay problem in the Adam variants of Adam, and that the SWD method is more stable than L1 and dec2 regularizations. Finally, the authors provide a theoretical analysis of the effect of the hyperparameter of Adam variants on the performance. ","This paper proposes a regularization technique for deep neural networks, called Weight decay. Weight decay is an extension of L2 regularization to weight decay in adaptive gradient methods such as Adaptive Momentum Estimation (Adam) and Decoupled Weight Decay (AdamW) for deep learning libraries. Momentum is used in optimizers such as stochastic gradient descent (SGD) and decoupled weight decay for optimizers with unstable weight decay. Adam variants of Adam have different hyperparameters, and SWD for weight decay is a variant of SWD. The authors show that the SWD method is more stable than the L2 standard regularization, and that the decoupling weight decay outperforms L2 and L1 regularization in deep learning. "
1289,SP:a3206dc71e32ba1830895bf442d3840f3331a532,"Translation Memory ( TM ) USED-FOR neural machine translation ( NMT ). translation quality EVALUATE-FOR neural machine translation ( NMT ). translation quality EVALUATE-FOR Translation Memory ( TM ). encoder USED-FOR TM. TM CONJUNCTION NMT. NMT CONJUNCTION TM. method USED-FOR NMT. method USED-FOR TM. encoder USED-FOR TM information. pre - trained language model ( PLM ) USED-FOR encoder. sentence level retrieval method USED-FOR n - gram retrieval method. methods USED-FOR information flow. TM CONJUNCTION NMT decoder. NMT decoder CONJUNCTION TM. translation quality EVALUATE-FOR methods. OtherScientificTerm are semantic relationship, and similarity score. Method is sentence level retrieval approach. ",This paper studies the translation quality of neural machine translation (NMT) with Translation Memory (TM) in the context of sentence level retrieval. The authors propose a new method to improve the performance of NMT with a pre-trained language model (PLM). The encoder is used to encode the TM information and the NMT decoder is trained to extract the semantic relationship between the encoder and the TM. The n-gram retrieval method is based on the sentence level retrieved method from the PLM. The paper shows that the proposed methods can improve the information flow between the TM and NMT. ,"This paper studies the translation quality of neural machine translation (NMT). The authors propose a new method to improve the quality of NMT, called Translation Memory (TM). The encoder is a pre-trained language model (PLM) that encodes the TM information and the NMT information. The semantic relationship between the encoder and NMT is computed using a similarity score. The authors also propose a sentence level retrieval approach that uses a sentence-level retrieval method to retrieve the n-gram retrieval method. They show that the proposed methods can improve the information flow between the TM and the nMT decoder. "
1305,SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,rate reduction CONJUNCTION ( shift ) invariant classification. ( shift ) invariant classification CONJUNCTION rate reduction. rate reduction USED-FOR deep ( convolutional ) networks. ( shift ) invariant classification USED-FOR deep ( convolutional ) networks. iterative gradient ascent scheme USED-FOR rate reduction of learned features. iterative gradient ascent scheme USED-FOR deep network. components PART-OF network. network USED-FOR discriminative deep representation. linear operators PART-OF multi - channel convolutions. spectral domain FEATURE-OF convolutional network. Generic is architectures. Method is back propagation training. Task is classification. ,This paper studies the problem of rate reduction in deep (convolutional) networks with (shift) invariant classification. The authors propose an iterative gradient ascent scheme for the rate reduction of learned features in a deep network. The proposed architectures are based on linear operators in multi-channel convolutions. The main contribution of the paper is to show that a convolutional network in the spectral domain with linear operators can learn a discriminative deep representation. ,This paper proposes a novel method for learning a discriminative deep representation of the spectral domain. The main idea is to use the rate reduction and (shift) invariant classification of deep (convolutional) networks with rate reduction. The authors propose an iterative gradient ascent scheme for rate reduction of learned features. The proposed architectures are based on linear operators in multi-channel convolutions. The paper also proposes to use back propagation training to improve the classification performance of the network.
1321,SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"implicit acceleration of gradient flow USED-FOR over - parameterized two - layer linear models. conservation law USED-FOR implicit acceleration. spectrum USED-FOR acceleration. matrix factorization problem CONJUNCTION Riccati type differential equations. Riccati type differential equations CONJUNCTION matrix factorization problem. small, balanced or spectral initialization FEATURE-OF weights. Method is gradient flow. OtherScientificTerm is Gramian matrices. ","This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models. The authors propose a conservation law for implicit acceleration based on the spectrum of the gradient flow. They show that the acceleration is a function of the width of the spectrum and the number of Gramian matrices. They also provide a matrix factorization problem as well as Riccati type differential equations. Finally, they provide a small, balanced or spectral initialization of the weights.","This paper studies the implicit acceleration of gradient flow in over-parameterized two-layer linear models. The authors propose a conservation law for implicit acceleration under the spectrum of Gramian matrices. They show that the acceleration can be computed in terms of the spectrum. They also provide a matrix factorization problem and Riccati type differential equations. Finally, they show that gradient flow can be decomposed into small, balanced or spectral initialization of weights."
1337,SP:e5f086c806be88d50e461a782b5b00124f4656fb,"approach USED-FOR opaque model ’s behavior. uniform sampling of user - defined subspaces USED-FOR framework. framework USED-FOR ML model. CLIME USED-FOR ML model. CLIME HYPONYM-OF framework. Method are machine learning techniques, LIME, surrogate interpretable model, LIME framework, and estimation algorithm. Task are explainable AI, OOD sampling problem, OOD sampling, and real - world problems. OtherScientificTerm are LIME ’s explanations, adversarial attacks, perturbation procedure, and logical constraints. Generic is model. Metric are fidelity, and accuracy. ","This paper proposes a new approach to learn an opaque model’s behavior in the context of explainable AI. The proposed framework, CLIME, is based on uniform sampling of user-defined subspaces in the LIME framework. The authors propose a surrogate interpretable model, LIME, which can be used to learn a model that is interpretable in the presence of adversarial attacks. The model is trained with a perturbation procedure, and the model is evaluated on a variety of real-world problems. The paper shows that CLIME can learn an ML model that can be interpretable using CLIME. ","This paper proposes a novel approach to learn opaque model’s behavior. The framework is based on CLIME, a framework for learning an ML model with uniform sampling of user-defined subspaces. The main idea is to learn a surrogate interpretable model, which is a LIME, that can be used to generate explanations of the observed behavior of the model. The model is trained using machine learning techniques. The authors show that the LIME framework is robust to adversarial attacks and can be applied to real-world problems. The paper also proposes an OOD sampling problem, where adversarial examples can be generated from LIME ’s explanations, and an estimation algorithm is proposed to estimate the fidelity of the surrogate explainable model. "
1353,SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,Pre - trained language models USED-FOR natural language understanding ( NLU ). BERT HYPONYM-OF Pre - trained language models. Chinese HYPONYM-OF languages. English HYPONYM-OF languages. multi - word expressions USED-FOR natural lexical units. fine - grained and coarse - grained tokenizations USED-FOR pre - trained language models. AMBERT HYPONYM-OF Multi - grained BERT. AMBERT HYPONYM-OF pre - trained language model. fine - grained and coarse - grained tokenizations USED-FOR pre - trained language model. contextualized representations of the words CONJUNCTION contextualized representations of the phrases. contextualized representations of the phrases CONJUNCTION contextualized representations of the words. encoder CONJUNCTION encoder. encoder CONJUNCTION encoder. English USED-FOR AMBERT. encoder USED-FOR AMBERT. GLUE CONJUNCTION SQuAD. SQuAD CONJUNCTION GLUE. SQuAD CONJUNCTION RACE. RACE CONJUNCTION SQuAD. CLUE CONJUNCTION GLUE. GLUE CONJUNCTION CLUE. Chinese CONJUNCTION English. English CONJUNCTION Chinese. Chinese HYPONYM-OF benchmark datasets. English HYPONYM-OF benchmark datasets. RACE HYPONYM-OF benchmark datasets. SQuAD HYPONYM-OF benchmark datasets. CLUE HYPONYM-OF benchmark datasets. GLUE HYPONYM-OF benchmark datasets. AMBERT COMPARE models. models COMPARE AMBERT. Chinese EVALUATE-FOR AMBERT. AMBERT COMPARE AMBERT. AMBERT COMPARE AMBERT. Method is coarse - grained tokenization. Metric is inference time. ,"This paper studies the problem of pre-trained language models for natural language understanding (NLU). The authors propose AMBERT, a Multi-grained BERT based on BERT, which is an extension of BERT with multi-word expressions to natural lexical units. The main idea is to use the fine-gained and coarse-gated tokenizations in the pre-train language model to train a pre-trained language model with contextualized representations of the words and contextualized embeddings of the phrases. The encoder and the encoder of the pre - trained language model is trained using English.  The authors show that the proposed models perform better than existing models on several benchmark datasets (e.g., CLUE, GLUE, SQuAD, RACE).  ","This paper proposes a new way to train pre-trained language models for natural language understanding (NLU). BERT is a family of Pre-train language models called AMBERT and Multi-grained BERT. Multi-word expressions are used to represent natural lexical units. The authors propose fine-gained and coarse-gated tokenizations to improve the performance of the pre-trainable language model. The main contribution of the paper is the coarse-rigid tokenization. The encoder and the encoder are trained in parallel, and the inference time is reduced to a single step. Experiments are conducted on three benchmark datasets (English, Chinese, and RACE) and three languages (GLUE, CLUE, and SQuAD) to show the performance improvement of the models compared to other models. "
1369,SP:fd1cfe80343d3789227d99d836a5674374a234f5,"task USED-FOR natural language utterance. Semantic parsing HYPONYM-OF task. natural language utterance USED-FOR machine - understandable information representation. task USED-FOR machine - understandable information representation. Transformer USED-FOR semantic parsing. PhraseTransformer architecture USED-FOR meaning representation. phrase dependencies USED-FOR PhraseTransformer architecture. phrase dependencies USED-FOR meaning representation. Long Short - Term Memory ( LSTM ) USED-FOR local context of phrases. Self - Attention mechanism USED-FOR local context of phrases. Long Short - Term Memory ( LSTM ) PART-OF Self - Attention mechanism. Self - Attention mechanism PART-OF Transformer. model COMPARE Transformer. Transformer COMPARE model. model USED-FOR detailed meaning. model USED-FOR local context awareness. Neural Network USED-FOR Atis dataset. Geo, MSParS datasets EVALUATE-FOR model. Method is Neural Machine Translation. OtherScientificTerm is long - range word dependencies. ","This paper proposes a new task for natural language utterance, called Semantic parsing. The task is to learn a machine-readable information representation from natural language. The authors propose a Transformer for semantic parsing, which is based on the PhraseTransformer architecture. The Self-Attention mechanism in the Transformer learns the local context of phrases using Long Short-Term Memory (LSTM). The model is trained on the Atis dataset using a Neural Network. The proposed model is evaluated on Geo, MSParS datasets and shows that the proposed model can learn detailed meaning. ","The paper proposes a new task for natural language utterance, Semantic parsing, which is an important task for machine-understanding information representation. The paper proposes to use the PhraseTransformer architecture to learn a meaning representation based on phrase dependencies. The Transformer uses Long Short-Term Memory (LSTM) and Self-Attention mechanism to learn the local context of phrases. The model is evaluated on the Atis dataset and the Geo, MSParS datasets. The proposed model is able to learn detailed meaning from long-range word dependencies. "
1385,SP:2056a65a7500d79465685af883083cd706277c1f,"combinations of multiple perturbations FEATURE-OF DNN robustness. composite adversarial training ( CAT ) HYPONYM-OF training method. robustness EVALUATE-FOR individual perturbations. training method USED-FOR multiple adversarial losses. pixel perturbations CONJUNCTION spatial transformations. spatial transformations CONJUNCTION pixel perturbations. CAT COMPARE adversarial training methods. adversarial training methods COMPARE CAT. benchmark datasets EVALUATE-FOR CAT. spatial transformations HYPONYM-OF adversarial perturbation models. Method are deep neural networks ( DNNs ), and individual perturbation models. OtherScientificTerm is adversarial perturbations. ",This paper proposes a new training method for multiple adversarial losses in deep neural networks (DNNs). The authors propose to use composite adversarial training (CAT) to improve the DNN robustness against the combinations of multiple perturbations. The authors show that CAT improves the robustness of individual perturbation models in terms of the number of perturbed pixels and spatial transformations. The paper also shows that CAT outperforms the state-of-the-art on several benchmark datasets. ,"This paper proposes a new training method for multiple adversarial losses, called composite adversarial training (CAT), to improve the robustness of DNN robustness to adversarial perturbations. The authors propose to train deep neural networks (DNNs) with multiple perturbation models, where each perturbator is represented as a set of pixels, and each pixel is represented by a different classifier. The training method is evaluated on two benchmark datasets, where CAT is shown to outperform other adversarial learning methods, such as pixel-based and spatial transformations."
1401,SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"Emergent Symbol Binding Network ( ESBN ) HYPONYM-OF recurrent network. external memory USED-FOR variable - binding. external memory USED-FOR recurrent network. binding mechanism USED-FOR symbol - like representations. ESBN USED-FOR rules. learning process USED-FOR symbol - like representations. architecture COMPARE competitive neural network architectures. competitive neural network architectures COMPARE architecture. Task are human intelligence, and induction of abstract rules. OtherScientificTerm are abstract rules, and symbol - processing machinery. Material are high - dimensional sensory data, and high - dimensional data. Method are Deep neural network algorithms, and symbol - processing mechanisms. ",This paper proposes a new recurrent network called Emergent Symbol Binding Network (ESBN) based on external memory for variable-binding. ESBN learns the rules of a set of abstract rules and uses the binding mechanism to learn the symbol-like representations in the learning process. The authors show that the proposed architecture outperforms competitive neural network architectures on high-dimensional sensory data. ,This paper proposes an emergent Symbol Binding Network (ESBN) which is a recurrent network with an external memory for variable-binding. The idea is to learn a set of abstract rules that can be applied to high-dimensional sensory data. These rules are learned using ESBN. The authors show that the proposed architecture outperforms competitive neural network architectures. 
1417,SP:4171ce45966ac499f51450a19fb233934c0847f0,"nested named entity recognition CONJUNCTION relation classification. relation classification CONJUNCTION nested named entity recognition. framework USED-FOR structured prediction language tasks. relation classification CONJUNCTION semantic role labeling. semantic role labeling CONJUNCTION relation classification. event extraction CONJUNCTION coreference resolution. coreference resolution CONJUNCTION event extraction. semantic role labeling CONJUNCTION event extraction. event extraction CONJUNCTION semantic role labeling. joint entity and relation extraction CONJUNCTION nested named entity recognition. nested named entity recognition CONJUNCTION joint entity and relation extraction. coreference resolution CONJUNCTION dialogue state tracking. dialogue state tracking CONJUNCTION coreference resolution. dialogue state tracking HYPONYM-OF structured prediction language tasks. coreference resolution HYPONYM-OF structured prediction language tasks. event extraction HYPONYM-OF structured prediction language tasks. semantic role labeling HYPONYM-OF structured prediction language tasks. joint entity and relation extraction HYPONYM-OF structured prediction language tasks. relation classification HYPONYM-OF structured prediction language tasks. nested named entity recognition HYPONYM-OF structured prediction language tasks. translation task USED-FOR it. augmented natural languages USED-FOR translation task. task - specific discriminative classifiers USED-FOR problem. FewRel CONJUNCTION TACRED. TACRED CONJUNCTION FewRel. approach COMPARE task - specific models. task - specific models COMPARE approach. relation classification CONJUNCTION semantic role labeling. semantic role labeling CONJUNCTION relation classification. joint entity and relation extraction CONJUNCTION relation classification. relation classification CONJUNCTION joint entity and relation extraction. relation classification CONJUNCTION FewRel. FewRel CONJUNCTION relation classification. relation classification CONJUNCTION TACRED. TACRED CONJUNCTION relation classification. approach COMPARE,., COMPARE approach. relation classification EVALUATE-FOR,. joint entity and relation extraction EVALUATE-FOR,. tasks EVALUATE-FOR task - specific models. tasks EVALUATE-FOR approach. joint entity and relation extraction EVALUATE-FOR approach. model USED-FOR tasks. hyperparameters USED-FOR tasks. architecture USED-FOR tasks. architecture CONJUNCTION","This paper proposes a framework for structured prediction language tasks where the task-specific discriminative classifiers are used to solve the problem. The proposed framework is based on the idea of a translation task, where the goal is to learn a language model from a set of augmented natural languages, and then use it to learn the model to solve a task. The model is trained on a variety of tasks, and the proposed model is evaluated on a number of tasks with different hyperparameters. The results show that the proposed approach is able to achieve state-of-the-art performance on most of the tasks.","This paper proposes a framework for structured prediction language tasks where the task-specific discriminative classifiers are trained on augmented natural languages. The problem is formulated as a translation task, and it can be seen as an extension of the translation task. The proposed framework is evaluated on a number of tasks, such as relational classification, semantic role labeling, event extraction, coreference resolution, dialogue state tracking, and nested named entity recognition. Results show that the proposed model is able to perform well on most of the tasks, and that the hyperparameters of the model can be used to improve the performance on other tasks. "
1433,SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"named entity recognition ( NER ) models USED-FOR unlabeled entity problem. approach USED-FOR misguidance. unlabeled entities USED-FOR NER models. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. model COMPARE prior baselines. prior baselines COMPARE model. real - world datasets EVALUATE-FOR model. synthetic datasets EVALUATE-FOR model. model USED-FOR unlabeled entity problem. well - annotated datasets EVALUATE-FOR model. OtherScientificTerm is negative instances. Method are pretraining language models, and negative sampling. ","This paper studies the problem of unlabeled entity recognition (NER) models trained with named entities. The authors propose a novel approach to improve the misguidance of NER models trained on unlabelled entities. In particular, the authors propose to use negative instances as negative samples to train the pretraining language models. The proposed model is evaluated on synthetic datasets and real-world datasets and shows that the proposed model performs better than prior baselines. The model also performs well on well-annotated datasets.","This paper proposes a novel approach to mitigate misguidance in NER models trained on unlabeled entities. The approach is based on pretraining language models on negative instances, where negative sampling is used. The proposed model is evaluated on synthetic datasets and real-world datasets, and compared to prior baselines."
1449,SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"stochastic neighbor embedding ( SNE ) USED-FOR sequential inputs. stochastic neighbor embedding ( SNE ) USED-FOR vector space of fixed, reduced dimensions. Acoustic Neighbor Embeddings HYPONYM-OF acoustic word embedding. Euclidean distance USED-FOR phonetic confusability. acoustic encoder CONJUNCTION text encoder. text encoder CONJUNCTION acoustic encoder. acoustic encoder USED-FOR speech signals. frame - wise subword posterior probabilities USED-FOR acoustic encoder. frame - wise subword posterior probabilities FEATURE-OF speech signals. acoustic model USED-FOR frame - wise subword posterior probabilities. acoustic encoder HYPONYM-OF encoder neural networks. text encoder HYPONYM-OF encoder neural networks. triplet loss criterion COMPARE method. method COMPARE triplet loss criterion. gradients USED-FOR neural network training. method USED-FOR neural network training. gradients EVALUATE-FOR method. text encoder network USED-FOR approximate phonetic matching task. encoder networks USED-FOR word ( name ) recognition task. low - dimensional embeddings USED-FOR it. encoder networks USED-FOR it. recognition accuracy EVALUATE-FOR finite state transducer(FST)-based decoding. test data USED-FOR finite state transducer(FST)-based decoding. Euclidean nearest - neighbor search USED-FOR isolated name recognition task. Material is speech. OtherScientificTerm are embedding space, subword transcriptions, embedding vectors, and embeddings. ","This paper proposes a new acoustic word embedding, called Acoustic Neighbor Embeddings, which is based on stochastic neighbor embedding (SNE) for sequential inputs. The key idea is to embed the embedding space of fixed, reduced dimensions in the vector space of the input word embeddings. The embedding vectors are then used to encode the subword transcriptions. The acoustic encoder is trained with frame-wise subword posterior probabilities for the speech signals, which are computed using an acoustic model. The authors show that the proposed method achieves better recognition accuracy than the triplet loss criterion for neural network training with the same number of gradients. ","This paper introduces Acoustic Neighbor Embeddings, an acoustic word embedding based on stochastic neighbor embedding (SNE) for sequential inputs of fixed, reduced dimensions. The embedding space of fixed dimensions is a vector space of subword transcriptions, where the embedding vectors are of low-dimensional embeddings. The authors propose to use a frame-wise subword posterior probabilities for the speech signals, which are learned by an acoustic model. The proposed method outperforms the triplet loss criterion for neural network training with respect to the gradients of the encoder neural networks (acoustic encoder and text encoder) and it is shown to improve the recognition accuracy of finite state transducer(FST)-based decoding on test data. The paper also proposes an isolated name recognition task using Euclidean nearest-neighbor search."
1465,SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,reinforcement learning algorithm USED-FOR stationary mean - field games. mean - field state USED-FOR Nash equilibrium. mean - field state CONJUNCTION policy. policy CONJUNCTION mean - field state. gradient - descent CONJUNCTION proximal policy optimization. proximal policy optimization CONJUNCTION gradient - descent. mean - field state CONJUNCTION policy. policy CONJUNCTION mean - field state. proximal policy optimization USED-FOR fictitious play algorithm. gradient - descent USED-FOR fictitious play algorithm. gradient - descent USED-FOR policy. proximal policy optimization USED-FOR policy. algorithm USED-FOR single - agent reinforcement learning problem. fictitious play algorithm USED-FOR Nash equilibrium. OtherScientificTerm is optimum. Task is mean - field games. ,This paper proposes a new reinforcement learning algorithm for stationary mean-field games. The authors propose a fictitious play algorithm based on gradient-descent and proximal policy optimization to learn a policy and a policy that is optimal for Nash equilibrium. The algorithm is applied to the single-agent reinforcement learning problem and is shown to converge to the optimum. ,"This paper proposes a reinforcement learning algorithm for stationary mean-field games. The main idea is to learn a mean-of-field state and a policy in a Nash equilibrium, which is the optimum. The authors propose a fictitious play algorithm based on gradient-descent and proximal policy optimization. The algorithm is applied to the single-agent reinforcement learning problem. "
1481,SP:c498f8a199da1818fe64ed88b0825c5aad688aec,joint distribution USED-FOR probabilistic inference. normalizing flow model USED-FOR probabilistic inference. normalizing flow model USED-FOR joint distribution. flow models USED-FOR task. framework USED-FOR approximate probabilistic inference. method USED-FOR generative model. flow model USED-FOR distribution. variational inference USED-FOR it. arbitrary differentiable transformations USED-FOR conditioning. likelihood evaluation CONJUNCTION inversion. inversion CONJUNCTION likelihood evaluation. it USED-FOR likelihood evaluation. inversion CONJUNCTION sampling. sampling CONJUNCTION inversion. it USED-FOR sampling. it USED-FOR inversion. inference tasks USED-FOR inverse problems. inference tasks EVALUATE-FOR method. approach COMPARE MCMC baselines. MCMC baselines COMPARE approach. sample quality EVALUATE-FOR MCMC baselines. sample quality EVALUATE-FOR approach. Generic is model. OtherScientificTerm is approximate posterior. ,"This paper proposes a new framework for approximate probabilistic inference with a normalizing flow model for the joint distribution. The authors propose a new method for training a generative model using variational inference, where the model is trained with arbitrary differentiable transformations. The task is to learn a joint distribution over a set of flow models for a given task. The distribution is learned using a flow model, and it is then used to train a distribution that is close to the approximate posterior. The paper shows that it can be used for likelihood evaluation, inversion, and sampling. The proposed approach is evaluated on a variety of inference tasks for inverse problems, and the sample quality of the proposed approach outperforms MCMC baselines.","This paper proposes a new framework for approximate probabilistic inference with a normalizing flow model for the joint distribution. The method is based on variational inference, and it can be applied to any generative model with arbitrary differentiable transformations for conditioning. The authors show that the proposed model outperforms MCMC baselines in terms of sample quality, likelihood evaluation, inversion, and sampling. They also show that it is more robust to inverse problems in inference tasks."
1497,SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,Computer vision technology USED-FOR biological and medical data analysis and understanding. Ultra - high Resolution Image Segmentation dataset USED-FOR Cell membrane. iterative annotations CONJUNCTION uncompressed high - resolution raw data. uncompressed high - resolution raw data CONJUNCTION iterative annotations. annotated Electron Microscopy ( EM ) dataset USED-FOR Cell membrane. U - RISC HYPONYM-OF annotated Electron Microscopy ( EM ) dataset. U - RISC HYPONYM-OF Ultra - high Resolution Image Segmentation dataset. segmentation evaluation criteria COMPARE human perception. human perception COMPARE segmentation evaluation criteria. evaluation criterion USED-FOR cell membrane segmentation. Perceptual Hausdorff Distance ( PHD ) HYPONYM-OF evaluation criterion. evaluation criteria CONJUNCTION PHD. PHD CONJUNCTION evaluation criteria. segmentation methods CONJUNCTION iterative manual annotation. iterative manual annotation CONJUNCTION segmentation methods. evaluation criteria USED-FOR iterative manual annotation. ,"This paper proposes a new evaluation criterion for cell membrane segmentation based on the Perceptual Hausdorff Distance (PHD). The evaluation criterion is based on a new dataset called U-RISC, which is an annotated Electron Microscopy (EM) dataset for the Cell membrane. The authors show that the proposed evaluation criteria can be applied to both segmentation methods and iterative manual annotation. ","This paper proposes a novel evaluation criterion for cell membrane segmentation based on Perceptual Hausdorff Distance (PHD) and U-RISC, an annotated Electron Microscopy (EM) dataset for biological and medical data analysis and understanding. The authors show that the proposed evaluation criteria outperform other segmentation evaluation criteria and iterative manual annotation on the Ultra-high Resolution Image Segmentation dataset for the Cell membrane. "
1513,SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"Continual Learning ( CL ) USED-FOR catastrophic forgetting. benchmarks USED-FOR CL algorithms. benchmarks USED-FOR forgetting. short streams of tasks USED-FOR benchmarks. short streams of tasks USED-FOR forgetting. modules USED-FOR atomic skills. modules PART-OF modular architecture. learning algorithm USED-FOR learning. exponential search space FEATURE-OF task - driven prior. task - driven prior USED-FOR learning algorithm. modular architecture CONJUNCTION learning algorithm. learning algorithm CONJUNCTION modular architecture. benchmarks EVALUATE-FOR learning algorithm. CL benchmarks EVALUATE-FOR modular architecture. CL benchmarks EVALUATE-FOR learning algorithm. Method is CL system. Generic are task, and Benchmark. ","This paper studies the problem of catastrophic forgetting in Continual Learning (CL) in the context of short streams of tasks. The authors propose a modular architecture and a learning algorithm based on this learning algorithm. The modular architecture consists of modules for atomic skills, and the learning algorithm is based on a task-driven prior in the exponential search space. The paper shows that the proposed learning algorithm outperforms the state-of-the-art CL benchmarks on a variety of benchmarks for catastrophic forgetting. ",This paper proposes a new continuous learning (CL) algorithm for catastrophic forgetting. The authors propose a modular architecture that consists of modules for atomic skills and a learning algorithm for learning on short streams of tasks. The learning algorithm is based on a task-driven prior in the exponential search space. The proposed learning algorithm has been evaluated on a number of benchmarks for CL algorithms. The results show that the proposed modular architecture outperforms other CL benchmarks for forgetting. 
1529,SP:cc819c61f408e88f247eb87946187ccec3dad32e,random selection CONJUNCTION clustering and/or augmentation. clustering and/or augmentation CONJUNCTION random selection. unsupervised meta - learning approaches USED-FOR synthetic meta - tasks. techniques USED-FOR synthetic meta - tasks. clustering and/or augmentation HYPONYM-OF techniques. random selection HYPONYM-OF techniques. approach USED-FOR metatasks. generative models USED-FOR approach. generative models USED-FOR metatasks. algorithms USED-FOR synthetic classes. synthetic classes PART-OF meta - task. approach COMPARE unsupervised learning baselines. unsupervised learning baselines COMPARE approach. benchmark datasets EVALUATE-FOR few - shot classification tasks. few - shot classification tasks EVALUATE-FOR unsupervised learning baselines. few - shot classification tasks EVALUATE-FOR approach. benchmark datasets EVALUATE-FOR unsupervised learning baselines. OtherScientificTerm is latent space. ,This paper proposes a new unsupervised meta-learning approach for synthetic meta-tasks. The proposed approach uses generative models to learn metatasks from the latent space. The authors propose two techniques: random selection and clustering and/or augmentation. The algorithms are able to learn synthetic classes for each meta-task. The experimental results on benchmark datasets show that the proposed approach outperforms the state-of-the-art in few-shot classification tasks.,"This paper presents a new approach to learning synthetic meta-tasks using unsupervised meta-learning approaches. The proposed techniques include random selection, clustering and/or augmentation. The key idea of the approach is to use generative models to learn metatasks from the latent space. The approach is evaluated on several benchmark datasets for few-shot classification tasks and outperforms other unsupersupervised learning baselines. The authors propose two algorithms to learn synthetic classes for each meta-task. "
1545,SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"inverse problems CONJUNCTION compressed sensing. compressed sensing CONJUNCTION inverse problems. it USED-FOR inference. Injectivity USED-FOR generative models. generative priors USED-FOR compressed sensing. injectivity FEATURE-OF fullyconnected and convolutional ReLU layers and networks. weight matrices USED-FOR injectivity. expansivity USED-FOR global injectivity. iid Gaussian matrices USED-FOR global injectivity. worst - case Lipschitz constants USED-FOR stability. arguments USED-FOR deep networks. differential topology USED-FOR arguments. injective ReLU network USED-FOR Lipschitz map. argument USED-FOR injectivity. random projections USED-FOR argument. neural networks USED-FOR nonlinear inverse and inference problems. OtherScientificTerm is well posedness. Method are layerwise analysis, tractable model, and injective network. ","This paper studies injectivity in fullyconnected and convolutional ReLU layers and networks. Injectivity is an important problem in generative models for compressed sensing and inverse problems. The authors consider the problem of well posedness, where it is important to be able to perform inference in a tractable model. In injectivity is defined as the difference between the weight matrices of the injectivity and the global injectivity of the iid Gaussian matrices. The main contribution of this paper is a new argument for injecting injectivity into deep networks based on differential topology. The argument is based on random projections of the Lipschitz map of an injective ReLU network, and is shown to improve the stability of the resulting injective network. Empirically, the authors show that the proposed argument can improve the performance of neural networks in nonlinear inverse and inference problems.","This paper studies the problem of generative models with injectivity in the context of inverse problems and compressed sensing with generative priors. Injectivity is an important problem in generative model, as it can be used for inference. The authors show that injectivity of fullyconnected and convolutional ReLU layers and networks with weight matrices can be defined as the global injectivity with respect to the iid Gaussian matrices. This is achieved by a layerwise analysis of the Lipschitz map of a tractable model, where the injective network is defined as an injective ReLU network that maps the input to a Lipschy map of the output of the tractable network. They show that the global injectionivity of the generative network is a function of the expansivity. They also show that it is related to the well posedness. Finally, they show that their arguments can be applied to deep networks with differential topology. The argument is based on random projections, and the stability is measured by the worst-case Lipscheschitz constants. "
1561,SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"generative model USED-FOR image generation. continuous conditional generative adversarial network ( CcGAN ) HYPONYM-OF generative model. conditional GANs ( cGANs ) USED-FOR categorical conditions. class labels HYPONYM-OF categorical conditions. hidden map CONJUNCTION one - hot encoded label. one - hot encoded label CONJUNCTION hidden map. hidden map PART-OF generator / discriminator. one - hot encoded label HYPONYM-OF label input methods. hidden map HYPONYM-OF label input methods. empirical cGAN losses HYPONYM-OF cGAN losses. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. empirical cGAN losses USED-FOR continuous scenario. regression labels PART-OF discriminator. regression labels PART-OF generator. method USED-FOR regression labels. method USED-FOR generator. method USED-FOR discriminator. empirical cGAN losses USED-FOR CcGAN. hard vicinal discriminator loss ( HVDL ) CONJUNCTION soft vicinal discriminator loss ( SVDL ). soft vicinal discriminator loss ( SVDL ) CONJUNCTION hard vicinal discriminator loss ( HVDL ). empirical generator loss HYPONYM-OF empirical discriminator losses. soft vicinal discriminator loss ( SVDL ) HYPONYM-OF empirical discriminator losses. hard vicinal discriminator loss ( HVDL ) HYPONYM-OF empirical discriminator losses. HVDL CONJUNCTION SVDL. SVDL CONJUNCTION HVDL. error bounds FEATURE-OF discriminator. SVDL USED-FOR discriminator. HVDL USED-FOR discriminator. benchmark dataset USED-FOR generative image modeling. RC-49 USED-FOR generative image modeling. RC-49 HYPONYM-OF benchmark dataset. RC-49 CONJUNCTION UTKFace datasets. UTKFace datasets CONJUNCTION RC-49. Circular 2 - D Gaussians CONJUNCTION RC-49. RC-49 CONJUNCTION Circular 2 - D Gaussians. regression label FEATURE-OF image distribution. CcGAN COMPARE cGAN. cGAN COMPARE CcGAN. OtherScientificTerm are continuous, scal",This paper proposes a continuous conditional generative adversarial network (CcGAN) for image generation. CcGAN is a generative model that learns a discriminator to generate images from a continuous dataset. The discriminator is trained using a combination of a generator and a hidden map. The generator is trained with a soft-vacuous loss (HVDL) and the discriminator with a hard vicinal loss (SVDL). Empirical results show that the proposed method outperforms the state-of-the-art. ,"This paper proposes a continuous conditional generative adversarial network (CcGAN) for image generation. CcGANs are conditional GANs that are trained on categorical conditions (e.g., class labels). The authors show that empirical cGAN losses are better than the standard cGAN loss in the continuous scenario. The authors also show that the discriminator is better than a generator and that the generator/discriminator can learn a hidden map, one-hot encoded label, and other label input methods. The empirical generator loss and the soft vicinal discriminator loss (HVDL) are also empirical discriminator losses. Finally, the authors propose a method to learn regression labels for the generator and discriminator. The discriminator has better error bounds than the generator. "
1577,SP:10dd09ab315870631d1451d200f2c87a023f8226,"sample complexity EVALUATE-FOR deep learning ( DL ). Semisupervised learning ( SSL ) USED-FOR task. unlabeled instances USED-FOR Semisupervised learning ( SSL ). unlabeled instances USED-FOR task. sample complexity EVALUATE-FOR Active learning ( AL ). SSL CONJUNCTION AL. AL CONJUNCTION SSL. SSL USED-FOR fully - supervised learning ( SL ). AL USED-FOR fully - supervised learning ( SL ). labeled samples USED-FOR fully - supervised learning ( SL ). SSL USED-FOR DL - based AL algorithms. annotation efficiency EVALUATE-FOR AL algorithms. diversity FEATURE-OF AL algorithms. SSL USED-FOR AL algorithms. AL algorithm USED-FOR classification network. AL algorithm USED-FOR convergence rate. convergence rate FEATURE-OF classification network. CRC CONJUNCTION SSL algorithm. SSL algorithm CONJUNCTION CRC. deep neural network COMPARE SL. SL COMPARE deep neural network. labeled samples USED-FOR deep neural network. SSL algorithm USED-FOR deep neural network. CRC USED-FOR deep neural network. AL CONJUNCTION SSL. SSL CONJUNCTION AL. our method USED-FOR ASSL. OtherScientificTerm is human - in - the - loop. Method are pool - based AL, convergence rate control ( CRC ), and AL and SSL ( ASSL ) algorithms. Metric is rate of convergence. Generic is method. ","This paper studies the sample complexity of deep learning (DL) with unlabeled instances for the task of Active learning (AL). Semisupervised learning (SSL) is used for this task, and SSL and AL are two popular DL-based AL algorithms. The authors show that the rate of convergence of AL algorithms with SSL can be bounded by the diversity of the data, and that the AL algorithm can achieve a better convergence rate than the classification network with the same number of labeled samples. They also show that AL and SSL (ASSL) algorithms can achieve better annotation efficiency than SL. Finally, the authors propose a pool-based Al, called convergence rate control (CRC), which can be used to improve the performance of the deep neural network trained with the SSL algorithm and the AL with the proposed AL algorithm. The proposed method is shown to outperform the state-of-the-art ASSL.","This paper studies the sample complexity of deep learning (DL) with Semisupervised learning (SSL) on the task of learning unlabeled instances. Active learning (AL) and SSL are two popular DL-based AL algorithms. The authors propose a new pool-based algorithm, called human-in-the-loop, which is based on convergence rate control (CRC). The authors show that the proposed method outperforms AL and SSL (ASSL) algorithms in terms of annotation efficiency and diversity. They also show that SSL and AL algorithms have better convergence rate than the standard deep neural network. Finally, the authors propose an AL algorithm to improve the convergence rate of the classification network. "
1593,SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"federated learning method USED-FOR distributively training neural network models. devices USED-FOR parallelizing gradient computation. scheme USED-FOR training. devices CONJUNCTION partial participation and unbalanced data. partial participation and unbalanced data CONJUNCTION devices. scheme USED-FOR convex and non - convex settings. Task are Federated Learning problem, and device level computations. OtherScientificTerm are local - device level empirical loss, global empirical loss, and device heterogeneity. Method are inexact minimization, and dynamic regularizer. Material is real and synthetic data. ","This paper proposes a federated learning method for distributively training neural network models. The proposed scheme is based on the notion of local-device level empirical loss, which is a global empirical loss that is a function of the number of devices in the network. The authors show that the proposed scheme can be applied to both convex and non-convex settings, where devices can be used for parallelizing gradient computation. The paper also shows that the inexact minimization can be achieved by using a dynamic regularizer.  The authors also provide a theoretical analysis of the Federated Learning problem, showing that the device level computations can be improved by using device heterogeneity. The theoretical analysis is also supported by experiments on real and synthetic data.","This paper proposes a federated learning method for distributively training neural network models. The authors propose a local-device level empirical loss, where the local empirical loss is the global empirical loss. The paper also proposes an inexact minimization, and a dynamic regularizer. The proposed scheme can be applied to both convex and non-convex settings, where devices are used for parallelizing gradient computation, and devices are also used for training. Experiments on real and synthetic data show that the proposed Federated Learning problem can be solved with device level computations. "
1609,SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"supervised counterparts USED-FOR computer vision tasks. representations COMPARE supervised counterparts. supervised counterparts COMPARE representations. representations USED-FOR computer vision tasks. self - supervised approaches USED-FOR representations. accuracy EVALUATE-FOR contrastive learning algorithms. contrastive learning USED-FOR similarity ( dissimilarity ). similarity FEATURE-OF intermediate layers. similarity USED-FOR similarity. intermediate losses USED-FOR selection. SimCLR CONJUNCTION SwAV. SwAV CONJUNCTION SimCLR. MOCO CONJUNCTION SimCLR. SimCLR CONJUNCTION MOCO. method USED-FOR MOCO. method USED-FOR SimCLR. method USED-FOR SwAV. ImageNet linear classification CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet linear classification. Method are self - supervised methods, and back - propagation. Task is optimizing similarity ( dissimilarity ). OtherScientificTerm are intermediate contrastive losses, and gradient descent update. Metric is computational cost. ","This paper studies the problem of optimizing similarity (dissimilarity) between self-supervised and supervised representations for computer vision tasks. The authors propose a new method, SimCLR, that uses intermediate contrastive losses to improve the performance of the representations compared to the supervised counterparts. The main idea is to use contrastive learning to learn the similarity between the intermediate layers of the two intermediate layers, and then use these intermediate losses to guide the selection of the final representation. The paper shows that the proposed method is able to achieve better performance than MOCO, MOCR, and SwAV on ImageNet linear classification and downstream tasks.","This paper proposes a new method for optimizing similarity (dissimilarity) between two layers of a neural network. The key idea is to use contrastive learning to improve the similarity between the intermediate layers of the network. This is done by learning intermediate contrastive losses for selection and back-propagation. The proposed method is evaluated on ImageNet linear classification, MOCO, SimCLR, and SwAV. The results show that the proposed method outperforms the state-of-the-art self-supervised methods."
1625,SP:5b5e705ea1ee1b857e17e64d560a39052804949d,actor - critic HYPONYM-OF reinforcement learning algorithms. global convergence CONJUNCTION global optimality. global optimality CONJUNCTION global convergence. global optimality FEATURE-OF actor - critic. global convergence FEATURE-OF actor - critic. bi - level or two - timescale updates USED-FOR actor - critic. policy gradient direction USED-FOR actor. critic USED-FOR policy gradient direction. Bellman evaluation operator USED-FOR critic update. linear or deep neural networks USED-FOR actor. linear or deep neural networks USED-FOR function approximation settings. rate of convergence CONJUNCTION global optimality. global optimality CONJUNCTION rate of convergence. global optimality FEATURE-OF single - timescale actor - critic. rate of convergence FEATURE-OF single - timescale actor - critic. linear function approximation USED-FOR single - timescale actor - critic. actorcritic with deep neural network USED-FOR globally optimal policy. nonlinear function approximation USED-FOR policy optimization. Task is single - timescale setting. Metric is sublinear O(K−1/2 ) rate. OtherScientificTerm is sublinear rate. ,"This paper studies the problem of actor-critic in reinforcement learning algorithms. The authors consider the single-timescale setting, where the bi-level or two-timecale updates are not available. The actor is trained using linear or deep neural networks to learn the policy gradient direction for the actor. The critic update is performed by a Bellman evaluation operator. The rate of convergence of the actor and the global optimality of the critic are studied. The sublinear O(K−1/2) rate is used to evaluate the performance of the sublinear rate. Finally, the authors propose a nonlinear function approximation for policy optimization, which can be combined with an actorcritic with deep neural network to obtain a globally optimal policy.","This paper proposes an actor-critic for reinforcement learning algorithms. The main idea is to use bi-level or two-timescale updates to improve the global convergence of the actor and the global optimality of the critic. The actor is trained with linear or deep neural networks, and the critic update is performed with a Bellman evaluation operator. The policy gradient direction is learned by the actor. The authors show that the rate of convergence of a single-timecale actor-conductor with linear function approximation is better than that of an actor with a nonlinear function approximation for policy optimization. The paper also shows that the sublinear O(K−1/2) rate is a better rate than a sublinear rate for the actor with deep neural network."
1641,SP:26705a4dc305cce336f657c5937d1f5b4209548a,events CONJUNCTION messages. messages CONJUNCTION events. messages CONJUNCTION transactions. transactions CONJUNCTION messages. Log files USED-FOR events. Log files USED-FOR messages. Log files USED-FOR transactions. computer systems FEATURE-OF Log files. they USED-FOR structured textual and numerical data. natural languages CONJUNCTION temporal signals. temporal signals CONJUNCTION natural languages. logs USED-FOR sequential forms of data. natural languages HYPONYM-OF sequential forms of data. temporal signals HYPONYM-OF sequential forms of data. log level CONJUNCTION log sequence level. log sequence level CONJUNCTION log level. field level CONJUNCTION log level. log level CONJUNCTION field level. representation USED-FOR level. vector format FEATURE-OF representations. Transformer Networks ( TNs ) USED-FOR numerical and textual information. Transformer Networks ( TNs ) USED-FOR log embeddings. representation USED-FOR log processing applications. Material is Logs. ,"This paper proposes a new representation for log embeddings, called Logs. Logs are a family of log files that encode events, messages, and transactions in a vector format. Log files are used in computer systems to store structured textual and numerical data, and they can be used to represent sequential forms of data such as natural languages, temporal signals, and log level. The authors propose to use Transformer Networks (TNs) to encode both numerical and textual information in the representation. The proposed representation can be applied to a variety of log processing applications, from the log level to the log sequence level.","This paper proposes a new representation for log processing applications based on Transformer Networks (TNs) for both numerical and textual information. Logs are a family of computer systems that can store structured textual and numerical data in a vector format. Log files are used to store events, messages, and transactions, and they can be used to encode sequential forms of data such as natural languages, temporal signals, and log level, log sequence level, and field level. "
1657,SP:165c51a16f17fb8726e968f8b34742b62011d60e,"CNN kernels CONJUNCTION oriented Gabor filters. oriented Gabor filters CONJUNCTION CNN kernels. freely - trained mixture weights USED-FOR wavelet packet decompositions. AlexNet architecture USED-FOR image classification. AlexNet architecture FEATURE-OF wavelet decompositions. wavelet decompositions USED-FOR approach. directional selectivity CONJUNCTION shift invariance. shift invariance CONJUNCTION directional selectivity. feature extraction properties USED-FOR two. shift invariance HYPONYM-OF feature extraction properties. directional selectivity HYPONYM-OF feature extraction properties. separable wavelet packet transform USED-FOR variant. accuracy rate EVALUATE-FOR AlexNet. mathematical theory USED-FOR network. Task is deep convolutional neural networks ( CNNs ). Generic are formalism, and them. Method is convolutional layers. ",This paper studies the problem of deep convolutional neural networks (CNNs). The authors propose a new approach based on wavelet decompositions based on freely-trained mixture weights. The proposed AlexNet architecture can be used for image classification. The two main feature extraction properties of the two are directional selectivity and shift invariance. The authors also propose a variant of the separable wavelet packet transform to improve the accuracy rate of AlexNet. The theoretical theory behind the proposed network is well-motivated and the experimental results demonstrate the effectiveness of the proposed method.,"This paper proposes a novel approach to learn wavelet decompositions of wavelet packet decomposions with freely-trained mixture weights. The proposed approach is based on the AlexNet architecture for image classification. The authors propose two novel feature extraction properties: directional selectivity and shift invariance. The two properties are derived from the mathematical theory of deep convolutional neural networks (CNNs). The formalism of the two properties is well-motivated, and the authors show that they can be used to improve the accuracy rate of the network. "
1673,SP:d0a284da462584724ba6a3a48c9e986d391233f6,"dynamic composition FEATURE-OF Coordinating teams. variational objective USED-FOR learning. attention mechanism USED-FOR dynamic team composition. attention mechanism USED-FOR heterogeneous agents. multi - agent particle environment FEATURE-OF resource collection tasks. resource collection tasks EVALUATE-FOR methods. zero - shot generalization USED-FOR team compositions. heterogeneous agents USED-FOR zero - shot generalization. coach USED-FOR dynamic teams. Task are real - world multi - agent teams, and real - world team sports. OtherScientificTerm is optimal team strategy. Method are coach - player framework, adaptive communication method, and adaptive communication strategy. Generic is method. ","This paper studies the problem of dynamic composition in Coordinating teams. The authors propose a coach-player framework where the goal is to learn the optimal team strategy in a multi-agent particle environment. The learning objective is based on a variational objective. The adaptive communication method is also proposed. The proposed method is evaluated on a variety of resource collection tasks, and the results show that the proposed methods achieve better zero-shot generalization for team compositions with heterogeneous agents. ","This paper proposes a novel approach to learning a dynamic composition of Coordinating teams. The authors propose a variational objective for learning the optimal team strategy. They propose a coach-player framework, where the adaptive communication method is used to learn the optimal communication strategy. The proposed method is evaluated on resource collection tasks in a multi-agent particle environment. They show that the proposed methods can achieve zero-shot generalization for team compositions with heterogeneous agents. They also show that dynamic teams can be learned with a single coach. "
1689,SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"machine learning interpretability CONJUNCTION uncertainty estimation. uncertainty estimation CONJUNCTION machine learning interpretability. Influence functions USED-FOR machine learning interpretability. Influence functions USED-FOR uncertainty estimation. gradients CONJUNCTION Hessian. Hessian CONJUNCTION gradients. Hessian FEATURE-OF model. gradients FEATURE-OF model. Hessian USED-FOR post - hoc method. gradients USED-FOR post - hoc method. influence functions USED-FOR linear models. Influence functions PART-OF deep learning. non - convex loss functions FEATURE-OF deep learning. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Iris CONJUNCTION MNIST. MNIST CONJUNCTION Iris. influence functions PART-OF neural network models. datasets USED-FOR neural network models. Iris HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. model parameterization CONJUNCTION regularization techniques. regularization techniques CONJUNCTION model parameterization. regularization techniques USED-FOR influence functions. influence functions EVALUATE-FOR network architecture. accuracy EVALUATE-FOR influence estimates. influence estimates EVALUATE-FOR shallow networks. influence estimation methods USED-FOR non - convex setups. influence functions PART-OF deep learning. OtherScientificTerm are test - time predictions, convexity of the underlying loss function, and model changes. Task is estimating group influences. Metric is accuracy of influence functions. Generic is deeper networks. Method are network architectures, and weight - decay regularization. ",This paper studies the problem of estimating group influences in deep neural networks. Influence functions in deep learning are important for machine learning interpretability and uncertainty estimation. The authors propose a post-hoc method based on the Hessian and the gradients of the model. They show that the influence functions of deep learning with non-convex loss functions can be used to improve the accuracy of the network architecture and improve the performance of deeper networks. They also show that these influence estimation methods can be applied to non-consvex setups. ,"This paper studies the influence functions in machine learning interpretability and uncertainty estimation. Influence functions are used in deep learning and linear models. The authors propose a post-hoc method based on the Hessian and gradients of the underlying loss function, which are used to estimate group influences on test-time predictions. They show that the accuracy of influence functions depends on the number of group influences. They also show that for deep learning with non-convex loss functions, influence functions can be used to improve the performance of the network architectures.  The authors also propose weight-decay regularization, which is used to regularize the weights of the model parameters. The influence functions are also used for model parameterization and regularization techniques. The paper also shows that influence functions improve the network architecture and accuracy of the influence estimates for shallow networks. "
1705,SP:5fea74a2031d097a99dacf613bedcb054b0c3831,Autoregressive language models USED-FOR downstream tasks. Autoregressive language models USED-FOR next word prediction. large text corpora USED-FOR next word prediction. large text corpora USED-FOR Autoregressive language models. zero - shot usage USED-FOR downstream tasks. next word prediction CONJUNCTION text classification. text classification CONJUNCTION next word prediction. language modeling HYPONYM-OF pretraining task. sentence completion tasks USED-FOR classification tasks of interest. language modeling USED-FOR downstream tasks. language models USED-FOR classification tasks. language models USED-FOR features. features USED-FOR classification tasks. crossentropy ( log - perplexity ) FEATURE-OF language models. objective function USED-FOR classification tasks. ,This paper proposes a new pretraining task called language modeling for next word prediction with large text corpora. Autoregressive language models are used to train downstream tasks with zero-shot usage. The authors show that language models can learn features for classification tasks with crossentropy (log-perplexity) and can be used as an objective function for the classification tasks of interest. ,"This paper proposes a new pretraining task, called language modeling, where Autoregressive language models are trained on a large text corpora for next word prediction and text classification with zero-shot usage. The authors show that language models can learn features for both classification tasks of interest with crossentropy (log-perplexity) and sentence completion tasks. They show that the proposed objective function can be used to perform classification tasks with the same objective function."
1721,SP:a67da438e9821010284416170c3699ae7ff96c99,"MIA approaches USED-FOR classification models. image translation HYPONYM-OF conditional image generation models. approach USED-FOR membership attacks. reconstruction error USED-FOR approach. difficulty score CONJUNCTION reconstruction error. reconstruction error CONJUNCTION difficulty score. difficulty score USED-FOR membership error. MIA accuracy EVALUATE-FOR membership error. Task is Membership inference attacks ( MIA ). Method are neural network model, machine learning, and MIA. OtherScientificTerm are overfitting, and Reconstruction error. Metric is reconstruction errors. Material is training set. ","This paper studies the problem of membership inference attacks (MIA) against classification models such as image translation and conditional image generation models. The authors propose a new approach for membership attacks based on the reconstruction error of the neural network model. The main idea is to use the difficulty score of the membership error as a proxy for membership error, and then use reconstruction error to estimate the reconstruction errors of the training set. The paper shows that the MIA accuracy is better than the original membership error in terms of both difficulty score and reconstruction error. ","This paper proposes a new approach for detecting membership attacks on classification models. Membership inference attacks (MIA) are commonly used in the context of machine learning, where a neural network model is trained with overfitting. The authors propose an approach to detect membership attacks based on the reconstruction error of conditional image generation models (e.g., image translation). The authors use the difficulty score and reconstruction error to measure the membership error, and show that the reconstruction errors are correlated with the membership errors. They also show that MIA improves the MIA accuracy on the training set."
1737,SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"it USED-FOR distribution learning problem. differentiable architecture search method USED-FOR distribution learning problem. random variables FEATURE-OF continuously relaxed architecture mixing weight. Dirichlet distribution USED-FOR random variables. pathwise derivatives USED-FOR Dirichlet parameters. gradient - based optimizer USED-FOR Dirichlet parameters. formulation USED-FOR stochasticity. generalization ability EVALUATE-FOR formulation. progressive learning scheme USED-FOR searching. searching USED-FOR large - scale tasks. progressive learning scheme USED-FOR large - scale tasks. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method are differentiable NAS, and neural architecture search algorithms. Generic are method, and datasets. Metric is test error. Material is NASBench-201. ",This paper proposes a differentiable architecture search method for the distribution learning problem with random variables in a continuously relaxed architecture mixing weight. The Dirichlet distribution of the random variables is learned by pathwise derivatives. The gradient-based optimizer is used to optimize the Dirichlett parameters. The authors show that the proposed formulation improves the generalization ability in terms of stochasticity. The proposed method is evaluated on NASBench201 and CIFAR-10 and ImageNet. The experimental results show the effectiveness of the proposed method. ,"This paper proposes a differentiable architecture search method for the distribution learning problem. The method is based on differentiable NAS, where the random variables of the continuously relaxed architecture mixing weight are sampled from the Dirichlet distribution. The Dirichlets are defined as pathwise derivatives of the parameters of the differentiable neural architecture search algorithms. The authors propose a gradient-based optimizer that optimizes Dirichle parameters with respect to the test error. The formulation is shown to improve the generalization ability of stochasticity. The proposed method is evaluated on CIFAR-10, ImageNet, and NASBench-201. The experiments show that the proposed method outperforms the existing neural architectures search algorithms on all three datasets. The paper also shows that the progressive learning scheme can be used for searching for large-scale tasks."
1753,SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"signed distance functions CONJUNCTION neural radiance fields. neural radiance fields CONJUNCTION signed distance functions. function approximators USED-FOR low - dimensional - but - complex functions. neural networks USED-FOR low - dimensional - but - complex functions. neural networks USED-FOR function approximators. high dimensional inputs USED-FOR deep networks. pixel coordinates USED-FOR images. sinusoidal nonlinearities CONJUNCTION Fourier features. Fourier features CONJUNCTION sinusoidal nonlinearities. elements USED-FOR positional encodings. Fourier features USED-FOR positional encodings. elements COMPARE ReLU networks. ReLU networks COMPARE elements. positional encodings COMPARE ReLU networks. ReLU networks COMPARE positional encodings. Fourier features HYPONYM-OF elements. sinusoidal nonlinearities HYPONYM-OF elements. function approximators USED-FOR problems. multiplicative filter networks HYPONYM-OF problems. multiplicative filter networks HYPONYM-OF function approximators. Fourier or Gabor basis functions USED-FOR linear function approximator. ReLU networks CONJUNCTION sinusoidal activation networks. sinusoidal activation networks CONJUNCTION ReLU networks. Fourier features CONJUNCTION ReLU networks. ReLU networks CONJUNCTION Fourier features. multiplicative filter networks COMPARE approaches. approaches COMPARE multiplicative filter networks. Fourier features CONJUNCTION sinusoidal activation networks. sinusoidal activation networks CONJUNCTION Fourier features. sinusoidal activation networks USED-FOR approaches. ReLU networks USED-FOR approaches. Fourier features USED-FOR approaches. OtherScientificTerm are differential equations, and compositional depth. Generic are networks, and representation. ","This paper proposes a new linear function approximator based on Fourier or Gabor basis functions. The authors propose to use neural networks to approximate low-dimensional-but-complex functions in the presence of signed distance functions and neural radiance fields. The proposed method can be applied to a wide range of problems, including multiplicative filter networks, sinusoidal nonlinearities, Fourier features, and positional encodings. The main contribution of the paper is to show that these elements can be combined with ReLU networks to improve the positional encoding performance of deep networks with high dimensional inputs. ","This paper proposes to use function approximators to approximate low-dimensional-but-complex functions in deep networks using neural networks. The authors use high dimensional inputs for training deep networks, where the input is a set of pixel coordinates, and the output of the networks is a representation of the input pixels. The input pixel coordinates are defined as the sum of the signed distance functions and the neural radiance fields. The corresponding differential equations can be expressed in terms of the compositional depth. The linear function approximation can be approximated using Fourier or Gabor basis functions. The proposed methods are evaluated on a variety of problems, including positional encodings, Fourier features, and sinusoidal nonlinearities. The results show that the proposed approaches outperform other approaches such as ReLU networks and sinuoidal activation networks."
1769,SP:f5be855300f63c185a006834302bd4b033b56258,"task - specific models CONJUNCTION meta - model. meta - model CONJUNCTION task - specific models. task - specific models PART-OF Gradient - based meta - learning. gradients USED-FOR meta - model. algorithm USED-FOR task - specific models. algorithm USED-FOR meta - model. meta - gradients USED-FOR meta - model. inner loop USED-FOR algorithm. inner loop USED-FOR task - specific models. teacherstudent scheme USED-FOR gradient - based meta - learning algorithms. student network USED-FOR task - specific models. lightweight computation graph USED-FOR meta - gradients. few - shot learning CONJUNCTION long - tailed classification. long - tailed classification CONJUNCTION few - shot learning. long - tailed classification CONJUNCTION meta - attack. meta - attack CONJUNCTION long - tailed classification. it USED-FOR meta - learning algorithms. it USED-FOR tasks. tasks EVALUATE-FOR meta - learning algorithms. meta - attack HYPONYM-OF tasks. meta - attack HYPONYM-OF meta - learning algorithms. few - shot learning HYPONYM-OF tasks. few - shot learning HYPONYM-OF meta - learning algorithms. long - tailed classification HYPONYM-OF tasks. long - tailed classification HYPONYM-OF meta - learning algorithms. Generic are loop, and approach. OtherScientificTerm are inner - loop optimization steps, high - order derivatives, and big memory footprints. ","This paper proposes a teacherstudent scheme for gradient-based meta-learning algorithms with an inner loop that learns task-specific models and a meta-model. The algorithm is based on a lightweight computation graph, where the meta-gradients are computed using the gradients of the meta - model. The inner-loop optimization steps are based on high-order derivatives, and the inner loop is used to learn the meta for each task. The authors show that the proposed algorithm is able to learn a good meta-neural network with high memory footprints, and it can be applied to a variety of tasks including few-shot learning, long-tailed classification, meta-attack, and long-tail classification.","This paper proposes a new algorithm for learning task-specific models and meta-model in Gradient-based meta-learning. The algorithm is based on the teacherstudent scheme. The main idea is to use the inner loop of the algorithm to learn the meta-neural network to predict the output of the inner-loop optimization steps. The inner loop is then used to train the task -specific models using the student network. The authors show that the proposed algorithm is able to generalize well to new tasks, and it can be applied to other meta-Learning algorithms such as meta-attack, few-shot learning, long-tailed classification, etc. The proposed approach is evaluated on a variety of tasks. "
1785,SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"Offline Reinforcement Learning ( RL ) USED-FOR policies. off - policy RL algorithms USED-FOR Offline RL. Behavior regularization USED-FOR off - policy algorithms. analytical upper bound USED-FOR behavior regularizor. analytical upper bound USED-FOR KL divergence. state - dependent Lagrange multipliers USED-FOR regularization term. state - dependent Lagrange multipliers USED-FOR distributing KL divergence penalty. Lagrange multipliers USED-FOR freedom of deviation. gradient penalty term USED-FOR gradient of the Q value. gradient penalty term USED-FOR policy evaluation objective. out - of - distribution actions FEATURE-OF gradient of the Q value. gradient penalty term USED-FOR catastrophic performance degradation. out - ofdistribution actions FEATURE-OF Q values. offline RL benchmarks EVALUATE-FOR BRAC+. offline RL benchmarks EVALUATE-FOR model - free and model - based approaches. BRAC+ COMPARE model - free and model - based approaches. model - free and model - based approaches COMPARE BRAC+. Method are Reinforcement Learning agent, and behavior regularized offline reinforcement learning. OtherScientificTerm are outof - distribution ( less explored ) actions, sample based estimations, sampled batch, low probability ( less explored ) states, and rare out - of - distribution actions. ","Offline Reinforcement Learning (RL) is an important problem in RL. Offline RL algorithms are usually off-policy RL algorithms. Behavior regularization is a common technique in off-Policy algorithms to improve the generalization performance of the Reformer Learning agent. However, behavior regularized offline reinforcement learning suffers from catastrophic performance degradation due to the out-of-distribution (less explored) actions. This paper proposes an analytical upper bound for the KL divergence of the behavior regularizor using state-dependent Lagrange multipliers for the regularization term. The gradient penalty term for the gradient of the Q value is used for the policy evaluation objective. The authors also provide sample based estimations for the sampled batch. The experimental results show that BRAC+ outperforms model-free and model-based approaches on several offline RL benchmarks.",Offline Reinforcement Learning (RL) is an important problem in reinforcement learning. Offline RL algorithms are typically off-policy RL algorithms for Offline RL. Behavior regularization is a key component of off-Policy algorithms. The authors propose a behavior regularizor that uses an analytical upper bound on the KL divergence between the outof-distribution (less explored) actions and the out-of-discretized (more explored) ones. The regularization term is based on the state-dependent Lagrange multipliers for the distributing KL divergence penalty and the freedom of deviation of the Q value. The gradient penalty term is used for the policy evaluation objective. The paper shows that BRAC+ outperforms model-free and model-based approaches on several offline RL benchmarks. 
1801,SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,"Smaller networks USED-FOR edge - devices. one - shot learning paradigm USED-FOR networks. regularization behavior FEATURE-OF adjoint training paradigm. Imagenet USED-FOR resnet-50. CIFAR-100 EVALUATE-FOR architecture. datasets EVALUATE-FOR network. network COMPARE network. network COMPARE network. datasets EVALUATE-FOR network. top-1 accuracy EVALUATE-FOR network. top-1 accuracy EVALUATE-FOR network. Task is compressing deep neural networks. Method are deep neural networks, Adjoined networks, CNN - based neural architecture, and adjoint networks. Generic is architectures. Metric are inference time, and accuracy. ",This paper studies the problem of compressing deep neural networks. The authors propose a new one-shot learning paradigm for training networks with edge-devices. Adjoined networks are trained with a CNN-based neural architecture. The main idea is to use the regularization behavior of the adjoint training paradigm in order to reduce the inference time. The proposed architecture is evaluated on CIFAR-100 and Resnet-50 using Imagenet. The experiments show that the proposed network achieves better top-1 accuracy than the original network. ,"This paper proposes a new architecture for compressing deep neural networks. Adjoined networks are a CNN-based neural architecture. The authors propose a one-shot learning paradigm to compress networks. They show that the regularization behavior of the adjoint training paradigm leads to better performance. They also show that smaller networks can be used to compress edge-devices. Finally, they show that their architecture can achieve better top-1 accuracy on CIFAR-100. "
1817,SP:dba40073f79143e5355d194aa16db9eee0267a5d,"exploration USED-FOR reinforcement learning ( RL ). exploration methods COMPARE counterparts. counterparts COMPARE exploration methods. -greedy HYPONYM-OF counterparts. -greedy USED-FOR exploration algorithm. duration distributions USED-FOR exploration. ecological models of animal foraging behaviour USED-FOR distributions. Generic is problem. Metric are complexity, and generality. OtherScientificTerm are dithering, temporal persistence, local optima, and random duration. Method is greedy exploration. ","This paper studies the problem of exploration in reinforcement learning (RL). The authors propose a new exploration algorithm, called greedy exploration, which is based on the notion of dithering. The authors show that the complexity of greedy exploration is bounded by a local optima, and that it is equivalent to the existing counterparts such as-greedy. They then propose to use duration distributions for exploration based on ecological models of animal foraging behaviour, and show that these distributions can be used to improve the generality.","This paper proposes a new approach to exploration in reinforcement learning (RL). The main idea is to use a greedy exploration algorithm called-greedy, where the goal is to maximize the complexity of the exploration algorithm. The authors show that the optimal exploration algorithm is -greedy with respect to its counterparts (e.g. the ones based on dithering, temporal persistence, and local optima). The authors also show that greedy exploration can be done with random duration distributions for exploration, which are based on ecological models of animal foraging behaviour. The generality of the problem is studied."
1833,SP:5efb581a368ace3bd085d48801a899559d6a43ef,"Matrix factorization USED-FOR implicit regularization of gradient descent. infinitesimal initialization USED-FOR Gradient Flow. gradient flow COMPARE heuristic rank minimization algorithm. heuristic rank minimization algorithm COMPARE gradient flow. infinitesimal initialization USED-FOR gradient flow. Greedy Low - Rank Learning HYPONYM-OF heuristic rank minimization algorithm. gradient flow USED-FOR depth-2 matrix factorization. OtherScientificTerm are nuclear norm, implicit regularization, and initialization magnitude. Method are norm minimization, rank minimization view, and rank minimization. Generic is convergence. ",This paper studies the problem of matrix factorization in implicit regularization of gradient descent. Gradient Flow with infinitesimal initialization is a variant of gradient flow with a nuclear norm. The authors show that gradient flow can be used as a heuristic rank minimization algorithm in Greedy Low-Rank Learning. They also show that the norm minimization can be improved by using rank minimisation view. ,"The paper proposes to use Matrix factorization for the implicit regularization of gradient descent. The authors show that the nuclear norm is a function of the initialization magnitude, and that the norm minimization can be viewed as a rank minimization view. Gradient Flow with infinitesimal initialization is based on gradient flow with gradient flow, and the authors show the convergence of gradient flow under depth-2 matrix factorization. They also show that gradient flow is equivalent to a heuristic rank maximization algorithm called Greedy Low-Rank Learning. "
1849,SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"Classifiers PART-OF machine learning. two - stage framework USED-FOR robustness. data augmentations USED-FOR subgroup features. data augmentations USED-FOR classifier. CycleGAN USED-FOR intra - class, inter - subgroup augmentations. theoretically - motivated subgroup consistency regularizer CONJUNCTION robust objective. robust objective CONJUNCTION theoretically - motivated subgroup consistency regularizer. CycleGAN USED-FOR CAMEL. CAMEL USED-FOR model patching. robust error EVALUATE-FOR baseline. CAMEL COMPARE baseline. baseline COMPARE CAMEL. robust error EVALUATE-FOR CAMEL. benchmark datasets EVALUATE-FOR CAMEL. CAMEL USED-FOR model. Generic is models. Task is skin cancer classification. OtherScientificTerm are spurious bandage, subgroup differences, class information, semantic transformations, and spurious features. Method is Model patching. Material is real - world skin cancer dataset. ","This paper proposes a two-stage framework to improve the robustness of models in the context of skin cancer classification. The main idea is to use CycleGAN to learn intra-class, inter-subgroup augmentations of the data augmentations for the classifier. The authors show that the proposed CAMEL improves the robust error of the baseline by a factor of 2.5 on the real-world skin cancer dataset. The paper also shows that the theoretically-motivated subgroup consistency regularizer and the robust objective of CAMEL can be applied to model patching. ","The paper proposes a two-stage framework to improve robustness of classifiers in machine learning. Specifically, the authors propose to use CycleGAN for intra-class, inter-subgroup augmentations, and a theoretically-motivated subgroup consistency regularizer to improve the robustness. Model patching is also proposed. Experiments on real-world skin cancer dataset show that CAMEL improves the robust error over the baseline and outperforms the model patching on several benchmark datasets. "
1865,SP:de6cea1e35a0555175e17546a93422e9a96a511e,transparent inner structures CONJUNCTION model expressivity. model expressivity CONJUNCTION transparent inner structures. model interpretability FEATURE-OF transparent inner structures. decision trees HYPONYM-OF Rule - based models. large data sets EVALUATE-FOR rule - based models. Ensemble methods CONJUNCTION fuzzy / soft rules. fuzzy / soft rules CONJUNCTION Ensemble methods. interpretable nonfuzzy rules USED-FOR data representation. classifier USED-FOR interpretable nonfuzzy rules. Rulebased Representation Learner ( RRL ) HYPONYM-OF classifier. it USED-FOR continuous space. training method USED-FOR discrete model. Gradient Grafting HYPONYM-OF training method. gradient descent USED-FOR training method. gradient descent USED-FOR discrete model. logical activation functions USED-FOR RRL. scalability EVALUATE-FOR RRL. it USED-FOR continuous features. logical activation functions USED-FOR it. RRL COMPARE approaches. approaches COMPARE RRL. small and 4 large data sets EVALUATE-FOR RRL. RRL COMPARE decision trees. decision trees COMPARE RRL. complexity EVALUATE-FOR decision trees. complexity EVALUATE-FOR RRL. OtherScientificTerm is discrete parameters and structures. Method is non - differentiable RRL. ,"This paper studies the problem of learning a non-differentiable RRL. Rule-based models such as decision trees, Ensemble methods, and fuzzy/soft rules have been shown to perform well on large data sets. The authors propose a new training method called Gradient Grafting to train a discrete model by gradient descent. The proposed classifier, Rulebased Representation Learner (RRL), learns interpretable nonfuzzy rules for the data representation and then uses it to map the continuous space to discrete parameters and structures. The paper shows that RRL achieves better scalability than existing approaches on both small and 4 large datasets. ","This paper proposes a non-differentiable RRL, which is based on the Rulebased Representation Learner (RRL), a classifier that learns interpretable nonfuzzy rules for the data representation. The authors show that RRL achieves better scalability on small and 4 large data sets compared to other rule-based models such as Ensemble methods and fuzzy/soft rules. In addition, the authors propose a new training method for a discrete model based on gradient descent. They show that it can learn continuous features in a continuous space with logical activation functions. They also show that the complexity of RRL is comparable to decision trees."
1881,SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"molecular property prediction HYPONYM-OF biochemical applications. models USED-FOR biochemical applications. molecular scaffolds CONJUNCTION protein families. protein families CONJUNCTION molecular scaffolds. natural environments USED-FOR tasks. complex descriptors USED-FOR natural environments. protein families HYPONYM-OF complex descriptors. molecular scaffolds HYPONYM-OF complex descriptors. regret minimization ( RGM ) algorithm USED-FOR structured environments. representation USED-FOR predictor. hindsight access FEATURE-OF held - out environments. representation USED-FOR RGM. invariant risk minimization ( IRM ) USED-FOR RGM. specialized domain perturbations USED-FOR structured extension. RGM COMPARE baselines. baselines COMPARE RGM. molecular property prediction CONJUNCTION protein homology and stability prediction. protein homology and stability prediction CONJUNCTION molecular property prediction. applications EVALUATE-FOR RGM. molecular property prediction EVALUATE-FOR method. applications EVALUATE-FOR method. protein homology and stability prediction HYPONYM-OF applications. molecular property prediction HYPONYM-OF applications. OtherScientificTerm are environments, simultaneous optimality condition, and complex environments. Metric is predictive regret. ","This paper proposes a new regret minimization (RGM) algorithm for structured environments. RGM is based on invariant risk minimization, which is an extension of IRM. The authors show that RGM can be applied to a wide range of biological applications, including molecular property prediction, protein homology and stability prediction, and molecular scaffolds. The proposed method is evaluated on a variety of synthetic and real-world datasets.","This paper proposes a novel regret minimization (RGM) algorithm for structured environments. The authors show that the proposed RGM can be applied to a wide range of models for a variety of biochemical applications, including molecular property prediction, protein homology and stability prediction, and molecular scaffolds. They also show that RGM is invariant risk minimized (IRM) and can be used in natural environments for tasks with complex descriptors (e.g., protein families, protein scaffolds, etc.). The authors also propose a structured extension of RGM with specialized domain perturbations. They show that in held-out environments with hindsight access, RGM achieves better predictive regret than baselines in terms of the simultaneous optimality condition. "
1897,SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,BERT USED-FOR NLP tasks. BERT USED-FOR text - vision BERT models. cross - modal attentions USED-FOR text - vision BERT models. text - vision BERT models USED-FOR language - vision tasks. text - image retrieval HYPONYM-OF language - vision tasks. cross - modal attentions USED-FOR textvision BERT models. cross - modal attentions USED-FOR textvision retrieval. textvision retrieval USED-FOR large - scale search. computation cost EVALUATE-FOR cross - modal attentions. cross - probe BERT HYPONYM-OF architecture. text and vision probes USED-FOR cross - modal attentions. text and vision probes USED-FOR It. It USED-FOR crossmodal attention. Generic is method. ,"This paper proposes a new BERT for NLP tasks. It uses cross-modal attentions for text-vision BERT models for language-vision tasks such as text-image retrieval and text-speech recognition. The proposed architecture, called cross-probe BERT, is based on text and vision probes. It is able to perform crossmodal attention with a small computation cost. The authors demonstrate the effectiveness of the proposed method on large-scale search and textvision retrieval.","This paper proposes a new architecture called cross-probe BERT for NLP tasks. It uses cross-modal attentions for text-vision BERT models for language-vision tasks such as text-image retrieval and large-scale search. It is based on text and vision probes to learn crossmodal attention. The proposed method is evaluated on textvision retrieval, and the computation cost is competitive with BERT."
1913,SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,Actor USED-FOR Actor - Critic algorithms. FORK HYPONYM-OF Actor. forward - looking Actor HYPONYM-OF Actor. FORK PART-OF model - free ActorCritic algorithm. continuous state and action spaces FEATURE-OF Box2D and MuJoCo environments. FORK USED-FOR BipedalWalkerHardcore. GPU USED-FOR FORK. Generic is algorithms. ,"This paper proposes a new Actor-Critic algorithms, called FORK, which is a model-free ActorCritic algorithm based on the forward-looking Actor. The actor is trained in continuous state and action spaces in Box2D and MuJoCo environments. ForK is trained on a GPU, and can be applied to BipedalWalkerHardcore. The authors show that the proposed algorithms outperform the baselines.","This paper proposes a new actor-critic algorithms, called FORK. The Actor is a forward-looking Actor. ForK is a model-free ActorCritic algorithm. The proposed algorithms are evaluated on continuous state and action spaces in Box2D and MuJoCo environments. For example, for BipedalWalkerHardcore, the GPU is used to compute the forK."
1929,SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"Federated learning USED-FOR global model. local models PART-OF global model. Bayesian inference perspective USED-FOR aggregation algorithm. FEDBE HYPONYM-OF aggregation algorithm. Bayesian model Ensemble USED-FOR them. Gaussian or Dirichlet distribution USED-FOR local models. Gaussian or Dirichlet distribution USED-FOR model distribution. FEDBE USED-FOR regularizing users ’ model training. Material is non - i.i.d. data. Method are global models, neural networks, aggregation method, and federated learning algorithm. Generic is it. ","This paper proposes a new aggregation algorithm, FEDBE, based on the Bayesian inference perspective. The authors propose a federated learning approach to learn a global model with local models. The local models are learned from the Gaussian or Dirichlet distribution of the local models, and the global model is learned from a Bayesian model Ensemble. The global models are trained using neural networks. The aggregation method is based on an existing aggregation method, and it can be applied to non-i.i.d. data.  The authors show that the proposed aggregation algorithm (FEDBE) can be used for regularizing users’ model training. ","This paper proposes a federated learning for learning a global model with local models. The aggregation algorithm is based on the Bayesian inference perspective. The local models are learned from the Gaussian or Dirichlet distribution of the model distribution. The global models are trained on non-i.i.d. data. The Bayesian model Ensemble is used to learn them. The authors show that the proposed aggregation method, FEDBE, can be used for regularizing users’ model training. "
1945,SP:3ac5f437fc349a33810d0645664d1c448528af74,,This paper proposes a new method for training deep neural networks. The proposed method is based on the idea that the weights of a neural network can be represented as a weighted sum of its weights. The authors show that the proposed method can be used to train a network with a large number of weights. They also show that their method is able to achieve better performance than the baselines. ,This paper presents a new method for learning a neural network model that is able to predict the future state of the world. The method is based on the idea of learning a model that predicts the future states of the environment. The authors show that the proposed method can be used to predict future states in the presence of the current state. 
1961,SP:efa2343ead47263a0d09e1c17f9aa044605b9650,settling time FEATURE-OF deep neural networks. priori upper bound FEATURE-OF deep neural networks. Lyapunov based analysis USED-FOR loss function. Lyapunov based analysis USED-FOR priori upper bound. settling time FEATURE-OF priori upper bound. control theory framework USED-FOR deep learning. deterministic control theoretic setting FEATURE-OF priori guarantees of finite - time convergence. tracking problem USED-FOR learning. control problem USED-FOR supervised learning framework. analytical formula USED-FOR finite - time upper bound. analytical formula USED-FOR settling time. settling time FEATURE-OF finite - time upper bound. input perturbations FEATURE-OF loss function. Method is priori finite time convergence analysis. Generic is network. OtherScientificTerm is control inputs. ,"This paper studies the priori finite time convergence analysis of deep neural networks with settling time in the deterministic control theoretic setting. The authors propose a Lyapunov based analysis of the settling time of the loss function with input perturbations, and derive a priori upper bound of the prior for this loss function based on the analytical formula. They then propose a control theory framework for deep learning with the tracking problem, and show that this priori guarantees of finite-time convergence can be obtained for any network with control inputs. Finally, they provide a supervised learning framework for this control problem.",This paper proposes a priori finite time convergence analysis of deep neural networks with settling time. The priori upper bound is based on Lyapunov based analysis of the loss function with input perturbations. The authors show that the priori guarantees of finite-time convergence in the deterministic control theoretic setting can be derived from the analytical formula for the settling time of the prior. The paper also proposes a control theory framework for deep learning with a tracking problem. The proposed supervised learning framework uses the control problem as a control problem to guide the learning of the network. 
1977,SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,Disentanglement of representations USED-FOR representations. incompressible - flow networks ( GIN ) USED-FOR latent variables. incompressible - flow networks ( GIN ) USED-FOR compact and disentangled representation. GIN USED-FOR informative latent variables selection. method USED-FOR informative latent variables selection. GIN USED-FOR method. mutual information USED-FOR informative latent variables. latent variables CONJUNCTION auxiliary variable. auxiliary variable CONJUNCTION latent variables. mutual information USED-FOR auxiliary variable. synthetic data EVALUATE-FOR method. outlier detection CONJUNCTION adversarial attack defence. adversarial attack defence CONJUNCTION outlier detection. classification CONJUNCTION outlier detection. outlier detection CONJUNCTION classification. downstream tasks EVALUATE-FOR method. synthetic and real data EVALUATE-FOR adversarial attack defence. classification EVALUATE-FOR method. adversarial attack defence HYPONYM-OF downstream tasks. classification HYPONYM-OF downstream tasks. outlier detection HYPONYM-OF downstream tasks. Task is machine learning. Method is nonlinear independent component analysis theory. ,This paper studies the problem of disentanglement of representations in machine learning. The authors propose a method based on incompressible-flow networks (GIN) to learn the compact and disentangled representation of latent variables. The proposed method uses GIN to perform informative latent variables selection using mutual information between the latent variables and an auxiliary variable. The method is evaluated on synthetic data and two downstream tasks: outlier detection and adversarial attack defence on synthetic and real data.,"This paper proposes a method for learning compact and disentangled representation by using incompressible-flow networks (GIN) for learning the latent variables and auxiliary variables. The method is based on the nonlinear independent component analysis theory. The authors propose to use GIN for informative latent variables selection and mutual information for the auxiliary variable. The proposed method is evaluated on synthetic data and on several downstream tasks including outlier detection, adversarial attack defence, and classification. "
1993,SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"Pooling PART-OF convolutional neural networks. pooling operations USED-FOR feature maps. feature maps HYPONYM-OF lossy process. LiftDownPool CONJUNCTION LiftUpPool. LiftUpPool CONJUNCTION LiftDownPool. LiftPool USED-FOR bidirectional pooling layers. Lifting Scheme USED-FOR signal processing. LiftUpPool HYPONYM-OF bidirectional pooling layers. LiftDownPool HYPONYM-OF bidirectional pooling layers. LiftDownPool HYPONYM-OF LiftPool. LiftUpPool HYPONYM-OF LiftPool. LiftDownPool USED-FOR feature map. LiftDownPool USED-FOR downsized sub - bands. pooling function PART-OF LiftDownPool. image classification and semantic segmentation EVALUATE-FOR methods. backbones USED-FOR image classification and semantic segmentation. backbones USED-FOR methods. input corruptions CONJUNCTION perturbations. perturbations CONJUNCTION input corruptions. input corruptions FEATURE-OF robustness. robustness EVALUATE-FOR LiftDownPool. OtherScientificTerm are receptive fields, input variations, and downscaled feature map. Generic is they. Task are downsampling, and image - to - image translation challenges. Method is up - pooling layer LiftUpPool. ","This paper studies the problem of downsampling in convolutional neural networks. In this lossy process, feature maps are generated by pooling operations on the feature maps. The goal is to reduce the number of receptive fields in the input variations. The authors propose LiftPool, a bidirectional pooling layers (LiftUpPool and LiftUpPool) that is based on LiftPool. LiftDownPool is an up-pooling layer that is trained with the pooling function of LiftUppool. The main contribution of the paper is the Lifting Scheme for signal processing.  The authors show that the proposed methods have better robustness against input corruptions and perturbations than existing backbones for image classification and semantic segmentation.  ","This paper proposes a new lossy process, called feature maps, which is an extension of pooling in convolutional neural networks. The key idea is to use pooling operations to generate feature maps that are more robust to downsampling. The authors propose LiftPool, which consists of two bidirectional pooling layers, LiftUpPool and LiftDownPool, where the first is an up-pooling layer and the second is a downscaled feature map. The Lifting Scheme is used for signal processing, and the authors show that they are robust to input corruptions and perturbations. They also show that the downsized sub-bands of LiftDownpool can be decomposed into two parts, one for receptive fields and one for input variations. The proposed methods are evaluated on image classification and semantic segmentation with backbones. "
2009,SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"stable noise - shaping quantization scheme USED-FOR embedding method. fast linear transformation USED-FOR ` 1 norm. fast linear transformation USED-FOR Euclidean distances. ` 1 norm USED-FOR Euclidean distances. well - spread data EVALUATE-FOR method. time complexity EVALUATE-FOR method. space complexity EVALUATE-FOR method. continuous valued Johnson - Lindenstrauss embedding CONJUNCTION quantization error. quantization error CONJUNCTION continuous valued Johnson - Lindenstrauss embedding. polynomial decay FEATURE-OF quantization error. accuracy EVALUATE-FOR binary codes. natural images EVALUATE-FOR method. Material is high - dimensional dataset. OtherScientificTerm are binary sequences, T, sparse Gaussian random matrix, Hamming distance, Walsh - Hadamard matrix, and embedding dimension. Method is binary embedding methods. Task is embedding. Generic are approach, and it. ","This paper proposes a stable noise-shaping quantization scheme for embedding method based on stable noise. The proposed approach is based on a fast linear transformation of Euclidean distances using the `1 norm' of the ` 1 norm. The embedding dimension of the embedding is defined by a sparse Gaussian random matrix, where T is the number of binary sequences. The authors show that the proposed method can be applied to well-spread data with high-dimensional dataset. The time complexity of the method is shown to be much lower than existing binary embedding methods. The quantization error of the proposed quantization based on polynomial decay is also demonstrated. The accuracy of the binary codes is shown on natural images.","The paper proposes a stable noise-shaping quantization scheme for embedding method. The approach is based on the recent work on binary embedding methods. The key idea is to use a fast linear transformation of the Euclidean distances to the `1 norm of the ` 1 norm' of the binary sequences, which is a sparse Gaussian random matrix. Hamming distance is defined by a Walsh-Hadamard matrix. The proposed method is evaluated on well-spread data, and the time complexity of the quantization error is measured by polynomial decay. The accuracy of binary codes is measured on natural images, and it is shown that the proposed quantization dimension is close to the true embedding dimension."
2025,SP:f65e229bca3904095743e7a501b1083cc60f1e22,"robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. process USED-FOR ANNs. synaptic plasticity rules USED-FOR Gradient Descent ( GD ). rule parameters USED-FOR GD. GD USED-FOR rules. plasticity rules USED-FOR recurrent neural nets ( RNNs ). GD USED-FOR plasticity rules. rules USED-FOR MNIST / Fashion MNIST. synthetic data USED-FOR rules. process USED-FOR plasticity rules. adversarial perturbations FEATURE-OF tolerance. tolerance FEATURE-OF classifiers. plasticity rules USED-FOR classifiers. perceptron algorithm CONJUNCTION multiplicative weights method. multiplicative weights method CONJUNCTION perceptron algorithm. GD USED-FOR plasticity rule. GD USED-FOR perceptron algorithm. GD USED-FOR multiplicative weights method. GD USED-FOR learning rules. evolutionary time FEATURE-OF it. Task are learning tasks, and genetic setting. Method are artificial neural nets ( ANNs ), backpropagation, and classification network. Generic is data. OtherScientificTerm is numerical parameter. ","This paper studies the problem of backpropagation in artificial neural nets (ANNs). The authors propose a new process for training ANNs based on synaptic plasticity rules, called Gradient Descent (GD). GD is used to learn rules for recurrent neural nets [1]. The rules are used to train MNIST/Fashion MNIST on synthetic data. The authors show that GD improves the robustness and generalization of the classifiers in terms of tolerance to adversarial perturbations in the genetic setting [2]. The authors also show how GD can be used to improve the performance of the perceptron algorithm and the multiplicative weights method [3]. ","This paper proposes a new process for training artificial neural nets (ANNs), called Gradient Descent (GD), which is a process to improve the robustness and generalization of ANNs. The main idea is to use synaptic plasticity rules to train recurrent neural nets on synthetic data, and then use GD to train the rules for MNIST/Fashion MNIST on the synthetic data. The authors show that GD is able to learn rules that are robust to adversarial perturbations in the genetic setting. They also show that the tolerance of classifiers can be improved by using GD to learn the rules, and that GD can also be used to learn learning rules in the context of a perceptron algorithm and a multiplicative weights method. "
2041,SP:f435530146fa975cb27cd375a857df9bcbd87682,"visual question generation ( VQG ) USED-FOR human - like questions. image CONJUNCTION side information. side information CONJUNCTION image. image USED-FOR visual question generation ( VQG ). side information USED-FOR human - like questions. image USED-FOR human - like questions. visual objects PART-OF image. side information CONJUNCTION image. image CONJUNCTION side information. image USED-FOR generating referential and meaningful questions. learning paradigm USED-FOR visual questions. answer - awareness CONJUNCTION region - reference. region - reference CONJUNCTION answer - awareness. region - reference USED-FOR learning paradigm. answer - awareness FEATURE-OF visual questions. Double Hints textual answers CONJUNCTION visual regions of interests. visual regions of interests CONJUNCTION Double Hints textual answers. Double Hints textual answers USED-FOR visual questions. methodology USED-FOR visual hints. dynamic graph USED-FOR them. VQA2.0 CONJUNCTION COCO - QA datasets. COCO - QA datasets CONJUNCTION VQA2.0. model COMPARE baselines. baselines COMPARE model. COCO - QA datasets EVALUATE-FOR model. VQA2.0 EVALUATE-FOR model. COCO - QA datasets EVALUATE-FOR baselines. setting EVALUATE-FOR model. Task are VQG, and one - to - many mapping issue. OtherScientificTerm are human annotations, implicit topology end - to - end, and double hints. Method is graph - to - sequence model. ","This paper proposes a graph-to-sequence model for visual question generation (VQG) that combines image, side information, and side information to generate human-like questions from an image. The image is used for generating referential and meaningful questions from visual objects in the image, and the side information is used to generate visual questions. The learning paradigm for visual questions is based on answer-awareness, region-reference, and a learning paradigm that combines answer-aware and region-based knowledge. The proposed VQG is evaluated on VQA2.0 and COCO-QA datasets, and shows that the proposed model performs better than baselines in the setting.   ","This paper proposes a graph-to-sequence model for visual question generation (VQG) for generating referential and meaningful questions from an image. The learning paradigm for visual questions is based on answer-awareness, region-reference, and side information of the image, which are visual objects in the image. In VQG, the human annotations are represented as a dynamic graph. The authors propose a new methodology for learning visual hints, which is a combination of Double Hints textual answers, visual regions of interests, and an implicit topology end-to -end. The proposed model is evaluated on COCO-QA datasets and VQA2.0 in a single setting, and is shown to outperform the baselines. "
2057,SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,sample size CONJUNCTION model size. model size CONJUNCTION sample size. linear regression CONJUNCTION neural networks. neural networks CONJUNCTION linear regression. linear regression HYPONYM-OF learning algorithms. neural networks HYPONYM-OF learning algorithms. optimal regularization USED-FOR double - descent phenomenon. sample size CONJUNCTION model size. model size CONJUNCTION sample size. isotropic data distribution FEATURE-OF linear regression models. optimally - tuned ` 2 regularization USED-FOR linear regression models. optimally - tuned ` 2 regularization USED-FOR double descent. test risk scalings EVALUATE-FOR algorithms. tuned regularization USED-FOR algorithms. tuned regularization USED-FOR test risk scalings. Task is generalization. ,"This paper studies the double-descent phenomenon in linear regression models with isotropic data distribution. The authors propose two learning algorithms, linear regression and neural networks, which are based on optimally-tuned `2 regularization for double descent. The proposed algorithms are evaluated on a variety of test risk scalings using tuned regularization. ","This paper studies the double-descent phenomenon of linear regression with optimally-tuned `2 regularization for linear regression models with isotropic data distribution. The authors propose two learning algorithms, linear regression and neural networks, with different sample size, model size, and model size. The main contribution of the paper is to study the generalization of the proposed algorithms with tuned regularization to test risk scalings."
2073,SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,spatial regularities FEATURE-OF images. spatial regularities USED-FOR generative modeling. neural network USED-FOR building image generators ( decoders ). it USED-FOR variational autoencoders ( VAEs ). sequential gating - based mechanism USED-FOR contextual information. feature maps PART-OF deep neural net. sequential gating - based mechanism USED-FOR feature maps. spatial dependency layers USED-FOR density estimation. decoder USED-FOR density estimation. decoder USED-FOR hierarchical VAE. baseline convolutional architectures USED-FOR density estimation. spatial dependency layers USED-FOR hierarchical VAE. spatial dependency layers USED-FOR decoder. SDN USED-FOR large images. SDN decoder USED-FOR learning disentangled representations. neural architectures USED-FOR task. SDN decoder USED-FOR vanilla VAE setting. spatial dependency COMPARE convolutional layers. convolutional layers COMPARE spatial dependency. Method is spatial dependency networks ( SDNs ). OtherScientificTerm is 2 - D space. Generic is models. Material is VAE settings. ,This paper studies the problem of building image generators (decoders) with spatial regularities in the context of generative modeling. The authors propose a neural network that can be used as a decoder to learn the spatial dependency networks (SDNs) in the 2-D space. The SDN decoder is trained with a sequential gating-based mechanism to extract contextual information from the feature maps in a deep neural net. The spatial dependency layers of the decoder are used for density estimation using baseline convolutional architectures and hierarchical VAE. The results show that the SDN is able to learn large images with high spatial dependency and can learn disentangled representations. ,"This paper proposes a novel way to learn spatial regularities for generative modeling. The authors use a neural network for building image generators (decoders) and use it to learn variational autoencoders (VAEs). The authors propose a sequential gating-based mechanism to capture contextual information in the feature maps of a deep neural net. The spatial dependency networks (SDNs) are trained in a 2-D space. The decoder is used for density estimation using baseline convolutional architectures, and the spatial dependency layers are used for hierarchical VAE. The proposed SDN decoder can be used for learning disentangled representations, and can be applied to the vanilla VAE setting. Experiments show that the proposed models can achieve better performance in VAE settings compared to other neural architectures for the task."
2089,SP:db91512a90e75675af03c2f197751c8526d6f5e9,"prior approach USED-FOR offline RL. backup operator USED-FOR algorithm. EMaQ USED-FOR sub - optimality bounds. complexity EVALUATE-FOR offline RL problems. proposal distribution USED-FOR EMaQ. offline RL setting EVALUATE-FOR EMaQ. D4RL benchmarks EVALUATE-FOR EMaQ. EMaQ COMPARE Soft Actor Critic ( SAC ). Soft Actor Critic ( SAC ) COMPARE EMaQ. online RL setting EVALUATE-FOR EMaQ. generative model design USED-FOR estimating behavior policies. complexity EVALUATE-FOR offline RL problems. Method are Off - policy reinforcement learning ( RL ), off - policy RL methods, and BCQ. Generic is methods. OtherScientificTerm are policies, dataset of interactions, heuristic design choice, behavior policy, distribution support, behavior policies, and function approximator. ","This paper studies the problem of Off-policy reinforcement learning (RL) in the offline setting. The authors propose a new prior approach for offline RL, called EMaQ, which uses a backup operator to estimate the sub-optimality bounds of the policies. The proposed algorithm is based on a proposal distribution, where the goal is to learn a set of policies from a dataset of interactions, and then use a heuristic design choice to choose the best behavior policy. Empirically, the authors show that EMaq achieves better performance than Soft Actor Critic (SAC) on D4RL benchmarks, and is also able to achieve sub-optimal performance in the online RL setting.  The authors also show that the proposed approach can be used to improve the performance of off-policy RL methods, such as BCQ. The main contribution of the paper is to use a generative model design for estimating behavior policies, and to use the function approximator as a proxy for the behavior policies. ","This paper proposes a novel approach to offline RL, called Off-policy reinforcement learning (RL), where the goal is to learn a set of policies that can be used for offline RL. The authors propose an algorithm called EMaQ that uses a backup operator to estimate the sub-optimality bounds of the policies. The proposed algorithm is based on a prior approach, BCQ, where the policies are learned from a dataset of interactions. The main difference between the proposed algorithm and BCQ is the heuristic design choice of the behavior policy, and the choice of distribution support. The proposal distribution of the proposed proposal distribution is then used to compute the suboptimality of the learned behavior policies. This is done using a generative model design for estimating behavior policies, and a function approximator is used for estimating the function. EmaQ is shown to outperform Soft Actor Critic (SAC) in the offline RL setting on D4RL benchmarks, and in the online RL setting in terms of complexity."
2105,SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"fair machine learning model USED-FOR demographic disparity. data preprocessing CONJUNCTION model training. model training CONJUNCTION data preprocessing. Existing techniques USED-FOR model fairness. outer optimizer USED-FOR inner problem. inner optimizer USED-FOR training algorithm. minibatch sizes USED-FOR model fairness. equal opportunity CONJUNCTION equalized odds. equalized odds CONJUNCTION equal opportunity. equalized odds CONJUNCTION demographic parity. demographic parity CONJUNCTION equalized odds. optimization USED-FOR batch selection algorithm. FairBatch HYPONYM-OF batch selection algorithm. fairness measures PART-OF batch selection algorithm. equal opportunity HYPONYM-OF fairness measures. demographic parity HYPONYM-OF fairness measures. equalized odds HYPONYM-OF fairness measures. data preprocessing CONJUNCTION model training. model training CONJUNCTION data preprocessing. PyTorch code USED-FOR batch selection. batch selection PART-OF model training. FairBatch USED-FOR fairness. fine - tuning USED-FOR FairBatch. It CONJUNCTION batch selection techniques. batch selection techniques CONJUNCTION It. faster convergence HYPONYM-OF batch selection techniques. Method are machine learning systems, and bilevel optimization. Generic are functionality, and it. Material is synthetic and benchmark real data. ","This paper studies the problem of fair machine learning model for demographic disparity in machine learning systems. Existing techniques for model fairness are based on data preprocessing and model training with minibatch sizes. The authors propose a new batch selection algorithm called FairBatch, which uses an outer optimizer to solve the inner problem, and an inner optimizer for the training algorithm. The optimization is based on the PyTorch code, which is used in batch selection in model training. It is shown to achieve faster convergence than other batch selection techniques, and it is also shown to improve fairness measures such as equal opportunity, equalized odds, and demographic parity. ","This paper proposes a fair machine learning model to reduce the demographic disparity in machine learning systems. Existing techniques for model fairness are limited to data preprocessing and model training. The authors propose a new batch selection algorithm called FairBatch, which is based on the PyTorch code. The main idea is to use an outer optimizer to solve the inner problem, and an inner optimizer for the training algorithm. It is an extension of the bilevel optimization. It can be combined with batch selection and batch selection techniques such as fine-tuning to achieve faster convergence. Experiments are conducted on synthetic and benchmark real data. The fairness measures of the batch selection algorithms are equal opportunity, equalized odds, and demographic parity."
2121,SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"robustness guarantees CONJUNCTION generalization bounds. generalization bounds CONJUNCTION robustness guarantees. Lipschitz constants FEATURE-OF deep networks. generalization bounds CONJUNCTION smoothness of decision boundaries. smoothness of decision boundaries CONJUNCTION generalization bounds. bounds USED-FOR models. deep equilibrium ( DEQ ) model HYPONYM-OF models. monotone DEQs HYPONYM-OF DEQs. Lipschitz constants FEATURE-OF monotone DEQs. input - output mapping CONJUNCTION weight - output mapping. weight - output mapping CONJUNCTION input - output mapping. simple - yet - tight bounds USED-FOR input - output mapping. simple - yet - tight bounds USED-FOR weight - output mapping. networks USED-FOR weight - output mapping. bounds USED-FOR monotone DEQ models. multiscale convolutional structure USED-FOR monotone DEQ models. bounds USED-FOR PAC - Bayes generalization bounds. Method are infinitely - deep network, and DNNs. OtherScientificTerm are monotonicity parameter, Lipschitz constant, and exponential depth - dependence of comparable DNN bounds. Generic is they. ","This paper studies the generalization bounds for monotone DEQs with Lipschitz constants. The authors show that the monotonicity parameter of the monotonic DEQ is a function of the number of parameters of the infinitely-deep network. They show that under certain assumptions on the Lipschnitz constant, the bounds for these models converge to a deep equilibrium (DEQ) model. They also show that these bounds can be extended to models with multiscale convolutional structure. ",This paper studies the robustness guarantees and generalization bounds of deep networks with Lipschitz constants. The authors show that models with monotone DEQs with deep equilibrium (DEQ) model are robust to perturbations of the monotonicity parameter. They also show that monotones with multiscale convolutional structure are robust. The main contribution of the paper is to show that the monotons of DNNs are robust and generalizable. The paper also shows that the PAC-Bayes generalization bound of monotsones with simple-yet-tight bounds for input-output mapping and weight-output mappings can be derived from networks that have a monotonic structure. They show that they can be approximated by exponential depth-dependent of comparable DNN bounds.
2137,SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,imitation learning CONJUNCTION goal - conditioned reinforcement learning. goal - conditioned reinforcement learning CONJUNCTION imitation learning. goal - conditioned reinforcement learning HYPONYM-OF settings. imitation learning HYPONYM-OF settings. probabilistic long - term dynamics CONJUNCTION desired value function. desired value function CONJUNCTION probabilistic long - term dynamics. density estimation USED-FOR approach. it USED-FOR hindsight bias. hindsight bias FEATURE-OF stochastic domains. it USED-FOR sparse rewards. expert data USED-FOR approach. Generic is solutions. Method is goalconditioned reinforcement learning. ,"This paper studies the problem of goalconditioned reinforcement learning in stochastic domains. The authors consider two settings: imitation learning and goal-conditioned learning. In imitation learning, the goal is to find a solution that maximizes the reward of the agent in a given task. In the goal conditioned learning setting, the agent is given a set of tasks and the goal function is to learn a solution to the task. The goal conditioned on the task is a probabilistic long-term dynamics and a desired value function. The proposed approach is based on density estimation. The paper shows that the proposed approach can achieve better performance than the state-of-the-art when it is trained with expert data. In goal conditioned reinforcement learning, it is shown that it is able to learn sparse rewards.","This paper proposes a novel approach to learning goal-conditioned reinforcement learning in stochastic domains. The approach is based on density estimation, where the goal is to learn a probabilistic long-term dynamics and a desired value function. The proposed approach is evaluated on expert data, and it is shown to be able to learn sparse rewards. "
2153,SP:d57550b2f323b356d7e609acc35ee33039f376b4,"probabilistic inference framework USED-FOR simultaneously learning multiple related tasks. variational multi - task learning VMTL HYPONYM-OF probabilistic inference framework. variational Bayesian inference problem USED-FOR multi - task learning. priors USED-FOR task relatedness. mixture of variational posteriors USED-FOR prior. representations CONJUNCTION classifiers. classifiers CONJUNCTION representations. VMTL USED-FOR multi - task learning. limited training data USED-FOR VMTL. limited training data USED-FOR multi - task learning. benchmark datasets EVALUATE-FOR it. Method is Multi - task learning. OtherScientificTerm are Gumbel - softmax priors, mixing weights, and shared inductive bias. Generic is tasks. ","This paper proposes a probabilistic inference framework for simultaneously learning multiple related tasks, called variational multi-task learning VMTL. The authors consider the variational Bayesian inference problem in multi-tasks learning, where the priors for the task relatedness are Gumbel-softmax priors. The prior is a mixture of variational posteriors, and the mixing weights are learned by learning a shared inductive bias between the representations and the classifiers. The paper shows that the proposed model can achieve state-of-the-art performance on a variety of benchmark datasets. ","This paper proposes a probabilistic inference framework for simultaneously learning multiple related tasks. The authors propose a variational multi-task learning VMTL, which is based on the variational Bayesian inference problem. The priors for task relatedness are derived from Gumbel-softmax priors. The prior is a mixture of variational posteriors, and the mixing weights are used to reduce the shared inductive bias. The paper shows that the proposed model is able to learn representations and classifiers with limited training data. The experiments show that it performs well on several benchmark datasets. "
2169,SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"Transformers USED-FOR long sequence lengths. fast Transformers USED-FOR problem. model quality EVALUATE-FOR vanilla Transformer models. systematic and unified benchmark EVALUATE-FOR model quality. Long - Range Arena EVALUATE-FOR model quality. long - context scenarios FEATURE-OF model quality. Long - Range Arena HYPONYM-OF systematic and unified benchmark. text CONJUNCTION natural, synthetic images. natural, synthetic images CONJUNCTION text. natural, synthetic images CONJUNCTION mathematical expressions. mathematical expressions CONJUNCTION natural, synthetic images. similarity USED-FOR mathematical expressions. Linear Transformers CONJUNCTION Sinkhorn Transformers. Sinkhorn Transformers CONJUNCTION Linear Transformers. Synthesizers CONJUNCTION Sparse Transformers. Sparse Transformers CONJUNCTION Synthesizers. Linformers CONJUNCTION Linear Transformers. Linear Transformers CONJUNCTION Linformers. Performers CONJUNCTION Synthesizers. Synthesizers CONJUNCTION Performers. Sparse Transformers CONJUNCTION Longformers. Longformers CONJUNCTION Sparse Transformers. Sinkhorn Transformers CONJUNCTION Performers. Performers CONJUNCTION Sinkhorn Transformers. Reformers CONJUNCTION Linformers. Linformers CONJUNCTION Reformers. Performers CONJUNCTION Sparse Transformers. Sparse Transformers CONJUNCTION Performers. Synthesizers CONJUNCTION Longformers. Longformers CONJUNCTION Synthesizers. benchmark suite EVALUATE-FOR long - range Transformer models. Longformers HYPONYM-OF long - range Transformer models. Sparse Transformers HYPONYM-OF long - range Transformer models. Sinkhorn Transformers HYPONYM-OF long - range Transformer models. Reformers HYPONYM-OF long - range Transformer models. Synthesizers HYPONYM-OF long - range Transformer models. Linformers HYPONYM-OF long - range Transformer models. Performers HYPONYM-OF long - range Transformer models. Linear Transformers HYPONYM-OF long - range Transformer models. Long - Range Arena USED-FOR Transformer models. Metric are quadratic self - attention complexity, and","This paper studies the problem of long sequence lengths in Transformers. The authors propose a systematic and unified benchmark, Long-Range Arena, to evaluate the model quality of vanilla Transformer models in long-context scenarios. The main idea is to use the similarity between the text and natural, synthetic images, and mathematical expressions, to measure the quadratic self-attention complexity of the models. The proposed benchmark suite shows that the performance of the proposed model quality is comparable to the state-of-the-art in the long-range environments. ","This paper presents a systematic and unified benchmark for evaluating model quality of vanilla Transformer models in long-range environments. The authors show that model quality in Long-Range Arena can be measured in terms of quadratic self-attention complexity, which is a measure of the difference between the similarity between the input text and natural, synthetic images, and mathematical expressions. They also show that the model quality on long-context scenarios can be evaluated in a similar way to the standard benchmark suite. "
2185,SP:e12e410c3335b76133ceda4c865b244fbbab8580,"Structure CONJUNCTION Context. Context CONJUNCTION Structure. Structure USED-FOR machine learning models. Context USED-FOR machine learning models. Structure of source code USED-FOR model. source code CONJUNCTION features. features CONJUNCTION source code. language - agnostic features USED-FOR model. features HYPONYM-OF language - agnostic features. AST USED-FOR features. source code HYPONYM-OF language - agnostic features. programming languages EVALUATE-FOR monolingual code summarization. Structure CONJUNCTION Context. Context CONJUNCTION Structure. Structure USED-FOR representation learning on code. Context USED-FOR representation learning on code. OtherScientificTerm are Source code ( Context ), and computer program. Method is multilingual code summarization model. Material are non - parallel data, and low - resource languages. ",This paper proposes a multilingual code summarization model for non-parallel data. The model is based on the Structure of source code (Context) and Context for machine learning models. The proposed model uses language-agnostic features such as source code and features from AST as the source code. Structure and Context are then used for representation learning on code. The authors show that the proposed model can perform well in a variety of programming languages. ,"This paper proposes a multilingual code summarization model. The model is based on the Structure of source code (Context) and Context for machine learning models. The proposed model uses language-agnostic features (e.g., source code, features) and AST to learn the features. The authors show that the proposed model is monolingual in the case of non-parallel data. The paper also shows that the model performs well on low-resource languages. "
2201,SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"sights USED-FOR sound source. recurrent aggregations of the audio observations USED-FOR models. reinforcement learning approach USED-FOR audio - visual navigation. elements USED-FOR reinforcement learning approach. waypoints PART-OF elements. audio and visual data USED-FOR geometry of an unmapped space. real - world 3D scenes CONJUNCTION Replica. Replica CONJUNCTION real - world 3D scenes. real - world 3D scenes EVALUATE-FOR approach. sights CONJUNCTION sounds. sounds CONJUNCTION sights. sounds CONJUNCTION space. space CONJUNCTION sounds. OtherScientificTerm are agent motion, acoustic memory, and audio_visual_waypoints. Method is navigation policy. Generic is model. ","This paper proposes a reinforcement learning approach for audio-visual navigation based on recurrent aggregations of the audio observations from the sound source. The key idea is to use the waypoints in the audio and visual data to learn the geometry of an unmapped space, and then use the learned waypoints to guide the navigation policy. The proposed approach is evaluated on real-world 3D scenes and Replica.","This paper proposes a reinforcement learning approach for audio-visual navigation. The key idea is to use recurrent aggregations of the audio observations to train models. The waypoints in the audio source are learned as a function of the agent motion, acoustic memory, and the position of the sound source in the space. The proposed approach is evaluated on real-world 3D scenes and Replica. Results show that the proposed model is able to learn audio_visual_waypoints."
2217,SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"optimization methods COMPARE weight initializations. weight initializations COMPARE optimization methods. learning abilities FEATURE-OF neural networks. small CNN USED-FOR update rules. architecture USED-FOR task. task USED-FOR networks. architecture USED-FOR networks. initialization parameters USED-FOR gradient descent. single sign change HYPONYM-OF small perturbations. OtherScientificTerm are lottery tickets, and lottery ticket hypothesis. Method are small convolutional networks, and minimal networks. ","This paper studies the problem of lottery tickets, where the goal is to find the best lottery ticket for each ticket in a lottery ticket hypothesis. The authors propose a new architecture for this task, which uses a small CNN to learn update rules for the lottery tickets. They show that this architecture can be used to train networks with different learning abilities. They also show that the initialization parameters for gradient descent can be learned with this architecture. ","This paper proposes a novel architecture for learning neural networks with learning abilities. The main idea is to use small convolutional networks with a small CNN to learn update rules for lottery tickets. The authors show that this architecture can be applied to any task where the networks are trained on a small set of lottery tickets (e.g., the lottery ticket hypothesis). The optimization methods are compared to weight initializations, and the authors also show that the optimization methods can be used to improve the performance of the networks. The paper also shows that small perturbations such as single sign change can be solved with minimal networks. Finally, the authors propose to use initialization parameters for gradient descent."
2233,SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"Semi - supervised learning ( SSL ) USED-FOR unlabeled data. consistency regularization USED-FOR SSL approaches. RankingMatch HYPONYM-OF method. computational efficiency EVALUATE-FOR objective function. BatchMean Triplet loss HYPONYM-OF objective function. accuracy EVALUATE-FOR SVHN. accuracy EVALUATE-FOR SVHN. accuracy CONJUNCTION accuracy. accuracy CONJUNCTION accuracy. accuracy EVALUATE-FOR RankingMatch. accuracy EVALUATE-FOR RankingMatch. SSL benchmarks EVALUATE-FOR RankingMatch. BatchMean Triplet loss COMPARE Triplet loss. Triplet loss COMPARE BatchMean Triplet loss. ablation study EVALUATE-FOR BatchMean Triplet loss. ablation study EVALUATE-FOR Triplet loss. Material are labeled data, CIFAR-10, and CIFAR-100. Generic is model. OtherScientificTerm is labeled data amounts. ","This paper proposes a new method for semi-supervised learning (SSL) for unlabeled data with consistency regularization. The proposed method, RankingMatch, is based on the BatchMean Triplet loss, which is a new objective function with computational efficiency. The authors show that RankingMatch achieves state-of-the-art performance on several SSL benchmarks, including CIFAR-10, and outperforms SVHN in terms of accuracy, accuracy, and accuracy. ","This paper proposes a method called RankingMatch, which uses Semi-supervised learning (SSL) to learn unlabeled data with consistency regularization. The proposed method is based on BatchMean Triplet loss, which improves the computational efficiency of the objective function and the accuracy of the SVHN. The model is evaluated on CIFAR-10, CifAR-100, and SSL benchmarks. Experiments show that RankingMatch improves the accuracy and accuracy on all three SSL benchmarks, while the Triplet Loss is shown to improve the ablation study. "
2249,SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"formulation USED-FOR sequential learning setting. meta - training CONJUNCTION adaptation. adaptation CONJUNCTION meta - training. sample complexity CONJUNCTION regret. regret CONJUNCTION sample complexity. sample complexity EVALUATE-FOR empirical risk minimization methods. regret EVALUATE-FOR empirical risk minimization methods. meta - learning USED-FOR online setting. meta - learning COMPARE empirical risk minimization methods. empirical risk minimization methods COMPARE meta - learning. regret EVALUATE-FOR meta - learning. sample complexity EVALUATE-FOR meta - learning. bi - level optimizations FEATURE-OF meta - learning algorithms. meta - training data USED-FOR meta - learning algorithms. meta - training data USED-FOR bi - level optimizations. meta - learning algorithms USED-FOR variable - shot settings. many - shot learning CONJUNCTION zero - shot learning. zero - shot learning CONJUNCTION many - shot learning. variable - shot settings PART-OF sequential learning. meta - learning algorithms USED-FOR sequential learning. zero - shot learning HYPONYM-OF variable - shot settings. meta - learning COMPARE supervised methods. supervised methods COMPARE meta - learning. cumulative performance EVALUATE-FOR supervised methods. sequential learning problems EVALUATE-FOR meta - learning. cumulative performance EVALUATE-FOR meta - learning. meta - learning USED-FOR learning systems. Method are Few - shot meta - learning methods, and metalearning. Generic is problem. ","This paper proposes a new formulation for the sequential learning setting. Few-shot meta-learning methods have been studied in the online setting, where meta-training and adaptation are used to train learning systems. The authors propose a new problem, called metalearning, where the goal is to find the optimal solution to a set of sequential learning problems. The problem is formulated as a binary optimization problem, and the authors show that meta-learned algorithms can achieve better sample complexity than empirical risk minimization methods in terms of regret, and can also achieve better bi-level optimizations in the variable-shot settings in sequential learning, such as zero-shot learning. The paper also provides a theoretical analysis of the performance of meta-trained algorithms on sequential learning algorithms in both the many-shot and zero-shoot settings. ","This paper proposes a new formulation for the sequential learning setting. Few-shot meta-learning methods are commonly used in the online setting, where meta-training and adaptation are used to train learning systems. The problem of sequential learning in the variable-shot settings in sequential learning is an important one. The paper proposes meta-Learning algorithms for sequential learning algorithms in the many-shot learning and zero-shot setting. The authors show that bi-level optimizations of meta-learned algorithms on meta-train data and meta-test data are better than empirical risk minimization methods in terms of sample complexity, regret, and cumulative performance. They also show that supervised methods are better at sequential learning problems than supervised methods. "
2265,SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"contextual representations USED-FOR NLP tasks. pretrained Transformer models USED-FOR contextual representations. representations USED-FOR sentence - level syntax. self - supervision USED-FOR Transformers networks. probes EVALUATE-FOR Transformer representations. random permutations of n - grams HYPONYM-OF perturbations. syntactic distance FEATURE-OF attention mechanism. local phrase structure FEATURE-OF sensitivity. Generic are they, network, probe, and representation. Task is computational and cognitive neuroscience. OtherScientificTerm are representational invariance, word position, syntactic phrase, global phrase structure, hierarchical phrase structure, and attention weights. Method are Transformer architecture, and Transformers. ","This paper studies the problem of learning contextual representations for NLP tasks using pretrained Transformer models. The authors propose a Transformer architecture that learns representations for sentence-level syntax by self-supervision. The proposed Transformer representations are evaluated on a variety of probes and show representational invariance to perturbations such as random permutations of n-grams. In particular, they show that the attention mechanism has a syntactic distance between the syntactic position and the local phrase structure, and that the sensitivity of the representations to perturbing the word position depends on the global phrase structure. The paper also shows that the representations learned by the Transformer networks can be used to improve the performance of Transformers networks in terms of self- supervision. ","This paper proposes a novel Transformer architecture for learning contextual representations for NLP tasks. The authors propose to use pretrained Transformer models to learn contextual representations that are invariant to sentence-level syntax. The Transformer networks are trained with self-supervision, and they are trained on a set of random permutations of n-grams, where each permutation corresponds to a word position in the syntactic phrase. The proposed Transformer representations are evaluated on two sets of probes, where they are shown to have representational invariance to the perturbations. The main contribution of the paper is to show that the sensitivity of the representation to the local phrase structure and the sensitivity to the global phrase structure is a function of the attention mechanism, which is defined as a syntactic distance between the attention weights of the network and the input word position. The paper also shows that the representation is more robust to perturbation than the original Transformer network. "
2281,SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"high - fidelity images USED-FOR Generative Adversarial Networks ( GAN ). large - scale GPU - clusters USED-FOR Generative Adversarial Networks ( GAN ). few - shot image synthesis task USED-FOR GAN. minimum computing cost FEATURE-OF few - shot image synthesis task. 1024 × 1024 resolution EVALUATE-FOR light - weight GAN structure. skip - layer channel - wise excitation module CONJUNCTION self - supervised discriminator. self - supervised discriminator CONJUNCTION skip - layer channel - wise excitation module. feature - encoder USED-FOR self - supervised discriminator. model COMPARE StyleGAN2. StyleGAN2 COMPARE model. datasets EVALUATE-FOR model. datasets EVALUATE-FOR StyleGAN2. image domains FEATURE-OF datasets. OtherScientificTerm are RTX-2080 GPU, and computing budget. ","This paper studies the problem of generating high-fidelity images for Generative Adversarial Networks (GAN) with large-scale GPU-clusters. The authors propose a few-shot image synthesis task for GAN with a minimum computing cost of $\mathcal{O}(\sqrt{T}$)$, where $T$ is the number of pixels in the image, and $O$ is a number of frames. They show that the light-weight GAN structure with 1024 × 1024 resolution can be learned with a skip-layer channel-wise excitation module and a self-supervised discriminator with a feature-encoder. They also show that their model is able to achieve better performance than StyleGAN2 on several datasets across different image domains. ","This paper presents a new method for generating high-fidelity images for Generative Adversarial Networks (GAN) on large-scale GPU-clusters. The authors propose a novel light-weight GAN structure with 1024 × 1024 resolution. The main idea is to use a skip-layer channel-wise excitation module and a self-supervised discriminator with a feature-encoder. The model is evaluated on two datasets, where it outperforms StyleGAN2 on both image domains and computing budget."
2297,SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"neural network bounding USED-FOR neural network verification systems. specialised dual solvers USED-FOR neural network bounds. linear program HYPONYM-OF relaxation. linear relaxation USED-FOR piecewise linear activations. dual algorithm USED-FOR relaxation. tightness CONJUNCTION linear separation oracle. linear separation oracle CONJUNCTION tightness. method USED-FOR relaxation. dual space FEATURE-OF relaxation. tightness HYPONYM-OF relaxation. linear separation oracle HYPONYM-OF relaxation. dual approaches USED-FOR weaker relaxations. massive parallelism CONJUNCTION GPU implementation. GPU implementation CONJUNCTION massive parallelism. dual approaches USED-FOR it. bounds COMPARE off - the - shelf solvers. off - the - shelf solvers COMPARE bounds. speed - accuracy trade - offs EVALUATE-FOR dual solvers. running time EVALUATE-FOR off - the - shelf solvers. Generic is they. Method is customised solver. OtherScientificTerm are dual variables, computational budget, and formal verification speed - ups. ","This paper studies neural network bounding for neural network verification systems. The authors propose specialised dual solvers for learning neural network bounds. The main idea is to learn a customised solver for each of the dual variables. The relaxation is based on a linear program, where the relaxation is defined as a linear relaxation of piecewise linear activations in the dual space, and the dual algorithm is used to learn the relaxation.  The authors show that the proposed method can achieve better bounds than off-the-shelf solvers in terms of speed-accuracy trade-offs. They also show that it can be combined with dual approaches to learn weaker relaxations, such as tightness and linear separation oracle.   ","This paper proposes a novel approach to neural network bounding for neural network verification systems. The main idea is to use specialised dual solvers for learning neural network bounds. The authors propose a dual algorithm for learning relaxation in the dual space. The relaxation is defined as a linear program, where the dual variables are the number of parameters of the customised solver and the computational budget is the sum of the two sets of parameters. They show that the relaxation can be learned in a dual space in terms of tightness, linear separation oracle, and piecewise linear activations. They also show that it is equivalent to existing dual approaches for weaker relaxations (e.g., massive parallelism and GPU implementation). The authors also demonstrate that the proposed bounds outperform off-the-shelf solvers on speed-accuracy trade-offs and running time. Finally, the authors provide some formal verification speed-ups."
2313,SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"masked token prediction CONJUNCTION masked span infilling. masked span infilling CONJUNCTION masked token prediction. masked span infilling USED-FOR T5 - style PTLMs. masked token prediction USED-FOR BERT - style PTLMs. everyday concepts FEATURE-OF relational commonsense knowledge. BERT - style PTLMs HYPONYM-OF pre - training objectives. masked token prediction HYPONYM-OF pre - training objectives. masked span infilling HYPONYM-OF pre - training objectives. intermediate self - supervised learning tasks USED-FOR PTLMs. them USED-FOR intermediate self - supervised learning tasks. generative and contrastive objectives USED-FOR common sense. task - specific fine - tuning USED-FOR PTLMs. concept - centric commonsense knowledge USED-FOR PTLMs. pre - training framework USED-FOR generative and contrastive objectives. concept - aware language model ( CALM)1 HYPONYM-OF method. CALM COMPARE PTLMs. PTLMs COMPARE CALM. CALM COMPARE baseline methods. baseline methods COMPARE CALM. CALM USED-FOR PTLM. baseline methods COMPARE PTLMs. PTLMs COMPARE baseline methods. Method are Pre - trained language models ( PTLM ), and text - to - text transformer. Generic is they. OtherScientificTerm are commonsense knowledge, and external knowledge graphs. Task is NLU and NLG tasks. ","This paper proposes a new pre-trained language models (PTLM) based on the concept-aware language model (CALM)1. The main idea is to learn commonsense knowledge of everyday concepts in relational commonsense Knowledge (e.g., text-to-text transformer). The PTLMs are trained using a pre-training framework that combines masked token prediction, masked span infilling, and T5-style PTLM. The authors show that they can achieve state-of-the-art performance on NLU and NLG tasks. They also show that the generative and contrastive objectives can be used to improve common sense. Finally, they show that task-specific fine-tuning can improve the performance of PTLm. ","This paper proposes a novel pre-trained language models (PTLM) based on the concept-aware language model (CALM)1. The key idea is to use commonsense knowledge from the external knowledge graphs (e.g. everyday concepts) as relational commonsense information. The authors propose two pre-training objectives: masked token prediction and masked span infilling for T5-style PTLMs, and BERT-style for PTLM based on a text-to-text transformer. They evaluate them on a variety of intermediate self-supervised learning tasks and show that they outperform the baseline methods on both NLU and NLG tasks. They also show that the generative and contrastive objectives can be used to improve common sense and improve the performance on task-specific fine-tuning. Finally, the authors show that their method outperforms CALM and other baseline methods in terms of concept-centric commonsense learning."
2329,SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"frameworks USED-FOR 2D segments. object interactions HYPONYM-OF physics. multi - scale pixel cues CONJUNCTION physical motion cues. physical motion cues CONJUNCTION multi - scale pixel cues. synthetic and real scenes EVALUATE-FOR model. object properties USED-FOR physical events. Task is unsupervised physical object discovery. OtherScientificTerm are 3D geometry, developmental psychology, and observable and partially occluded objects. Material is video. ","This paper studies the problem of unsupervised physical object discovery in the context of 3D geometry. The authors propose a new framework to learn 2D segments from video frames. The framework is based on the idea of object interactions in physics, where the object properties of physical events are encoded in multi-scale pixel cues and physical motion cues. The model is evaluated on both synthetic and real scenes. ","This paper proposes a new framework for unsupervised physical object discovery. The key idea is to learn 2D segments of a 3D geometry, where the object interactions are modeled as physics, and the 2D segment is modeled as developmental psychology. The model is evaluated on both synthetic and real scenes. The authors show that the model is able to learn object properties for physical events, multi-scale pixel cues, and physical motion cues. They also show that it can learn both observable and partially occluded objects. "
2345,SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"Deep neural networks ( DNNs ) USED-FOR adversarial attacks. convolutional neural networks PART-OF Deep neural networks ( DNNs ). attack algorithms USED-FOR Adversarial samples. training method USED-FOR DNN robustness. adversarial noises FEATURE-OF DNN robustness. Increasing Margin Adversarial ( IMA ) Training HYPONYM-OF training method. IMA method USED-FOR margins. decision boundaries USED-FOR DNN model. IMA method USED-FOR training. robustness EVALUATE-FOR IMA method. clean data EVALUATE-FOR accuracy. noisy data EVALUATE-FOR method. accuracy EVALUATE-FOR method. clean data EVALUATE-FOR method. classification accuracy EVALUATE-FOR method. approach USED-FOR robust DNN applications. COVID-19 diagnosis HYPONYM-OF robust DNN applications. CT images USED-FOR COVID-19 diagnosis. Task is life - critical applications. OtherScientificTerm is white noises. Material are COVID-19 CT image dataset, and 100 - PGD white - box adversarial attacks. ","This paper proposes a new training method for improving the DNN robustness against adversarial attacks on convolutional neural networks (DNNs). Adversarial samples are generated by attack algorithms that are trained on white noises. The authors propose an IMA method to improve the margins of a DNN model by learning decision boundaries. The proposed method, Increasing Margin Adversary (IMA) Training, is evaluated on clean data and noisy data. The results show that the proposed method improves the robustness and classification accuracy of DNN applications such as COVID-19 diagnosis and life-critical applications.","This paper proposes an approach to improve the robustness of deep neural networks (DNNs) against adversarial attacks. The authors propose a training method for improving DNN robustness to adversarial noises. The training method is called Increasing Margin Adversarial (IMA) Training, which is based on the IMA method to increase the margins of the DNN model in the decision boundaries. The method is evaluated on both clean data and noisy data, and is shown to improve robustness and classification accuracy on COVID-19 diagnosis and other robust DNN applications (e.g., life-critical applications). "
2361,SP:276ffd59fbf49e3ee02756da8920218102214917,"Knowledge distillation USED-FOR compact models. teacher model USED-FOR compact student network. poor local optima FEATURE-OF optimization. ProKT HYPONYM-OF model - agnostic method. supervision signals USED-FOR teacher model. student ’s parameter space FEATURE-OF supervision signals. local intermediate targets FEATURE-OF training objective. approximate mirror descent technique USED-FOR local intermediate targets. approximate mirror descent technique USED-FOR projection. quirks USED-FOR optimization. quirks USED-FOR method. ProKT COMPARE knowledge distillation methods. knowledge distillation methods COMPARE ProKT. image and text datasets EVALUATE-FOR ProKT. Method is Deep neural networks. OtherScientificTerm are computation capacity, and local optima. ",This paper proposes a model-agnostic method called ProKT. ProKT distills the knowledge from a teacher model into a compact student network. The teacher model is trained using supervision signals in the student’s parameter space. The training objective is based on local intermediate targets. The proposed method uses an approximate mirror descent technique to learn the projection of the teacher model. The authors show that ProKT outperforms existing knowledge distillation methods on both image and text datasets. ,"This paper proposes ProKT, a model-agnostic method for compact models. ProKT learns a compact student network using a teacher model. The student’s parameter space consists of supervision signals and computation capacity. The training objective consists of local intermediate targets and an approximate mirror descent technique for projection. The authors show that ProKT outperforms knowledge distillation methods on both image and text datasets. They also show that the proposed method is robust to quirks in the optimization. "
2377,SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"channel pruning method USED-FOR compression. compression FEATURE-OF Convolutional Neural Networks ( CNNs ). hyper - structure network USED-FOR architecture of the main network. hypernet COMPARE hyperstructure network. hyperstructure network COMPARE hypernet. regular backpropagation USED-FOR hyperstructure network. regularization term USED-FOR computational resource. regularization term USED-FOR compact network. computational resource FEATURE-OF compact network. FLOPs USED-FOR computational resource. FLOPs USED-FOR regularization. FLOPs USED-FOR it. layer - wise scaling factors USED-FOR gradients. hyper - gradient descent USED-FOR they. ImageNet EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. CIFAR-10 EVALUATE-FOR method. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method is channel pruning methods. OtherScientificTerm are layers, and gates. ","This paper proposes a channel pruning method for compression of Convolutional Neural Networks (CNNs). The main idea is to use a hyper-structured network instead of a hyperstructure network to learn the architecture of the main network. The authors use FLOPs to regularize the computational resource of the compact network, and then use the regularization term to improve the performance of the network. They also use layer-wise scaling factors to reduce the gradients, and they use hyper-gradient descent to optimize the gates. The proposed method is evaluated on CIFAR-10 and ImageNet, and compared to state-of-the-art methods.",This paper proposes a channel pruning method for compression of Convolutional Neural Networks (CNNs). The hyper-structured network is a variant of the architecture of the main network with regular backpropagation. The main difference between the hypernet and the hyperstructure network is that it uses FLOPs to compute the computational resource of the compact network. The authors show that the proposed method outperforms state-of-the-art methods on CIFAR-10 and ImageNet. They also show that they can compute gradients with layer-wise scaling factors. 
2393,SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"exploration mechanism USED-FOR theorem prover. imitation CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION imitation. It USED-FOR prover. imitation USED-FOR prover. reinforcement learning USED-FOR prover. Task are automated higher - order logic theorem proving, and exploration of premises. OtherScientificTerm are human proofs, deep reinforcement learning scenario, and DeepHOL Zero. Method is exploration approach. ","This paper studies the problem of automated higher-order logic theorem proving. The authors propose a new exploration mechanism for the theorem prover based on imitation and reinforcement learning. It can be seen as an extension of the exploration approach in the deep reinforcement learning scenario, where the goal is to learn a prover that can be used in combination with imitation or reinforcement learning to solve the prover. The main contribution of the paper is the exploration of premises in the context of human proofs. The exploration approach is based on DeepHOL Zero.","This paper proposes an exploration mechanism for the theorem prover. It is based on imitation and reinforcement learning. The main idea is to learn an automated higher-order logic theorem proving, and then use the exploration of premises to learn human proofs. The exploration approach is evaluated on DeepHOL Zero, a deep reinforcement learning scenario."
2409,SP:88209417a8ad07e6103084e41709be900303ce5f,"models USED-FOR machine learning tasks. data augmentation USED-FOR models. augmentation methods USED-FOR data modality. image processing functions CONJUNCTION word - replacing rules. word - replacing rules CONJUNCTION image processing functions. word - replacing rules USED-FOR text data. image processing functions USED-FOR image data. MODALS USED-FOR automated data augmentation. universal data transformation operations USED-FOR transform. MODALS USED-FOR universal data transformation operations. latent space FEATURE-OF universal data transformation operations. Method are Data augmentation, and automated data augmentation approach. Material is artificial data. OtherScientificTerm are modality, and modalities. ","This paper studies the problem of data augmentation in models for machine learning tasks. Data augmentation methods are typically used to augment the data modality of a model, but the modality can be different from the original data. The authors propose a new and automated data augmentmentation approach, called MODALS, which can be applied to image processing functions and word-replacing rules for text data. They show that MODALS can perform universal data transformation operations in the latent space, which allows them to transform the input data into different modalities. ","This paper proposes a new data augmentation approach for machine learning tasks. The authors propose two augmentation methods for each data modality: image processing functions and word-replacing rules for image data, and for text data. The modality is defined as the number of modalities. The transform is performed using universal data transformation operations in the latent space, and the authors show that the proposed MODALS can be used for the automated data augmentmentation. The paper is well-written and easy to follow. "
2425,SP:6d84670d321b0d584b097c630574bd748e85c9a2,"nonlinear and nontrivial dynamical limit FEATURE-OF learning dynamics. mean field limit HYPONYM-OF nonlinear and nontrivial dynamical limit. neural networks USED-FOR mean field regime. mean field limit USED-FOR large - width neural networks. analysis USED-FOR two - layer networks. mean field regime FEATURE-OF optimization efficiency. global convergence result FEATURE-OF unregularized feedforward three - layer networks. mean field regime FEATURE-OF unregularized feedforward three - layer networks. rigorous framework USED-FOR mean field limit. mean field limit FEATURE-OF three - layer networks. stochastic gradient descent training USED-FOR three - layer networks. stochastic gradient descent training USED-FOR mean field limit. probability space FEATURE-OF neural networks. neural networks PART-OF neuronal embedding. probability space PART-OF neuronal embedding. mean field limit USED-FOR global convergence guarantee. regularity CONJUNCTION convergence mode assumptions. convergence mode assumptions CONJUNCTION regularity. convergence mode assumptions FEATURE-OF global convergence guarantee. regularity USED-FOR global convergence guarantee. universal approximation property FEATURE-OF neural networks. algebraic topology argument USED-FOR universal approximation property. algebraic topology argument USED-FOR neural networks. OtherScientificTerm are global convergence guarantees, multilayer ones, and convexity. ","This paper studies the mean field limit of neural networks in the nonlinear and nontrivial dynamical limit of learning dynamics. The authors propose a rigorous framework for computing the meanfield limit for large-width neural networks. The main contribution of the paper is a theoretical analysis of the two-layer networks and the three-layer ones. The mean field regime of the optimization efficiency of the unregularized feedforward three-layered networks is studied, and the global convergence result of the mean-field limit is obtained. The paper also provides a theoretical guarantee for the global approximation property of the neural networks under the algebraic topology argument for neural networks with neural networks consisting of neuronal embedding in the probability space. The global convergence guarantee is based on regularity and convergence mode assumptions. ",This paper studies the nonlinear and nontrivial dynamical limit of learning dynamics in the mean field regime for large-width neural networks. The authors propose a rigorous framework for estimating the global convergence result of unregularized feedforward three-layer networks with mean field limit in terms of optimization efficiency. The main contribution of the paper is a theoretical analysis of the two-layer neural networks and the multilayer ones. The paper shows that the global approximation property of neural networks in the probability space of the neuronal embedding is equivalent to the universal approximation property for neural networks with regularity and convergence mode assumptions. The theoretical analysis is based on stochastic gradient descent training for three-layered networks. 
2441,SP:b90f893f927db9c439595fd119a565cf43c971f4,"interpretable parameterizations USED-FOR real - world decision - making. interpretable parameterizations USED-FOR introspecting and auditing policies. counterfactual reasoning USED-FOR batch inverse reinforcement learning. counterfactual reasoning USED-FOR costbenefit tradeoffs. reward functions USED-FOR expert behavior. counterfactuals USED-FOR policy evaluation. batch setting FEATURE-OF policy evaluation. real and simulated medical environments EVALUATE-FOR batch, counterfactual inverse reinforcement learning approach. OtherScientificTerm are unknown reward function, reward function, expert ’s actions, and expert policies. Task are learning explanations of expert decisions, and active experimentation. Material is healthcare. ","This paper studies the problem of learning explanations of expert decisions in real-world decision-making with interpretable parameterizations for introspecting and auditing policies. The authors propose a batch inverse reinforcement learning approach based on counterfactual reasoning for costbenefit tradeoffs between expert’s actions and the unknown reward function. The key idea is to learn the reward functions for expert behavior and then use these reward functions to guide the policy evaluation in the batch setting. Experiments on both real and simulated medical environments demonstrate the effectiveness of the batch,counterfactual inverse RL learning approach.","This paper proposes a novel approach to learning explanations of expert decisions. The authors propose to use interpretable parameterizations for real-world decision-making, which can be used for introspecting and auditing policies. The key idea is to use counterfactual reasoning for batch inverse reinforcement learning for costbenefit tradeoffs between expert’s actions and the unknown reward function. The idea is that the expert behavior depends on the reward function of the expert, and that the cost benefit tradeoffs depend on the expert “s actions”. The paper also proposes a new way to evaluate expert policies in the batch setting, which is based on active experimentation. Experiments on real and simulated medical environments demonstrate the effectiveness of the batch,counterfactual inverse RL learning approach."
2457,SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"generalisation CONJUNCTION data efficiency. data efficiency CONJUNCTION generalisation. data efficiency CONJUNCTION robustness. robustness CONJUNCTION data efficiency. Multitask Reinforcement Learning USED-FOR models. generalisation EVALUATE-FOR models. they USED-FOR graphs. physical morphology USED-FOR graph. edges USED-FOR nodes. morphological information PART-OF graph. AMORPHEUS HYPONYM-OF transformer - based approach. graph structure USED-FOR GNNs. it COMPARE GNN - based methods. GNN - based methods COMPARE it. morphological information USED-FOR message - passing scheme. GNN - based methods USED-FOR message - passing scheme. AMORPHEUS COMPARE it. it COMPARE AMORPHEUS. AMORPHEUS USED-FOR morphological information. morphological information USED-FOR GNN - based methods. OtherScientificTerm are state and action space dimensions, and limb features. Method is Graph Neural Networks ( GNN ). Generic are They, and methods. Task are graph - based continuous control, and message passing. ","This paper studies the problem of graph-based continuous control. The authors propose a transformer-based approach called AMORPHEUS, which is based on graph neural networks (GNN). They show that GNNs with a graph structure that is similar to graph structure can generalize better than GNN-based methods. They also show that they can generalise to graphs with physical morphology that are similar to the state and action space dimensions of the graph. They then show that the models trained with Multitask Reinforcement Learning can achieve better generalisation and data efficiency. Finally, they show that their message passing scheme can be combined with morphological information in a message-passing scheme. ","This paper proposes a transformer-based approach, called AMORPHEUS, to improve generalisation and data efficiency of Graph Neural Networks (GNN). They use Multitask Reinforcement Learning (MRL) to train models on graphs with different state and action space dimensions. They use the graph structure of GNNs as input, and they use the physical morphology of the graph to learn the nodes and edges of the graphs. They show that the proposed method AMOR PHEUS improves generalisation, data efficiency, and robustness. They also show that it is more robust than GNN-based methods that use morphological information in the message-pass scheme, and that it can be used to improve the generalisation of the GNN. They evaluate their methods on graph-based continuous control and on message passing."
2473,SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"modulated convolutions USED-FOR alternative. forward - pass USED-FOR inference. forward - pass USED-FOR MoVie. MoVie USED-FOR counting. module USED-FOR number ’ related questions. number ’ related questions PART-OF generic VQA models. module PART-OF generic VQA models. COCO HYPONYM-OF common object counting. module USED-FOR VQA challenge. counting - specific VQA tasks EVALUATE-FOR MoVie. common object counting HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR prior - art. COCO HYPONYM-OF benchmarks. modulated convolutions USED-FOR reasoning tasks. counting HYPONYM-OF reasoning tasks. MoVie HYPONYM-OF modulated convolutions. Task is visual counting. Material is natural image. Method are explicit, symbolic models, residual bottleneck, and Modulated conVolutional bottlenecks. ","This paper studies the problem of visual counting in the context of explicit, symbolic models. The authors propose a new alternative based on modulated convolutions, called MoVie, which uses a forward-pass to perform inference in the presence of a residual bottleneck. The proposed module consists of two modules: (1) the number’ related questions in the original generic VQA models, and (2) a module for counting the number of objects in a natural image. The experimental results show that the proposed module outperforms prior-art on three benchmarks, including common object counting, COCO, and reasoning tasks. ","This paper proposes a new method for visual counting, which is based on modulated convolutions. The idea is to use explicit, symbolic models to compute the number of objects in a natural image, and then use a forward-pass to perform inference. The authors propose a module for the VQA challenge, where the number’ related questions are modulated with a residual bottleneck. The proposed MoVie is evaluated on three different benchmarks, including common object counting, COCO, and reasoning tasks. The results show that the proposed prior-art outperforms the proposed method on all three benchmarks."
2489,SP:c64e77507e562f236cb69361b22fb1a7951ffb22,poisoning attack USED-FOR model. online convex optimization USED-FOR poisoning attack. model - targeted poisoning attacks COMPARE attack. attack COMPARE model - targeted poisoning attacks. provable convergence FEATURE-OF target classifier. provable convergence FEATURE-OF attack. attack HYPONYM-OF model - targeted poisoning attack. it COMPARE attacks. attacks COMPARE it. attack success rate EVALUATE-FOR it. attack success rate EVALUATE-FOR attacks. attack USED-FOR online attack. Method is classifier. ,"This paper proposes a poisoning attack on a model. The attack is based on online convex optimization. The authors show that the attack achieves provable convergence on the target classifier, and that it is more robust than other model-targeted poisoning attacks. They also show that their attack can be used as an online attack to improve the attack success rate. ","This paper proposes a poisoning attack on a model. The poisoning attack is based on online convex optimization. The authors show provable convergence of the target classifier. They also show that the proposed attack is more robust than other model-targeted poisoning attacks. Finally, the authors show the attack success rate of the proposed online attack compared to other attacks. "
2505,SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"BiPointNet HYPONYM-OF model binarization approach. model binarization approach USED-FOR deep learning on point clouds. resource constraint USED-FOR real - time point cloud applications. edge devices USED-FOR real - time point cloud applications. binarized models USED-FOR point clouds. scale distortion USED-FOR optimization. scale distortion HYPONYM-OF challenges. aggregation - induced feature homogenization HYPONYM-OF challenges. Entropy - Maximizing Aggregation ( EMA ) USED-FOR distribution. Layer - wise Scale Recovery ( LSR ) USED-FOR feature representation capacity. Entropy - Maximizing Aggregation ( EMA ) USED-FOR BiPointNet. Layer - wise Scale Recovery ( LSR ) PART-OF BiPointNet. BiPointNet COMPARE full precision counterpart. full precision counterpart COMPARE BiPointNet. binarization methods COMPARE full precision counterpart. full precision counterpart COMPARE binarization methods. BiPointNet COMPARE binarization methods. binarization methods COMPARE BiPointNet. tasks EVALUATE-FOR techniques. speedup CONJUNCTION storage saving. storage saving CONJUNCTION speedup. storage saving EVALUATE-FOR BiPointNet. real - world resource - constrained devices EVALUATE-FOR BiPointNet. speedup EVALUATE-FOR BiPointNet. Metric are information entropy, and maximum information entropy. OtherScientificTerm is scale - sensitive structures. ","This paper proposes BiPointNet, a model binarization approach for deep learning on point clouds based on edge devices. The authors propose a new resource constraint for real-time point cloud applications with edge devices, where the information entropy is bounded by the number of scale-sensitive structures. They show that binarized models can improve the performance of point clouds with scale distortion in optimization, and propose two new challenges: scale distortion and aggregation-induced feature homogenization. They also propose a distribution based on Entropy-Maximizing Aggregation (EMA) to improve the feature representation capacity by using Layer-wise Scale Recovery (LSR) to reduce the size of the distribution. Experimental results show that the proposed techniques perform well on both speedup and storage saving on real-world resource-constrained devices. ","This paper presents a model binarization approach for deep learning on point clouds. The authors propose BiPointNet, a model based on the idea of scale distortion, which is an extension of binarized models for point clouds with edge devices. The paper introduces two challenges, namely, scale distortion and aggregation-induced feature homogenization, where the information entropy of a point cloud is a function of the number of edge devices in the point cloud, and the maximum information entropy is defined as the difference between the maximum and minimum information entropy. The distribution of the distribution is learned using Entropy-Maximizing Aggregation (EMA) and Layer-wise Scale Recovery (LSR) to improve the feature representation capacity. The proposed techniques are evaluated on two tasks: speedup and storage saving on real-world resource-constrained devices, and outperform the full precision counterpart."
2521,SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"natural language processing tasks EVALUATE-FOR Transformer - based models. self - attention architecture USED-FOR transformer. transformer USED-FOR context - aware representations. trainable memory USED-FOR Transformer model. Memory - augmented neural networks ( MANNs ) USED-FOR representations. general - purpose memory USED-FOR representations. neural architectures USED-FOR representations. neural architectures USED-FOR Memory - augmented neural networks ( MANNs ). general - purpose memory USED-FOR neural architectures. MANNs USED-FOR algorithms. MANNs USED-FOR tasks. question answering CONJUNCTION language modeling. language modeling CONJUNCTION question answering. backpropagation USED-FOR tasks. RNNs CONJUNCTION LSTMs. LSTMs CONJUNCTION RNNs. complexity EVALUATE-FOR RNNs. language modeling HYPONYM-OF tasks. backpropagation USED-FOR MANNs. question answering HYPONYM-OF tasks. complexity EVALUATE-FOR LSTMs. Copy HYPONYM-OF algorithms. memory tokens USED-FOR non - local representations. memory bottleneck USED-FOR global information. dedicated layer USED-FOR memory update. machine translation and language modelling tasks EVALUATE-FOR model. GLUE benchmark EVALUATE-FOR tasks. memory tokens USED-FOR masked language model. it USED-FOR global context. it USED-FOR model. model USED-FOR global context. Method are element - wise representations, Transformer baseline, and memory augmented Transformers. OtherScientificTerm is memory. ","This paper proposes a self-attention architecture for Transformer-based models for natural language processing tasks. The transformer learns context-aware representations from a trainable memory. The Transformer model is trained with a dedicated layer to store the global information in the memory. Memory-augmented neural networks (MANNs) are used to learn representations from the general-purpose memory of the representations. The authors show that MANNs are able to perform better than RNNs and LSTMs on a variety of tasks such as question answering, language modeling, and backpropagation. The model is also able to learn the global context of a masked language model from memory tokens. ","This paper proposes a self-attention architecture for Transformer-based models for natural language processing tasks. The key idea is to use a transformer to generate context-aware representations from a set of element-wise representations. Memory-augmented neural networks (MANNs) are used to generate representations from general-purpose memory. The Transformer model is trained with a trainable memory, which is then used to update the Transformer baseline. The model is evaluated on machine translation and language modelling tasks on the GLUE benchmark. The authors show that MANNs outperform other algorithms such as RNNs and LSTMs in terms of complexity and question answering. The paper also shows that the memory bottleneck for global information can be alleviated by adding a dedicated layer for memory update. "
2537,SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,Prototypical Contrastive Learning ( PCL ) HYPONYM-OF unsupervised representation learning method. contrastive learning CONJUNCTION clustering. clustering CONJUNCTION contrastive learning. it USED-FOR semantic structures. PCL USED-FOR instance discrimination. PCL USED-FOR low - level features. low - level features USED-FOR instance discrimination. PCL USED-FOR semantic structures. embedding space FEATURE-OF semantic structures. clustering USED-FOR semantic structures. prototypes USED-FOR maximum - likelihood estimation. maximum - likelihood estimation USED-FOR network parameters. prototypes USED-FOR latent variables. Expectation - Maximization framework USED-FOR maximum - likelihood estimation. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. distribution of prototypes USED-FOR E - step. clustering USED-FOR distribution of prototypes. contrastive learning USED-FOR network. InfoNCE loss USED-FOR contrastive learning. ProtoNCE loss HYPONYM-OF InfoNCE loss. PCL COMPARE instance - wise contrastive learning methods. instance - wise contrastive learning methods COMPARE PCL. low - resource transfer learning EVALUATE-FOR PCL. low - resource transfer learning EVALUATE-FOR instance - wise contrastive learning methods. Task is maximum - likelihood estimation of the network parameters. ,"This paper proposes Prototypical Contrastive Learning (PCL), an unsupervised representation learning method based on the Expectation-Maximization framework. PCL learns low-level features for instance discrimination by clustering the semantic structures in the embedding space. These prototypes are then used for maximum-likelihood estimation of the network parameters. The authors show that PCL outperforms instance-wise contrastive learning and clustering in low-resource transfer learning. ","This paper proposes Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that combines contrastive learning and clustering to learn semantic structures in the embedding space. PCL is able to learn low-level features for instance discrimination. The authors propose a maximum-likelihood estimation of the network parameters based on the prototypes of the latent variables. They use the Expectation-Maximization framework for maximum-lihood estimation. The E-step and M-step are based on a distribution of prototypes. The InfoNCE loss is used to train the network. The experiments show that PCL outperforms instance-wise contrastive and low-resource transfer learning methods."
2553,SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,clean images USED-FOR deep neural networks. invisible perturbations FEATURE-OF clean images. block USED-FOR robust features. block USED-FOR adversarial attacks. Orthogonal Multi - Path ( OMP ) block PART-OF neural network. forward learning CONJUNCTION backward correction. backward correction CONJUNCTION forward learning. OMP block USED-FOR neural networks. forward learning USED-FOR OMP block. backward correction USED-FOR OMP block. neural networks USED-FOR features. OMP block USED-FOR features. robustness EVALUATE-FOR neural networks. variety CONJUNCTION accuracy. accuracy CONJUNCTION variety. accuracy EVALUATE-FOR vanilla neural networks. l∞ bound FEATURE-OF white - box PGD attack. accuracy EVALUATE-FOR VGG16. CIFAR10 EVALUATE-FOR vanilla neural networks. OMP block USED-FOR VGG16. OMP block USED-FOR neural networks. accuracy EVALUATE-FOR neural networks. neural networks USED-FOR black - box attacks. white - box and black - box attacks COMPARE adversarial defenders. adversarial defenders COMPARE white - box and black - box attacks. Generic is paths. OtherScientificTerm is orthogonality constraint. ,This paper studies the problem of training deep neural networks on clean images with invisible perturbations. The authors propose a block called Orthogonal Multi-Path (OMP) block that can be used to train neural networks with robust features. The OMP block combines forward learning and backward correction to improve the performance of the neural networks.  The authors show that the l∞ bound of the white-box PGD attack is tighter than for vanilla neural networks trained on CIFAR10. They also show that neural networks can defend against black-box attacks and adversarial defenders. ,"This paper proposes a novel orthogonal Multi-Path (OMP) block for training deep neural networks on clean images with invisible perturbations. The OMP block is an extension of the orthogonality constraint. The authors show that the block can be used to learn robust features for adversarial attacks. The main idea is to use forward learning and backward correction to improve the robustness of the neural networks. Experiments are conducted on CIFAR10 and VGG16, showing that vanilla neural networks outperform black-box attacks on the l∞ bound of the white-box PGD attack. The paper also shows that adversarial defenders are more robust than neural networks in terms of variety and accuracy."
2569,SP:776df66274ed12449fde8dcef873a593980f397c,Attention mechanism USED-FOR graph neural networks. graph attention model USED-FOR noisy graphs. self - supervised task USED-FOR edges. attention forms USED-FOR edges. attention forms USED-FOR self - supervised task. expressive attention USED-FOR mislinked neighbors. SuperGAT USED-FOR expressive attention. edges USED-FOR SuperGAT. homophily CONJUNCTION average degree. average degree CONJUNCTION homophily. attention forms CONJUNCTION self - supervision. self - supervision CONJUNCTION attention forms. graph characteristics USED-FOR attention forms. self - supervision HYPONYM-OF graph characteristics. average degree HYPONYM-OF self - supervision. homophily HYPONYM-OF self - supervision. average degree HYPONYM-OF graph characteristics. homophily HYPONYM-OF graph characteristics. recipe USED-FOR attention design. graph characteristics USED-FOR attention design. models COMPARE baselines. baselines COMPARE models. real - world datasets EVALUATE-FOR recipe. recipe USED-FOR models. Method is graph attention. Material is graphs. ,"This paper proposes a new attention mechanism for graph neural networks. The authors propose a graph attention model for noisy graphs. The proposed method, SuperGAT, is based on a self-supervised task where edges are represented as attention forms, and the edges are modeled as edges of a graph. The attention forms are then used to learn the edges of the graph, which are then fed to a self supervised task. The paper shows that the expressive attention can be used to identify mislinked neighbors in the graph.  The authors also show that the proposed method can be combined with other graph characteristics such as homophily, average degree, self-smoothing, and attention forms to improve the performance of the proposed models. Experiments are conducted on several real-world datasets to demonstrate the effectiveness of their proposed recipe.","This paper proposes a novel attention mechanism for graph neural networks. The authors propose a graph attention model for noisy graphs. The key idea is to use SuperGAT to learn the expressive attention for mislinked neighbors in the graph. The paper also proposes a self-supervised task to learn edges and attention forms for the edges. The proposed recipe is based on graph characteristics such as homophily, average degree, self-smoothing, etc. Experiments on real-world datasets show the effectiveness of the proposed recipe for attention design compared to other models."
2585,SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,Dialogue system USED-FOR medical automatic diagnosis ( DSMAD ). Dialogue system USED-FOR agent. reinforcement learning methods USED-FOR it. Markov decisionmaking process USED-FOR DSMAD. medical rationality FEATURE-OF inquiring process. diagnostic accuracy EVALUATE-FOR DSMAD agents. agent USED-FOR diagnosis. agent USED-FOR diagnosing processes. agent USED-FOR medical application. inquiry module CONJUNCTION introspective module. introspective module CONJUNCTION inquiry module. inquiry module USED-FOR symptom - inquiries. cooperative modules PART-OF DSMAD agent. introspective module PART-OF DSMAD agent. introspective module HYPONYM-OF cooperative modules. inquiry module HYPONYM-OF cooperative modules. inquiry module CONJUNCTION introspective module. introspective module CONJUNCTION inquiry module. evaluation metrics EVALUATE-FOR DSMAD methods. reliability EVALUATE-FOR DSMAD methods. evaluation metrics EVALUATE-FOR reliability. INS - DS COMPARE methods. methods COMPARE INS - DS. robustness EVALUATE-FOR methods. reliability EVALUATE-FOR methods. reliability CONJUNCTION robustness. robustness CONJUNCTION reliability. robustness EVALUATE-FOR INS - DS. reliability EVALUATE-FOR INS - DS. OtherScientificTerm is inquiring symptoms. Generic is interventions. ,"This paper proposes a dialogue system for medical automatic diagnosis (DSMAD). The agent is trained using reinforcement learning methods, and it is shown that DSMAD agents achieve better diagnostic accuracy than DSMAD with a Markov decisionmaking process. The agent can be used for diagnosis and for diagnosing processes. The proposed DSMAD agent consists of three cooperative modules: an inquiry module, an introspective module, and a diagnostic module. The evaluation metrics show that the proposed INS-DS achieves better reliability and robustness compared to other DSMAD methods. ","This paper proposes a dialogue system for medical automatic diagnosis (DSMAD) that is based on reinforcement learning methods. DSMAD is an extension of the Markov decisionmaking process for DSMAD. The agent is trained on a set of cooperative modules (i.e., an inquiry module, an introspective module, and a cooperative modules for symptom-inquiries) and evaluated on diagnostic accuracy of DSMAD agents. The authors show that the agent is able to perform well in the medical application and is robust to interventions. They also show that INS-DS outperforms other methods in terms of reliability and robustness. "
2601,SP:10ae09d90d465125433a9b4f15b1405ab017920d,"adaptive batch - wise regularization USED-FOR natural world distribution. Batch Confusion Norm ( BCN ) USED-FOR natural world distribution. fine - grained and long - tailed properties PART-OF natural world distribution. Batch Confusion Norm ( BCN ) USED-FOR adaptive batch - wise regularization. inter - class similarity CONJUNCTION intra - class variations. intra - class variations CONJUNCTION inter - class similarity. task USED-FOR FGVC classifier. attention mechanism USED-FOR discriminative parts. long - tailed distribution of visual classification USED-FOR class imbalance problem. class - balancing strategies CONJUNCTION classifier normalization. classifier normalization CONJUNCTION class - balancing strategies. classifier normalization CONJUNCTION negative gradient of tailed categories. negative gradient of tailed categories CONJUNCTION classifier normalization. adaptive confusion concept USED-FOR problems. BCN term USED-FOR overfitting. network learning USED-FOR cross - entropy loss. class predictions USED-FOR BCN loss. confusion energy - based framework USED-FOR long - tailed scenario. BCN USED-FOR distribution of confusion strength. BCN USED-FOR confusion energy - based framework. extra attention mechanism USED-FOR FGVC model. BCN technique USED-FOR FGVC model. iNaturalist2018 HYPONYM-OF natural world distribution dataset. iNaturalist2018 EVALUATE-FOR approach. natural world distribution dataset EVALUATE-FOR approach. Generic is approaches. OtherScientificTerm are image features of fine details, and tailed and head categories. Material is FGVC datasets. ","This paper proposes a new adaptive batch-wise regularization for natural world distribution with fine-grained and long-tailed properties. Batch Confusion Norm (BCN) is used to improve the performance of FGVC classifier by learning the discriminative parts of the image features of fine details. The BCN term is used for overfitting the FGVC model with an extra attention mechanism to learn the discriminant parts. The authors show that the BCN loss can be used for cross-entropy loss with class predictions. The proposed approach is evaluated on iNaturalist2018, which is a natural world distributions dataset. ","This paper proposes an adaptive batch-wise regularization for the natural world distribution with fine-grained and long-tailed properties. Batch Confusion Norm (BCN) is used to improve the generalization performance of FGVC classifier on the task of visual classification. The authors show that the BCN term can be used to reduce overfitting and improve the cross-entropy loss in network learning. The BCN loss is based on the class predictions of the image features of fine details and intra-class variations of the tailed and head categories.  The authors also propose an extra attention mechanism for the discriminative parts of the FGVC model, which is a BCN technique that can be applied to any FGVC method.  "
2617,SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"Bayesian inference USED-FOR ill - posed nature. ill - posed nature FEATURE-OF inverse reinforcement learning problem. Bayesian inference USED-FOR inverse reinforcement learning problem. reward USED-FOR Bayesian inference. methods COMPARE small tabular setting. small tabular setting COMPARE methods. variational approach USED-FOR latent reward. reward FEATURE-OF approximate posterior distribution. variational approach USED-FOR policy. method USED-FOR Bayesian reward inference. real medical data CONJUNCTION control simulations. control simulations CONJUNCTION real medical data. real medical data USED-FOR method. methods USED-FOR Bayesian reward inference. Method are inner - loop MDP solver, non - Bayesian methods, Approximate Variational Reward Imitation Learning ( AVRIL ), and offline imitation learning algorithms. Task is healthcare. ","This paper studies the inverse reinforcement learning problem in ill-posed nature, where the goal is to learn a policy that maximizes the reward of an inner-loop MDP solver. The authors propose Approximate Variational Reward Imitation Learning (AVRIL), a new method for Bayesian reward inference in this setting. AVRIL uses a variational approach to learn the latent reward of the policy, which is then used to estimate the approximate posterior distribution of the reward. The proposed method is evaluated on real medical data and control simulations. The results show that the proposed methods perform better than existing methods in the small tabular setting. ","This paper proposes an inverse reinforcement learning problem with ill-posed nature, which is a Bayesian inference of the ill-posited nature. The authors propose an inner-loop MDP solver, which uses a variational approach to learn the latent reward for the policy. The proposed method is evaluated on real medical data and control simulations, and compared to other methods for Bayesian reward inference in the small tabular setting. The results show that the proposed method outperforms other non-Bayesian methods, including Approximate Variational Reward Imitation Learning (AVRIL) and offline imitation learning algorithms. "
2633,SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,Search USED-FOR policies. singleand multiagent environments FEATURE-OF policies. prior search approaches USED-FOR partially observable environments. computational cost EVALUATE-FOR hidden information. search procedure USED-FOR partially observable environments. Learned Belief Search ( LBS ) HYPONYM-OF search procedure. supervised task USED-FOR approximate auto - regressive counterfactual belief. supervised task USED-FOR LBS. approximate auto - regressive counterfactual belief USED-FOR LBS. public - private model architecture USED-FOR policies. LBS USED-FOR policies. LBS USED-FOR multi - agent settings. rollouts FEATURE-OF policies. public - private model architecture USED-FOR LBS. Hanabi EVALUATE-FOR LBS. exact search USED-FOR LBS. compute requirements EVALUATE-FOR LBS. OtherScientificTerm is exact belief distribution. Generic is it. Method is search methods. ,This paper proposes a new search procedure called Learned Belief Search (LBS) for partially observable environments. LBS is an extension of prior search approaches for fully observable environments where the exact belief distribution is not known. The key idea is to use a supervised task to learn approximate auto-regressive counterfactual belief for the policies in both single and multiagent environments. The proposed LBS uses a public-private model architecture to learn policies with rollouts. The computational cost of learning the hidden information is reduced by using LBS. The empirical results show that LBS achieves state-of-the-art performance on Hanabi in multi-agent settings. ,"This paper proposes a new search procedure for partially observable environments, called Learned Belief Search (LBS), which is based on prior search approaches. The key idea of LBS is to learn policies in singleand multiagent environments where the exact belief distribution is not known. The authors propose a supervised task to learn approximate auto-regressive counterfactual belief for LBS. The proposed LBS uses a public-private model architecture to train policies in multi-agent settings with rollouts. The computational cost of learning the hidden information is much lower than existing search methods. The empirical results on Hanabi show that LBS outperforms other search methods in terms of compute requirements."
2649,SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"MCTS CONJUNCTION random shooting. random shooting CONJUNCTION MCTS. It USED-FOR bias - variance trade - off. Task is Planning in large state spaces. Method are Shoot Tree Search ( STS ), TD(n ), and STS. Generic is algorithm. OtherScientificTerm is tree search context. ","This paper studies the problem of Planning in large state spaces. The authors propose a new algorithm, Shoot Tree Search (STS), which is a combination of MCTS and random shooting. The main idea of STS is to use TD(n) as the tree search context. It is shown that STS can reduce the bias-variance trade-off.","This paper proposes a new algorithm for planning in large state spaces. The main idea is to use Shoot Tree Search (STS), TD(n), and MCTS for planning. It is shown that STS is more robust to bias-variance trade-off. The paper also provides a tree search context."
2665,SP:5efc271ccc555fd9aa542548838170bd4c98e957,"transformer networks USED-FOR inductive bias. inductive bias PART-OF neural architectures. transformer networks USED-FOR tasks. tasks USED-FOR inductive bias. datasets USED-FOR inductive bias. induction CONJUNCTION abduction. abduction CONJUNCTION induction. deduction CONJUNCTION induction. induction CONJUNCTION deduction. model USED-FOR synthetic tasks. tasks USED-FOR reasoning biases. Inductive bias USED-FOR Mathematical rEasoning. LIME HYPONYM-OF pre - training methodology. Models COMPARE vanilla transformers. vanilla transformers COMPARE Models. LIME USED-FOR Models. large mathematical reasoning benchmarks EVALUATE-FOR vanilla transformers. large mathematical reasoning benchmarks EVALUATE-FOR Models. computation cost EVALUATE-FOR pre - training approaches. computation cost FEATURE-OF downstream task. pre - training approaches USED-FOR LIME. downstream task USED-FOR LIME. computation cost USED-FOR LIME. Method is architecture engineering. OtherScientificTerm are reasoning primitives, and mathematical knowledge. Generic is they. ","This paper studies inductive bias in neural architectures. The authors propose a new pre-training methodology called LIME, which uses transformer networks to learn inductive biases in different tasks. They show that these inductive biased tasks can be used to improve the performance of the model on synthetic tasks such as induction, deduction, and abduction. They also show that LIME can achieve better performance on large mathematical reasoning benchmarks than vanilla transformers. The main contribution of the paper is to show that the computation cost of LIME is much lower than other pre-train approaches. ","This paper studies the inductive bias in neural architectures. The authors propose a new pre-training methodology called LIME, which is based on transformer networks for tasks that require reasoning primitives, i.e. induction, deduction, and abduction. They show that LIME outperforms vanilla transformers on large mathematical reasoning benchmarks. They also show that the computation cost of LIME is much lower than the downstream task, and they show that they are more robust to reasoning biases. "
2681,SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"gradient descent USED-FOR weight normalized smooth homogeneous neural nets. inductive bias FEATURE-OF weight normalized smooth homogeneous neural nets. inductive bias FEATURE-OF gradient descent. gradient flow USED-FOR networks. gradient flow path COMPARE gradient flow. gradient flow COMPARE gradient flow path. EWN USED-FOR gradient flow path. adaptive learning rate USED-FOR gradient flow. adaptive learning rate USED-FOR gradient descent. weight normalization ( SWN ) CONJUNCTION unnormalized architectures. unnormalized architectures CONJUNCTION weight normalization ( SWN ). inductive bias CONJUNCTION unnormalized architectures. unnormalized architectures CONJUNCTION inductive bias. inductive bias FEATURE-OF weight normalization ( SWN ). synthetic data sets EVALUATE-FOR unnormalized architectures. simple data sets CONJUNCTION architectures. architectures CONJUNCTION simple data sets. architectures EVALUATE-FOR sparse EWN solutions. SGD USED-FOR sparse EWN solutions. OtherScientificTerm are exponential or cross - entropy loss, radial direction, and asymptotic relative sparsity. Method is exponential weight normalization ( EWN ). Metric is asymptotic convergence rate. Task is learning prunable neural networks. ","This paper studies the gradient descent for weight normalized smooth homogeneous neural nets with inductive bias. The authors propose a new exponential weight normalization (EWN), which is an extension of the exponential or cross-entropy loss. The main idea is to use gradient flow to train networks with gradient flow instead of gradient flow in the gradient flow path in the EWN. They show that gradient flow with adaptive learning rate can converge faster than gradient flow without gradient flow. They also show that the asymptotic convergence rate of EWN can be improved by using SGD for sparse EWN solutions. ","This paper studies the gradient descent for weight normalized smooth homogeneous neural nets with inductive bias. The authors propose exponential weight normalization (EWN) and unnormalized architectures (SGD) to improve the gradient flow for networks with exponential or cross-entropy loss. The gradient flow path is based on the EWN, and the authors propose an adaptive learning rate for gradient flow to improve gradient descent. They show that the asymptotic convergence rate of SGD for sparse EWN solutions is better than SGD on simple data sets and architectures. "
2697,SP:c71f9d2a602516865a0b103028186e83b52e5f00,Generative Adversarial Networks ( GANs ) HYPONYM-OF generative models. mode collapse FEATURE-OF generator. Catastrophic Forgetting PART-OF continual learning. classification accuracy EVALUATE-FOR discriminator. training procedure USED-FOR discriminators. training scheme USED-FOR mode collapse. metrics EVALUATE-FOR GAN evaluation. GAN frameworks USED-FOR mode collapse. training scheme USED-FOR GAN frameworks. metrics EVALUATE-FOR training scheme. Generic is they. Task is mode collapse problem. Method is data generation procedure. ,"This paper studies the mode collapse problem in generative adversarial networks (GANs). The authors propose a new training scheme for GANs that is based on the Catastrophic Forgetting in continual learning. The authors show that the proposed training procedure improves the classification accuracy of the discriminators, and that they are more robust to mode collapse than existing GAN frameworks. Empirical results show the effectiveness of the proposed metrics in GAN evaluation. ","Generative Adversarial Networks (GANs) are one of the most popular generative models. However, they suffer from mode collapse due to Catastrophic Forgetting in continual learning. The authors propose a training scheme to mitigate mode collapse in GAN frameworks. The proposed training scheme improves classification accuracy of the discriminator and the data generation procedure. Experiments are conducted on several metrics for GAN evaluation."
2713,SP:52c48198c95826e042f9e5a512ef3265daaff882,"proxy score USED-FOR head importance. proxy score USED-FOR attention heads. AUBER HYPONYM-OF regularization method. attention heads PART-OF BERT. reinforcement learning USED-FOR regularization method. heuristics CONJUNCTION rule - based policies. rule - based policies CONJUNCTION heuristics. pruning policy USED-FOR attention heads. AUBER USED-FOR pruning policy. rule - based policies USED-FOR AUBER. heuristics USED-FOR AUBER. AUBER COMPARE pruning methods. pruning methods COMPARE AUBER. accuracy EVALUATE-FOR pruning methods. accuracy EVALUATE-FOR AUBER. Generic are it, and they. Method is heuristic - based methods. OtherScientificTerm is regularization. ","This paper proposes a regularization method called AUBER, which is based on reinforcement learning. The main idea is to learn a proxy score for the head importance of the attention heads in BERT. The proxy score is then used to train a pruning policy that prunes attention heads based on the proxy score. The authors show that the pruned attention heads outperform other heuristic-based methods in terms of accuracy. They also show that they can be combined with other heuristics such as rule-based policies to improve the performance.","This paper proposes a regularization method called AUBER, which is based on reinforcement learning. The main idea is to use a proxy score to measure the head importance of attention heads in BERT. The authors show that it is better than other heuristic-based methods, and they also show that they are more accurate than other pruning methods in terms of accuracy. The pruning policy for attention heads is a combination of heuristics and rule-based policies."
2729,SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"physics parameters CONJUNCTION morphology. morphology CONJUNCTION physics parameters. representation CONJUNCTION physics parameters. physics parameters CONJUNCTION representation. unpaired and randomly collected data USED-FOR correspondences. dynamics cycles USED-FOR dynamic robot behavior. cycle - consistency constraint USED-FOR dynamics cycles. simulation CONJUNCTION real robot. real robot CONJUNCTION simulation. real robot HYPONYM-OF problem domains. simulation HYPONYM-OF problem domains. dynamic state - action trajectories FEATURE-OF simulated arm. framework USED-FOR uncalibrated monocular video. framework USED-FOR dynamic state - action trajectories. real robot arm FEATURE-OF uncalibrated monocular video. Task are robotics problems, imitation learning, sim - to - real, and transfer learning. OtherScientificTerm are physics simulators, and robotics environments. Generic are correspondence, and policy. Method is fine - tuning. Material is paired data. ","This paper studies the problem of imitation learning in robotics problems, where the goal is to learn a correspondence between physics simulators and a real robot. The correspondences are learned from unpaired and randomly collected data. The authors propose a framework for fine-tuning the correspondence between the representation, physics parameters, and morphology. The correspondence is learned by learning dynamics cycles for dynamic robot behavior with a cycle-consistency constraint. The paper shows that the proposed framework can learn dynamic state-action trajectories for the simulated arm and the real robot arm in a variety of problem domains (simulation, simulation, and real robot). The paper also shows that this framework can be applied to uncalibrated monocular video. ","This paper proposes a new framework for learning the correspondence between physics simulators and real robots. The correspondence is learned by fine-tuning on unpaired and randomly collected data. The correspondences are learned using a combination of physics parameters, morphology, and physics parameters. The key idea is to use dynamics cycles to model the dynamic robot behavior. The dynamics cycles are modeled by a cycle-consistency constraint. The authors show that the proposed framework is able to learn dynamic state-action trajectories of a simulated arm and a real robot in a variety of problem domains, including simulation and real robot. They also show that imitation learning can be applied to sim-to-real, and transfer learning can also be applied in robotics environments. "
2745,SP:006434d56992836ab9420d7d4215bc70664de304,"Explainability USED-FOR model development. Explainability USED-FOR AI. solutions USED-FOR Shapley explainability. data manifold USED-FOR solutions. data manifold USED-FOR Shapley explainability. Shapley value - function USED-FOR other. generative modelling USED-FOR solution. unintelligible explanations FEATURE-OF higher - dimensional data. OtherScientificTerm are operational nuance, model ’s features, off - manifold ” Shapley values, implicit model dependence, and sensitive attributes. Method are Shapley framework, and on - manifold explainability. Task is data imputations. ",This paper studies the problem of explainability in model development. Explainability is an important problem in AI. The authors propose two solutions for Shapley explainability on the data manifold. The first is based on the Shapley framework. The second is a Shapley value-function based on generative modelling. The theoretical results show that the two solutions are equivalent to each other. ,"This paper proposes a new Shapley framework for model explainability for model development. Explainability is an important problem in AI. The authors propose two solutions for Shapley explainability on the data manifold, one based on the Shapley value-function, and the other based on generative modelling. The first is based on “off-manifold” Shapley values, where the operational nuance of the model’s features is modeled as a function of the number of parameters in the model. The second one relies on implicit model dependence, which allows to model sensitive attributes. The proposed solution is evaluated on higher-dimensional data with unintelligible explanations. The results show that the proposed solution can achieve better on-magnitude explainability than existing methods. "
2761,SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,Existing methods USED-FOR opponent modelling. local observations USED-FOR Existing methods. chosen actions CONJUNCTION received rewards. received rewards CONJUNCTION chosen actions. observed world state CONJUNCTION chosen actions. chosen actions CONJUNCTION observed world state. variational autoencoders USED-FOR local actions. local observations USED-FOR embeddings. observed world state HYPONYM-OF local observations. chosen actions HYPONYM-OF local observations. variational autoencoders USED-FOR modelling technique. embeddings USED-FOR modelling agent ’s decision policy. deep reinforcement learning USED-FOR embeddings. deep reinforcement learning USED-FOR modelling agent ’s decision policy. method COMPARE baseline method. baseline method COMPARE method. method COMPARE baseline. baseline COMPARE method. baseline COMPARE baseline method. baseline method COMPARE baseline. opponent ’s information USED-FOR baseline. embeddings USED-FOR baseline method. Generic is policy. OtherScientificTerm is opponent observations. ,"This paper proposes a new method for opponent modelling based on local observations. Existing methods are based on using local observations to train the model. The authors propose a new modelling technique based on variational autoencoders to learn the embeddings for the local actions and the chosen actions. The proposed method is based on deep reinforcement learning, where the embedding is learned using the opponent’s information. The paper shows that the proposed method outperforms the baseline method in terms of the number of observed world state and chosen actions as well as the received rewards. ","This paper proposes a new method for opponent modelling based on local observations. Existing methods are based on variational autoencoders. The authors propose a new modelling technique that combines local observations (e.g., observed world state, chosen actions, and received rewards) with the learned embeddings from deep reinforcement learning for the modelling agent’s decision policy. The proposed method is shown to outperform a baseline method based on the opponent ’s information. "
2777,SP:c239bc531bcf7293032748af29a1b786e9d893dd,"Contrastive learning USED-FOR unsupervised visual representation learning. consistency regularization term PART-OF contrastive learning framework. consistency regularization USED-FOR semi - supervised learning. Consistent Contrast ( CO2 ) USED-FOR contrastive learning framework. unlabeled data USED-FOR consistency regularization. unlabeled data USED-FOR semi - supervised learning. consistency regularization term PART-OF Consistent Contrast ( CO2 ). CO2 USED-FOR Momentum Contrast ( MoCo ). top-1 accuracy EVALUATE-FOR Momentum Contrast ( MoCo ). ImageNet linear protocol EVALUATE-FOR CO2. top-1 accuracy EVALUATE-FOR CO2. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. It USED-FOR image classification. It USED-FOR semantic segmentation. It USED-FOR object detection. PASCAL VOC USED-FOR semantic segmentation. PASCAL VOC USED-FOR object detection. PASCAL VOC USED-FOR image classification. visual representations USED-FOR tasks. CO2 USED-FOR visual representations. CO2 USED-FOR tasks. OtherScientificTerm are human annotation, heterogeneous similarity, semantic class, consistency term, and labeled semi - supervised settings. Task is instance discrimination task. Generic is task. Method is label assignment strategy. Metric is top-5 accuracy. ","This paper proposes a new contrastive learning framework, Consistent Contrast (CO2), for unsupervised visual representation learning with unlabeled data. CO2 uses a consistency regularization term in order to improve the top-1 accuracy of Momentum Contrast (MoCo) on the ImageNet linear protocol. It is also applied to image classification, object detection, and semantic segmentation using PASCAL VOC. Experiments show that CO2 is able to improve top-5 accuracy on several tasks with labeled semi-supervised settings.","Contrastive learning is an important problem in unsupervised visual representation learning. Consistent Contrast (CO2) is a contrastive learning framework that uses the consistency regularization term in the unlabeled data for semi-supervised learning. The consistency term is used to distinguish between human annotation and heterogeneous similarity. It is used for image classification, object detection, and semantic segmentation. CO2 outperforms Momentum Contrast (MoCo) on the ImageNet linear protocol with top-1 accuracy and top-5 accuracy on the instance discrimination task with label assignment strategy. It also outperforms MoCo in labeled semi -supervised settings. The experiments show that CO2 improves the performance of visual representations for tasks such as PASCAL VOC and object detection."
2793,SP:d18bab21790713e2facb053c47298fc9079ab783,"Optimistic Gradient Descent Ascent ( OGDA ) CONJUNCTION Optimistic Multiplicative Weights Update ( OMWU ). Optimistic Multiplicative Weights Update ( OMWU ) CONJUNCTION Optimistic Gradient Descent Ascent ( OGDA ). Optimistic Multiplicative Weights Update ( OMWU ) USED-FOR saddle - point optimization. Optimistic Gradient Descent Ascent ( OGDA ) USED-FOR saddle - point optimization. uniqueness of the optimal solution HYPONYM-OF assumptions. probability simplex FEATURE-OF bilinear games. OGDA CONJUNCTION OMWU. OMWU CONJUNCTION OGDA. OMWU USED-FOR constrained setting. last - iterate convergence FEATURE-OF OGDA. bilinear games FEATURE-OF OMWU. universal constant FEATURE-OF learning rate. simplex FEATURE-OF bilinear games. learning rate USED-FOR linear last - iterate convergence. constant learning rate FEATURE-OF last - iterate convergence rates. last - iterate convergence rates FEATURE-OF OGDA. condition FEATURE-OF bilinear games. polytope FEATURE-OF bilinear games. condition USED-FOR strongly - convex - stronglyconcave functions. Metric is convergence rates. OtherScientificTerm are exponentially small learning rate, equilibrium, smoothness of the objective function, and unique equilibrium assumption. Method is projected OGDA algorithm. ",This paper studies the convergence rates of the Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization in the constrained setting. The authors show that OGDA and OMWU converge faster than OGDA in bilinear games with probability simplex under the condition of an exponentially small learning rate. They also show that the learning rate of OGDA converges faster than linear last-iterate convergence with a universal constant.  The authors also provide a theoretical analysis of the smoothness of the objective function and the uniqueness of the optimal solution. ,"This paper proposes two new convergence rates for linear last-iterate optimization. The first is Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization, which are based on the uniqueness of the optimal solution. The second is the projected OGDA algorithm, which is based on OMWU in a constrained setting. The authors show that OGDA converges to a constant learning rate with universal constant, while OMWUs converge to a linear learning rate. The main contribution of the paper is to show that the convergence rates of OGDA and OMWU converge to the universal constant. The paper also shows that for bilinear games with probability simplex, the convergence rate can be bounded by an exponentially small learning rate, and that for strongly-convex-stronglyconcave functions with polytope, the learning rate is bounded by a universal constant with smoothness of the objective function. "
2809,SP:bbc7f77308b298c332a39747f693bc396f00a89f,"federated setup USED-FOR User Verification ( UV ) models. framework USED-FOR private and secure training of UV models. Federated User Verification ( FedUV ) USED-FOR private and secure training of UV models. secret user - defined linear combination USED-FOR instance embeddings. FedUV COMPARE approaches. approaches COMPARE FedUV. voice, face, and handwriting data USED-FOR user verification. Method are loss functions, and UV models. OtherScientificTerm are user embeddings, linear combinations, error - correcting code, embedding vectors, and embeddings. Generic is model. ","This paper proposes a federated setup for User Verification (UV) models. The framework is based on the Federated UserVerification (FedUV) framework for private and secure training of UV models. FedUV uses a secret user-defined linear combination between instance embeddings and the loss functions of the UV models, where the embedding vectors of the model are encoded as linear combinations. The authors then use voice, face, and handwriting data to perform user verification. The error-correcting code is then used to improve the performance of FedUV compared to other approaches.","This paper proposes a federated setup for User Verification (UV) models. The framework is designed for private and secure training of UV models, where the user embeddings are encoded in linear combinations of linear combinations, and the loss functions of the UV models are updated. The model is trained on voice, face, and handwriting data. The embedding vectors are encoded as a secret user-defined linear combination. The error-correcting code of the model is also updated. FedUV outperforms other approaches in terms of performance on user verification. "
2825,SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"geometry FEATURE-OF class manifolds ( CMs ). geometry FEATURE-OF model. technique USED-FOR boundaries. technique USED-FOR CMs. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. geometry of CMs CONJUNCTION generalization. generalization CONJUNCTION geometry of CMs. label randomization CONJUNCTION training set size. training set size CONJUNCTION label randomization. ensemble size CONJUNCTION label randomization. label randomization CONJUNCTION ensemble size. stage of training CONJUNCTION class. class CONJUNCTION stage of training. training set size CONJUNCTION model robustness. model robustness CONJUNCTION training set size. architecture CONJUNCTION random initialization. random initialization CONJUNCTION architecture. random initialization CONJUNCTION stage of training. stage of training CONJUNCTION random initialization. class CONJUNCTION ensemble size. ensemble size CONJUNCTION class. data corruption FEATURE-OF model robustness. dataset USED-FOR CM dimension. CMs USED-FOR ensembling. Method are Deep neural network classifiers, and real neural networks. OtherScientificTerm are margin, and random affine subspaces. Generic are method, and models. ","This paper proposes a new method for training deep neural network classifiers. The proposed method is based on the geometry of class manifolds (CMs), which is a well-studied geometry of the model. The authors propose a technique to learn the boundaries between two CMs, which are defined by the margin between the input and the output of the network. The margin is defined as the distance between two points in the random affine subspaces of the CMs.  The authors show that the proposed technique can be used to train CMs with different generalization, robustness, and training set size, as well as label randomization, stage of training, class, ensemble size, and random initialization. They also show that their method can be applied to real neural networks.   ","This paper proposes a new technique for learning class manifolds (CMs) with geometry. The authors show that the geometry of CMs is related to the generalization and robustness of the model. They also show that this technique can be used to learn boundaries between CMs. The method is based on Deep neural network classifiers, and is shown to outperform real neural networks in terms of generalization, robustness, training set size, label randomization, ensemble size, and stage of training. The paper also shows that this method can be applied to other models as well. "
2841,SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. RL methods USED-FOR problem. action noise USED-FOR policies. entropy FEATURE-OF policy. entropy temperature FEATURE-OF policy. entropy temperature USED-FOR entropy. Soft Actor - Critic ( SAC ) HYPONYM-OF policies. Curiosity - Aware entropy Temperature USED-FOR SAC ( CAT - SAC ). curiosity mechanism USED-FOR instance - level entropy temperature. curiosity mechanism USED-FOR Curiosity - Aware entropy Temperature. state prediction error USED-FOR curiosity. state prediction error USED-FOR CAT - SAC. entropy USED-FOR CAT - SAC. MuJoCo benchmark EVALUATE-FOR CAT - SAC. sample efficiency EVALUATE-FOR CAT - SAC. Task is reinforcement learning ( RL ). Generic is temperature. OtherScientificTerm are environment states, prediction error, and unfamiliar states. ","This paper studies the problem of learning policies with action noise in reinforcement learning (RL). The authors consider the setting where the environment states are unknown and the goal is to learn a policy that maximizes the entropy of the policy with respect to the entropy temperature of the environment. The authors propose Soft Actor-Critic (SAC), a method to learn policies with high entropy in the presence of action noise. SAC is based on Curiosity-Aware entropy Temperature (CAT-SAC) which uses a curiosity mechanism to estimate the instance-level entropy temperature for each environment state. The state prediction error is used as a proxy for the curiosity of the learned policy. The experiments on the MuJoCo benchmark show that the entropy improves the sample efficiency of CAT-SAC.","This paper proposes a novel method for learning policies with action noise in reinforcement learning (RL). The key idea is to learn policies with soft actor-critic (SAC) which is an extension of Soft Actor-Critic (SAIC) where action noise is added to the environment states to encourage exploration and exploitation. The problem is formulated as an optimization problem, and RL methods are used to solve the problem. The key insight is that the entropy of a policy is the entropy temperature of the environment state, and the entropy is the temperature of an environment state. The authors propose Curiosity-Aware entropy Temperature (CAT-SAC), which is a curiosity mechanism for instance-level entropy temperature. The curiosity mechanism is based on the state prediction error of the policy. The experiments on the MuJoCo benchmark show that the proposed CAT-SAIC achieves better sample efficiency than the baseline SAC. "
2857,SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"Reinforcement learning algorithms USED-FOR policies. meta - reinforcement learning methods USED-FOR agents. model identification CONJUNCTION experience relabeling ( MIER ). experience relabeling ( MIER ) CONJUNCTION model identification. experience relabeling ( MIER ) HYPONYM-OF meta - reinforcement learning algorithm. meta - reinforcement learning algorithm USED-FOR out - of - distribution tasks. policies CONJUNCTION value functions. value functions CONJUNCTION policies. dynamics models COMPARE value functions. value functions COMPARE dynamics models. dynamics models COMPARE policies. policies COMPARE dynamics models. off - policy data USED-FOR dynamics models. synthetic experience USED-FOR task. dynamics models USED-FOR policies. Generic are approaches, and method. Method are on - policy meta - training, and meta - reinforcement learning. ","This paper proposes a meta-reinforcement learning algorithm for learning policies in the out-of-distribution (OOD) setting. The proposed method is based on the idea of meta-regressive meta-learning, where the agent is trained on a set of policies and the goal is to learn a policy that maximizes the performance of the agent. The authors show that the proposed method can achieve better performance than existing methods on a variety of OOD tasks. ","This paper proposes a meta-reinforcement learning algorithm for learning policies that can be used for out-of-distribution (OOD) tasks. The proposed method is based on the idea of on-policy meta-training, where agents are trained on a set of agents that are trained using meta-Reinforcement Learning methods (e.g., model identification and experience relabeling). The authors show that the proposed method outperforms existing approaches in terms of performance. The authors also show that their method can be combined with dynamics models trained on off-policy data to learn policies and value functions. The paper also shows that this task can be learned using synthetic experience."
2873,SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"meta - learning techniques USED-FOR few - shot learning ( FSL ) problem. label noise USED-FOR FSL. label noise FEATURE-OF meta - learner. gradient noise problem USED-FOR meta - overfitting problem. Eigen - Reptile ( ER ) USED-FOR gradient noise. Eigen - Reptile ( ER ) USED-FOR meta - parameters. historical taskspecific parameters USED-FOR gradient noise. historical taskspecific parameters USED-FOR Eigen - Reptile ( ER ). Introspective Self - paced Learning ( ISPL ) USED-FOR prior models. Eigen - Reptile CONJUNCTION ISPL. ISPL CONJUNCTION Eigen - Reptile. tasks EVALUATE-FOR methods. methods COMPARE state - of - the - art methods. state - of - the - art methods COMPARE methods. tasks EVALUATE-FOR state - of - the - art methods. noisy labels USED-FOR state - of - the - art methods. OtherScientificTerm are meta - overfit, sampling noise, and gradient step. Task is overfitting. ","This paper studies meta-learning techniques for the few-shot learning (FSL) problem with label noise. The authors consider the gradient noise problem in the meta-overfitting problem, where the label noise of a meta-learner is added to the FSL by sampling noise from the training data. They propose an Eigen-Reptile (ER) that uses historical taskspecific parameters to learn meta-parameters for gradient noise. They also propose Introspective Self-Paced Learning (ISPL) to train prior models with noisy labels. They show that the proposed methods outperform state-of-the-art methods on several tasks. ","This paper proposes a meta-learning techniques for the few-shot learning (FSL) problem with label noise. The authors consider the meta-overfit problem, where the label noise of a particular meta-learner is added to the training data. The gradient noise problem is formulated as an extension of the Eigen-Reptile (ER) for gradient noise with historical taskspecific parameters. The prior models are based on Introspective Self-paced Learning (ISPL). The authors show that the proposed methods outperform state-of-the-art methods with noisy labels on several tasks. "
2889,SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"Adversarial training USED-FOR models. feature statistics COMPARE image pixels. image pixels COMPARE feature statistics. distributional shifts USED-FOR models. adversarially crafted distributions USED-FOR images. Stylized - ImageNet CONJUNCTION ImageNetInstagram. ImageNetInstagram CONJUNCTION Stylized - ImageNet. AdvBN USED-FOR semantic segmentation. generalization EVALUATE-FOR semantic segmentation. generalization EVALUATE-FOR AdvBN. goldfinch CONJUNCTION sulphur butterfly. sulphur butterfly CONJUNCTION goldfinch. goldfinch CONJUNCTION bulbul. bulbul CONJUNCTION goldfinch. hummingbird CONJUNCTION goldfinch. goldfinch CONJUNCTION hummingbird. house finch CONJUNCTION goldfinch. goldfinch CONJUNCTION house finch. bulbul CONJUNCTION house finch. house finch CONJUNCTION bulbul. brambling CONJUNCTION guillotine. guillotine CONJUNCTION brambling. bolete CONJUNCTION fox squirrel. fox squirrel CONJUNCTION bolete. fox squirrel CONJUNCTION hen - of - the - woods. hen - of - the - woods CONJUNCTION fox squirrel. gong CONJUNCTION bolete. bolete CONJUNCTION gong. Ibizan hound CONJUNCTION flamingo. flamingo CONJUNCTION Ibizan hound. ResNet-50 model USED-FOR classification scores. OtherScientificTerm are small adversarial perturbations, and mean and variance of deep image features. Method are adversarial training, Adversarial Batch Normalization ( AdvBN ), ResNet-50, ImageNet variants, and Adversarial Batch Normalization module. Generic is method. ","This paper studies the problem of adversarial training for models trained with distributional shifts in the presence of small adversarial perturbations. The authors propose Adversarial Batch Normalization (AdvBN) to improve the generalization performance of semantic segmentation. AdvBN is based on adversarially crafted distributions for images, and is able to learn the mean and variance of deep image features. The proposed method is evaluated on ImageNet variants such as Stylized-ImageNet and ImageNetInstagram. The ResNet-50 model is shown to improve classification scores.","Adversarial training is used to train models with distributional shifts. The main idea of adversarial training, Adversarial Batch Normalization (AdvBN), is to learn adversarially crafted distributions for images. AdvBN improves the generalization of semantic segmentation over image pixels. The proposed method is based on ResNet-50 and ImageNet variants. The authors show the mean and variance of deep image features, and the classification scores of the Resnet-50 model. "
2905,SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"proxy metric USED-FOR outliers. outliers PART-OF data distribution. Variance of Gradients ( VoG ) HYPONYM-OF proxy metric. Task are machine learning, and human - in - the - loop auditing. Method are model, models, and VoG. OtherScientificTerm is VoG scores. ",This paper proposes a proxy metric for outliers in the data distribution. The proxy metric is Variance of Gradients (VoG) and is based on human-in-the-loop auditing. The model is trained to predict the VoG scores of the outliers. The authors show that the models are able to identify outliers with high VoG.,"The paper proposes a proxy metric for outliers in the data distribution. The proxy metric is Variance of Gradients (VoG) which is used in machine learning. The model is trained on a set of samples from the dataset, and the models are trained to predict the VoG scores. The paper also proposes human-in-the-loop auditing."
2930,SP:074bfacc75837bb19049be8a2890e10de073dd8e,"real - world data FEATURE-OF simulated samples. images HYPONYM-OF simulated samples. generation quality EVALUATE-FOR model. technique USED-FOR generated samples. non - linear Fokker - Plank equation USED-FOR gradient flow. wasteful sample rejection USED-FOR methods. DRS HYPONYM-OF methods. refinement approach USED-FOR GANs. VAEs CONJUNCTION Normalizing Flows. Normalizing Flows CONJUNCTION VAEs. GANs CONJUNCTION deep generative models. deep generative models CONJUNCTION GANs. vector - valued critics CONJUNCTION deep generative models. deep generative models CONJUNCTION vector - valued critics. vector - valued critics USED-FOR GANs. Normalizing Flows HYPONYM-OF deep generative models. VAEs HYPONYM-OF deep generative models. DGf low COMPARE Discriminator Driven Latent Sampling ( DDLS ) methods. Discriminator Driven Latent Sampling ( DDLS ) methods COMPARE DGf low. Discriminator Optimal Transport ( DOT ) CONJUNCTION Discriminator Driven Latent Sampling ( DDLS ) methods. Discriminator Driven Latent Sampling ( DDLS ) methods CONJUNCTION Discriminator Optimal Transport ( DOT ). quality of generated samples EVALUATE-FOR generative models. synthetic, image, and text datasets EVALUATE-FOR DGf low. DGf low USED-FOR generative models. DGf low COMPARE Discriminator Optimal Transport ( DOT ). Discriminator Optimal Transport ( DOT ) COMPARE DGf low. quality of generated samples EVALUATE-FOR DGf low. Method are Deep generative modeling, Discriminator Gradient f low ( DGf low ), McKean - Vlasov process, and GAN variants. OtherScientificTerm are entropy - regularized f -divergences, and real and the generated data distributions. ","This paper studies the problem of deep generative modeling. The authors propose a new technique for generating generated samples from real-world data. The proposed method, Discriminator Gradient f low (DGf low), is based on the non-linear Fokker-Plank equation for the gradient flow. The main idea is to use entropy-regularized f-divergences between the generated samples and the real and the generated data distributions.  The authors show that DGf low improves the quality of generated samples for generative models such as VAEs and Normalizing Flows as well as GANs and vector-valued critics.   ","This paper proposes a new method for deep generative modeling, Discriminator Gradient f low (DGf low), which is based on the non-linear Fokker-Plank equation of the gradient flow. The authors propose a new technique for generating generated samples from real-world data (images, images, and images) that are more realistic than simulated samples from simulated samples in terms of the generation quality of the model. The proposed method is a refinement approach to GANs, where the entropy-regularized f-divergences between the generated data distributions and the real data distributions are modeled as a function of the entropy of the generated samples. The main contribution of the paper is to propose a novel method, DRS, which is an extension of previous methods such as wasteful sample rejection. The McKean-Vlasov process is applied to the GAN variants, and the authors show that the proposed method outperforms existing methods on synthetic, image, and text datasets.  The authors also show that DGf low outperforms other generative models such as VAEs, VAEs and Normalizing Flows, as well as vector-valued critics for GAN models. "
2955,SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,encoder - only Transformer CONJUNCTION encoder - decoder Transformer. encoder - decoder Transformer CONJUNCTION encoder - only Transformer. encoder - decoder Transformer USED-FOR generation tasks. encoder - only Transformer USED-FOR understanding tasks. tasks CONJUNCTION frameworks. frameworks CONJUNCTION tasks. encoder - decoder Transformer USED-FOR understanding tasks. model architectures CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION model architectures. sub - modules USED-FOR understanding and generation tasks. inference USED-FOR understanding and generation tasks. sub - modules PART-OF Transformer block. sub - modules PART-OF VECO. innersequence and cross - sequence masked language modeling USED-FOR sub - modules. sequence labeling CONJUNCTION question answering. question answering CONJUNCTION sequence labeling. question answering CONJUNCTION sentence retrieval. sentence retrieval CONJUNCTION question answering. text classification CONJUNCTION sequence labeling. sequence labeling CONJUNCTION text classification. XTREME benchmark EVALUATE-FOR cross - lingual understanding tasks. cross - lingual understanding tasks EVALUATE-FOR VECO. text classification HYPONYM-OF XTREME benchmark. text classification HYPONYM-OF cross - lingual understanding tasks. sentence retrieval HYPONYM-OF cross - lingual understanding tasks. sequence labeling HYPONYM-OF cross - lingual understanding tasks. XTREME benchmark EVALUATE-FOR VECO. question answering HYPONYM-OF cross - lingual understanding tasks. VECO COMPARE Transformer variants. Transformer variants COMPARE VECO. VECO COMPARE cross - lingual models. cross - lingual models COMPARE VECO. cross - lingual models CONJUNCTION Transformer variants. Transformer variants CONJUNCTION cross - lingual models. generation tasks EVALUATE-FOR Transformer variants. VECO USED-FOR generation tasks. cross - lingual models USED-FOR generation tasks. Method is multilingual representations. Task is downstream cross - lingual tasks. ,"This paper proposes a new model architecture for cross-lingual understanding tasks, called VECO. The proposed model consists of two modules: an encoder-decoder Transformer and a sub-modular Transformer. The sub-modules of the Transformer block consist of innersequence and cross-sequence masked language modeling, which are used to learn multilingual representations. The model architectures are trained on a variety of pre-training tasks and various tasks. The inference for understanding and generation tasks is performed using the sub-modes. The authors show that the proposed model performs well on the XTREME benchmark, and is able to achieve state-of-the-art performance on a number of cross-languages understanding tasks (e.g., question answering, sequence labeling, and sentence retrieval). The authors also show that VECo can achieve better performance than other cross-latent models on a range of generation tasks. ","This paper presents a method for learning multilingual representations. The main idea is to learn a Transformer block that consists of sub-modules that are used to encode the multilingual representation of the input language. The sub-modules are learned using innersequence and cross-sequence masked language modeling. The proposed model architectures and pre-training tasks are evaluated on the XTREME benchmark for cross-lingual understanding tasks as well as other tasks such as question answering, sequence labeling, text classification, and sentence retrieval. Experiments show that the proposed model outperforms other model architectures in both understanding and generation tasks. The authors also show that inference is used to perform both in-domain and out-of-domain inference for understanding and generating tasks. "
2980,SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"intrinsic motivation USED-FOR Reinforcement Learning ( RL ). K - means USED-FOR auditory event clusters. neural network USED-FOR auditory events. prediction errors USED-FOR intrinsic rewards. Atari games USED-FOR module. model USED-FOR audio - visual exploration. model USED-FOR active learning. Habitat simulator CONJUNCTION active learning. active learning CONJUNCTION Habitat simulator. Habitat simulator USED-FOR model. Habitat simulator USED-FOR audio - visual exploration. ThreeDWorld ( TDW ) simulator USED-FOR active learning. audio signals USED-FOR intrinsic rewards. vision - based models USED-FOR intrinsic rewards. vision - based models USED-FOR RL explorations. audio signals USED-FOR RL explorations. audio signals COMPARE vision - based models. vision - based models COMPARE audio signals. Task are causal understanding of the physical world, and RL exploration. Method is auditory event prediction. Material is acoustic data. ","This paper studies the problem of Reinforcement Learning (RL) with intrinsic motivation in the context of auditory event prediction, where the goal is to learn a causal understanding of the physical world. The authors propose a novel module based on Atari games, where K-means are used to model auditory event clusters. The neural network is trained to predict auditory events from acoustic data. The model is then used to perform audio-visual exploration in a Habitat simulator and active learning in a ThreeDWorld (TDW) simulator.  The authors show that the prediction errors of the intrinsic rewards can be used to improve the performance of the model in active learning. They also show that audio signals are able to learn intrinsic rewards better than vision-based models for RL explorations.","This paper proposes a novel approach to learn intrinsic motivation for Reinforcement Learning (RL) by using K-means to predict auditory event clusters. The authors propose a new module based on Atari games. The key idea is to use a neural network to predict the auditory events from acoustic data, and then use the causal understanding of the physical world to guide the RL exploration. The model is evaluated on a Habitat simulator for audio-visual exploration and a ThreeDWorld (TDW) simulator for active learning. The paper shows that the intrinsic rewards can be learned from prediction errors, and that the model can be used in active learning without vision-based models. "
3005,SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,singleand multimodal data USED-FOR category discovery. end - to - end framework USED-FOR representation. unlabelled data USED-FOR clusters. it USED-FOR labelled and unlabelled data. noise - contrastive estimation USED-FOR self - supervised representation learning. category discrimination CONJUNCTION cross - modal discrimination. cross - modal discrimination CONJUNCTION category discrimination. instance discrimination USED-FOR contrastive learning approaches. cross - modal discrimination USED-FOR instance discrimination. category discrimination USED-FOR instance discrimination. cross - modal discrimination USED-FOR multi - modal data. category discrimination CONJUNCTION labelled data. labelled data CONJUNCTION category discrimination. pairwise pseudo labels USED-FOR unlabelled data. pairwise pseudo labels USED-FOR cluster assignments. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. Kinetics-400 CONJUNCTION VGG - Sound. VGG - Sound CONJUNCTION Kinetics-400. image benchmarks CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION image benchmarks. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. CIFAR10 HYPONYM-OF image benchmarks. ImageNet HYPONYM-OF image benchmarks. image benchmarks EVALUATE-FOR framework. OtherScientificTerm is shared representation space. ,"This paper proposes a new framework for self-supervised representation learning based on noise-contrastive estimation. The proposed framework is based on the end-to-end framework, where the cluster assignments are learned from unlabelled data, and the clusters are used to train the representation. The authors show that the proposed framework outperforms existing contrastive learning approaches in terms of instance discrimination, category discrimination, and cross-modal discrimination. The paper also shows that it can be applied to both labelled and unlabeled data. The experimental results on image benchmarks such as CIFAR10 and ImageNet demonstrate the effectiveness of the framework.","This paper proposes a novel end-to-end framework for category discovery from singleand multimodal data. The main idea is to use noise-contrastive estimation for self-supervised representation learning. The authors show that it can be used for both labelled and unlabelled data. They also show that cross-modal discrimination and category discrimination can be applied to multi- modal data as well as instance discrimination for contrastive learning approaches. The proposed framework is evaluated on several image benchmarks, including Kinetics-400, CIFAR100, ImageNet, and VGG-Sound. "
3030,SP:4df640f502e88ddba2d7e183625231d70b083e82,"image - level tags CONJUNCTION object bounding boxes. object bounding boxes CONJUNCTION image - level tags. image - level tags HYPONYM-OF partial annotations. object bounding boxes HYPONYM-OF partial annotations. broad region coverage FEATURE-OF sparse annotations. Class activation maps USED-FOR coarse labels. conditional random fields USED-FOR sparse labels. Class activation maps USED-FOR segmentation model. Existing methods USED-FOR weak supervision. Class activation maps CONJUNCTION conditional random fields. conditional random fields CONJUNCTION Class activation maps. semi - supervised metric learning problem USED-FOR weakly supervised segmentation. semantic annotation CONJUNCTION co - occurrence. co - occurrence CONJUNCTION semantic annotation. low - level image similarity CONJUNCTION semantic annotation. semantic annotation CONJUNCTION low - level image similarity. co - occurrence CONJUNCTION feature affinity. feature affinity CONJUNCTION co - occurrence. contrastive relationships USED-FOR low - level image similarity. contrastive relationships USED-FOR semantic annotation. partial annotations USED-FOR pixel - wise feature. data - driven grouping CONJUNCTION discriminative feature learning. discriminative feature learning CONJUNCTION data - driven grouping. Pascal VOC EVALUATE-FOR universal weakly supervised segmenter. Task are Weakly supervised segmentation, and pixel localization. Generic are task, and code. OtherScientificTerm are coarse annotations, feature space, and priors. ","This paper studies the problem of weakly supervised segmentation, where the task is to learn a segmentation model from a set of partial annotations (e.g., image-level tags, object bounding boxes). The authors consider the semi-supervised metric learning problem, which is an important problem in the semi supervised learning literature. Existing methods for weak supervision are based on weak supervision, where coarse annotations are used to learn coarse annotations in the feature space. In this paper, the authors propose to use Class activation maps and conditional random fields to learn sparse labels with broad region coverage in sparse annotations. The authors show that by using these two methods, they can achieve better performance than existing methods in the weak supervision task. They also show that the proposed Pascal VOC can be used to train a universal weakly trained segmenter. ","This paper proposes a new task of Weakly supervised segmentation, where the task is a semi-supervised metric learning problem. The authors propose a new way to learn weak supervision for weak supervision. The main idea is to use Class activation maps for coarse labels and conditional random fields for sparse labels with broad region coverage. The coarse annotations are defined in the feature space, and the sparse annotations are used for pixel localization. The code is written in Pascal VOC. The paper is well-written and easy to follow. "
3055,SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"slow convergence FEATURE-OF pretext task. small scale models COMPARE supervised counterpart. supervised counterpart COMPARE small scale models. distillation strategy USED-FOR unsupervised learning. ResNet-18 CONJUNCTION ResNet-34. ResNet-34 CONJUNCTION ResNet-18. top-1 accuracies EVALUATE-FOR linear evaluation. BINGO COMPARE baselines. baselines COMPARE BINGO. ImageNet EVALUATE-FOR linear evaluation. small scale models EVALUATE-FOR BINGO. top-1 accuracies EVALUATE-FOR baselines. top-1 accuracies EVALUATE-FOR BINGO. Method are self - supervised learning, contrastive learning based methods, and distillation. Task are optimization, and Bag of InstaNces aGgregatiOn. Generic is method. OtherScientificTerm is bag of instances. ","This paper studies the problem of self-supervised learning in a pretext task with slow convergence. The authors propose a distillation strategy for unsupervised training, where the goal is to improve the performance of small scale models compared to their supervised counterpart. The proposed method is based on contrastive learning based methods. The main contribution of the paper is a new method, Bag of InstaNces aGgregatiOn, which distills the training data into a bag of instances, which is then used for optimization. Experiments on ImageNet show that BINGO achieves better top-1 accuracies than other baselines on linear evaluation. ","This paper proposes a novel method for self-supervised learning. The authors propose a distillation strategy to improve the convergence of unsupervised training on a pretext task with slow convergence. The method is based on contrastive learning based methods, and the authors propose to use Bag of InstaNces aGgregatiOn to distill a bag of instances into a single instance. Experiments show that the proposed method outperforms BINGO on ImageNet, ResNet-18, and a small scale models compared to the supervised counterpart. The top-1 accuracies for linear evaluation are also better than the baselines."
3071,SP:328866aad6544c81ded8980934df31dc4472435f,"Simulation - based inference ( SBI ) HYPONYM-OF statistical inference. stochastic models USED-FOR statistical inference. SBI algorithms COMPARE generative adversarial networks ( GANs ). generative adversarial networks ( GANs ) COMPARE SBI algorithms. adversarial approach USED-FOR SBI. GATSBI HYPONYM-OF adversarial approach. SBI CONJUNCTION GANs. GANs CONJUNCTION SBI. GATSBI USED-FOR variational objective. variational objective USED-FOR implicit posterior distributions. adversarial setting FEATURE-OF variational objective. high - dimensional posterior spaces USED-FOR Inference. GATSBI USED-FOR Inference. implicit priors USED-FOR Inference. SBI benchmark problems CONJUNCTION high - dimensional simulators. high - dimensional simulators CONJUNCTION SBI benchmark problems. SBI benchmark problems EVALUATE-FOR GATSBI. high - dimensional simulators EVALUATE-FOR GATSBI. GATSBI USED-FOR well - calibrated posterior estimates. model USED-FOR wave propagation. surface of a shallow water body FEATURE-OF wave propagation. high dimensions FEATURE-OF well - calibrated posterior estimates. it USED-FOR high - dimensional posterior. it COMPARE SBI approach. SBI approach COMPARE it. model of camera optics USED-FOR it. implicit prior USED-FOR it. implicit prior USED-FOR high - dimensional posterior. GATSBI USED-FOR sequential posterior estimation. GANs USED-FOR Bayesian inference. GATSBI USED-FOR Bayesian inference. high - dimensional simulation - based models USED-FOR Bayesian inference. GANs USED-FOR GATSBI. OtherScientificTerm are likelihoods, and explicit likelihoods. ","This paper studies the problem of simulating-based inference (SBI) with stochastic models in the context of statistical inference. The authors propose a new adversarial approach called GATSBI, which combines SBI with generative adversarial networks (GANs). Inference is performed in high-dimensional posterior spaces with implicit priors. The variational objective of the implicit posterior distributions is defined in the adversarial setting, and the likelihoods of the posterior distributions are computed using the model of camera optics. The paper shows that the GANs can be used to perform Bayesian inference with high-dimensionality in the presence of high-dimensions, and that GATS can be applied to sequential posterior estimation with high dimensions. The experimental results on SBI benchmark problems and high-dense simulators show that the proposed method performs better than the SBI approach.","This paper proposes a new adversarial approach for statistical inference based on Simulation-based inference (SBI) in the context of stochastic models. Inference is performed in high-dimensional posterior spaces in the adversarial setting, and the variational objective is based on the implicit posterior distributions of the likelihoods of the implicit priors. The authors compare SBI algorithms against generative adversarial networks (GANs) and GANs on a variety of SBI benchmark problems and high-deterministic simulators. They show that GATSBI outperforms the SBI approach in terms of well-calibrated posterior estimates in high dimensions. They also show that it outperforms a model of camera optics for wave propagation on the surface of a shallow water body. Finally, they show that the GAN can be used for sequential posterior estimation, and that it can be combined with GGANs for Bayesian inference. "
3087,SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"latent variable USED-FOR prognostic score. prognostic score USED-FOR biostatistics. prognostic score USED-FOR TEs. model USED-FOR individualized treatment effects. latent variable USED-FOR prognostic score. method COMPARE methods. methods COMPARE method. ( semi-)synthetic datasets EVALUATE-FOR method. ( semi-)synthetic datasets EVALUATE-FOR methods. Task is causal inference. OtherScientificTerm are limited overlap, features, TE error bounds, and individualized features. Method are generative prognostic model, and variational autoencoder ( VAE ). ","This paper proposes a generative prognostic model for the task of causal inference. The model learns to predict the individualized treatment effects from a latent variable, which is then used as a prognostic score for biostatistics. The proposed method is evaluated on a variety of (semi-)synthetic datasets and shows that the proposed method outperforms existing methods. ","This paper proposes a generative prognostic model that predicts individualized treatment effects using a variational autoencoder (VAE) with limited overlap. The model is trained on a set of biostatistics where the prognostic score is a latent variable. The authors show that the TEs with the same prognostic score have similar TE error bounds, but with different individualized features. The proposed method is evaluated on two (semi-)synthetic datasets and compared to other methods."
3103,SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"benchmark tasks PART-OF RL. RL algorithms USED-FOR episodic simulated environments. RL algorithms USED-FOR them. real - world platforms USED-FOR them. robots HYPONYM-OF real - world platforms. framework PART-OF simulated benchmark EARL1. simulated tasks PART-OF simulated benchmark EARL1. algorithms USED-FOR reinforcement learning. approaches USED-FOR episodic RL. approaches COMPARE approaches. approaches COMPARE approaches. autonomy FEATURE-OF algorithms. Method are Reinforcement learning ( RL ), and real - world embodied learning. OtherScientificTerm are human supervision, extrinsic intervention, and interventions. ","This paper studies the problem of Reinforcement learning (RL) in episodic environments, where the goal is to learn a model that is robust to extrinsic interventions. The authors propose a new framework called EARL1, which is a combination of two existing RL algorithms for episodic simulated environments. The main idea of the framework is to use a set of simulated benchmark tasks in RL, and then use them to train them on real-world platforms such as robots. Experiments show that the proposed approaches are able to achieve state-of-the-art performance on episodic RL, while also achieving higher levels of autonomy than existing approaches. ","This paper proposes a new framework for evaluating RL algorithms on episodic simulated environments. The framework is based on Reinforcement learning (RL), which is an extension of real-world embodied learning. The authors show that RL algorithms can be used to evaluate them on several different simulated benchmark tasks (e.g., robots) as well as on several real -world platforms. The proposed framework is evaluated on simulated benchmark EARL1, where the goal is to evaluate the ability of RL algorithms to learn to perform well in episodic RL without human supervision. Experiments show that the proposed approaches can outperform existing approaches in terms of autonomy, and also outperform other approaches for reinforcement learning. "
3119,SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"Question Answering ( QA ) HYPONYM-OF AI and NLP fields. human - level reasoning capability FEATURE-OF QA systems. modules USED-FOR reasoning. QA systems USED-FOR human reasoning process. modules USED-FOR QA systems. Graph Neural Networks ( GNNs ) USED-FOR modules. knowledge graphs ( KGs ) USED-FOR reasoning. pre - trained language models ( LMs ) USED-FOR QA systems. reasoning functionality FEATURE-OF GNN - based modules. GNN - based modules USED-FOR reasoning process. they USED-FOR QA. GNN modules USED-FOR QA. reasoning capability FEATURE-OF GNN modules. CommonsenseQA CONJUNCTION OpenBookQA. OpenBookQA CONJUNCTION CommonsenseQA. graph neural counter COMPARE GNN modules. GNN modules COMPARE graph neural counter. OpenBookQA HYPONYM-OF QA benchmark datasets. knowledge - aware reasoning USED-FOR QA benchmark datasets. OpenBookQA EVALUATE-FOR GNN modules. CommonsenseQA EVALUATE-FOR GNN modules. knowledge - aware GNN modules USED-FOR reasoning. counting HYPONYM-OF reasoning. reasoning modules USED-FOR knowledge - powered QA. Method are LMs, and GNN. ","This paper studies the question answering (QA) problem in AI and NLP fields. The authors propose a set of modules for QA systems with human-level reasoning capability. The modules are based on Graph Neural Networks (GNNs) and can be used as pre-trained language models (LMs). The modules can be applied to a variety of tasks in the human reasoning process. The reasoning can be done using knowledge graphs (KGs). The reasoning functionality of the GNN-based modules is evaluated on two QA benchmark datasets, CommonsenseQA and OpenBookQA. The GNN modules are shown to be able to perform better than the graph neural counter in QA, and they can also be used to perform QA in a knowledge-powered QA. ","This paper proposes a new module for QA, Question Answering (QA), which aims to improve the human-level reasoning capability of AI and NLP fields. The modules are based on Graph Neural Networks (GNNs) and can be used to train QA systems with pre-trained language models (LMs). The authors show that they can improve the reasoning process of GNN-based modules in terms of reasoning capability. The authors also show that GNN modules can improve QA performance on several QA benchmark datasets, including CommonsenseQA and OpenBookQA. "
3135,SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,pruning CONJUNCTION quantization. quantization CONJUNCTION pruning. pruning HYPONYM-OF Deep Neural Networks ( DNN ) compression. quantization HYPONYM-OF Deep Neural Networks ( DNN ) compression. low - cost devices USED-FOR them. performance CONJUNCTION space consumption. space consumption CONJUNCTION performance. three - stage framework USED-FOR DNN inference. Succinct Compression HYPONYM-OF three - stage framework. Succinct Compression USED-FOR DNN inference. near - optimal compression FEATURE-OF DNN inference. Succinct Data Structures USED-FOR fast queries. compressed representation USED-FOR fast queries. Succinct Data Structures USED-FOR method. method USED-FOR DNN models. formulations USED-FOR DNN models. Element - wise or Block - wise manner FEATURE-OF formulations. method USED-FOR transformed DNN models. Succinct Data Structures USED-FOR method. Succinct Data Structures USED-FOR transformed DNN models. execution pipelines USED-FOR model formulations. method USED-FOR DNN inference. execution pipelines USED-FOR method. method COMPARE Huffman Coding. Huffman Coding COMPARE method. AlexNet / VGG-16 inference EVALUATE-FOR Huffman Coding. near - optimal compression FEATURE-OF method. AlexNet / VGG-16 inference EVALUATE-FOR method. Pruning CONJUNCTION Quantization. Quantization CONJUNCTION Pruning. method COMPARE Quantization. Quantization COMPARE method. method CONJUNCTION Pruning. Pruning CONJUNCTION method. Generic is techniques. OtherScientificTerm is inference runtime. ,"This paper proposes a three-stage framework for Deep Neural Networks (DNN) compression, called Succinct Compression, which combines pruning, quantization, and pruning. The proposed method is based on a combination of two existing techniques: Pruning and Quantization. The method is evaluated on AlexNet/VGG-16 inference, where it outperforms Huffman Coding in terms of performance, space consumption, and inference runtime. The authors also show that the proposed method can be applied to DNN models trained with Sucinct Data Structures for fast queries using a compressed representation.    The authors propose three different formulations of the proposed methods in the Element-wise or Block-wise manner. The execution pipelines for the proposed model formulations are based on the execution pipelines of the three existing methods. The experimental results show the effectiveness of the method compared to Quantization, Pruning, and Pruning. ","This paper proposes a three-stage framework for Deep Neural Networks (DNN) compression: Pruning, quantization, and Pruning with low-cost devices. The proposed method is based on Succinct Compression for DNN inference with execution pipelines for model formulations in Element-wise or Block-wise manner. Experiments show that the proposed method outperforms Huffman Coding on AlexNet/VGG-16 inference. Quantization is also compared to Pruning and Quantization in terms of performance and space consumption. "
3151,SP:94c395afc794a9cc163e362078769ff83f3d20d0,"training method USED-FOR tiny neural networks. Network Augmentation ( NetAug ) HYPONYM-OF training method. data augmentation CONJUNCTION dropout. dropout CONJUNCTION data augmentation. noise USED-FOR over - fitting. regularization techniques USED-FOR large neural networks. dropout HYPONYM-OF regularization techniques. data augmentation HYPONYM-OF regularization techniques. techniques USED-FOR tiny neural networks. tiny models COMPARE large models. large models COMPARE tiny models. under - fitting CONJUNCTION over - fitting. over - fitting CONJUNCTION under - fitting. NetAug USED-FOR network ( reverse dropout ). It USED-FOR tiny model. tiny model PART-OF larger models. tiny model USED-FOR inference. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. NetAug USED-FOR image classification. NetAug USED-FOR object detection. ImageNet CONJUNCTION Cars. Cars CONJUNCTION ImageNet. NetAug USED-FOR tiny models. ImageNet EVALUATE-FOR NetAug. Pascal VOC EVALUATE-FOR NetAug. computational cost EVALUATE-FOR NetAug. Generic are model, and it. OtherScientificTerm are limited capacity, network, supervision, and inference overhead. Method is independent model. ","This paper proposes a new training method for tiny neural networks called Network Augmentation (NetAug). The training method is based on the idea of training a network (reverse dropout) on a small subset of the training data and then fine-tuning the model on the rest of the data. The main idea is to train a small model with a limited capacity, which is then used to fine-tune the network. The authors show that the proposed method can achieve better performance than existing regularization techniques such as dropout, data augmentation, and dropout on large neural networks with noise. They also show that NetAug is able to perform well on ImageNet, Cars, and Pascal VOC. ","This paper proposes a new training method for tiny neural networks. Network Augmentation (NetAug) is a training method that combines two techniques: data augmentation and dropout. Both regularization techniques are commonly used in large neural networks to reduce the over-fitting and noise. However, the authors argue that these techniques are not suitable for tiny models because of the limited capacity of the network. To address this issue, they propose a network (reverse dropout) that replaces the dropout with an independent model. The authors show that the proposed NetAug can achieve better performance on ImageNet, Cars, and ImageNet with a smaller computational cost. They also show that it can be used as a tiny model for inference on larger models. "
3167,SP:9c24549b980e415616f818acbf4cf680ef8edb52,"Point cloud sequence HYPONYM-OF data representation. flexible shape and motion information FEATURE-OF data representation. model USED-FOR temporally coherent feature spaces. real - world environments FEATURE-OF point correspondence information. generator USED-FOR temporally coherent output. point cloud sequence USED-FOR temporal coherence. learnable masking module USED-FOR upsampling ratio. point distribution USED-FOR learnable masking module. point distribution USED-FOR upsampling ratio. fluid dynamical system CONJUNCTION human action scanned data. human action scanned data CONJUNCTION fluid dynamical system. particles CONJUNCTION human action scanned data. human action scanned data CONJUNCTION particles. particles PART-OF fluid dynamical system. domains FEATURE-OF point cloud sequences. particles HYPONYM-OF point cloud sequences. fluid dynamical system HYPONYM-OF domains. human action scanned data HYPONYM-OF domains. particles HYPONYM-OF domains. quantitative and qualitative evaluation EVALUATE-FOR method. quantitative and qualitative evaluation EVALUATE-FOR upsampling task. method USED-FOR temporal coherence. quantitative and qualitative evaluation EVALUATE-FOR learning temporal coherence. upsampling task CONJUNCTION learning temporal coherence. learning temporal coherence CONJUNCTION upsampling task. irregular point cloud sequences USED-FOR temporal coherence. upsampling task EVALUATE-FOR method. OtherScientificTerm are scene flow information, and point correspondence annotation. Material is dynamic point cloud sequences. ","This paper proposes a new model for learning temporally coherent feature spaces from point cloud sequence. The model is based on Point cloud sequence, a data representation with flexible shape and motion information. The point correspondence information is learned from real-world environments, where scene flow information is available. The generator is trained to generate a temporally coherence output, and the point distribution is used as a learnable masking module to improve the upsampling ratio. The authors show that the proposed method can achieve state-of-the-art performance on both quantitative and qualitative evaluation for learning temporal coherence, and also improves the performance on an upampling task.   The authors propose to learn dynamic point cloud sequences in two domains: a fluid dynamical system with particles and human action scanned data, where the point correspondence annotation is provided. ","This paper proposes a novel model for learning temporally coherent feature spaces. The model is based on flexible shape and motion information in the data representation of a point cloud sequence. The point correspondence information is learned in real-world environments in the form of scene flow information. The generator is used to generate a temporally coherence output from the generator, and the learnable masking module learns the upsampling ratio from the point distribution of the generator. The authors show that the proposed method is able to learn temporal coherence in irregular point cloud sequences, and is shown to perform well in both quantitative and qualitative evaluation of the proposed proposed method in the downsampling task and in the learning of point cloud sequential sequences. The proposed model is evaluated on three domains: a fluid dynamical system, a human action scanned data, and a set of particles. "
3183,SP:67efe60ad37807505369b7852bc0abed29ffdda8,"robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. pre - training USED-FOR detection transformers. it USED-FOR object detection. task adapter USED-FOR it. textual prompts USED-FOR NLP. query positional embeddings USED-FOR model. visual prompts USED-FOR model. visual prompts USED-FOR query positional embeddings. self - attention USED-FOR task adapter. COCO dataset EVALUATE-FOR PT - DETR. it COMPARE detection transformers. detection transformers COMPARE it. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. generalization EVALUATE-FOR detection transformers. small - size datasets EVALUATE-FOR detection transformers. generalization FEATURE-OF small - size datasets. generalization EVALUATE-FOR it. robustness EVALUATE-FOR it. small - size datasets EVALUATE-FOR it. Method are Large - scale pre - training, 12 - layer transformer, FP - DETR, and encoder - only transformer. OtherScientificTerm are separated training paradigm, and common corruptions. Task is upstream and downstream tasks. Generic is method. ","This paper studies the problem of large-scale pre-training for detection transformers. The authors propose a separated training paradigm, FP-DETR, which is a 12-layer transformer with self-attention. The model is trained with query positional embeddings and visual prompts from textual prompts for NLP. The task adapter is then used to learn it for object detection. The proposed method is evaluated on the COCO dataset and shows that it achieves better robustness and generalization compared to other recent methods on small-size datasets.","This paper proposes a method to improve the robustness and generalization of detection transformers during pre-training for object detection. The main idea is to use a separate training paradigm for upstream and downstream tasks. The authors propose a 12-layer transformer, called FP-DETR, where the encoder-only transformer is used for downstream tasks, and a task adapter for NLP using textual prompts and visual prompts. The task adapter is based on self-attention. The proposed method is evaluated on the COCO dataset, where it is shown to improve robustness, generalization, and performance on small-size datasets."
3199,SP:a1f9897496303984fc7ad469222106b14b4a6233,"Federated Averaging ( FedAvg HYPONYM-OF federated learning algorithm. FedPAGE HYPONYM-OF federated learning algorithm. communication complexity EVALUATE-FOR FedPAGE. optimal PAGE method USED-FOR federated learning algorithm. optimal PAGE method USED-FOR FedPAGE. local methods USED-FOR federated convex and nonconvex optimization. FedPAGE COMPARE local methods. local methods COMPARE FedPAGE. FedPAGE USED-FOR federated convex and nonconvex optimization. communication rounds USED-FOR FedPAGE. communication rounds FEATURE-OF FedPAGE. nonconvex setting FEATURE-OF FedPAGE. number of communication rounds EVALUATE-FOR FedPAGE. FedPAGE CONJUNCTION SCAFFOLD. SCAFFOLD CONJUNCTION FedPAGE. FedPAGE USED-FOR federated convex and nonconvex optimization. communication complexity EVALUATE-FOR FedPAGE. Method are Local - SGD ), local SGD steps, and federated learning. Task is convex setting. OtherScientificTerm are communication round, and communication cost. ","This paper studies federated Averaging (FedAvg), a federated learning algorithm called FedPAGE, which is based on the optimal PAGE method. The authors consider the convex setting, where the communication round is the number of communication rounds, and the goal is to minimize the communication cost between the clients and the server. In this setting, the authors show that FedAE can achieve better communication complexity than local methods for federated convex and nonconvex optimization. They also provide a theoretical analysis of the communication rounds of FedPAGES in the nonconvolutional setting. ","This paper proposes FedPAGE, a federated Averaging (FedAvg) algorithm for federated learning, which is a variant of Local-SGD. The main idea is to use the optimal PAGE method to optimize the federate learning algorithm in the convex setting, where the communication round is the number of communication rounds, and the local SGD steps are the communication cost. The authors show that the communication complexity is lower than the local methods in both federated convex and nonconvex optimization. The communication rounds are also lower in the nonconvolutional setting. The paper also shows the communication costs are lower than local methods. The experimental results show that FedAEGE can achieve better communication complexity compared to local methods, and is more computationally efficient than SCAFFOLD."
3215,SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"mathematical operations USED-FOR Artificial neural networks ( ANNs ). networks USED-FOR adversarial input perturbations. decision boundary geometry FEATURE-OF ANN classifiers. adversarial perturbations USED-FOR decision boundary geometry. adversarial subspace COMPARE random subspace. random subspace COMPARE adversarial subspace. adversarial attacks PART-OF training procedure. redistribution of proximal class labels CONJUNCTION boundary curvature. boundary curvature CONJUNCTION redistribution of proximal class labels. boundary distance CONJUNCTION redistribution of proximal class labels. redistribution of proximal class labels CONJUNCTION boundary distance. Generic are network, and analysis. OtherScientificTerm are adversarial subspaces, minimal perturbation, decision boundary, boundary, and dimensionality of the subspace. Task is test - time adversarial attacks. Method is adversarial training. ","This paper studies the problem of adversarial attacks on artificial neural networks (ANNs) using mathematical operations. The authors show that the decision boundary geometry of ANN classifiers with adversarial perturbations can be approximated by a decision boundary in the form of a random subspace, and that the adversarial subspace can be represented as a subset of the original network. The paper then proposes a training procedure that combines adversarial attack with a redistribution of proximal class labels, boundary curvature, and boundary distance. The proposed training procedure is evaluated on a variety of benchmark datasets.",This paper presents a theoretical analysis of the decision boundary geometry of ANN classifiers in terms of adversarial perturbations. The authors show that adversarial subspaces with minimal perturbation are more robust to adversarial attacks compared to the random subspace. They also show that the loss function of the adversarial training is more robust than the one of the training procedure. The paper is well-written and easy to follow. 
3231,SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"hashtags USED-FOR auxiliary information. similar representations CONJUNCTION dissimilar representations. dissimilar representations CONJUNCTION similar representations. self - supervised representations COMPARE auxiliary - information - infused representations. auxiliary - information - infused representations COMPARE self - supervised representations. auxiliary - information - infused representations COMPARE supervised representations. supervised representations COMPARE auxiliary - information - infused representations. direct downstream labels USED-FOR supervision signals. self - supervised representations COMPARE supervised representations. supervised representations COMPARE self - supervised representations. direct downstream labels USED-FOR supervised representations. approach COMPARE baseline representation learning methods. baseline representation learning methods COMPARE approach. approach COMPARE approach. approach COMPARE approach. approach COMPARE baseline representation learning methods. baseline representation learning methods COMPARE approach. auxiliary data information USED-FOR approach. auxiliary data information USED-FOR baseline representation learning methods. approach USED-FOR unsupervised constructed clusters. approach USED-FOR unsupervised representation learning approach. auxiliary information HYPONYM-OF unsupervised constructed clusters. OtherScientificTerm are data clustering information, and cluster. Material is Instagram image. Method is weakly - supervised contrastive learning approach. ",This paper proposes a weakly-supervised contrastive learning approach for unsupervised clustering. The proposed approach is based on auxiliary data information (e.g. hashtags) that is used to represent the clusters in the Instagram image. The auxiliary information is learned by using direct downstream labels to represent supervision signals. The authors show that the proposed approach outperforms the baseline representation learning methods on unstructured constructed clusters with auxiliary information. ,"This paper proposes a weakly-supervised contrastive learning approach to unsupervised clustering. The idea is to use a set of hashtags as auxiliary information for each cluster, and then use direct downstream labels for supervision signals. The proposed approach is evaluated on two datasets and compared to baseline representation learning methods that do not use auxiliary data information. The results show that the proposed approach outperforms the existing approach on the two datasets. "
3247,SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"Recovering sparse parameters PART-OF machine learning. observational data USED-FOR Recovering sparse parameters. algorithms USED-FOR problem. path - following algorithm USED-FOR PLISA. recovery accuracy EVALUATE-FOR PLISA. empirical Rademacher complexity EVALUATE-FOR PLISA. empirical Rademacher complexity EVALUATE-FOR generalization ability. generalization ability EVALUATE-FOR PLISA. PLISA USED-FOR sparse estimation problems. stability CONJUNCTION convergence. convergence CONJUNCTION stability. generalization ability CONJUNCTION algorithmic properties. algorithmic properties CONJUNCTION generalization ability. convergence FEATURE-OF unrolled algorithm. stability FEATURE-OF unrolled algorithm. convergence HYPONYM-OF algorithmic properties. stability HYPONYM-OF algorithmic properties. techniques USED-FOR learning - based algorithms. OtherScientificTerm are hyperparameters, and problem distribution of interest. Generic are they, and analysis. Method are Provable Learning - based Iterative Sparse recovery Algorithm, and generalization analysis. ","This paper studies the problem of recovering sparse parameters from observational data in machine learning. Recovering sparse parameters in observational data is an important problem in the context of sparse estimation problems, where the hyperparameters of the problem distribution of interest are unknown. The authors propose two algorithms to solve this problem. The first algorithm, PLISA, is a path-following algorithm that is based on the Provable Learning-based Iterative Sparse recovery Algorithm. The empirical Rademacher complexity of PLISA improves the recovery accuracy and generalization ability of the PLISA with respect to the number of observations. The second algorithm is a unrolled algorithm with convergence, stability, and other algorithmic properties. The theoretical analysis shows that the two techniques are useful for learning-based algorithms, and that they can be used to improve the generalization performance. ","This paper proposes a method for recovering sparse parameters in machine learning from observational data. The authors propose a Provable Learning-based Iterative Sparse recovery Algorithm (PLISA) to solve the problem. PLISA is based on a path-following algorithm, where the hyperparameters of the problem distribution of interest are sampled from a set of existing algorithms. They show that PLISA improves the recovery accuracy and generalization ability of PLISA in terms of empirical Rademacher complexity and the theoretical properties such as stability and convergence of the unrolled algorithm. They also show that they can improve the generalization analysis of the proposed algorithms. "
3263,SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"robot control CONJUNCTION game AI. game AI CONJUNCTION robot control. unified homogeneous action space FEATURE-OF hybrid action space. discretization USED-FOR unified homogeneous action space. Hybrid Action Representation ( HyAR ) USED-FOR compact and decodable latent representation space. compact and decodable latent representation space USED-FOR hybrid action space. embedding table CONJUNCTION conditional Variational Auto - Encoder ( VAE ). conditional Variational Auto - Encoder ( VAE ) CONJUNCTION embedding table. HyAR USED-FOR latent space. embedding table USED-FOR HyAR. unsupervised environmental dynamics prediction USED-FOR action representation. DRL algorithms USED-FOR representation space. action space FEATURE-OF hybrid action embeddings. discrete - continuous action space USED-FOR HyAR. HyAR COMPARE baselines. baselines COMPARE HyAR. baselines USED-FOR high - dimensional action spaces. OtherScientificTerm are Discrete - continuous hybrid action space, discrete or continuous action space, approximation difficulties, discrete action, and continuous parameter. Method are Reinforcement Learning ( RL ), and RL algorithms. Task is hybrid action RL. ","This paper studies the problem of Reinforcement Learning (RL) in the hybrid action RL setting. The authors propose Hybrid Action Representation (HyAR), a compact and decodable latent representation space for a unified homogeneous action space with discretization. The latent space is represented by an embedding table and a conditional Variational Auto-Encoder (VAE). The action representation is learned by unsupervised environmental dynamics prediction. The representation space is learned using DRL algorithms. The paper shows that HyAR outperforms existing baselines for high-dimensional action spaces. ","This paper proposes Hybrid Action Representation (HyAR) for compact and decodable latent representation space, which is a unified homogeneous action space with discretization. The key idea is to use Reinforcement Learning (RL) to learn a discrete or continuous action space, and then use a conditional Variational Auto-Encoder (VAE) and an embedding table to represent the latent space. The paper also proposes an unsupervised environmental dynamics prediction to learn the action representation. Experiments show that HyAR outperforms other baselines for high-dimensional action spaces, and also outperforms DRL algorithms for representation space. "
3279,SP:5128bf712f6b197de113c7a371b4bec36f978eca,SGEM USED-FOR general non - convex stochastic optimization problems. Stochastic Gradient USED-FOR general non - convex stochastic optimization problems. Stochastic Gradient USED-FOR SGEM. AEGD method USED-FOR SGEM. energy CONJUNCTION momentum. momentum CONJUNCTION energy. energy USED-FOR SGEM. momentum PART-OF SGEM. energydependent convergence rates CONJUNCTION regret bound. regret bound CONJUNCTION energydependent convergence rates. regret bound USED-FOR online convex setting. energydependent convergence rates FEATURE-OF nonconvex stochastic setting. unconditional energy stability property FEATURE-OF SGEM. threshold USED-FOR energy variable. SGEM COMPARE AEGD. AEGD COMPARE SGEM. SGDM USED-FOR deep neural networks. SGEM USED-FOR deep neural networks. SGEM COMPARE SGDM. SGDM COMPARE SGEM. Method is Adaptive Gradient Descent. ,"This paper proposes Adaptive Gradient Descent (AGD), a new method for stochastic gradient descent (SGEM) for nonconvex optimization problems. AEGD is based on the Stochastic Gradient (SG) method, which is an extension of SGEM. The authors show that SGEM has the same unconditional energy stability property as SGDM, and that it can converge faster than SGDM in the online convex setting. ","This paper proposes Adaptive Gradient Descent (AGD), a new method for solving stochastic optimization problems with Stochastic Gradient. AEGD is an extension of SGEM, where the energy of the SGEM is defined as the sum of energy and momentum, and the regret bound is the energy dependent convergence rates in the online convex setting. The authors show that SGEM has an unconditional energy stability property. They also show that the threshold for the energy variable is the same for SGEM and SGDM for deep neural networks."
3295,SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"non - autoregressive ( NAR ) approaches USED-FOR inference. NAR models COMPARE AR counterparts. AR counterparts COMPARE NAR models. training CONJUNCTION inference. inference CONJUNCTION training. multiple datasets EVALUATE-FOR NAR models. NAR EVALUATE-FOR CMLMC. raw data USED-FOR CMLMC. multiple datasets EVALUATE-FOR AR. multiple datasets EVALUATE-FOR CMLMC. Metric is human - level accuracy. Method are AR framework, and distillation. OtherScientificTerm is raw data without distillation. ","This paper studies the problem of non-autoregressive (NAR) approaches for inference in the context of CMLMC. NAR models have been shown to perform better than their AR counterparts on multiple datasets. The main contribution of this paper is to propose a new AR framework, which is based on the distillation of the raw data without distillation. The authors show that the performance of the NAR model on the multiple datasets is comparable to the state-of-the-art NAR counterparts. ","This paper proposes a new framework for learning non-autoregressive (NAR) approaches for inference and training. NAR models are shown to outperform their AR counterparts in terms of human-level accuracy, training and inference on multiple datasets. The main contribution of the paper is the introduction of a new AR framework, CMLMC, which uses raw data without distillation. Experiments show that the proposed method outperforms NAR on CML MCMC on raw data with distillation, and multiple datasets with and without."
3311,SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,Ultra - low power local signal processing USED-FOR edge applications. always - on devices USED-FOR edge applications. limited power budget FEATURE-OF domain. spiking neural networks USED-FOR Neuromorphic processors. computational power FEATURE-OF Neuromorphic processors. limited power budget FEATURE-OF Neuromorphic processors. spiking neural dynamics COMPARE dilated temporal convolutions. dilated temporal convolutions COMPARE spiking neural dynamics. WaveSense HYPONYM-OF spiking neural network. WaveNet architecture USED-FOR spiking neural network. neural dynamics CONJUNCTION fixed time - constants. fixed time - constants CONJUNCTION neural dynamics. fixed time - constants CONJUNCTION feed - forward architecture. feed - forward architecture CONJUNCTION fixed time - constants. WaveSense USED-FOR neuromorphic implementation. feed - forward architecture USED-FOR WaveSense. fixed time - constants USED-FOR WaveSense. neural dynamics USED-FOR WaveSense. datasets USED-FOR keyword - spotting. keyword - spotting EVALUATE-FOR model. datasets EVALUATE-FOR model. CNNs CONJUNCTION LSTMs. LSTMs CONJUNCTION CNNs. network COMPARE spiking neural networks. spiking neural networks COMPARE network. network COMPARE artificial neural networks. artificial neural networks COMPARE network. LSTMs HYPONYM-OF artificial neural networks. CNNs HYPONYM-OF artificial neural networks. ,"This paper studies the problem of ultra-low power local signal processing for edge applications on always-on devices. Neuromorphic processors have a limited power budget in this domain due to their computational power. The authors propose WaveSense, a spiking neural network based on the WaveNet architecture. WaveSense uses neural dynamics and fixed time-constant to learn the neural dynamics of a neural network, and a feed-forward architecture to perform the neuromorphic implementation. The proposed model is evaluated on two datasets for keyword-spouting, showing that the proposed network performs better than the state-of-the-art CNNs and LSTMs.","This paper proposes a novel neural network architecture, WaveSense, for edge applications with ultra-low power local signal processing on always-on devices. WaveSense is a spiking neural network based on the WaveNet architecture. The authors show that WaveSense outperforms existing Neuromorphic processors in terms of computational power and limited power budget in the domain. The proposed network outperforms other neural networks such as CNNs and LSTMs. The paper also shows that the neural dynamics of WaveSense and fixed time-constant of the WaveSense can be combined with a feed-forward architecture for neuromorphic implementation. The model is evaluated on two datasets for keyword-spotting."
3327,SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"machine learning USED-FOR social applications. machine learning USED-FOR injustice. algorithms USED-FOR high - confidence behavioral guarantees. Shifty algorithms HYPONYM-OF algorithms. algorithms USED-FOR demographic shift ’s challenges. Shifty HYPONYM-OF technique. real - world dataset of university entrance exams EVALUATE-FOR Shifty. algorithm USED-FOR models. high - confidence fairness guarantees FEATURE-OF algorithm. Method is machine learning algorithms. OtherScientificTerm are unfair behavior, and demographic shift. Generic are approaches, and methods. Metric is fairness assurances. ","This paper studies the problem of fairness in machine learning for social applications. The authors propose a technique called Shifty, which is based on the Shifty algorithms. The main idea is to learn algorithms that have high-confidence behavioral guarantees on the unfair behavior, and then use these algorithms to solve “demographic shift’s challenges. The proposed algorithm is evaluated on a real-world dataset of university entrance exams, and shows that the proposed algorithm can achieve state-of-the-art performance on these models. ","This paper proposes a new technique called Shifty, which aims to address the problem of fairness in machine learning for social applications. The authors propose two algorithms for achieving high-confidence behavioral guarantees. The first algorithm is based on the Shifty algorithm, which is used to train models on a real-world dataset of university entrance exams. The second algorithm, Shifty is a variant of Shifty. The main difference between the two algorithms is that Shifty does not rely on unfair behavior, while Shifty relies on fairness assurances. The proposed algorithms are evaluated on two “demographic shift’s challenges”. The results show that the proposed methods are able to achieve better fairness assurances than existing approaches."
3343,SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"Stochastic dual dynamic programming ( SDDP ) USED-FOR multi - stage stochastic optimization. Stochastic dual dynamic programming ( SDDP ) USED-FOR modeling real - world process optimization tasks. worst - case complexity EVALUATE-FOR SDDP. trainable neural model USED-FOR problem instances. trainable neural model USED-FOR piece - wise linear value function. trainable neural model USED-FOR SDDP. intrinsic low - dimension space FEATURE-OF piece - wise linear value function. SDDP CONJUNCTION reinforcement learning algorithms. reinforcement learning algorithms CONJUNCTION SDDP. solution quality EVALUATE-FOR competitors. ν - SDDP COMPARE competitors. competitors COMPARE ν - SDDP. problem solving cost EVALUATE-FOR ν - SDDP. reinforcement learning algorithms HYPONYM-OF competitors. SDDP HYPONYM-OF competitors. solution quality EVALUATE-FOR ν - SDDP. synthetic and real - world process optimization problems EVALUATE-FOR ν - SDDP. OtherScientificTerm are decision variables, and successive problems. Material is low dimensional problems. Method is SDDP solver. Task is optimization. ",This paper proposes Stochastic dual dynamic programming (SDDP) for multi-stage stochastic optimization. SDDP is a trainable neural model that learns a piece-wise linear value function in the intrinsic low-dimension space. The authors show that SDDP has the worst-case complexity in terms of the number of decision variables in the decision space. They also show that the SDDP solver is able to solve low dimensional problems. ,This paper proposes Stochastic dual dynamic programming (SDDP) for multi-stage stochastic optimization tasks. SDDP is an extension of the SDDP solver to low dimensional problems. The authors show that SDDP has the worst-case complexity in terms of the number of decision variables in the intrinsic low-dimension space. They also show that a trainable neural model can be used to learn a piece-wise linear value function for the problem instances. They compare the performance of SDDP and other reinforcement learning algorithms on synthetic and real-world process optimization problems. They show that ν-SDDP has better solution quality than other competitors. 
3359,SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,protocol USED-FOR private next - token prediction. protocol USED-FOR privacy violations. language models USED-FOR privacy violations. private corpus USED-FOR language models. relaxation of group differentially private prediction USED-FOR SUBMIX. data - dependent privacy accounting mechanism USED-FOR it. it USED-FOR data - extraction attacks. data - dependent privacy accounting mechanism USED-FOR data - extraction attacks. data - dependent privacy accounting mechanism USED-FOR SUBMIX. SUBMIX HYPONYM-OF protocol. transformer - based models USED-FOR next - token predictions. GPT-2 HYPONYM-OF transformer - based models. Generic is model. Method is language model. ,"This paper proposes a new protocol for private next-token prediction based on the relaxation of group differentially private prediction in language models. The protocol, called SUBMIX, is a data-dependent privacy accounting mechanism, and it can be used to prevent data-extraction attacks. The authors show that the proposed protocol can prevent privacy violations on language models trained on a private corpus. The proposed model is based on transformer-based models, such as GPT-2, and is shown to be able to perform well on the task.","This paper proposes a protocol for private next-token prediction, called SUBMIX, which is based on the relaxation of group differentially private prediction. The protocol is designed to prevent privacy violations in language models. In particular, it uses a data-dependent privacy accounting mechanism to prevent it from being used for data-extraction attacks. The paper proposes to use a private corpus of language models to train the privacy violations. The model is trained using transformer-based models such as GPT-2."
3375,SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,unsupervised method USED-FOR OOD samples. classification model USED-FOR k - NN density estimate. k - NN density estimate USED-FOR unsupervised method. k - NN density estimator COMPARE OOD detection method. OOD detection method COMPARE k - NN density estimator. Label Smoothed Embedding Hypothesis HYPONYM-OF label smoothing. label smoothing USED-FOR model. proposal COMPARE OOD baselines. OOD baselines COMPARE proposal. k - NN density estimation USED-FOR OOD examples. finite - sample high - probability statistical results USED-FOR k - NN density estimation. Material is indistribution samples. ,"This paper proposes a new unsupervised method for OOD samples. The proposed method is based on the k-NN density estimate from a classification model. The model is trained using label smoothing, which is a variant of Label Smoothed Embedding Hypothesis. The paper shows that the proposed k-NN density estimation can be used to estimate OOD examples with finite-sample high-probability statistical results. The experimental results show the effectiveness of the proposed proposal compared to other OOD baselines.",This paper proposes an unsupervised method for OOD samples. The proposed method uses a classification model to estimate the k-NN density estimate using finite-sample high-probability statistical results. The model is based on label smoothing based on Label Smoothed Embedding Hypothesis. The paper shows that the proposed proposal outperforms existing OOD baselines on OOD examples.
3391,SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"continuous time domain FEATURE-OF stochastic differential equations. stochastic differential equations USED-FOR Diffusion - based methods. denoising score matching USED-FOR models. denoising score matching framework USED-FOR representation learning. GANs CONJUNCTION VAEs. VAEs CONJUNCTION GANs. VAEs USED-FOR representations. GANs USED-FOR representations. denoising score matching objective USED-FOR diffusion - based representation learning. approach USED-FOR infinite - dimensional latent code. infinite - dimensional latent code USED-FOR state - of - the - art models. semi - supervised image classification EVALUATE-FOR state - of - the - art models. adversarial training USED-FOR diffusionbased models. adversarial training USED-FOR sample quality. smaller noise scales FEATURE-OF approximation of the prior. sampling speed EVALUATE-FOR adversarial training. approximation of the prior USED-FOR adversarial training. Method are non - adversarial generative model, and multi - scale denoising autoencoders. OtherScientificTerm are supervised signal, and latent codes. Task is denoising. Generic is representation. ","This paper proposes a denoising score matching objective for diffusion-based representation learning with stochastic differential equations in the continuous time domain. Diffusion-based methods are commonly used in the literature for learning representations in the discrete time domain, and the denoised score matching framework is a popular method for representation learning. The authors propose to use denoizing score matching to improve the performance of models trained with denoises. The proposed approach is based on a non-adversarial generative model, where the supervised signal is generated from the latent codes of the latent code. The representations are trained with GANs and VAEs. The results show that the proposed approach can improve the sample quality of state-of-the-art models in semi-supervised image classification. The paper also provides an approximation of the prior for adversarial training on smaller noise scales, which improves the sampling speed. ","This paper proposes a denoising score matching framework for representation learning in the continuous time domain. Diffusion-based methods are based on stochastic differential equations in the discrete time domain, and the authors propose to use a non-adversarial generative model to learn the representation. The authors use GANs and VAEs to generate the representations, which are then used to train the models. The proposed approach can be applied to infinite-dimensional latent code for state-of-the-art models such as semi-supervised image classification. The paper shows that adversarial training improves the sample quality of diffusionbased models in terms of sampling speed and the approximation of the prior on smaller noise scales. "
3407,SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"Goal - conditioned reinforcement learning ( RL ) USED-FOR tasks. navigation CONJUNCTION manipulation. manipulation CONJUNCTION navigation. expert demonstrations CONJUNCTION reward shaping. reward shaping CONJUNCTION expert demonstrations. offline data CONJUNCTION expert demonstrations. expert demonstrations CONJUNCTION offline data. planning USED-FOR curriculum of intermediate states. algorithm USED-FOR distant goal - reaching task. planning USED-FOR algorithm. M - step USED-FOR goal - conditioned policy. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. expectation maximization USED-FOR goal - conditioned policies. goal - conditioned RL CONJUNCTION graph search. graph search CONJUNCTION goal - conditioned RL. prior methods COMPARE ours. ours COMPARE prior methods. planning USED-FOR ours. goal - conditioned RL USED-FOR prior methods. graph search USED-FOR prior methods. method COMPARE prior methods. prior methods COMPARE method. it USED-FOR tasks. graph search USED-FOR methods. Method are Classifier - Planning ( C - Planning ), and graph planning. Generic is policy. ","The paper proposes a new algorithm for goal-conditioned reinforcement learning (RL) for tasks where the goal is to reach a distant goal. The algorithm is based on the idea of Classifier-Planning (C-Placing), which is a curriculum of intermediate states that are learned by planning. The goal is learned by M-step and E-step, and the goal-conditional policy is learned using expectation maximization. The paper shows that the proposed method is able to achieve better performance than prior methods on a variety of tasks, including navigation, manipulation, expert demonstrations, reward shaping, offline data, and graph planning. ","This paper proposes Classifier-Planning (C-Plans), a method for learning goal-conditioned reinforcement learning (RL) for tasks such as navigation, manipulation, reward shaping, and expert demonstrations. The authors propose an algorithm for a distant goal-reaching task, where the goal is to learn a curriculum of intermediate states. The algorithm is based on the M-step for learning a goal-conditional policy, and the E-step to learn the policy. The main idea is to use expectation maximization to learn goal- conditioned policies. The proposed method is evaluated on three tasks, and it outperforms prior methods such as graph search, goal-constrained RL, and graph planning."
3423,SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"regularization technique USED-FOR deep neural networks. Mixup HYPONYM-OF regularization technique. mixup USED-FOR k - mixup. Wasserstein metric FEATURE-OF interpolation. interpolation HYPONYM-OF displacement interpolation. mixup USED-FOR k - mixup case. k - mixup USED-FOR cluster and manifold structures. network architectures CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION network architectures. mixup HYPONYM-OF data augmentation approach. data augmentation approach USED-FOR models. beta distribution USED-FOR Averaging weights. mixup USED-FOR Perturbations. embedded manifold USED-FOR distributions. α USED-FOR procedure. fully - connected network USED-FOR binary classification. synthetic datasets USED-FOR binary classification. 1 - mixup CONJUNCTION 32 - mixup regularization. 32 - mixup regularization CONJUNCTION 1 - mixup. synthetic datasets USED-FOR fully - connected network. k - mixup USED-FOR local structure. blur CONJUNCTION contrast. contrast CONJUNCTION blur. displacement interpolation USED-FOR optimal transport. global cluster CONJUNCTION manifold support structure. manifold support structure CONJUNCTION global cluster. k - mixup USED-FOR perturbed training datasets. manifold support structure FEATURE-OF perturbed training datasets. global cluster FEATURE-OF perturbed training datasets. Metric are generalization, adversarial robustness, and robustness. Generic is It. OtherScientificTerm are local distributional structure, clusters, data manifold, and discrete distributions. Task is regularization. Method is mixup regularization. ","This paper proposes a new regularization technique for deep neural networks called Mixup. Mixup is a data augmentation approach to improve the generalization performance of models trained with a data augmentation approach. It is based on the idea that the local distributional structure of the data manifold can be used to learn the cluster and manifold structures of the training data, and the local structure can then be used for the k-mixup case.  Mixup uses the Wasserstein metric of the interpolation of the displacement interpolation between the global cluster and the manifold support structure, which is used to estimate the optimal transport between clusters and manifolds. Averaging weights are computed by using the beta distribution of the Averaged weights of the two distributions.  The authors propose a procedure based on α, where α is the distance between the clusters and the manifolds of the clusters. The authors show that the proposed procedure can improve the robustness of a fully-connected network trained with synthetic datasets for binary classification on a set of perturbed training datasets with a global cluster.    The main contribution of the paper is to show that mixup regularization can improve generalization and robustness against adversarial robustness.  It is also shown that the performance of the proposed method can be improved by k-Mixup for perturbed learning datasets with k-mixture regularization and 1-mixed regularization. ","This paper proposes a regularization technique for deep neural networks, called Mixup. Mixup is a data augmentation approach to improve generalization and adversarial robustness. It is based on the Wasserstein metric of the interpolation of the local distributional structure between clusters and manifold structures. The paper shows that Mixup can be applied to the k-mixup case, where the local structure is the data manifold, and the manifold support structure is a global cluster. It also shows that it can be used to improve Perturbations. The main contribution of the paper is the proposed procedure, which uses α to compute the Averaging weights of the beta distribution of the clusters and the corresponding distributions in the embedded manifold. The authors show that the proposed mixup regularization is robust to perturbations, and that it is able to improve the robustness of models trained with the proposed data augmentations. Experiments are conducted on synthetic datasets for binary classification on a fully-connected network and on several network architectures and benchmark datasets. "
3439,SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,lightweight network USED-FOR embeddings. nonlinear classification layer USED-FOR lightweight network. nonlinear classification layer USED-FOR embeddings. nonlinearity USED-FOR representation ( embedding ) learning. deep networks USED-FOR representation ( embedding ) learning. embeddings USED-FOR linear classifier. linear classifier USED-FOR they. nonlinearity USED-FOR deep networks. embedding vector space FEATURE-OF nonlinear classifiers. limited - capacity backbone USED-FOR network. nonlinear kernelized classification layer USED-FOR deep networks. classification layer USED-FOR nonlinear classifier. radial kernel functions USED-FOR nonlinear classifier. embeddings FEATURE-OF radial kernel functions. radial kernel functions USED-FOR classification layer. layer USED-FOR model - efficient classifiers. layer USED-FOR computer vision and natural language processing tasks. ,"This paper proposes a lightweight network with a nonlinear classification layer to learn embeddings for a linear classifier. The proposed network is based on a limited-capacity backbone. The authors show that the nonlinearity in the embedding vector space of nonlinear classifiers can be used to improve the performance of deep networks for representation (embedding) learning. The nonlinear kernelized classification layer is then used to train deep networks with a classification layer. The radial kernel functions in the classification layer are used to learn the radial kernel function of the non-linear classifier, which is then applied to train the layer for model-efficient classifiers on computer vision and natural language processing tasks.","This paper proposes a lightweight network for learning embeddings from a nonlinear classification layer. The authors propose to use nonlinearity in representation (embedding) learning in deep networks to improve the performance of the linear classifier. The proposed network is based on a limited-capacity backbone. The nonlinear kernelized classification layer is used to train deep networks in the embedding vector space. The classification layer uses radial kernel functions to train the nonlinear classifier, and the proposed layer is applied to model-efficient classifiers for computer vision and natural language processing tasks."
3455,SP:01ee8ec81619784788eb0ce9785098e437d17a7c,Graph Neural Networks ( GNNs ) USED-FOR node representations. nodal features CONJUNCTION graph structure. graph structure CONJUNCTION nodal features. fairness - aware data augmentation frameworks USED-FOR intrinsic bias. nodal features CONJUNCTION graph structure. graph structure CONJUNCTION nodal features. nodal features USED-FOR fairness - aware data augmentation frameworks. graph structure USED-FOR fairness - aware data augmentation frameworks. schemes USED-FOR GNN - based learning mechanisms. schemes USED-FOR fairness. fairness EVALUATE-FOR GNN - based learning mechanisms. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. real networks USED-FOR graph contrastive learning. real networks USED-FOR node classification. real networks USED-FOR link prediction. statistical parity CONJUNCTION equal opportunity. equal opportunity CONJUNCTION statistical parity. augmentation strategies COMPARE contrastive methods. contrastive methods COMPARE augmentation strategies. augmentation strategies USED-FOR fairness. statistical parity FEATURE-OF fairness. Method is Node representation learning. Material is graphs. Generic is representations. ,"Graph Neural Networks (GNNs) are used to learn node representations. Node representation learning is an important problem in the context of graph classification and link prediction. In this paper, the authors propose two fairness-aware data augmentation frameworks based on nodal features and graph structure. The authors show that the proposed schemes improve the fairness of GNN-based learning mechanisms in terms of statistical parity and equal opportunity. They also show that real networks can be used for graph contrastive learning and node classification. ",Graph Neural Networks (GNNs) are used to learn node representations. Node representation learning is an important problem in the context of graph classification and link prediction. The authors propose two fairness-aware data augmentation frameworks based on nodal features and graph structure to mitigate intrinsic bias in GNN-based learning mechanisms. They show that the proposed schemes can improve fairness in terms of statistical parity and equal opportunity. They also show that real networks can be used for graph contrastive learning and for node classification. 
3471,SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"observational data USED-FOR estimating treatment effects. instrumental variable ( IV ) USED-FOR two - stage regression. 2SLS HYPONYM-OF two - stage regression. IVs CONJUNCTION confounders. confounders CONJUNCTION IVs. nonlinear IV regression variants USED-FOR confounding bias. confounders USED-FOR nonlinear IV regression variants. balancing USED-FOR treatment effect estimation. bias - variance trade - off FEATURE-OF imbalanced treatment distributions. balanced confounders representation USED-FOR treatment effect estimation. nonlinear IV methods USED-FOR confounding. balanced representation of confounders USED-FOR confounder balancing. treatment regression PART-OF modules. modules PART-OF CB - IV algorithm. outcome regression PART-OF CB - IV algorithm. treatment regression PART-OF CB - IV algorithm. confounder balancing USED-FOR treatment effect estimation. IV regression USED-FOR treatment effect estimation. confounder balancing USED-FOR IV regression. multiplicative assumption COMPARE additive separability assumption. additive separability assumption COMPARE multiplicative assumption. multiplicative assumption FEATURE-OF CB - IV algorithm. IV regression CONJUNCTION confounder balancing methods. confounder balancing methods CONJUNCTION IV regression. CB - IV algorithm USED-FOR treatment effect estimation. CB - IV algorithm COMPARE state - of - the - art methods. state - of - the - art methods COMPARE CB - IV algorithm. state - of - the - art methods USED-FOR treatment effect estimation. confounder balancing methods USED-FOR treatment effect estimation. CB - IV algorithm COMPARE confounder balancing methods. confounder balancing methods COMPARE CB - IV algorithm. IV regression HYPONYM-OF CB - IV algorithm. confounder balancing methods HYPONYM-OF state - of - the - art methods. IV regression HYPONYM-OF state - of - the - art methods. OtherScientificTerm are unmeasured confounders, additive separability of noise, and observed confounders. Generic are they, and second stage. Material is linear setting. ","This paper studies the problem of estimating treatment effects from observational data in two-stage regression with instrumental variable (IV) in a linear setting. The authors propose a new method, 2SLS, that combines two modules: treatment regression and outcome regression. The CB-IV algorithm is based on a multiplicative assumption on the additive separability of noise, which is a well-studied result in the literature. The main contribution of the paper is the theoretical analysis of the bias-variance trade-off of imbalanced treatment distributions in the presence of unmeasured confounders and the confounding bias of nonlinear IV regression variants. The paper also provides a balanced representation of the confounder balancing for treatment effect estimation based on the balanced confoundingers representation. The experimental results show that the proposed CB- IV algorithm performs better than state-of-the-art methods such as IV regression and confounding methods in the setting of IV regression.","This paper presents a two-stage regression algorithm for estimating treatment effects from observational data. The authors propose to use an instrumental variable (IV) as a surrogate for the treatment effect, which is used as a measure of the confounding bias of the treatment distribution. Two modules of the proposed CB-IV algorithm are proposed: 1) 2SLS, where the instrumental variable is used for estimating the treatment effects, and 2) a second stage where the IVs and the confounders are used as measures of the nonlinear IV regression variants. Experiments show that the proposed algorithm outperforms state-of-the-art methods on both IV regression and confounder balancing for treatment effect estimation. "
3487,SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"stochastic gradient descent steps USED-FOR models. MAML objective COMPARE non - adaptive learning ( NAL ). non - adaptive learning ( NAL ) COMPARE MAML objective. MAML COMPARE NAL. NAL COMPARE MAML. easy and hard tasks PART-OF linear regression setting. MAML COMPARE NAL. NAL COMPARE MAML. hardness EVALUATE-FOR tasks. MAML USED-FOR hard tasks. Method are gradient descent, and two - layer neural networks. OtherScientificTerm is easy tasks optimal solutions. Task is few - shot image classification. ","This paper studies the problem of few-shot image classification in the linear regression setting. The authors propose a new gradient descent algorithm, called MAML, which is based on stochastic gradient descent steps. The main idea is to learn the optimal solutions for a set of easy tasks optimal solutions, and then use these optimal solutions to train two-layer neural networks. The paper shows that the proposed algorithm achieves better performance than non-adaptive learning (NAL) in both easy and hard tasks. The empirical results show that the MAMPL outperforms NAL in both the hard tasks and in the easy tasks.","This paper proposes a new MAML objective for non-adaptive learning (NAL). The authors propose to use stochastic gradient descent steps to train models. The authors show that gradient descent can be applied to two-layer neural networks. They also show that MAL outperforms NAL on both easy and hard tasks in the linear regression setting. They show that for easy tasks optimal solutions can be found, and that for hard tasks the hardness of the proposed tasks can be reduced. The paper also shows that the proposed method can be used for few-shot image classification."
3503,SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"astrophysics CONJUNCTION remote sensing. remote sensing CONJUNCTION astrophysics. Sparse Blind Source Separation ( BSS ) USED-FOR applications. remote sensing HYPONYM-OF applications. astrophysics HYPONYM-OF applications. Proximal Alternating Linearized Minimization ( PALM ) algorithm HYPONYM-OF sparse BSS methods. PALM hyperparameters CONJUNCTION variables. variables CONJUNCTION PALM hyperparameters. PALM hyperparameters USED-FOR Unrolling PALM. data - driven knowledge USED-FOR Unrolling PALM. Learned PALM ( LPALM ) algorithm USED-FOR semi - blind source separation. algorithm COMPARE PALM. PALM COMPARE algorithm. LPALM USED-FOR astrophysical multispectral imaging. cumbersome hyperparameter FEATURE-OF PALM. LPALM COMPARE PALM. PALM COMPARE LPALM. separation quality EVALUATE-FOR algorithm. unrolled source separation methods USED-FOR semi - blind setting. LPALM COMPARE unrolled source separation methods. unrolled source separation methods COMPARE LPALM. LPALM USED-FOR semi - blind setting. OtherScientificTerm are hyperparameter choice, and variable mixing matrices. Method are algorithm unfolding / unrolling, and unrolled algorithms. Task is real - world applications. ","This paper studies the problem of semi-blind source separation in sparse BSS. Sparse Blind Source Separation (BSS) is a well-studied problem in real-world applications such as astrophysics, remote sensing, etc. The authors propose a Proximal Alternating Linearized Minimization (PALM) algorithm, which is an extension of the learned Learned PALM (LPALM), which is used in the unrolled source separation setting. Unrolling PALM uses the learned PALM hyperparameters and the variables in the learned LPALM to learn the hyperparameter choice. The proposed algorithm is shown to achieve better separation quality than the standard unrolled algorithms. ","The paper proposes Sparse Blind Source Separation (BSS) for applications such as astrophysics, remote sensing, etc. The paper proposes a Proximal Alternating Linearized Minimization (PALM) algorithm, which is a variant of sparse BSS methods. Unrolling PALM is based on data-driven knowledge, where the algorithm unfolding/unrolling is done by learning a set of hyperparameter choice. The authors show that the proposed algorithm outperforms existing unrolled algorithms in terms of separation quality. The main difference is that the authors propose to use the learned PALM hyperparameters instead of the standard variables. The proposed algorithm is evaluated on astrophysical multispectral imaging and on real-world applications. The results show that LPALM outperforms unrolled source separation methods in the semi-blind setting, and that the learned Learned PALM (LPALM ) algorithm is able to achieve better separation quality than the standard unrolled methods. The algorithm is also shown to be more computationally efficient than the conventional PALM, and does not require any additional computation. "
3519,SP:7716315001949ab88c8a216302fe51bae872fc87,"transformers USED-FOR language modeling. power - law relationship FEATURE-OF transformers. memory CONJUNCTION computation. computation CONJUNCTION memory. attention module USED-FOR Legendre Memory Unit based model. implicit self - attention HYPONYM-OF attention module. loss EVALUATE-FOR transformers. transformers COMPARE LSTMs. LSTMs COMPARE transformers. model COMPARE transformers. transformers COMPARE model. model COMPARE transformers. transformers COMPARE model. model COMPARE LSTMs. LSTMs COMPARE model. transformers COMPARE LSTMs. LSTMs COMPARE transformers. loss EVALUATE-FOR model. global self - attention USED-FOR architecture. OtherScientificTerm are model size, and sequence length. Metric is computational and memory requirements. ",This paper proposes a Legendre Memory Unit based model with implicit self-attention. The proposed model is based on transformers with a power-law relationship between the model size and the sequence length. The authors show that the proposed model has better performance than LSTMs with the same loss. The paper also shows that the model is able to learn a sequence of words in a sequence with global self attention. ,"This paper proposes a Legendre Memory Unit based model with an attention module that is based on implicit self-attention. The authors show that the power-law relationship between transformers and LSTMs can be approximated by the model size and the sequence length. The proposed model is evaluated on a variety of computational and memory requirements, including memory, computation, and sequence length, and the proposed model outperforms transformers in terms of the loss. The architecture is also based on global self attention."
3535,SP:832f422b3554e89702e13c8c5690ee26f2289e3b,Generative adversarial networks ( GANs ) USED-FOR image generation. photo - realistic quality EVALUATE-FOR Generative adversarial networks ( GANs ). LatentKeypointGAN HYPONYM-OF two - stage GAN. internal conditioning FEATURE-OF space keypoints. appearance embeddings PART-OF keypoints. domain knowledge CONJUNCTION supervision signals. supervision signals CONJUNCTION domain knowledge. network architectures CONJUNCTION training schemes. training schemes CONJUNCTION network architectures. LatentKeypointGAN USED-FOR interpretable latent space. re - positioning USED-FOR LatentKeypointGAN. generating portraits HYPONYM-OF keypoint embeddings. GAN - based method USED-FOR unsupervised keypoint detection. Material is image content. OtherScientificTerm is spatial and appearance factors. ,"This paper studies the problem of unsupervised keypoint detection using GANs. The authors propose LatentKeypointGAN, a two-stage GAN with internal conditioning on the space keypoints. The keypoint embeddings in the keypoints are generated by the appearance embedding of the image content. LatentkeypointGAN is able to learn the interpretable latent space by re-positioning the key points in the image. The paper also proposes a GAN-based method for unsupersupervised learning of keypoints in the latent space. Experiments show the effectiveness of the proposed method. ","This paper proposes a novel two-stage GAN, LatentKeypointGAN, for image generation with photo-realistic quality. The keypoint embeddings in the keypoints are generated from domain knowledge, supervision signals, and network architectures. The space keypoints have internal conditioning on the spatial and appearance factors, and the image content is generated by re-positioning the keypoint in the interpretable latent space. The paper also proposes a GAN-based method for unsupervised keypoint detection. "
3551,SP:9206ae6e31077569313838504ef6daa89ad3b59c,"layer normalization USED-FOR deep fully - connected neural networks. mean field formalism USED-FOR deep fully - connected neural networks. initialization scheme CONJUNCTION activation function. activation function CONJUNCTION initialization scheme. normalization techniques USED-FOR problems. method USED-FOR residual networks. method USED-FOR initialization variances. Task is non - perturbative analysis of signal propagation. OtherScientificTerm are depth, gradient explosion, and representation shrinkage. Method is fully - connected architecture. ",This paper studies the problem of non-perturbative analysis of signal propagation in deep fully-connected neural networks with mean field formalism. The authors propose a new layer normalization method for deep fully connected neural networks. The proposed method is based on a combination of an initialization scheme and an activation function. The method is shown to improve the initialization variances of residual networks.  ,"This paper proposes a non-perturbative analysis of signal propagation in the context of layer normalization for deep fully-connected neural networks with mean field formalism. The authors propose a new initialization scheme and an activation function based on the idea of gradient explosion. The main contribution of the paper is a new way to measure the depth of the network, which is a result of the representation shrinkage. The method is applied to residual networks, and the authors show that the proposed method can be used to measure initialization variances and activation variances. The paper also shows that the fully-connect architecture is more robust to the gradient explosion, and that normalization techniques can be applied to these problems."
3567,SP:2177be818b5843c580c787f1b2d725154846feb6,"optimal step sizes USED-FOR stochastic gradient descent. line searches USED-FOR step sizes. line searches PART-OF optimization. step sizes FEATURE-OF full - batch loss. line - search method USED-FOR full - batch loss. parabola USED-FOR line - search method. parabolas USED-FOR Learning rates. approach COMPARE SGD. SGD COMPARE approach. line search approaches USED-FOR Deep Learning across models. approach COMPARE line search approaches. line search approaches COMPARE approach. SGD COMPARE line search approaches. line search approaches COMPARE SGD. piece - wise constant learning rate schedule USED-FOR approach. validation and test accuracy EVALUATE-FOR batch sizes. piece - wise constant learning rate schedule USED-FOR SGD. validation and test accuracy EVALUATE-FOR approach. Task is Deep Learning. OtherScientificTerm are inherent noise, noisy update step directions, and optimal update step size. ",This paper studies the problem of finding optimal step sizes for stochastic gradient descent in the presence of inherent noise. The authors propose a new line-search method for the full-batch loss with step sizes that are close to the optimal update step size. The proposed approach is based on a piece-wise constant learning rate schedule and is shown to outperform SGD in terms of validation and test accuracy. The paper also shows that the proposed approach outperforms line search approaches in Deep Learning across models. ,The paper proposes a method for learning stochastic gradient descent with optimal step sizes. The idea is to use line searches to optimize the step sizes in optimization. Learning rates are computed using a parabola. The authors show that the proposed approach outperforms SGD and other line search approaches in Deep Learning across models. They also show that their approach can achieve better validation and test accuracy than SGD with a piece-wise constant learning rate schedule. 
3583,SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"Noise - contrastive estimation ( NCE ) HYPONYM-OF statistically consistent method. statistically consistent method USED-FOR unnormalized probabilistic models. noise distribution USED-FOR NCE. noise distribution USED-FOR NCE. exponential loss USED-FOR eNCE. eNCE HYPONYM-OF NCE. exponential loss USED-FOR NCE. OtherScientificTerm are flat ) loss landscape, and exponential family. Method is normalized gradient descent. ","This paper proposes a statistically consistent method for unnormalized probabilistic models based on noise-contrastive estimation (NCE). NCE is based on the exponential loss in the flat (loss landscape) loss landscape. The noise distribution of NCE can be used as the noise distribution for NCE. The exponential loss is used for eNCE, which is an extension of the NCE in the exponential family. The authors show that normalized gradient descent can be applied to NCE, and show that it can improve the performance.","This paper proposes Noise-contrastive estimation (NCE), a statistically consistent method for unnormalized probabilistic models. NCE is based on the exponential family of the noise distribution of the NCE, which is a flat (loss landscape) loss landscape. The authors show that eNCE is a variant of NCE with an exponential loss, and that the normalized gradient descent is equivalent to the exponential loss."
3599,SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"Privacy HYPONYM-OF distributed machine learning. distributed SGD algorithm USED-FOR model. noisy information USED-FOR differential privacy ( DP ). parameter - server architecture FEATURE-OF distributed SGD algorithm. DP CONJUNCTION BR. BR CONJUNCTION DP. convergence FEATURE-OF distributed SGD. Byzantine faults FEATURE-OF distributed SGD. ( α, f)-Byzantine resilience USED-FOR those. ( α, f)-BR USED-FOR approximate convergence guarantee. hyperparameter optimization USED-FOR guarantee. approaches USED-FOR DP. approaches USED-FOR BR. DP CONJUNCTION BR. BR CONJUNCTION DP. DP CONJUNCTION BR. BR CONJUNCTION DP. Method is learning algorithm. Metric is learning accuracy. ","This paper studies the problem of differential privacy in distributed machine learning with noisy information. The authors propose a distributed SGD algorithm based on parameter-server architecture, where the model is trained using a distributed learning algorithm. They provide an approximate convergence guarantee based on (α, f)-Byzantine resilience to Byzantine faults in the distributedSGD. They also provide a theoretical guarantee for hyperparameter optimization for DP and BR. Finally, they show that the proposed approaches can converge faster than DP, BR, and DP. ","This paper proposes a new privacy framework for distributed machine learning. The authors propose to use differential privacy (DP) to protect the privacy of the model from noisy information in the parameter-server architecture of a distributed SGD algorithm. The proposed learning algorithm is based on (α, f)-Byzantine resilience, and the authors provide an approximate convergence guarantee based on hyperparameter optimization. Experiments on DP, BR, and DP show that the proposed approaches outperform DP and BR in terms of learning accuracy. "
3615,SP:bc783f0c829f90931535e63687d13172879631b3,code editing USED-FOR query code snippet. support exemplars USED-FOR query code snippet. editing exemplar USED-FOR editorial pattern. common pattern USED-FOR code editing. support exemplars USED-FOR common pattern. deep learning approach USED-FOR code editing problem. them USED-FOR query code snippet editing. support exemplars USED-FOR edit representations. edit representations PART-OF learning approach. multi - extent similarities ensemble USED-FOR query code snippet editing. language - specific grammar USED-FOR abstract syntax trees. similarities measurement USED-FOR collective tree representations. collective tree representations USED-FOR query and support sample matching. method COMPARE non - composition baselines. non - composition baselines COMPARE method. C # and Python datasets EVALUATE-FOR method. Task is computer source code editing. Material is support and query code snippets. Method is similarity - ranking error estimator. ,"This paper studies the problem of query code snippet editing in the context of support and query code snippets. The authors propose a deep learning approach to solve the code editing problem, where the goal is to find a common pattern for each query code, and then use support exemplars to generate edit representations for that common pattern. The proposed learning approach is based on a multi-extent similarities ensemble, where each editing exemplar is used to generate an editorial pattern that is then used as a support exemplar for the query code. The similarity-ranking error estimator is then applied to the edit representations generated by the multi-expectation similarities ensemble. The results show that the proposed method outperforms non-composite baselines on C # and Python datasets.  The authors also propose a language-specific grammar for abstract syntax trees, which can be used for query and support sample matching.","This paper proposes a deep learning approach to the code editing problem. The authors propose a multi-extent similarities ensemble for query code snippet editing, where support exemplars are used to generate query code snippets and edit representations are used for editing the edit representations. The proposed method is evaluated on C # and Python datasets and compared to non-compositional baselines. The paper also proposes a similarity-ranking error estimator. "
3631,SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,text CONJUNCTION music. music CONJUNCTION text. deep generative models USED-FOR realistic sequence data. text HYPONYM-OF realistic sequence data. music HYPONYM-OF realistic sequence data. high - level structure USED-FOR generative process. local coherence CONJUNCTION global coherence. global coherence CONJUNCTION local coherence. global coherence EVALUATE-FOR models. local coherence EVALUATE-FOR models. approach USED-FOR global structure. relational constraints FEATURE-OF global structure. model USED-FOR realistic data. model USED-FOR relational constraints. model PART-OF generative model. model PART-OF generative model. program synthesis algorithm USED-FOR relational constraints. constraint data USED-FOR generative model. approach COMPARE state - of - the - art. state - of - the - art COMPARE approach. approach USED-FOR high - level structure. state - of - the - art USED-FOR high - level structure. capturing high - level structure EVALUATE-FOR approach. low - level structure EVALUATE-FOR approach. OtherScientificTerm is measures of music. Generic is constraints. ,"This paper studies the problem of generating high-level structure in deep generative models for realistic sequence data (e.g., text and music). The authors propose a novel approach to capture the global structure of a generative model by using relational constraints on the relational constraints of the model. The relational constraints are learned by a program synthesis algorithm. The authors show that the proposed approach is able to capture both local coherence and global coherence of the models. They also show that their approach can capture the high -level structure better than state-of-the-art. ","This paper proposes a novel approach to capture high-level structure in deep generative models for realistic sequence data (e.g., text and music). The key idea is to use relational constraints to capture the global structure and local coherence of the models. The authors propose a program synthesis algorithm to generate relational constraints for the generative model, and then use the model to generate realistic data from the relational constraints. The proposed approach is compared to state-of-the-art for capturing high -level structure and low-level structures. The results show that the proposed approach can capture the local structure and global coherence. The paper also shows that the approach is able to capture constraints that are not present in the real world. "
3647,SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"biological systems CONJUNCTION combinatorial optimization. combinatorial optimization CONJUNCTION biological systems. particle physics CONJUNCTION biological systems. biological systems CONJUNCTION particle physics. scaling problems FEATURE-OF set - to - hypergraph tasks. run - time complexity HYPONYM-OF scaling problems. training method USED-FOR iterative refinement. efficiency CONJUNCTION constant memory usage. constant memory usage CONJUNCTION efficiency. contributions PART-OF set - to - hypergraph model. model COMPARE state - of - the - art. state - of - the - art COMPARE model. Task is set - to - hypergraph prediction. OtherScientificTerm are hyperedges, and positive edges. Metric are memory requirements, and asymptotic memory scaling. ","This paper studies the problem of set-to-hypergraph prediction in the setting where hyperedges are not available. The authors propose a new training method for iterative refinement, which is based on the idea that the hyperedge can be represented as a set of positive edges. The paper also proposes a new model that is able to learn the contributions of each set-of-hypergraphic model. The proposed model is evaluated on a variety of tasks, including particle physics, biological systems, combinatorial optimization, and scaling problems such as run-time complexity. The results show that the proposed model can achieve better performance than state of-the-art in terms of efficiency and constant memory usage.","This paper studies the scaling problems of set-to-hypergraph tasks with run-time complexity, including biological systems, particle physics, and combinatorial optimization. The authors propose a new training method for iterative refinement of hyperedges, which is based on the idea of asymptotic memory scaling. The paper shows that the proposed model outperforms state-of-the-art in terms of efficiency and constant memory usage. The contributions of the proposed set -to-hologram model are two-fold: 1) the model is able to learn a set of contributions that can be used to improve the performance of the model, and 2) it is possible to use the contributions of a set-t-hugger to improve set- to-hogger prediction. "
3663,SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"biases FEATURE-OF models. post - processing method USED-FOR models. deep embeddings PART-OF pre - trained model. shallow neural network USED-FOR It. Ethical Module HYPONYM-OF shallow neural network. methodology COMPARE bias mitigation. bias mitigation COMPARE methodology. Method is deep learning algorithms. OtherScientificTerm are representation power, von Mises - Fisher loss, and latent space. Task is gender bias in facial recognition. ","This paper studies the problem of gender bias in facial recognition. The authors propose a post-processing method for training models with biases. It is based on a shallow neural network, called Ethical Module, which is a combination of deep embeddings in a pre-trained model and the representation power of a deep learning algorithms. The paper also proposes a new von Mises-Fisher loss for the latent space. The proposed methodology is shown to achieve better performance than bias mitigation.","This paper proposes a post-processing method for models with gender bias in facial recognition. It uses a shallow neural network with an Ethical Module, where deep embeddings of the pre-trained model are added to the latent space. The authors show that the representation power of the deep learning algorithms can be reduced by minimizing the von Mises-Fisher loss. The proposed methodology is shown to outperform bias mitigation."
3679,SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"new class data USED-FOR KD loss. phase model USED-FOR old class knowledge. free image stream USED-FOR placebo data. placebo data USED-FOR KD loss. Google Images HYPONYM-OF free image stream. ImageNet-1k CONJUNCTION ImageNet - Subset. ImageNet - Subset CONJUNCTION ImageNet-1k. supervision CONJUNCTION memory budget. memory budget CONJUNCTION supervision. memory budget FEATURE-OF old class exemplars. CIL methods EVALUATE-FOR method. higher - resolution benchmarks EVALUATE-FOR top - performing CIL methods. ImageNet-1k HYPONYM-OF higher - resolution benchmarks. ImageNet - Subset HYPONYM-OF higher - resolution benchmarks. Task are class - incremental learning ( CIL ), and learning of new classes. Method are knowledge distillation ( KD ), evaluation function, and reinforcement learning algorithm. Material are old - class data, and image stream. OtherScientificTerm are class overlap, placebos, and pseudo CIL tasks. Generic is function. ","This paper studies the problem of class-incremental learning (CIL) in the context of knowledge distillation (KD), where the goal is to distill the KD loss from new class data to old class data. The authors propose a phase model to learn the old class knowledge from the new class knowledge. The evaluation function is a combination of a reinforcement learning algorithm and a free image stream (e.g., Google Images). The authors show that the new image stream can be used as a substitute for the placebo data in KD loss. The paper also shows that old class exemplars with a large memory budget can be learned from the image stream. The proposed method is evaluated on a number of higher-resolution benchmarks (ImageNet-1k, ImageNet-Subset) and compared to other top-performing CIL methods. ",This paper proposes a novel method for learning knowledge distillation (KD) from old-class data. The key idea is to use a phase model to distill old class knowledge from the free image stream (e.g. Google Images) to the KD loss from the new class data. This is done by minimizing the class overlap between the old and new classes. The evaluation function is based on a reinforcement learning algorithm. The paper shows that the proposed method outperforms other top-performing CIL methods on higher-resolution benchmarks (ImageNet-1k and ImageNet-Subset) as well as on pseudo CIL tasks. The authors also show that the memory budget of the old class exemplars and the supervision of the new classes can be reduced.
3695,SP:506e0a888c03a955b708464eed3670c04baf4912,"approach USED-FOR modeling discrete structure. Energy - based Models ( EBMs ) USED-FOR modeling discrete structure. inference CONJUNCTION learning of EBM. learning of EBM CONJUNCTION inference. Energy - based Models ( EBMs ) USED-FOR approach. inference USED-FOR EBM. sampling from discrete distributions USED-FOR it. Markov Chain Monte Carlo ( MCMC ) USED-FOR sampling. informed proposal USED-FOR Markov Chain Monte Carlo ( MCMC ). local updates FEATURE-OF informed proposal. energy changes USED-FOR it. composition of local moves USED-FOR path auxiliary algorithm. sampling CONJUNCTION inference. inference CONJUNCTION sampling. inference CONJUNCTION learning. learning CONJUNCTION inference. path auxiliary algorithms COMPARE generic samplers. generic samplers COMPARE path auxiliary algorithms. generic samplers USED-FOR sampling. generic samplers USED-FOR discrete models. path auxiliary algorithms USED-FOR discrete models. discrete models USED-FOR sampling. discrete models USED-FOR inference. generic samplers USED-FOR inference. high dimensional discrete data USED-FOR deep EBMs. OtherScientificTerm are discrete distributions, evaluation of energy function, and linearization of the energy function. Generic is algorithm. ","This paper proposes a new approach for modeling discrete structure in Energy-based Models (EBMs) by sampling from discrete distributions. The approach is based on the Markov Chain Monte Carlo (MCMC) and it uses an informed proposal to learn the energy changes of a Markov chain Monte Carlo with local updates. The authors propose a path auxiliary algorithm based on composition of local moves, which can be applied to any EBM. The proposed algorithm is evaluated on high dimensional discrete data for deep EBMs. The results show that the proposed path auxiliary algorithms outperform generic samplers and discrete models in both sampling and inference.","This paper proposes a novel approach to modeling discrete structure in Energy-based Models (EBMs) for the problem of deep EBMs on high dimensional discrete data. The proposed approach is based on sampling from discrete distributions and learning of EBM. The authors propose an informed proposal for Markov Chain Monte Carlo (MCMC) based on local updates of the energy changes of the model, which allows for efficient evaluation of energy function. They also propose a path auxiliary algorithm based on the composition of local moves. They show that the proposed algorithm outperforms other path auxiliary algorithms for discrete models in terms of sampling, inference, and learning. "
3711,SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"Discovery and learning of an underlying spatiotemporal hierarchy PART-OF machine learning. sequential data USED-FOR Discovery and learning of an underlying spatiotemporal hierarchy. layerwise representations USED-FOR hierarchical generative models. Variational Predictive Routing ( VPR ) HYPONYM-OF neural probabilistic inference system. neural probabilistic inference system USED-FOR latent representations of video features. temporal hierarchy FEATURE-OF latent representations of video features. hierarchical renewal process USED-FOR continuous data. VPR USED-FOR organisation of representations. event detection mechanism USED-FOR VPR. organisation of representations PART-OF model. latent hierarchy USED-FOR organisation of representations. system USED-FOR event detection mechanism. latent representations USED-FOR event detection mechanism. VPR USED-FOR event boundaries. VPR USED-FOR timeagnostic rollouts. video datasets EVALUATE-FOR VPR. framework USED-FOR model - based reinforcement learning. approach USED-FOR framework. approach USED-FOR model - based reinforcement learning. neuroscience USED-FOR approach. OtherScientificTerm are spatiotemporal hierarchy, temporal dynamics, spatiotemporal features, hierarchy, and flexible and informative state - space rollouts. ","This paper proposes a new neural probabilistic inference system called Variational Predictive Routing (VPR) for learning the latent representations of video features in a temporal hierarchy. The idea is to learn the spatiotemporal hierarchy of the video features from sequential data, and then use the learned representations to train hierarchical generative models with layerwise representations. VPR learns the organisation of representations in the model and the event detection mechanism based on the learned latent representations. The hierarchical renewal process is applied to continuous data. The VPR is able to learn event boundaries for timeagnostic rollouts. Experiments on several video datasets demonstrate the effectiveness of the proposed framework in model-based reinforcement learning. ","This paper proposes a new framework for model-based reinforcement learning based on the discovery and learning of an underlying spatiotemporal hierarchy in the context of machine learning on sequential data. The authors propose Variational Predictive Routing (VPR), a neural probabilistic inference system for learning the latent representations of video features in the temporal hierarchy of the video features. The model is based on a hierarchical generative models with layerwise representations. The idea is to learn the organisation of representations in the latent hierarchy and the event detection mechanism of the model based on VPR. The hierarchical renewal process is applied to continuous data, and the authors show that VPR can learn the event boundaries of the temporal dynamics of the latent representation. The proposed framework is evaluated on a variety of video datasets, and is shown to outperform the state-of-the-art in terms of flexibility and informative state-space rollouts. "
3727,SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,global features USED-FOR methods. re - ranking process USED-FOR global features. it USED-FOR accurate and semantic local information. spatial and channel attention CONJUNCTION intermediate supervision. intermediate supervision CONJUNCTION spatial and channel attention. convolutional neural networks COMPARE RANSAC algorithm. RANSAC algorithm COMPARE convolutional neural networks. it USED-FOR UGALR. spatial and channel attention USED-FOR it. intermediate supervision USED-FOR it. RANSAC algorithm USED-FOR local feature matching. spatial and channel attention USED-FOR accurate and semantic local information. convolutional neural networks USED-FOR local feature matching. Oxford and Paris datasets EVALUATE-FOR approach. Task is Image retrieval. OtherScientificTerm is features. Method is local feature learning. Metric is memory consumption. ,This paper studies the problem of image retrieval with global features. The authors propose a re-ranking process to extract global features from the original images. They show that it improves the performance of local feature learning by using spatial and channel attention and intermediate supervision. The proposed approach is evaluated on Oxford and Paris datasets and shows that it outperforms the RANSAC algorithm for local feature matching with convolutional neural networks. ,This paper proposes a new method for learning global features for Image retrieval. The key idea is to use a re-ranking process to learn global features. The authors show that it improves the performance of UGALR over the RANSAC algorithm for local feature matching with spatial and channel attention and intermediate supervision. The proposed approach is evaluated on Oxford and Paris datasets. 
3743,SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"computer vision CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION computer vision. Multitask learning USED-FOR applications domains. computer vision HYPONYM-OF applications domains. reinforcement learning HYPONYM-OF applications domains. algorithm USED-FOR negative transfer. it USED-FOR gradient magnitudes. RotoGrad USED-FOR negative transfer. RotoGrad HYPONYM-OF algorithm. multi - label classification CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION multi - label classification. RotoGrad COMPARE methods. methods COMPARE RotoGrad. CelebA CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION CelebA. RotoGrad USED-FOR complex problems. methods USED-FOR complex problems. NYUv2 dataset USED-FOR computer vision tasks. CelebA USED-FOR multi - label classification. computer vision tasks HYPONYM-OF complex problems. multi - label classification HYPONYM-OF complex problems. OtherScientificTerm are shared network parameters, gradient magnitude, gradient directions, and training convergence. Method is Pytorch implementation. ","This paper proposes a new algorithm called RotoGrad for negative transfer, which is an extension of Pytorch implementation. The main idea is to use shared network parameters to learn the gradient magnitude of the gradient directions, and then apply it to the gradient magnitudes. Multitask learning is an important problem in applications domains such as computer vision, reinforcement learning, and multi-label classification. The authors show that RotoGr outperforms existing methods on a variety of complex problems such as CelebA and computer vision tasks on the NYUv2 dataset.","This paper proposes a new algorithm for negative transfer in applications domains such as computer vision, reinforcement learning, multi-label learning, and applications domains with shared network parameters. The proposed algorithm, RotoGrad, is based on Pytorch implementation. The authors show that it can be used to reduce the gradient magnitudes of gradient directions, which leads to better training convergence. Experiments are conducted on the NYUv2 dataset for a variety of computer vision tasks including multi-labels classification, CelebA, and several other complex problems. The results show that the proposed methods are better than other methods for complex problems such as multi-Label classification and computer vision."
3759,SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,soft neuron association USED-FOR pre - trained networks. soft neuron association USED-FOR Layer - wise model fusion. optimal transport USED-FOR Layer - wise model fusion. networks USED-FOR OTFusion. model fusion framework USED-FOR neural networks. CLAFusion HYPONYM-OF model fusion framework. cross - layer alignment USED-FOR model fusion framework. cross - layer alignment USED-FOR heterogeneous neural networks. unbalanced assignment problem USED-FOR cross - layer alignment problem. dynamic programming USED-FOR cross - layer alignment problem. cross - layer alignment USED-FOR framework. framework USED-FOR layer - wise model fusion. number of layers of neural networks USED-FOR framework. CLAFusion USED-FOR fused network. finetuning process USED-FOR it. finetuning process USED-FOR residual networks. residual networks EVALUATE-FOR it. CIFAR10 dataset EVALUATE-FOR residual networks. CIFAR10 dataset EVALUATE-FOR it. model compression CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION model compression. knowledge distillation USED-FOR teacher - student setting. Material is heterogeneous data. OtherScientificTerm is retraining. ,"This paper proposes a model fusion framework for pre-trained neural networks based on soft neuron association for Layer-wise model fusion with optimal transport. The proposed framework, CLAFusion, is based on the cross-layer alignment for heterogeneous neural networks with a number of layers of neural networks. The authors show that networks trained with OTFusion on heterogeneous data are able to achieve better performance than networks trained on homogeneous data. The framework is able to perform layer-wide model fusion using dynamic programming, and it is also able to learn residual networks with finetuning process. Experiments on the CIFAR10 dataset demonstrate the effectiveness of the proposed model compression and knowledge distillation in the teacher-student setting.",This paper proposes a model fusion framework for neural networks with soft neuron association for pre-trained networks. Layer-wise model fusion is based on optimal transport between two networks. The framework uses a number of layers of neural networks to solve the cross-layer alignment problem with dynamic programming. The fused network is trained with CLAFusion and the finetuning process is applied to the residual networks of the CIFAR10 dataset. Experiments are conducted in a teacher-student setting where the retraining is performed on heterogeneous data.
3775,SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"generalization EVALUATE-FOR deep networks. supervised learning USED-FOR deep networks. implicit regularization FEATURE-OF overparameterized deep networks. stochastic gradient descent USED-FOR implicit regularization. SGD USED-FOR supervised learning. SGD USED-FOR implicit regularization. regularizer USED-FOR degenerate solutions. implicit regularization USED-FOR temporal difference learning. regularizer COMPARE supervised learning case. supervised learning case COMPARE regularizer. representations USED-FOR state - action pairs. bootstrapping USED-FOR deep network value function. deep network value function USED-FOR feature representations. bootstrapping USED-FOR feature representations. explicit regularizer USED-FOR implicit regularizer. DR3 HYPONYM-OF explicit regularizer. D4RL domains CONJUNCTION robotic manipulation. robotic manipulation CONJUNCTION D4RL domains. Atari 2600 games CONJUNCTION D4RL domains. D4RL domains CONJUNCTION Atari 2600 games. DR3 USED-FOR unlearning. DR3 USED-FOR robotic manipulation. performance CONJUNCTION stability. stability CONJUNCTION performance. unlearning CONJUNCTION D4RL domains. D4RL domains CONJUNCTION unlearning. offline RL methods COMPARE DR3. DR3 COMPARE offline RL methods. unlearning CONJUNCTION robotic manipulation. robotic manipulation CONJUNCTION unlearning. Atari 2600 games FEATURE-OF unlearning. stability EVALUATE-FOR DR3. images USED-FOR robotic manipulation. performance EVALUATE-FOR DR3. Method are overparameterization, and deep reinforcement learning ( RL ) methods. OtherScientificTerm are parsimonious solutions, degenerate feature representations, and Bellman backup. Task is offline deep RL setting. ","This paper studies the implicit regularization of overparameterized deep networks in the context of supervised learning in the offline deep RL setting. The authors propose DR3, a new implicit regularizer based on stochastic gradient descent (SGD) for supervised learning. DR3 is based on the explicit regularizer, DR3 HYPONYM, which is a special case of SGD in the supervised learning setting.  The authors show that DR3 improves the performance and stability of DR3 in unlearning, robotic manipulation, D4RL domains, and Atari 2600 games.   ","This paper studies the implicit regularization of overparameterized deep networks with stochastic gradient descent. The authors show that this regularizer can be used to improve the generalization of deep networks in the offline deep RL setting. They also show that the proposed SGD is equivalent to supervised learning for deep networks, and that the explicit regularizer, DR3, is similar to SGD for supervised learning. The main difference between the two regularizers is that DR3 uses bootstrapping to learn the deep network value function for the feature representations, while SGD uses Bellman backup for the degenerate feature representations.  The authors also provide a theoretical analysis of the proposed implicit regularizer for temporal difference learning, showing that the regularizer is better than the supervised learning case. Finally, the authors provide an empirical evaluation of the performance of DR3 in unlearning, robotic manipulation, D4RL domains, and Atari 2600 games.  "
3791,SP:6fd793b27123bf80504e2ad5957455b7ec311612,RLSVI USED-FOR posterior samples. algorithm USED-FOR deep RL. HyperDQN HYPONYM-OF algorithm. non - linear neural network USED-FOR Q - values. base model HYPONYM-OF non - linear neural network. meta model HYPONYM-OF probabilistic hypermodel. probabilistic hypermodel USED-FOR method. hypermodel USED-FOR approximate posterior samples. Q - value functions USED-FOR exploratory action sequences. RLSVI USED-FOR exploration. posterior samples USED-FOR Q - value function. Atari suite EVALUATE-FOR HyperDQN. Atari suite EVALUATE-FOR DQN. HyperDQN COMPARE DQN. DQN COMPARE HyperDQN. maximum human - normalized score EVALUATE-FOR DQN. maximum human - normalized score EVALUATE-FOR HyperDQN. HyperDQN COMPARE exploration bonus and randomized exploration methods. exploration bonus and randomized exploration methods COMPARE HyperDQN. HyperDQN USED-FOR SuperMarioBros. exploration bonus and randomized exploration methods USED-FOR SuperMarioBros. Method is exploration method. OtherScientificTerm is feature. Generic is models. Metric is efficiency. ,"This paper proposes HyperDQN, a new algorithm for deep RL based on RLSVI for obtaining posterior samples from a non-linear neural network trained with Q-values. The proposed method uses a probabilistic hypermodel, a meta model, which is a combination of the base model and the probabilistically hypermodel. The hypermodel is used to generate approximate posterior samples for the Q-value functions for the exploratory action sequences. The posterior samples are then used for exploration. The exploration method is evaluated on SuperMarioBros in the Atari suite and shows that the proposed method outperforms DQN with a maximum human-normalized score, and outperforms exploration bonus and randomized exploration methods.","This paper proposes a new algorithm for deep RL, HyperDQN, which is a probabilistic hypermodel with a non-linear neural network that predicts Q-values from a base model and a meta model. The main idea is to use RLSVI to generate posterior samples from a set of Q-value functions for exploratory action sequences. The authors show that the proposed exploration method outperforms DQN on the Atari suite and achieves the maximum human-normalized score. HyperDQLN outperforms exploration bonus and randomized exploration methods on SuperMarioBros. "
3807,SP:b428383660928374c953f659ea1e05852dbdcd6e,"representation learning USED-FOR model. image classification CONJUNCTION recommender systems. recommender systems CONJUNCTION image classification. representation learning USED-FOR downstream tasks. downstream tasks EVALUATE-FOR model. recommender systems HYPONYM-OF real - world scenarios. image classification HYPONYM-OF real - world scenarios. cause, effect and spurious correlated variables PART-OF representation. hypothetical causal graph USED-FOR mutual information measures. mutual information measures USED-FOR learning procedure. learning procedure USED-FOR causal representation. hypothetical causal graph USED-FOR learning procedure. observational data USED-FOR causal representation. reduced sample complexity CONJUNCTION generalization ability. generalization ability CONJUNCTION reduced sample complexity. counterfactual loss PART-OF optimization. reduced sample complexity EVALUATE-FOR causality - inspired learning. generalization ability EVALUATE-FOR causality - inspired learning. adversarial attacks CONJUNCTION distribution shift. distribution shift CONJUNCTION adversarial attacks. causal representations USED-FOR models. adversarial attacks USED-FOR models. approach USED-FOR causal representations. approach USED-FOR models. Method is learning approaches. OtherScientificTerm is features. Metric is generalizability. ","This paper proposes a new approach to learn causal representations from observational data. The authors propose a new learning procedure based on a hypothetical causal graph to learn mutual information measures between the cause, effect and spurious correlated variables in the representation. The proposed model is evaluated on a variety of downstream tasks, including image classification and recommender systems. The results show that the proposed approach improves the generalization ability and reduced sample complexity of causality-inspired learning with a counterfactual loss in optimization. ","This paper proposes a new approach to learn causal representations for learning approaches. The model is trained using representation learning for downstream tasks such as image classification, recommender systems, and real-world scenarios. The proposed learning procedure is based on a learning procedure that learns a causal representation from observational data, which is composed of a hypothetical causal graph and mutual information measures. The representation is constructed from the cause, effect and spurious correlated variables. The authors show that the proposed approach can improve the generalizability of models in the presence of adversarial attacks, distribution shift, and distribution shift. They also show the reduced sample complexity and generalization ability of causality-inspired learning with a counterfactual loss in optimization."
3823,SP:1258c05a80a17949b50e6dae13deea1d2235f456,Federated learning HYPONYM-OF distributed learning scheme. edge devices USED-FOR model. training USED-FOR edge devices. gradient compression CONJUNCTION distillation. distillation CONJUNCTION gradient compression. gradient compression HYPONYM-OF compact formats. distillation HYPONYM-OF compact formats. progressive training framework USED-FOR federated learning. ProgFed HYPONYM-OF progressive training framework. It COMPARE models. models COMPARE It. asymptotic rate EVALUATE-FOR ProgFed. ResNet CONJUNCTION ConvNets. ConvNets CONJUNCTION ResNet. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. ConvNets CONJUNCTION U - nets. U - nets CONJUNCTION ConvNets. computation CONJUNCTION communication costs. communication costs CONJUNCTION computation. simple classification CONJUNCTION medical image segmentation. medical image segmentation CONJUNCTION simple classification. communication costs EVALUATE-FOR converged models. VGG CONJUNCTION ConvNets. ConvNets CONJUNCTION VGG. training approach USED-FOR converged models. tasks EVALUATE-FOR training approach. tasks HYPONYM-OF architectures. medical image segmentation HYPONYM-OF tasks. simple classification HYPONYM-OF tasks. computation EVALUATE-FOR training approach. communication costs EVALUATE-FOR training approach. VGG HYPONYM-OF architectures. ConvNets HYPONYM-OF architectures. ResNet HYPONYM-OF architectures. U - nets HYPONYM-OF architectures. approach COMPARE compression. compression COMPARE approach. OtherScientificTerm is limited network bandwidth. Generic is full models. ,"This paper proposes a federated learning scheme called ProgFed, which is a distributed learning scheme that uses edge devices to train a model. The authors propose a progressive training framework called ProGFed, where each edge device is trained on a limited network bandwidth, and the model is trained using a combination of gradient compression and distillation. The asymptotic rate of Progfed is shown to be a good trade-off between the computation and communication costs of the proposed models. The proposed training approach is evaluated on a variety of tasks, including simple classification, medical image segmentation, and convolutional neural networks. The experiments show that the proposed approach performs better than compression and other existing architectures such as ResNet, ConvNets, VGG, and U-net.","This paper proposes a federated learning scheme, which is a distributed learning scheme. The authors propose a progressive training framework, ProgFed, to train a model on edge devices. It is based on the idea that the training of edge devices can be done on a limited network bandwidth, and that the model can be trained using only a small number of edges. The paper shows that Progfed achieves a asymptotic rate that is much faster than the full models. It outperforms other models in terms of computation, communication costs, and compact formats such as gradient compression and distillation. The proposed training approach is evaluated on a variety of tasks, including simple classification, medical image segmentation, ConvNets, ResNet, VGG, and U-nets. The results show that the proposed approach outperforms compression and other converged models."
3839,SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"adversarial attacks FEATURE-OF Deep neural networks. Adversarial training USED-FOR model. adversarial Rademacher complexity FEATURE-OF adversarial training. two - layer neural networks USED-FOR adversarial Rademacher complexity. adversarial Rademacher complexity FEATURE-OF deep neural networks. Rademacher complexity EVALUATE-FOR neural nets. product of weight norms PART-OF bound. adversarially trained weight norms COMPARE trained weight norms. trained weight norms COMPARE adversarially trained weight norms. Generic are models, and method. Task is adversarial settings. OtherScientificTerm is layer. ","This paper studies the problem of adversarial attacks on Deep neural networks. Adversarial training is an important problem in deep neural networks, as it can be used to improve the performance of the model. The authors propose a new method to reduce the adversarial Rademacher complexity of deep neural nets by using two-layer neural networks with a single layer. The proposed method is based on the product of weight norms, and the authors show that the proposed method can achieve better performance than adversarially trained weight norms.",This paper studies adversarial attacks on Deep neural networks. Adversarial training is used to train a model. The authors show that adversarial Rademacher complexity of adversarial training on two-layer neural networks is a measure of the adversarial robustness of the model in adversarial settings. The main contribution of the paper is a new bound on the product of weight norms of the two layers of the network. The paper shows that adversarially trained weight norms are more robust to adversarial attack than trained weight norm. The method is evaluated on a variety of datasets.
3855,SP:925d6bb051e9b384669fb695085b678c11f7c11a,Estimation of ( differential ) entropy CONJUNCTION mutual information. mutual information CONJUNCTION Estimation of ( differential ) entropy. estimators USED-FOR differential entropy. approach USED-FOR KNIFE - based estimators. KNIFE - based estimators USED-FOR mutual information. neural networks USED-FOR real - world tasks. it USED-FOR neural networks. high - dimensional synthetic data EVALUATE-FOR method. visual domain adaptation CONJUNCTION textual fair classification. textual fair classification CONJUNCTION visual domain adaptation. textual fair classification CONJUNCTION textual fine - tuning. textual fine - tuning CONJUNCTION textual fair classification. tasks EVALUATE-FOR KNIFE - based estimation. textual fine - tuning EVALUATE-FOR KNIFE - based estimation. textual fine - tuning HYPONYM-OF tasks. visual domain adaptation HYPONYM-OF tasks. textual fair classification HYPONYM-OF tasks. Method is KNIFE. ,"This paper proposes a novel approach to improve the performance of KNIFE-based estimators for differential entropy and mutual information in real-world tasks. The proposed method is based on high-dimensional synthetic data, and it can be used to train neural networks to perform well on a variety of tasks such as visual domain adaptation, textual fine-tuning, and textual fair classification. Experiments show the effectiveness of the proposed method.","This paper proposes a novel approach to improve the performance of existing KNIFE-based estimators for differential entropy and mutual information. The method is based on high-dimensional synthetic data, and it can be applied to neural networks for real-world tasks such as visual domain adaptation, textual fine-tuning, and textual fair classification. Experiments show that the proposed method improves the performance on all three tasks. "
3871,SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"ε - greedy CONJUNCTION softmax. softmax CONJUNCTION ε - greedy. action - value methods USED-FOR reinforcement learning. Soft - greedy operators USED-FOR exploration. exploration USED-FOR action - value methods. ε - greedy HYPONYM-OF Soft - greedy operators. softmax HYPONYM-OF Soft - greedy operators. resmax HYPONYM-OF soft - greedy operator. It USED-FOR coverage of the state - space. It USED-FOR exploration. it COMPARE softmax. softmax COMPARE it. non - expansion USED-FOR it. exploration hyperparameter USED-FOR non - expansion. mellowmax HYPONYM-OF non - expansion. state - action specific temperature USED-FOR softmax policy. resmax COMPARE softmax. softmax COMPARE resmax. resmax COMPARE ε - greedy. ε - greedy COMPARE resmax. ε - greedy CONJUNCTION softmax. softmax CONJUNCTION ε - greedy. Generic is operators. OtherScientificTerm are suboptimality gap, and overemphasizing sub - optimal actions. Task is learning. Material is tabular and deep RL. ","This paper studies the suboptimality gap in reinforcement learning in tabular and deep RL. The authors propose Soft-greedy operators for exploration in action-value methods for reinforcement learning, such as   ε - greedy, softmax, and resmax. The main contribution of the paper is to show that these operators are suboptimal in practice.    The authors then propose a non-expansion of the exploration hyperparameter in softmax to allow it to be used for non-explanation.  The softmax policy is based on the state-action specific temperature of the softmax operator. It is shown that it is able to achieve better coverage of the states-space than resmax and softmax. ","This paper studies the suboptimality gap in reinforcement learning in tabular and deep RL. The authors propose two operators, ε-greedy and softmax, for exploration in action-value methods for reinforcement learning. Softmax is a variant of the popular resmax, while softmax is an extension of resmax. It is used for coverage of the state-space, and non-expansion is used as an exploration hyperparameter. The softmax policy is based on a state-action specific temperature, and the authors show that it outperforms resmax in terms of exploration. "
3887,SP:792ae8808aa6902758146aef1548c975492b833c,"learnability FEATURE-OF deep learning models. concept USED-FOR model. concept USED-FOR learnability. learnability lock USED-FOR model. learnability lock USED-FOR learnability. learnability EVALUATE-FOR model. learnability FEATURE-OF dataset. universal transformation function USED-FOR class - wise perturbation. class - wise perturbation USED-FOR learnability lock. inverse transformation USED-FOR learnability. visual classification tasks EVALUATE-FOR method. Task are information technology, learnability attack, and preventing unauthorized exploitation. Method are deep learning, commercial models, adversarial invertible transformation, and machine learning models. OtherScientificTerm are digital formats, and visual features. Material is image. Generic is models. ","This paper studies the problem of how to improve the performance of deep learning models in the presence of adversarial invertible transformation. The authors propose a new concept called ""learnability lock"" that can improve the learnability of a model using a learnability lock. The key idea is to use a universal transformation function to learn the class-wise perturbation of the model to improve its learnability on the dataset. The proposed method is evaluated on a variety of visual classification tasks.",This paper proposes a novel approach to improve the learnability of deep learning models. The key idea is to use a concept called learnability lock to enforce the model’s learnability on the dataset. The proposed method is based on adversarial invertible transformation. The authors propose a universal transformation function to enforce class-wise perturbation on the learned representations of the image. They show that the proposed method can improve the learningability of the model on visual classification tasks.
3903,SP:9af10703605e620e563241e2602a50b629f3d37a,"Graph Neural Networks ( GNNs ) USED-FOR modeling relational data. node or edge features FEATURE-OF graph. features PART-OF real - world applications. approach USED-FOR missing features. approach USED-FOR graph machine learning applications. approach USED-FOR diffusion - type differential equation. graph FEATURE-OF diffusion - type differential equation. minimization of the Dirichlet energy USED-FOR approach. Feature Propagation HYPONYM-OF algorithm. approach COMPARE methods. methods COMPARE approach. missing features EVALUATE-FOR approach. common node - classification benchmarks EVALUATE-FOR approach. common node - classification benchmarks EVALUATE-FOR methods. nodes CONJUNCTION edges. edges CONJUNCTION nodes. edges PART-OF GPU. nodes FEATURE-OF graph. edges PART-OF graph. Generic are they, and equation. Material is social networks. ","This paper proposes a new approach to learn missing features in graph neural networks (GNNs) for modeling relational data. The proposed approach is based on the minimization of the Dirichlet energy of the diffusion-type differential equation between the graph and the edges of the graph. The authors show that the proposed algorithm, Feature Propagation, can be applied to real-world applications where features in the graph are not available in social networks. They also show that their approach can be used for graph machine learning applications where missing features can be useful. The experimental results show that this approach can achieve better performance than existing methods on common node-classification benchmarks.","This paper proposes a novel approach for learning missing features in graph machine learning applications. The key idea is to use Graph Neural Networks (GNNs) for modeling relational data with node or edge features in real-world applications. Specifically, the authors propose an algorithm called Feature Propagation, which is based on the minimization of the Dirichlet energy. The authors show that the proposed approach can be used to solve a diffusion-type differential equation over the graph of a graph with nodes and edges in the GPU. They also show that they can be applied to social networks. The proposed approach is evaluated on several common node-classification benchmarks and compared to other methods."
3919,SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"limited labeled data USED-FOR model. heuristics USED-FOR sample selection strategies. integer optimization problem USED-FOR core set. discrete Wasserstein distance FEATURE-OF unlabeled pool. Generalized Benders Decomposition algorithm USED-FOR problem. unlabeled pool USED-FOR unsupervised learning. unsupervised learning USED-FOR latent features. latent features USED-FOR strategy. optimization approach COMPARE baselines. baselines COMPARE optimization approach. data sets EVALUATE-FOR optimization approach. optimization approach COMPARE them. them COMPARE optimization approach. data sets EVALUATE-FOR baselines. them USED-FOR low budget regime. optimization approach USED-FOR low budget regime. Method are Active learning, and deep learning. OtherScientificTerm is unlabeled data pool. ","This paper studies the problem of unsupervised learning with limited labeled data. Active learning is an important problem in the context of deep learning, where the goal is to learn a model that is able to learn from limited labelled data. The authors propose a novel heuristics for sample selection strategies based on the fact that the core set of the unlabeled data pool has a discrete Wasserstein distance. The core set is an integer optimization problem, and the problem is formulated as a Generalized Benders Decomposition algorithm. The main contribution of the paper is to propose a new strategy based on latent features learned by unstructured learning. The optimization approach is shown to outperform existing baselines on three different data sets, and outperforms them in the low budget regime.","This paper proposes a new model for unsupervised learning on limited labeled data. Active learning is an important problem in deep learning. The authors propose to use heuristics for sample selection strategies. The core set of the unlabeled data pool is a discrete Wasserstein distance. The problem is formulated as an integer optimization problem for the core set, and the authors propose a Generalized Benders Decomposition algorithm to solve the problem. The key idea is to use latent features learned by the proposed strategy to learn the latent features. The proposed optimization approach is evaluated on three different data sets and compared to baselines. The optimization approach outperforms them in the low budget regime. "
3935,SP:4c72923f78ca6590dc11e10d1a2403076a583718,"manual inspection USED-FOR genome reconstruction. approach USED-FOR assembling genomes. method USED-FOR approach. method USED-FOR assembling genomes. geometric deep learning USED-FOR genome assembly. geometric deep learning USED-FOR assembly graph. genomic sequence USED-FOR assembly graph. graph convolutional network USED-FOR genome. dataset USED-FOR graph convolutional network. human genomic data USED-FOR graph convolutional network. human genomic data USED-FOR dataset. greedy search algorithm COMPARE greedy search. greedy search COMPARE greedy search algorithm. greedy search algorithm USED-FOR graph topology. greedy search algorithm USED-FOR model. graph machine learning algorithms USED-FOR de novo genome assembly problem. graph machine learning algorithms COMPARE human handcrafted techniques. human handcrafted techniques COMPARE graph machine learning algorithms. Material is human DNA sequence. OtherScientificTerm are telomere, and graph. Metric is assembly speed. Generic is it. Method is de novo assemblers. ",This paper proposes a method for assembling genomes using geometric deep learning. The proposed approach is based on the idea of manual inspection for genome reconstruction. The assembly graph is composed of a genomic sequence and a graph convolutional network is used to map the genome to the telomere. The dataset is generated from human genomic data. The model is trained using a greedy search algorithm to find the best graph topology. The authors show that the proposed graph machine learning algorithms can solve the de novo genome assembly problem faster than human handcrafted techniques. ,"This paper proposes a new approach for assembling genomes. The approach is based on geometric deep learning for genome assembly. The authors propose a graph convolutional network to assemble a genome from a human DNA sequence. The model is trained using a greedy search algorithm for graph topology, and it is shown that the proposed de novo assemblers outperform human handcrafted techniques. The paper also presents a dataset of human genomic data."
3951,SP:24de906e4289c9073b6c55c747b0913b8df5e053,"catastrophic forgetting FEATURE-OF Continual learning. meta - learning USED-FOR metacontinual learning algorithms. experience replay ( ER ) PART-OF meta - testing. ER USED-FOR meta - testing. ER USED-FOR metatraining. ER USED-FOR continual learning representations. ER PART-OF meta - training. reservoir sampling USED-FOR replay buffer. meta - learned Predictive Sample Selection USED-FOR replay buffer. meta - learned Predictive Sample Selection COMPARE reservoir sampling. reservoir sampling COMPARE meta - learned Predictive Sample Selection. method COMPARE state - of - the - art. state - of - the - art COMPARE method. clustering structures FEATURE-OF learned representations. Method are online aware meta - learning ( OML ), and OML. Generic is model. Task is online - aware nature of OML. OtherScientificTerm is randomness. ","This paper studies the problem of catastrophic forgetting in Continual learning with meta-learning. The authors propose a new meta-testing method based on experience replay (ER) to improve the performance of metacontinual learning algorithms. The ER is used in meta-training for continual learning representations, and the authors show that ER can be used for meta-test in the context of metatraining. The paper also shows that reservoir sampling can improve the replay buffer in the case of meta-learned Predictive Sample Selection, and that the proposed method can achieve better performance than state-of-the-art in the online-aware nature of OML.","This paper proposes an online aware meta-learning (OML) framework for continual learning. The authors propose to use experience replay (ER) in meta-testing for metacontinual learning algorithms. The ER is used for meta-training in the context of continual learning representations, where the model is trained on a large number of tasks. The paper shows that the ER can be used for metatraining, and the authors show that the proposed method outperforms state-of-the-art in terms of randomness and clustering structures. In addition, the authors propose a new replay buffer based on meta-learned Predictive Sample Selection, which is more robust to catastrophic forgetting in Continual learning. "
3967,SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"Multi - agent joint Q - learning USED-FOR multi - agent cooperation. Centralized Training with Decentralized Execution ( CTDE ) USED-FOR Multi - agent joint Q - learning. methods USED-FOR multi - agent credit assignment problem. Bellman optimality equation FEATURE-OF joint Q - value. Bellman optimality FEATURE-OF joint Q - value. Q - values USED-FOR joint Q - value. gradient ascent solution USED-FOR problem. ECAQ COMPARE baselines. baselines COMPARE ECAQ. ECAQ USED-FOR credit assignment. Method are centralized training, and deep neural networks. Task are explicit credit assignment problem, and multi - agent cooperation in complex problems. OtherScientificTerm is time horizon. ","This paper studies the problem of multi-agent joint Q-learning with Decentralized Execution (CTDE) in the context of explicit credit assignment problem. The authors propose two methods to solve the multi-adversarial credit assignment in complex problems. The first method, called ECAQ, is based on centralized training, where each agent is given a set of Q-values and the goal is to maximize the Bellman optimality equation of the joint Q -value of the Q-value. The second method, CTDE, uses gradient ascent solution to solve this problem. Experiments show that the proposed method outperforms the baselines in credit assignment. ","This paper proposes a new method for multi-agent joint Q-learning for multi -agent cooperation. The authors propose Centralized Training with Decentralized Execution (CTDE) for Multi-agent Joint Q-Learning, which is based on centralized training with deep neural networks. The main idea of CTDE is to solve the explicit credit assignment problem, where each agent is given a set of Q-values and the goal is to maximize the joint Q -value of the Q-value over the Bellman optimality equation. The paper presents a gradient ascent solution for the problem, and shows that the proposed ECAQ can be used for credit assignment in complex problems. "
3983,SP:0d2b225ac697679d10df25f371b2a718d4949b42,"transductive learning USED-FOR adversarial robustness. defenses COMPARE defense mechanisms. defense mechanisms COMPARE defenses. defenses USED-FOR bilevel optimization problem. test - time input USED-FOR model. threat analysis perspective EVALUATE-FOR defense mechanisms. threat models USED-FOR transductive - learning based defenses. attacking model space USED-FOR bilevel attack objectives. Greedy Model Space Attack ( GMSA ) HYPONYM-OF attack framework. GMSA USED-FOR transductive - learning based defenses. weak instantiations USED-FOR GMSA. Material is NeurIPS 2020. Task are ICML 2020, and adaptive attacks. Method are transductivelearning based defenses, and transductive adversarial training. OtherScientificTerm is AutoAttack. Metric is robustness. ","This paper studies the problem of adversarial robustness in the context of NeurIPS 2020. The authors propose a new attack framework called Greedy Model Space Attack (GMSA), which attacks the model space of a transductive adversarial model. GMSA is based on weak instantiations of the test-time input of the model, which is used to train the model. They show that GMSA can achieve better robustness than other defense mechanisms in terms of the threat analysis perspective. ",This paper proposes a novel attack framework called Greedy Model Space Attack (GMSA) to improve adversarial robustness against adversarial attacks. GMSA is based on the idea of adversarial training with adversarial perturbations. The authors show that GMSA can be used to improve the robustness of a model to adversarial attack. They also show that the proposed method is robust to adaptive attacks. 
3999,SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"batch normalization USED-FOR training of neural networks. batch renormalization USED-FOR small minibatches. function class FEATURE-OF inference model. Method are neural networks, and per - example training procedure. OtherScientificTerm are gradient, and identity shortcuts. Generic are approximation, and normalization. Metric are training step computation, and model accuracy. ","This paper studies the problem of batch normalization in the training of neural networks. The authors propose a new per-example training procedure where the gradient of the training step computation is approximated by a function class of the inference model. They show that the approximation can be improved by using identity shortcuts. They also show that batch renormalization can be used for small minibatches. Finally, they show that normalization can improve model accuracy.","This paper proposes batch normalization for the training of neural networks. The authors propose a per-example training procedure, where the gradient of the model is approximated using identity shortcuts, and the training step computation is performed using batch renormalization for small minibatches. The inference model is trained on the function class, and normalization is used to improve the model accuracy."
4015,SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,large - scale pretraining CONJUNCTION adaptation. adaptation CONJUNCTION large - scale pretraining. general domain data USED-FOR large - scale pretraining. large - scale pretraining PART-OF natural language processing. fine - tuning USED-FOR models. trainable parameters USED-FOR downstream tasks. trainable rank decomposition matrices PART-OF Transformer architecture. LoRA HYPONYM-OF Low - Rank Adaptation. GPT-3 175B COMPARE LoRA. LoRA COMPARE GPT-3 175B. LoRA USED-FOR trainable parameters. GPU memory requirement EVALUATE-FOR LoRA. Adam USED-FOR GPT-3 175B. LoRA COMPARE finetuning. finetuning COMPARE LoRA. GPT-2 CONJUNCTION GPT-3. GPT-3 CONJUNCTION GPT-2. RoBERTa EVALUATE-FOR finetuning. model quality EVALUATE-FOR finetuning. model quality EVALUATE-FOR LoRA. Method is fine - tuned models. Metric is training throughput. OtherScientificTerm is inference latency. Task is language model adaptation. ,"This paper proposes LoRA, a Low-Rank Adaptation (LRA) method for language model adaptation. LoRA uses a Transformer architecture with trainable rank decomposition matrices to learn the trainable parameters for downstream tasks. The authors show that LoRA achieves better performance than GPT-3 175B on RoBERTa with a small GPU memory requirement. ","This paper proposes LoRA, a Low-Rank Adaptation (LRA) method for language model adaptation. The authors propose a Transformer architecture with trainable rank decomposition matrices, which allows for large-scale pretraining and adaptation on general domain data. They show that fine-tuning of models with such trainable parameters leads to better performance on downstream tasks. They also show that LoRA achieves better model quality on RoBERTa compared to GPT-3 175B with the same GPU memory requirement. "
4031,SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"CRFs USED-FOR distributions with nonlocal dependencies. nonlocal constraints FEATURE-OF CRFs. global arity constraints HYPONYM-OF nonlocal constraints. CRFs USED-FOR constraints. nonlocal ones HYPONYM-OF constraints. regular - constrained CRF ( RegCCRF ) COMPARE CRF. CRF COMPARE regular - constrained CRF ( RegCCRF ). RegCCRFs COMPARE models. models COMPARE RegCCRFs. constraints PART-OF training. constraints FEATURE-OF decoding. constraints PART-OF RegCCRFs. constrained training COMPARE constrained decoding. constrained decoding COMPARE constrained training. deep neural model USED-FOR semantic role labeling. RegCCRF PART-OF deep neural model. RegCCRF USED-FOR semantic role labeling. dataset EVALUATE-FOR RegCCRF. RegCCRF USED-FOR downstream tasks. Task is structured prediction. OtherScientificTerm are local dependencies, CRF ’s Markov assumption, and output structures. Generic is it. ","This paper studies the problem of structured prediction with non-local constraints in CRFs. The authors show that CRFs with global arity constraints (i.e., nonlocal ones) are more robust to distributions with nonlocal dependencies than regular-constrained CRF (RegCCRF). The authors also show that models trained with these constraints perform better than RegCCRFs in terms of performance on downstream tasks.   The authors propose a deep neural model for semantic role labeling, where the constraints in the decoding are incorporated into the constraints of the training. They show that the constraints can be used to improve the performance of constrained decoding, and that it can be combined with the constraints from the deep neural network. They also provide a theoretical analysis of the CRF’s Markov assumption, and show that it holds for the output structures. ","This paper proposes a new regular-constrained CRF (RegCCRF) for distributions with nonlocal dependencies. The authors show that CRFs with global arity constraints are equivalent to nonlocal constraints on the local dependencies. They also show that models with global constraints are more robust to local dependencies than models with CRF’s Markov assumption. RegCCRF is evaluated on a dataset of structured prediction, where it is shown to outperform other models in terms of robustness to local constraints. The paper also shows that the constraints in the decoding and in the constraints of the constraints for the downstream tasks are similar to those in constrained training, and that constrained decoding outperforms constrained training on the semantic role labeling using a deep neural model. "
4047,SP:74c186a96c12adff178264aa84ace8d04dc7d725,"preprocessing steps USED-FOR methods. computational budget EVALUATE-FOR core ” network. normalization CONJUNCTION color space transformation. color space transformation CONJUNCTION normalization. segmentation CONJUNCTION normalization. normalization CONJUNCTION segmentation. face detection CONJUNCTION segmentation. segmentation CONJUNCTION face detection. neural models USED-FOR camera - based physiological measurement. color space transformation CONJUNCTION preprocessing steps. preprocessing steps CONJUNCTION color space transformation. face detection USED-FOR neural models. segmentation HYPONYM-OF neural models. normalization HYPONYM-OF neural models. preprocessing steps PART-OF neural models. EfficientPhys HYPONYM-OF neural models. raw video frames USED-FOR models. accuracy EVALUATE-FOR models. latency EVALUATE-FOR networks. efficiency EVALUATE-FOR light weight network. Task are Camera - based physiological measurement, and replication. Method are endto - end ” models, and transformer or convolutional backbone. Generic is operations. ","This paper studies the problem of camera-based physiological measurement. The authors propose two methods: 1) “endto-end” models, which are based on transformer or convolutional backbone, and 2) a “core” network with a computational budget of $O(\sqrt{T})$ where $T$ is the number of images. The main contribution of the paper is to propose two neural models, EfficientPhys, which combines preprocessing steps in neural models such as segmentation, normalization, color space transformation, and face detection. Experimental results show that the proposed models achieve better accuracy and lower latency on raw video frames.","This paper proposes a new method for camera-based physiological measurement, called EfficientPhys, which is based on the idea of “endto-end” models. The authors propose two methods: (1) a “core” network with a computational budget of $O(\sqrt{n})$ and (2) a transformer or convolutional backbone with $O(n)$ computational budget. The core network is trained on raw video frames, and the “main” networks are trained on a set of preprocessing steps (e.g., color space transformation, normalization, segmentation, and face detection). The authors show that the networks are able to achieve better accuracy and reduce the latency of the light weight network. They also show that their models can be used to improve the accuracy of the models. "
4063,SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"Structural pruning USED-FOR network architecture. inference speed EVALUATE-FOR Structural pruning. Hardware - Aware Latency Pruning ( HALP ) USED-FOR structural pruning. global resource allocation optimization problem USED-FOR structural pruning. latency reduction potential CONJUNCTION global saliency score. global saliency score CONJUNCTION latency reduction potential. latency lookup table USED-FOR latency reduction potential. latency lookup table USED-FOR global saliency score. HALP USED-FOR accuracy drop. HALP USED-FOR global saliency score. HALP USED-FOR latency reduction potential. latency lookup table USED-FOR HALP. HALP USED-FOR filter importance ranking. latency lookup table USED-FOR filter importance ranking. metrics EVALUATE-FOR pruning. metrics USED-FOR global structural pruning. reward maximization problem USED-FOR global structural pruning. pruning efficacy CONJUNCTION accuracy - efficiency trade - off. accuracy - efficiency trade - off CONJUNCTION pruning efficacy. pruning efficacy EVALUATE-FOR HALP. accuracy - efficiency trade - off EVALUATE-FOR HALP. augmented knapsack solver USED-FOR problem. HALP USED-FOR classification and detection tasks. ImageNet and VOC datasets USED-FOR classification and detection tasks. ImageNet and VOC datasets EVALUATE-FOR HALP. ImageNet USED-FOR ResNet-50/-101 pruning. HALP USED-FOR ResNet-50/-101 pruning. network throughput EVALUATE-FOR HALP. HALP USED-FOR SSD pruning. VOC EVALUATE-FOR SSD pruning. throughput EVALUATE-FOR HALP. HALP COMPARE prior art. prior art COMPARE HALP. Metric is accuracy. OtherScientificTerm are latency, and top-1 accuracy changes. ","This paper proposes Hardware-Aware Latency Pruning (HALP) for structural pruning of a network architecture. Structural pruning is a global resource allocation optimization problem in which the goal is to reduce the latency of the network architecture and improve the inference speed. The problem is formulated as an augmented knapsack solver, where the top-1 accuracy changes are computed as a reward maximization problem. The paper proposes two metrics to evaluate the performance of the proposed pruning: latency reduction potential and global saliency score. The latency lookup table is used to compute the latency reduction of the filter importance ranking and the global salency score, and HALP is also used to estimate the accuracy drop. The performance of HALP on ImageNet and VOC datasets for classification and detection tasks is compared with prior art on ResNet-50/-101 pruning and SSD pruning on VOC. The pruning efficacy and accuracy-efficiency trade-off between HALP and prior art are also compared.","This paper proposes Hardware-Aware Latency Pruning (HALP), a method for pruning the network architecture. Structural pruning improves the inference speed of a network architecture by reducing the latency. The paper proposes a global resource allocation optimization problem for structural pruning. The problem is formulated as an augmented knapsack solver, where the top-1 accuracy changes are computed as a function of the latency, and top-2 accuracy changes can be computed as the accuracy drop. The proposed method is evaluated on ImageNet, ResNet-50/-101 pruning, and SSD pruning on both ImageNet and VOC datasets for classification and detection tasks. The authors show that HALP outperforms prior art in terms of pruning efficacy and accuracy-efficiency trade-off. "
4079,SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"permutation invariance CONJUNCTION multi - objective generation. multi - objective generation CONJUNCTION permutation invariance. GraphEBM HYPONYM-OF molecular graph generation method. GraphEBM USED-FOR permutation invariant and multi - objective molecule generation. energy - based models ( EBMs ) USED-FOR molecular graph generation method. GraphEBM USED-FOR permutation invariant distribution. EBMs CONJUNCTION parameterized permutation - invariant energy function. parameterized permutation - invariant energy function CONJUNCTION EBMs. parameterized permutation - invariant energy function USED-FOR GraphEBM. molecular graphs FEATURE-OF permutation invariant distribution. contrastive divergence USED-FOR energy function. compositional generation USED-FOR drug discovery. compositional generation USED-FOR GraphEBM. Task is molecular graph generation. Method are Langevin dynamics, and learning strategy. OtherScientificTerm is flexible degrees. Generic is method. ","This paper proposes GraphEBM, a molecular graph generation method based on energy-based models (EBMs) that can be used for both permutation invariance and multi-objective generation. The authors propose a learning strategy based on Langevin dynamics, where the energy function is defined as a contrastive divergence between the permutation and the objective function. The graphEBM is trained using compositional generation to generate molecular graphs that are invariant to permutation changes in the molecular graphs. The proposed method is evaluated on a variety of molecular graph discovery tasks.","This paper proposes a molecular graph generation method called GraphEBM, which combines permutation invariance and multi-objective molecule generation with energy-based models (EBMs). The authors propose a learning strategy based on Langevin dynamics. The authors also propose a parameterized permutation-invariant energy function based on contrastive divergence. The proposed method is evaluated on two molecular graphs, and is shown to be able to generate molecules with flexible degrees. The compositional generation is also used for drug discovery."
4095,SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"approaches USED-FOR program synthesis. neural models USED-FOR combinatorial search algorithms. neural model USED-FOR hands - on search policy. hands - on search policy USED-FOR bottom - up synthesis. search history CONJUNCTION partial program executions. partial program executions CONJUNCTION search history. neural model USED-FOR approach. bottom - up searches USED-FOR data. - policy USED-FOR CROSSBEAM. data USED-FOR - policy. data USED-FOR CROSSBEAM. string manipulation CONJUNCTION logic programming. logic programming CONJUNCTION string manipulation. string manipulation EVALUATE-FOR CROSSBEAM. domains EVALUATE-FOR CROSSBEAM. string manipulation HYPONYM-OF domains. logic programming HYPONYM-OF domains. OtherScientificTerm are search space, search space blowup, and program space. Method is combinatorial search algorithm. Task is structured prediction. Generic is state - of - the - art. ","This paper proposes a new combinatorial search algorithm CROSSBEAM, which is based on a neural model to learn a hands-on search policy for bottom-up synthesis. The proposed approach uses the learned neural model as the search history and partial program executions to learn the search space. The authors show that the proposed approach can achieve state-of-the-art performance on three domains: string manipulation, logic programming, and string manipulation.","This paper proposes a combinatorial search algorithm, CROSSBEAM, for structured prediction. The approach is based on a neural model that learns a hands-on search policy for bottom-up synthesis using search history and partial program executions. The authors show that the proposed approach outperforms previous approaches for program synthesis in terms of search space, search space blowup, and program space. The paper also shows that CROSSBAM outperforms state-of-the-art on three domains: string manipulation, logic programming, and string manipulation. "
4111,SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,minimizing the squared Bellman error USED-FOR Deep Reinforcement Learning. target networks USED-FOR training. lagging parameters USED-FOR target networks. functional regularizer USED-FOR squared Bellman error. target networks COMPARE regularization. regularization COMPARE target networks. up - to - date parameters USED-FOR regularization. Atari environments EVALUATE-FOR target - network based methods. sample efficiency CONJUNCTION performance. performance CONJUNCTION sample efficiency. performance EVALUATE-FOR target - network based methods. sample efficiency EVALUATE-FOR target - network based methods. approach COMPARE squared Bellman error. squared Bellman error COMPARE approach. OtherScientificTerm is fast - changing target Q - values. Method is training method. ,This paper studies the problem of minimizing the squared Bellman error in Deep Reinforcement Learning. The authors propose a functional regularizer to reduce the squared bellman error of the target networks during training. The proposed regularization is based on the up-to-date parameters of target networks with lagging parameters. They show that the fast-changing target Q-values improve the sample efficiency and improve the performance of target-network based methods in Atari environments. They also show that their approach is more efficient than the squared-Bellman error. ,This paper proposes a method for minimizing the squared Bellman error in Deep Reinforcement Learning. The authors propose a functional regularizer to reduce the squared bellman error. The proposed training method is based on the idea of fast-changing target Q-values. The main idea is to use target networks with lagging parameters to guide training. The regularization is done by adding up-to-date parameters to the target networks. Experiments on Atari environments show that target-network based methods improve sample efficiency and performance compared to standard regularization. 
4127,SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"neighborhood subgraphs FEATURE-OF hierarchy of local isomorphism. message - passing GNNs COMPARE Weisfeiler Lehman test. Weisfeiler Lehman test COMPARE message - passing GNNs. model COMPARE Weisfeiler Lehman test. Weisfeiler Lehman test COMPARE model. model USED-FOR graph structures. Weisfeiler Lehman test USED-FOR graph structures. GraphSNN HYPONYM-OF neural model. graph learning tasks EVALUATE-FOR model. model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE model. benchmark tasks EVALUATE-FOR state - of - the - art methods. benchmark tasks EVALUATE-FOR model. Method are Graph Neural Networks ( GNNs ), and GNNs. OtherScientificTerm is structural properties of graphs. ","This paper studies the problem of graph neural networks (GNNs) in the hierarchy of local isomorphism in graph subgraphs. The authors propose a new neural model called GraphSNN, which is based on the Weisfeiler Lehman test. The proposed model is able to learn the graph structures of the nodes in the graph by using a simple model. The model outperforms state-of-the-art methods on several benchmark tasks.","The paper proposes a new model for graph neural networks, Graph Neural Networks (GNNs). The model is based on the Weisfeiler Lehman test, which considers the hierarchy of local isomorphism in graph subgraphs. GraphSNN is a neural model that is trained on graph learning tasks. The authors show that the proposed model is able to learn graph structures that match the structural properties of graphs. The proposed model outperforms state-of-the-art methods on several benchmark tasks."
4143,SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"prediction interval ( PI ) method USED-FOR uncertainty quantification. retraining of neural networks ( NNs ) USED-FOR confidence level. retraining of neural networks ( NNs ) USED-FOR PI methods. fine tuning USED-FOR well - calibrated PI. sensitive hyperparameters FEATURE-OF customized loss functions. customized loss functions USED-FOR they. PI3NN method USED-FOR PIs. standard mean squared error loss USED-FOR NNs. root - finding algorithms USED-FOR PIs. root - finding algorithms USED-FOR linear combinations. PI3NN USED-FOR PIs. it USED-FOR crossing issue. OOD samples COMPARE in - distribution samples. in - distribution samples COMPARE OOD samples. initialization scheme USED-FOR PIs. PIs FEATURE-OF OOD samples. initialization scheme USED-FOR OOD samples. initialization scheme USED-FOR in - distribution samples. initialization scheme USED-FOR OOD identification challenge. predictive uncertainty quality CONJUNCTION robustness. robustness CONJUNCTION predictive uncertainty quality. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. robustness CONJUNCTION OOD samples identification. OOD samples identification CONJUNCTION robustness. OOD samples identification EVALUATE-FOR method. robustness EVALUATE-FOR state - of - the - art approaches. robustness EVALUATE-FOR method. predictive uncertainty quality EVALUATE-FOR state - of - the - art approaches. predictive uncertainty quality EVALUATE-FOR method. OtherScientificTerm are over - confident PIs, confidence levels, and hyperparameters. Method is retraining NNs. ","This paper proposes a new prediction interval (PI) method for uncertainty quantification based on retraining of neural networks (NNs) to improve the confidence level. Previous PI methods are based on the standard mean squared error loss for NNs, but they are over-confident PIs. The authors propose a new PI3NN method for training PIs with customized loss functions with sensitive hyperparameters, and fine tuning the well-calibrated PI. They show that their method improves predictive uncertainty quality and robustness compared to state-of-the-art approaches in OOD samples identification and in-distribution samples. They also show that the initialization scheme for PIs trained with the new initialization scheme can be used to identify OOD and PIs in the OOD identification challenge. ",This paper proposes a prediction interval (PI) method for uncertainty quantification. The PI methods are based on retraining of neural networks (NNs) for confidence level. The authors propose a PI3NN method for training PIs with customized loss functions with sensitive hyperparameters. The main idea is to use standard mean squared error loss to train the NNs and then fine tune the well-calibrated PI with fine tuning. They show that the over-confident PIs are more robust to changes in confidence levels. They also show that PIs trained with the initialization scheme for OOD samples and in-distribution samples with the same initialization scheme are better at OOD identification challenge than PIs that are trained with root-finding algorithms for linear combinations. The paper also shows that it is more robust against the crossing issue. The proposed method is shown to improve predictive uncertainty quality and robustness compared to state-of-the-art approaches. 
4159,SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"deep networks USED-FOR functions. classifiers CONJUNCTION detectors. detectors CONJUNCTION classifiers. detectors CONJUNCTION trackers. trackers CONJUNCTION detectors. deep networks USED-FOR classifiers. models USED-FOR applications. trackers HYPONYM-OF functions. classifiers HYPONYM-OF functions. detectors HYPONYM-OF functions. learning algorithms USED-FOR slow adaptation. gradient descent HYPONYM-OF learning algorithms. learning algorithms USED-FOR model. Meta - learning USED-FOR adaptation. metalearning USED-FOR online problems. meta - learning USED-FOR online setting. known ground - truth task boundaries FEATURE-OF discrete notion of tasks. discrete notion of tasks USED-FOR they. discrete boundaries PART-OF real - world settings. ground truth knowledge FEATURE-OF task boundaries. FOML COMPARE online learning methods. online learning methods COMPARE FOML. Rainbow - MNIST, and CIFAR100 datasets EVALUATE-FOR online learning methods. Rainbow - MNIST, and CIFAR100 datasets EVALUATE-FOR FOML. OtherScientificTerm are input distributions, and environmental conditions. Method are intelligent system, and Fully Online MetaLearning ( FOML ) algorithm. Task is complex and high - dimensional problems. Generic is methods. ","This paper studies the problem of online meta-learning for complex and high-dimensional problems, where the input distributions are not known and the goal is to learn an intelligent system. The authors propose a new Fully Online MetaLearning (FOML) algorithm, which is able to learn functions such as trackers, detectors, and classifiers from deep networks. They show that FOML outperforms existing online learning methods on the Rainbow-MNIST, and CIFAR100 datasets. They also show that the proposed model can be adapted to new environments by learning algorithms such as gradient descent. ","This paper proposes a new approach to learning deep neural networks for complex and high-dimensional problems. The proposed approach is based on the Fully Online MetaLearning (FOML) algorithm. FOML uses deep networks to learn functions such as detectors, classifiers, and trackers. The authors show that the proposed models can be applied to a wide range of applications. They also show that they can be used in real-world settings with discrete notion of tasks with known ground-truth task boundaries. The paper also shows that the learning algorithms for slow adaptation using gradient descent and meta-learning are effective for adaptation in the online setting with metalearning for online problems. Finally, they show that their methods outperform other online learning methods on Rainbow-MNIST, and CIFAR100 datasets."
4175,SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"chemical science and engineering task USED-FOR applications. structural design of functional molecules HYPONYM-OF chemical science and engineering task. molecular optimization HYPONYM-OF structural design of functional molecules. drug discovery HYPONYM-OF applications. Deep generative models CONJUNCTION combinatorial optimization methods. combinatorial optimization methods CONJUNCTION Deep generative models. knowledge network USED-FOR discrete chemical structures. knowledge network USED-FOR differentiable scaffolding tree ( DST ). DST USED-FOR gradient - based optimization. chemical graph structure USED-FOR gradient - based optimization. Method are brute - force enumeration, graph neural network ( GNN ), and gradient - based molecular optimizations. OtherScientificTerm are molecule structures, locally differentiable ones, derivatives, graph parameters, and domain experts. ","This paper studies the problem of structural design of functional molecules in the chemical science and engineering task of molecular optimization, which is an important application in many applications such as drug discovery. Deep generative models and combinatorial optimization methods have been used in recent years to solve this problem. The authors propose a differentiable scaffolding tree (DST), which is a graph neural network (GNN) that can be used for gradient-based molecular optimizations based on the chemical graph structure. The DST is trained by brute-force enumeration, where each node in the graph is represented as a set of locally differentiable ones, and the derivatives of each node are represented by a graph parameters. The knowledge network is then used to learn discrete chemical structures from the graph parameters, which can then be used as input to the GNN. ","This paper proposes a novel framework for molecular optimization in the context of structural design of functional molecules, a chemical science and engineering task with applications in applications such as drug discovery and drug discovery. Deep generative models and combinatorial optimization methods are used to learn discrete chemical structures. The authors propose a differentiable scaffolding tree (DST), a knowledge network that learns to predict differentiable molecule structures from locally differentiable ones. The DST is used for gradient-based optimization based on the chemical graph structure. The paper also proposes a graph neural network (GNN) to learn the derivatives of the graph parameters. The GNN is trained using brute-force enumeration and domain experts. "
4191,SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"diseases CONJUNCTION lab tests. lab tests CONJUNCTION diseases. patient representation USED-FOR sequential information. drug - lab interactions CONJUNCTION diagnosis - lab interactions. diagnosis - lab interactions CONJUNCTION drug - lab interactions. graphs USED-FOR drug - lab interactions. graphs USED-FOR diagnosis - lab interactions. real - world datasets EVALUATE-FOR solution. prediction errors EVALUATE-FOR solution. Method are Personalized medical systems, and knowledge - augmented approach. OtherScientificTerm is lab test responses. ","This paper proposes a knowledge-augmented approach to improve the performance of Personalized medical systems. The authors propose a patient representation for sequential information, which can be used to represent both disease and lab tests. The proposed solution is evaluated on two real-world datasets and shows that the proposed solution has better prediction errors than the state-of-the-art. ",This paper proposes a knowledge-augmented approach to improve the performance of Personalized medical systems. The idea is to use a patient representation to capture sequential information about the disease and lab tests. The authors propose to use graphs to capture the drug-lab interactions and the diagnosis-Lab interactions. The proposed solution is evaluated on real-world datasets and shows that the proposed solution can reduce prediction errors. 
4207,SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"Single domain generalization ( SDG ) HYPONYM-OF domain generalization. diversity of source domain USED-FOR robust model. adversarial data augmentation strategy USED-FOR SDG methods. OS - SDG USED-FOR model. CrossMatch approach USED-FOR SDG methods. SDG methods USED-FOR identifying unknown classes. multi - binary classifier USED-FOR SDG methods. adversarial data augmentation strategy USED-FOR CrossMatch. model USED-FOR unknown class identification. multibinary classifiers CONJUNCTION model. model CONJUNCTION multibinary classifiers. consistency regularization USED-FOR auxiliary samples. consistency regularization USED-FOR model. SDG methods USED-FOR model. CrossMatch USED-FOR SDG methods. benchmark datasets EVALUATE-FOR CrossMatch. SDG methods USED-FOR OS - SDG setting. benchmark datasets EVALUATE-FOR SDG methods. OtherScientificTerm are label space, source label space, target domains, and unknown classes. Task is real - world applications. ","This paper studies the problem of single domain generalization (SDG) in the context of real-world applications. The authors propose a CrossMatch approach to improve the performance of SDG methods for identifying unknown classes. CrossMatch uses an adversarial data augmentation strategy to train a robust model from the diversity of source domain. The model is trained using OS-SDG, where the source label space is the source domain and target domains are the target domains. The proposed model uses consistency regularization on the auxiliary samples to ensure that the model is robust to unknown class identification.  The authors show that the proposed CrossMatch improves the performance on several benchmark datasets for the OS- SDG setting. ","This paper proposes a new domain generalization (SDG) framework for single domain generalisation. The authors propose a robust model based on diversity of source domain and target domain. They propose a CrossMatch approach to train SDG methods for identifying unknown classes using a multi-binary classifier. The model is trained with multibinary classifiers and a model trained with the source label space. CrossMatch is based on an adversarial data augmentation strategy to improve the robust model. The proposed model is evaluated on three benchmark datasets for the OS-SDG setting, where it is shown that the model is able to identify unknown class identification with consistency regularization of the auxiliary samples. The paper also shows that the proposed CrossMatch can be applied to real-world applications."
4223,SP:126f8ffb855aa22eda4d681a499953879ed3679e,Trust - region methods USED-FOR policy optimization. policy optimization USED-FOR reinforcement learning. Kullback - Leibler divergence USED-FOR Trust - region methods. Wasserstein policy optimization ( WPO ) CONJUNCTION Sinkhorn policy optimization ( SPO ). Sinkhorn policy optimization ( SPO ) CONJUNCTION Wasserstein policy optimization ( WPO ). Wasserstein and Sinkhorn trust regions FEATURE-OF policy optimization. parametric distribution class FEATURE-OF policy. Lagrangian duality USED-FOR close - form policy updates. SPO COMPARE WPO. WPO COMPARE SPO. monotonic performance improvement FEATURE-OF WPO. robotic locomotion tasks EVALUATE-FOR approaches. tabular domains CONJUNCTION robotic locomotion tasks. robotic locomotion tasks CONJUNCTION tabular domains. SPO COMPARE policy gradient methods. policy gradient methods COMPARE SPO. tabular domains EVALUATE-FOR approaches. approaches COMPARE policy gradient methods. policy gradient methods COMPARE approaches. sample insufficiency FEATURE-OF WPO. OtherScientificTerm is policy distribution. Method is entropic regularizer. ,This paper proposes Trust-region methods for policy optimization in reinforcement learning with Kullback-Leibler divergence. The key idea is to use Lagrangian duality to encourage close-form policy updates. The authors show that Wasserstein policy optimization (WPO) and Sinkhorn policy optimization with Lagrangians can achieve better monotonic performance improvement than SPO and WPO with sample insufficiency. They also show that SPO is more efficient than policy gradient methods on tabular domains and robotic locomotion tasks. ,This paper introduces Trust-region methods for policy optimization for reinforcement learning. The key idea is to use Kullback-Leibler divergence between the Wasserstein and Sinkhorn trust regions in the policy optimization. The authors show that WPO achieves monotonic performance improvement with Lagrangian duality for close-form policy updates. They also show that SPO achieves better sample insufficiency compared to WPO and other approaches in tabular domains and robotic locomotion tasks. 
4239,SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"forgetting USED-FOR learning. learning trajectories FEATURE-OF artificial neural networks. forget - and - relearn USED-FOR learning trajectories. relearning step USED-FOR features. forgetting step USED-FOR undesirable information. forget - and - relearn framework USED-FOR iterative training algorithms. image classification FEATURE-OF iterative training algorithms. forgetting operations USED-FOR algorithms. iterative training PART-OF neural networks. OtherScientificTerm are Forgetting, and disproportionate forgetting of undesirable information. Task is human and machine learning. Generic is model. ","This paper studies the problem of forgetting in the context of learning trajectories in artificial neural networks. The authors propose a new forget-and-reluarn framework for iterative training algorithms for image classification. The main idea is to use a relearning step to learn the features of the model, and then use a forgetting step to remove undesirable information from the model. The forgetting operations are then used to train the algorithms with different forgetting operations in different stages of the training process. ","This paper proposes a novel forgetting-and-reluarn framework for learning trajectories of artificial neural networks. The forgetting step is used to remove undesirable information from the training data, and the relearning step re-learns the features. The authors show that the forgetting operations can be used to improve the performance of existing algorithms in image classification. The paper is well written and the model is well-motivated."
4255,SP:2789859517b6624730b14a7e010444a72d3dd3ed,"sufficient coverage CONJUNCTION policy. policy CONJUNCTION sufficient coverage. RL agents COMPARE agents. agents COMPARE RL agents. offline - online manner USED-FOR RL agents. Method are Batch RL, batch RL agents, and batch RL. OtherScientificTerm are data - collection process, and agent. Task is offline - online setting. Generic is setting. ","This paper studies the problem of Batch RL in the offline-online setting, where the data-collection process is offline and the agent has access to only a small subset of the data. The authors propose a new offline-offline manner to train RL agents in a more efficient and scalable manner than existing RL agents. They show that batch RL agents can achieve sufficient coverage and a better policy than existing agents in this setting. ","This paper proposes an offline-online setting for Batch RL, where the data-collection process is performed in an offline manner, and the agent is trained in a batch RL setting. The authors show that RL agents trained in this setting are more robust to data-losses than agents trained offline in an online manner. They also show that batch RL agents are better than RL agents that are trained in an off-line manner. "
4271,SP:76625a25e770415599a34122110d61cb3b7e614c,learning USED-FOR domain shift. episodic training procedure USED-FOR learning. episodic training procedure USED-FOR domain generalization ( DG ). learning USED-FOR domain generalization ( DG ). episodic training procedure USED-FOR domain shift. Y - discrepancy USED-FOR domain shift. source - domain samples USED-FOR Y - discrepancy. ERM CONJUNCTION domain - invariant learning. domain - invariant learning CONJUNCTION ERM. PAC - style generalization bound USED-FOR discrepancyoptimal meta - learning. PAC - style generalization bound COMPARE DG bounds. DG bounds COMPARE PAC - style generalization bound. domain - invariant learning HYPONYM-OF DG bounds. ERM HYPONYM-OF DG bounds. computational complexity EVALUATE-FOR discrepancy - optimal meta - learning. classification EVALUATE-FOR discrepancy - optimal meta - learning. classification CONJUNCTION computational complexity. computational complexity CONJUNCTION classification. bilevel optimization algorithm USED-FOR DG. DomainBed EVALUATE-FOR algorithm. DG benchmarks EVALUATE-FOR algorithm. ,"This paper proposes a new episodic training procedure for domain generalization (DG) based on episodic learning procedure for learning the domain shift in the presence of Y-divergence between source-domain samples. The authors provide a PAC-style generalization bound for discrepancyoptimal meta-learning, which is a combination of ERM and domain-invariant learning. The proposed algorithm is evaluated on DomainBed and DomainNet and achieves state-of-the-art performance on both classification and computational complexity. ","This paper proposes a novel episodic training procedure for domain generalization (DG) that can be applied to domain shift. The authors propose a PAC-style generalization bound for discrepancyoptimal meta-learning, where the Y-divergence between the source-domain samples and the target domain samples is minimized. The proposed algorithm is evaluated on DomainBed and on several DG benchmarks. The results show that the proposed algorithm outperforms existing DG bounds such as ERM, domain-invariant learning, and domain-agnostic learning in terms of computational complexity. "
4287,SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"deep reinforcement learning USED-FOR two - player games. combinatorial search methods USED-FOR NP - complete domains. SAT CONJUNCTION CSP. CSP CONJUNCTION SAT. deep reinforcement learning USED-FOR combinatorial search methods. Go HYPONYM-OF two - player games. CSP HYPONYM-OF NP - complete domains. SAT HYPONYM-OF NP - complete domains. exponential combinatorial search space FEATURE-OF hard instances. best - first search CONJUNCTION Monte Carlo tree search. Monte Carlo tree search CONJUNCTION best - first search. Monte Carlo tree search HYPONYM-OF search methods. best - first search HYPONYM-OF search methods. methods USED-FOR hard planning instances. policy and value networks USED-FOR DNN - based best - first search. Sokoban domain EVALUATE-FOR DNN - based best - first search. value network USED-FOR policy network. cost distribution FEATURE-OF search algorithms. heavy - tailed runtime distributions FEATURE-OF Sokoban planning instances. abstract tree model USED-FOR tails. policy network USED-FOR search. polynomial scaling FEATURE-OF left heavy tails. random restart strategies USED-FOR DNN - based search. random restart strategies USED-FOR combinatorial solvers. DNN - based search USED-FOR left and right heavy tails. Task is PSPACE - hard planning problems. Method is domain - specific solvers. Generic are specialized solvers, approaches, and model. OtherScientificTerm is exponentially sized sub - trees. ","This paper studies the problem of PSPACE-hard planning problems, where the goal is to find the optimal solution to a set of NP-complete domains. The authors propose two search methods, best-first search and Monte Carlo tree search, which are both based on deep reinforcement learning. The main idea is to use the policy and value networks in the DNN-based best-faster search, and use random restart strategies in the Monte-Carlo tree search. The paper shows that the two search algorithms have similar cost distribution, and that the cost distribution of the search algorithms can be improved by using the value network in the policy network. ","This paper presents a new approach to solving PSPACE-hard planning problems. The authors propose to use deep reinforcement learning for two-player games, such as Go, where the goal is to solve a set of NP-complete domains (e.g., SAT, CSP, etc.). The authors show that existing search methods such as best-first search, Monte Carlo tree search, and best-second search can find hard instances in the exponential combinatorial search space of the hard instances. They also show that these search algorithms have the same cost distribution as other domain-specific solvers, and that these specialized solvers can be used to solve hard planning instances with exponentially sized sub-trees. Finally, they show that DNN-based search for left and right heavy tails in the Sokoban domain can be performed with polynomial scaling, and the authors propose random restart strategies to improve the performance of the combinatorially solvers. The proposed approaches are evaluated on two different domains, and show that the proposed model is able to find left heavy tails using an abstract tree model. The main contribution of the paper is the use of policy and value networks for DNN -based best-First search. The policy network is used to guide the search, while the value network serves as the policy network for the search. "
4303,SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"approach USED-FOR meta - policy. adaptive loss USED-FOR meta - policy. human videos USED-FOR approach. method COMPARE baseline. baseline COMPARE method. vision - based tasks EVALUATE-FOR method. Method are Meta - Imitation Learning, and meta - imitation learning. OtherScientificTerm are human demonstrations, robot demonstrations, robot demonstration, and human imitation behavior. Generic is it. Task are meta - training phase, and data collection. ",This paper proposes a new method for meta-imitation learning from human demonstrations. The proposed method is based on the adaptive loss to learn a meta-policy from human videos. The method is evaluated on a variety of vision-based tasks and shows that it outperforms the baseline. ,"This paper proposes a new approach to learn a meta-policy for learning from human demonstrations. Meta-Imitation Learning is an important topic in meta-imitation learning, but it is not well-studied. This paper proposes an approach based on human videos and human videos. The approach is based on an adaptive loss for learning a meta - policy. The meta-training phase consists of two steps: (1) learning a robot demonstration, and (2) learning the human imitation behavior. The proposed method is evaluated on vision-based tasks and outperforms the baseline. The method is also evaluated on data collection."
4319,SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"Over - parameterized deep networks USED-FOR classification and ranking problems. gradient - based optimizers USED-FOR Over - parameterized deep networks. weight space FEATURE-OF adaptivity. Adam HYPONYM-OF Adaptive optimizers. weight decay ( WD ) CONJUNCTION normal hyper - parameter tuning. normal hyper - parameter tuning CONJUNCTION weight decay ( WD ). adaptive optimizers COMPARE SGD. SGD COMPARE adaptive optimizers. normal hyper - parameter tuning USED-FOR adaptive optimizers. weight decay ( WD ) USED-FOR adaptive optimizers. image classification domain EVALUATE-FOR SGD. image classification domain EVALUATE-FOR adaptive optimizers. generalization performance EVALUATE-FOR SGD. generalization performance EVALUATE-FOR adaptive optimizers. OtherScientificTerm are tuned regularization, network weights, training loss, and train loss. Generic are networks, and network. ",This paper proposes Adaptive Optimizers for Over-parameterized deep networks for classification and ranking problems. Adaptive optimizers such as Adam are gradient-based optimizers that are able to adaptively tune the weights of the networks in the weight space. The authors show that adaptive optimizers can achieve better generalization performance than SGD in the image classification domain with weight decay (WD) and normal hyper-parameters tuning. The main contribution of the paper is to provide a theoretical analysis of the effect of tuned regularization on the network weights. ,"This paper proposes Adaptive Optimizers (Adam), a family of gradient-based optimizers for classification and ranking problems. Adaptive optimizers can be seen as a variant of SGD, where the network weights are tuned regularization and the training loss is fixed. The adaptivity of the network is measured in the weight space. The authors show that adaptive optimizers with weight decay (WD) and normal hyper-parameter tuning outperform SGD in the image classification domain. "
4335,SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"model USED-FOR equivariance. edge orientations FEATURE-OF face. edge orientations CONJUNCTION face poses. face poses CONJUNCTION edge orientations. symmetries USED-FOR low and high - level features. face poses FEATURE-OF camera. edge orientations HYPONYM-OF low and high - level features. edge orientations HYPONYM-OF symmetries. equivariant networks USED-FOR partial and full equivariances. Partial G - CNNs HYPONYM-OF equivariant networks. full equivariance FEATURE-OF Partial G - CNNs. Partial G - CNNs COMPARE G - CNNs. G - CNNs COMPARE Partial G - CNNs. discrete groups CONJUNCTION continuous groups. continuous groups CONJUNCTION discrete groups. method USED-FOR discrete groups. method USED-FOR continuous groups. Task are generalization, and natural image classification. Method is group equivariant architectures. OtherScientificTerm are distribution, and rotations. Material is rotated MNIST. Generic is them. ","This paper proposes a new model for equivariance in group equivariant architectures. The model is based on the symmetries between edge orientations and face poses of a camera. The distribution is rotated MNIST. The authors show that partial and full equivariances can be obtained by equivarient networks such as Partial G-CNNs, which are able to achieve a full-equivariance. The proposed method can be applied to discrete groups, continuous groups, and continuous groups with rotations. ","This paper proposes a model for learning equivariance for group equivariant architectures. The model is based on the notion of generalization. The authors show that partial and full equivariances can be obtained by equivariants of equivarient networks. Partial G-CNNs are more general than G- CNNs, and the authors also show that the symmetries of low and high-level features (e.g. edge orientations and face poses) of a camera are equivariantly correlated with the distribution of the rotations of the camera. The paper also proposes a method for learning discrete groups and continuous groups. The proposed method is evaluated on rotated MNIST. "
4351,SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,Markov chain Monte Carlo ( MCMC ) USED-FOR approximating intractable distributions. Langevin dynamics HYPONYM-OF Markov chain Monte Carlo ( MCMC ). datapoint - wise iterations CONJUNCTION slow convergence. slow convergence CONJUNCTION datapoint - wise iterations. its USED-FOR deep latent variable models. inference model USED-FOR latent variables. ALD USED-FOR scalable inference. large - scale datasets USED-FOR ALD. datapoint - wise iterations USED-FOR it. large - scale datasets USED-FOR scalable inference. MCMC USED-FOR it. stationary distribution USED-FOR ALD. ALD USED-FOR generative modeling. it USED-FOR prior distribution. it USED-FOR latent variable. prior distribution FEATURE-OF latent variable. ALD USED-FOR unconditional distribution. it USED-FOR generative modeling. energy - based model HYPONYM-OF unconditional distribution. ALD USED-FOR deep latent variable model. Langevin autoencoder ( LAE ) HYPONYM-OF deep latent variable model. ALD USED-FOR autoencoder - like posterior inference. LAE USED-FOR autoencoder - like posterior inference. latent space EBM USED-FOR LAE. ALD USED-FOR LAE. ALD COMPARE LD. LD COMPARE ALD. ALD USED-FOR target distributions. toy datasets EVALUATE-FOR ALD. conditional and unconditional cases FEATURE-OF target distributions. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. CIFAR-10 CONJUNCTION CelebA - HQ. CelebA - HQ CONJUNCTION CIFAR-10. datasets USED-FOR image generation task. image generation task EVALUATE-FOR LAE. SVHN HYPONYM-OF datasets. datasets EVALUATE-FOR LAE. CelebA - HQ HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. LAE COMPARE non - amortized MCMC methods. non - amortized MCMC methods COMPARE LAE. LAE COMPARE AVI - based methods. AVI - based methods COMPARE LAE. Fréchet,"This paper proposes a new Markov chain Monte Carlo (MCMC) for approximating intractable distributions with Langevin dynamics.  The authors propose a deep latent variable model called Langevin autoencoder (LAE) which is based on the latent space EBM. The inference model learns the latent variables from the prior distribution of the latent variable. The latent variable is a prior distribution over the energy-based model. The stationary distribution is used to train the ALD. The authors show that ALD can be used for scalable inference on large-scale datasets, and that it is able to achieve better performance than LD and AVI-based methods. ALD is also able to learn target distributions in both conditional and unconditional cases.   The experiments show that LAE outperforms existing non-amortized MCMC methods on a variety of datasets including SVHN, CIFAR-10, CelebA-HQ, and toy datasets. ","Markov chain Monte Carlo (MCMC) is a popular method for approximating intractable distributions. However, it is not widely used in deep latent variable models. This paper introduces Langevin autoencoder (LAE) which is a deep latent variables model that uses an inference model to predict the latent variables. The prior distribution of the latent variable is the energy-based model. The latent space EBM is used to compute the prior distribution. The stationary distribution is used as the target distribution. ALD is evaluated on toy datasets and large-scale datasets for scalable inference. The results show that ALD outperforms LD and AVI-based methods in both conditional and unconditional cases. "
4367,SP:5631097031c7e599bdeae64366ffa6e4558837c6,hypergraph reasoning USED-FOR large domains. logical rules PART-OF logical reasoning. structured neural network USED-FOR hypergraph reasoning. neural networks CONJUNCTION finite - domain quantification operations. finite - domain quantification operations CONJUNCTION neural networks. SpaLoc USED-FOR grounding of relationships. sparse tensors USED-FOR SpaLoc. sparse tensors USED-FOR grounding of relationships. finite - domain quantification operations USED-FOR SpaLoc. neural networks USED-FOR SpaLoc. sparsification loss USED-FOR SpaLoc model. intermediate layers PART-OF SpaLoc model. sparsification loss USED-FOR intermediate layers. training and inference - time sub - sampling USED-FOR SpaLoc. real - world knowledge graphs HYPONYM-OF large - scale graphs. information loss FEATURE-OF sampled sub - graphs. information - theoretic measure information sufficiency USED-FOR sampling and label calibration paradigm. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. accuracy EVALUATE-FOR SpaLoc. efficiency EVALUATE-FOR SpaLoc. synthetic datasets EVALUATE-FOR SpaLoc. real - world knowledge graph reasoning benchmarks EVALUATE-FOR SpaLoc. OtherScientificTerm is grandparent relationship. Method is hypergraph neural networks. ,"This paper studies hypergraph reasoning in large domains. The authors propose a structured neural network that can be used as a structured hypergraph neural network to learn logical rules in the context of logical reasoning. The proposed model, called SpaLoc, uses a sparsification loss on the intermediate layers in the SpaLoc model to learn the grounding of relationships using sparse tensors and finite-domain quantification operations. In addition, a training and inference-time sub-sampling is used to improve the performance of SpaLoc. The sampling and label calibration paradigm is based on information-theoretic measure information sufficiency. The paper shows that SpaLoc achieves state-of-the-art accuracy and efficiency on several real-world knowledge graph reasoning benchmarks.","This paper proposes a new model for hypergraph reasoning for large domains. The model is based on a structured neural network. The authors propose to use logical rules in logical reasoning. The grounding of relationships is done by using sparse tensors in the form of a GP, and the authors propose a sparsification loss on the intermediate layers of the SpaLoc model. The paper also proposes a sampling and label calibration paradigm based on information-theoretic measure information sufficiency. Experiments on synthetic datasets and real-world knowledge graph reasoning benchmarks show the effectiveness of the proposed model. "
4383,SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"top - k classification accuracy EVALUATE-FOR machine learning. probability distribution USED-FOR k. top-1 CONJUNCTION top-5 accuracy. top-5 accuracy CONJUNCTION top-1. ImageNet EVALUATE-FOR models. top-5 accuracy EVALUATE-FOR models. top-1 EVALUATE-FOR models. Task is differentiable sorting and ranking. Metric are top-5 accuracies, and top-1 accuracy. Method is ImageNet models. ",This paper studies the problem of differentiable sorting and ranking in machine learning. The authors study the top-k classification accuracy of top-1 and top-5 accuracy of ImageNet models on ImageNet. They show that these two models are close to each other in terms of both the number of iterations and the number (number of iterations) of the training set. They also show that top-2 accuracies are better than top-3 accuracies. ,"This paper studies the top-k classification accuracy in machine learning. The authors consider differentiable sorting and ranking, where the probability distribution for k is the same for all classes. They show that top-1 and top-5 accuracies of ImageNet models are close to each other. They also show that models with top-2 or top-3 accuracies are more accurate than models that have top-4 accuracy."
4399,SP:cb3188f435c54a365890e20e4d582c250d919833,"speed CONJUNCTION accuracy. accuracy CONJUNCTION speed. accuracy EVALUATE-FOR method. speed EVALUATE-FOR method. method USED-FOR OT problem. Douglas - Rachford splitting technique USED-FOR method. method USED-FOR approximate regularized problem. entropic regularization USED-FOR methods. algorithm COMPARE Sinkhorn method. Sinkhorn method COMPARE algorithm. method COMPARE Sinkhorn method. Sinkhorn method COMPARE method. iteration complexity EVALUATE-FOR method. linear convergence rate USED-FOR OT problem. primal - dual stopping criterion FEATURE-OF method. computation times CONJUNCTION robustness. robustness CONJUNCTION computation times. robustness EVALUATE-FOR method. computation times EVALUATE-FOR method. OtherScientificTerm are sparse transport plans, and numerical issues. ",This paper proposes a method for approximate regularized problem with sparse transport plans. The proposed method is based on the Douglas-Rachford splitting technique. The authors show that the speed and accuracy of the proposed method are better than the Sinkhorn method in terms of both computation times and accuracy. The method also achieves a linear convergence rate for the OT problem with primal-dual stopping criterion. ,"This paper proposes a method for solving the OT problem with sparse transport plans. The proposed method is based on the Douglas-Rachford splitting technique. The authors show that the proposed method can solve the approximate regularized problem with entropic regularization. The method is evaluated on the primal-dual stopping criterion and is shown to outperform the Sinkhorn method in terms of speed, accuracy, and iteration complexity. "
4415,SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"distribution of distributions USED-FOR Federated learning data. generalization studies USED-FOR federated learning. framework USED-FOR performance gaps. dataset synthesis strategy USED-FOR realistic simulations of generalization. OtherScientificTerm are meta - distribution, local data distributions, out - of - sample gap, and participation gap. Material are natural and synthetic federated datasets, and naturally - partitioned data. Method is semantic synthesis strategy. ","This paper studies the distribution of distributions in Federated learning data. The authors propose a new dataset synthesis strategy for realistic simulations of generalization in federated learning. The main idea is to learn a meta-distribution over the local data distributions, which is then used to learn the performance gaps between the local and global data distributions. The performance gap is defined as the difference between the out-of-sample gap and the participation gap between the global and local distributions.  The authors then propose a semantic synthesis strategy to solve the performance gap. The proposed framework is evaluated on both natural and synthetic federated datasets.","This paper proposes a new distribution of distributions for Federated learning data, which is based on the meta-distribution of distributions in generalization studies for federated learning. The authors propose a semantic synthesis strategy to reduce the performance gaps between the local data distributions and the global data distributions. The main contribution of the paper is to propose a framework to reduce performance gaps in the meta -distribution. The paper shows that the out-of-sample gap can be reduced to a participation gap, and that the participation gap can also be reduced. Experiments are conducted on natural and synthetic federated datasets, and naturally-partitioned data. The proposed dataset synthesis strategy is evaluated on realistic simulations of generalization. "
4431,SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"generalization EVALUATE-FOR LMs. self - supervision objectives USED-FOR LMs. supervised finetuning USED-FOR pre - trained language models ( PLMs ). model capacity CONJUNCTION data size. data size CONJUNCTION model capacity. pre - training techniques USED-FOR PLMs. PLMs USED-FOR few - shot setting. PLMs USED-FOR few - shot tasks. PLMs USED-FOR zero - shot setting. BERT family USED-FOR models. IMDB dataset CONJUNCTION Amazon dataset. Amazon dataset CONJUNCTION IMDB dataset. IMDB dataset HYPONYM-OF datasets. Amazon dataset HYPONYM-OF datasets. PLMs USED-FOR language understanding tasks. zero - shot setting FEATURE-OF PLMs. GLUE HYPONYM-OF language understanding tasks. Method are deep learning, language models ( LMs ), and prompt - based learning. OtherScientificTerm are manually / automatically created prompts, and manually created prompts. Metric is accuracy. ","This paper studies the problem of self-supervision of pre-trained language models (PLMs) trained with supervised finetuning. The authors consider the few-shot setting, where the goal is to improve the generalization of the LMs. They propose two pre-training techniques for PLMs: (1) the BERT family, and (2) the PLMs trained with PLMs in the zero-shoot setting. They show that PLMs can achieve state-of-the-art performance on a variety of language understanding tasks, including GLUE, IMDB, and Amazon dataset.","This paper proposes a new way to improve the generalization of language models (LMs) in the context of deep learning. The main idea is to use self-supervision objectives to train LMs with manually/automatically created prompts. The authors propose to use supervised finetuning to train pre-trained language models with pre-training techniques to increase the model capacity and the data size. The proposed models are based on the BERT family and are evaluated on the IMDB dataset, the Amazon dataset, and the GLUE dataset. The results show that PLMs are able to generalize better in the zero-shot setting compared to PLMs for few-shot tasks in the language understanding tasks. "
4447,SP:9817dccb1a121058b23a2ef825ed339cf8b53674,Attention mechanism USED-FOR tasks. sharpener module USED-FOR attention mechanism. it USED-FOR representation. alignment FEATURE-OF attention. real - world scene text recognition datasets EVALUATE-FOR approach. approach COMPARE ones. ones COMPARE approach. approach COMPARE soft and hard attention. soft and hard attention COMPARE approach. soft and hard attention HYPONYM-OF ones. ,This paper proposes a new attention mechanism for tasks where the attention mechanism is trained with a sharpener module. The key idea is to learn the alignment between the attention and the representation. The proposed approach is evaluated on a variety of real-world scene text recognition datasets and shows that the proposed approach performs better than existing ones such as soft and hard attention.,"This paper proposes a novel attention mechanism for tasks where the attention mechanism can be applied to multiple tasks. The attention mechanism is based on a sharpener module. The authors show that it can be used to align the representation of the attention with the alignment of the task. The proposed approach is evaluated on real-world scene text recognition datasets. The approach is compared to other ones, including soft and hard attention."
4463,SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"Learning USED-FOR combinatorial optimization problems. operations research solvers CONJUNCTION heuristics. heuristics CONJUNCTION operations research solvers. vehicle routing problem HYPONYM-OF combinatorial optimization problems. apriori given number of available vehicles USED-FOR complex assignment problem. bounded fleet size FEATURE-OF logistic service providers. post - processing scheme USED-FOR supervised approach. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. Method are deep reinforcement learning approaches, learning - based approaches, and supervised deep learning framework. Generic is them. OtherScientificTerm is apriori fixed number of available vehicles. ","This paper studies the problem of combinatorial optimization problems such as the vehicle routing problem, where the apriori given number of available vehicles is a complex assignment problem. The authors propose a supervised deep learning framework to solve this problem. They propose a post-processing scheme to train the supervised approach. They show that their method outperforms state-of-the-art approaches.","This paper proposes a new approach for learning combinatorial optimization problems such as the vehicle routing problem. The main idea is to use deep reinforcement learning approaches to solve the complex assignment problem. In contrast to previous learning-based approaches, the proposed supervised deep learning framework is based on a post-processing scheme. Experiments show that the proposed method outperforms state-of-the-art approaches on the bounded fleet size of logistic service providers."
4479,SP:594a813c0d0baa66738b9c8331370f861ad3c416,"Existing methods USED-FOR variables. clustering effect HYPONYM-OF observed graph structure. observed graph structure HYPONYM-OF variables. clustering effect HYPONYM-OF variables. causal relationship FEATURE-OF variables. link prediction method USED-FOR graph learning. counterfactual inference USED-FOR link prediction method. counterfactual inference USED-FOR graph learning. It USED-FOR counterfactual links. It USED-FOR representations. observed and counterfactual links USED-FOR representations. benchmark datasets EVALUATE-FOR graph learning method. link prediction EVALUATE-FOR graph learning method. Task is graph - based applications. Generic is it. Method are causal models, and graph representations. OtherScientificTerm is global graph structural properties. ","This paper proposes a new link prediction method for graph learning based on counterfactual inference. The key idea is to use the clustering effect between the observed graph structure and the correlated variables (e.g., clustering of nodes) as a proxy for the causal relationship between the variables. The authors show that the proposed method is able to learn the causal relationships between the two variables, and that it can be applied to a wide range of graph-based applications. It is also shown that the learned representations can be used to learn both observed and counterfactually links for graph representations. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed graph learning method.","This paper proposes a link prediction method for graph learning based on counterfactual inference. The idea is to use existing methods to predict variables such as clustering effect, observed graph structure, and the causal relationship between two variables. It is shown that it is possible to predict these variables using causal models. The paper also shows that the proposed graph learning method is able to predict the global graph structural properties. It can also predict the representations based on observed and counterfactually links. Experiments are conducted on several benchmark datasets to show the effectiveness of the proposed method. "
4495,SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"unsupervised feature selection methods USED-FOR ranking features. second - order covariance matrix CONJUNCTION first - order data matrix. first - order data matrix CONJUNCTION second - order covariance matrix. sparse attention matrix USED-FOR second - order relations between features. graph segmentation USED-FOR feature selection. attention matrix USED-FOR relational graph. SOFT COMPARE state - of - the - art methods. state - of - the - art methods COMPARE SOFT. SOFT COMPARE method. method COMPARE SOFT. Task are Unsupervised feature selection, and unsupervised feature selection. OtherScientificTerm is external guidance information. Method is knowledge contrastive disTillation ( SOFT ) model. ","This paper studies the problem of unsupervised feature selection for ranking features. The authors propose a knowledge contrastive disTillation (SOFT) model that uses sparse attention matrix to learn the second-order relations between features, which is then used to select the best ranking features from a relational graph by graph segmentation. They show that SOFT outperforms state-of-the-art methods in terms of performance. ",This paper proposes a knowledge contrastive disTillation (SOFT) model to improve the performance of unsupervised feature selection methods for ranking features. The main idea is to use a sparse attention matrix for second-order relations between features and a first-order covariance matrix for the first order data matrix. The attention matrix is used for relational graph segmentation for feature selection. The authors show that SOFT outperforms state-of-the-art methods in terms of performance. They also show that the proposed method outperforms SOFT in the presence of external guidance information.
4511,SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"Multimodal variational autoencoders ( VAEs ) USED-FOR joint distribution. vision CONJUNCTION language. language CONJUNCTION vision. heterogeneous data USED-FOR joint distribution. language HYPONYM-OF heterogeneous data. vision HYPONYM-OF heterogeneous data. idiosyncratic representations PART-OF recognition model. mixtures CONJUNCTION factorisations. factorisations CONJUNCTION mixtures. mixtures USED-FOR idiosyncratic representations. MEME COMPARE baselines. baselines COMPARE MEME. metrics EVALUATE-FOR partial and complete observation schemes. metrics EVALUATE-FOR MEME. metrics EVALUATE-FOR baselines. representations COMPARE approaches. approaches COMPARE representations. mutual supervision USED-FOR representations. OtherScientificTerm are modalities, and relatedness between data. Generic are alternative, and formulation. Method are Mutually supErvised Multimodal VAE ( MEME ), and semisupervised VAEs. Material is partiallyobserved data. ","This paper proposes a new multimodal variational autoencoders (VAEs) for the joint distribution of heterogeneous data such as vision, language, and language. The authors propose Mutually supErvised MultimodAL VAE (MEME), which is an alternative to semisupervised VAEs. The key idea is to use idiosyncratic representations in the recognition model, which are mixtures of mixtures and factorisations, to learn the modalities and the relatedness between data. The paper shows that MEME achieves better performance than existing baselines on a variety of metrics for partial and complete observation schemes. The proposed representations are also able to be trained with mutual supervision.","This paper proposes Mutually supErvised Multimodal VAE (MEME), an alternative to the popular multi-modal variational autoencoders (VAEs) for the joint distribution of heterogeneous data (e.g., vision, language, and language). The key idea of the formulation is that the modalities of the recognition model can be represented as idiosyncratic representations of the mixtures of mixtures and factorisations. The authors show that MEME outperforms other baselines on both partial and complete observation schemes. They also show that the proposed representations are more robust to mutual supervision than other approaches. "
4527,SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"Reinforcement Learning agent USED-FOR directed behaviors. Intrinsic Motivation USED-FOR Reinforcement Learning agent. options USED-FOR simple tabular cases. Deep Explore Options USED-FOR complex visual problems. Explore Options PART-OF Deep Reinforcement Learning paradigm. unrelated intrinsic rewards USED-FOR Deep Explore Options. J - PER HYPONYM-OF transitionselection algorithm. interest of multiple agents USED-FOR transitionselection algorithm. intrinsic reward learning USED-FOR auxiliary task. architecture USED-FOR shared representation. Atari Suite FEATURE-OF hard and easy exploration games. Atari Suite EVALUATE-FOR Deep Explore Options. hard and easy exploration games EVALUATE-FOR Deep Explore Options. they COMPARE weighted sum of rewards. weighted sum of rewards COMPARE they. weighted sum of rewards COMPARE baselines. baselines COMPARE weighted sum of rewards. they COMPARE baselines. baselines COMPARE they. intrinsic rewards USED-FOR they. OtherScientificTerm are sparse or noisy rewards, intrinsic and extrinsic rewards, interesting behaviors, high dimensional spaces, and exploitative or exploratory behaviors. Method is intrinsically motivated agent. Task is exploration. ","This paper proposes a new approach to explore options in a Reinforcement Learning agent to learn directed behaviors. Intrinsic Motivation is used to motivate the Reinforcement learning agent to explore the environment. Deep Explore Options is a new Deep Reinforcement LMs with unrelated intrinsic rewards. The authors propose a transitionselection algorithm called J-PER, which is based on the interest of multiple agents to learn an auxiliary task with intrinsic reward learning. The proposed architecture is able to learn a shared representation between the intrinsic and extrinsic rewards, which can be used to learn interesting behaviors in high dimensional spaces. Experiments on the Atari Suite show the effectiveness of Deep Explore options in both hard and easy exploration games. ","This paper proposes a new approach to learn directed behaviors from sparse or noisy rewards. Intrinsic Motivation is used to motivate a Reinforcement Learning agent to explore directed behaviors. Deep Explore Options are an extension of the Explore Options in the Deep Reinforcement learning paradigm. The authors propose a transitionselection algorithm called J-PER, which is based on the interest of multiple agents in high dimensional spaces. They show that they outperform the weighted sum of rewards and the intrinsic rewards in both hard and easy exploration games. They also show that intrinsic reward learning can be used to learn an auxiliary task that is independent of the intrinsic and extrinsic rewards. Finally, the authors propose an architecture to learn a shared representation of the exploration."
4543,SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,method USED-FOR Hamiltonian dynamical systems. stiffness FEATURE-OF dynamical system. SANN USED-FOR stiff and nonstiff portions. stiffness - aware index USED-FOR SANN. classification CONJUNCTION resampling technique. resampling technique CONJUNCTION classification. time integration strategies USED-FOR dynamical characteristics. dynamical characteristics FEATURE-OF Hamiltonian vector fields. step size adaptation USED-FOR dynamical characteristics. resampling technique USED-FOR time integration strategies. classification USED-FOR time integration strategies. step size adaptation HYPONYM-OF time integration strategies. three - body problem CONJUNCTION billiard model. billiard model CONJUNCTION three - body problem. billiard model EVALUATE-FOR SANN. complex physical systems EVALUATE-FOR SANN. three - body problem HYPONYM-OF complex physical systems. billiard model HYPONYM-OF complex physical systems. SANN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE SANN. energy EVALUATE-FOR SANN. accuracy EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR SANN. Method is stiffness - aware neural network ( SANN ). ,"This paper proposes a method for learning Hamiltonian dynamical systems with high stiffness. The main idea is to use the stiffness-aware neural network (SANN) to learn the stiff and nonstiff portions of a dynamical system. The SANN is based on the stiffness index of the SANN, which is a combination of the stiffness of the Hamiltonian vector fields. The authors show that SANN can learn both the stiff-and-nonstiff components of a system with high energy. They also show that the accuracy of SANN outperforms state-of-the-art methods on the three-body problem and the billiard model. ","This paper proposes a method for learning Hamiltonian dynamical systems. The authors propose a stiffness-aware neural network (SANN) that is able to learn both the stiff and nonstiff portions of the dynamical system. The SANN is based on a stiff-aware index, and the authors propose two time integration strategies for learning dynamical characteristics of Hamiltonian vector fields: classification and a resampling technique. The proposed SANN outperforms state-of-the-art methods in terms of energy and accuracy on complex physical systems such as the three-body problem and the billiard model."
4559,SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"language models USED-FOR tasks. generating realistic text CONJUNCTION synthesizing computer programs. synthesizing computer programs CONJUNCTION generating realistic text. generating realistic text HYPONYM-OF tasks. synthesizing computer programs HYPONYM-OF tasks. they USED-FOR tasks. unbounded multi - step computation USED-FOR tasks. models USED-FOR multistep computations. Transformers USED-FOR multi - step computations. intermediate computation steps PART-OF scratchpad ”. language models USED-FOR multi - step computations. scratchpads USED-FOR language models. OtherScientificTerm are computer programs, integers, and programs. Method is intermediate computations. ","This paper proposes a new multi-step computation method for the task of generating realistic text and synthesizing computer programs. The proposed method is based on the scratchpad method. The authors show that the proposed method can be applied to a variety of tasks, and that it is able to achieve state-of-the-art performance. The paper also shows that it can be used to train language models for multistep computations.","This paper proposes a new way to learn language models for tasks such as generating realistic text and synthesizing computer programs. The authors propose a “scratchpad”, which consists of intermediate computation steps in the scratchpad’s first layer, where the computer programs are represented as integers, and the intermediate computations are computed in the second layer. They show that they can be used for tasks with unbounded multi-step computation. They also show that Transformers can also be used to perform multistep computations in the first layer. Finally, they show that language models trained on scratchpads can be combined with other language models to perform multi-stage computations."
4575,SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"deep networks USED-FOR adversarial attacks. methods USED-FOR adversarial perturbations. deep image generators CONJUNCTION optimization objective. optimization objective CONJUNCTION deep image generators. deep image generators USED-FOR feature - level adversarial perturbations. optimization objective USED-FOR feature - level adversarial perturbations. them USED-FOR targeted feature - level attacks. they USED-FOR targeted feature - level attacks. ImageNet scale FEATURE-OF targeted feature - level attacks. them USED-FOR copy / paste ” adversaries. OtherScientificTerm are feature - class associations, natural objects, and targeted misclassification. Method is featurefool attacks. Generic is attacks. Material is natural image. ","This paper studies the problem of featurefool attacks on deep networks. The authors propose two methods to attack adversarial perturbations on deep image generators and an optimization objective to improve the performance of feature-level adversarial attacks. The main idea is to learn feature-class associations between natural objects in the input image and the target image, and then use them to train targeted feature -level attacks on the ImageNet scale. The proposed attacks are based on targeted misclassification, where the target is a natural image. The paper shows that these attacks are effective against “copy/paste” adversaries.","This paper proposes a new attack method for adversarial attacks on deep networks. The authors propose two methods to attack adversarial perturbations on the feature-class associations of natural objects. The main idea is to use deep image generators and an optimization objective to learn feature-level adversarial primitives. Then, they use them for targeted feature level attacks on the ImageNet scale. The proposed attacks are based on featurefool attacks, where the target is a natural image and the target misclassification is performed on the natural image. They also use them to attack the “copy/paste” adversaries."
4591,SP:873618263dc4246a39c44d0abfecfb5f688817e3,Simulated annealing ( SA ) HYPONYM-OF stochastic global optimisation technique. stochastic global optimisation technique USED-FOR discrete and continuous variable problems. neighbour proposal distribution CONJUNCTION temperature annealing schedule. temperature annealing schedule CONJUNCTION neighbour proposal distribution. SA optimiser USED-FOR problem. temperature annealing schedule HYPONYM-OF handpicked components. handpicked components USED-FOR SA optimiser. neighbour proposal distribution HYPONYM-OF handpicked components. policy USED-FOR solution quality. policy USED-FOR proposal distribution. Knapsack problem CONJUNCTION Bin Packing problem. Bin Packing problem CONJUNCTION Knapsack problem. Bin Packing problem CONJUNCTION Travelling Salesperson problem. Travelling Salesperson problem CONJUNCTION Bin Packing problem. Rosenbrock ’s function CONJUNCTION Knapsack problem. Knapsack problem CONJUNCTION Rosenbrock ’s function. Neural SA COMPARE SA baselines. SA baselines COMPARE Neural SA. hand - selected parameters USED-FOR SA baselines. hand - selected parameters USED-FOR Neural SA. problems EVALUATE-FOR Neural SA. problems EVALUATE-FOR SA baselines. proposal distribution USED-FOR Neural SA. Travelling Salesperson problem HYPONYM-OF problems. Rosenbrock ’s function HYPONYM-OF problems. Bin Packing problem HYPONYM-OF problems. Knapsack problem HYPONYM-OF problems. solution quality CONJUNCTION wall clock time. wall clock time CONJUNCTION solution quality. Neural SA COMPARE solvers. solvers COMPARE Neural SA. wall clock time EVALUATE-FOR Neural SA. solution quality EVALUATE-FOR Neural SA. wall clock time EVALUATE-FOR solvers. solution quality EVALUATE-FOR solvers. Method is SA. OtherScientificTerm is computational budget. ,This paper proposes Simulated annealing (SA) which is a stochastic global optimisation technique for discrete and continuous variable problems. The main idea of SA is to use a SA optimiser with handpicked components to solve the problem. The proposed method uses a policy to learn the proposal distribution of the problem and the neighbour proposal distribution. The handpicked component is composed of a temperature annealed schedule and a set of neighbours. The authors show that the proposed proposal distribution is better than the existing SA baselines with hand-selected parameters. They also show that Neural SA outperforms the existing solvers in terms of solution quality and wall clock time. ,"Simulated annealing (SA) is a stochastic global optimisation technique for discrete and continuous variable problems. The main idea of SA is to use handpicked components in the SA optimiser to solve the problem. The handpicked components are the neighbour proposal distribution, temperature annealed schedule, and Rosenbrock’s function. The proposal distribution is learned using a policy that maximizes the solution quality and minimizes the computational budget. Experiments show that Neural SA outperforms other SA baselines on hand-selected parameters and solvers on wall clock time. "
4607,SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"policy changes of agents USED-FOR learning process. measurement indicators USED-FOR non - stationarity. notion USED-FOR non - stationarity. non - stationarity FEATURE-OF policy sequence. δ - stationarity measurement HYPONYM-OF notion. trust - region constraint FEATURE-OF joint policy. policy factorization USED-FOR policy divergence. trust - region constraints FEATURE-OF factorized policies. mean - field approximation HYPONYM-OF policy factorization. computational complexity EVALUATE-FOR it. pairwise Markov random field USED-FOR joint policy. MAMT HYPONYM-OF Multi - Agent Mirror descent policy algorithm. end - to - end manner USED-FOR trust - region of the local policies. Trust region decomposition USED-FOR Multi - Agent Mirror descent policy algorithm. MAMT USED-FOR non - stationarity problem. MAMT USED-FOR joint policies ’ divergence. method COMPARE baselines. baselines COMPARE method. complexity FEATURE-OF cooperative tasks. cooperative tasks EVALUATE-FOR baselines. cooperative tasks EVALUATE-FOR method. complexity EVALUATE-FOR baselines. complexity EVALUATE-FOR method. Task is Non - stationarity. Generic is algorithms. Metric is KL - divergence. OtherScientificTerm are trust - region decomposition dilemma, joint policy divergence, and δ - stationarity. Method is message passing. ","This paper studies the problem of non-stationarity in the learning process. The authors propose a notion called δ-stability measurement, which is a new notion for measuring the non-starity of a policy sequence. The key idea is to use the measurement indicators to measure the policy changes of agents, and then use the learned algorithms to estimate the KL-divergence between the two policies. The joint policy divergence is defined by the trust-region decomposition dilemma, where the joint policy is computed using a pairwise Markov random field, and the joint policies’ divergence is computed by a policy factorization based on mean-field approximation.  The authors then propose a Multi-Agent Mirror descent policy algorithm, called MAMT, which solves the non -stationarity problem using MAMR. The proposed method has a lower computational complexity than existing baselines on cooperative tasks.   ","This paper proposes a new notion of non-stationarity of the policy sequence of an agent, i.e., the change in the policy changes of agents during the learning process. The notion is based on the δ-stability measurement, which is a set of measurement indicators that measure the difference between the stationarity of two agents in the same policy sequence. The authors propose a new algorithm called MAMT, a Multi-Agent Mirror descent policy algorithm based on Trust region decomposition (MAMT). The key idea is to use the trust-region decomposition dilemma, where the joint policy divergence between two agents is defined as the KL-divergence between the two policies. The joint policy is learned in a pairwise Markov random field, and the policy factorization is used to reduce the policy divergence. The paper also proposes a mean-field approximation to the joint policies. This is done in an end-to-end manner. The proposed method is evaluated on cooperative tasks and shows better computational complexity compared to other baselines. "
4623,SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"correlated audio and visual information PART-OF Video recordings of speech. self - supervised representation learning framework USED-FOR audio - visual speech. AV - HuBERT USED-FOR audio - visual speech representation. audio - visual speech representation USED-FOR lip - reading and automatic speech recognition. AV - HuBERT COMPARE state - of - the - art approach. state - of - the - art approach COMPARE AV - HuBERT. labeled data USED-FOR AV - HuBERT. transcribed video data USED-FOR state - of - the - art approach. WER EVALUATE-FOR AV - HuBERT. labeled data USED-FOR LRS3. audio - visual representation USED-FOR audio - only speech recognition. benchmark USED-FOR audio - only speech recognition. benchmark EVALUATE-FOR audio - visual representation. relative WER reduction EVALUATE-FOR audio - visual representation. Method are speech representation, and self - training. Material is multi - stream video input. OtherScientificTerm is multimodal hidden units. Metric is lip - reading WER. ","This paper proposes a self-supervised representation learning framework for audio-visual speech. Video recordings of speech contain both related audio and visual information, and the goal is to learn a speech representation that can be used for lip-reading and automatic speech recognition. AV-HuBERT uses transcribed video data and labeled data from LRS3. The proposed self-training is based on multi-stream video input, where the multimodal hidden units are represented as multi-batches. The authors show that AV-HuberT achieves better relative WER reduction than state-of-the-art approach on WER. ",This paper proposes a self-supervised representation learning framework for audio-visual speech. Video recordings of speech contain both related audio and visual information. The key idea is to use multi-stream video input with multimodal hidden units to learn the speech representation. The authors show that the proposed AV-HuBERT is able to achieve better performance than state-of-the-art approach on transcribed video data and labeled data for LRS3. The paper also shows that the relative WER reduction is better than the standard lip-reading WER. 
4639,SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,combinatorial optimisation USED-FOR real - world applications. logistics CONJUNCTION natural sciences. natural sciences CONJUNCTION logistics. natural sciences FEATURE-OF combinatorial optimisation. graphs USED-FOR combinatorial optimisation. pre - solved instances USED-FOR problems. graph neural networks ( GNNs ) USED-FOR decision step. ECORD HYPONYM-OF RL algorithm. pre - processing step USED-FOR GNN. recurrent unit USED-FOR fast - acting exploratory phase. SOTA USED-FOR RL algorithms. ECORD USED-FOR RL algorithms. RL algorithms USED-FOR Maximum Cut problem. ECORD USED-FOR SOTA. SOTA USED-FOR Maximum Cut problem. Maximum Cut problem EVALUATE-FOR ECORD. scalability EVALUATE-FOR ECORD. speed EVALUATE-FOR ECORD. nearest competitor COMPARE ECORD. ECORD COMPARE nearest competitor. optimality gap EVALUATE-FOR ECORD. Method is Reinforcement learning ( RL ). Generic is it. OtherScientificTerm is wall - clock time. ,"This paper studies the problem of combinatorial optimisation in real-world applications such as logistics and natural sciences. In particular, it considers Reinforcement learning (RL), where the goal is to find the optimal solution to a set of problems with pre-solved instances. In this setting, graph neural networks (GNNs) are used to solve the decision step, and a pre-processing step is used to train the GNN. A recurrent unit is used for the fast-acting exploratory phase. The authors show that ECORD, an RL algorithm based on ECORD HYPONYM, can solve the Maximum Cut problem with SOTA. ECORD is also shown to be faster than other RL algorithms in terms of scalability. ","This paper proposes a novel approach to combinatorial optimisation for real-world applications such as logistics and natural sciences. The main idea is to use graph neural networks (GNNs) to solve problems with pre-solved instances. The GNN is trained with a pre-processing step, and then a decision step is performed on the decision step. The decision step consists of two steps: 1) a fast-acting exploratory phase with a recurrent unit, and 2) a slow-acting exploration phase. The authors show that ECORD, an RL algorithm based on SOTA, outperforms other RL algorithms on the Maximum Cut problem, which is a variant of Reinforcement learning (RL). ECORD also outperforms the nearest competitor in terms of scalability and the optimality gap. "
4655,SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"Discrete latent variables USED-FOR generation process of real world data. discrete latents USED-FOR Variational Autoencoders ( VAEs ). discrete VAEs COMPARE ones. ones COMPARE discrete VAEs. direct discrete optimization USED-FOR encoding model. direct discrete optimization USED-FOR discrete nature of the latents. reparameterization trick CONJUNCTION amortization. amortization CONJUNCTION reparameterization trick. sampling approximation CONJUNCTION reparameterization trick. reparameterization trick CONJUNCTION sampling approximation. amortization HYPONYM-OF VAE mechanisms. reparameterization trick HYPONYM-OF VAE mechanisms. sampling approximation HYPONYM-OF VAE mechanisms. truncated posteriors CONJUNCTION evolutionary algorithms. evolutionary algorithms CONJUNCTION truncated posteriors. variational setting USED-FOR Discrete optimization. approach USED-FOR evolutionary algorithms. evolutionary algorithms USED-FOR Discrete optimization. truncated posteriors USED-FOR variational setting. truncated posteriors USED-FOR Discrete optimization. decoder network USED-FOR latent states. gradient ascent USED-FOR network weights. gradient ascent USED-FOR discrete variational method. binary latents FEATURE-OF VAEs. discrete variational method USED-FOR VAEs. amortized training COMPARE direct discrete optimization. direct discrete optimization COMPARE amortized training. direct discrete optimization USED-FOR large neural networks. amortized training USED-FOR large neural networks. smaller networks USED-FOR direct optimization. direct optimization COMPARE zero - shot ’ learning. zero - shot ’ learning COMPARE direct optimization. large supervised neural networks COMPARE VAEs. VAEs COMPARE large supervised neural networks. sampling - based approximation CONJUNCTION reparameterization. reparameterization CONJUNCTION sampling - based approximation. approach USED-FOR training of VAEs. sampling - based approximation USED-FOR training of VAEs. direct optimization USED-FOR VAEs. direct optimization USED-FOR denoising. VAEs USED-FOR denoising. they COMPARE non - generative approaches. non - generative approaches COMPARE they. VAEs COMPARE non - generative approaches. non - generative approaches COMPARE VAEs. Method are VAE training, and small","Discrete latent variables are an important part of the generation process of real world data. Variational Autoencoders (VAEs) are a class of VAEs with discrete latents. The authors propose a discrete variational method to approximate the latent states of the decoder network. The approach is based on a combination of two VAE mechanisms: amortization and reparameterization trick. The main contribution of the paper is a direct discrete optimization of the encoding model, which is a variant of direct discrete optimizer. The direct optimization can be seen as an extension of direct optimization in zero-shot’ learning, where the network weights are computed by gradient ascent.  The authors show that direct optimization is able to achieve better performance than direct optimization on large neural networks and smaller networks.  They also show that they can achieve better results than non-generative approaches.   ","This paper proposes a novel approach to learn latent latent variables for the generation process of real world data. The authors propose to use discrete latents for Variational Autoencoders (VAEs), which are more efficient than discrete VAEs. The main idea is to use direct discrete optimization for the encoding model, where the latent states are represented by a decoder network, and the latents are represented as a discrete latent variable. Direct discrete optimization is motivated by the discrete nature of the Latents, which is not present in the original VAE training, and can be seen as an extension of direct optimization for smaller networks. The proposed approach is evaluated on a number of datasets, and compared to other VAE mechanisms such as amortization, reparameterization, sampling approximation, and evolutionary algorithms. The results show that the proposed discrete variational method outperforms VAEs on binary latents, and outperforms non-generative approaches on larger datasets. "
4671,SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,reward - based task EVALUATE-FOR approach. action - prediction USED-FOR methods. Controlled Effect Network ( CEN ) HYPONYM-OF unsupervised method. counterfactual measures of blame USED-FOR unsupervised method. it USED-FOR controlled effects. it PART-OF exploration method. CEN USED-FOR intrinsic motivator. it COMPARE action - prediction models. action - prediction models COMPARE it. CEN COMPARE action - prediction models. action - prediction models COMPARE CEN. Task is Identifying controllable aspects of the environment. Method is reinforcement learning agents. ,"This paper proposes a new unsupervised method called Controlled Effect Network (CEN), which is based on counterfactual measures of blame. The proposed approach is a reward-based task where the goal is to identify the controllable aspects of the environment. The authors show that CEN is a better exploration method than existing methods based on action-prediction. CEN can be used as an intrinsic motivator for reinforcement learning agents, and it can be applied to controlled effects. The experimental results show that it outperforms existing action-precision models.","The paper proposes an unsupervised method called Controlled Effect Network (CEN) which is based on counterfactual measures of blame. The idea is to use reinforcement learning agents to learn to identify controllable aspects of the environment. The approach is evaluated on a reward-based task. The authors show that CEN outperforms existing methods based on action-prediction and action-propagation. They also show that it is able to identify controlled effects in the environment, and that it can be used as an exploration method. The intrinsic motivator of CEN is a CEN. The paper also shows that it outperforms action-precision models."
4687,SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"they USED-FOR networks. neural architecture search CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION neural architecture search. neural architecture search HYPONYM-OF model compression techniques. knowledge distillation HYPONYM-OF model compression techniques. network pruning HYPONYM-OF model compression technique. it USED-FOR SR networks. filter pruning USED-FOR residual blocks. regularization FEATURE-OF pruned structure. regularization USED-FOR structure - regularized pruning ( SRP ). filters USED-FOR unimportant filters. residual USED-FOR layers. filters USED-FOR layers. SRP USED-FOR image SR networks. lightweight network SRPN - L CONJUNCTION SRPN. SRPN CONJUNCTION lightweight network SRPN - L. SRP USED-FOR lightweight network SRPN - L. SRPN - L CONJUNCTION SRPN. SRPN CONJUNCTION SRPN - L. OtherScientificTerm are moderate model size, and pruned filters. Generic is network. Method are L2 regularization, and lightweight and larger image SR networks. ","This paper proposes a new model compression technique called network pruning, which is a combination of neural architecture search and knowledge distillation. The main idea is to regularize the structure-regularized pruning (SRP) by using filters to remove unimportant filters from the network. The authors show that it can be used to improve the performance of SR networks by reducing the number of residual blocks in the residual. They also show that SRP can be applied to image SR networks such as lightweight network SRPN-L and SRPN.","This paper proposes a new model compression technique, network pruning, which is a combination of neural architecture search and knowledge distillation. The main idea is to regularize the pruned structure of the network with L2 regularization. The authors show that it can be applied to SR networks with moderate model size. The paper also shows that the filters can be used to prune the unimportant filters in the layers, and the residual blocks can be pruned with filter pruning. Experiments on lightweight and larger image SR networks show that the proposed SRP can improve the performance of the original SRPN and SRPN-L."
4703,SP:0dee45001ae9600f485614dfe6874a516ac01db5,"model USED-FOR few - shot learning methods. sparsely labeled novel category data USED-FOR model. abundantly labeled base category data USED-FOR model. abundantly labeled base category data USED-FOR few - shot learning methods. framework USED-FOR large domain shift. framework USED-FOR few - shot learning. contrastive loss FEATURE-OF base category data. feature extracting backbone USED-FOR framework. contrastive loss USED-FOR feature extracting backbone. masking module USED-FOR target domain classification. backbone USED-FOR features. backbone USED-FOR classifier. it EVALUATE-FOR framework. cross - domain few - shot learning benchmark EVALUATE-FOR it. cross - domain few - shot learning benchmark EVALUATE-FOR framework. framework COMPARE cross - domain methods. cross - domain methods COMPARE framework. framework COMPARE meta - learning approaches. meta - learning approaches COMPARE framework. meta - learning approaches COMPARE cross - domain methods. cross - domain methods COMPARE meta - learning approaches. Generic is methods. OtherScientificTerm are distant domain categories, and supervision. ","This paper proposes a new model for few-shot learning methods based on sparsely labeled novel category data. The model is based on the abundantly labeled base category data, which is used to train a model on the sparsely unlabeled domain categories. The proposed framework is able to handle large domain shift by using a contrastive loss on the base categories data and a feature extracting backbone to learn the features. The masking module is used for target domain classification and the backbone is used as a classifier. The authors show that it can achieve state-of-the-art performance on the cross-domain few-hot learning benchmark. The framework is also able to perform better than cross-domains methods and meta-learning approaches. ","This paper proposes a model for few-shot learning methods on sparsely labeled novel category data. The model is based on the recently proposed model on abundantly labeled base category data, and the authors propose a framework to address the large domain shift. The framework uses a feature extracting backbone to learn the features from the base categories using contrastive loss. The backbone is then used to train a classifier to predict the features of distant domain categories, and a masking module is used for target domain classification. Experiments show that the proposed framework outperforms existing meta-learning approaches and cross-domain methods on a few-domain few-Shot learning benchmark. "
4719,SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"gradient descent USED-FOR generalisation. networks USED-FOR gradient descent. infinite width networks CONJUNCTION finite width networks. finite width networks CONJUNCTION infinite width networks. Bayesian inference USED-FOR infinite width networks. gradient descent USED-FOR finite width networks. error COMPARE chance. chance COMPARE error. gradient descent USED-FOR functions. implicit biases of architecture CONJUNCTION gradient descent. gradient descent CONJUNCTION implicit biases of architecture. gradient descent PART-OF generalisation. implicit biases of architecture PART-OF generalisation. Method are neural networks, and network architecture. OtherScientificTerm are implicit bias of architecture, architecture, NNGP posterior, and minimum a posteriori functions. Metric is average test error. Generic is function. ","This paper studies the problem of generalization of neural networks with infinite width networks via Bayesian inference. The authors show that gradient descent can be used for generalisation of networks with finite width networks. The main contribution of the paper is to show that the implicit bias of architecture is a function of the width of the network, and that the NNGP posterior can be computed by gradient descent. The paper also shows that the average test error of gradient descent for functions with minimum a posteriori functions is the same as the error of chance. Finally, the paper provides a theoretical analysis of the effect of the implicit biases of architecture and gradient descent on generalisation.","The paper proposes a new generalisation framework for neural networks. The main idea is to use networks for gradient descent and finite width networks with Bayesian inference. The generalisation is based on the implicit bias of architecture, where the architecture is the NNGP posterior, and the function is the average test error. The paper shows that the error is lower than the chance, and that the generalisation can be achieved by gradient descent for functions with implicit biases of architecture. The authors also provide some theoretical analysis of the network architecture."
4735,SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"large - scale pre - trained multilingual representations USED-FOR cross - lingual transfer methods. transfer EVALUATE-FOR cross - lingual transfer methods. XTREME benchmark EVALUATE-FOR X - Mixup. X - Mixup COMPARE baselines. baselines COMPARE X - Mixup. text understanding tasks EVALUATE-FOR baselines. XTREME benchmark EVALUATE-FOR baselines. text understanding tasks EVALUATE-FOR X - Mixup. OtherScientificTerm are cross - lingual representation discrepancy, and representation discrepancy. Task is cross - lingual transfer. Generic is representations. ","This paper studies the problem of cross-lingual transfer in the context of large-scale pre-trained multilingual representations. In particular, the authors study the cross-linking representation discrepancy between the two representations. The authors show that X-Mixup performs better than other baselines on the XTREME benchmark on text understanding tasks. ",This paper proposes to use large-scale pre-trained multilingual representations for cross-lingual transfer methods. The key idea is to minimize the cross-laggingual representation discrepancy between the source and target languages. The authors show that X-Mixup outperforms other baselines on the XTREME benchmark on text understanding tasks. 
4751,SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"defenses USED-FOR attacks. robust algorithms USED-FOR heterogeneous datasets. bucketing scheme USED-FOR robust algorithms. bucketing CONJUNCTION robust algorithms. robust algorithms CONJUNCTION bucketing. robust algorithms USED-FOR attacks. bucketing USED-FOR attacks. guaranteed convergence FEATURE-OF non - iid Byzantine robust problem. Method are Byzantine robust distributed or federated learning, and machine learning model. Generic are algorithm, problem, and approach. OtherScientificTerm are heterogeneous ( non - iid ), and realistic assumptions. ","This paper studies Byzantine robust distributed or federated learning, where the goal is to defend against attacks on heterogeneous (non-iid) datasets. The authors propose a robust algorithms for heterogeneous datasets using the bucketing scheme, and show that the proposed robust algorithms are robust against attacks using bucketing and robust algorithms with robust algorithms without bucketing are robust to attacks. They also show that under certain realistic assumptions, the proposed algorithm achieves a guaranteed convergence in the non-Iid Byzantine robust problem. ",This paper proposes a Byzantine robust distributed or federated learning algorithm for heterogeneous (non-iid) data. The proposed approach is based on the bucketing scheme for training robust algorithms on heterogeneous datasets. The authors show that bucketing can be used to defend against attacks on robust algorithms and robust algorithms against attacks using bucketing. They also show that the proposed algorithm achieves guaranteed convergence in the non-Iid Byzantine robust problem under realistic assumptions. The paper also shows that the algorithm can be applied to any problem with a machine learning model.
4767,SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,disentanglement CONJUNCTION multi - task learning. multi - task learning CONJUNCTION disentanglement. hard parameter sharing USED-FOR multi - task learning. hard parameter sharing USED-FOR disentanglement. automatically generated supervised tasks USED-FOR neural networks. automatically generated supervised tasks USED-FOR representations. neural networks USED-FOR representations. disentanglement PART-OF multi - task neural network training. metrics EVALUATE-FOR disentanglement. Method is disentangled representations. Generic is they. Task is multi - task learning setting. ,"This paper studies the problem of disentanglement in multi-task learning. The authors propose a new metric for disentangled representations, which is based on hard parameter sharing between the representations learned by automatically generated supervised tasks for neural networks. The paper shows that the proposed metrics can be used to evaluate the performance of different types of representations trained with neural networks, and that they can be applied to a wide range of tasks. ",This paper studies the problem of disentanglement in multi-task learning. The main idea is to use hard parameter sharing to improve the performance of the disentangled representations. The authors propose to use automatically generated supervised tasks to train neural networks to disentangle the representations from the ones generated by the neural networks. They show that they are able to achieve better performance on a number of metrics. 
4783,SP:9851adb72e2918780f661f83f7da06eb866787be,Certifying Robust Policies ( CROP ) USED-FOR reinforcement learning. framework USED-FOR Certifying Robust Policies ( CROP ). framework USED-FOR state level robustness certification. adversarial state perturbations FEATURE-OF reinforcement learning. robustness CONJUNCTION lower bound of cumulative rewards. lower bound of cumulative rewards CONJUNCTION robustness. per - state actions CONJUNCTION lower bound of cumulative rewards. lower bound of cumulative rewards CONJUNCTION per - state actions. robustness CONJUNCTION per - state actions. per - state actions CONJUNCTION robustness. per - state actions HYPONYM-OF robustness certification criteria. lower bound of cumulative rewards HYPONYM-OF robustness certification criteria. robustness HYPONYM-OF robustness certification criteria. policy USED-FOR robustness. Gaussian noise USED-FOR Q - functions. Q - functions USED-FOR policy. policy USED-FOR local smoothing algorithm. Gaussian noise USED-FOR policy. global smoothing algorithm USED-FOR robustness. robustness FEATURE-OF finite - horizon cumulative reward. global smoothing algorithm USED-FOR finite - horizon cumulative reward. adversarial state perturbations FEATURE-OF finite - horizon cumulative reward. tight certification bounds FEATURE-OF reward. adaptive search USED-FOR tight certification bounds. adaptive search USED-FOR local smoothing approach. adversarial training CONJUNCTION regularization. regularization CONJUNCTION adversarial training. methods USED-FOR empirically robust RL. RL robustness certification framework EVALUATE-FOR methods. regularization HYPONYM-OF methods. adversarial training HYPONYM-OF methods. adversarial training USED-FOR empirically robust RL. Atari games EVALUATE-FOR methods. RegCVX CONJUNCTION RadialRL. RadialRL CONJUNCTION RegCVX. RegPGD CONJUNCTION RegCVX. RegCVX CONJUNCTION RegPGD. certified robustness EVALUATE-FOR RadialRL. adversarial attacks EVALUATE-FOR algorithms. OtherScientificTerm is cumulative rewards. Generic is trajectory. ,"This paper proposes a framework for certifying Robust Policies (CROP) in reinforcement learning with adversarial state perturbations. CROP uses a local smoothing algorithm with Gaussian noise to estimate the Q-functions of the policy, and then uses adaptive search to compute the lower bound of cumulative rewards and the robustness certification criteria such as robustness and per-state actions. The authors provide tight certification bounds for the reward of a finite-horizon cumulative reward under adversarial attacks. The proposed methods are evaluated on a variety of Atari games and demonstrate the effectiveness of the proposed methods for empirically robust RL. ","This paper proposes a framework for state level robustness certification for reinforcement learning. The framework is based on Certifying Robust Policies (CROP) which is an existing framework for certification of the robustness of policies in reinforcement learning under adversarial state perturbations. CROP is a simple framework for certifying the state-of-the-art methods for empirically robust RL. The key idea is to use Gaussian noise for the Q-functions of the policy, and then use adaptive search for the local smoothing approach. The authors show that the global smoothing algorithm improves robustness as well as the lower bound of cumulative rewards and per-state actions. The paper also shows that the proposed methods are robust to adversarial training and regularization. The proposed methods have been evaluated on a variety of Atari games, and show that they are robust against adversarial attacks. "
4799,SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"approach USED-FOR conformal prediction. conformal prediction USED-FOR model uncertainty. calibrated candidate set USED-FOR conformal prediction. in - silico screening USED-FOR drug discovery. coverage CONJUNCTION precision. precision CONJUNCTION coverage. natural language processing CONJUNCTION computer vision. computer vision CONJUNCTION natural language processing. computer vision CONJUNCTION computational chemistry. computational chemistry CONJUNCTION computer vision. natural language processing FEATURE-OF classification tasks. computational chemistry FEATURE-OF classification tasks. classification tasks EVALUATE-FOR approach. computer vision HYPONYM-OF classification tasks. OtherScientificTerm are coverage property, conformal sets, noisy candidates, predicted conformal sets, and user - specified tolerance. Task is large - scale settings. Generic are constraint, and algorithm. Metric is true positive rate. ","This paper proposes a new approach for conformal prediction in large-scale settings where the coverage property is not known. The proposed approach is based on a calibrated candidate set, which is used to reduce the model uncertainty in the presence of noisy candidates. The authors show that the proposed algorithm is able to achieve a true positive rate of $\epsilon$ with a small number of iterations. They also show that their algorithm can be applied to a variety of classification tasks including computer vision, natural language processing, and computational chemistry.","This paper proposes a new approach for conformal prediction to reduce model uncertainty. The proposed algorithm is based on in-silico screening for drug discovery, where the coverage property of the predicted conformal sets is the true positive rate, and the coverage and precision are the true negative rate. The paper also proposes a constraint on the number of noisy candidates, and a user-specified tolerance. Experiments on classification tasks such as computer vision and natural language processing demonstrate the effectiveness of the proposed algorithm."
4815,SP:b126d2f3c397633745c8833e22ace93a2470e963,complexity EVALUATE-FOR functions. neural network USED-FOR functions. unit - length curve USED-FOR network. random initialization USED-FOR ReLU networks. higher moments of the length distortion CONJUNCTION distortion of higher - dimensional volumes. distortion of higher - dimensional volumes CONJUNCTION higher moments of the length distortion. upper bounds USED-FOR higher moments of the length distortion. upper bounds USED-FOR distortion of higher - dimensional volumes. OtherScientificTerm is curve of outputs. Metric is expected length distortion. ,"This paper studies the problem of learning ReLU networks with random initialization. The authors propose a new unit-length curve for the network, which is a function of the number of parameters of the neural network. They show that the complexity of the functions can be reduced by using this curve of outputs. They also provide upper bounds for the higher moments of the length distortion and the distortion of higher-dimensional volumes. ",This paper studies the complexity of ReLU networks with random initialization. The main idea is to use a unit-length curve as a unit of the network. The authors show that this curve of outputs is more complex than the expected length distortion. They also show that the higher moments of the length distortion and the distortion of higher-dimensional volumes can be approximated by upper bounds. The paper is well-written and easy to follow. 
4831,SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"methods USED-FOR policies. safety constraints CONJUNCTION sparse rewards. sparse rewards CONJUNCTION safety constraints. learning policies PART-OF reinforcement learning ( RL ) problems. Behavioral priors USED-FOR RL. Behavioral priors USED-FOR policy primitives. behavioral priors USED-FOR safe policy learning. SAFEty skill pRiors ( SAFER ) HYPONYM-OF behavioral prior learning algorithm. policy learning USED-FOR complex control tasks. behavioral prior learning algorithm USED-FOR policy learning. SAFER USED-FOR safety variable. contrastive training USED-FOR SAFER. offline data USED-FOR safety requirements. safe and unsafe data USED-FOR contrastive training. abstract actions USED-FOR safe primitive skills. offline data USED-FOR safety variable. SAFER USED-FOR safe and successful policy. safety variable CONJUNCTION abstract action. abstract action CONJUNCTION safety variable. SAFER USED-FOR inference stage. safety variable USED-FOR safety skills. safety skills USED-FOR SAFER. safety skills USED-FOR safe and successful policy. SAFER USED-FOR policies. baseline methods USED-FOR policies. SAFER COMPARE baseline methods. baseline methods COMPARE SAFER. SAFER USED-FOR safety. policies CONJUNCTION safety. safety CONJUNCTION policies. baseline methods USED-FOR safety. Material is offline datasets. OtherScientificTerm are unsafe behavior, and undesirable behaviors. Task is complex safety - critical robotic grasping tasks. ","This paper studies the problem of learning policies in reinforcement learning (RL) problems with safety constraints and sparse rewards. Behavioral priors are commonly used in RL to learn policy primitives. The authors propose SAFEty skill pRiors (SAFER), a behavioral prior learning algorithm for safe policy learning with behavioral priors. SAFER uses offline data to learn the safety variable and abstract action, and contrastive training between safe and unsafe data to enforce the safety requirements. The paper shows that SAFER outperforms baseline methods for learning policies and policies with safety and safety skills in the inference stage. ","This paper proposes a behavioral prior learning algorithm, SAFEty skill pRiors (SAFER), for learning policies in reinforcement learning (RL) problems. SAFER is based on behavioral priors for RL, which are used to learn policy primitives. The authors show that SAFER can learn policies with safety constraints and sparse rewards. The safety variable is a combination of the safety variable and abstract action, and the safety skills are learned by contrastive training on both the safe and unsafe data. The proposed SAFER outperforms baseline methods for safety and for policies in the inference stage. "
4847,SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"multi - branch restoration model USED-FOR restoration tasks. Human Visual System USED-FOR multi - branch restoration model. Retinal Ganglion Cells HYPONYM-OF Human Visual System. deraindrop CONJUNCTION deblurring. deblurring CONJUNCTION deraindrop. image dehazing CONJUNCTION deraindrop. deraindrop CONJUNCTION image dehazing. datasets EVALUATE-FOR multi - branch architecture. CMFNet HYPONYM-OF multi - branch architecture. deblurring HYPONYM-OF datasets. deraindrop HYPONYM-OF datasets. image dehazing HYPONYM-OF datasets. pretrained models USED-FOR restoration tasks. Task are Image restoration, and autonomous cars. Method is learning based restoration methods. OtherScientificTerm is generalization. ","This paper proposes a multi-branch restoration model based on the Human Visual System, Retinal Ganglion Cells. The authors propose a new multi-branch architecture based on CMFNet, which can be applied to a variety of restoration tasks, including image restoration, image dehazing, deraindrop, deblurring, etc. The proposed method is evaluated on three different datasets and compared to other learning based restoration methods. The results show that the proposed method can achieve state-of-the-art performance on all three tasks.","The paper proposes a multi-branch restoration model for restoration tasks. The Human Visual System is an extension of the Retinal Ganglion Cells. Image restoration is an important problem in the field of learning based restoration methods. The authors propose to use the multi-branch architecture of CMFNet, which is based on the idea of generalization. The paper presents three datasets: image dehazing, deraindrop, and deblurring. The results show that the proposed pretrained models outperform the state-of-the-art restoration tasks on all three datasets. "
4863,SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"FL USED-FOR data heterogeneity. Personalized federated learning ( PFL ) USED-FOR data heterogeneity. FL USED-FOR Personalized federated learning ( PFL ). FL CONJUNCTION PFL. PFL CONJUNCTION FL. unlabeled clients USED-FOR model. hypernetwork module CONJUNCTION encoder module. encoder module CONJUNCTION hypernetwork module. approach USED-FOR IT - PFLHN. encoder module USED-FOR approach. hypernetwork module USED-FOR approach. encoder module USED-FOR IT - PFLHN. hypernetwork module USED-FOR IT - PFLHN. hypernetwork USED-FOR personalized model. client representation PART-OF hypernetwork. benchmark datasets EVALUATE-FOR IT - PFL - HN. IT - PFL - HN COMPARE FL and PFL methods. FL and PFL methods COMPARE IT - PFL - HN. benchmark datasets EVALUATE-FOR FL and PFL methods. multi - task learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION multi - task learning. multi - task learning USED-FOR it. Method are Federated learning ( FL ), personalized models, prediction service, and encoder network. Material is labeled data. OtherScientificTerm are unlabeled data, large domain shift, data privacy, and differential privacy. Generic is learning setup. Metric is generalization error. ","This paper proposes a personalized federated learning (PFL) approach to tackle the problem of data heterogeneity in PFL. The authors propose a new approach called IT-PFLHN, which uses a hypernetwork module and an encoder module to learn a personalized model from unlabeled clients. The hypernetwork is a client representation of the personalized model and the encoder network is a prediction service. The proposed approach is evaluated on three benchmark datasets and compared to FL and PFL methods. The results show that it outperforms FL in terms of multi-task learning and domain adaptation.","This paper proposes Personalized federated learning (PFL) to address data heterogeneity in the learning setup. The authors propose a new approach called IT-PFLHN, which uses a hypernetwork module and an encoder module to learn a personalized model with unlabeled clients. The hypernetwork is a client representation of the personalized models, and the encoder network is a prediction service. The proposed approach is evaluated on three benchmark datasets and compared to FL and PFL methods on multi-task learning and domain adaptation. "
4879,SP:960d0a63a82593f6e72275b65f0501f0469d1924,"classification PART-OF selfsupervised learning. conditional diffusion based generative model ( RCDM ) USED-FOR representations. self - supervised models USED-FOR representations. model COMPARE generative models. generative models COMPARE model. generation quality COMPARE generative models. generative models COMPARE generation quality. generation quality EVALUATE-FOR model. tool USED-FOR self - supervised models. SSL projector embedding USED-FOR tasks. classifications HYPONYM-OF tasks. SSL model USED-FOR image manipulation. inherent structure USED-FOR image manipulation. SSL model USED-FOR inherent structure. supervised representation CONJUNCTION SSL representation. SSL representation CONJUNCTION supervised representation. Method are neural networks, SSL ( backbone ) representation, and SSL representations. Generic is representation. OtherScientificTerm is conditioning. ","This paper proposes a conditional diffusion based generative model (RCDM) to learn representations from self-supervised models. The proposed model is able to achieve better generation quality than existing generative models, and is also able to generate representations that are more interpretable than neural networks. The authors demonstrate that the proposed SSL (backbone) representation can be used to learn SSL representations from a supervised representation and an SSL representation from an SSL model. The SSL projector embedding is also used to perform tasks such as classifications. ","This paper proposes a conditional diffusion based generative model (RCDM) to generate representations for selfsupervised learning. The model is compared to other generative models in terms of generation quality. The authors also propose a tool to train self-supervised models on SSL projector embedding for tasks such as classifications, image manipulation, and conditioning. The SSL (backbone) representation is used to generate SSL representations. The supervised representation and SSL representation are used to learn the inherent structure of the SSL model."
4895,SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,Fp sketch HYPONYM-OF well - celebrated streaming algorithm. well - celebrated streaming algorithm USED-FOR frequency moments estimation. DP baselines COMPARE non - private baseline. non - private baseline COMPARE DP baselines. Fp sketch COMPARE DP baselines. DP baselines COMPARE Fp sketch. Fp sketch COMPARE non - private baseline. non - private baseline COMPARE Fp sketch. logarithmic factor USED-FOR non - private baseline. polylogarithmic space USED-FOR Fp sketch. differential privacy guarantee FEATURE-OF accuracy. differential privacy guarantee FEATURE-OF Fp sketch. accuracy EVALUATE-FOR Fp sketch. OtherScientificTerm is evaluation code. ,"This paper proposes a well-celebrated streaming algorithm called Fp sketch, which is a variant of the well-computed streaming algorithm for frequency moments estimation. The key idea is to use the logarithmic factor of the non-private baseline as a proxy for the evaluation code. The authors show that Fp sketches can achieve better accuracy than DP baselines in terms of differential privacy guarantee compared to the standard DP baseline. ","This paper proposes a well-celebrated streaming algorithm for frequency moments estimation, called Fp sketch. The main idea is to use a logarithmic factor as the non-private baseline, and then use the DP baselines as the private baseline. The evaluation code is computed in polylogarithmics space. The differential privacy guarantee of Fp sketches is shown to improve the accuracy."
4911,SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"paradigm USED-FOR diverse strategies. Reward - Switching Policy Optimization ( RSPO ) USED-FOR diverse strategies. diverse strategies USED-FOR complex RL environments. Reward - Switching Policy Optimization ( RSPO ) HYPONYM-OF paradigm. trajectory - based novelty measurement USED-FOR optimization process. RSPO USED-FOR extrinsic and intrinsic rewards. trajectory - based novelty measurement USED-FOR RSPO. trajectory - based novelty measurement USED-FOR extrinsic and intrinsic rewards. policy optimization USED-FOR RSPO. extrinsic rewards USED-FOR policy optimization. extrinsic rewards USED-FOR RSPO. intrinsic diversity reward USED-FOR exploration. RSPO USED-FOR exploration. RSPO USED-FOR trajectories. policies USED-FOR trajectories. intrinsic diversity reward USED-FOR RSPO. single - agent particle - world tasks CONJUNCTION MuJoCo continuous control. MuJoCo continuous control CONJUNCTION single - agent particle - world tasks. multi - agent stag - hunt games CONJUNCTION StarCraftII challenges. StarCraftII challenges CONJUNCTION multi - agent stag - hunt games. RSPO USED-FOR strategies. MuJoCo continuous control CONJUNCTION multi - agent stag - hunt games. multi - agent stag - hunt games CONJUNCTION MuJoCo continuous control. single - agent particle - world tasks CONJUNCTION multi - agent stag - hunt games. multi - agent stag - hunt games CONJUNCTION single - agent particle - world tasks. OtherScientificTerm are learning policy, and sampled trajectory. ","This paper proposes a new paradigm for diverse strategies in complex RL environments. Reward-Switching Policy Optimization (RSPO) is a paradigm that uses trajectory-based novelty measurement for the optimization process. RSPO uses extrinsic rewards for policy optimization and intrinsic rewards for exploration. The learning policy learns a sampled trajectory from the sampled trajectory, and the goal is to maximize the intrinsic diversity reward between the trajectories learned by the two policies. The authors show that RSPo is able to learn trajectories that are more diverse than existing strategies, and that the learned trajectories can be used for exploration and policy optimization. Experiments are conducted on single-agent particle-world tasks, MuJoCo continuous control, multi-agent stag-hunt games, and StarCraftII challenges.","The paper proposes a paradigm for diverse strategies in complex RL environments. The key idea is to use Reward-Switching Policy Optimization (RSPO) to learn diverse strategies. RSPO uses extrinsic rewards for policy optimization and trajectory-based novelty measurement for the optimization process. The learning policy is learned by sampling from a sampled trajectory. The exploration is done by using intrinsic diversity reward for exploration. The authors show that RSPo can learn trajectories that are more diverse than existing policies. The paper also shows that the strategies learned by RSPOO can be used to learn new strategies. The experiments are conducted on single-agent particle-world tasks, MuJoCo continuous control, and multi-agent stag-hunt games."
4927,SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"Diffusion models HYPONYM-OF generative models. GANs COMPARE autoregressive models. autoregressive models COMPARE GANs. sample quality CONJUNCTION autoregressive models. autoregressive models CONJUNCTION sample quality. autoregressive models USED-FOR likelihood scores. GANs HYPONYM-OF generative models. autoregressive models HYPONYM-OF generative models. sample quality EVALUATE-FOR GANs. method USED-FOR fast samplers. fast samplers USED-FOR diffusion model. flexible non - Markovian samplers USED-FOR diffusion models. Generalized Gaussian Diffusion Models ( GGDM ) HYPONYM-OF flexible non - Markovian samplers. degrees of freedom FEATURE-OF GGDM samplers. sample quality scores USED-FOR GGDM samplers. gradient descent USED-FOR sample quality scores. sample quality scores USED-FOR degrees of freedom. gradient descent USED-FOR GGDM samplers. reparametrization trick CONJUNCTION gradient rematerialization. gradient rematerialization CONJUNCTION reparametrization trick. sampling process USED-FOR optimization procedure. reparametrization trick USED-FOR optimization procedure. gradient rematerialization USED-FOR optimization procedure. DDSS USED-FOR unconditional image generation. datasets EVALUATE-FOR DDSS. FID scores HYPONYM-OF datasets. fine - tuning CONJUNCTION re - training. re - training CONJUNCTION fine - tuning. method CONJUNCTION pre - trained diffusion model. pre - trained diffusion model CONJUNCTION method. Generic is model. OtherScientificTerm are LSUN, and inference steps. Method is DDPM / DDIM baselines. ","This paper studies diffusion models, a class of generative models that can be seen as a combination of GANs and autoregressive models. The authors propose a new sampling process for the optimization procedure based on gradient rematerialization and reparametrization trick. The diffusion model is trained using flexible non-Markovian samplers (GGDM). The authors show that the sample quality scores of the GGDM sampler can be used to improve sample quality of the GAN, and that the likelihood scores obtained by the diffusion model can be improved by the proposed method.  The authors also show that their method can be applied to a pre-trained diffusion model as well as a fast sampler.  ","Diffusion models are widely used in generative models such as GANs and autoregressive models to improve sample quality and improve the likelihood scores. However, diffusion models with flexible non-Markovian samplers such as Generalized Gaussian Diffusion Models (GGDM) are not widely used. The authors propose a new method to train a diffusion model with fast sampler that can be used in combination with existing methods such as DDPM/DDIM baselines. The proposed method is based on the reparametrization trick and gradient rematerialization. The sampling process of the optimization procedure is done in a sampling process where the model is trained using LSUN, and inference steps are performed using a pre-trained diffusion model.  The authors show that the gradient descent improves the sample quality scores of GGDM Samplers and the degrees of freedom of the models. They also show that DDSS is able to perform unconditional image generation on several datasets such as FID scores, fine-tuning and re-training. "
4943,SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,Large Language Models ( LLMs ) USED-FOR factual information. embedding layer PART-OF LLMs. lightweight models HYPONYM-OF P - Adapters. embedding layer PART-OF lightweight models. continuous prompts USED-FOR LLM. LLM embeddings USED-FOR continuous prompts. LLM embeddings USED-FOR They. Mixture of Experts ( MoE ) models USED-FOR continuous prompts. classifier USED-FOR natural language prompts. classifier USED-FOR They. human - annotated data USED-FOR classifier. P - Adapters COMPARE MoE models. MoE models COMPARE P - Adapters. MoE models USED-FOR factual information. P - Adapters USED-FOR factual information. consistency EVALUATE-FOR baseline. PAdapters COMPARE baseline. baseline COMPARE PAdapters. precision CONJUNCTION consistency. consistency CONJUNCTION precision. consistency EVALUATE-FOR PAdapters. natural language queries USED-FOR baseline. precision EVALUATE-FOR PAdapters. LLM ’s embeddings USED-FOR natural language prompt. OtherScientificTerm is continuous ones. Method is P - Adapter. ,"This paper proposes a new lightweight model for P-Adapters, called P-Adapter, which is a combination of lightweight models (e.g., lightweight models) and LLMs. The main idea is to use LLM’s embeddings to generate a natural language prompt for the P-adapters, and then use the embedding layer of the LLMs to encode the factual information. The authors show that the PAdapters are able to achieve better precision and consistency than the baseline on natural language queries. ","This paper proposes a new class of lightweight models, called P-Adapters, which are based on lightweight models. The authors propose to use LLM embeddings in the embedding layer of LLMs to extract factual information from continuous prompts. They use Mixture of Experts (MoE) models to generate the continuous prompts and use a classifier to generate natural language prompts from human-annotated data. The P-Adapter is trained on the P- Adapter and the classifier is used to generate a natural language prompt from the LLM’s embedding. They show that the PAdapters outperform MoE models for generating factual information. They also show that they are able to achieve better precision and consistency compared to the baseline on natural language queries."
4959,SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"one - shot classification COMPARE CCTS. CCTS COMPARE one - shot classification. catastrophic forgetting CONJUNCTION over fitting. over fitting CONJUNCTION catastrophic forgetting. unclear distribution division FEATURE-OF continual learning task. continual learning task USED-FOR CCTS. Adaptive model training policy USED-FOR CCTS. Adaptive multi - distribution extraction policy PART-OF adaptability. fixed rules CONJUNCTION prior knowledge. prior knowledge CONJUNCTION fixed rules. ACCTS USED-FOR data distributions. data distributions USED-FOR time series evolution. time series evolution CONJUNCTION model change. model change CONJUNCTION time series evolution. method COMPARE baselines. baselines COMPARE method. real - world datasets EVALUATE-FOR method. Task is real - world applications. OtherScientificTerm are vital signs, features, multi - distribution form, independent identically distributed premise, and fixed division rule. Generic are concept, models, and model. Method are Continuous Classification of Time Series ( CCTS ), and Adaptive importance - based replay policy. ","This paper proposes a new continual learning task, Continuous Classification of Time Series (CCTS), where the goal is to learn the vital signs in a multi-distribution form. CCTS is an extension of one-shot classification, where the key idea is to train a model on a set of data distributions, and then use the learned model to predict the next time series. The authors propose a new concept, called Adaptive importance-based replay policy, which is based on Adaptive Multi-Distribution Extraction Policy (AMDP). The authors show that the adaptability of the Adaptive model training policy can be improved by adapting to different data distributions. They also show that their method can achieve better performance on real-world datasets than baselines.","The paper proposes a new continual learning task, Continuous Classification of Time Series (CCTS), where the goal is to classify time series with vital signs. The key idea is to learn a multi-distribution form of the time series, where each time series is represented by a fixed division rule. The authors show that CCTS is more robust to catastrophic forgetting and overfitting compared to one-shot classification. The adaptability is achieved by adapting the Adaptive model training policy to the CCCS. The proposed method is evaluated on two real-world applications: time series evolution and model change. "
4975,SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,internal representations of past inputs USED-FOR language models. approximate kNN lookup USED-FOR language modeling. benchmarks CONJUNCTION tasks. tasks CONJUNCTION benchmarks. approximate kNN lookup PART-OF memory. tasks EVALUATE-FOR language modeling. benchmarks EVALUATE-FOR language modeling. generic webtext ( C4 ) HYPONYM-OF tasks. books ( PG-19 ) HYPONYM-OF tasks. books ( PG-19 ) HYPONYM-OF benchmarks. generic webtext ( C4 ) HYPONYM-OF benchmarks. theorems USED-FOR model. Method is Language models. ,"This paper studies the problem of language models with internal representations of past inputs. The authors propose to use approximate kNN lookup in the memory of the language models to improve the performance on a variety of benchmarks, including generic webtext (C4) and books (PG-19) for language modeling. Language models are trained using theorems from the literature, and the authors show that the model is able to perform well on these benchmarks.","This paper proposes to use internal representations of past inputs in language models to improve the performance of language models. The authors propose to use approximate kNN lookup in memory for language modeling. The model is trained using theorems from the literature and evaluated on a variety of benchmarks, including generic webtext (C4) and books (PG-19) to demonstrate the effectiveness of the proposed model. "
4991,SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"MLMs USED-FOR probability distribution. masked language modeling ( MLM ) objective USED-FOR models. energy - based sequence models USED-FOR MLMs. MLMs USED-FOR energy parametrizations. Metropolis – Hastings Monte Carlo algorithm USED-FOR tractable sampling scheme. masked conditionals USED-FOR masked language models. energybased models USED-FOR open - ended unconditional generation. approach COMPARE undirected generation approaches. undirected generation approaches COMPARE approach. stationary distribution FEATURE-OF Markov chain. Method are parametrizations, and sampling algorithm. Task is machine translation. ","This paper proposes a new masked language modeling (MLM) objective for models trained with energy-based sequence models. The authors propose a tractable sampling scheme based on the Metropolis-Hastings Monte Carlo algorithm. The sampling algorithm is based on two parametrizations: (1) the stationary distribution of the Markov chain, and (2) the probability distribution. The proposed approach is shown to perform better than undirected generation approaches. ",This paper proposes a masked language modeling (MLM) objective for models that can be used for machine translation. The authors propose a tractable sampling scheme based on the Metropolis-Hastings Monte Carlo algorithm. The proposed approach is compared to undirected generation approaches and is shown to outperform them in terms of the stationary distribution of the Markov chain.  The authors also propose a sampling algorithm based on energy-based sequence models for MLMs to learn energy parametrizations for the probability distribution. They show that the proposed approach outperforms undirecting generation approaches on open-ended unconditional generation. 
5007,SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"data augmentation USED-FOR deep neural networks. deep neural networks USED-FOR NLP tasks. data augmentation USED-FOR NLP tasks. labeled samples USED-FOR It. low - data or classimbalanced regimes HYPONYM-OF labeled samples. parameter tuning CONJUNCTION inherent randomness. inherent randomness CONJUNCTION parameter tuning. inherent randomness USED-FOR augmentation techniques. parameter tuning USED-FOR augmentation techniques. reward function USED-FOR policy. augmentation USED-FOR NLP tasks. augmentation strategy USED-FOR task. learning data augmentation policy USED-FOR augmentation strategy. reward function USED-FOR augmentation policy. learning - based augmentation COMPARE augmentation schemes. augmentation schemes COMPARE learning - based augmentation. augmentations USED-FOR task. text classification tasks EVALUATE-FOR augmentation schemes. text classification tasks EVALUATE-FOR learning - based augmentation. augmentation policy USED-FOR tasks. method USED-FOR low - data and class - imbalanced regimes. OtherScientificTerm are informative training signals, and semantic similarity. Method are data augmentation policy, and sample re - weighting scheme. Generic is model. ","This paper studies the problem of data augmentation in deep neural networks for NLP tasks. It focuses on low-data or classimbalanced regimes with labeled samples, where informative training signals are not available. It proposes an augmentation strategy for each task based on a learning data augmentmentation policy based on the learned augmentation policy. The proposed policy is based on learning a reward function to encourage the policy to perform well on each task. The paper also proposes a sample re-weighting scheme to improve the performance of the model. Experiments on text classification tasks show that the proposed augmentation schemes perform better than learning-based augmentation on the same task. ","This paper proposes a new data augmentation for deep neural networks for NLP tasks. It uses labeled samples from the low-data or classimbalanced regimes, where informative training signals are not available, and the model is trained using a sample re-weighting scheme. The proposed augmentation techniques are based on parameter tuning and inherent randomness. The authors propose an augmentation strategy for each task that uses a learning data augment policy and a reward function to guide the augmentation policy. The method is evaluated on text classification tasks and shows that the proposed method outperforms learning-based augmentation on both low-datasets and class-imbalanced regime. "
5023,SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"Meta - learning USED-FOR offline reinforcement learning ( OMRL ). augmented state USED-FOR task identity. intra - task attention mechanism CONJUNCTION inter - task contrastive learning objectives. inter - task contrastive learning objectives CONJUNCTION intra - task attention mechanism. sparse reward CONJUNCTION distribution shift. distribution shift CONJUNCTION sparse reward. sparse reward USED-FOR task representation learning. distribution shift USED-FOR task representation learning. FOCAL HYPONYM-OF SOTA OMRL algorithms. meta - RL benchmarks EVALUATE-FOR prior algorithms. Method are RL algorithms, and context - based encoder. ","Meta-learning is an important problem in offline reinforcement learning (OMRL). The augmented state of the task identity is important for RL algorithms. The paper proposes to use the intra-task attention mechanism and inter-task contrastive learning objectives as well as a sparse reward for task representation learning. The proposed SOTA OMRL algorithms, FOCAL, are evaluated on a variety of meta-RL benchmarks. ","Meta-learning is an important problem in offline reinforcement learning (OMRL) where the goal is to learn a task identity. The authors propose two RL algorithms, FOCAL and SOTA OMRL algorithms, which are based on the augmented state of the task identity, which is a context-based encoder. The intra-task attention mechanism and the inter-task contrastive learning objectives are also proposed. The prior algorithms are evaluated on meta-RL benchmarks. The distribution shift and sparse reward are used for task representation learning."
5039,SP:ed86c60850d5c8302dcf1c2167db303e778fe681,belief state FEATURE-OF partially observable Markov system. parametric sequential generative modeling methods USED-FOR problem setting. methods USED-FOR belief state modeling. belief state modeling USED-FOR multi - agent settings. policies PART-OF belief model. inference - time improvement framework USED-FOR parametric sequential generative modeling methods. belief fine - tuning ( BFT ) HYPONYM-OF inference - time improvement framework. approximate dynamic programming USED-FOR model parameters. fine - tuning USED-FOR model parameters. BFT USED-FOR model parameters. fine - tuning FEATURE-OF approximate dynamic programming. fine - tuning USED-FOR BFT. approximate dynamic programming USED-FOR BFT. accuracy EVALUATE-FOR belief model. It USED-FOR belief model. it USED-FOR model. accuracy EVALUATE-FOR It. belief model USED-FOR BFT. BFT USED-FOR approximate public belief state search. imperfect - information games FEATURE-OF approximate public belief state search. Method is dynamics model. OtherScientificTerm is specialization. ,This paper proposes a new inference-time improvement framework for parametric sequential generative modeling methods for the problem setting where the belief state of a partially observable Markov system is unknown. The authors propose two methods for belief state modeling in multi-agent settings: (1) belief fine-tuning (BFT) and (2) approximate dynamic programming to fine-tune the model parameters. BFT improves the accuracy of the belief model in the approximate public belief state search in imperfect-information games. It also improves the performance of the model when the dynamics model is trained with specialization.,This paper proposes an inference-time improvement framework for parametric sequential generative modeling methods for the problem setting of partially observable Markov system. The authors propose two methods for belief state modeling for multi-agent settings. The first one is belief fine-tuning (BFT) which is an extension of the existing belief model. The second one is a variant of BFT where the model parameters are learned by approximate dynamic programming. It is shown that it improves the accuracy of the belief model in the context of approximate public belief state search in imperfect-information games. 
5055,SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"accuracy loss CONJUNCTION slow training runtime. slow training runtime CONJUNCTION accuracy loss. accuracy loss EVALUATE-FOR methods. sparse matrices USED-FOR sparsity mask. fixed structure FEATURE-OF sparse matrices. products of butterfly matrices HYPONYM-OF fixed structure. hardware USED-FOR butterfly ( block and flat ). attention CONJUNCTION MLP. MLP CONJUNCTION attention. fixed sparsity pattern USED-FOR network layers. method USED-FOR network layers. Pixelated Butterfly USED-FOR method. flat block butterfly and low - rank matrices USED-FOR fixed sparsity pattern. fixed sparsity pattern USED-FOR Pixelated Butterfly. fixed sparsity pattern USED-FOR method. flat block butterfly and low - rank matrices USED-FOR method. attention HYPONYM-OF network layers. MLP HYPONYM-OF network layers. Pixelated Butterfly COMPARE butterfly. butterfly COMPARE Pixelated Butterfly. Pixelated Butterfly USED-FOR training. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR sparse models. dense MLP - Mixer CONJUNCTION Vision Transformer. Vision Transformer CONJUNCTION dense MLP - Mixer. sparse models COMPARE dense MLP - Mixer. dense MLP - Mixer COMPARE sparse models. Vision Transformer CONJUNCTION GPT-2 medium. GPT-2 medium CONJUNCTION Vision Transformer. sparse models COMPARE GPT-2 medium. GPT-2 medium COMPARE sparse models. sparse models COMPARE Vision Transformer. Vision Transformer COMPARE sparse models. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR dense MLP - Mixer. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR GPT-2 medium. accuracy EVALUATE-FOR sparse models. Method are Overparameterized neural networks, Sparse model training, and model components. Metric are computational cost, and accuracy – efficiency tradeoffs. OtherScientificTerm is butterfly matrices. ","This paper studies the problem of overparameterized neural networks. Sparse model training can be expensive due to the computational cost and slow training runtime. The authors propose a new method called Pixelated Butterfly, which uses sparse matrices as the sparsity mask. The proposed method uses a fixed sparsity pattern to train the network layers (e.g., attention, MLP). The fixed structure is composed of products of butterfly matrices. The paper shows that the proposed method outperforms dense MLP-Mixer and GPT-2 medium on ImageNet classification and WikiText-103 language modeling tasks. ","The paper proposes a new method for training Overparameterized neural networks. Sparse model training is an important problem in the context of learning sparse matrices for sparsity mask. The paper proposes to use hardware for butterfly (block and flat) matrices, which are the products of butterfly matrices. The proposed method uses a fixed sparsity pattern on the network layers (e.g., attention, MLP) and network layers. The method is evaluated on ImageNet classification and WikiText-103 language modeling tasks with sparse models (Vision Transformer, GPT-2 medium, and dense MLP-Mixer). The accuracy loss and slow training runtime of the proposed methods are shown to be competitive with existing methods in terms of accuracy loss, but the computational cost is much smaller. The authors also show that the proposed method can be applied to any network layers, including MLP, attention, and MLP. "
5071,SP:136e31054a55abca840f6478491972023c2296cb,"score matching USED-FOR data distribution. formulation USED-FOR controllable generation. class center PART-OF forward and reverse process. class center USED-FOR conditional diffusion probabilistic model. faster sampling USED-FOR method. inception score CONJUNCTION FID score. FID score CONJUNCTION inception score. state - of - the - art methods COMPARE conditional image generation. conditional image generation COMPARE state - of - the - art methods. framework COMPARE state - of - the - art methods. state - of - the - art methods COMPARE framework. CIFAR-10 USED-FOR conditional image generation. FID score EVALUATE-FOR conditional image generation. inception score EVALUATE-FOR conditional image generation. Method are Score - based generative models, and diffusion probabilistic models. OtherScientificTerm are Markov chain, and class clustering phenomenon. ","This paper proposes a score matching method for controllable generative models. The key idea is to use the Markov chain as the class center of a conditional diffusion probabilistic model, and then use the score matching to learn the data distribution. The method is based on faster sampling. The authors show that the proposed framework outperforms state-of-the-art methods on CIFAR-10 and FID score for conditional image generation. ","This paper proposes a novel formulation for controllable generation of images from a Markov chain. Score-based generative models can be seen as a variant of diffusion probabilistic models. The key idea is to use score matching to learn the data distribution. The proposed method is based on faster sampling, where the class center of the forward and reverse process is the same. The authors show that the proposed framework outperforms state-of-the-art methods on CIFAR-10 for conditional image generation and FID score. "
5087,SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"fixed domain - invariant features CONJUNCTION common hypotheses. common hypotheses CONJUNCTION fixed domain - invariant features. generalization EVALUATE-FOR domain generalization ( DG ) approaches. generalization EVALUATE-FOR prediction tasks. label - informative features USED-FOR label prediction task. label - informative features USED-FOR latent sub - spaces. DG benchmarks EVALUATE-FOR it. DG benchmarks EVALUATE-FOR method. Metric is generalization capacity. Generic are assumption, and approaches. OtherScientificTerm are invariant hypothesis, and sub - spaces. Method is LASSO. ","This paper studies the problem of domain generalization (DGD) approaches with fixed domain-invariant features and common hypotheses. The assumption is that the invariant hypothesis is invariant in the latent sub-spaces, and the label-informative features are invariant for the label prediction task. The authors propose a new method, LASSO, which uses label-influential features to learn latent subspaces. The proposed method is evaluated on a variety of DG benchmarks and shows that it achieves state-of-the-art generalization performance.","This paper studies the generalization capacity of domain generalization (DG) approaches. The assumption is that the fixed domain-invariant features and common hypotheses are invariant. The authors show that the invariant hypothesis holds for all sub-spaces of the domain, and the assumption holds for the sub-space that is invariant to the label prediction task with label-informative features. They show that this assumption holds under certain assumptions, and show that existing approaches do not generalize well. They propose a new method, LASSO, which is evaluated on a number of DG benchmarks."
5103,SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"kernel thinning ( KT ) algorithm USED-FOR probability distribution. kernel thinning ( KT ) algorithm COMPARE independent sampling. independent sampling COMPARE kernel thinning ( KT ) algorithm. reproducing kernel Hilbert space ( RKHS ) USED-FOR independent sampling. KT USED-FOR RKHS. kernel CONJUNCTION distribution. distribution CONJUNCTION kernel. KT USED-FOR kernel. KT USED-FOR dimension - free guarantees. dimension - free guarantees FEATURE-OF kernel. inverse multiquadric CONJUNCTION sinc. sinc CONJUNCTION inverse multiquadric. Gaussian CONJUNCTION inverse multiquadric. inverse multiquadric CONJUNCTION Gaussian. target KT COMPARE square - root KT. square - root KT COMPARE target KT. maximum mean discrepancy ( MMD ) guarantees EVALUATE-FOR square - root KT. target KT HYPONYM-OF analytic kernels. maximum mean discrepancy ( MMD ) guarantees EVALUATE-FOR target KT. sinc HYPONYM-OF analytic kernels. Gaussian HYPONYM-OF analytic kernels. inverse multiquadric HYPONYM-OF analytic kernels. Laplace CONJUNCTION Matérn. Matérn CONJUNCTION Laplace. fractional power kernel USED-FOR KT. Laplace HYPONYM-OF non - smooth kernels. Matérn HYPONYM-OF non - smooth kernels. MMD guarantees CONJUNCTION individual function guarantees. individual function guarantees CONJUNCTION MMD guarantees. KT USED-FOR target and power kernels. individual function guarantees FEATURE-OF target KT. target KT CONJUNCTION KT+. KT+ CONJUNCTION target KT. integration error EVALUATE-FOR target KT. integration error EVALUATE-FOR KT+. OtherScientificTerm are square - root kernel, square - roots, and differential equation posteriors. ",This paper studies the problem of reproducing kernel Hilbert space (RKHS) in the context of independent sampling. The authors propose a kernel thinning (KT) algorithm for estimating the probability distribution of a kernel with dimension-free guarantees. They show that KT can be used to approximate the RKHS in the case of a square-root kernel. They also show that the target KT has the same maximum mean discrepancy (MMD) guarantees as the squared-root KT.   The authors also provide some theoretical guarantees for the target and power kernels. ,"This paper proposes reproducing kernel Hilbert space (RKHS) as an alternative to independent sampling in the context of kernel thinning (KT) algorithm for estimating the probability distribution of the kernel. The authors show that the square-root kernel, square-roots, and the inverse multiquadric of the RKHS can be used as independent sampling. The paper also provides dimension-free guarantees for the kernel and its distribution.  The authors also provide a theoretical analysis of the KHS and show that KT is a good choice for reproducing RKHHS. The main contribution of the paper is to show that using the fractional power kernel for the target and power kernels is a better choice than using a fractional number of kernels. The theoretical analysis is complemented by numerical experiments on the differential equation posteriors."
5119,SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"Combinatorial optimization USED-FOR real - world problems. open - source benchmark suite USED-FOR NP - hard MAXIMUM INDEPENDENT SET problem. weighted and unweighted variants FEATURE-OF open - source benchmark suite. benchmark suite USED-FOR guided tree search algorithm. graph convolution network USED-FOR tree search. code quality CONJUNCTION extensibility. extensibility CONJUNCTION code quality. graph convolution network USED-FOR solution structure. random values USED-FOR graph convolution network. extensibility EVALUATE-FOR algorithm. code quality EVALUATE-FOR algorithm. graph kernelization HYPONYM-OF algorithmic techniques. algorithmic techniques USED-FOR tree search. tree search implementations COMPARE solvers. solvers COMPARE tree search implementations. competitive solution quality EVALUATE-FOR GNN. GNN USED-FOR solver. reinforcement learning USED-FOR solver. Method are graph neural networks ( GNNs ), machine learning - based solvers, and classical algorithmic solvers. OtherScientificTerm are NP - hard problems, and problem - specific solution structure. Generic is suite. ","This paper studies the problem of tree search in graph neural networks (GNNs), where the goal is to find a solution to a set of NP-hard problems. The authors propose a guided tree search algorithm based on the benchmark suite of weighted and unweighted variants of the open-source benchmark suite for the NP-Hard MAXIMUM inDEPENDENT SET problem. The proposed algorithm is based on a graph convolution network that learns a solution structure from random values, and then uses a GNN to train a solver based on reinforcement learning. The algorithm achieves competitive solution quality and extensibility in terms of code quality and code size. The paper also shows that the proposed tree search is competitive with other algorithmic techniques such as graph kernelization. ",This paper presents an open-source benchmark suite for the NP-hard MAXIMUM in the setting of real-world problems. The authors propose a guided tree search algorithm based on a graph convolution network with weighted and unweighted variants of the problem-specific solution structure. The algorithm is evaluated on both code quality and extensibility. The paper shows that the proposed GNN outperforms classical algorithmic solvers in terms of competitive solution quality and code quality. 
5135,SP:155ecd17d264a084b014abdfd0362146d8fb07e0,Quantization USED-FOR compressing Convolutional Neural Networks ( CNNs ). computational resources USED-FOR compressing Convolutional Neural Networks ( CNNs ). semantic segmentation CONJUNCTION depth prediction. depth prediction CONJUNCTION semantic segmentation. prediction accuracy EVALUATE-FOR networks. depth prediction HYPONYM-OF image - to - image tasks. semantic segmentation HYPONYM-OF image - to - image tasks. approach USED-FOR activation maps compression. activation maps compression USED-FOR 1 × 1 convolutions. 1 × 1 convolutions PART-OF CNNs. compression ratios CONJUNCTION computational savings. computational savings CONJUNCTION compression ratios. computational savings CONJUNCTION low bit quantization rates. low bit quantization rates CONJUNCTION computational savings. low bit quantization rates EVALUATE-FOR WCC. computational savings EVALUATE-FOR WCC. accuracy EVALUATE-FOR WCC. compression ratios EVALUATE-FOR WCC. hardware - friendly Haar - wavelet transform USED-FOR image compression. convolution PART-OF compressed activation map. 1× 1 convolution PART-OF network architecture. 1× 1 convolution USED-FOR WCC. network architecture USED-FOR WCC. WCC CONJUNCTION light quantization. light quantization CONJUNCTION WCC. Method is Convolutional Neural Networks ( CNNs ). OtherScientificTerm is quantization. Metric is compression rates. ,This paper studies the problem of quantization for compressing Convolutional Neural Networks (CNNs). The authors propose a new approach for activation maps compression for 1 × 1 convolutions in 1×1 convolutions of CNNs. The main idea is to use a hardware-friendly Haar-wavelet transform to compress the image compression. The authors show that the compression ratios and the computational savings of WCC with low bit quantization rates and low compression ratios can be improved by the proposed network architecture. ,"This paper proposes a new approach for compressing Convolutional Neural Networks (CNNs) to reduce computational resources. The authors propose to use activation maps compression to compress 1 × 1 convolutions in CNNs, which is an extension of the approach of 1×1 convolutions. The main idea is to use a hardware-friendly Haar-wavelet transform for image compression. The proposed method is evaluated on several image-to-image tasks (semantic segmentation, depth prediction, and semantic segmentation). The authors show that the proposed quantization improves the prediction accuracy of the networks and the computational savings and low bit quantization rates of WCC and other compression ratios. They also show that WCC can be decomposed into a single network architecture and a single convolution in the compressed activation map. "
5151,SP:004865e6affad32403b7965493a53c8a7ffdda0a,"accelerated learning dynamics USED-FOR correlated and coarse correlated equilibria. normal - form games FEATURE-OF correlated and coarse correlated equilibria. sequential and simultaneous moves CONJUNCTION imperfect information. imperfect information CONJUNCTION sequential and simultaneous moves. imperfect information FEATURE-OF extensive - form games. sequential and simultaneous moves FEATURE-OF extensive - form games. no - regret learning dynamics USED-FOR extensive - form correlated equilibrium ( EFCE ). O(T 3/4)-approximate EFCE FEATURE-OF correlated distribution of play. structured Markov chain USED-FOR refined perturbation analysis. refined perturbation analysis USED-FOR stability of certain fixed point strategies. Task is learning in games. Method are accelerated dynamics, and framework of -regret. OtherScientificTerm is prior rate. ","This paper studies the problem of learning in games with correlated and coarse correlated equilibria in normal-form games with sequential and simultaneous moves and imperfect information. The authors propose a new accelerated learning dynamics that can be used to learn correlated-and-covariant equilibriums in such games. The main idea is to use no-regret learning dynamics to learn an extensive-form correlated equilibrium (EFCE) with O(T 3/4)-approximate EFCE in the correlated distribution of play. Theoretically, the authors show that the accelerated dynamics can be combined with a structured Markov chain to provide a refined perturbation analysis for the stability of certain fixed point strategies.  The authors also provide theoretical guarantees for the prior rate of the learned framework of -regret.","This paper studies accelerated learning dynamics for correlated and coarse correlated equilibria in normal-form games. The authors propose a framework of-regret, where the accelerated dynamics can be decomposed into two parts: (1) the prior rate, and (2) the loss function. They show that extensive-form correlated equilibrium (EFCE) is an O(T 3/4)-approximate EFCE in the correlated distribution of play. They also show that in extensive-forms games with sequential and simultaneous moves and imperfect information, no-regression learning dynamics is sufficient to achieve this. They propose a refined perturbation analysis based on a structured Markov chain, and show that the stability of certain fixed point strategies is guaranteed by the proposed framework."
5167,SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"discrete actions COMPARE continuous actions. continuous actions COMPARE discrete actions. maximum of the action - value function USED-FOR dynamic programmingbased methods. Action Quantization from Demonstrations ( AQuaDem ) USED-FOR discretization of continuous action spaces. method USED-FOR discretization of continuous action spaces. priors of demonstrations USED-FOR discretization of continuous action spaces. discrete action deep RL algorithm USED-FOR continuous control problem. RL CONJUNCTION RL. RL CONJUNCTION RL. demonstrations CONJUNCTION RL. RL CONJUNCTION demonstrations. play data USED-FOR RL. Imitation Learning EVALUATE-FOR method. demonstrations USED-FOR RL. setups EVALUATE-FOR method. Imitation Learning HYPONYM-OF setups. RL HYPONYM-OF setups. RL HYPONYM-OF setups. human data COMPARE synthetic data. synthetic data COMPARE human data. human data USED-FOR setups. AQuaDem COMPARE continuous control methods. continuous control methods COMPARE AQuaDem. hard manipulation tasks EVALUATE-FOR continuous control methods. hard manipulation tasks EVALUATE-FOR AQuaDem. sample efficiency EVALUATE-FOR continuous control methods. sample efficiency EVALUATE-FOR AQuaDem. Task are Reinforcement Learning ( RL ), exploration problems, and exploration problem. OtherScientificTerm is action space. ","This paper proposes Action Quantization from Demonstrations (AQuaDem), a method for discretization of continuous action spaces using priors of demonstrations. The authors propose a discrete action deep RL algorithm to solve the continuous control problem in Reinforcement Learning (RL), where the goal is to maximize the maximum of the action-value function in the action space. They show that the proposed method achieves better sample efficiency than existing continuous control methods on a variety of hard manipulation tasks. The proposed method is evaluated on a number of different setups, including Imitation Learning, RL, demonstrations, and RL with human data.","This paper proposes Action Quantization from Demonstrations (AQuaDem), a method for discretization of continuous action spaces based on priors of demonstrations. The authors propose a discrete action deep RL algorithm for the continuous control problem, where the action space is a continuous action space and the maximum of the action-value function is a discrete function. The proposed method is evaluated on three different setups: Imitation Learning, RL, and demonstrations for RL and RL with play data. The experiments show that the proposed method achieves better sample efficiency compared to continuous control methods on hard manipulation tasks. The paper also provides a theoretical analysis of Reinforcement Learning (RL) and exploration problems. Experiments are conducted on synthetic data and human data."
5183,SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,domain generalization USED-FOR robust model. domain generalization USED-FOR semantic segmentation. labeled synthetic ( source ) data USED-FOR robust model. channelwise mean USED-FOR style features. adversarial style augmentation ( AdvStyle ) approach USED-FOR hard stylized images. style feature USED-FOR AdvStyle. adversarial training USED-FOR it. adversarial style feature USED-FOR adversarial image. adversarial image USED-FOR robust model training. AdvStyle USED-FOR models. synthetic - to - real semantic segmentation benchmarks EVALUATE-FOR AdvStyle. AdvStyle USED-FOR model. AdvStyle USED-FOR domain generalized image classification. datasets EVALUATE-FOR AdvStyle. OtherScientificTerm is image style variation. Task is overfitting. ,"This paper proposes an adversarial style augmentation (AdvStyle) approach for hard stylized images. AdvStyle uses domain generalization to train a robust model on labeled synthetic (source) data. The style features are learned by using a channelwise mean of the style features. The adversarial image is trained using adversarial training, and the style feature is used to train the adversarial model. Experiments on synthetic-to-real semantic segmentation benchmarks show that AdvStyle improves the performance of models trained with AdvStyle in domain generalized image classification. ","This paper proposes an adversarial style augmentation (AdvStyle) approach for hard stylized images. AdvStyle is based on domain generalization for semantic segmentation on labeled synthetic (source) data. The key idea is to use the channelwise mean of the style features to train the robust model. The adversarial image is generated from the adversarial training. The authors show that AdvStyle improves the performance of models trained on synthetic-to-real semantic segmentsation benchmarks. They also show that it improves overfitting. Finally, the authors evaluate AdvStyle for domain generalized image classification on three datasets."
5199,SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"rigid, dramatic gestures USED-FOR recognition. rigid, dramatic gestures USED-FOR systems. neuromorphic gesture analysis system USED-FOR event - based gesture data. high temporal resolution FEATURE-OF neuromorphic gesture analysis system. high temporal resolution FEATURE-OF event - based gesture data. latent space representation USED-FOR similarity of mid - air gesture data. Dynamic Vision Sensor ( DVS ) USED-FOR event - based data. it USED-FOR sparse, noisy inputs. it USED-FOR interpretable latent space representation. interpretable latent space representation USED-FOR sparse, noisy inputs. DVSGesture dataset EVALUATE-FOR Hybrid GuidedVAE. classification accuracy EVALUATE-FOR Hybrid GuidedVAE. T - SNE plots USED-FOR interpretable latent space representation. neuromorphic hardware USED-FOR model. Method is mid - air gesture recognition systems. Generic are they, approach, and algorithm. ",This paper proposes a neuromorphic gesture analysis system with high temporal resolution for event-based gesture data from Dynamic Vision Sensor (DVS). The key idea is to use a latent space representation to learn the similarity of mid-air gesture data. The model is trained using neuromorphic hardware. The experimental results on the DVSGesture dataset show that Hybrid GuidedVAE improves classification accuracy.,"This paper proposes a neuromorphic gesture analysis system for event-based gesture data with high temporal resolution. The main idea is to use Dynamic Vision Sensor (DVS) to capture the similarity of mid-air gesture data. The authors show that they can achieve state-of-the-art performance on the DVSGesture dataset. The proposed approach is based on neuromorphic hardware. The model is trained on T-SNE plots, and it is able to generate an interpretable latent space representation for sparse, noisy inputs. "
5215,SP:2e66468a6b94177e54b0052b97713ee63902c278,"accuracy EVALUATE-FOR neuron - based networks. tabular data USED-FOR Deep learning. Internet of Things ( IoT ) CONJUNCTION drone. drone CONJUNCTION Internet of Things ( IoT ). drone CONJUNCTION Natural User Interface ( NUI ) application. Natural User Interface ( NUI ) application CONJUNCTION drone. annealing mechanism USED-FOR S - HTE inference. S - HTE USED-FOR internal representations. ferns USED-FOR S - HTE. classification and regression benchmark EVALUATE-FOR accuracy. OtherScientificTerm are computational capacity, deep learning capabilities, and neurons. Method are deep learning methods, and PyTorch implementation. Generic is it. Metric is computational complexity. ","This paper studies the accuracy of neuron-based networks on tabular data. Deep learning is an important area of research in the field of machine learning, where the computational capacity of deep learning methods is limited due to the lack of computational capacity. The authors propose PyTorch implementation, which uses an annealing mechanism to perform S-HTE inference on the internal representations of ferns. The accuracy is evaluated on a classification and regression benchmark, and it is shown that it is better than the state-of-the-art. ",This paper studies the accuracy of neuron-based networks on tabular data. Deep learning is an important problem in the context of Deep learning. The computational capacity of deep learning methods is limited due to the computational complexity of the deep learning capabilities. This paper proposes PyTorch implementation to address this issue. The authors propose an annealing mechanism for S-HTE inference based on ferns. They show that it can improve the accuracy on classification and regression benchmark. They also show that S- HTE can learn internal representations of neurons. The paper also proposes a Natural User Interface (NUI) application and a drone.
5231,SP:b238db9252d83a13438bb747d70e635bb9945958,undirected stateonly experience USED-FOR learning value functions. tabular Q - learning USED-FOR value function. tabular Q - learning USED-FOR discrete Markov decision processes ( MDPs ). refinement of the action space FEATURE-OF value function. offline RL method USED-FOR value functions. Latent Action Q - learning HYPONYM-OF offline RL method. state - only experience USED-FOR value functions. Latent Action Q - learning ( LAQ ) USED-FOR value functions. discrete latent actions USED-FOR Q - learning. latent - variable future prediction model USED-FOR Q - learning. latent - variable future prediction model USED-FOR discrete latent actions. Q - learning USED-FOR Latent Action Q - learning ( LAQ ). Q - learning USED-FOR value functions. LAQ USED-FOR value functions. value functions CONJUNCTION value functions. value functions CONJUNCTION value functions. ground truth actions USED-FOR value functions. Value functions USED-FOR acquisition of goal - directed behavior. Value functions USED-FOR domain - specific low - level controllers. LAQ USED-FOR acquisition of goal - directed behavior. LAQ USED-FOR Value functions. imitation learning oracles CONJUNCTION competing methods. competing methods CONJUNCTION imitation learning oracles. alternatives CONJUNCTION imitation learning oracles. imitation learning oracles CONJUNCTION alternatives. 2D grid world CONJUNCTION 3D visual navigation. 3D visual navigation CONJUNCTION 2D grid world. LAQ COMPARE alternatives. alternatives COMPARE LAQ. environments EVALUATE-FOR LAQ. LAQ COMPARE competing methods. competing methods COMPARE LAQ. LAQ CONJUNCTION imitation learning oracles. imitation learning oracles CONJUNCTION LAQ. 3D visual navigation HYPONYM-OF environments. 2D grid world HYPONYM-OF environments. ,"This paper proposes a new offline RL method for learning value functions based on undirected stateonly experience. Latent Action Q-learning (LAQ) uses tabular Q-Learning to learn the value function in discrete Markov decision processes (MDPs). The refinement of the action space of a value function is based on a latent-variable future prediction model. The value functions are learned using ground truth actions. Value functions are then used to train domain-specific low-level controllers.  The paper shows that LAQ outperforms competing methods in three environments: 2D grid world, 3D visual navigation, and imitation learning oracles.  ",This paper proposes a novel offline RL method for learning value functions in the context of state-only experience. The value function is learned by tabular Q-learning for discrete Markov decision processes (MDPs). The refinement of the action space is done by a latent-variable future prediction model. Latent Action Q-Learning (LAQ) is a variant of the offline RL Method. Value functions are learned for domain-specific low-level controllers. The paper shows that value functions can be learned using ground truth actions.  The paper also shows that LAQ outperforms other alternatives and imitation learning oracles.  
5247,SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"large models USED-FOR deep learning applications. low - latency and high - bandwidth interconnect USED-FOR distributed training algorithms. distributed training algorithms USED-FOR models. GPU clusters USED-FOR models. model parallelism USED-FOR large models. SWARM Parallelism1 HYPONYM-OF model - parallel training algorithm. model - parallel training algorithm USED-FOR swarms of poorly connected, heterogeneous unreliable devices. SWARM USED-FOR temporary randomized pipelines. compression - aware architecture modifications USED-FOR approach. network throughput FEATURE-OF swarm of preemptible T4 GPUs. shared parameters USED-FOR large Transformer language model. swarm of preemptible T4 GPUs USED-FOR large Transformer language model. OtherScientificTerm are dedicated GPU clusters, distributed training setups, and preemptible ” instances. Generic is setups. ","This paper proposes a new model-parallel training algorithm, SWARM Parallelism1, for large models for deep learning applications with low-latency and high-bortwidth interconnect. The proposed models are based on GPU clusters and are trained with dedicated GPU clusters. The authors show that the proposed approach is able to reduce the network throughput of a large Transformer language model trained with a swarm of preemptible T4 GPUs by using shared parameters. They also show that SWARM can be used to train temporary randomized pipelines. ","This paper proposes a model-parallel training algorithm for swarms of poorly connected, heterogeneous unreliable devices with low-latency and high-width interconnect. The authors propose to use distributed training algorithms to train models with GPU clusters. The proposed approach is based on compression-aware architecture modifications. The main idea is to train a large Transformer language model on a swarm of preemptible T4 GPUs with shared parameters. The approach is evaluated on two different setups: dedicated GPU clusters and distributed training setups with “preemptible” instances."
5263,SP:91d2f094d5481651b554f58aecc2a6207057a47c,"Offline reinforcement learning USED-FOR policies. fixed dataset USED-FOR real - world applications. fixed dataset USED-FOR Offline reinforcement learning. fixed dataset USED-FOR policies. behavior policy COMPARE policy. policy COMPARE behavior policy. transition dynamics PART-OF offline experiences. offline training CONJUNCTION online tuning. online tuning CONJUNCTION offline training. online data USED-FOR agent policy. deployment efficiency CONJUNCTION sample efficiency. sample efficiency CONJUNCTION deployment efficiency. online transition correction ( OTC ) USED-FOR biased transition dynamics. offline and online experiences USED-FOR online transition correction ( OTC ). sampling probabilities USED-FOR online transition correction ( OTC ). distances USED-FOR similarity between transitions. transition similarity USED-FOR adaptive rank - based prioritization. embedding - based and valuebased distance HYPONYM-OF distances. OTC USED-FOR agent policies. OTC USED-FOR online tuning. agent policies USED-FOR online tuning. data efficiency EVALUATE-FOR OTC. OTC COMPARE baselines. baselines COMPARE OTC. tasks EVALUATE-FOR baselines. tasks EVALUATE-FOR OTC. Task is offline decentralized multi - agent reinforcement learning. OtherScientificTerm are online execution, value estimates, uncoordinated and suboptimal policies, and transition bias. Material is online experiences. ","Offline reinforcement learning with a fixed dataset is an important problem in real-world applications. Offline reinforcement learning for policies trained on the fixed dataset can be challenging because of the transition dynamics in offline experiences. Offline and online experiences can be biased due to the online transition correction (OTC) based on sampling probabilities. To address this issue, the authors propose to use the online data to train an agent policy that is more similar to the behavior policy than the policy learned on the offline data. The agent policies are then trained using OTC for online tuning and online execution. The authors show that OTC improves the deployment efficiency and sample efficiency compared to other baselines on a variety of tasks. ","Offline reinforcement learning for policies on a fixed dataset for real-world applications is an important problem. Offline reinforcement learning is a very important problem in the context of offline decentralized multi-agent reinforcement learning, where the goal is to learn policies on the fixed dataset, and not on the online dataset. The paper proposes to use a behavior policy to learn the policy on the offline dataset, rather than the behavior policy being learned from the online data. The key idea is to use online transition correction (OTC) for biased transition dynamics in the offline experiences, which is a combination of offline and online experiences. The authors show that OTC can improve the deployment efficiency and sample efficiency of the agent policy on online data, and online tuning on agent policies for online tuning. They show that the transition bias can be reduced by using the distances between transitions (e.g. embedding-based and valuebased distance) between the online and offline experiences. They also show that transition similarity between transitions can be used for adaptive rank-based prioritization, which can be achieved by using value estimates. They evaluate OTC on a variety of tasks and compare it to other baselines."
5279,SP:d0e650d568214481b07a0452ec606ccbf6d05410,"computational footprint EVALUATE-FOR Deep Neural Networks ( DNNs ) training. 4 - bit quantization USED-FOR methods. computational footprint EVALUATE-FOR training process. loss gradients HYPONYM-OF neural gradients. unbiased quantization USED-FOR quantized neural network training. logarithmic unbiased quantization ( LUQ ) method USED-FOR forward and backward phase. high precision fine - tuning CONJUNCTION variance reduction method. variance reduction method CONJUNCTION high precision fine - tuning. ImageNet FEATURE-OF ResNet50. method USED-FOR low precision format. OtherScientificTerm are intermediate neural layers, multiplications, and multiplier. Generic is it. Method is 4 - bit training. ","This paper studies the computational footprint of Deep Neural Networks (DNNs) training with 4-bit quantization. The authors propose a new unbiased quantization for quantized neural network training, which is based on logarithmic unbiased quantized (LUQ) method for both the forward and backward phase. The proposed method is able to handle the low precision format in the training process, and it is also able to be applied to intermediate neural layers. Experiments on ResNet50 on ImageNet show that the proposed method performs better than high precision fine-tuning and a variance reduction method.","This paper studies the computational footprint of Deep Neural Networks (DNNs) training with 4-bit quantization. The authors show that the training process has a large computational footprint in terms of the number of intermediate neural layers and the loss gradients of the neural gradients. They propose two methods for quantized neural network training with unbiased quantization: logarithmic unbiased quantized (LUQ) method for the forward and backward phase, and high precision fine-tuning and variance reduction method for low precision format. Experiments on ResNet50 on ImageNet show the effectiveness of the proposed method. "
5295,SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"Polythetic classifications COMPARE monothetic classifications. monothetic classifications COMPARE Polythetic classifications. features USED-FOR monothetic classifications. shared patterns of features USED-FOR Polythetic classifications. threshold meta - learners USED-FOR functions. embedding dimension USED-FOR functions. embedding dimension USED-FOR threshold meta - learners. Prototypical Networks HYPONYM-OF threshold meta - learners. attentional classifiers USED-FOR problems. linear embedding dimension USED-FOR attentional classifiers. Matching Networks HYPONYM-OF attentional classifiers. linear embedding dimension USED-FOR problems. attentional models USED-FOR misclassification. selfattention feature - selection mechanism USED-FOR non - discriminative features. approach USED-FOR meta - learning Boolean functions. Material is natural world. OtherScientificTerm are task - relevant features, and task - irrelevant features. Task is meta - learning problems. ","This paper studies the problem of meta-learning Boolean functions in the natural world. The authors propose to use threshold meta-learners (Prototypical Networks, Matching Networks) with a linear embedding dimension to learn the functions. Polythetic classifications based on shared patterns of features are shown to outperform monothetic classifications in terms of the number of features. The main contribution of the paper is to show that attentional classifiers trained on these problems with the linear embeddings can be used to learn better performance on other problems with attentional models for misclassification. The paper also proposes a selfattention feature-selection mechanism to select the non-discriminative features to be used in the meta-training process. ","This paper proposes a novel approach to learn meta-learning Boolean functions. The key idea is to use threshold meta-learners (Prototypical Networks) with a linear embedding dimension to learn functions with shared patterns of features. Polythetic classifications are shown to outperform monothetic classifications in terms of the number of features in the natural world. The paper also proposes a selfattention feature-selection mechanism to learn non-discriminative features. The proposed approach is evaluated on a set of problems with attentional classifiers (e.g., Matching Networks). The paper shows that the proposed approach can be used to learn task-relevant features that are more likely to be misclassified than task-irrelevant features."
5311,SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"multi - agent reinforcement learning USED-FOR emergent communication. continuous acoustic channel USED-FOR Human communication. reinforcement learning USED-FOR continuous communication channel. continuous communication channel USED-FOR emergent language. channel characteristics USED-FOR emerging language. vocoder USED-FOR continuous waveform. noise FEATURE-OF communication channel. continuous signalling USED-FOR language learning. platform USED-FOR continuous signalling. deep reinforcement learning USED-FOR platform. OtherScientificTerm are discrete symbols, lossy continuous channel, continuous signal, and concept combinations. Method are environment and training methodology, and deep Q - learning. Material is messaging environment. ","This paper proposes a continuous acoustic channel for learning emergent communication in multi-agent reinforcement learning for Human communication. The continuous communication channel is used to learn emergent language with channel characteristics that are useful for emerging language. The communication channel consists of discrete symbols and a lossy continuous channel. A vocoder is trained to generate a continuous waveform of the communication channel, which is then fed to a deep Q-learning platform. The platform is trained using deep reinforcement learning to learn the continuous signalling. The proposed environment and training methodology is evaluated on a variety of tasks in the messaging environment.","This paper proposes a multi-agent reinforcement learning for emergent communication in a continuous acoustic channel for Human communication. The proposed environment and training methodology is based on reinforcement learning to learn a continuous communication channel for the emergent language. The communication channel consists of discrete symbols and a lossy continuous channel. The continuous signal is generated by a vocoder that generates a continuous waveform, which is then fed to the vocoder. The authors propose a new platform for continuous signalling, which uses deep reinforcement learning. The key idea of the proposed platform is to learn the communication channel with noise, and then use the channel characteristics of the emerging language to guide the learning of the evolving language. This is done by using deep Q-learning. The paper also proposes a new messaging environment, where the continuous signal can be used to learn concept combinations."
5327,SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"backdoor attacks FEATURE-OF NLP models. NLP backdoor attacks USED-FOR tasks. NLP models CONJUNCTION tasks. tasks CONJUNCTION NLP models. attacks USED-FOR NLP models. attacks USED-FOR tasks. BadPre HYPONYM-OF task - agnostic backdoor attack. task - agnostic backdoor attack USED-FOR pre - trained NLP models. backdoor USED-FOR pre - trained model. backdoor USED-FOR downstream models. transfer learning process USED-FOR downstream models. approach USED-FOR downstream NLP tasks. Task is downstream language tasks. Method is language models. OtherScientificTerm are model misprediction, and prior information. Generic are attack, malicious model, and strategy. ","This paper studies the problem of NLP backdoor attacks on NLP models trained on downstream language tasks. The authors propose a new task-agnostic backdoor attack called BadPre, which can be applied to any pre-trained NLP model. The key idea of BadPre is to train a malicious model to misprediction the target language, and then use the backdoor to train downstream models using a transfer learning process. The proposed approach is shown to improve performance on downstream NLP tasks.","This paper studies the problem of NLP backdoor attacks for tasks where the model misprediction is high. The authors propose BadPre, a task-agnostic backdoor attack for pre-trained NLP models. The key idea of BadPre is that the attack is based on a malicious model that is trained on the prior information of the target task. The backdoor is then applied to the downstream models via a transfer learning process. The proposed approach is evaluated on downstream NLP tasks and shows that the proposed approach can improve the performance of downstream language models."
5343,SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"skill pre - training methods COMPARE RL techniques. RL techniques COMPARE skill pre - training methods. skill learning USED-FOR evolving or expanding environment. evolving environment USED-FOR skill discovery. framework USED-FOR skill discovery. incremental skills COMPARE skill discovery methods. skill discovery methods COMPARE incremental skills. evolving and static environments EVALUATE-FOR incremental skills. skill quality EVALUATE-FOR skill discovery methods. skill quality EVALUATE-FOR incremental skills. Task are Reward - free, unsupervised discovery of skills, hand - designing rewards, task supervision, and discovery - of - incremental - skill. OtherScientificTerm are stationary environments, agent dynamics, and learned skills. Generic are methods, and them. ","This paper proposes a new framework for skill discovery in the evolving or expanding environment. The goal is to learn a set of skills that can be used to improve the performance of the agent in the future. The proposed framework is based on the idea of reward-free, unsupervised discovery of skills, where the agent is given a sequence of tasks and a reward that encourages the agent to perform well in each task. The authors show that the learned skills can improve the skill quality in both evolving and static environments. ","This paper proposes a framework for skill discovery in an evolving or expanding environment, where the goal is to learn skills that can be used in the future. The authors propose to use reward-free, unsupervised discovery of skills, where hand-designing rewards are used to encourage the agent to learn new skills. The proposed framework is evaluated on a number of tasks, and shows that the proposed method outperforms other skill discovery methods in terms of skill quality in both evolving and static environments. "
5359,SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"Convolutional neural networks USED-FOR features. regular quadrilateral convolution kernels USED-FOR features. regular quadrilateral convolution kernels USED-FOR Convolutional neural networks. small convolution kernels USED-FOR models. relative directions CONJUNCTION logarithmic distances. logarithmic distances CONJUNCTION relative directions. LPSC USED-FOR local spatial structures. LPSC USED-FOR single - layer receptive field. LPSC USED-FOR network architecture. convolutions PART-OF network architecture. convolution USED-FOR LPSC. log - polar space pooling USED-FOR convolution. log - polar space pooling USED-FOR LPSC. tasks EVALUATE-FOR LPSC. OtherScientificTerm are convolution kernel, small local receptive fields, and local receptive field. ",This paper proposes a new convolutional neural network architecture based on regular quadrilateral convolution kernels. The proposed LPSC is able to learn local spatial structures in a single-layer receptive field. The network architecture is based on convolutions in the convolutions of the convolution kernel. The authors also propose a log-polar space pooling technique to improve the performance of the proposed convolution. Experiments on a variety of tasks demonstrate the effectiveness of the new network architecture.,"This paper proposes a novel network architecture based on convolutional neural networks with regular quadrilateral convolution kernels. The models are based on small convolution kernel, where each layer is represented by a single-layer receptive field. The authors show that LPSC can learn local spatial structures with relative directions and logarithmic distances. The convolutions in the network architecture are learned by convolution with log-polar space pooling. The paper also shows that the convolution can learn small local receptive fields. Experiments on a variety of tasks demonstrate the effectiveness of the proposed network architecture. "
5375,SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"non - vacuous bounds USED-FOR NNs. PAC - Bayes theorem USED-FOR NNs generalization. IIW ’s property USED-FOR deep learning. algorithm USED-FOR approximation of IIW. information complexity EVALUATE-FOR NNs. accuracy CONJUNCTION information complexity. information complexity CONJUNCTION accuracy. accuracy EVALUATE-FOR NNs. PIB HYPONYM-OF NNs. IIW compression CONJUNCTION generalization. generalization CONJUNCTION IIW compression. IIW USED-FOR NNs. overparameterization CONJUNCTION noisy labels. noisy labels CONJUNCTION overparameterization. varying batch sizes CONJUNCTION overparameterization. overparameterization CONJUNCTION varying batch sizes. IIW USED-FOR NNs. MCMC - based algorithm USED-FOR optimal weight posterior. PIB FEATURE-OF optimal weight posterior. Task are ML research, and compressing phase transition. Method are IIW - based information bottleneck, and NNs ’ training. ","This paper studies the non-vacuous bounds for NNs under PAC-Bayes theorem. The authors show that the IIW-based information bottleneck is a major bottleneck in ML research, and propose a new algorithm for the approximation of IIW. The algorithm is based on IIW’s property for deep learning, and can be applied to NNs with varying batch sizes, overparameterization, and noisy labels. The paper also shows that the optimal weight posterior of the MCMC-based algorithm can be approximated by the PIB of NNs (PIB). The authors also show that IIW compression and generalization can be used to improve the performance of the NNs. ","This paper proposes a new algorithm for computing the approximation of IIW’s property in deep learning. The authors prove non-vacuous bounds for NNs under PAC-Bayes theorem for generalization and for accuracy of NNs with PIB. They show that IIW compression and generalization can be approximated by a MCMC-based algorithm. They also show that the optimal weight posterior of the PIB can be computed by the MCMC based algorithm. Finally, the authors provide a theoretical analysis of the IIW-based information bottleneck in ML research. "
5391,SP:a733847ade77ffbf38760fc79da17893dea8d53f,"perturbations USED-FOR attacks. linear separable FEATURE-OF perturbations. linear separability USED-FOR attacks. synthetic perturbations COMPARE deliberately crafted attacks. deliberately crafted attacks COMPARE synthetic perturbations. linear separable data USED-FOR perturbations. imperceptible scale FEATURE-OF they. shortcuts USED-FOR deep models. Method are Indiscriminate data poisoning attacks, and pre - trained feature extractors. OtherScientificTerm are imperceptible perturbations, and normal features. Task is shortcut learning problem. ","This paper studies Indiscriminate data poisoning attacks against deep models. The authors propose to use perturbations that are linear separable, i.e. imperceptible to normal features. They show that these attacks are more effective than the standard synthetic perturbation, and that they are more robust to the imperceptibility of the data. They also show that the linear separability of these attacks is the main reason for their effectiveness. Finally, they show that such shortcuts can be used to improve the performance of deep models in practice. ","This paper studies Indiscriminate data poisoning attacks against deep models. The authors propose to use perturbations that are linear separable, i.e. they are imperceptible on the imperceptibly scale. They show that these attacks are more robust to linear separability of the perturbation than the attacks that are not. They also show that the attacks are not as robust to synthetic perturbs as the deliberately crafted attacks. They further show that they are more sensitive to normal features than the ones that are induced by pre-trained feature extractors. Finally, they show that shortcuts to deep models can be used to solve the shortcut learning problem. "
5407,SP:7b50be406138ad01db3ee112899f622637896fe9,Offline policy optimization USED-FOR real - world decisionmaking problems. estimator USED-FOR offline policy evaluation. function approximations USED-FOR value functions. Importance sampling HYPONYM-OF estimator. value functions CONJUNCTION process models. process models CONJUNCTION value functions. function approximations USED-FOR process models. Importance sampling USED-FOR offline policy evaluation. algorithm USED-FOR overfitting. overfitting phenomenon FEATURE-OF importance weighted return. per - state - neighborhood normalization condition USED-FOR algorithm. healthcare - inspired simulator CONJUNCTION logged dataset. logged dataset CONJUNCTION healthcare - inspired simulator. healthcare - inspired simulator EVALUATE-FOR method. logged dataset EVALUATE-FOR method. method COMPARE batch reinforcement learning algorithms. batch reinforcement learning algorithms COMPARE method. overfitting EVALUATE-FOR method. Task is online learning. Generic is approach. ,"Offline policy optimization is an important problem in real-world decisionmaking problems. In this paper, the authors propose a new estimator called Importance sampling for offline policy evaluation, which uses function approximations for value functions and process models. The algorithm is based on a per-state-neighborhood normalization condition. The proposed method is evaluated on a healthcare-inspired simulator and a logged dataset. The authors show that the proposed method achieves better overfitting than batch reinforcement learning algorithms.","This paper proposes a method for offline policy optimization for real-world decisionmaking problems. The main idea is to use an estimator called Importance sampling to perform offline policy evaluation on value functions and process models. The authors show that the proposed algorithm is able to avoid overfitting due to the importance weighted return, which is an overfitting phenomenon in online learning. The algorithm is based on a per-state-neighborhood normalization condition. The proposed method is evaluated on a healthcare-inspired simulator and a logged dataset. The method is shown to outperform batch reinforcement learning algorithms."
5423,SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,model USED-FOR continual learning. CoLLIE HYPONYM-OF model. CoLLIE USED-FOR continual learning. transformation function USED-FOR language embeddings. language CONJUNCTION images. images CONJUNCTION language. transformation function USED-FOR new language use. transformation function USED-FOR CoLLIE. semantic space FEATURE-OF images. few - shot learning COMPARE model. model COMPARE few - shot learning. it COMPARE model. model COMPARE it. zero - shot EVALUATE-FOR model. continual learning EVALUATE-FOR model. Material is vision. Method is multimodal embedding model. OtherScientificTerm is similar language use. ,"This paper proposes a multimodal embedding model, CoLLIE, which is a model for continual learning. The model is based on a transformation function to learn new language embeddings and images in the semantic space. The authors show that the model performs better than few-shot learning on zero-shot and continual learning than it does on similar language use. ","This paper proposes a model for continual learning, CoLLIE, which is a multimodal embedding model. The model is based on CoLLie. The key idea is to learn a transformation function for language embeddings and images in the semantic space. The transformation function is used to learn new language use and images that are similar language use. Experiments show that the model outperforms few-shot learning on zero-shot and continual learning."
5439,SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"captioning models USED-FOR visual data. fluency CONJUNCTION fidelity. fidelity CONJUNCTION fluency. fidelity CONJUNCTION adequacy. adequacy CONJUNCTION fidelity. Fidelity CONJUNCTION Fluency. Fluency CONJUNCTION Fidelity. Visual - Linguistic Adequacy CONJUNCTION Fidelity. Fidelity CONJUNCTION Visual - Linguistic Adequacy. VLAF2 USED-FOR Visual - Linguistic Adequacy. VLAF2 USED-FOR Fidelity. VLAF2 USED-FOR Fluency. linguistics USED-FOR Fluency. linguistics USED-FOR VLAF2. BERT CONJUNCTION CLIP. CLIP CONJUNCTION BERT. intrinsic language knowledge USED-FOR models. nocaps dataset EVALUATE-FOR framework. method COMPARE captioning models. captioning models COMPARE method. method COMPARE SPICE scores of human baseline. SPICE scores of human baseline COMPARE method. caption evaluation metrics EVALUATE-FOR captioning models. caption evaluation metrics EVALUATE-FOR method. fluency CONJUNCTION fidelity. fidelity CONJUNCTION fluency. fidelity CONJUNCTION adequacy. adequacy CONJUNCTION fidelity. model USED-FOR object captions. quantitative and qualitative analysis EVALUATE-FOR model. fluency EVALUATE-FOR model. adequacy EVALUATE-FOR model. fidelity EVALUATE-FOR model. Method are object captioning ( NOC ), image captioning models, object captioning models, and visual / language models. OtherScientificTerm is caption annotations. ","This paper studies the problem of object captioning (NOC) in the context of image captioning models for visual data. The authors propose a new framework that combines the VLAF2 for Visual-Linguistic Adequacy, Fidelity, and Fluency with linguistics to improve the Fidelity and the fidelity. The proposed method is evaluated on the nocaps dataset and compared to BERT and CLIP. The results show that the proposed method improves the SPICE scores of human baseline and provides better caption evaluation metrics than existing captioned models. The model also performs well on both quantitative and qualitative analysis. ","This paper proposes a new framework for learning captioning models for visual data. The framework is based on object captioning (NOC), which is an extension of the work of BERT and CLIP. The key idea of the framework is to use intrinsic language knowledge to train the models. The proposed framework is evaluated on the nocaps dataset. The authors show that the proposed method outperforms the SPICE scores of human baseline in terms of fluency, fidelity, and adequacy. The model is also evaluated on both quantitative and qualitative analysis."
5455,SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"foundation models USED-FOR representations. representations USED-FOR classification. special - purpose algorithms USED-FOR problems. classifier USED-FOR representations. special - purpose algorithms USED-FOR representations. clustering property FEATURE-OF features. neural collapse HYPONYM-OF clustering property. overparameterized classification networks USED-FOR features. foundation models USED-FOR feature maps. foundation models USED-FOR transfer learning. feature maps USED-FOR transfer learning. Task are few - shot learning problems, and few - shot setting. ","This paper studies few-shot learning problems where the goal is to learn representations for classification. The authors propose two special-purpose algorithms to solve these problems. First, the authors propose a clustering property called neural collapse, which is a regularization term for the features of overparameterized classification networks. Second, they propose a classifier to learn the representations of the classifier. The main contribution of the paper is to use these two foundation models to learn feature maps for transfer learning. ","This paper proposes a new framework for few-shot learning problems, where the goal is to learn representations for classification. The authors propose two special-purpose algorithms to solve these problems. First, the authors propose a classifier that predicts the representations of the classifier. Second, they propose a clustering property of the features, which is based on neural collapse. The features are learned using overparameterized classification networks. The main contribution of the paper is the use of these two foundation models for learning feature maps for transfer learning. "
5471,SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"3D scanning USED-FOR Point cloud. tasks USED-FOR point cloud reconstruction. 3D sparse stacked - hourglass network USED-FOR densification and denoising. 3D sparse stacked - hourglass network HYPONYM-OF stages. refinement HYPONYM-OF stages. stages PART-OF deep point cloud reconstruction network. 3D sparse stacked - hourglass network PART-OF deep point cloud reconstruction network. transformers USED-FOR refinement. refinement PART-OF deep point cloud reconstruction network. amplified positional encoding HYPONYM-OF module. module USED-FOR transformer. points ’ distances USED-FOR adaptive refinements. module USED-FOR positional encoding vectors. points ’ distances USED-FOR module. points ’ distances USED-FOR positional encoding vectors. ScanNet CONJUNCTION ICL - NUIM. ICL - NUIM CONJUNCTION ScanNet. ICL - NUIM CONJUNCTION ShapeNetPart datasets. ShapeNetPart datasets CONJUNCTION ICL - NUIM. ICL - NUIM EVALUATE-FOR network. ShapeNetPart datasets EVALUATE-FOR network. ScanNet EVALUATE-FOR network. network USED-FOR real - world and unmet scenes. OtherScientificTerm are discrete voxels, and 3D points. ","This paper proposes a method for point cloud reconstruction based on 3D scanning. Point cloud reconstruction is an important problem in 3D imaging, and this paper focuses on the problem of 3D point cloud. The main idea is to use a 3D sparse stacked-hourglass network to perform densification and denoising. The proposed method is based on the 3D dense stacked-hoursglass network. The method is evaluated on the ICL-NUIM, ScanNet, and ShapeNetPart datasets.","The paper proposes a new method for point cloud reconstruction based on 3D scanning. Point cloud reconstruction is an important problem in 3D Scanning, where the goal is to reconstruct discrete voxels from 3D points. The paper proposes to use 3D sparse stacked-hourglass network for densification and denoising. The proposed stages of the deep point cloud reconstruct network consists of two stages: 1) refinement and 2) amplified positional encoding. The refinement part uses transformers for refinement. The module for augmented positional encoding uses points’ distances for adaptive refinements. The network is evaluated on ScanNet, ICL-NUIM, and ShapeNetPart datasets. Results show that the network can reconstruct real-world and unmet scenes."
5487,SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"communicating node features CONJUNCTION feature gradients. feature gradients CONJUNCTION communicating node features. training efficiency CONJUNCTION model scalability. model scalability CONJUNCTION training efficiency. stale features CONJUNCTION stale feature gradients. stale feature gradients CONJUNCTION stale features. convergence rate EVALUATE-FOR GCN training. stale features USED-FOR GCN training. stale feature gradients USED-FOR GCN training. convergence rate EVALUATE-FOR PipeGCN. PipeGCN COMPARE vanilla distributed GCN training. vanilla distributed GCN training COMPARE PipeGCN. convergence rate EVALUATE-FOR vanilla distributed GCN training. smoothing method USED-FOR PipeGCN. PipeGCN COMPARE full - graph training methods. full - graph training methods COMPARE PipeGCN. accuracy EVALUATE-FOR full - graph training methods. training throughput EVALUATE-FOR PipeGCN. accuracy EVALUATE-FOR PipeGCN. Method are Graph Convolutional Networks ( GCNs ), large - scale GCNs, and distributed GCN training. Material is graph - structured data. OtherScientificTerm are partitioned subgraph, GCN layer, communication overhead, intra - partition computation, convergence, and staleness. Metric is theoretical convergence guarantee. ","This paper studies the convergence of Graph Convolutional Networks (GCNs) on graph-structured data. The authors show that the convergence rate of GCN training with stale features and stale feature gradients is better than vanilla distributed GCN with a smoothing method. They also provide theoretical convergence guarantee for large-scale GCNs, and show that PipeGCN has a better convergence rate than full-graph training methods in terms of accuracy, training efficiency, and model scalability. ","This paper proposes a theoretical convergence guarantee for Graph Convolutional Networks (GCNs), which is an extension of large-scale GCNs. The authors show that the convergence rate of GCN training with stale features and stale feature gradients can be improved by computing the communicating node features and the feature gradient of the partitioned subgraph. The paper also shows that the training efficiency and model scalability of PipeGCN are improved by using the smoothing method of the GCN layer, and that the communication overhead can be reduced by intra-partition computation. The convergence is shown to be better than the convergence of vanilla distributed GCN with the same convergence rate, and the paper also provides a theoretical analysis of the convergence and the training throughput of PipGCN compared to full-graph training methods. "
5503,SP:8302d49558ee0f16392d623d4e604e92db10d041,"robustness USED-FOR applications. in - distribution test points EVALUATE-FOR deep neural networks. accuracy EVALUATE-FOR deep neural networks. methods USED-FOR test time adaptation. ResNet-50 models CONJUNCTION robust vision transformer model. robust vision transformer model CONJUNCTION ResNet-50 models. approach COMPARE prior augmentation and adaptation strategies. prior augmentation and adaptation strategies COMPARE approach. approach COMPARE model evaluation. model evaluation COMPARE approach. baseline ResNet models CONJUNCTION ResNet-50 models. ResNet-50 models CONJUNCTION baseline ResNet models. ImageNet - C CONJUNCTION ImageNet - R. ImageNet - R CONJUNCTION ImageNet - C. ResNet-50 models CONJUNCTION ImageNet - A distribution shift benchmarks. ImageNet - A distribution shift benchmarks CONJUNCTION ResNet-50 models. OtherScientificTerm are distribution shift, model training process, data augmentations, and model parameters. Task is test time robustification. Metric is model robustness. Generic are assumptions, model, and augmentations. ",This paper studies the problem of test time robustification in deep neural networks with in-distribution test points. The authors propose two methods for test time adaptation: (1) ResNet-50 models and (2) a robust vision transformer model. The proposed approach outperforms prior augmentation and adaptation strategies in terms of model evaluation. The main contribution of the paper is a theoretical analysis of the model robustness in the presence of distribution shift in the model training process. The paper also provides some theoretical guarantees for the assumptions. ,"This paper studies the problem of test time robustification in deep neural networks on in-distribution test points. The authors propose two methods for test time adaptation: (1) a robust vision transformer model, and (2) a prior augmentation and adaptation strategies. The proposed approach is evaluated on ImageNet-C, ResNet-50 models, and ImageNet - R, and compared to prior augmentmentation and adaptions. The results show that the proposed approach outperforms the baseline ResNet models and the robust ResNet50 models on all three datasets. "
5519,SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"RL CONJUNCTION planning. planning CONJUNCTION RL. model USED-FOR planning. model USED-FOR RL. accuracy EVALUATE-FOR they. model CONJUNCTION policy. policy CONJUNCTION model. single objective USED-FOR policy. single objective USED-FOR model. expected return FEATURE-OF lower bound. joint optimization USED-FOR objective mismatch. global lower bound FEATURE-OF expected return. global lower bound FEATURE-OF objective. algorithm ( MnM ) COMPARE GAN. GAN COMPARE algorithm ( MnM ). classifier USED-FOR real and fake transitions. Generic are template, models, it, bound, and algorithm. Metric is MSE. Task is control. Method is RL agent. OtherScientificTerm is policies. ","This paper studies the problem of RL and planning with a single objective. The authors propose a new template, called MSE, to learn a model for RL and a policy for planning. The goal is to minimize the expected return of the lower bound of the objective mismatch between the model and the policy. The paper shows that the proposed algorithm (MnM) achieves better performance than GAN. The main contribution of the paper is to show that MSE can be used to train a model that can learn a policy that is more accurate than the current state-of-the-art. ","This paper proposes a new model for RL and planning. The model is a combination of a single objective and a policy. The lower bound of the expected return of the lower bound is a joint optimization between the model and the policy, and the upper bound is an MSE. The authors show that the proposed algorithm (MnM) outperforms GAN in terms of accuracy, and they show that they can also outperform GAN on real and fake transitions. They also show that it is possible to improve the accuracy of the model by using joint optimization. "
5535,SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"single observations CONJUNCTION observation histories. observation histories CONJUNCTION single observations. single observations USED-FOR behavioral cloning policies. observation histories USED-FOR behavioral cloning policies. human decision making USED-FOR model combination approach. instantaneous observation USED-FOR coarse action. images CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks CONJUNCTION images. this COMPARE baselines. baselines COMPARE this. CARLA autonomous driving CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks CONJUNCTION CARLA autonomous driving. images USED-FOR CARLA autonomous driving. MuJoCo continuous control tasks EVALUATE-FOR this. CARLA autonomous driving EVALUATE-FOR this. MuJoCo continuous control tasks EVALUATE-FOR baselines. CARLA autonomous driving EVALUATE-FOR baselines. Attention maps of baseline imitation methods CONJUNCTION method. method CONJUNCTION Attention maps of baseline imitation methods. CARLA driving task EVALUATE-FOR method. single observation ( BC - SO ) USED-FOR Behavioral cloning. observation history ( BC - OH ) USED-FOR Behavioral cloning. method USED-FOR coarse - to - fine ” imitator. Method are control policy, and imitation - learned policies. Generic are approach, them, and it. OtherScientificTerm are instantaneous observations, historical information, and visual cues. ","This paper proposes a model combination approach for behavioral cloning based on human decision making. Behavioral cloning policies are learned from single observations and observation histories rather than from observation histories. The key idea is to use instantaneous observation for coarse action, and then use a control policy to predict the next state of the environment based on the current state. The authors show that this approach is able to learn a coarse-to-fine “imitation-learned” imitator, and that it can be combined with other baselines such as CARLA autonomous driving, MuJoCo continuous control tasks, and images. Experiments on the CARLA driving task demonstrate the effectiveness of the proposed method. ","This paper proposes a model combination approach for learning behavioral cloning policies based on single observations and observation histories. Behavioral cloning policies are learned from single observation (BC-SO) and observation history (BC-)OH. The key idea is to learn a control policy based on instantaneous observations and historical information, and then use them to learn imitation-learn policies. This is done by using human decision making. The proposed approach is evaluated on CARLA autonomous driving and MuJoCo continuous control tasks, where this outperforms baselines on the CARLA driving task. Experiments show that the proposed method can learn a coarse-to-fine “imitator” imitator, which is able to learn from instantaneous observation for coarse action, and from historical information to learn the coarse action from visual cues. "
5551,SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,deep learning models USED-FOR dynamics forecasting. generalization EVALUATE-FOR deep learning models. external forces CONJUNCTION boundary conditions. boundary conditions CONJUNCTION external forces. external forces FEATURE-OF systems. them USED-FOR tasks. DyAd HYPONYM-OF model - based meta - learning method. forecaster USED-FOR shared dynamics. encoder USED-FOR time - invariant hidden features. encoder USED-FOR task. parts PART-OF DyAd. encoder HYPONYM-OF parts. forecaster HYPONYM-OF parts. weak supervision USED-FOR encoder. forecaster PART-OF DyAd. encoder PART-OF DyAd. adaptive instance normalization CONJUNCTION adaptive padding. adaptive padding CONJUNCTION adaptive instance normalization. encoder USED-FOR forecaster. forecaster USED-FOR inference. adaptive padding USED-FOR encoder. adaptive instance normalization USED-FOR encoder. adaptive instance normalization USED-FOR forecaster. generalization error EVALUATE-FOR procedure. model COMPARE approaches. approaches COMPARE model. Generic is They. ,"This paper proposes a model-based meta-learning method called DyAd. DyAd is based on the idea that deep learning models can be used for dynamics forecasting. The authors show that DyAd has two parts: an encoder that predicts time-invariant hidden features, and a forecaster that predicts the shared dynamics between the encoder and the forecaster. The encoder is trained with weak supervision. The forecaster is trained using adaptive instance normalization and adaptive padding. The procedure is shown to have a lower generalization error than existing approaches. ","This paper proposes a model-based meta-learning method called DyAd, which is based on deep learning models for dynamics forecasting. DyAd consists of two parts: an encoder that predicts time-invariant hidden features, and a forecaster that predicts shared dynamics. The encoder is trained with weak supervision, and the forecaster is trained on a task with external forces and boundary conditions. The proposed procedure is evaluated on the generalization error of the proposed model compared to other approaches. "
5567,SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"Monocular 3D object detection HYPONYM-OF 3D scene understanding. manually annotated 3D box labels USED-FOR LiDAR point clouds. manually annotated 3D box labels USED-FOR monocular 3D detection methods. 2D boxes PART-OF image. 2D boxes USED-FOR RoI LiDAR points. network USED-FOR 3D boxes. 3D box estimates CONJUNCTION RoI LiDAR points. RoI LiDAR points CONJUNCTION 3D box estimates. 3D alignment loss FEATURE-OF 3D box estimates. 3D alignment loss USED-FOR network. method COMPARE fully supervised methods. fully supervised methods COMPARE method. KITTI EVALUATE-FOR method. OtherScientificTerm are ill - posed nature of monocular imagery, 3D box labels, weak supervision, and 3D box label. Task are annotation process, weakly supervised monocular 3D detection, and learning problem. ","This paper studies the problem of monocular 3D object detection in 3D scene understanding. The authors propose a new annotation process for weakly supervised monocular3D detection, which is based on manually annotated 3D box labels for LiDAR point clouds. The proposed method is evaluated on KITTI and shows that the proposed method outperforms fully supervised methods. ","This paper proposes a novel method for monocular 3D object detection based on the ill-posed nature of monocular imagery. Specifically, the authors propose to use manually annotated 3D box labels for LiDAR point clouds. The authors show that the proposed method outperforms fully supervised methods on KITTI. "
5583,SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"models USED-FOR natural language processing. rigid subword tokenization algorithms USED-FOR models. model inductive bias USED-FOR subword tokenization. characters USED-FOR latent subword representations. block scoring network USED-FOR GBST. CHARFORMER HYPONYM-OF deep Transformer model. GBST PART-OF deep Transformer model. CHARFORMER COMPARE subword - based models. subword - based models COMPARE CHARFORMER. CHARFORMER COMPARE byte - level baselines. byte - level baselines COMPARE CHARFORMER. multilingual, and noisy text datasets EVALUATE-FOR CHARFORMER. English GLUE CONJUNCTION multilingual, and noisy text datasets. multilingual, and noisy text datasets CONJUNCTION English GLUE. English GLUE EVALUATE-FOR CHARFORMER. byte - level baselines COMPARE subword - based models. subword - based models COMPARE byte - level baselines. Generic is model. OtherScientificTerm is byte level. Metric is competitive quality. ","This paper proposes a deep Transformer model called CHARFORMER, which is a combination of a GBST and a block scoring network. The main idea is to learn a set of characters that represent the latent subword representations of the input language. The authors show that the proposed model is competitive with existing models in terms of competitive quality. The experimental results on English GLUE and multilingual, and noisy text datasets show that CHARFORMer performs better than other subword-based models.","This paper proposes a novel deep Transformer model, called CHARFORMER, which is based on rigid subword tokenization algorithms for natural language processing. The authors propose to use model inductive bias to improve the performance of subwordtokenization. The key idea is to use characters to encode latent subword representations, and then use GBST, a block scoring network, to compute the competitive quality of the model. Experiments on English GLUE, multilingual, and noisy text datasets show that the proposed model outperforms other byte-level baselines."
5599,SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"Deep neural networks ( DNNs ) USED-FOR backdoor attacks. backdoor PART-OF DNNs. backdoor trigger USED-FOR DNNs. on - device deployed DNNs HYPONYM-OF real - world applications. adversarial objective USED-FOR backdoor detection. optimization perspective USED-FOR problem. adversarial objective USED-FOR solution. singularity FEATURE-OF adversarial map. adversarial map FEATURE-OF backdoorinfected example. skewed distribution FEATURE-OF solution. adversarial extreme value analysis ( AEVA ) USED-FOR backdoors. backdoors PART-OF black - box neural networks. extreme value analysis of the adversarial map USED-FOR AEVA. monte - carlo gradient estimation USED-FOR extreme value analysis of the adversarial map. approach USED-FOR backdoor attacks. backdoor attacks EVALUATE-FOR approach. black - box hard - label scenarios FEATURE-OF detecting backdoor attacks. Method are backdoor detection methods, and DNN. Material is poisoned training data. OtherScientificTerm are predictive confidence, and adversarial singularity phenomenon. ","This paper studies the problem of detecting backdoor attacks on deep neural networks (DNNs) in the presence of a backdoor in the training data. The authors propose a new adversarial objective for backdoor detection based on an optimization perspective. The proposed solution is based on the idea of adversarial extreme value analysis (AEVA), which can be applied to backdoors in black-box neural networks. The main idea of AEVA is to use monte-carlo gradient estimation to estimate the singularity of the adversarial map of the backdoorinfected example, which is then used to train a DNN with a backdoor trigger. The paper shows that the proposed approach is able to detect backdoor attacks in a variety of black-board hard-label scenarios.","This paper proposes a novel approach to detecting backdoor attacks in black-box hard-label scenarios. The authors propose a new adversarial objective for backdoor detection, which is based on the adversarial singularity phenomenon. The proposed approach is evaluated on two real-world applications: on-device deployed DNNs and on-Device deployed DNFs. The paper shows that the proposed approach outperforms other backdoor detection methods. The main contribution of the paper is to propose a novel adversarial extreme value analysis (AEVA) for detecting backdoors in the backdoors of black-Box neural networks. AEVA uses monte-carlo gradient estimation to estimate the singularity of an adversarial map of the backdoorinfected example, which can be used as a solution to the problem from an optimization perspective. In addition, the authors propose to use poisoned training data to improve predictive confidence of the DNN."
5615,SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"uncertainty estimation USED-FOR classifier. KLoS HYPONYM-OF Kullback – Leibler divergence criterion. class - probability simplex FEATURE-OF Kullback – Leibler divergence criterion. evidential models USED-FOR secondorder uncertainty representation. distributional information USED-FOR KLoS. KLoS USED-FOR class confusion. class - wise divergence measure EVALUATE-FOR KLoS. in - distribution samples USED-FOR class - wise divergence measure. auxiliary neural network USED-FOR refined criterion. KLoSNet USED-FOR refined criterion. KLoSNet HYPONYM-OF auxiliary neural network. misclassifications CONJUNCTION OOD samples. OOD samples CONJUNCTION misclassifications. KLoSNet USED-FOR OOD samples. KLoSNet USED-FOR misclassifications. measures COMPARE KLoS. KLoS COMPARE measures. Material are OOD training data, OOD data, and OOD dataset. Metric is second - order uncertainty measures. OtherScientificTerm is evidential training objective. ","This paper proposes a new Kullback-Leibler divergence criterion, KLoS, which is based on the class-probability simplex. The authors propose to use in-distribution samples to improve the performance of the classifier in the context of uncertainty estimation in OOD training data. The main idea is to use evidential models to learn the secondorder uncertainty representation of the OOD data, which can be used as a second-order uncertainty measures. The paper also proposes a refined criterion based on an auxiliary neural network, KloSNet, that is trained with the KLoLSNet. The empirical results show that the new class-wise divergence measure is better than existing measures, and that the proposed measures are more robust to misclassifications and OOD samples. ","This paper proposes a new Kullback-Leibler divergence criterion, KLoS, which is based on the class-probability simplex. The authors show that the proposed method can be applied to OOD training data, where the classifier is trained on OOD data, and the OOD dataset is used to train a second-order uncertainty measures. The secondorder uncertainty representation is learned using evidential models. The proposed method is evaluated on two OOD datasets, where it is shown to outperform other measures on misclassifications and OOD samples. "
5631,SP:8b4f3916dca4e627931558e14836749bd4a6792f,natural image data USED-FOR CNNs. semi - supervised algorithm USED-FOR linear classifier. datadependent features USED-FOR linear classifier. unlabeled data USED-FOR datadependent features. algorithm USED-FOR CNNs. natural distributional assumptions USED-FOR algorithm. it USED-FOR CNNs. low - dimensional structure FEATURE-OF distribution of patches. low - dimensional manifold USED-FOR patches. dimension of the patch distribution USED-FOR algorithm. Method is Convolutional networks ( CNN ). OtherScientificTerm is lower bound. ,"This paper proposes a semi-supervised algorithm for learning a linear classifier with datadependent features from unlabeled data. The algorithm is based on natural distributional assumptions on the distribution of patches in a low-dimensional manifold. The authors provide a lower bound on the dimension of the patch distribution, and show that it can be used to train CNNs with natural image data. ","This paper proposes a semi-supervised algorithm for learning a linear classifier with datadependent features from unlabeled data. The algorithm is based on natural distributional assumptions on CNNs on natural image data, and it can be applied to CNNs with Convolutional networks (CNN). The main idea of the algorithm is to learn a low-dimensional structure of the distribution of patches, which is defined as the dimension of the patch distribution. The authors provide a lower bound on the lower bound. They also provide a theoretical analysis of the proposed algorithm. "
5647,SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"face images USED-FOR Face clustering. representation capacity FEATURE-OF Graph Convolutional Networks ( GCN ). GCN - based methods USED-FOR face graphs. feature space FEATURE-OF kNN relations. kNN relations USED-FOR GCN - based methods. clean graphs USED-FOR GCNs. clean graphs USED-FOR algorithm. Ada - NETS HYPONYM-OF algorithm. face features USED-FOR robust features. adaptive neighbour discovery strategy USED-FOR edges. It USED-FOR graph. It USED-FOR noise edges. graph USED-FOR GCNs. clean yet rich edges FEATURE-OF graph. public clustering datasets EVALUATE-FOR Ada - NETS. Ada - NETS COMPARE state - of - the - art methods. state - of - the - art methods COMPARE Ada - NETS. public clustering datasets EVALUATE-FOR state - of - the - art methods. generalization EVALUATE-FOR Ada - NETS. OtherScientificTerm are structure space, and face image. ","This paper studies the representation capacity of Graph Convolutional Networks (GCN) in the face images. The authors propose a new algorithm called Ada-NETS, which is based on clean graphs. The main idea is to learn the kNN relations in the feature space of kNNs, and then use these relations to train GCN-based methods to learn face graphs. It is shown that this graph can be used to learn GCNs with clean yet rich edges. It can also be used as an adaptive neighbour discovery strategy to find edges in the graph. The paper also shows that Ada-NetS achieves better generalization than state-of-the-art methods on public clustering datasets. ","This paper studies the representation capacity of Graph Convolutional Networks (GCN) in terms of representation capacity. The authors propose a novel algorithm called Ada-NETS, which is based on clean graphs. The main idea is to learn face graphs in the feature space of kNN relations. The graph is constructed from clean yet rich edges of the graph. It is then used as an adaptive neighbour discovery strategy to find edges that are more robust to noise edges. The proposed algorithm is evaluated on public clustering datasets and shows better generalization compared to state-of-the-art methods. "
5663,SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"crossdomain representations USED-FOR direct cross - data evaluation. domain information CONJUNCTION camera IDs. camera IDs CONJUNCTION domain information. It USED-FOR features. demographics information USED-FOR features. demographics information USED-FOR It. domain information HYPONYM-OF demographics information. camera IDs HYPONYM-OF demographics information. distributionally robust optimization ( DRO ) USED-FOR learning robust models. uncertainty set HYPONYM-OF data distributions. convex condition FEATURE-OF KL DRO. convex condition FEATURE-OF overparameterized neural networks. real scenarios FEATURE-OF distribution shifts. change - of - measure technique USED-FOR approach. Unit DRO HYPONYM-OF approach. reweighted dataset USED-FOR Unit DRO. large - scale DG ReID CONJUNCTION cross - domain ReID benchmarks. cross - domain ReID benchmarks CONJUNCTION large - scale DG ReID. Unit DRO COMPARE baselines. baselines COMPARE Unit DRO. cross - domain ReID benchmarks EVALUATE-FOR baselines. large - scale DG ReID EVALUATE-FOR baselines. cross - domain ReID benchmarks EVALUATE-FOR Unit DRO. large - scale DG ReID EVALUATE-FOR Unit DRO. OtherScientificTerm are protected demographic features, and demographics. Method are robust models, and models. ","This paper proposes a new approach to cross-domain representation learning for cross-dataset cross-data evaluation. The proposed approach, Unit DRO, is based on distributionally robust optimization (DRO), which is a general framework for learning robust models with protected demographic features. It leverages demographics information such as domain information, camera IDs, and domain information to learn the features. The authors show that under the convex condition of KL DRO with the uncertainty set, the proposed approach outperforms existing baselines such as large-scale DG ReID and cross-domains ReID on a reweighted dataset. ","This paper proposes a new approach for direct cross-data evaluation based on crossdomain representations for learning robust models. It leverages domain information and camera IDs, which are protected demographic features, as well as demographics information for the features. The approach is based on distributionally robust optimization (DRO), which is an extension of the work of [Zhang et al., 2017] that uses a change-of-measure technique to measure the distribution shifts in overparameterized neural networks under the convex condition of KL DRO. The authors show that the proposed approach outperforms Unit DRO on a reweighted dataset, and outperforms other baselines such as large-scale DG ReID and cross-domain ReID benchmarks on real scenarios. "
5679,SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"Graph Neural Networks ( GNNs ) USED-FOR molecular property prediction. noise correction loss USED-FOR oversmoothing. noise USED-FOR overfitting. generic architectures USED-FOR quantum chemistry. methods USED-FOR regulariser. Noisy Nodes CONJUNCTION non - spatial architectures. non - spatial architectures CONJUNCTION Noisy Nodes. Open Graph Benchmark ( OGB ) datasets EVALUATE-FOR non - spatial architectures. Open Graph Benchmark ( OGB ) datasets EVALUATE-FOR Noisy Nodes. GNN toolkit USED-FOR 3D molecular property prediction. Method is GNNs. OtherScientificTerm are noise correcting node - level loss, and node latents. ",This paper studies the problem of molecular property prediction with Graph Neural Networks (GNNs) in the context of oversmoothing. The authors propose a novel noise correction loss to prevent overfitting in GNNs. The proposed noise correcting node-level loss is a regulariser that can be applied to any node latents. The paper shows that the proposed methods can be used to improve the performance of the regulariser. The experimental results on Noisy Nodes and non-spatial architectures on the Open Graph Benchmark (OGB) datasets demonstrate the effectiveness of the proposed GNN toolkit for 3D molecular property detection. ,"Graph Neural Networks (GNNs) are used for molecular property prediction in quantum chemistry. The authors propose to use generic architectures for quantum chemistry, where the noise correcting node-level loss is used to prevent oversmoothing due to overfitting due to noise. The main idea of GNNs is to learn a regulariser that is independent of the node latents. The proposed methods are evaluated on the Open Graph Benchmark (OGB) datasets and on non-spatial architectures. The GNN toolkit is used for 3D Molecular property prediction."
5695,SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"approaches USED-FOR complex interaction between set elements. ( Set)Transformers HYPONYM-OF approaches. self attention USED-FOR approaches. ( Set)Transformers HYPONYM-OF self attention. inducing - point attention CONJUNCTION optimal transport kernel embedding ( OTKE ). optimal transport kernel embedding ( OTKE ) CONJUNCTION inducing - point attention. mixture distribution FEATURE-OF i.i.d. samples. marginal likelihood maximization CONJUNCTION empirical Bayes. empirical Bayes CONJUNCTION marginal likelihood maximization. balanced assignment constraints FEATURE-OF E - step. OTKE HYPONYM-OF framework. balanced assignment constraints FEATURE-OF single - step EM. single - step EM HYPONYM-OF framework. set embedding CONJUNCTION prior - induced model regularization. prior - induced model regularization CONJUNCTION set embedding. OTKE COMPARE approach. approach COMPARE OTKE. approach USED-FOR set embedding. approach USED-FOR prior - induced model regularization. tasks EVALUATE-FOR approach. Task is set2vec problem. Method are vector representation, set embedding feed - forward network, ExpectationMaximization ( EM ) steps, MAP - EM steps, auto - diff backpropagation, and mixture set data fitting framework. OtherScientificTerm are variable number of feature vectors, mixture, and mixture parameters. Metric are computational overhead, and reduced computational cost. ","This paper studies the set2vec problem, where the goal is to learn a vector representation of the set embedding feed-forward network. The authors propose two approaches to learn the complex interaction between set elements: (Set)Transformers and (Optimal Transport Kernel Embedding) embedding (OTKE). The proposed approaches are based on self attention, which is an extension of self attention in self attention. The proposed framework is based on the single-step EM with balanced assignment constraints on the E-step. Theoretically, the authors show that the mixture distribution of the i.i.d. samples is a function of the variable number of feature vectors in the mixture, and the mixture parameters can be represented by a mixture set data fitting framework. ExpectationMaximization (EM) steps are used to approximate the MAP-EM steps. The paper also shows that the proposed approach outperforms OTKE and prior-induced model regularization on two tasks.","This paper presents a set2vec problem where the vector representation is a set embedding feed-forward network. The authors propose two approaches for learning complex interaction between set elements, namely (Set)Transformers and (OTKE). Both approaches are based on self attention, where the variable number of feature vectors is a mixture of i.i.d. samples, and the mixture parameters are a mixture distribution. ExpectationMaximization (EM) steps are used to learn the MAP-EM steps. The proposed framework is based on single-step EM with balanced assignment constraints on the E-step, marginal likelihood maximization and empirical Bayes. Experiments show that the proposed approach outperforms OTKE on set embeddings and prior-induced model regularization. The paper also shows that the reduced computational overhead is due to auto-diff backpropagation, which leads to a mixture set data fitting framework."
5711,SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,unsupervised feature selection USED-FOR informative features. informative features USED-FOR unknown downstream tasks. unsupervised feature selection USED-FOR unknown downstream tasks. CA setting USED-FOR feature selection. machine learning community USED-FOR feature selection. method USED-FOR feature selection. feature selection USED-FOR CA setting. semi - synthetic dataset CONJUNCTION real - world biomedical datasets. real - world biomedical datasets CONJUNCTION semi - synthetic dataset. state - of - the - art methods USED-FOR unsupervised feature selection scenarios. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. semi - synthetic dataset EVALUATE-FOR method. real - world biomedical datasets EVALUATE-FOR method. Task is contrastive analysis ( CA ) setting. Generic is background dataset. OtherScientificTerm is genes. Material is genomic data. ,This paper proposes a contrastive analysis (CA) setting where the goal is to find the most informative features for unknown downstream tasks using unsupervised feature selection in the CA setting. The proposed method is based on the machine learning community for feature selection. The authors show that the proposed method outperforms state-of-the-art methods in a number of unsupersupervised and semi-synthetic datasets as well as real-world biomedical datasets. ,This paper proposes a contrastive analysis (CA) setting where the goal is to learn informative features for unknown downstream tasks. The authors propose a method for feature selection in the CA setting using the machine learning community. The proposed method is evaluated on a semi-synthetic dataset and two real-world biomedical datasets. The results show that it outperforms state-of-the-art methods in unsupervised feature selection scenarios. 
5727,SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"Early stopping USED-FOR over - training neural networks. optimal early stopping time CONJUNCTION model dimension. model dimension CONJUNCTION optimal early stopping time. optimal early stopping USED-FOR double descent ”. early stopping USED-FOR generalization. Method are linear regression models, linear models, and deep neural network. Task is deep learning tasks. Generic is model. OtherScientificTerm is features. ","This paper studies the problem of early stopping in over-training neural networks. The authors consider the case of linear regression models, where the model is trained on a set of features and the goal is to generalize the learned model to new tasks. They show that the optimal early stopping time and the model dimension can be used to improve the generalization of linear models. They also show that early stopping can improve the performance of “double descent” in the presence of a deep neural network. ",This paper proposes to use early stopping in over-training neural networks to improve generalization of linear regression models. The authors show that optimal early stopping time and model dimension can be used to improve the “double descent” of linear models. They also show that early stopping improves generalization in deep learning tasks. The main contribution of the paper is to show that the model can generalize better if the features of the deep neural network are more diverse.
5743,SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"Policy gradient algorithms USED-FOR reinforcement learning ( RL ) problems. Regularization USED-FOR exploration. Regularization USED-FOR stability. entropy functions USED-FOR stability. entropy functions USED-FOR exploration. entropy functions FEATURE-OF Regularization. quasi - Newton method USED-FOR policy gradient algorithm. entropy regularization USED-FOR quasi - Newton method. algorithm USED-FOR natural policy gradient ( NPG ) algorithm. method USED-FOR policy gradient algorithms. method USED-FOR entropy functions. Newton - type quadratic convergence FEATURE-OF algorithms. quasi - Newton method COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE quasi - Newton method. synthetic and industrial - scale examples EVALUATE-FOR quasi - Newton method. single - digit iterations FEATURE-OF quasi - Newton method. OtherScientificTerm are Shannon entropy, and optimal policy. ",This paper proposes a new quasi-Newton method for learning policy gradient algorithms for reinforcement learning (RL) problems. Regularization of the entropy functions of the policy gradient algorithm is used to improve the stability of entropy functions for exploration. The proposed algorithm is a natural policy gradient (NPG) algorithm with Shannon entropy. The authors show that the proposed method achieves Newton-type quadratic convergence with respect to state-of-the-art algorithms in both synthetic and industrial-scale examples. ,"This paper proposes a new method for learning policy gradient algorithms for reinforcement learning (RL) problems. The authors propose a quasi-Newton method that uses entropy regularization to improve the stability of the policy gradient algorithm. Regularization is a regularization of the entropy functions for stability and exploration. The proposed algorithm is a natural policy gradient (NPG) algorithm with Shannon entropy. The method is evaluated on synthetic and industrial-scale examples, and is shown to outperform state-of-the-art algorithms in terms of Newton-type quadratic convergence. Experiments are performed on single-digit iterations."
5759,SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"generalization CONJUNCTION sample efficiency. sample efficiency CONJUNCTION generalization. Text - based games ( TBG ) USED-FOR grounded language understanding. generalization HYPONYM-OF problems. sample efficiency HYPONYM-OF problems. deep reinforcement learning ( RL ) methods USED-FOR TBGs. case - based reasoning USED-FOR general method. on - policy neural agent USED-FOR TBGs. method CONJUNCTION on - policy neural agent. on - policy neural agent CONJUNCTION method. method USED-FOR TBGs. approach COMPARE methods. methods COMPARE approach. out - of - distribution generalization EVALUATE-FOR methods. out - of - distribution generalization EVALUATE-FOR approach. OtherScientificTerm is distributional shifts. Method are deep RL approaches, and case - based reasoner. ",This paper studies the problem of grounded language understanding in Text-based games (TBG). The authors consider two problems: generalization and sample efficiency in TBGs. The authors propose a general method based on case-based reasoning based on a on-policy neural agent to solve TBGs using deep reinforcement learning (RL) methods. They show that the proposed approach can achieve better out-of-distribution generalization than existing methods. ,"This paper proposes a novel approach to tackle the problem of grounded language understanding in Text-based games (TBG) where the goal is to improve generalization and sample efficiency. The authors propose to use deep reinforcement learning (RL) methods to tackle TBGs. The general method is based on case-based reasoning, where the distributional shifts are considered. The method is combined with an on-policy neural agent to solve TBGs, and is shown to outperform other deep RL approaches. The approach is evaluated on out-of-distribution generalization, and outperforms other methods. "
5775,SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,Pre - trained contextual language models USED-FOR language understanding tasks. sense information USED-FOR multi - sense embeddings. multi - sense embeddings PART-OF skip - gram - like framework. pre - trained language model ( BERT ) USED-FOR two - stage method. approach USED-FOR sense disambiguation mechanism. sense disambiguation mechanism PART-OF model. output layer embeddings PART-OF BERT. distribution over word senses USED-FOR approach. BERT USED-FOR distribution over word senses. output layer embeddings USED-FOR distribution over word senses. method COMPARE multi - sense embeddings. multi - sense embeddings COMPARE method. contextual word similarity CONJUNCTION sense induction tasks. sense induction tasks CONJUNCTION contextual word similarity. sense induction tasks EVALUATE-FOR method. contextual word similarity EVALUATE-FOR method. embedding - based topic model ( ETM ) EVALUATE-FOR multi - sense embedding. multiple benchmark data sets EVALUATE-FOR method. multiple benchmark data sets EVALUATE-FOR multi - sense embeddings. Task is resource - constrained systems. Method is Noncontextual word embeddings. Generic is methods. OtherScientificTerm is polysemy. ,"This paper proposes a two-stage method for learning multi-sense embeddings from pre-trained contextual language models for language understanding tasks with resource-constrained systems. The proposed skip-gram-like framework is based on the BERT model, which is a combination of the output layer of BERT and the sense disambiguation mechanism of the model. The approach uses the distribution over word senses from BERT to learn the sense information for the multi- sense embedding. The method is evaluated on contextual word similarity and sense induction tasks on multiple benchmark data sets. The results show that the proposed method performs better than existing methods on the embedding-based topic model (ETM). ",This paper proposes a two-stage method for learning contextual language models for language understanding tasks. The first stage uses a pre-trained language model (BERT) to learn multi-sense embeddings in a skip-gram-like framework. The second stage uses an approach to learn a sense disambiguation mechanism based on the distribution over word senses. The authors show that the proposed method is able to learn more contextual word similarity and more sense induction tasks than other methods. The method is evaluated on multiple benchmark data sets and outperforms other multi- sense embedding based topic model (ETM). 
5800,SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"3D point - clouds CONJUNCTION 2D images. 2D images CONJUNCTION 3D point - clouds. 3D point - clouds HYPONYM-OF visual representations of the physical world. 2D images HYPONYM-OF visual representations of the physical world. computer vision models USED-FOR 2D image and 3D point - cloud understanding. human vision USED-FOR representations. 2D model architectures USED-FOR 3D point - clouds. neural net model USED-FOR images. architecture USED-FOR neural net model. image - pretrained model CONJUNCTION point - cloud model. point - cloud model CONJUNCTION image - pretrained model. 2D convolutional filters CONJUNCTION 3D convolutional filters. 3D convolutional filters CONJUNCTION 2D convolutional filters. finetuning efforts FEATURE-OF models. batch normalization layers USED-FOR models. 3D point - cloud classification EVALUATE-FOR models. task - specific architectures USED-FOR point - cloud models. few - shot classification EVALUATE-FOR FIP. data efficiency EVALUATE-FOR FIP. It USED-FOR point - cloud models. Generic are transfer, and model. Method is inflated imagepretrained models ( FIP ). ","This paper studies the problem of 2D image and 3D point-cloud understanding using computer vision models. The authors propose a new architecture for training a neural net model to generate images from a neural network and then transfer the images to a 2D model. The model is trained using a combination of image-pretrained model and point cloud model, where the 2D convolutional filters are used to generate the 3D points and the point cloud is trained with batch normalization layers. The proposed model is evaluated on few-shot classification, and shows that the FIP improves the data efficiency and finetuning efforts of existing models. ","This paper proposes a new architecture for learning 3D point-clouds and 2D images. The authors propose to use an image-pretrained model (FIP) to learn the point clouds, and a point-precision model (PPC) to generate the 2D point clouds. The point clouds are generated by a neural net model that is trained on a set of images, and the point cloud is generated by the PPC. The PPC is then used to train a point cloud model on top of the image-prerained model. The proposed architecture is evaluated on few-shot classification and 3D Point-Cloud classification tasks. The results show that the proposed FIP improves the data efficiency of the models in terms of finetuning efforts. "
5825,SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"Autoregressive generative models USED-FOR tasks. sequential data USED-FOR tasks. exposure bias CONJUNCTION long - range coherence. long - range coherence CONJUNCTION exposure bias. method USED-FOR autoregressive generative model. energy - based learning objective USED-FOR method. method USED-FOR exposure bias problem. constraint USED-FOR joint distributions. method USED-FOR temporal coherence. exposure bias problem CONJUNCTION temporal coherence. temporal coherence CONJUNCTION exposure bias problem. constraint USED-FOR method. energy - based models USED-FOR energy scores. autoregressive network USED-FOR energy scores. importance sampling USED-FOR model. language modeling CONJUNCTION neural machine translation. neural machine translation CONJUNCTION language modeling. neural machine translation CONJUNCTION image generation. image generation CONJUNCTION neural machine translation. benchmarks EVALUATE-FOR approach. image generation HYPONYM-OF benchmarks. language modeling HYPONYM-OF benchmarks. neural machine translation HYPONYM-OF benchmarks. Generic are They, and network. Method are chain - style conditional modeling, and MCMC process. OtherScientificTerm is distributions. ","This paper proposes a method to train an autoregressive generative model on sequential data for tasks with sequential data. The proposed method is based on an energy-based learning objective, where the goal is to learn the energy scores of the network. The method is motivated by the observation that, in chain-style conditional modeling, the distribution of the learned energy scores can be highly correlated with the number of samples in the MCMC process. The authors propose a constraint to ensure that the joint distributions of the two distributions are close to each other. They then use this constraint to train the model by importance sampling, which is a common practice in the literature. They show that the proposed method can improve the performance of their method on the exposure bias problem and the long-range coherence. They also show that their method can also improve the temporal coherence and the performance on various benchmarks such as language modeling, neural machine translation, and image generation.","This paper proposes a method for training an autoregressive generative model on sequential data for tasks with sequential data. The method is based on chain-style conditional modeling. The authors propose a new method that uses an energy-based learning objective and a constraint on the joint distributions of the distributions. They show that the proposed method can improve the exposure bias and long-range coherence. They also show that energy scores obtained by the proposed model can be computed by using an auto-regressive network with importance sampling. The proposed approach is evaluated on three benchmarks: image generation, language modeling, and neural machine translation. "
5850,SP:51e748c55bd4134047098559577fa3f37aa7433a,"adversarial attacks FEATURE-OF deep neural networks ( DNNs ). adversarial training ( AT ) method USED-FOR DNN - based classifier. adversarial training ( AT ) method USED-FOR robustness. robustness EVALUATE-FOR DNN - based classifier. adversarial examples USED-FOR adversarial training ( AT ) method. pointwise adversary USED-FOR worst - case adversarial example. PGD - AT and TRADES HYPONYM-OF AT - based methods. pointwise adversary USED-FOR AT - based methods. unified framework USED-FOR Wasserstein distributional robustness. Wasserstein distributional robustness COMPARE AT methods. AT methods COMPARE Wasserstein distributional robustness. Wasserstein cost function CONJUNCTION risk functions. risk functions CONJUNCTION Wasserstein cost function. AT methods PART-OF framework. distributional robustness AT algorithms COMPARE AT counterparts. AT counterparts COMPARE distributional robustness AT algorithms. Method are deep learning systems, classifier, and distributional robustness AT - based algorithms. OtherScientificTerm is adversarial effects. ","This paper studies the problem of robustness against adversarial attacks in deep neural networks (DNNs). The authors propose an adversarial training (AT) method to improve the robustness of a DNN-based classifier trained with adversarial examples. The AT-based methods are based on PGD-AT and TRADES, where the pointwise adversary is used to train the worst-case adversarial example. The authors then propose a unified framework for Wasserstein distributional robustness using AT methods and risk functions. They show that the AT methods are more robust than their AT counterparts. ","This paper studies adversarial attacks on deep neural networks (DNNs). The authors propose an adversarial training (AT) method to improve the robustness of a DNN-based classifier against adversarial examples. The authors introduce a unified framework for Wasserstein distributional robustness. The framework consists of two AT methods, PGD-AT and TRADES, where AT-based methods use a pointwise adversary to train a worst-case adversarial example, while AT methods use Wassersteins distributional and risk functions to train the classifier. The proposed framework is evaluated on a variety of deep learning systems. The results show that the AT methods are more robust to adversarial effects than their AT counterparts. "
5875,SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"complex dynamics CONJUNCTION sparse annotations. sparse annotations CONJUNCTION complex dynamics. Unsupervised representation learning USED-FOR multivariate time series. it COMPARE problem. problem COMPARE it. data augmentation techniques USED-FOR contrastive training. time slicing USED-FOR segmentlevel augmentation. Bilinear Temporal - Spectral Fusion ( BTSF ) HYPONYM-OF framework. dropout USED-FOR capturing long - term dependencies. dropout USED-FOR global context. instance - level augmentation USED-FOR capturing long - term dependencies. dropout USED-FOR time series. segment - level augmentation COMPARE instance - level augmentation. instance - level augmentation COMPARE segment - level augmentation. global context CONJUNCTION capturing long - term dependencies. capturing long - term dependencies CONJUNCTION global context. dropout USED-FOR instance - level augmentation. iterative bilinear temporal - spectral fusion module USED-FOR affinities. iterative bilinear temporal - spectral fusion module USED-FOR representations of time series. cross - domain interactions USED-FOR representations of time series. alignment CONJUNCTION uniformity. uniformity CONJUNCTION alignment. BTSF USED-FOR bilinear feature representations. forecasting CONJUNCTION anomaly detection. anomaly detection CONJUNCTION forecasting. classification CONJUNCTION forecasting. forecasting CONJUNCTION classification. anomaly detection HYPONYM-OF tasks. tasks USED-FOR time series. anomaly detection HYPONYM-OF time series. classification HYPONYM-OF time series. forecasting HYPONYM-OF time series. anomaly detection HYPONYM-OF tasks. classification HYPONYM-OF tasks. forecasting HYPONYM-OF tasks. BTSF COMPARE state - of - the - art methods. state - of - the - art methods COMPARE BTSF. BTSF COMPARE them. them COMPARE BTSF. Method are contrastive learning, representation learning framework, augmentation methods, and feature representation. OtherScientificTerm is sampling bias. Task is optimization. ","This paper studies the problem of unsupervised representation learning for multivariate time series with complex dynamics and sparse annotations. The authors propose a new contrastive learning framework, Bilinear Temporal-Spectral Fusion (BTSF), which combines data augmentation techniques for contrastive training with dropout for capturing long-term dependencies and capturing global context. BTSF uses an iterative bilinear temporal-spectral fusion module to learn representations of time series based on cross-domain interactions, and then uses dropout to augment the time series by segment-level augmentation using time slicing. The paper shows that the proposed method outperforms state-of-the-art methods in terms of alignment and uniformity.","This paper proposes a new representation learning framework for multivariate time series. The proposed Bilinear Temporal-Spectral Fusion (BTSF) is a framework for unsupervised representation learning in multivariate multi-modal time series, where the data augmentation techniques are used for contrastive training. In contrastive learning, the goal is to minimize the sampling bias of the feature representation. The authors propose two augmentation methods: (1) segmentlevel augmentation using time slicing, and (2) dropout for capturing long-term dependencies and capturing the global context. BTSF uses an iterative bilinear temporal-spectral fusion module to capture the affinities of time series with cross-domain interactions. Experiments are conducted on a variety of tasks, including classification, forecasting, anomaly detection, and anomaly detection. Results show that the proposed model outperforms state-of-the-art methods on all of them. "
5900,SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"model architecture CONJUNCTION batch size. batch size CONJUNCTION model architecture. learning rate USED-FOR deep neural networks. algorithm USED-FOR learning rate. gradient descent USED-FOR learning rate. learning rate CONJUNCTION model weights. model weights CONJUNCTION learning rate. approach USED-FOR learning rate. gradient descent step USED-FOR Learning rate. learning rate FEATURE-OF first and second - order gradients. scheme USED-FOR learning rates. learning rate CONJUNCTION batch size. batch size CONJUNCTION learning rate. it USED-FOR optimizing scheme. optimizing scheme EVALUATE-FOR method. Method are line - search, and neural networks. OtherScientificTerm is weight gradients. ","This paper proposes a new algorithm for optimizing the learning rate of deep neural networks by gradient descent. The main idea is to use the gradient descent step as a learning rate for the first and second-order gradients of the weight gradients. The authors show that this approach can be applied to any learning rate, model architecture and batch size. They also show that it can be used as an optimizing scheme for learning rates, learning rate and model weights. ",The paper proposes a new algorithm for learning rate for deep neural networks. The idea is to use gradient descent to optimize the learning rate of the first and second-order gradients of the model architecture and batch size. The authors propose a new approach to optimize learning rate using gradient descent step. They show that the proposed scheme can optimize learning rates for both learning rate and model weights. They also show that it can be combined with an existing optimizing scheme. 
5925,SP:263c787361cd6d4443ce516d389c694d0fe44b28,"continual meta - learning method USED-FOR sequential multi - task learning. RL USED-FOR offline meta - learning. prior continual learning CONJUNCTION off - policy meta - reinforcement methods. off - policy meta - reinforcement methods CONJUNCTION prior continual learning. CoMPS COMPARE off - policy meta - reinforcement methods. off - policy meta - reinforcement methods COMPARE CoMPS. CoMPS COMPARE prior continual learning. prior continual learning COMPARE CoMPS. continuous control tasks EVALUATE-FOR off - policy meta - reinforcement methods. continuous control tasks EVALUATE-FOR CoMPS. Method are Prior meta - reinforcement learning algorithms, continual reinforcement learning algorithms, continual meta - policy search ( CoMPS ), and meta - training. Generic are they, and method. ","This paper proposes a continuous meta-learning method for sequential multi-task learning. Prior meta-reinforcement learning algorithms, such as continual meta-policy search (CoMPS) and continual continual learning (CoL) are not well-suited for offline meta-training. The authors propose a new method, CoMPS, that combines prior continual learning with off-policy meta-regression methods. The proposed method is evaluated on a variety of continuous control tasks, and shows that CoL outperforms prior continual training.","This paper proposes a continual meta-learning method for sequential multi-task learning. Prior meta-reinforcement learning algorithms have been shown to outperform continual reinforcement learning algorithms. However, they do not perform well in offline meta - learning in RL. To address this issue, the authors propose continual meta meta-policy search (CoMPS), which is a continuous meta-training method. CoMPS outperforms prior continual learning and off-policy meta-Reinforcement methods on continuous control tasks."
5950,SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"“ backdoor ” poisoning attack USED-FOR classification models. threat model USED-FOR poisoned classifier. threat model USED-FOR poisoned classifier. adversarial examples USED-FOR classifier. human interaction FEATURE-OF smoothed adversarial images. ImageNet CONJUNCTION TrojAI. TrojAI CONJUNCTION ImageNet. ImageNet HYPONYM-OF high - resolution datasets. TrojAI HYPONYM-OF high - resolution datasets. high - resolution datasets EVALUATE-FOR attack. method USED-FOR triggers. approach COMPARE method. method COMPARE approach. approach COMPARE modeling trigger distributions. modeling trigger distributions COMPARE approach. backdoors PART-OF poisoned classifiers. secret backdoor PART-OF poisoned classifiers. Method are backdoored classifiers, and Denoised Smoothing. Generic is procedure. OtherScientificTerm is trigger distributions. ","This paper studies the “backdoor” poisoning attack against classification models. The authors propose to use a threat model to poison the poisoned classifier by using adversarial examples to fool the classifier. The proposed procedure is based on Denoised Smoothing, which is a well-studied technique for backdoored classifiers. The paper shows that the proposed method is able to generate triggers that are more sensitive to human interaction in smoothed adversarial images. Experiments on two high-resolution datasets, ImageNet and TrojAI, show that this approach can achieve better performance than the state-of-the-art on modeling trigger distributions.","This paper proposes a “backdoor” poisoning attack against classification models. The proposed procedure is based on Denoised Smoothing, where a threat model is used to train a poisoned classifier on adversarial examples. The authors demonstrate the effectiveness of the proposed attack on high-resolution datasets such as ImageNet and TrojAI. They also show that the proposed method is able to detect triggers that are not present in the training data, and that modeling trigger distributions is more effective than backdoors. "
5975,SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"Generative adversarial networks ( GANs ) USED-FOR content generation tasks. GAN compression methods USED-FOR conditional GANs. distilling unconditional GAN USED-FOR StyleGAN2 architecture. output discrepancy issue FEATURE-OF unconditional GAN distillation. heterogeneous distillation scenario FEATURE-OF knowledge distillation losses. style module USED-FOR semantic information. initialization strategy USED-FOR student model. initialization strategy USED-FOR output consistency. semantic consistency FEATURE-OF teacher and student model. latent - direction - based distillation loss USED-FOR semantic relations. latent space FEATURE-OF semantic relations. latent - direction - based distillation loss USED-FOR semantic consistency. approach USED-FOR StyleGAN2. approach COMPARE GAN distillation methods. GAN distillation methods COMPARE approach. GAN distillation methods USED-FOR distilling StyleGAN2. OtherScientificTerm are computation, resource - constrained devices, latent code, and discrepancy issue. ","This paper studies the problem of distilling unconditional GANs in the heterogeneous distillation scenario in the context of content generation tasks. The authors propose a new distillation method, called StyleGAN2, which is based on the distillation of a GAN with a style module. The idea is to distill the semantic information from the teacher and student model using an initialization strategy to ensure that the output consistency between the two. The proposed approach is shown to achieve better performance than existing GAN distillation methods in the distilling styleGAN2 architecture.","This paper proposes a novel approach to distilling StyleGAN2, a generative adversarial networks (GANs) for content generation tasks. The main idea is to distill conditional GANs using GAN compression methods. The authors show that the output discrepancy issue of unconditional GAN distillation suffers from a heterogeneous distillation scenario in the context of knowledge distillation losses, and propose an initialization strategy to improve the output consistency of the student model and the teacher and student model. They also propose a style module to extract semantic information from the latent code. The proposed approach is evaluated on a variety of tasks and compared to other GANdistillation methods. "
6000,SP:2c2231743fa33b95828c6615263954ce1c05f95d,methodology USED-FOR offline algorithms. online settings FEATURE-OF offline algorithms. multi - task learning model USED-FOR behavioral structures. graphs USED-FOR offline algorithms. synthetic data CONJUNCTION historical stock market data. historical stock market data CONJUNCTION synthetic data. historical stock market data EVALUATE-FOR methodology. synthetic data EVALUATE-FOR methodology. ,This paper proposes a new methodology for offline algorithms in online settings. The authors propose a multi-task learning model to learn behavioral structures from graphs. The proposed methodology is evaluated on synthetic data and historical stock market data.,This paper presents a methodology for learning offline algorithms in online settings. The authors propose a multi-task learning model to learn behavioral structures that can be used for offline algorithms on graphs. The methodology is evaluated on synthetic data and historical stock market data.
6025,SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"Gaussian Processes ( GPs ) HYPONYM-OF Bayesian models. Bayesian models USED-FOR uncertainty estimates. variational inference USED-FOR inferring q. Sparse GPs CONJUNCTION variational inference. variational inference CONJUNCTION Sparse GPs. Sparse GPs USED-FOR GPs. it COMPARE sparse variational GP approaches. sparse variational GP approaches COMPARE it. neural network USED-FOR inducing points locations. training and prediction times EVALUATE-FOR method. Generic are They, them, model, and they. Method is sparse GP approximations. OtherScientificTerm is latent function. Task are learning tasks, and prediction. ","This paper studies the problem of estimating uncertainty estimates for Bayesian models with Gaussian Processes (GP). They consider the case where the latent function is Gaussian, and the model is trained with sparse GP approximations. Sparse GPs and variational inference are used for inferring q. The authors show that the proposed method can achieve better training and prediction times than sparse variational GP approaches. They also show that it is possible to learn inducing points locations from a neural network. ","This paper proposes Gaussian Processes (GPs), a family of Bayesian models for uncertainty estimates. Sparse GPs and variational inference for inferring q are used for learning tasks. They can be seen as a special case of sparse GP approximations, where the latent function is a function of the number of parameters of the model. The authors show that they can be used to improve the training and prediction times. They also show that it outperforms sparse variational GP approaches on inducing points locations. "
6050,SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"scientific collaborations CONJUNCTION volunteer computing. volunteer computing CONJUNCTION scientific collaborations. hardest problems PART-OF deep learning. Byzantine tolerance FEATURE-OF distributed training algorithms. algorithms USED-FOR large - scale distributed deep learning. protocol USED-FOR secure ( Byzantinetolerant ) decentralized training. communication efficiency FEATURE-OF protocol. theoretical bounds USED-FOR resistance. Byzantine and Sybil attacks FEATURE-OF resistance. communication overhead FEATURE-OF it. Byzantine attackers FEATURE-OF image classification and language modeling. Generic are systems, and models. Metric is efficiency. OtherScientificTerm are redundant communication, and trusted server. ","This paper proposes a new protocol for secure (Bhinetolerant) decentralized training with Byzantine tolerance in distributed training algorithms. The authors consider the hardest problems in deep learning, such as scientific collaborations and volunteer computing, where the number of clients is large and the communication overhead is high. They show that such systems are vulnerable to Byzantine attackers in image classification and language modeling. They propose a protocol that is robust to Byzantine and Sybil attacks, and provide theoretical bounds for the resistance. They also show that the proposed protocol has communication efficiency in terms of communication efficiency. ","This paper proposes a protocol for secure (Bhenryantinetolerant) decentralized training, which is based on the Byzantine tolerance of distributed training algorithms for the hardest problems in deep learning. The authors show that the proposed protocol improves the communication efficiency and the communication overhead for large-scale distributed deep learning with redundant communication. They also provide theoretical bounds for the resistance to Byzantine and Sybil attacks in the context of image classification and language modeling, and show that it is more robust to Byzantine attackers than existing models. "
6075,SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"Smoothed particle hydrodynamics ( SPH ) HYPONYM-OF mesh - free Lagrangian method. astrophysics and engineering applications FEATURE-OF weaklyand strongly compressible turbulence. physics based parameters CONJUNCTION Neural Networks. Neural Networks CONJUNCTION physics based parameters. Neural Networks USED-FOR universal function approximators. Neural Networks USED-FOR SPH informed fluid simulators. physics based parameters USED-FOR SPH informed fluid simulators. forward and adjoint based sensitivity analyses USED-FOR gradient based optimization. learning algorithm USED-FOR mixed mode approach. physics informed learning method USED-FOR inverse problems. physically interpretable parameter space FEATURE-OF inverse problems. time scales CONJUNCTION Reynolds numbers. Reynolds numbers CONJUNCTION time scales. interpretability CONJUNCTION generalizability. generalizability CONJUNCTION interpretability. hierarchy of models USED-FOR physical structure. Reynolds numbers FEATURE-OF generalizability. time scales FEATURE-OF generalizability. OtherScientificTerm are Neural Network parameters, Lagrangian statistics of turbulence, and physical symmetries. Material is training data. ","This paper proposes a mesh-free Lagrangian method, Smoothed particle hydrodynamics (SPH), for the problem of weaklyand strongly compressible turbulence in astrophysics and engineering applications. The authors use physics based parameters and Neural Networks to learn universal function approximators for SPH informed fluid simulators. They use forward and adjoint based sensitivity analyses to perform gradient based optimization. The learning algorithm is based on a mixed mode approach, where the Neural Network parameters are learned in a hierarchy of models, and the physical structure of the physical symmetries are learned from training data. The physics informed learning method is applied to inverse problems with physically interpretable parameter space, and is shown to improve generalizability on time scales and Reynolds numbers.","This paper proposes a mesh-free Lagrangian method for Smoothed particle hydrodynamics (SPH) for astrophysics and engineering applications. Neural Networks are used to learn universal function approximators, and physics based parameters are used in SPH informed fluid simulators for weaklyand strongly compressible turbulence. The authors use forward and adjoint based sensitivity analyses for gradient based optimization, and use a learning algorithm for mixed mode approach. The Neural Network parameters are learned in a hierarchy of models, and the physical structure of turbulence is learned using physical symmetries. The proposed physics informed learning method is applied to inverse problems in the physically interpretable parameter space, and is shown to improve generalizability on time scales and Reynolds numbers."
6100,SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"approach USED-FOR deterministic neural network. accuracy CONJUNCTION uncertainty estimates. uncertainty estimates CONJUNCTION accuracy. entropy maximization regularizer USED-FOR predictive distribution. entropy maximization regularizer USED-FOR approach. embedding space FEATURE-OF predictive distribution. cross - entropy loss USED-FOR approach. entropy FEATURE-OF samples. images USED-FOR convex combination. convex combination USED-FOR synthetically generating between - cluster samples. data - dependent regularization USED-FOR maximum likelihood estimation. data - dependent regularization USED-FOR solution. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. real - world datasets EVALUATE-FOR Mix - MaxEnt. calibrated probabilities USED-FOR in - distribution data. Mix - MaxEnt USED-FOR uncertainty estimates. ResNet and Wide - ResNet architectures USED-FOR real - world datasets. CIFAR-10 HYPONYM-OF real - world datasets. CIFAR-100 HYPONYM-OF real - world datasets. classification accuracy EVALUATE-FOR Mix - MaxEnt. OtherScientificTerm are class clusters, out - of - distribution samples, high entropy regions, entropy barrier, and superficial input perturbations. ","This paper proposes Mix-MaxEnt, a new approach for training a deterministic neural network. The proposed approach uses an entropy maximization regularizer to learn a predictive distribution over the embedding space of a class clusters. The approach is based on a cross-entropy loss, where the samples are generated by a convex combination of images. The key idea is to generate out-of-distribution samples with high entropy regions, which are then used for maximum likelihood estimation using a data-dependent regularization. The authors show that the proposed solution achieves better classification accuracy and uncertainty estimates than Mix-maxEnt on a variety of real-world datasets such as CIFAR-10, CifAR-100, and Cifar-100. ",This paper proposes a novel approach for training a deterministic neural network. The approach is based on an entropy maximization regularizer that maximizes the predictive distribution in the embedding space of the class clusters. The proposed approach uses a cross-entropy loss on the samples generated by a convex combination of images. The authors show that the proposed solution can be combined with data-dependent regularization for maximum likelihood estimation. They also show that Mix-MaxEnt can improve classification accuracy and uncertainty estimates on in-distribution data with calibrated probabilities. The paper also shows that the approach can be applied to synthetically generating between-cluster samples with high entropy regions. 
6125,SP:365490b872464f00634dc7a50d024fceaf0a61ee,"Generative Adversarial Networks ( GANs ) CONJUNCTION auto - encoder animating images. auto - encoder animating images CONJUNCTION Generative Adversarial Networks ( GANs ). driving videos USED-FOR structure representation. structure representation USED-FOR animation - approaches. modules USED-FOR animation - model. modules USED-FOR extraction of structure information. Latent Image Animator ( LIA ) HYPONYM-OF self - supervised autoencoder. linear combination USED-FOR latent space. model COMPARE state - of - art methods. state - of - art methods COMPARE model. TED - talk datasets EVALUATE-FOR state - of - art methods. TED - talk datasets EVALUATE-FOR model. VoxCeleb EVALUATE-FOR state - of - art methods. VoxCeleb EVALUATE-FOR model. LIA USED-FOR motion of a driving video. landmarks CONJUNCTION region representations. region representations CONJUNCTION landmarks. LIA USED-FOR images. structure representations USED-FOR LIA. region representations HYPONYM-OF structure representations. landmarks HYPONYM-OF structure representations. Material are still images, LIA animation examples, and VoxCeleb dataset. Generic are approaches, and models. OtherScientificTerm are appearance variation, motion, and orthogonal motion directions. Metric are complexity, and generated quality. ",This paper proposes a self-supervised autoencoder called Latent Image Animator (LIA) that learns a structure representation for driving videos from driving videos. The proposed model is a combination of two existing approaches: Generative Adversarial Networks (GANs) and auto-encoder animating images. The main contribution of the paper is to use the structure representation from GANs to train an animation-model with modules for the extraction of structure information from the video. The model is evaluated on two TED-talk datasets and one VoxCeleb dataset. The results show that the proposed model performs better than state-of-art methods on all three datasets. ,"This paper proposes a self-supervised autoencoder, Latent Image Animator (LIA), for generating images and auto-encoder animating images. The main idea is to learn a structure representation of a driving videos from driving videos, which can then be used to train an animation-model. The proposed model outperforms state-of-the-art methods on TED-talk datasets and VoxCeleb on still images, and outperforms other approaches in terms of complexity, generated quality, and appearance variation. The authors propose two modules for the extraction of structure information from the driving videos: 1) a linear combination of the latent space, 2) an orthogonal motion directions. The model is evaluated on two different datasets, one of which is a dataset of LIA animation examples, and the other is a Voxceleb dataset. The models are evaluated on both of these datasets. The results show that the proposed model is able to generate images with different structure representations (e.g., landmarks, region representations, etc.)."
6150,SP:86f9f89f84e117c86478b9afaf087f65524f5472,"meta - training tasks USED-FOR meta - learning algorithms. approach USED-FOR tasks. data - adaptive meta - regularization USED-FOR MLTI. generalization EVALUATE-FOR MLTI. pose prediction CONJUNCTION molecule property prediction. molecule property prediction CONJUNCTION pose prediction. molecule property prediction CONJUNCTION medical image classification. medical image classification CONJUNCTION molecule property prediction. image recognition CONJUNCTION pose prediction. pose prediction CONJUNCTION image recognition. MLTI framework COMPARE state - of - the - art strategies. state - of - the - art strategies COMPARE MLTI framework. MLTI framework CONJUNCTION representative meta - learning algorithms. representative meta - learning algorithms CONJUNCTION MLTI framework. datasets EVALUATE-FOR MLTI framework. image recognition HYPONYM-OF datasets. medical image classification HYPONYM-OF datasets. pose prediction HYPONYM-OF datasets. molecule property prediction HYPONYM-OF datasets. Method is Meta - learning. OtherScientificTerm are real - world scenarios, and interpolation. ","This paper proposes a new meta-learning framework, called MLTI, which is based on data-adaptive meta-regularization. The main idea is to learn a set of meta-training tasks that can be used to improve the generalization performance of the proposed MLTI. The proposed approach is evaluated on a variety of tasks, including image recognition, pose prediction, molecule property prediction, and medical image classification. The MLTI framework outperforms state-of-the-art strategies on all three datasets. ","This paper proposes a new approach to meta-training tasks for meta-learning algorithms. The approach is based on data-adaptive meta-regularization, which can be applied to a variety of tasks, including image recognition, pose prediction, molecule property prediction, and medical image classification. Meta-learning is an important problem in real-world scenarios, and the authors propose to use MLTI to improve the generalization of MLTI. The MLTI framework is shown to outperform state-of-the-art strategies on three datasets, compared to other representative meta-Learning algorithms. "
6175,SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"approach USED-FOR fairness of downstream predictors. encoding sensitive data USED-FOR fairness of downstream predictors. unfairness FEATURE-OF adversarial predictors. representations USED-FOR sensitive attributes. fairness guarantees FEATURE-OF learned representations. probability density USED-FOR sensitive groups. normalizing flow USED-FOR statistical distance. statistical distance FEATURE-OF latent representations. normalizing flow USED-FOR encoder. maximum unfairness EVALUATE-FOR adversarial downstream predictor. interpretability CONJUNCTION transfer learning. transfer learning CONJUNCTION interpretability. FNF USED-FOR group fairness notions. FNF USED-FOR properties. real - world datasets EVALUATE-FOR properties. transfer learning HYPONYM-OF properties. interpretability HYPONYM-OF properties. real - world datasets EVALUATE-FOR FNF. Method are Fair representation learning, Fair Normalizing Flows ( FNF ), and likelihood computation. ","This paper proposes a new approach to improve the fairness of downstream predictors trained on encoding sensitive data. Fair representation learning is a well-studied problem in the literature, and this paper proposes to use Fair Normalizing Flows (FFN) to reduce the maximum unfairness of an adversarial downstream predictor. The proposed FNF is based on normalizing flow, which allows the encoder to learn the statistical distance between the latent representations of the sensitive groups. The authors show that FNF improves the fairness guarantees of learned representations with respect to the sensitive attributes. The FNF also improves the properties of FNF in terms of interpretability and transfer learning. ","This paper proposes a new approach to improve the fairness of downstream predictors on encoding sensitive data. Fair representation learning, Fair Normalizing Flows (FFN), is a popular approach for improving the fairness guarantees of learned representations on sensitive attributes. The main idea is to use a normalizing flow to reduce the statistical distance between the latent representations of the sensitive groups and the corresponding probability density of the encoder. The authors show that the maximum unfairness of an adversarial downstream predictor can be reduced to the maximum fairness of the original adversarial predictors. They also show that FNF can be used to improve group fairness notions, and demonstrate the properties of FNF on several real-world datasets, including interpretability and transfer learning. "
6200,SP:404d5643327f60f0f06f820033a56081f9e01900,"subgraph patterns on graphs USED-FOR graph - based tasks. graph neural networks ( GNNs ) USED-FOR low - dimensional representation. node - centric message passing mechanism USED-FOR GNNs. they USED-FOR complex structure matching. complex structure matching USED-FOR isomorphism counting. COUNT - GNN USED-FOR subgraph isomorphism counting. GNN USED-FOR subgraph isomorphism counting. COUNT - GNN HYPONYM-OF GNN. edge - centric message passing scheme USED-FOR edge level. COUNT - GNN USED-FOR fine - grained structural information. edge USED-FOR encoding graph structures. graph representation USED-FOR graph level. benchmark datasets EVALUATE-FOR COUNT - GNN. COUNT - GNN COMPARE baselines. baselines COMPARE COUNT - GNN. OtherScientificTerm are graph structures, subgraph isomorphisms, nodes, graph, query graphs, structured query graphs, edges, edge adjacency, and first - class citizens. Material is graph data. Method is backtracking framework. Metric is computational cost. Task are node - oriented tasks, and matching. ","This paper studies the problem of learning subgraph patterns on graphs for graph-based tasks. The authors propose a low-dimensional representation for graph neural networks (GNNs) based on the node-centric message passing mechanism in GNNs. They show that they can be used for complex structure matching and subgraph isomorphism counting, and they show that COUNT-GNN is a GNN that can be applied to subgraphs. The paper also proposes a backtracking framework to reduce the computational cost. The main contribution of the paper is to provide a graph representation for each edge in the graph, which is then used to encode the fine-grained structural information in the edge. The graph representation is used to represent the graph level, and the edge is used for encoding graph structures. The edge-centric message passing scheme is also used for the edge level. ","This paper proposes a novel way to learn subgraph patterns on graphs for graph-based tasks. The key idea is to use graph neural networks (GNNs) to learn a low-dimensional representation of the graph, and then use a node-centric message passing mechanism to encode the subgraph isomorphisms of the nodes in the graph. The authors show that they can be used to perform complex structure matching for isomorphism counting, which is an important problem in graph-oriented tasks. They show that COUNT-GNN, an extension of GNN, outperforms baselines on several benchmark datasets, and can encode fine-grained structural information at the edge level. The paper also proposes a backtracking framework to improve the computational cost of the GNN. "
6225,SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"Agnostic Personalized Federated Learning ( APFL ) HYPONYM-OF loosely constrained federated learning. Similarity Matching CONJUNCTION Kernel Factorization ( SimFed ). Kernel Factorization ( SimFed ) CONJUNCTION Similarity Matching. Kernel Factorization ( SimFed ) HYPONYM-OF method. Similarity Matching HYPONYM-OF method. ones USED-FOR personalized knowledge reflection. method USED-FOR task - level similarity. locally learned knowledge USED-FOR method. locally learned knowledge USED-FOR task - level similarity. knowledge collapse CONJUNCTION information loss. information loss CONJUNCTION knowledge collapse. dimensionlaity of parameter space USED-FOR knowledge collapse. information loss FEATURE-OF heterogeneous knowledge. basis vectors PART-OF model parameters. method COMPARE federated learning methods. federated learning methods COMPARE method. singleand multi - domain datasets EVALUATE-FOR method. singleand multi - domain datasets EVALUATE-FOR method. Task is federated learning. OtherScientificTerm are personalized labels, Label Heterogeneity, and Domain Heterogeneity. Generic is they. Method are labeling schemes, and agnostic personalized federated learning. Material is local data. ","This paper proposes Agnostic Personalized Federated Federated Learning (APFL), a loosely constrained federated learning framework for personalized labels. The proposed method combines Similarity Matching and Kernel Factorization (SimFed) to achieve task-level similarity using locally learned knowledge. The method is evaluated on both singleand multi-domain datasets and shows that the proposed method is able to achieve state-of-the-art performance in terms of knowledge collapse and information loss on heterogeneous knowledge. ","This paper proposes Agnostic Personalized Federated Learning (APFL), a framework for loosely constrained federated learning. The proposed method combines Similarity Matching and Kernel Factorization (SimFed) to improve the task-level similarity between two sets of tasks. The method is evaluated on both singleand multi-domain datasets. The results show that the proposed method can improve the performance on heterogeneous knowledge with knowledge collapse and information loss. The authors also show that APFL can be applied to personalized knowledge reflection. "
6250,SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"Object Dynamics Distillation Network ( ODDN ) USED-FOR object dynamic representations. velocity HYPONYM-OF object dynamic representations. raw video input USED-FOR object dynamic representations. it USED-FOR dynamic representations of objects. relation module USED-FOR object - pair interactions. relation module USED-FOR dynamic representations of objects. video events reasoning CONJUNCTION video prediction. video prediction CONJUNCTION video events reasoning. video events reasoning EVALUATE-FOR approach. scene representation methods USED-FOR representaions. occlusion CONJUNCTION objects collision. objects collision CONJUNCTION occlusion. object dynamic clues USED-FOR model. segmentation CONJUNCTION reconstruction. reconstruction CONJUNCTION segmentation. scene decomposition quality EVALUATE-FOR reconstruction. scene decomposition quality EVALUATE-FOR segmentation. scene decomposition quality EVALUATE-FOR model. OtherScientificTerm are abstract entities, and physical events. Method are object - centric representations of scenes, and ODDN. Material is static images. Task are object dynamics, and video understanding. ","This paper proposes an Object Dynamics Distillation Network (ODDN) to learn object dynamic representations from raw video input. The key idea is to use object-centric representations of scenes as object-centric representations of objects. The object dynamics are represented as abstract entities, and the abstract entities can be represented as physical events. The ODDN uses a relation module to learn the object-pair interactions between objects, and then uses it to learn dynamic representations of the objects, such as velocity. The proposed approach is evaluated on video events reasoning and video prediction. The model is trained using object dynamic clues, occlusion, objects collision, and reconstruction. The results show that the proposed model achieves state-of-the-art scene decomposition quality and segmentation quality.","This paper proposes an Object Dynamics Distillation Network (ODDN) to learn object dynamic representations of objects with velocity, i.e., object-centric representations of scenes. The key idea is to use the raw video input as a representation of the object dynamics, and then use the object-pair interactions between abstract entities and physical events to learn the representation. The proposed approach is evaluated on video events reasoning and video prediction. The model is based on object dynamic clues extracted from the scene representation methods, and is able to learn representaions of objects that are invariant to occlusion and objects collision. Experiments are conducted on static images, and on video understanding. Results show that the proposed model achieves better scene decomposition quality, segmentation, and reconstruction."
6275,SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"Graph neural networks ( GNN ) USED-FOR graph - based learning tasks. nodes USED-FOR task. link / motif prediction HYPONYM-OF nodes. random node features CONJUNCTION node distance features. node distance features CONJUNCTION random node features. slow convergence CONJUNCTION inaccurate prediction. inaccurate prediction CONJUNCTION slow convergence. Laplacian Eigenmap CONJUNCTION Deepwalk. Deepwalk CONJUNCTION Laplacian Eigenmap. positional encoding ( PE ) techniques USED-FOR positional features. PE USED-FOR GNNs. positional features USED-FOR GNNs. Laplacian Eigenmap HYPONYM-OF positional encoding ( PE ) techniques. Deepwalk HYPONYM-OF positional encoding ( PE ) techniques. mathematical analysis USED-FOR PEG. PEG HYPONYM-OF GNN layers. mathematical analysis USED-FOR GNN layers. node features CONJUNCTION positional features. positional features CONJUNCTION node features. node features USED-FOR PEG. positional features USED-FOR PEG. permutation equivariance FEATURE-OF PEG. node features CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION node features. link prediction EVALUATE-FOR PEG. real - world networks USED-FOR link prediction. generalization EVALUATE-FOR PEG. Generic are they, and solution. Metric is complexity. ","Graph neural networks (GNN) are widely used in graph-based learning tasks, where the goal is to learn a set of nodes for a given task. The task is to predict the link/motif between two nodes in a graph. The nodes are represented as a set (e.g. link/mosaic) of nodes, and they are represented by random node features, node distance features, and node features. The problem is that the number of node features can be very large, and the complexity of the solution can be quite high. The authors propose to use positional encoding (PE) techniques such as Laplacian Eigenmap and Deepwalk to encode the positional features in GNNs. The PEG is a mathematical analysis of the PEG layers and the node features in the GNN layers. The theoretical analysis shows that the permutation equivariance of PEG has a significant effect on the link prediction and the generalization. The paper also shows that PEG can be used in real-world networks for link prediction.","Graph neural networks (GNN) are used for graph-based learning tasks, where the task is to predict a set of nodes for a given task. The nodes are known as link/motif prediction, and they can be either random node features or node distance features. The authors propose two positional encoding (PE) techniques, Laplacian Eigenmap and Deepwalk, to encode the positional features of GNNs. The proposed PEG is a mathematical analysis of the PEG layers and the node features of the GNN layers. The PEG has permutation equivariance, node features, rotation equivariant, and slow convergence and inaccurate prediction. The generalization of PEG to real-world networks is shown. The complexity of the solution is also shown. "
6300,SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,non - parallel datasets USED-FOR models. non - parallel datasets USED-FOR text style transfer models. weak supervision USED-FOR style transfer models. LaMer HYPONYM-OF text style transfer framework. large - scale language models USED-FOR text style transfer framework. MLE training CONJUNCTION imitation learning refinement. imitation learning refinement CONJUNCTION MLE training. imitation learning refinement USED-FOR intrinsic parallelism. parallel expressions PART-OF non - parallel datasets. intrinsic parallelism FEATURE-OF data. imitation learning refinement USED-FOR LaMer. MLE training USED-FOR LaMer. scene graphs USED-FOR LaMer. scene graphs USED-FOR parallel expressions. content preservation CONJUNCTION fluency. fluency CONJUNCTION content preservation. transfer accuracy CONJUNCTION content preservation. content preservation CONJUNCTION transfer accuracy. task EVALUATE-FOR model. sentiment & formality transfer EVALUATE-FOR model. sentiment & formality transfer CONJUNCTION political stance transfer. political stance transfer CONJUNCTION sentiment & formality transfer. political stance transfer EVALUATE-FOR model. political stance transfer HYPONYM-OF task. fluency EVALUATE-FOR model. content preservation EVALUATE-FOR model. transfer accuracy EVALUATE-FOR model. model COMPARE models. models COMPARE model. OtherScientificTerm is style - independent information. ,"This paper proposes LaMer, a text style transfer framework based on large-scale language models. LaMer uses non-parallel datasets to train models with weak supervision. The authors show that the intrinsic parallelism of the data can be improved by imitation learning refinement and MLE training. The paper also shows that LaMer can learn parallel expressions from scene graphs. The model is evaluated on a new task called sentiment & formality transfer and political stance transfer. ","This paper proposes a text style transfer framework called LaMer, which is based on large-scale language models. LaMer uses non-parallel datasets to train models with weak supervision. The intrinsic parallelism of the data is achieved by MLE training and imitation learning refinement. The model is evaluated on the task of sentiment & formality transfer and political stance transfer. The paper shows that LaMer is able to learn parallel expressions from scene graphs. The authors also show that the model can achieve better transfer accuracy and fluency than other models."
6325,SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"Multi - hop logical reasoning HYPONYM-OF representation learning on knowledge graphs ( KGs ). one - hop link prediction CONJUNCTION logical queries. logical queries CONJUNCTION one - hop link prediction. one - hop link prediction PART-OF It. logical queries PART-OF It. classical, triple - based graphs USED-FOR algorithms. hyper - relational modeling paradigm USED-FOR KGs. key - value pairs FEATURE-OF typed edges. approaches USED-FOR approximate query answering ( QA ). Hyper - relational queries PART-OF real - world KG applications. qualifier pairs USED-FOR approaches. hyper - relational KGs USED-FOR complex queries. multi - hop reasoning problem USED-FOR complex queries. multi - hop reasoning problem USED-FOR hyper - relational KGs. Graph Neural Networks CONJUNCTION query embedding techniques. query embedding techniques CONJUNCTION Graph Neural Networks. method USED-FOR queries. qualifiers USED-FOR QA. query patterns USED-FOR QA. query patterns EVALUATE-FOR qualifiers. Generic is paradigm. OtherScientificTerm are fine - grained context, and hyper - relational conjunctive queries. ","This paper proposes a new paradigm for representation learning on knowledge graphs (KGs) called Multi-Hop logical reasoning, which combines one-hop link prediction and logical queries. It uses a hyper-relational modeling paradigm to model KGs in a fine-grained context, where the key-value pairs of typed edges are represented by classical, triple-based graphs. The authors show that the proposed algorithms are able to answer complex queries in hyper-Relational KGs, which is an important problem in real-world KG applications. The proposed approaches can be applied to approximate query answering (QA) in the context of Graph Neural Networks and query embedding techniques. The main contribution of the paper is the use of qualifier pairs in the proposed approaches, which can be used to improve the performance of the QA. In particular, the proposed method is able to generate queries that are query patterns that are similar to the query patterns used in QA, which are then used as qualifiers for the queries. ","This paper proposes a new paradigm for representation learning on knowledge graphs (KGs) called multi-hop logical reasoning. It combines one-hop link prediction and logical queries. It is based on classical, triple-based graphs. The key idea is to use a hyper-relational modeling paradigm to model KGs in a fine-grained context, where the key-value pairs of typed edges are represented as hyper-relationships. The proposed approaches are applied to approximate query answering (QA) in real-world KG applications with Hyper-Relational queries in the context of complex queries. The method is evaluated on queries generated by Graph Neural Networks and query embedding techniques. The results show that the proposed method is able to generate queries that are more complex than the query patterns used in QA. The authors also show that these queries can be generated using the proposed query patterns. "
6350,SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"DYHPO HYPONYM-OF method. Bayesian optimization USED-FOR gray - box setup. Bayesian optimization USED-FOR technique. surrogate USED-FOR Gaussian Processes. multi - budget information FEATURE-OF acquisition function. acquisition function PART-OF surrogate. learning curve dynamics PART-OF surrogate. DYHPO COMPARE hyperparameter optimization baselines. hyperparameter optimization baselines COMPARE DYHPO. Method are Gray - box hyperparameter optimization techniques, and multibudget search mechanisms. OtherScientificTerm is hyperparameter configurations. ","This paper proposes a new method called DYHPO, which is based on Bayesian optimization for the gray-box setup. The authors propose a surrogate for Gaussian Processes with multi-budget information, where the acquisition function of the surrogate depends on the learning curve dynamics and the hyperparameter configurations. The paper shows that the proposed technique can achieve better performance than existing Gray-box hyperparameters optimization techniques, and can be applied to multibudget search mechanisms.  The authors also show that the performance of the proposed method is comparable to other hyper-parameter optimization baselines.","This paper proposes a new method called DYHPO, which is based on Bayesian optimization for the gray-box setup. The authors propose a surrogate for Gaussian Processes with multi-budget information, where the acquisition function of the surrogate depends on the learning curve dynamics of the hyperparameter configurations. They show that the proposed method outperforms existing Gray-box hyperparameters optimization techniques. They also show that their method can be applied to multibudget search mechanisms. "
6375,SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"learned image compression COMPARE image coding techniques. image coding techniques COMPARE learned image compression. rate - distortion EVALUATE-FOR learned image compression. deterministic inference USED-FOR Gaussian mixture models. methods USED-FOR image compression models. cross - platform consistent manner FEATURE-OF image compression models. Method are non - deterministic calculation, and training and fine - tuning based approaches. Task is decoding. OtherScientificTerm are post - training quantization, and entropy parameters. ",This paper studies the problem of image compression with rate-distortion. The authors propose a non-deterministic calculation for the post-training quantization of the image compression model. The main idea is to use deterministic inference for the Gaussian mixture models. They show that the proposed methods can achieve better performance than the state-of-the-art image compression models in a cross-platform consistent manner. They also show that their training and fine-tuning based approaches can achieve comparable performance. ,"This paper studies the rate-distortion of learned image compression compared to image coding techniques. The authors propose a non-deterministic calculation of the post-training quantization, which is based on deterministic inference for Gaussian mixture models. They show that this is equivalent to decoding. They also propose two methods for training image compression models in a cross-platform consistent manner. The training and fine-tuning based approaches are shown to outperform other methods in terms of entropy parameters."
6400,SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"nanoscale resolution FEATURE-OF imaging and analysis of cellular ultrastructure. fully unsupervised Noise Reconstruction and Removal Network USED-FOR denoising scanning electron microscopy images. architecture USED-FOR noise. gated recurrent units USED-FOR architecture. sequential data USED-FOR noise. fully unsupervised training USED-FOR network. fully unsupervised training COMPARE supervised approaches. supervised approaches COMPARE fully unsupervised training. 3D electron microscopy data sets EVALUATE-FOR supervised approaches. Material is labels and/or noise - free data sets. OtherScientificTerm are time consuming manual annotations, and imaging artifacts. Metric is empirical metrics. ","This paper proposes a fully unsupervised Noise Reconstruction and Removal Network for denoising scanning electron microscopy images with nanoscale resolution. The proposed architecture is based on gated recurrent units, which can be used to remove noise from the sequential data. The network is trained with a combination of partially supervised and fully supervised training. Experiments show that the proposed network performs better than supervised approaches on 3D electron microscopeopy data sets. ","The paper proposes a fully unsupervised Noise Reconstruction and Removal Network for denoising scanning electron microscopy images with nanoscale resolution. The architecture is based on gated recurrent units, which can be used to remove noise from sequential data. The network is evaluated on three 3D electron microscope data sets and compared to supervised approaches. The paper also provides empirical metrics to evaluate the performance of the proposed network. "
6425,SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"Graph neural networks ( GNNs ) CONJUNCTION label propagation. label propagation CONJUNCTION Graph neural networks ( GNNs ). graph structure USED-FOR tasks. node property prediction HYPONYM-OF tasks. stacked message - passing layers USED-FOR predictive embeddings. stacked message - passing layers USED-FOR node features. neighborhood information FEATURE-OF stacked message - passing layers. stacked message - passing layers USED-FOR former. spreading label information USED-FOR unlabeled nodes. parameter - free diffusion process USED-FOR spreading label information. features CONJUNCTION labels. labels CONJUNCTION features. statistical properties PART-OF training pipeline. label trick USED-FOR training pipeline. deterministic training objective USED-FOR stochastic label trick. data - fitting term USED-FOR label leakage issues. graph structure FEATURE-OF regularization factor. Generic are latter, and two. OtherScientificTerm is GNN inputs. Material is Open Graph Benchmark ( OGB ) leaderboard. Task is label trick use cases. ","This paper studies the problem of label trick use cases in graph neural networks (GNNs) and label propagation. The authors consider two tasks: node property prediction and node classification. In the former, stacked message-passing layers are used to generate predictive embeddings with neighborhood information. The latter is used to propagate unlabeled nodes by spreading label information through a parameter-free diffusion process. The training pipeline is based on statistical properties of the features and labels, and the label trick is a deterministic training objective. The main contribution of the paper is to provide a data-fitting term for the label leakage issues, which is a regularization factor that depends on the graph structure. The paper also provides an Open Graph Benchmark (OGB) leaderboard.","This paper proposes a new regularization factor based on graph structure for graph neural networks (GNNs) and label propagation. The main idea is to use stacked message-passing layers for predicting predictive embeddings based on neighborhood information. The latter is based on the Open Graph Benchmark (OGB) leaderboard, while the former is built on top of the former. The authors also propose a stochastic label trick based on a deterministic training objective. The training pipeline consists of two steps: 1) learning the statistical properties of the GNN inputs, 2) spreading label information to unlabeled nodes via a parameter-free diffusion process. The paper also proposes a data-fitting term to prevent label leakage issues. The experiments show that the proposed label trick use cases are more robust to label leakage."
6450,SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"Theory of mind ( ToM ) HYPONYM-OF human intelligence. machines USED-FOR theory of mind. ToM agents USED-FOR tasks. predefined roles FEATURE-OF tasks. speaker - listener scenarios HYPONYM-OF predefined roles. strategy USED-FOR SymmToM. theory of mind USED-FOR strategy. theory of mind USED-FOR SymmToM. multi - agent deep reinforcement learning models USED-FOR mental states. modeling of theory of mind USED-FOR multi - agent scenarios. OtherScientificTerm are machine theory of mind, and grid world. Method are multiagent environment SymmToM, and ToM model. ","Theory of mind (ToM) is a well-studied topic in human intelligence and machines. ToM agents are trained to solve tasks with predefined roles (e.g., speaker-listener scenarios). In this paper, the authors consider the multiagent environment SymmToM, where the agent is given a grid world and the goal is to solve a set of tasks. The authors propose a strategy based on the theory of mind to solve SymmtoM using the ToM model. They show that the proposed strategy is able to achieve better performance than existing multi-agent deep reinforcement learning models on a variety of mental states. ","Theory of mind (ToM) is an important topic in human intelligence. ToM agents can be used to solve tasks with predefined roles (e.g., speaker-listener scenarios). The paper proposes to use machines to learn the theory of mind. The paper presents a multiagent environment SymmToM, where the agent is trained in a grid world. The proposed strategy is based on the theoretical of mind, and the paper shows that the proposed strategy improves the performance of SymmtoM. The authors also propose to use multi-agent deep reinforcement learning models to model mental states. The ToM model is evaluated on a variety of tasks, where it is shown that the model can learn to solve multi-agents scenarios."
6475,SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"visual data collecting and processing technology USED-FOR robots. orientations CONJUNCTION illumination. illumination CONJUNCTION orientations. smart manufacturing CONJUNCTION high - mix - low - volume production. high - mix - low - volume production CONJUNCTION smart manufacturing. Zero - shot object detection HYPONYM-OF unsupervised learning. car CONJUNCTION people. people CONJUNCTION car. bikes CONJUNCTION car. car CONJUNCTION bikes. car HYPONYM-OF outdoor scenes. people HYPONYM-OF outdoor scenes. bikes HYPONYM-OF outdoor scenes. indoor scenes FEATURE-OF zero - shot detection of daily objects. zero - shot detection USED-FOR object size level. dataset EVALUATE-FOR zero - shot detection. Method are robot vision system, vision system, zero - shot object detection, and zero - shot object detection algorithm. Task are manufacturing environment, production process, and detection of daily objects. Generic is it. OtherScientificTerm is manufacturing setup. Material is YCB Video Dataset. ","This paper studies the problem of unsupervised object detection in a robot vision system. The authors propose a new zero-shot object detection algorithm, called Zero-Shot Object Detection (ZOD), which uses visual data collecting and processing technology to train robots to detect objects in a manufacturing environment. The proposed method is based on a manufacturing setup, where the robot is given a set of orientations and illumination, and a vision system is trained to predict which objects are in the scene. The object size level of the object is then used to train the vision system to predict the object size. The method is evaluated on indoor scenes and outdoor scenes, including a car, a bike, and people. The results show that ZOD outperforms the state-of-the-art on the dataset.","This paper proposes a new vision system for robots that uses visual data collecting and processing technology for robots. Zero-shot object detection is an important problem in unsupervised learning, especially in the context of manufacturing environment, where the robot vision system needs to be able to distinguish between orientations, illumination, and other orientations in the production process. The key idea is to use a manufacturing setup that combines smart manufacturing with high-mix-low-volume production. The authors propose a novel zero-shoot object detection algorithm that uses zero-shot detection at the object size level. The proposed algorithm is evaluated on the YCB Video Dataset on indoor scenes and on outdoor scenes (e.g., car, people, and bikes). The results show that it outperforms the state-of-the-art on the dataset."
6500,SP:aa1dcd9217270010f16a00004facede942efea17,"generating future frames CONJUNCTION learning environment dynamics. learning environment dynamics CONJUNCTION generating future frames. Video prediction HYPONYM-OF problem. autoregressive prediction model USED-FOR image generator. autoregressive latent video models USED-FOR video prediction tool. autoregressive latent video prediction model USED-FOR high - fidelity future frames. autoregressive latent video prediction model USED-FOR high - resolution ( 256x256 ) videos. top - k sampling CONJUNCTION data augmentation. data augmentation CONJUNCTION top - k sampling. data augmentation USED-FOR video prediction quality. top - k sampling USED-FOR video prediction quality. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. method USED-FOR highresolution video prediction. complex and large - scale datasets USED-FOR highresolution video prediction. video prediction benchmarks EVALUATE-FOR state - of - the - art approaches. video prediction benchmarks EVALUATE-FOR method. Task are video prediction, and predicting high - fidelity future frames. Method are image generator model, prior models, and causal transformer model. OtherScientificTerm is latent space of the image generator. Generic is models. ","This paper studies the problem of video prediction, where the goal is to generate high-resolution (256x256) videos with an autoregressive latent video prediction model for high-fidelity future frames. Video prediction is a very important problem in the video prediction literature, and this paper proposes a video prediction tool based on the idea of autoregression of the latent space of the image generator model. The authors propose a method that combines top-k sampling and data augmentation to improve video prediction quality. The proposed method is shown to outperform state-of-the-art approaches on several video prediction benchmarks. ","This paper proposes a new video prediction model for high-resolution (256x256) videos. Video prediction is an important problem in video prediction, where the goal is to predict future frames from the latent space of the image generator. The authors propose to use an autoregressive prediction model to predict the future frames generated by an image generator model. The proposed video prediction tool is based on the idea of using autoresgressive latent video models to predict high-fidelity future frames. The method is evaluated on a variety of video prediction benchmarks and compared to state-of-the-art approaches that use top-k sampling and data augmentation to improve the video prediction quality. The results show that the proposed method is able to achieve better performance on highresolution video prediction on complex and large-scale datasets. "
6525,SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,vision - specific inductive biases USED-FOR Vision Transformers ( ViTs ). image recognition EVALUATE-FOR Vision Transformers ( ViTs ). ViT architecture PART-OF generative adversarial networks ( GANs ). regularization methods USED-FOR GANs. regularization methods USED-FOR self - attention. regularization methods USED-FOR ViT discriminators. regularization techniques USED-FOR GANs. ViTs USED-FOR GANs. CIFAR-10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR-10. approach COMPARE CNNbased GAN models. CNNbased GAN models COMPARE approach. CelebA CONJUNCTION LSUN bedroom. LSUN bedroom CONJUNCTION CelebA. ViTGAN HYPONYM-OF approach. LSUN bedroom HYPONYM-OF datasets. datasets EVALUATE-FOR CNNbased GAN models. CIFAR-10 HYPONYM-OF datasets. datasets EVALUATE-FOR approach. CelebA HYPONYM-OF datasets. Task is image generation. Method is ViT generators. OtherScientificTerm is latent and pixel mapping layers. ,"This paper studies vision-specific inductive biases in Vision Transformers (ViTs) for image recognition. The ViT architecture consists of generative adversarial networks (GANs), and the ViT generators are trained with regularization methods to improve self-attention. The authors show that the ViTs trained with these regularization techniques can outperform GANs trained with ViTs. The proposed approach is evaluated on three datasets: CIFAR-10, CelebA and LSUN bedroom.","This paper studies vision-specific inductive biases in Vision Transformers (ViTs) for image recognition. The ViT architecture is an extension of generative adversarial networks (GANs) that uses regularization methods to improve self-attention. ViTs are used to train GANs with regularization techniques to improve the performance of ViT discriminators. The authors show that the proposed approach outperforms CNNbased GAN models on three datasets (CIFAR-10, CelebA, and LSUN bedroom) and outperforms ViT generators. "
6550,SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"sample quality EVALUATE-FOR models. variational autoencoders USED-FOR image generative modeling. entropy FEATURE-OF natural image distributions. visually imperceptible information FEATURE-OF entropy. models USED-FOR competitive likelihoods. imperceptible information PART-OF likelihood signal. good sample quality EVALUATE-FOR modeling of visually perceptible information. OtherScientificTerm are Good likelihoods, high - dimensional image data distributions, and visually perceptible bits. Task is generative modeling. ",This paper studies the problem of image generative modeling with variational autoencoders. Good likelihoods are known to be competitive for high-dimensional image data distributions. The authors consider the case where the entropy of natural image distributions with visually imperceptible information is bounded by the number of visually perceptible bits in the likelihood signal. They show that models trained with competitive likelihoods can achieve good sample quality in terms of both the performance of the models as well as the quality of the training data. ,This paper proposes variational autoencoders for image generative modeling. Good likelihoods are learned from high-dimensional image data distributions. The entropy of natural image distributions is modeled as a function of visually imperceptible information in the likelihood signal. The authors show that the models can learn competitive likelihoods with respect to the number of visually perceptible bits. They also show that good sample quality can be achieved for the modeling of visual perceptible information. 
6575,SP:bfed56018134ec66cde9a7e958df964d4cca3164,"Diffusion probabilistic models ( DPMs ) HYPONYM-OF generative models. inference USED-FOR variance. reverse process FEATURE-OF variance. optimal reverse variance CONJUNCTION optimal KL divergence. optimal KL divergence CONJUNCTION optimal reverse variance. optimal KL divergence FEATURE-OF DPM. optimal reverse variance FEATURE-OF DPM. analytic forms FEATURE-OF optimal KL divergence. analytic forms FEATURE-OF optimal reverse variance. Monte Carlo method CONJUNCTION pretrained score - based model. pretrained score - based model CONJUNCTION Monte Carlo method. Analytic - DPM HYPONYM-OF training - free inference framework. training - free inference framework USED-FOR analytic forms. Analytic - DPM USED-FOR analytic forms. Monte Carlo method USED-FOR Analytic - DPM. pretrained score - based model USED-FOR Analytic - DPM. analytic - DPM USED-FOR DPMs. analytic - DPM USED-FOR log - likelihood. log - likelihood FEATURE-OF DPMs. Task is inference of DPMs. OtherScientificTerm are score function, and lower and upper bounds. Method is score - based model. Generic is estimate. ","Diffusion probabilistic models (DPMs) are widely used in generative models. In this paper, the authors consider the inference of DPMs, where the score function is computed by a score-based model. The authors propose a training-free inference framework, called Analytic-DPM, to estimate the variance in the reverse process of a DPM with optimal reverse variance and optimal KL divergence. The analytic forms of the optimal reverse variances are derived from the Monte Carlo method and the pretrained score-by-model. The lower and upper bounds of the estimate are obtained by using the analytic-DPM as the log-likelihood of the DPM. ","This paper proposes a new training-free inference framework for generative models, Diffusion probabilistic models (DPMs). The authors propose a score-based model, where the score function is computed as a sum of the lower and upper bounds on the variance of the variance in the reverse process. The authors show that the optimal reverse variance of a DPM is a function of the optimal KL divergence between the two analytic forms of the DPM, which is a combination of the Monte Carlo method and an analytic-DPM. The paper also shows that the log-likelihood of DPMs can be approximated by the analytic-DPM. "
6600,SP:3f935ba5784c3e86db72421426bc479061af1a4b,"vision transformers ( ViTs ) COMPARE CNNs. CNNs COMPARE vision transformers ( ViTs ). transformer - based models USED-FOR medical image classification. CNNs CONJUNCTION transformers. transformers CONJUNCTION CNNs. medical image benchmark datasets CONJUNCTION tasks. tasks CONJUNCTION medical image benchmark datasets. vision transformers COMPARE CNNs. CNNs COMPARE vision transformers. CNNs COMPARE vision transformers. vision transformers COMPARE CNNs. ImageNet EVALUATE-FOR CNNs. Method is Convolutional Neural Networks ( CNNs ). Task are automated medical image diagnosis, classification, detection and segmentation tasks, medical imaging tasks, and supervised and self - supervised setting. Material is natural image domain. ",This paper studies the problem of automated medical image diagnosis in the context of convolutional Neural Networks (CNNs). The authors propose a transformer-based models for medical image classification and segmentation tasks. The authors show that vision transformers (ViTs) perform better than CNNs and transformers on a variety of tasks including medical image benchmark datasets and tasks in a supervised and self-supervised setting. They also show that CNNs perform better on ImageNet compared to CNNs. ,This paper proposes a new method for automated medical image diagnosis. The idea is to use Convolutional Neural Networks (CNNs) for medical image classification and segmentation tasks. The authors propose to use transformer-based models to improve the performance of CNNs (ViTs) and transformers (VTs) in the medical imaging classification. The proposed method is evaluated on a number of tasks and medical image benchmark datasets. The results show that the proposed CNNs outperform vision transformers on ImageNet and other CNNs in both the natural image domain and in the supervised and self-supervised setting. 
6625,SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"pretrained NLM COMPARE it. it COMPARE pretrained NLM. NLM training heuristics USED-FOR pretraining and fine - tuning stages. NLM pretraining USED-FOR Natural Language Understanding tasks. sentence representations CONJUNCTION open domain question answering abilities. open domain question answering abilities CONJUNCTION sentence representations. Method are Pretraining Neural Language Models ( NLMs ), neural architecture, pretraining example design, and self - improving representations. OtherScientificTerm is semantically related non - neighboring sentences. ","This paper studies the problem of pretraining Neural Language Models (NLMs) in the presence of semantically related non-related non-neighboring sentences. The authors propose a new neural architecture, which is based on the pretraining example design. The pretrained NLM is trained with NLM training heuristics for both pretraining and fine-tuning stages, and it is shown that it performs better than the standard pre-trained NLM.  The authors also show that the self-improving representations of self-optimized NLM can improve the performance of NLM pretraining on Natural Language Understanding tasks such as sentence representations and open domain question answering abilities.","This paper proposes a novel way to train Pretraining Neural Language Models (NLMs), a new type of neural architecture where the pretraining example design is based on self-improving representations of semantically related non-related non-nexting sentences. The authors use NLM training heuristics for both pretraining and fine-tuning stages, and show that it outperforms the pretrained NLM on a variety of Natural Language Understanding tasks. They also show that sentence representations and open domain question answering abilities can be learned by NLM pretraining."
6650,SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"L2O models USED-FOR optimization rules. neural networks USED-FOR L2O models. neural networks USED-FOR optimization rules. meta - training USED-FOR numerical rules. optimization rule USED-FOR L2O model. neural networks USED-FOR numerical rules. holistic symbolic representation and analysis framework USED-FOR L2O. L2O model COMPARE human - designed and tuned optimizers. human - designed and tuned optimizers COMPARE L2O model. large - scale problems EVALUATE-FOR L2O model. Task are Learning to Optimize ( L2O ), optimization procedure, and L2O research. Metric are scalability, and interpretability. OtherScientificTerm is memory overhead. Method is symbolic regression. ","This paper proposes a new optimization procedure called Learning to Optimize (L2O), which uses neural networks to learn optimization rules using meta-training. The L2O model is based on a holistic symbolic representation and analysis framework. The authors show that the optimization rule can be learned by the neural networks, and that the scalability of the optimization procedure can be improved by the use of the learned optimization rule. The paper also provides a theoretical analysis of the interpretability and memory overhead of the L2Os. Finally, the authors demonstrate that the L1O model performs better than human-designed and tuned optimizers on large-scale problems.","This paper proposes Learning to Optimize (L2O), an optimization procedure that is based on meta-training of neural networks for learning optimization rules. The authors propose a holistic symbolic representation and analysis framework for learning L2O models. The main contribution of the paper is to propose a new optimization rule for the optimization rule. The paper also proposes a new scalability metric to measure the interpretability of the learned rules. Experiments are conducted on large-scale problems and show that the proposed model outperforms human-designed and tuned optimizers. "
6675,SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"provable adversarial robustness FEATURE-OF deep neural networks ( DNNs ). provable adversarial robustness USED-FOR static supervised learning tasks. image classification HYPONYM-OF static supervised learning tasks. DNNs USED-FOR real - world adaptive tasks. reinforcement learning ( RL ) HYPONYM-OF real - world adaptive tasks. methods USED-FOR static setting. provable robustness FEATURE-OF RL. RL adversary USED-FOR defense strategy. procedure USED-FOR adaptive RL adversary. worst - case scenario USED-FOR certificates. Pong CONJUNCTION Freeway. Freeway CONJUNCTION Pong. Freeway CONJUNCTION Mountain Car. Mountain Car CONJUNCTION Freeway. Cartpole CONJUNCTION Pong. Pong CONJUNCTION Cartpole. environments EVALUATE-FOR method. robustness guarantees EVALUATE-FOR method. Mountain Car HYPONYM-OF environments. Cartpole HYPONYM-OF environments. Freeway HYPONYM-OF environments. Pong HYPONYM-OF environments. Generic are systems, and attacks. OtherScientificTerm are adversarial attacks, non - adaptive adversary, policy, Neyman - Pearson Lemma, adversarial perturbation, Gaussian noise, policy function, and robustness certificates. Task is randomized smoothing based defenses. Method are smoothingbased certificates, and policy smoothing. ","This paper studies the problem of randomized smoothing based defenses against adversarial attacks. The authors propose a new defense strategy based on the Neyman-Pearson Lemma, where the adversarial perturbation is modeled as Gaussian noise, and the goal is to learn a policy that is robust to the perturbations. The defense strategy is based on an adaptive RL adversary, which is trained with the same procedure as the adaptive RL adversaries.  The authors show that the proposed method is robust in a variety of environments, including Pong, Cartpole, Mountain Car, Freeway, and Pong. ","This paper studies the problem of provable adversarial robustness of deep neural networks (DNNs) for static supervised learning tasks such as image classification. The authors propose a defense strategy based on a randomized smoothing based defenses, where the adversarial attacks are generated by a non-adaptive adversary. The defense strategy is based on the Neyman-Pearson Lemma. The adversarial perturbation is generated by Gaussian noise, and the smoothingbased certificates are computed using the policy smoothing. The certificates are obtained in a worst-case scenario. The proposed method is evaluated on three environments: Pong, Cartpole, and Freeway. The results show that the proposed method provides robustness guarantees."
6700,SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"labeled source data CONJUNCTION unlabeled target data. unlabeled target data CONJUNCTION labeled source data. methods USED-FOR predicting the target domain accuracy. labeled source data USED-FOR methods. unlabeled target data USED-FOR methods. method USED-FOR threshold. BREEDS CONJUNCTION CIFAR. CIFAR CONJUNCTION BREEDS. ImageNet CONJUNCTION BREEDS. BREEDS CONJUNCTION ImageNet. WILDS CONJUNCTION ImageNet. ImageNet CONJUNCTION WILDS. CIFAR CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR. ATC COMPARE methods. methods COMPARE ATC. synthetic corruptions CONJUNCTION dataset reproduction. dataset reproduction CONJUNCTION synthetic corruptions. ImageNet CONJUNCTION CIFAR. CIFAR CONJUNCTION ImageNet. WILDS CONJUNCTION BREEDS. BREEDS CONJUNCTION WILDS. model architectures EVALUATE-FOR methods. datasets EVALUATE-FOR ATC. dataset reproduction HYPONYM-OF distribution shifts. WILDS HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. CIFAR HYPONYM-OF datasets. BREEDS HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. ATC COMPARE prior methods. prior methods COMPARE ATC. toy distributions USED-FOR method. Task is Real - world machine learning deployments. Method is Average Thresholded Confidence ( ATC ). OtherScientificTerm are model ’s confidence, and model confidence. Metric is predicting accuracy. Generic are problem, and it. ","This paper studies the problem of predicting the target domain accuracy from labeled source data and unlabeled target data. The authors propose Average Thresholded Confidence (ATC), a new threshold for predicting the model’s confidence. The proposed method is based on toy distributions, and it is shown that ATC outperforms prior methods on a variety of datasets including MNIST, CIFAR, BREEDS, ImageNet, WILDS, and dataset reproduction. ","This paper proposes a new method for predicting the target domain accuracy. The proposed method is Average Thresholded Confidence (ATC), which is a threshold that measures the difference between the model’s confidence and the true target domain confidence. The authors show that ATC is more robust to distribution shifts and synthetic corruptions in the dataset reproduction. They also show that the proposed method outperforms prior methods on toy distributions. "
6725,SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"registration USED-FOR transformation. outliers CONJUNCTION unknown non - rigid deformations. unknown non - rigid deformations CONJUNCTION outliers. outliers FEATURE-OF robustness. partial distribution matching ( PDM ) problem USED-FOR registration problem. method USED-FOR large scale PDM problem. partial Wasserstein-1 ( PW ) discrepancy USED-FOR method. Kantorovich – Rubinstein duality USED-FOR PW discrepancy. partial Wasserstein adversarial network ( PWAN ) USED-FOR PW discrepancy. neural network USED-FOR partial Wasserstein adversarial network ( PWAN ). coherence regularizer USED-FOR non - rigid transformations. coherence regularizer USED-FOR unrealistic deformations. coherence regularizer PART-OF It. PWAN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PWAN. point set registration tasks EVALUATE-FOR PWAN. point set registration tasks EVALUATE-FOR PWAN. Generic are task, and network. OtherScientificTerm are discrete distributions, and gradient. ","This paper studies the partial distribution matching (PDM) problem in the registration problem, where the goal is to obtain a good registration for a transformation. The authors propose a method to solve the large scale PDM problem with partial Wasserstein-1 (PW) discrepancy. The PW discrepancy is derived from the Kantorovich-Rubinstein duality of the PW discrepancy, and a neural network is trained to generate PW discrepancy using the neural network. The paper shows that the proposed PWAN performs better than state-of-the-art methods on several point set registration tasks. It is also shown that the coherence regularizer can be used to avoid unrealistic deformations in non-rigid transformations. ","This paper proposes a method to solve the partial distribution matching (PDM) problem for the registration problem for transformation. The proposed method is based on the partial Wasserstein-1 (PW) discrepancy, which is defined by the Kantorovich–Rubinstein duality. The authors show that the PW discrepancy is a function of the number of outliers and unknown non-rigid deformations in the training data, and that the proposed method can be applied to the large scale PDM problem. It uses a coherence regularizer to prevent unrealistic deformations, and a neural network to learn the full PW discrepancy. Experiments on point set registration tasks show that PWAN outperforms state-of-the-art methods. "
6750,SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"Hyperparameter optimization ( HPO ) PART-OF machine learning models. transfer learning USED-FOR HPO. approaches COMPARE Deep Kernel Gaussian Process surrogate. Deep Kernel Gaussian Process surrogate COMPARE approaches. Landmark Meta - features ( DKLM ) PART-OF Deep Kernel Gaussian Process surrogate. DKLM USED-FOR similarity between hyperparameter configurations. DKLM USED-FOR contextualized dataset - specific similarity representations. DKLM USED-FOR hyperparameter configurations. method COMPARE stateof - the - art baselines. stateof - the - art baselines COMPARE method. OpenML FEATURE-OF HPO meta - datasets. HPO meta - datasets EVALUATE-FOR DKLM. Generic is it. OtherScientificTerm are hyperparameter evaluations, and evaluated configurations. ",This paper studies hyperparameter optimization (HPO) in machine learning models. The authors consider transfer learning for HPO and propose a new approach called Landmark Meta-features (DKLM) which is based on the Deep Kernel Gaussian Process surrogate. DKLM is able to learn the similarity between hyperparameters configurations in a contextualized dataset-specific similarity representations. The proposed method is shown to outperform stateof-the-art baselines on OpenML and HPO meta-datasets.,"This paper proposes a new hyperparameter optimization (HPO) method for machine learning models. The authors propose a Deep Kernel Gaussian Process surrogate, called Landmark Meta-features (DKLM), which is based on transfer learning. The key idea is to use DKLM to learn the similarity between hyperparameters configurations, and to use contextualized dataset-specific similarity representations. The proposed method is evaluated on two HPO meta-datasets, OpenML and CIFAR10, and it outperforms stateof-the-art baselines."
6775,SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"Generated data COMPARE real data. real data COMPARE Generated data. technology USED-FOR deep fakes. technology USED-FOR misinformation. 128 - bit fingerprint USED-FOR identifiable models. deep fake detection CONJUNCTION attribution. attribution CONJUNCTION deep fake detection. method USED-FOR deep fake detection. method USED-FOR attribution. fingerprinting mechanism USED-FOR method. Method are deep generative models, deep fake detection methods, and generative models. Generic are work, and technique. OtherScientificTerm is fingerprint. ",This paper proposes a new method for deep fake detection based on the fingerprinting mechanism. The key idea is to use a 128-bit fingerprint to identify the identifiable models. The authors show that the proposed method is able to achieve better performance than the state-of-the-art in terms of both deep fake and real-world datasets. ,"This paper proposes a new method for deep generative models to detect deep fakes. The method is based on the fingerprinting mechanism of deep fake detection methods. The key idea is to use a 128-bit fingerprint to identify identifiable models. Generated data can be used as a surrogate for real data, while real data cannot be used. The authors show that the proposed work can detect misinformation in the generated data. The technique is evaluated on several datasets. The results show the effectiveness of the proposed technique. The proposed method is able to achieve good performance on both deepfake detection and attribution."
6800,SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"Post - hoc explanations USED-FOR black box models. Post - hoc explanations PART-OF classification and regression settings. model agnostic local explanations USED-FOR similarity learners. tabular and text data USED-FOR model agnostic local explanations. tabular and text data USED-FOR similarity learners. method USED-FOR feature attributions. analogies USED-FOR machine learning. analogies USED-FOR explanation. explanation PART-OF machine learning. ( latent ) factors USED-FOR model. feature attributions USED-FOR analogies. submodular FEATURE-OF analogy objective function. Generic are models, and approaches. Method are black box similarity learner, and sentence encoder. OtherScientificTerm are similarity, and complementarity. Task is healthcare utilization application. ",This paper proposes a method for learning post-hoc explanations for black box models in classification and regression settings using tabular and text data. The main idea is to use model agnostic local explanations for similarity learners trained on tabular or text data to learn the feature attributions for the black box similarity learner. The proposed method is based on the idea that the similarity between two models is a submodular function of the (latent) factors of the model. The authors show that the proposed method can be used for learning the feature attribute for the explanation in the context of machine learning by learning the analogies between the two models. They also show that their method can also be applied to the healthcare utilization application. ,"This paper proposes a method for learning post-hoc explanations for black box models in classification and regression settings. The main idea is to learn model agnostic local explanations for similarity learners on tabular and text data. The authors propose two approaches: 1) a black box similarity learner that learns to predict the similarity between a sentence and a sentence encoder, and 2) a method to learn feature attributions for analogies to explain the explanation in machine learning using (latent) factors. The proposed method is based on a submodular of the analogy objective function, where the similarity is defined as the complementarity between the two sentences. The paper also proposes a healthcare utilization application."
6825,SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"deep neural networks ( DNN ) USED-FOR adversarial examples. empirical and theoretical defense approaches USED-FOR single ML model. robustness EVALUATE-FOR ensemble protocols. certified robustness EVALUATE-FOR ensemble ML models. ensemble models COMPARE single model. single model COMPARE ensemble models. ensemble models COMPARE single model. single model COMPARE ensemble models. certified robustness EVALUATE-FOR ensemble models. diversified gradient CONJUNCTION confidence margin. confidence margin CONJUNCTION diversified gradient. diversified gradient USED-FOR ensemble models. Ensemble - before - Smoothing strategy USED-FOR bounded model - smoothness analysis. ensemble model COMPARE single base model. single base model COMPARE ensemble model. certified robustness EVALUATE-FOR single base model. certified robustness EVALUATE-FOR ensemble model. lightweight Diversity Regularized Training ( DRT ) USED-FOR certifiably robust ensemble ML models. DRT enhanced ensembles COMPARE single and ensemble ML models. single and ensemble ML models COMPARE DRT enhanced ensembles. certified robustness EVALUATE-FOR single and ensemble ML models. ImageNet datasets EVALUATE-FOR certified L2 - robustness. certified robustness EVALUATE-FOR DRT enhanced ensembles. Method is DNNs. OtherScientificTerm are perturbations, and model - smoothness assumption. ","This paper studies the robustness of deep neural networks (DNN) against adversarial examples in the presence of perturbations. The authors propose a new ensemble-before-Smoothing strategy for bounded model-smoothing analysis. The ensemble models are trained with a diversified gradient and a confidence margin, and the authors show that the ensemble models achieve better certified robustness than a single model. The paper also shows that ensemble protocols are more robust than single ML model under empirical and theoretical defense approaches. ","This paper presents a theoretical analysis of the robustness of deep neural networks (DNN) against adversarial examples. The authors show that DNNs can be robust to perturbations, but not to adversarial attacks. They also show that a single ML model trained with empirical and theoretical defense approaches can be as robust as a single model trained using ensemble protocols. They further show that ensemble models trained with a diversified gradient and a confidence margin are more robust than the single model. Finally, the authors propose an Ensemble-before-Smoothing strategy for bounded model-smoothing analysis. They show that the ensemble model is more robust compared to a single base model and the certified robustness for ensemble models is better than a single baseline model. "
6850,SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"message passing Graph Neural Networks ( GNNs ) USED-FOR learning with graphs. expressive power EVALUATE-FOR higherorder GNNs. strategies CONJUNCTION lower bounds. lower bounds CONJUNCTION strategies. computational cost CONJUNCTION expressive power. expressive power CONJUNCTION computational cost. model USED-FOR subgraphs. recursive pooling COMPARE higher - order GNNs. higher - order GNNs COMPARE recursive pooling. computational complexity EVALUATE-FOR higher - order GNNs. sparsity USED-FOR recursive pooling. computational complexity EVALUATE-FOR recursive pooling. near ) matching information - theoretic lower bound USED-FOR counting subgraphs. graph representations USED-FOR representations of derived ( sub-)graphs. graph representations USED-FOR near ) matching information - theoretic lower bound. lower bounds FEATURE-OF time complexity. Method are recursive pooling technique of local neighborhoods, and low - order GNNs. ","This paper studies the problem of message passing Graph Neural Networks (GNNs) for learning with graphs. The authors propose a recursive pooling technique of local neighborhoods, where each node is represented as a subgraph, and each node in the subgraph is represented by a graph. The model is trained to learn subgraphs from the graph representations of derived (sub-)graphs, which are then aggregated to form the final subgraph.  The authors show that the proposed strategies and lower bounds on the computational cost and expressive power of higherorder GNNs are better than the state-of-the-art in terms of computational complexity compared to the higher-order gNNs. They also provide a near (matching information-theoretic lower bound on the counting subgraph's computational complexity, which is a result of sparsity. ","This paper proposes a new message passing Graph Neural Networks (GNNs) for learning with graphs. The authors propose a recursive pooling technique of local neighborhoods, where subgraphs are represented by a model that can be represented as a set of nodes in a graph. They show that the proposed strategies and lower bounds improve the computational cost and expressive power of higherorder GNNs compared to higher-order gNNs. They also provide a near (matching information-theoretic lower bound) for counting subgraph and graph representations for representations of derived (sub-)graphs. In addition, they show that sparsity improves the computational complexity of the lower bounds of the time complexity of lower bounds. They evaluate their method on low-order and high-order graph neural networks."
6875,SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,Pretrained language models ( LMs ) USED-FOR factual knowledge. external knowledge PART-OF pretrained LMs. knowledge integration ( KI ) methods USED-FOR pretrained LMs. external knowledge PART-OF knowledge integration ( KI ) methods. KI methods COMPARE vanilla LMs. vanilla LMs COMPARE KI methods. integration USED-FOR catastrophic forgetting of already learned knowledge. graph convolution operation USED-FOR KI. probe model USED-FOR knowledge - enhanced LMs. Graph Convolution Simulator ( GCS ) HYPONYM-OF probe model. GCS model USED-FOR KI process. it USED-FOR knowledge - enhanced LMs. K - Adapter CONJUNCTION ERNIE. ERNIE CONJUNCTION K - Adapter. ERNIE HYPONYM-OF knowledge - enhanced LMs. K - Adapter HYPONYM-OF knowledge - enhanced LMs. models USED-FOR factual knowledge. complex relational knowledge USED-FOR ERNIE. K - Adapter USED-FOR simple relational knowledge. K - Adapter USED-FOR time - related knowledge. Generic is methods. Method is LMs. OtherScientificTerm is relations. Material is KI corpus. ,This paper studies the problem of knowledge integration (KI) methods for pretrained language models (LMs) to extract factual knowledge from external knowledge in the context of LMs. The authors show that KI methods perform better than vanilla LMs in terms of catastrophic forgetting of already learned knowledge due to the lack of external knowledge. They propose a probe model based on Graph Convolution Simulator (GCS) to train knowledge-enhanced LMs such as K-Adapter and ERNIE. The GCS model is used in the KI process and it is shown that it is able to learn knowledge-augmented LMs with KI. The KI corpus is then used to train KI models that are able to capture factual knowledge.  The authors also show that the K-Adapters can be used to learn time-related knowledge such as relational knowledge from K-ADAM and K-ACADAM. ,This paper proposes a new framework for learning knowledge integration (KI) methods that incorporate external knowledge into pretrained language models (LMs) for factual knowledge. The authors show that KI methods are more robust to catastrophic forgetting of already learned knowledge compared to vanilla LMs. The main contribution of the paper is to propose a new probe model for knowledge-enhanced LMs based on Graph Convolution Simulator (GCS) which is an extension of the GCS model to the KI process. Experiments show that it is able to learn knowledge-augmented LMs such as K-Adapter and ERNIE with complex relational knowledge. 
6900,SP:7e73948421e98307fceb69a316d8a4e7c4926cda,adaptation ( inner loop ) learning rate USED-FOR fast adaptation. adaptation ( inner loop ) learning rate USED-FOR MAML. adaptation learning rate USED-FOR meta - learning. adaptation learning rate FEATURE-OF mixed linear regression. mixed linear regression USED-FOR meta - learning. optimal adaptation learning rates USED-FOR MAML. optimal adaptation learning rates USED-FOR population risk. population risk FEATURE-OF MAML. empirical risk minimization ( ERM ) COMPARE MAML. MAML COMPARE empirical risk minimization ( ERM ). MAML USED-FOR initialization. average distance FEATURE-OF initialization. Metric is adaptation error. OtherScientificTerm is optimal adaptation learning rate. ,This paper proposes a new adaptation (inner loop) learning rate for fast adaptation in MAML. The adaptation learning rate of mixed linear regression is used in meta-learning. The authors show that the optimal adaptation learning rates for MAMM can be used to reduce the population risk of the initialization of the model. The empirical risk minimization (ERM) is also shown to be better than the empirical risk maximization (EMM) for initialization. ,"This paper proposes an adaptation (inner loop) learning rate for fast adaptation in MAML. The adaptation learning rate of mixed linear regression is used for meta-learning. The authors show that the optimal adaptation learning rates for MAMM can reduce the population risk of the MAL under population risk. The empirical risk minimization (ERM) is shown to be more robust to the adaptation error than the MML. The paper also shows that for initialization, the average distance of the initialization is larger than that of the optimal learning rate. "
6925,SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"source - domain data USED-FOR adaptation. Source - free domain adaptation ( SFDA ) USED-FOR model. unlabelled data USED-FOR model. labelled data USED-FOR model. methods USED-FOR SFDA. source model USED-FOR feature - space class - separation. entropy - minimization techniques USED-FOR methods. measurement shift HYPONYM-OF domain shift. it USED-FOR features. bottom - up training scheme USED-FOR FR. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. BUFR COMPARE SFDA methods. SFDA methods COMPARE BUFR. calibration CONJUNCTION data efficiency. data efficiency CONJUNCTION calibration. BUFR COMPARE source model. source model COMPARE BUFR. data efficiency EVALUATE-FOR SFDA methods. calibration EVALUATE-FOR SFDA methods. real and synthetic data EVALUATE-FOR SFDA methods. data efficiency EVALUATE-FOR BUFR. calibration EVALUATE-FOR BUFR. accuracy EVALUATE-FOR SFDA methods. real and synthetic data EVALUATE-FOR BUFR. accuracy EVALUATE-FOR BUFR. Task are classification, and model calibration. OtherScientificTerm are measurement system, source features, and approximate feature distribution. Method are feature - extractor, and Feature Restoration ( FR ). Generic is network. ","This paper studies the problem of source-free domain adaptation (SFDA), where the source-domain data is used to train a model on unlabelled data and the target model is trained on labelled data. The authors propose two methods to improve the performance of SFDA by using entropy-minimization techniques. The first method, Feature Restoration (FR), uses a bottom-up training scheme to train the FR. The second method, BUFR, uses a source model to learn the feature-space class-separation between the source and target data. BUFR is shown to achieve better accuracy and calibration on both real and synthetic data compared to standard SFDA methods.","This paper proposes a new model for source-free domain adaptation (SFDA) based on source-domain data. The model is trained on unlabelled data, where the source features are extracted from the measurement system. The authors propose two methods for SFDA, which are based on entropy-minimization techniques. The first one is Feature Restoration (FR), where the network is trained using a bottom-up training scheme, and the second one is called Feature Reconstruction (BUFR). The authors show that BUFR outperforms SFDA methods in terms of accuracy, calibration, and data efficiency. "
6950,SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"distributed learning schema USED-FOR model. Federated learning ( FL ) HYPONYM-OF distributed learning schema. adversarial training ( AT ) USED-FOR centralized learning. learning setting USED-FOR adversarial robustness. adversarial robustness FEATURE-OF non - iid users. batch - normalization statistics USED-FOR propagation approach. method USED-FOR FL remarkable robustness. Material is raw data. Method are FL, and FL techniques. OtherScientificTerm are FL users, highresource users, and low - resource users. Metric is model robustness. Generic is it. Task are FL process, and learning. ","This paper proposes a distributed learning schema called Federated learning (FL) for model robustness. The authors propose a propagation approach based on batch-normalization statistics to improve the adversarial robustness of non-iid users in the learning setting. The propagation approach is based on adversarial training (AT) for centralized learning, where the FL users are highresource users and the FL process is decentralized. The proposed method is shown to achieve FL remarkable robustness on raw data. ","This paper proposes Federated learning (FL) which is a distributed learning schema for training a model on raw data. The authors show that FL is robust to adversarial training (AT) for centralized learning, and that it is robust against adversarial robustness of non-iid users in the learning setting.  The authors propose a propagation approach based on batch-normalization statistics. The propagation approach is based on FL, and FL techniques are evaluated on highresource users and low-resource users. Experiments show that the proposed method achieves FL remarkable robustness. "
6975,SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"utility function FEATURE-OF game. utility function USED-FOR Existing methods. transformer - like architecture USED-FOR mapping. transformer - like architecture USED-FOR symmetries. network structure inference EVALUATE-FOR methods. method COMPARE methods. methods COMPARE method. network structure inference EVALUATE-FOR method. network games EVALUATE-FOR method. synthetic and real - world data EVALUATE-FOR method. synthetic and real - world data USED-FOR network games. OtherScientificTerm are Strategic interactions, network structure, equilibrium actions, real - world scenarios, and network structure of the game. Task is economics and social sciences. ",This paper studies the utility function of a game with a utility function. The utility function is defined as a function of the number of interactions in the game. The authors propose a transformer-like architecture for mapping symmetries between the network structure and the equilibrium actions. The proposed method is evaluated on both synthetic and real-world data for network games and shows that the proposed method outperforms existing methods in terms of network structure inference. ,"This paper proposes a new utility function for the game of the game. The utility function is a simple extension of Existing methods that use a transformer-like architecture for mapping symmetries in the network structure to equilibrium actions. The authors show that the proposed method outperforms existing methods in terms of network structure inference and network games on both synthetic and real-world data. The paper is well-written and well-motivated, and the contributions of the paper are clear and convincing.  "
7000,SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"heterogeneous graphs USED-FOR relation prediction. ANalogy SubGraph Embedding Learning ( GraphANGEL ) HYPONYM-OF relation prediction framework. graph pattern USED-FOR logical rule. graph pattern USED-FOR explainable predictive models. inductive bias USED-FOR generalization. heterogeneous graph based recommendation CONJUNCTION knowledge graph completion. knowledge graph completion CONJUNCTION heterogeneous graph based recommendation. model COMPARE models. models COMPARE model. knowledge graph completion EVALUATE-FOR model. heterogeneous graph based recommendation EVALUATE-FOR model. knowledge graph completion EVALUATE-FOR models. heterogeneous graph based recommendation EVALUATE-FOR models. model USED-FOR explainable heat maps of attention scores. Material is knowledge graphs. OtherScientificTerm are embeddings, and subgraphs. Task is transductive setting. ","This paper proposes a new relation prediction framework called ANalogy Subgraph Embedding Learning (GraphANGEL) which uses heterogeneous graphs for relation prediction. The graph pattern is used as a logical rule for explainable predictive models, and the inductive bias is used for generalization. The authors show that the proposed model is able to generate explainable heat maps of attention scores, which are more interpretable than existing models such as heterogeneous graph based recommendation and knowledge graph completion. ","This paper proposes an extension of ANalogy SubGraph Embedding Learning (GraphANGEL) to relation prediction framework. The key idea is to use knowledge graphs as embeddings, and then use subgraphs to predict a logical rule. The authors show that the proposed graph pattern can be used to train explainable predictive models with inductive bias. The proposed model is shown to produce explainable heat maps of attention scores, and is compared to other models on heterogeneous graph based recommendation and knowledge graph completion. The experimental results show the effectiveness of the proposed model in the transductive setting."
7025,SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"well - labeled datasets CONJUNCTION rare abnormal samples. rare abnormal samples CONJUNCTION well - labeled datasets. Few - shot learning HYPONYM-OF natural images. cross - domain tasks USED-FOR real clinics problems. contrastive learning ( CL ) USED-FOR few - shot system. label - efficient learning CONJUNCTION generalizability. generalizability CONJUNCTION label - efficient learning. contrastive learning ( CL ) CONJUNCTION latent augmentation ( LA ). latent augmentation ( LA ) CONJUNCTION contrastive learning ( CL ). latent augmentation ( LA ) USED-FOR few - shot system. LA USED-FOR semantic variations. CL COMPARE LA. LA COMPARE CL. components USED-FOR label - hungry problems. unlabeled training data USED-FOR components. LA COMPARE baselines. baselines COMPARE LA. supervised learning USED-FOR histology images. models COMPARE supervised learning. supervised learning COMPARE models. CL COMPARE supervised learning. supervised learning COMPARE CL. CL USED-FOR models. ImageNet - like images USED-FOR self - supervised learning. CL COMPARE supervised learning. supervised learning COMPARE CL. generalization EVALUATE-FOR data. generalization EVALUATE-FOR CL. generalization EVALUATE-FOR supervised learning. representation learning CONJUNCTION histological image analysis. histological image analysis CONJUNCTION representation learning. model USED-FOR representation learning. model USED-FOR histological image analysis. Task is few - shot learning in histology images. OtherScientificTerm is manual labels. Material are images, and Histology images. ","This paper studies few-shot learning in histology images. The authors focus on cross-domain tasks in real clinics problems, where the goal is to learn a few images from a set of well-labeled datasets and rare abnormal samples. They propose contrastive learning (CL) and latent augmentation (LA) to improve the few-shoot system by learning semantic variations between the labeled and unlabeled training data. They show that CL outperforms supervised learning and supervised learning in terms of generalization on the data. ","This paper studies few-shot learning in histology images, which are natural images (e.g. Few-Shot learning in natural images). The authors propose contrastive learning (CL) and latent augmentation (LA) to improve the few-Shot system in real clinics problems with cross-domain tasks. The two components are used to solve label-hungry problems with unlabeled training data. The authors show that CL outperforms supervised learning and supervised learning outperforms models in representation learning and histological image analysis.  The authors also show that LA can be used to learn semantic variations between images, and that CL improves generalization on data with manual labels.  "
7050,SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"Recurrent neural networks ( RNNs ) USED-FOR irregularly - sampled time series. continuous - time hidden states USED-FOR Recurrent neural networks ( RNNs ). training FEATURE-OF gradient. memory compartment FEATURE-OF continuous - time networks. continuous - time dynamical flow PART-OF RNN. constant error propagation FEATURE-OF memory path. Mixed - MemoryRNNs COMPARE RNN - based counterparts. RNN - based counterparts COMPARE Mixed - MemoryRNNs. long - term dependencies FEATURE-OF non - uniformly sampled data. non - uniformly sampled data EVALUATE-FOR RNN - based counterparts. non - uniformly sampled data EVALUATE-FOR Mixed - MemoryRNNs. Generic are models, and it. Method are RNNs, ODE solver, and Mixed - Memory - RNNs ( mmRNNs ). OtherScientificTerm are hidden state, timecontinuous state, and time - lags. ","This paper studies the problem of irregularly-sampled time series in Recurrent neural networks (RNNs) with continuous-time hidden states. The authors consider the case where the hidden state is a timecontinuous state, and the RNN is trained with the ODE solver. They show that the gradient of the gradient during training is a function of the memory compartment of the recurrent neural networks. The memory path of an RNN can be decomposed into a continuous -time dynamical flow, and it is shown that the memory path has constant error propagation. They also show that Mixed-Memory-RNN can achieve better performance than RNN-based counterparts on non-uniform sampled data with long-term dependencies. ","This paper proposes a new model for irregularly-sampled time series, called Mixed-Memory-RNNs (MMMRNN). The main idea is to use continuous-time hidden states in Recurrent neural networks (RNN) to learn irregularly sampled time series. The authors propose a novel ODE solver, where the hidden state is a timecontinuous state, and the training of the gradient depends on the time-lags. The memory compartment is a continuous time dynamical flow in the RNN, and it can be decomposed into two parts. The first part of the memory path is a constant error propagation, while the second part is a memory compartment with long-term dependencies. Experiments are conducted on non-uniform sampled data and on RNN-based counterparts, showing that the mixed-MemoryRNN outperforms RNNs in terms of performance."
7075,SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,pre - trained BERT USED-FOR Natural Language Processing ( NLP ) tasks. 1 - bit parameters CONJUNCTION bitwise operations. bitwise operations CONJUNCTION 1 - bit parameters. binarization HYPONYM-OF compression approaches. 1 - bit parameters USED-FOR binarization. computation and memory consumption EVALUATE-FOR binarization. bitwise operations USED-FOR binarization. 1 - bit weight CONJUNCTION embedding. embedding CONJUNCTION 1 - bit weight. embedding CONJUNCTION activation. activation CONJUNCTION embedding. activation HYPONYM-OF BERT. 1 - bit weight HYPONYM-OF BERT. embedding HYPONYM-OF BERT. information degradation CONJUNCTION optimization direction mismatch. optimization direction mismatch CONJUNCTION information degradation. BiBERT USED-FOR performance bottlenecks. BiBERT HYPONYM-OF fully binarized BERT. optimization direction mismatch FEATURE-OF forward and backward propagation. DirectionMatching Distillation ( DMD ) scheme USED-FOR full binarized BERT. Bi - Attention structure USED-FOR representation information statistically. Bi - Attention structure CONJUNCTION DirectionMatching Distillation ( DMD ) scheme. DirectionMatching Distillation ( DMD ) scheme CONJUNCTION Bi - Attention structure. DirectionMatching Distillation ( DMD ) scheme USED-FOR BiBERT. Bi - Attention structure USED-FOR BiBERT. BiBERT COMPARE quantized BERTs. quantized BERTs COMPARE BiBERT. BiBERT COMPARE baseline. baseline COMPARE BiBERT. baseline COMPARE quantized BERTs. quantized BERTs COMPARE baseline. ultra - low bit activations FEATURE-OF quantized BERTs. NLP benchmark EVALUATE-FOR quantized BERTs. NLP benchmark EVALUATE-FOR BiBERT. FLOPs CONJUNCTION model size. model size CONJUNCTION FLOPs. fully binarized BERT model USED-FOR real - world resource - constrained scenarios. method USED-FOR fully binarized BERT model. fully binarized BERT EVALUATE-FOR method. model size EVALUATE-FOR method,"This paper proposes BiBERT, a pre-trained BERT for Natural Language Processing (NLP) tasks where the goal is to reduce the computation and memory consumption of BERT. Previous compression approaches such as binarization are based on bitwise operations and 1-bit parameters, but the performance bottlenecks of binarized BERT are not well-studied. The authors propose a new method that uses Bi-Attention structure and DirectionMatching Distillation (DMD) scheme to train a fully binarised BERT model. The proposed method is evaluated on the NLP benchmark and compared with quantized and fully-bounded BERT models. BiBERt outperforms the baseline and the fully-binarized models. ","This paper proposes a new pre-trained BERT for Natural Language Processing (NLP) tasks, BiBERT, which is a variant of BERT. The main idea is to use binarization to reduce the computation and memory consumption of compression approaches. The authors show that the proposed method outperforms the baseline in terms of performance bottlenecks, including information degradation, optimization direction mismatch, and forward and backward propagation. The proposed method is based on the Bi-Attention structure and the DirectionMatching Distillation (DMD) scheme. The paper also shows that the model size and FLOPs can be reduced by a factor of 1/\sqrt{n}/n, and that the performance of the fully binarized BERT model can be improved in real-world resource-constrained scenarios with ultra-low bit activations. "
7100,SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"method USED-FOR keypoint detection. method USED-FOR instance association. keypoint detection CONJUNCTION instance association. instance association CONJUNCTION keypoint detection. Transformer USED-FOR method. Transformer USED-FOR problems. association information USED-FOR keypoints grouping. self - attention PART-OF Transformer. supervising self - attention USED-FOR multi - person keypoint detection. supervising self - attention USED-FOR instance association. approach USED-FOR supervising self - attention. multi - person keypoint detection CONJUNCTION instance association. instance association CONJUNCTION multi - person keypoint detection. approach USED-FOR instance association. approach USED-FOR multi - person keypoint detection. instance masks USED-FOR self - attention. pre - defined offset vector fields CONJUNCTION embedding. embedding CONJUNCTION pre - defined offset vector fields. embedding CONJUNCTION CNN - based bottom - up models. CNN - based bottom - up models CONJUNCTION embedding. supervised attention matrix USED-FOR instance segmentation. person instance segmentation task EVALUATE-FOR method. COCO multi - person keypoint detection challenge EVALUATE-FOR method. COCO multi - person keypoint detection challenge CONJUNCTION person instance segmentation task. person instance segmentation task CONJUNCTION COCO multi - person keypoint detection challenge. OtherScientificTerm are associative information, naive attention patterns, pairwise attention scores, and self - attention behavior. Method is pixel assignment pipeline. ",This paper proposes a method for keypoint detection and instance association based on Transformer. The key idea is to use the self-attention in the Transformer to learn the association information between the keypoints grouping and the instance association. The authors show that the proposed approach is able to learn an instance association with the help of supervising self-tweets. They also show that their approach can be applied to multi-person keypoint prediction and instance segmentation. ,"This paper proposes a method for keypoint detection and instance association. The proposed method is based on Transformer, which is an extension of Transformer for problems where the keypoints grouping is done using association information. The key idea is to learn a pixel assignment pipeline, where each pixel is assigned a pairwise attention scores. The authors also propose a new approach for supervising self-attention for multi-person keypoint prediction and for instance association using instance masks. Experiments are conducted on COCO multi-posterior and a person instance segmentation task with embedding and pre-defined offset vector fields. "
7125,SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"reinforcement learning ( RL ) USED-FOR sequential decision making. Pareto efficiency FEATURE-OF MV - efficient policies. Generic are existing methods, approach, it, and methods. OtherScientificTerm are variance term, MV trade - off, expected quadratic utility function, Pareto efficient policy, computational difficulties, and gradient estimation of the variance. ","This paper proposes a new approach for sequential decision making in reinforcement learning (RL). The authors propose a new variance term, MV trade-off, to measure the Pareto efficiency of MV-efficient policies. The variance term is defined as the difference between the expected quadratic utility function and the expected value function of the policy. The authors show that the variance term can be computed by gradient estimation of the variance. The proposed approach can be applied to existing methods, and it is shown that it can achieve better performance than existing methods in terms of computational difficulties.","This paper proposes a new approach to sequential decision making using reinforcement learning (RL) that is based on the variance term in the MV trade-off between the expected quadratic utility function and the Pareto efficiency of MV-efficient policies. The approach is not novel, but it is an extension of existing methods, and it can be seen as an improvement over existing methods in terms of computational difficulties (e.g., gradient estimation of the variance). The authors also provide a theoretical analysis of their approach."
7150,SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"end - to - end learning USED-FOR communication system. autoencoder USED-FOR communication system. autoencoder USED-FOR end - to - end learning. test - time domain adaptation USED-FOR autoencoder system. fully - trained channel model CONJUNCTION autoencoder. autoencoder CONJUNCTION fully - trained channel model. error rate EVALUATE-FOR autoencoder. method USED-FOR autoencoder. feature transformations USED-FOR channel distribution. feature transformations USED-FOR decoder. feature transformations USED-FOR method. method USED-FOR MDN channel. simulated datasets CONJUNCTION real mmWave wireless channels. real mmWave wireless channels CONJUNCTION simulated datasets. error rate EVALUATE-FOR autoencoder. real mmWave wireless channels EVALUATE-FOR method. simulated datasets EVALUATE-FOR method. method USED-FOR autoencoder. error rate EVALUATE-FOR method. Generic is approach. Method are mixture density network ( MDN ), encoder and decoder neural networks, and MDN channel model. Material is unlabeled data. OtherScientificTerm are wireless link, and source distribution. ","This paper proposes a new approach to learn a mixture density network (MDN) for end-to-end learning for a communication system. The authors propose a test-time domain adaptation for the autoencoder system, which is based on a fully-trained channel model and an autoncoder. The encoder and decoder neural networks are trained in parallel, and the MDN channel model is trained on unlabeled data. The proposed method uses feature transformations to learn the channel distribution of the decoder and the encoder. They show that the proposed method has a lower error rate than the standard auto-encoder, and is able to learn an MDN with a higher error rate. The method is tested on simulated datasets and real mmWave wireless channels.","This paper proposes a method for end-to-end learning for communication system. The approach is based on a mixture density network (MDN), where the encoder and decoder neural networks are trained on unlabeled data. The authors propose a test-time domain adaptation for the autoencoder system, which is a fully-trained channel model and an auto-encoder with feature transformations for the channel distribution. The proposed method is evaluated on simulated datasets and real mmWave wireless channels, and the error rate of the proposed method on the MDN channel is shown to be better than that of a fully trained channel model. "
7175,SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"joint softmax focal loss HYPONYM-OF structural loss. model USED-FOR αNLI. ACC CONJUNCTION AUC. AUC CONJUNCTION ACC. AUC EVALUATE-FOR IMSL. RoBERTa - large pretrained model EVALUATE-FOR IMSL. ACC EVALUATE-FOR IMSL. Task are abductive natural language inference task ( αNLI ), and αNLI task. Method are inference network, interactive language model, and Interactive Model with Structural Loss ( IMSL ). OtherScientificTerm is reasoning abilities. ","This paper studies the abductive natural language inference task (αNLI), where the inference network is trained with an interactive language model. The authors propose a new structural loss, called joint softmax focal loss, which is an extension of the existing structural loss (joint softmax focal loss) to the αNLI task. The proposed model can be used to train the model in the context of the αNsLI task, and the authors show that IMSL with RoBERTa-large pretrained model can outperform ACC and AUC on ICSL with Interactive Model with Structural Loss (IMSL). The authors also show that the reasoning abilities of the proposed model are better than those of ACC.","This paper proposes an extension of the abductive natural language inference task (αNLI) to the αNLI task, where the inference network is an interactive language model. The authors propose an Interactive Model with Structural Loss (IMSL), which is a structural loss, i.e., joint softmax focal loss. The model is used to learn the parameters of the αNsLI, and the authors show that IMSL outperforms ACC and AUC in terms of reasoning abilities. They also show that a RoBERTa-large pretrained model can be used to train ImsL."
7200,SP:17cd72df5fc19398f582d27516fd742b073f79e3,"machine learning USED-FOR safety - critical systems. assessment of uncertainy USED-FOR machine learning. deep neural networks USED-FOR overconfident predictions. certifiable OOD detector CONJUNCTION classifier. classifier CONJUNCTION certifiable OOD detector. classifier PART-OF OOD aware classifier. OOD aware classifier PART-OF method. classifier PART-OF method. certifiable OOD detector PART-OF method. prediction accuracy CONJUNCTION detection. detection CONJUNCTION prediction accuracy. detection performance EVALUATE-FOR non - manipulated OOD data. classifier USED-FOR asymptotic overconfidence problem. classifier COMPARE neural networks. neural networks COMPARE classifier. asymptotic overconfidence problem FEATURE-OF neural networks. Material is OOD data. Task is certifiably adversarially robust OOD detection. OtherScientificTerm are OOD samples, and in - distribution. ",This paper studies the problem of certifiably adversarially robust OOD detection. The authors propose a method that combines a certifiable OOD detector and a classifier in an OOD aware classifier. The proposed method is based on the assessment of uncertainy in machine learning for safety-critical systems. The main idea is to use deep neural networks to make overconfident predictions for OOD samples. They show that the proposed classifier can be used to solve the asymptotic overconfidence problem in neural networks. They also provide a theoretical analysis of the detection performance of non-manifold OOD data.,"This paper proposes a method for certifiably adversarially robust OOD detection. The method consists of a certifiable OOD detector and an OOD aware classifier. The classifier is trained with deep neural networks to avoid overconfident predictions. The OOD samples are generated from the in-distribution, and the OOD detectors are trained on OOD data. The authors show that the classifier outperforms the neural networks in the asymptotic overconfidence problem in terms of prediction accuracy and detection performance on non-manifold OODs. "
7225,SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"Deep Neural Networks ( DNNs ) USED-FOR transfer attacks. query - free black - box setting FEATURE-OF transfer attacks. white - box surrogate models CONJUNCTION black - box victim models. black - box victim models CONJUNCTION white - box surrogate models. datasets USED-FOR surrogate models. method USED-FOR classification information. Image Classification Eraser ( ICE ) USED-FOR classification information. Image Classification Eraser ( ICE ) HYPONYM-OF method. Cifar-10 CONJUNCTION Cifar-100. Cifar-100 CONJUNCTION Cifar-10. Cifar-100 CONJUNCTION TieredImageNet. TieredImageNet CONJUNCTION Cifar-100. ICE USED-FOR GTA problem. Cifar-10 EVALUATE-FOR ICE. TieredImageNet EVALUATE-FOR ICE. transfer attack methods USED-FOR GTA problem. transfer attack methods COMPARE ICE. ICE COMPARE transfer attack methods. Task are transfer attack, and Generalized Transferable Attack ( GTA ) problem. Generic are dataset, and them. Method is victim model. ","This paper studies transfer attacks on Deep Neural Networks (DNNs) in the query-free black-box setting. The authors propose a method called Image Classification Eraser (ICE) to extract the classification information from the surrogate models and the white-box surrogate models. The transfer attack is based on the Generalized Transferable Attack (GTA) problem, where the target dataset is the victim model and the target model is the surrogate model. ICE is shown to outperform other transfer attack methods in the GTA problem on Cifar-10, CIFar-100, and TieredImageNet.","This paper proposes a new method for transfer attacks on Deep Neural Networks (DNNs) in the query-free black-box setting. The authors propose a method called Image Classification Eraser (ICE) to extract the classification information from the target dataset and use them as surrogate models for the transfer attack. The proposed method is evaluated on CIFAR-10, Cifar-100, and TieredImageNet, and compared to other transfer attack methods for the GTA problem. "
7250,SP:2e0447c741a3f09be1095633d870200355211260,discriminative PrLM USED-FOR contextualized representation. robustness EVALUATE-FOR PrLMs. pre - training methods USED-FOR false negative predictions. pre - training methods USED-FOR pre - training language models. false negative issue FEATURE-OF discriminative PrLMs. false negative predictions FEATURE-OF gradient updates. Generic is model. Material is GLUE and SQuAD benchmarks. ,"This paper proposes a discriminative PrLM for contextualized representation. The authors show that PrLMs with pre-training methods can suffer from a false negative issue in the presence of gradient updates, and propose a pre-trained language models to address this issue. The proposed model is evaluated on GLUE and SQuAD benchmarks.",This paper proposes a discriminative PrLM for contextualized representation. The authors show that the robustness of PrLMs with pre-training methods to false negative predictions can be improved by pre-train language models. The model is evaluated on GLUE and SQuAD benchmarks. The paper also shows that the false negative issue can be alleviated by the use of gradient updates. 
7275,SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"semi - supervised learning USED-FOR real - world settings. ORCA HYPONYM-OF end - to - end approach. uncertainty adaptive margin USED-FOR ORCA. discriminability FEATURE-OF model. discriminability EVALUATE-FOR ORCA. image classification datasets CONJUNCTION single - cell dataset. single - cell dataset CONJUNCTION image classification datasets. single - cell dataset EVALUATE-FOR ORCA. ORCA COMPARE baselines. baselines COMPARE ORCA. seen CONJUNCTION novel classes. novel classes CONJUNCTION seen. image classification datasets EVALUATE-FOR ORCA. image classification datasets EVALUATE-FOR baselines. single - cell dataset EVALUATE-FOR baselines. ORCA USED-FOR novel classes. novel classes PART-OF ImageNet dataset. seen EVALUATE-FOR ORCA. ImageNet dataset EVALUATE-FOR ORCA. Material are unlabeled test data, labeled training data, open - world semi - supervised learning setting, and labeled and unlabeled data. Generic is assumption. Task is class distribution mismatch problem. OtherScientificTerm is prior knowledge. ","This paper proposes ORCA, an end-to-end approach to semi-supervised learning in real-world settings where unlabeled test data is available and labeled training data is unavailable. ORCA is based on the uncertainty adaptive margin, which is an extension of ORCA. The authors show that ORCA improves the discriminability of the model in terms of the uncertainty of the class distribution mismatch problem.  The authors also show that the ORCA outperforms other baselines on image classification datasets and a single-cell dataset, and novel classes in the ImageNet dataset. ","This paper proposes a novel end-to-end approach to semi-supervised learning for real-world settings. The authors propose ORCA, a method to improve the discriminability of the model with an uncertainty adaptive margin. The assumption is that unlabeled test data will not be correlated with labeled training data, and that the class distribution mismatch problem can be solved in an open-world semi-Supervised learning setting. To address this assumption, the authors propose to use prior knowledge. Experiments on ImageNet dataset and single-cell dataset show that ORCA outperforms baselines on both seen and novel classes. "
7300,SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"SLIM - QN HYPONYM-OF light stochastic quasi - Newton optimizer. second - order methods USED-FOR large - scale DNNs. SLIM - QN USED-FOR second - order methods. L - BFGS HYPONYM-OF stochastic training. BFGS update rule USED-FOR Hessian inverse. gradients USED-FOR BFGS update rule. BFGS update rule USED-FOR SLIM - QN. momentum CONJUNCTION adaptive damping mechanism. adaptive damping mechanism CONJUNCTION momentum. momentum USED-FOR Hessian updates. adaptive damping mechanism USED-FOR SLIM - QN. momentum USED-FOR SLIM - QN. SLIM - QN USED-FOR stable convergence. SLIM - QN USED-FOR stochastic setting. convergence EVALUATE-FOR SLIM - QN. SLIM - QN COMPARE second - order methods. second - order methods COMPARE SLIM - QN. compute and memory overhead EVALUATE-FOR second - order methods. compute and memory overhead EVALUATE-FOR SLIM - QN. SLIM - QN COMPARE SGD. SGD COMPARE SLIM - QN. large datasets EVALUATE-FOR SLIM - QN. near optimal accuracy EVALUATE-FOR SGD. wall - clock time EVALUATE-FOR SGD. near optimal accuracy EVALUATE-FOR SLIM - QN. ImageNet HYPONYM-OF large datasets. compute resources USED-FOR SGD. compute resources USED-FOR SLIM - QN. SLIM - QN USED-FOR non - convolutional architectures. Transformers HYPONYM-OF non - convolutional architectures. Metric is computational cost. OtherScientificTerm are Hessian matrix, KFAC, and convergence instability. ","This paper proposes SLIM-QN, a light stochastic quasi-Newton optimizer for large-scale DNNs. The authors propose to use momentum and adaptive damping mechanism to improve the stability of the Hessian updates in the L-BFGS training in stochedastic training. The BFGS update rule is based on a Hessian inverse with gradients, and the authors show the convergence of SLIM - QN with stable convergence under the same computational cost as SGD.   The authors also show that the convergence instability of SGD with the same number of compute resources is lower than the convergence stability in the same amount of wall-clock time.  Finally, the authors provide a theoretical analysis of the computational cost of the second-order methods in the second order methods in large-size DNN. They also provide theoretical analysis on the performance of the SLIM QN on large datasets such as ImageNet, Transformers, and other non-convolutional architectures. ","This paper proposes SLIM-QN, a light stochastic quasi-Newton optimizer for large-scale DNNs. The authors propose to use L-BFGS, an extension of the L -BFGS to stoChastic training. The main idea is to use momentum and adaptive damping mechanism to encourage Hessian updates to the Hessian matrix to be stable.  The authors also propose a BFGS update rule that is based on Hessian inverse and gradients. The computational cost is reduced by computing KFAC. The convergence instability is also reduced by using momentum.  "
7325,SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"Graph Neural Networks ( GNNs ) USED-FOR graph - related tasks. GNNs USED-FOR problems. redundant components PART-OF large graphs. pre - processing step USED-FOR graph. node or edge removals USED-FOR inference. LocalitySensitive Pruning ( LSP ) USED-FOR graph pruning. systematic method USED-FOR graph pruning. Locality - Sensitive Hashing USED-FOR LocalitySensitive Pruning ( LSP ). Locality - Sensitive Hashing USED-FOR systematic method. pruning COMPARE pruning strategies. pruning strategies COMPARE pruning. locality properties USED-FOR pruning. local graph properties USED-FOR pruning. synthetic and real - world datasets EVALUATE-FOR LSP. LSP USED-FOR edges. edges PART-OF large graphs. LSP USED-FOR large graphs. Task is real - world problems. OtherScientificTerm are real - world graphs, and sparsified graph. Method is GNNs layers. ",Graph Neural Networks (GNNs) are well-suited for graph-related tasks. GNNs can be used to solve problems with redundant components in large graphs. The paper proposes a systematic method for graph pruning called Locality-Sensitive Pruning (LSP). LSP is a pre-processing step that prunes the graph with node or edge removals. The pruning is based on locality properties that are learned during pruning. LSP can be applied to large graphs with edges and sparsified graph. Experiments on synthetic and real-world datasets demonstrate the effectiveness of LSP. ,"This paper proposes a systematic method for graph pruning, LocalitySensitive Pruning (LSP), which is based on Locality-Sensitive Hashing. LSP prunes large graphs by removing redundant components in large graphs. The authors propose a pre-processing step to prune the graph and then use node or edge removals for inference. They show that LSP can prune edges of large graphs with different locality properties. They also show that the pruning results in better performance than other pruning strategies on synthetic and real-world datasets. "
7350,SP:c5e024f4e2079586298519ca868630efd7579eca,"Data augmentation USED-FOR contrastive self - supervised learning. identitydistinctive information FEATURE-OF R(x ). it PART-OF R(x ). VAE ’s bottleneck space FEATURE-OF G(x ). identity - disentangled adversarial augmentation ( IDAA ) USED-FOR self - supervised learning methods. benchmark datasets EVALUATE-FOR IDAA. efficiency CONJUNCTION generalization performance. generalization performance CONJUNCTION efficiency. efficiency EVALUATE-FOR IDAA. generalization performance EVALUATE-FOR IDAA. dataset USED-FOR IDAA. Generic is augmentations. Task is ineffective learning. Method are adversarial augmentation method, and contrastive learning. OtherScientificTerm are hard positives / negatives, variational auto - encoder ( VAE ) reconstruction, VAE objective, augmentation, and sample identity. ","This paper studies the problem of data augmentation in contrastive self-supervised learning. The authors propose a new adversarial augmentation method, IDAA, which is based on identity disentangled adversarial perturbation (IDAA). IDAA augments the input data with hard positives/negative samples, and the VAE objective is a variational auto-encoder (VAE) reconstruction. IDAA is shown to improve the efficiency and generalization performance on several benchmark datasets. ",This paper proposes a novel adversarial augmentation method for contrastive learning. The idea is to use data augmentation in contrastive self-supervised learning to improve the efficiency and generalization performance of the adversarial learning methods. The authors propose an identity-disentangled adversarial augmentmentation (IDAA) method that is based on a variational auto-encoder (VAE) reconstruction. The VAE objective is to learn the identitydistinctive information of the R(x) of R(y) in the VAE’s bottleneck space of G(x). The authors show that IDAA improves the efficiency of IDAA on several benchmark datasets. They also show that the augmentations do not lead to ineffective learning. 
7375,SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"tests USED-FOR distribution shifts. these USED-FOR arbitrary shifts. non - sequential methods USED-FOR these. method USED-FOR harmful shifts. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. sequential tools USED-FOR risk function of interest. calibration HYPONYM-OF risk function of interest. accuracy HYPONYM-OF risk function of interest. aggregation of statistical evidence USED-FOR tracking process. constructing time - uniform confidence sequences USED-FOR aggregation of statistical evidence. simulated and real datasets EVALUATE-FOR framework. Method is machine learning models. OtherScientificTerm are data distribution, and benign shifts. Generic is model. Metric is false alarm rate. ","This paper proposes a new method to detect harmful shifts in machine learning models. The authors propose to use a set of tests to detect distribution shifts in the data distribution. These tests are based on non-sequential methods, and these are designed to detect arbitrary shifts. The proposed method is based on sequential tools to estimate the risk function of interest (i.e., the accuracy and calibration). The tracking process is then based on the aggregation of statistical evidence from the tracking process by constructing time-uniform confidence sequences. The model is trained with a false alarm rate that depends on the number of samples in the model. Experiments on simulated and real datasets demonstrate the effectiveness of the proposed framework.","This paper proposes a new method to detect harmful shifts in machine learning models. The authors propose two tests to detect distribution shifts in the data distribution. They use non-sequential methods to detect these, and these can be used to detect arbitrary shifts. The proposed method uses sequential tools to estimate the risk function of interest (accuracy, calibration, etc) and the tracking process is based on the aggregation of statistical evidence. They also propose constructing time-uniform confidence sequences to detect benign shifts. Experiments on simulated and real datasets demonstrate the effectiveness of the proposed framework. "
7400,SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,Neural networks USED-FOR dynamics of diverse physical systems. neural implicit representations USED-FOR appearance modeling. neural implicit representations CONJUNCTION neural ordinary differential equations ( ODEs ). neural ordinary differential equations ( ODEs ) CONJUNCTION neural implicit representations. neural ordinary differential equations ( ODEs ) USED-FOR interpretable physical models. visual observations USED-FOR interpretable physical models. neural implicit representations USED-FOR processing of high - resolution videos. neural implicit representations USED-FOR synthesis of photo - realistic imagery. processing of high - resolution videos CONJUNCTION synthesis of photo - realistic imagery. synthesis of photo - realistic imagery CONJUNCTION processing of high - resolution videos. model COMPARE approaches. approaches COMPARE model. model USED-FOR physical parameters. large training datasets USED-FOR approaches. embedded neural ODE USED-FOR identification of interpretable physical parameters. known parametric form FEATURE-OF embedded neural ODE. scenes USED-FOR photo - realistic rendering. physical parameters USED-FOR scenes. physical parameters USED-FOR photo - realistic rendering. method USED-FOR physical parameters. real - world videos USED-FOR method. pendulum motion HYPONYM-OF real - world videos. real - world videos USED-FOR physical parameters. physical parameters USED-FOR reconstruction. model USED-FOR metric length of the pendulum. monocular video USED-FOR model. monocular video USED-FOR metric length of the pendulum. Generic is they. Material is high - resolution videos. Task is long - term prediction in state space. OtherScientificTerm is state space. Metric is relative error. ,"This paper proposes a new method for long-term prediction in state space. The proposed method is based on neural networks to model the dynamics of diverse physical systems. The authors use neural implicit representations for appearance modeling and neural ordinary differential equations (ODEs) for interpretable physical models based on visual observations. They show that the proposed model is able to identify physical parameters for reconstruction from real-world videos such as pendulum motion, processing of high-resolution videos, and synthesis of photo-realistic imagery. They also show that they can identify the relative error of the embedding of the embedded neural ODE in the known parametric form. They demonstrate that their proposed method outperforms existing approaches on large training datasets.","This paper proposes a novel approach to learning the dynamics of diverse physical systems. The authors propose to use neural networks to model the dynamic of the physical system. The model is based on neural implicit representations for appearance modeling and neural ordinary differential equations (ODEs) for interpretable physical models based on visual observations. The proposed method is evaluated on real-world videos of pendulum motion, and on the synthesis of photo-realistic imagery and the processing of high-resolution videos. The paper shows that the proposed model is able to learn physical parameters for reconstruction and reconstruction of scenes, and that they can be used for long-term prediction in state space. Experiments are conducted on large training datasets, and the proposed approach outperforms other approaches in terms of relative error. "
7425,SP:51efd1451343f4994d857daa5490e299b812bc2d,"abrupt ( discontinuous ) context changes CONJUNCTION Markovian context evolution. Markovian context evolution CONJUNCTION abrupt ( discontinuous ) context changes. Bayesian approach CONJUNCTION variational inference. variational inference CONJUNCTION Bayesian approach. Bayesian approach USED-FOR it. variational inference USED-FOR it. sticky Hierarchical Dirichlet Process ( HDP ) prior USED-FOR model learning. sticky Hierarchical Dirichlet Process ( HDP ) prior USED-FOR Markov process modeling. context distillation procedure USED-FOR spurious contexts. optimal policy USED-FOR policy learning. RL algorithms USED-FOR policy learning. state - of - the - art methods USED-FOR frameworks. Generic are case, components, and approach. OtherScientificTerm are context cardinality assumption, and drone. ","This paper proposes a new framework for Markov process modeling based on the sticky Hierarchical Dirichlet Process (HDP) prior for model learning. The key idea is to use a context distillation procedure to remove spurious contexts from the training data. The authors show that it can be combined with a Bayesian approach and variational inference to improve the performance of the model. They also show that the optimal policy can be learned using RL algorithms for policy learning. Finally, they show that their framework can be applied to various state-of-the-art methods.",This paper proposes a new framework for Markov process modeling based on the sticky Hierarchical Dirichlet Process (HDP) prior for model learning. The key idea is to use a Bayesian approach and variational inference to learn the context cardinality assumption. The main contribution of the paper is the use of a context distillation procedure to identify spurious contexts. The authors show that the proposed framework can be applied to any case where the case is not well-studied. They also show that it can be used in conjunction with other state-of-the-art methods to improve the performance of these frameworks. 
7450,SP:ea167b126212b2092bc1190d7f8376bf7c54a888,Knowledge enriched language representation learning USED-FOR knowledge - intensive NLP tasks. monolingual knowledge graph data USED-FOR knowledge based language models. framework USED-FOR knowledge based multilingual language models ( KMLMs ). pretraining tasks USED-FOR knowledge learning. language models USED-FOR logical patterns. intraand inter - sentence structures USED-FOR pretraining tasks. intraand inter - sentence structures FEATURE-OF data. language models USED-FOR factual knowledge. factual knowledge retrieval CONJUNCTION relation classification. relation classification CONJUNCTION factual knowledge retrieval. named entity recognition CONJUNCTION factual knowledge retrieval. factual knowledge retrieval CONJUNCTION named entity recognition. pretrained KMLMs USED-FOR knowledge - intensive cross - lingual NLP tasks. relation classification CONJUNCTION task. task CONJUNCTION relation classification. task HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. logic reasoning HYPONYM-OF task. named entity recognition HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. relation classification HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. factual knowledge retrieval HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. Material is Wikidata knowledge graphs. Method is pretrained language models. ,"Knowledge enriched language representation learning is an important problem in knowledge-intensive NLP tasks. The paper proposes a framework to train knowledge based multilingual language models (KMLMs) on monolingual knowledge graph data. The pretraining tasks are based on intraand inter-sentence structures of the data, and the goal is to learn logical patterns from the language models. The authors show that the pretrained language models are able to learn factual knowledge from Wikidata knowledge graphs. They also show that pretrained KMLMs can be used to perform knowledge-intensive cross-lingual NLP task such as factual knowledge retrieval, relation classification, and named entity recognition.","This paper proposes a framework for learning knowledge based multilingual language models (KMLMs) from monolingual knowledge graph data. The framework is based on knowledge enriched language representation learning for knowledge-intensive NLP tasks. The key idea is to use language models to predict logical patterns in the data, and then use intraand inter-sentence structures to perform pretraining tasks for knowledge learning. Experiments on Wikidata knowledge graphs show that the pretrained language models are able to predict factual knowledge as well as relation classification, factual knowledge retrieval, and named entity recognition. "
7475,SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"agents USED-FOR altruistic behaviour. task - agnostic manner USED-FOR altruistic behaviour. multi - agent environments EVALUATE-FOR approach. unsupervised agents COMPARE them. them COMPARE unsupervised agents. Method are artificial agents, reinforcement learning agents, altruistic agent, and human agents. OtherScientificTerm are external supervision, and altruistic agent ’s behaviour. Generic is concept. ","This paper studies the problem of learning agents that can be trained in a task-agnostic manner to learn an altruistic behaviour in a multi-agent environment. The authors propose a new concept called “external supervision”, which is an extension of reinforcement learning agents where the goal is to learn a “good” agent’s behaviour in the presence of external supervision. They show that this concept can be applied to both artificial agents as well as human agents. They also show that the proposed approach can be used in multi- agent environments. Finally, they show that unsupervised agents can learn them better than them in some cases.","This paper proposes a novel approach to learning agents that can be trained in a task-agnostic manner to learn altruistic behaviour in a multi-agent manner. The authors propose to use artificial agents as reinforcement learning agents, where the goal is to learn an agent’s behaviour without external supervision. The main idea is to train an altruistic agent that is independent of the environment and the environment itself. This concept is not new, but this paper introduces a new way of learning the concept. The proposed approach is evaluated on multi-agents environments and shows that the proposed approach outperforms unsupervised agents in terms of the number of agents trained and the amount of time it takes to train them compared to human agents."
7500,SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"Neural Tangent Kernel USED-FOR neural networks. finite - width neural networks USED-FOR double descent. interpolation threshold FEATURE-OF double descent behaviour. optimum FEATURE-OF Hessian. loss function USED-FOR double descent. neural networks CONJUNCTION Hessian spectra. Hessian spectra CONJUNCTION neural networks. OtherScientificTerm are Double descent, and population loss. Generic is models. Method are linear and kernel regression models, influence functions, and parametric model. ","This paper proposes a Neural Tangent Kernel for neural networks with finite-width neural networks. Double descent is a well-studied problem in linear and kernel regression models. The authors show that the interpolation threshold of double descent behaviour is the optimum of the Hessian of a Hessian with a population loss. They then propose a new loss function for double descent with the same loss function in neural networks and Hessian spectra. They show that under certain assumptions on the influence functions of the parametric model, the double descent can be approximated by the population loss, and they show that their models are able to approximate double descent.","This paper proposes a Neural Tangent Kernel for neural networks with finite-width neural networks for double descent. Double descent is an important problem in linear and kernel regression models. The authors propose two models for this problem. The first model is a parametric model, and the second model is an extension of the existing models. In the first model, the authors propose a population loss function for the double descent, which is an interpolation threshold of double descent behaviour. The Hessian of the Hessian is defined as the optimum of a Hessian with respect to the input data. The influence functions of the two models are defined as a function of the number of samples and the size of the population loss. Experiments are performed on neural networks and Hessian spectra."
7525,SP:b485114712055f39a7afb951dbc3db482ff523fd,"Graph convolutional networks ( GCNs ) USED-FOR graph - structured data. over - smoothing problem FEATURE-OF deep GCNs. expressive power CONJUNCTION trainability. trainability CONJUNCTION expressive power. trainability CONJUNCTION optimization perspective. optimization perspective CONJUNCTION trainability. expressive power CONJUNCTION optimization perspective. optimization perspective CONJUNCTION expressive power. expressive power EVALUATE-FOR deep GCNs. expressivity COMPARE trainability. trainability COMPARE expressivity. Graph Neural Tangent Kernel ( GNTK ) USED-FOR optimization trajectory. Graph Neural Tangent Kernel ( GNTK ) USED-FOR wide GCNs. gradient descent USED-FOR wide GCNs. Graph Neural Tangent Kernel ( GNTK ) USED-FOR gradient descent. gradient descent USED-FOR optimization trajectory. asymptotic behaviors USED-FOR dropping trainability. dropping trainability FEATURE-OF wide and deep GCNs. exponential rate FEATURE-OF optimization process. asymptotic behaviors FEATURE-OF GNTK. large depth FEATURE-OF asymptotic behaviors. exponential rate FEATURE-OF dropping trainability. large depth FEATURE-OF GNTK. exponential rate FEATURE-OF wide and deep GCNs. theoretical framework USED-FOR residual connection - based techniques. residual connection - based techniques USED-FOR exponential decay of trainability. method COMPARE counterparts. counterparts COMPARE method. infinite - width and finite - width FEATURE-OF counterparts. Method are node representations, gradient descentbased optimizer, and DropEdge. OtherScientificTerm are expressive space, and exponential decay problem. ","Graph convolutional networks (GCNs) are widely used for graph-structured data. The over-smoothing problem of deep GCNs is a well-studied and well-understood problem in the literature. The authors consider the problem of learning node representations in the expressive space, and propose a gradient descentbased optimizer, DropEdge. The optimization trajectory is based on gradient descent with Graph Neural Tangent Kernel (GNTK) for wide GCNs, and gradient descent for deep GCN. The theoretical framework provides a theoretical framework for residual connection-based techniques for the exponential decay of trainability and dropping trainability in GNTK. The empirical results show that the exponential rate of the optimization process in the optimization of a wide and deep GCNN with asymptotic behaviors such as large depth, expressive power, trainability, and optimization perspective are better than the expressivity and trainability of the counterparts in infinite-width and finite-width.","Graph convolutional networks (GCNs) are used for graph-structured data. The over-smoothing problem of deep GCNs is a well-studied and well-known problem. The paper proposes a novel approach to solve this problem by learning node representations. The authors propose a gradient descentbased optimizer, called DropEdge, which uses Graph Neural Tangent Kernel (GNTK) to learn the optimization trajectory of wide GCNs, and an optimization perspective that combines the expressive power and the trainability. The proposed method is evaluated on infinite-width and finite-width datasets, and outperforms its counterparts in terms of expressivity, trainability, and dropping trainability in the optimization process. The exponential decay problem is studied in the theoretical framework of residual connection-based techniques. The empirical results show that GNTK has asymptotic behaviors and large depth in the expressive space."
7550,SP:25a92b3583afdc6892e59f1e769125d52c8011af,Computer vision methods USED-FOR first - order dynamics. optical flow HYPONYM-OF first - order dynamics. acceleration HYPONYM-OF higher - order changes. blood pressure CONJUNCTION arterial disease. arterial disease CONJUNCTION blood pressure. second derivative USED-FOR blood pressure. second derivative USED-FOR arterial disease. heart rate HYPONYM-OF summary statistics. videos USED-FOR cardiac measurements. waveform morphology USED-FOR clinically impactful scenarios. accuracy EVALUATE-FOR waveform morphology. loss function USED-FOR neural models. neural models USED-FOR higher - order dynamics. second - derivative inputs USED-FOR second - order dynamics. model USED-FOR left ventricle ejection time ( LVET ) intervals. second derivative PART-OF training procedure. second derivative USED-FOR model. second derivative FEATURE-OF vital sign signals. OtherScientificTerm is cardiac pulse. Task is camera - based vital sign measurement. ,"This paper studies the problem of camera-based vital sign measurement. The authors propose a new loss function for neural models to capture higher-order dynamics such as optical flow and acceleration, which are commonly used in computer vision methods for first-order dynamic such as blood pressure and arterial disease. The proposed loss function is based on the second derivative in the training procedure of a second derivative of the first derivative in a training procedure. The second derivative is used to capture the cardiac pulse in cardiac measurements, such as heart rate and summary statistics. The paper shows that the waveform morphology of the waveforms can be used for clinically impactful scenarios, and that the accuracy of waveform morphologies can be improved. The model is also able to capture left ventricle ejection time (LVET) intervals. ","This paper proposes a novel method for learning the first-order dynamics of cardiac measurements, i.e., cardiac pulse, cardiac rate, and heart rate, from videos. The authors propose to use computer vision methods to learn the first -order dynamics, e.g., optical flow, which is a more general form of optical flow. They show that the second derivative of the first derivative can be used to model the blood pressure, arterial disease, and acceleration of higher-order changes. They also show that neural models trained with the second-order inputs are able to learn higher-orders dynamics, and that the loss function of neural models can be learned in a similar way to the one used in camera-based vital sign measurement. Finally, the authors propose a new training procedure, which uses a second derivative in the training procedure to train the model to predict the left ventricle ejection time (LVET) intervals. They demonstrate that the waveform morphology of the cardiac measurements is able to capture clinically impactful scenarios, and the accuracy of the model can be improved."
7575,SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"simple interactions USED-FOR human language. architecture USED-FOR communication system. symbolic mapping PART-OF communication system. symbolic mapping HYPONYM-OF architecture. symbolic mapping USED-FOR language learning. referential games USED-FOR symbolic mapping. symbolic mapping USED-FOR language learning. process USED-FOR multi - agent language learning. simplicity CONJUNCTION complexity. complexity CONJUNCTION simplicity. Task are emergent communication, and vocabulary expansion. OtherScientificTerm are language, and compositional and symmetric language. Material is dialog games. ","This paper studies the problem of emergent communication in multi-agent language learning. The authors propose a new architecture, symbolic mapping, for the communication system. The symbolic mapping is based on referential games, where the goal is to learn simple interactions between agents in a human language. The paper shows that the symbolic mapping can be used for language learning in a variety of settings, including compositional and symmetric language. They also show that the process can be applied to multi-adversarial language learning, and that the complexity of the process is also reduced. ","This paper proposes a new architecture for the communication system, called symbolic mapping, which is based on the idea of simple interactions between human language. The idea of emergent communication is to learn a language that is compositional and symmetric. The authors show that the symbolic mapping can be used for language learning in referential games, and that the process can be applied to multi-agent language learning. Experiments are conducted on dialog games, showing that the simplicity and complexity of the proposed architecture can be improved by vocabulary expansion."
7600,SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,Robotic agents USED-FOR domestic chores. natural language directives USED-FOR Robotic agents. hierarchical modular approach USED-FOR agents. hierarchy FEATURE-OF policy. language instructions USED-FOR subgoals. navigation policy CONJUNCTION independent interaction policies. independent interaction policies CONJUNCTION navigation policy. master policy USED-FOR agent ’s navigation. interaction policy USED-FOR object masks. object masks USED-FOR manipulation actions. interaction policy USED-FOR manipulation actions. ALFRED benchmark EVALUATE-FOR hierarchical agent. OtherScientificTerm is divide - andconquer manner. Method is Compositional Reasoning. ,"This paper proposes a hierarchical modular approach to train Robotic agents to perform domestic chores using natural language directives. The agents are trained in a divide-andconquer manner, where the goal is to learn a set of subgoals from the language instructions. Each subgoal is represented by a hierarchy of policy that is learned in a hierarchical manner. The agent’s navigation policy and independent interaction policies are learned from the master policy. The interaction policy is used to learn object masks for manipulation actions. The hierarchical agent is evaluated on the ALFRED benchmark. Compositional Reasoning is also applied.","This paper proposes a hierarchical modular approach to train agents that can be trained on natural language directives for domestic chores. The key idea is to learn a hierarchy of subgoals in a divide-andconquer manner. The hierarchy is composed of a navigation policy, independent interaction policies, and a master policy that guides the agent’s navigation. The authors propose to use language instructions to guide the agents’ subgates. The interaction policy is used to learn object masks for manipulation actions. Compositional Reasoning is also used. The proposed hierarchical agent is evaluated on the ALFRED benchmark."
7625,SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"relationship USED-FOR model. Nuisance - Randomized Distillation ( NURD ) USED-FOR predictive models. nuisance - label relationship FEATURE-OF distributions. nuisance - randomized distribution HYPONYM-OF distribution. representations COMPARE representations. representations COMPARE representations. NURD USED-FOR representation. distribution PART-OF nuisance - varying family. NURD USED-FOR models. models USED-FOR pneumonia. NURD USED-FOR pneumonia. non - lung patches USED-FOR NURD. tasks EVALUATE-FOR NURD. non - lung patches USED-FOR nuisance. spurious correlations USED-FOR models. tasks EVALUATE-FOR NURD. chest X - ray classification EVALUATE-FOR NURD. chest X - ray classification HYPONYM-OF tasks. Task is prediction problems. OtherScientificTerm are nuisance variable, covariates, and background. Material is natural images. ","This paper proposes Nuisance-Randomized Distillation (NURD) for predictive models. NURD is based on the notion of nuisance-label relationship between two distributions, the nuisance-randomized distribution and the distribution of the nuisance variable. The authors show that the relationship between the two distributions can be used to improve the model's performance in prediction problems. The main contribution of the paper is a new representation of the distribution in a nuisance-differential family, which can be combined with existing representations such as the NIST. The paper also shows that the proposed models can improve the performance of models trained with non-lung patches for pneumonia by using spurious correlations between the nuisance and covariates in the background. The experiments on two tasks, chest X-ray classification and a few other tasks demonstrate the effectiveness of the proposed model.","This paper proposes Nuisance-Randomized Distillation (NURD) for predictive models. NURD is based on the notion of nuisance-label relationship between distributions of distributions with a nuisance variable. The distribution is a nuisance-randomized distribution, which is a family of distribution in the nuisance-varying family. The authors show that the relationship between the distribution and the model can be used to improve the performance of the model. They also show that models trained with non-lung patches can be more robust to pneumonia due to spurious correlations between the covariates in the background and the distribution of the source distribution. The paper also shows that the representations learned by the proposed representations are more robust against pneumonia than other representations. The experiments are conducted on several tasks, including chest X-ray classification, and show the effectiveness of the proposed model."
7650,SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"computer vision models USED-FOR predefined categories. natural language USED-FOR supervision. InfoNCE loss USED-FOR model. model USED-FOR text captions. images CONJUNCTION text captions. text captions CONJUNCTION images. image - text pairs USED-FOR CLIP. Optimal TransporT distillation USED-FOR zero - shot Recognition. online entropic optimal transport USED-FOR soft image - text match. online entropic optimal transport USED-FOR contrastive learning. soft image - text match USED-FOR contrastive learning. Optimal TransporT distillation USED-FOR OTTER. online entropic optimal transport USED-FOR OTTER. pretrained image and text encoders USED-FOR models. image text pairs USED-FOR models. OTTER USED-FOR models. label smoothing CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION label smoothing. InfoNCE loss CONJUNCTION label smoothing. label smoothing CONJUNCTION InfoNCE loss. InfoNCE loss COMPARE OTTER. OTTER COMPARE InfoNCE loss. OTTER COMPARE baselines. baselines COMPARE OTTER. Google Open Images CONJUNCTION multi - labeled ImageNet. multi - labeled ImageNet CONJUNCTION Google Open Images. label smoothing COMPARE OTTER. OTTER COMPARE label smoothing. knowledge distillation COMPARE OTTER. OTTER COMPARE knowledge distillation. Google Open Images EVALUATE-FOR zero - shot evaluation. zero - shot evaluation EVALUATE-FOR baselines. zero - shot evaluation EVALUATE-FOR OTTER. multi - labeled ImageNet EVALUATE-FOR OTTER. OTTER COMPARE baselines. baselines COMPARE OTTER. dataset / architecture settings EVALUATE-FOR OTTER. OtherScientificTerm are visual concepts, and supervised "" gold "" labels. ","This paper proposes Optimal TransporT distillation for zero-shot Recognition (OTTER), a new method for training computer vision models for predefined categories. OTTER is based on online entropic optimal transport for soft image-text match for contrastive learning. The model is trained using pretrained image and text encoders. The authors show that OTTER outperforms the baselines on a variety of dataset/architecture settings, including Google Open Images, multi-label ImageNet, and ImageNet-C.","This paper proposes a novel way to train computer vision models for predefined categories. The authors propose to use natural language as the supervision. The model is trained on images and text captions, and then the model is used to predict the captions of the images and the text. The supervised ""gold"" labels are generated from visual concepts. The CLIP is a set of image-text pairs for CLIP. The models are trained using pretrained image and text encoders. The proposed Optimal TransporT distillation is used for zero-shot Recognition, and online entropic optimal transport is used in contrastive learning for soft image-Text match. OTTER is evaluated on three dataset/architecture settings: Google Open Images, multi-labeled ImageNet, and zero-shoot evaluation. The results show that OTTER outperforms the baselines in terms of label smoothing, InfoNCE loss, and knowledge distillation. "
7675,SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,Pix2Seq USED-FOR object detection. prior knowledge USED-FOR task. prior knowledge USED-FOR approaches. language modeling task USED-FOR object detection. bounding boxes CONJUNCTION class labels. class labels CONJUNCTION bounding boxes. neural net USED-FOR image. class labels HYPONYM-OF Object descriptions. bounding boxes HYPONYM-OF Object descriptions. it COMPARE detection algorithms. detection algorithms COMPARE it. task - specific data augmentations USED-FOR approach. COCO dataset EVALUATE-FOR detection algorithms. COCO dataset EVALUATE-FOR approach. COCO dataset EVALUATE-FOR it. Pix2Seq framework USED-FOR object detection. bounding boxes CONJUNCTION class labels. class labels CONJUNCTION bounding boxes. neural net USED-FOR image. ,"This paper proposes Pix2Seq for object detection using prior knowledge from a language modeling task. Object descriptions such as bounding boxes, class labels, and class labels are used to train a neural net to generate an image. The proposed approach is based on task-specific data augmentations, and it is shown to outperform existing detection algorithms on the COCO dataset. ","This paper proposes Pix2Seq for object detection, a language modeling task where prior knowledge is used to guide the task. The proposed approach is based on task-specific data augmentations. The approach is evaluated on the COCO dataset and it outperforms other detection algorithms on the same task. Object descriptions such as bounding boxes, class labels, and class labels are used to train the neural net to generate the image. "
7700,SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"Deep vision models PART-OF visual reinforcement learning ( RL ). visual reinforcement learning ( RL ) USED-FOR policy networks. Deep vision models USED-FOR policy networks. hierarchical reasoning PART-OF stage - wise approach. geometric and numerical symbols CONJUNCTION operators. operators CONJUNCTION geometric and numerical symbols. approach USED-FOR policy network. approach USED-FOR interpretable symbolic policy. policy network USED-FOR interpretable symbolic policy. geometric and numerical symbols PART-OF interpretable symbolic policy. operators PART-OF interpretable symbolic policy. policy regression algorithm USED-FOR symbolic rules. RoundTourMix HYPONYM-OF policy regression algorithm. distilled symbolic policy COMPARE CNN based RL agents. CNN based RL agents COMPARE distilled symbolic policy. symbolic distillation approach USED-FOR CNN policy. policy distillation USED-FOR geometric relations. numerical state USED-FOR Detected bounding box Velocity. CNN policy network knowledge USED-FOR symbolic policy. Pong CONJUNCTION CircusCharlie. CircusCharlie CONJUNCTION Pong. Airstriker - Genesis CONJUNCTION Pong. Pong CONJUNCTION Airstriker - Genesis. CircusCharlie CONJUNCTION Seaquest. Seaquest CONJUNCTION CircusCharlie. Generic is policies. OtherScientificTerm are interpretability, input distribution shifts, and teacher - student. Method are end - to - end learning pipeline, and symbolic distillation. ","This paper proposes a new approach to train policy networks using Deep vision models in visual reinforcement learning (RL). The approach is based on hierarchical reasoning, where the goal is to learn an interpretable symbolic policy with geometric and numerical symbols and operators. The key idea is to distill the interpretability of the policy network into a policy network that can be used to train a policy. The authors propose a policy regression algorithm called RoundTourMix, which distills the symbolic rules into a set of symbolic rules. They show that the distilled symbolic policy outperforms CNN based RL agents in terms of interpretability. ","The paper proposes a novel approach to learn an interpretable symbolic policy from a set of geometric and numerical symbols. The approach is based on Deep vision models in visual reinforcement learning (RL). The key idea is to learn a policy network that is able to interpretable from the input distribution shifts. This is achieved by hierarchical reasoning. The authors propose a novel end-to-end learning pipeline, where the teacher-student learns the interpretability of the learned policies. The proposed symbolic distillation approach uses the CNN policy network knowledge to learn the geometric relations between the operators and the operators. The symbolic rules are learned using a policy regression algorithm called RoundTourMix, which is a variant of the proposed symbolic rules.    The authors show that the proposed method can learn a symbolic policy that is interpretable. The paper also shows that the learned symbolic relations are interpretable in a numerical state. "
7725,SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"coarse - level object arrangements ( posture ) CONJUNCTION fine - grained level styling ( identity ). fine - grained level styling ( identity ) CONJUNCTION coarse - level object arrangements ( posture ). exemplar sources USED-FOR fine - grained level styling ( identity ). techniques PART-OF StyleGAN2. techniques USED-FOR PIVQGAN. generator USED-FOR pose - identity disentanglement. VQSN module USED-FOR shaping and composition information. GANInversion encoder CONJUNCTION generator. generator CONJUNCTION GANInversion encoder. self - supervision methods USED-FOR GANInversion encoder. joint - training scheme USED-FOR generator. joint - training scheme CONJUNCTION self - supervision methods. self - supervision methods CONJUNCTION joint - training scheme. joint - training scheme USED-FOR GANInversion encoder. self - supervision methods USED-FOR generator. one CONJUNCTION other. other CONJUNCTION one. other USED-FOR identity. one USED-FOR pose. one HYPONYM-OF ones. training scheme USED-FOR VQSN module. VQSN module USED-FOR pose - related representations. VQSN module CONJUNCTION training scheme. training scheme CONJUNCTION VQSN module. synthesis image quality EVALUATE-FOR model. disentangling scores EVALUATE-FOR model. synthesis image quality CONJUNCTION disentangling scores. disentangling scores CONJUNCTION synthesis image quality. latent - space reducing feature USED-FOR VQSN module. posture - identity disentangling FEATURE-OF model applications. VQSN module USED-FOR model applications. latent - space reducing feature USED-FOR model applications. PIVQGAN USED-FOR Unsupervised image - to - image translation. disentangled posture and identity control USED-FOR PIVQGAN. PIVQGAN USED-FOR segmentation - like ” masks. Task is image - to - image translation task. Material are training - set images, pose images, and referential identity images. ","This paper proposes a new unsupervised image-to-image translation task called PIVQGAN. StyleGAN2.0 is a combination of StyleGAN and GANInversion encoders, where the GAN inversion encoder is trained with a joint-training scheme and self-supervision methods, and the generator is used for pose-identity disentanglement and fine-grained level styling (identity) from exemplar sources. The VQSN module is used to extract shaping and composition information from the training-set images. The latent-space reducing feature is also used to improve the model applications such as posture-identification disentangling, synthesis image quality, and disentangled scores.    The authors show that the PivQGAN can achieve state-of-the-art performance on Unsupervised Image-to -image translation. The authors also demonstrate that the segmentation-like“masks” masks generated by PIVZGAN can be used to learn the identity of the pose. ","This paper presents StyleGAN2.2, a new method for image-to-image translation task. The main idea is to learn coarse-level object arrangements (posture) and fine-grained level styling (identity) from exemplar sources. The authors propose two techniques, StyleGANGAN2 and PIVQGAN, to achieve this goal. The first technique is a GANInversion encoder, and the second one is a joint-training scheme with self-supervision methods to train the generator and the generator for pose-identity disentanglement. The VQSN module is used to capture shaping and composition information, and a latent-space reducing feature is used for model applications such as synthesis image quality, disentangling scores, and disentangled posture and identity control. Experiments show that the proposed method is able to achieve state-of-the-art performance on Unsupervised Image-To-Image translation and segmentation-like “masking” masks. "
7750,SP:e51a7f45493064972585109f203a867e9828eb15,"speech synthesis CONJUNCTION speech enhancement. speech enhancement CONJUNCTION speech synthesis. speech recognition CONJUNCTION speech synthesis. speech synthesis CONJUNCTION speech recognition. Transformers USED-FOR speech processing tasks. speech enhancement HYPONYM-OF speech processing tasks. speech recognition HYPONYM-OF speech processing tasks. speech synthesis HYPONYM-OF speech processing tasks. models USED-FOR speech related tasks. speech - MLP HYPONYM-OF multi - layer perceptron ( MLP ) architecture. speech - MLP USED-FOR multiscale local temporal dependency. keyword spotting CONJUNCTION speech enhancement. speech enhancement CONJUNCTION keyword spotting. keyword spotting EVALUATE-FOR model. speech enhancement EVALUATE-FOR model. tasks EVALUATE-FOR model. speech enhancement HYPONYM-OF tasks. keyword spotting HYPONYM-OF tasks. Google speech command V2 - 35 CONJUNCTION LibriWords. LibriWords CONJUNCTION Google speech command V2 - 35. dataset ( VoiceBank ) USED-FOR speech enhancement. benchmark datasets USED-FOR keyword spotting. benchmark datasets EVALUATE-FOR speech enhancement. benchmark datasets CONJUNCTION dataset ( VoiceBank ). dataset ( VoiceBank ) CONJUNCTION benchmark datasets. Google speech command V2 - 35 HYPONYM-OF benchmark datasets. LibriWords HYPONYM-OF benchmark datasets. speech - MLP COMPARE transformer - based solutions. transformer - based solutions COMPARE speech - MLP. Material is speech signals. OtherScientificTerm are feature channels, contextual window sizes, and resource - constrained scenarios. Generic is chunks. Metric is GFLOPS. Method is transformers. ","This paper proposes a new multi-layer perceptron (MLP) architecture called speech-MLP, which is based on the idea of multiscale local temporal dependency. The proposed model is able to handle a variety of speech processing tasks such as speech synthesis, speech enhancement, speech recognition, and speech enhancement. The model is evaluated on three benchmark datasets: Google speech command V2-35, LibriWords, and a dataset (VoiceBank) for speech enhancement and keyword spotting. The authors show that the proposed model can achieve better performance on these tasks than transformer-based solutions. ","This paper proposes a multi-layer perceptron (MLP) architecture, called speech-MLP, which is an extension of Transformers for speech processing tasks such as speech synthesis, speech enhancement, speech recognition, and speech synthesis. The proposed model is evaluated on a variety of tasks including keyword spotting and speech enhancement. The main contribution of the paper is to introduce a multiscale local temporal dependency between the feature channels and the contextual window sizes. The authors show that the proposed model can achieve better performance than transformer-based solutions in terms of GFLOPS. "
7775,SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"labeled data USED-FOR massive models. data collection CONJUNCTION labeling. labeling CONJUNCTION data collection. generalization error EVALUATE-FOR transfer learning algorithm. transfer learning algorithm USED-FOR lower bound. generalization error EVALUATE-FOR lower bound. computational complexity EVALUATE-FOR transfer learning algorithm. it USED-FOR source / target data distributions. source domains USED-FOR knowledge transfer. bounds USED-FOR setting. bounds USED-FOR generalization error. real image classification CONJUNCTION action recognition data sets. action recognition data sets CONJUNCTION real image classification. lower bounds COMPARE upper - bounds. upper - bounds COMPARE lower bounds. lower bounds COMPARE transfer learning base - lines. transfer learning base - lines COMPARE lower bounds. transfer learning base - lines USED-FOR upper - bounds. source(s ) and target data sets USED-FOR weighted empirical risk minimization. weighted empirical risk minimization USED-FOR upper - bounds. weighted empirical risk minimization USED-FOR transfer learning base - lines. Task are machine learning, and binary classification problems. Method is Transfer learning. Material are labeled training data, and real world data sets. ","This paper proposes a new lower bound on the generalization error of a transfer learning algorithm with respect to the number of labeled training data for massive models trained on labeled data. The lower bound is based on the computational complexity of the transfer learning algorithms. The authors show that it is possible to learn the source/target data distributions of the source domains for knowledge transfer in the setting where the data collection and labeling are done in parallel. The upper-bounds are based on weighted empirical risk minimization on the source(s) and target data sets, and the lower bounds are derived from transfer learning base-lines.","This paper proposes a new lower bound on the generalization error of a transfer learning algorithm for massive models trained on labeled data. The authors show that the lower bound is a function of the computational complexity of the proposed lower bound. They also show that it can be extended to the source/target data distributions. They provide bounds for the setting where the source domains are used for knowledge transfer and the target domain is used for data collection and labeling. Finally, the authors provide upper-bounds based on weighted empirical risk minimization on both source(s) and target data sets. "
7800,SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"probabilistic shape completion method USED-FOR continuous geometry of large - scale 3D scenes. Generative Cellular Automata USED-FOR multi - modal distribution. formulation USED-FOR large - scale continuous geometry. latent code PART-OF sparse voxel embedding. sparse voxel embedding USED-FOR local continuous shape. progressive generation USED-FOR generative model. training objective USED-FOR sparse voxel embedding. variational lower bound FEATURE-OF complete shape distribution. variational lower bound USED-FOR training objective. probabilistic formulation USED-FOR geometry completion. approach COMPARE deterministic models. deterministic models COMPARE approach. Material are Real - world scans of 3D scenes, and missing data. Task is shape completion. Generic is model. ","This paper proposes a probabilistic shape completion method for continuous geometry of large-scale 3D scenes. The formulation is based on Generative Cellular Automata, which is a multi-modal distribution. The key idea is to learn a sparse voxel embedding of the local continuous shape from the latent code of the generative model, and then use progressive generation to generate a complete shape distribution with a variational lower bound. The proposed approach is shown to perform better than deterministic models. ","The paper proposes a probabilistic shape completion method for continuous geometry of large-scale 3D scenes. The formulation is based on Generative Cellular Automata, which is a multi-modal distribution. The key idea is to use sparse voxel embedding of the local continuous shape as a latent code in the latent code of the generative model, and then use progressive generation to generate the complete shape distribution with a variational lower bound. The proposed approach is shown to outperform deterministic models in geometry completion. "
7825,SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"exploration PART-OF deep reinforcement learning. Behavioral priors USED-FOR problem. reduced generality CONJUNCTION restricted transferability. restricted transferability CONJUNCTION reduced generality. exploration USED-FOR reinforcement learning. temporal consistency USED-FOR state - independent temporal priors. probabilistic mixture of policy and temporal prior USED-FOR off - policy reinforcement learning. approach COMPARE baselines. baselines COMPARE approach. long - horizon continuous control tasks EVALUATE-FOR baselines. sparse reward settings USED-FOR baselines. sparse reward settings USED-FOR long - horizon continuous control tasks. long - horizon continuous control tasks EVALUATE-FOR approach. sparse reward settings USED-FOR approach. OtherScientificTerm are temporal priors, and behavioral priors. ","This paper studies the problem of exploration in deep reinforcement learning, where the goal is to learn a policy that is able to transfer from one task to another. Behavioral priors are used to solve this problem, and the authors propose a probabilistic mixture of policy and temporal prior for off-policy reinforcement learning. The authors show that the temporal consistency of state-independent temporal priors leads to reduced generality and restricted transferability. The proposed approach is evaluated on a variety of sparse reward settings and long-horizon continuous control tasks and outperforms baselines.","This paper proposes an extension of exploration in deep reinforcement learning to the problem of learning temporal priors. The authors propose a probabilistic mixture of policy and temporal prior for off-policy reinforcement learning, where the temporal consistency of the state-independent tempor priors is preserved. The proposed approach is evaluated on several long-horizon continuous control tasks with sparse reward settings and compared to baselines. The paper shows that the proposed approach outperforms baselines in terms of reduced generality and restricted transferability. "
7850,SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,stochastic optimization USED-FOR deep neural networks. Learning rate scheduling HYPONYM-OF stochastic optimizers. Adam HYPONYM-OF stochastic optimizers. pre - defined rules USED-FOR scheduling. GNS USED-FOR dynamics. directed graph USED-FOR neural network. agent USED-FOR learning rate. directed graph USED-FOR GNS. GNS USED-FOR agent. reinforcement learning USED-FOR GNS. graph message passing network USED-FOR GNS. reinforcement learning USED-FOR agent. reinforcement learning USED-FOR learning rate. graph message passing network USED-FOR dynamics. scheduler USED-FOR intermediate layer information. reward collection procedure USED-FOR training. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR10 CONJUNCTION GLUE. GLUE CONJUNCTION CIFAR10. Fashion - MNIST CONJUNCTION GLUE. GLUE CONJUNCTION Fashion - MNIST. GLUE USED-FOR language understanding. Fashion - MNIST CONJUNCTION image classification. image classification CONJUNCTION Fashion - MNIST. CIFAR10 USED-FOR image classification. image classification CONJUNCTION GLUE. GLUE CONJUNCTION image classification. Fashion - MNIST HYPONYM-OF benchmarking datasets. CIFAR10 HYPONYM-OF benchmarking datasets. GLUE HYPONYM-OF benchmarking datasets. Fashion - MNIST EVALUATE-FOR framework. GLUE EVALUATE-FOR framework. benchmarking datasets EVALUATE-FOR framework. GNS COMPARE baselines. baselines COMPARE GNS. GNS USED-FOR CNN and Transformer models. baselines USED-FOR CNN and Transformer models. network structures USED-FOR GNS. Method is scheduling mechanism. ,"This paper studies the problem of stochastic optimization in deep neural networks. The authors propose a scheduling mechanism called Learning rate scheduling, which uses pre-defined rules to optimize the learning rate of a neural network using a directed graph. The scheduling mechanism is based on reinforcement learning, where the agent learns a learning rate using reinforcement learning and a graph message passing network to learn the dynamics. The scheduler learns intermediate layer information and the reward collection procedure for training. The proposed framework is evaluated on several benchmarking datasets, including Fashion-MNIST, CIFAR10, GLUE, and image classification. The results show that the proposed GNS outperforms baselines for CNN and Transformer models in terms of learning rate and language understanding.","This paper proposes a new scheduling mechanism for stochastic optimization in deep neural networks. The scheduling mechanism is based on Adam, which is one of the most well-known learning rate scheduling methods. The main idea is to use pre-defined rules for scheduling. The agent learns the learning rate using reinforcement learning and a GNS with a directed graph. The dynamics of the GNS are modeled by a graph message passing network. The scheduler is used to store intermediate layer information. The reward collection procedure is used for training. The framework is evaluated on several benchmarking datasets, including CIFAR10, Fashion-MNIST, GLUE, and image classification for language understanding. GNS outperforms baselines on CNN and Transformer models. "
7875,SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"high - level relational reasoning CONJUNCTION scalable machine intelligence. scalable machine intelligence CONJUNCTION high - level relational reasoning. point cloud USED-FOR high - level relational reasoning. point cloud USED-FOR scalable machine intelligence. point cloud USED-FOR deep object - centric learning. framework USED-FOR 3D point cloud. framework USED-FOR spatial mixture model. Chamfer Mixture Loss PART-OF variational training pipeline. point clouds USED-FOR spatial mixture model. scheme USED-FOR SPAIR3D. unsupervised scene decomposition EVALUATE-FOR method. Method are object - specification scheme, and unsupervised manner. OtherScientificTerm is local voxel grid cell. ",This paper proposes a new object-specification scheme for deep object-centric learning. The proposed framework is based on the Chamfer Mixture Loss in the variational training pipeline. The authors show that the proposed framework can learn a 3D point cloud for high-level relational reasoning and scalable machine intelligence. They also show that their scheme can be applied to SPAIR3D and unsupervised scene decomposition. ,"This paper proposes a new object-specification scheme for deep object-centric learning. The proposed framework is based on a 3D point cloud, which is used for high-level relational reasoning and scalable machine intelligence. The authors propose a Chamfer Mixture Loss in the variational training pipeline. The paper also proposes an unsupervised manner to decompose the scene into a local voxel grid cell. The scheme is applied to SPAIR3D, where it is shown that the proposed scheme is able to achieve state-of-the-art results in terms of scene decomposition."
7900,SP:3c57e921c1bf23e482551ceb71702931a7f07439,"world knowledge USED-FOR interactive environments. large language models ( LLMs ) USED-FOR interactive environments. large language models ( LLMs ) USED-FOR world knowledge. natural language USED-FOR high - level tasks. they USED-FOR high - level tasks. LMs USED-FOR high - level tasks. LMs USED-FOR they. VirtualHome environment EVALUATE-FOR method. method COMPARE LLM baseline. LLM baseline COMPARE method. executability EVALUATE-FOR LLM baseline. VirtualHome environment EVALUATE-FOR LLM baseline. executability EVALUATE-FOR method. executability CONJUNCTION correctness. correctness CONJUNCTION executability. language models1 USED-FOR actionable knowledge. OtherScientificTerm are actionable steps, low - level plans, and admissible actions. Method is LLMs. Generic is procedure. Metric is human evaluation. ","This paper proposes a method to learn actionable knowledge from large language models (LLMs) for interactive environments with natural language. LMs are used to learn high-level tasks from natural language, and they can be used to train LMs to perform actionable steps. The method is evaluated on the VirtualHome environment, where it outperforms the LLM baseline in terms of executability, correctness, and human evaluation. ","This paper proposes a method for learning world knowledge for interactive environments using large language models (LLMs) for high-level tasks in natural language. The method is evaluated on the VirtualHome environment and compared to the LLM baseline. The authors show that they can achieve better executability and correctness compared to LLMs, and that they are more robust to low-level plans than LLMs. The procedure is also evaluated on human evaluation. The main contribution of the paper is the use of language models1 for actionable knowledge. "
7925,SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,geometrical interpretation USED-FOR Variational Autoencoder framework. VAEs USED-FOR Riemannian structure of the learned latent space. vanilla VAE COMPARE VAE models. VAE models COMPARE vanilla VAE. geometrical considerations USED-FOR vanilla VAE. benchmark datasets EVALUATE-FOR VAE models. VAE USED-FOR Riemannian manifold. Riemannian manifold USED-FOR uniform distribution. method USED-FOR deep generative models. low data regime EVALUATE-FOR method. high dimensional data CONJUNCTION low sample sizes. low sample sizes CONJUNCTION high dimensional data. high dimensional data FEATURE-OF complex neuroimaging dataset. low sample sizes FEATURE-OF complex neuroimaging dataset. complex neuroimaging dataset EVALUATE-FOR method. ,"This paper proposes a Variational Autoencoder framework based on geometrical interpretation of the Riemannian structure of the learned latent space. The authors show that vanilla VAE models do not perform as well as the VAEs on the benchmark datasets.  The authors then propose a new VAE that is able to learn a Riemanian manifold for the uniform distribution. The proposed method is shown to perform well on the complex neuroimaging dataset with high dimensional data and low sample sizes. Finally, the authors demonstrate that the proposed method can be applied to deep generative models in the low data regime.","This paper proposes a Variational Autoencoder framework based on the geometrical interpretation of the learned latent space of VAEs. The authors show that vanilla VAE can be seen as a variant of VAE, and that VAE models can be viewed as VAEs that are able to capture the Riemannian structure of the learning latent space.  The authors also show that the VAE is able to represent a uniform distribution of the data, which is not the case for vanilla VAEs, and they show that this is a result of the Geometrical considerations. The proposed method is evaluated on a complex neuroimaging dataset with high dimensional data and low sample sizes. The method is shown to outperform deep generative models in the low data regime. "
7950,SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"natural language processing ( NLP ) CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION natural language processing ( NLP ). computer vision tasks EVALUATE-FOR transformers. natural language processing ( NLP ) EVALUATE-FOR transformers. attention heads USED-FOR applications. redundant heads PART-OF transformers. mixture of keys PART-OF transformer architecture. redundant heads PART-OF transformer architecture. Gaussian mixture model USED-FOR mixtures of keys. Transformer - MGK USED-FOR training. transformer counterpart COMPARE Transformer - MGK. Transformer - MGK COMPARE transformer counterpart. FLOPs USED-FOR Transformer - MGK. accuracy EVALUATE-FOR Transformer - MGK. linear attentions USED-FOR Transformer - MGK. language modeling CONJUNCTION tasks. tasks CONJUNCTION language modeling. applications EVALUATE-FOR Transformer - MGK. tasks HYPONYM-OF applications. tasks EVALUATE-FOR Transformer - MGK. language modeling HYPONYM-OF applications. Wikitext-103 and Long Range Arena benchmark EVALUATE-FOR Transformer - MGKs. Transformer - MGKs COMPARE baseline transformers. baseline transformers COMPARE Transformer - MGKs. Wikitext-103 and Long Range Arena benchmark EVALUATE-FOR baseline transformers. Method are Multi - head attention, and Transformer. OtherScientificTerm are redundant embedding, and attention head. Generic is model. ","This paper proposes a new transformer architecture, Transformer-MGK, that combines redundant heads in the transformer architecture with a mixture of keys in a Gaussian mixture model. Multi-head attention is added to the attention heads to improve the performance of transformers on natural language processing (NLP) and computer vision tasks. The proposed model is evaluated on the Wikitext-103 and Long Range Arena benchmark and shows that the proposed model outperforms the baseline transformers in terms of accuracy. The authors also show that the Transformer - MGK can be used for training with FLOPs. ","This paper proposes a new transformer architecture, Multi-head attention, which combines redundant heads in transformers with attention heads in applications such as natural language processing (NLP) and computer vision tasks. The transformer architecture consists of a mixture of keys in the form of a Gaussian mixture model, where the mixtures of keys are represented by a Transformer with redundant embedding. Transformer-MGK is used for training and for evaluation. The proposed model is evaluated on the Wikitext-103 and Long Range Arena benchmark, where it outperforms the baseline transformers on accuracy and FLOPs. The attention head of the Transformer is a linear attentions, and the attention head is a single attention head. Experiments are conducted on language modeling and tasks."
7975,SP:82731dcce233e748f63382e09b6224a513fe9689,"biological agents USED-FOR Spatial navigation. proprioception CONJUNCTION linear and angular velocity. linear and angular velocity CONJUNCTION proprioception. direct - inverse model of environment dynamics USED-FOR image and action related signals. direct - inverse model of environment dynamics USED-FOR reconstruction of the action. direct - inverse model of environment dynamics USED-FOR two – dimensional continuous environment. Resetting Path Integrator ( RPI ) HYPONYM-OF minimalistic recurrent architecture. RPI USED-FOR internal state. it USED-FOR cognitive map. internal state FEATURE-OF minimal model. architecture COMPARE LSTM networks. LSTM networks COMPARE architecture. architecture USED-FOR internal dynamics. tasks EVALUATE-FOR LSTM networks. tasks EVALUATE-FOR architecture. Generic is models. OtherScientificTerm are image signal, resetting, and integration of past movement. Method is direct - inverse models. ","This paper proposes a new minimalistic recurrent architecture called Resetting Path Integrator (RPI) which is based on a direct-inverse model of environment dynamics for the reconstruction of the action in a two-dimensional continuous environment with proprioception and linear and angular velocity. The authors show that RPI is able to recover the internal state of a minimal model with respect to the image signal, and it is also able to learn a cognitive map of the environment. The proposed architecture is evaluated on a variety of tasks and compared to LSTM networks.",This paper proposes a minimalistic recurrent architecture called Resetting Path Integrator (RPI) which is based on a direct-inverse model of environment dynamics for Spatial navigation with biological agents. The main idea is to use proprioception and linear and angular velocity to model the image and action related signals. The authors show that the proposed architecture is able to capture the internal dynamics of the two-dimensional continuous environment and the reconstruction of the action. They also show that it can capture the cognitive map of the environment and that it is possible to use RPI to learn the internal state of the minimal model. 
8000,SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"features USED-FOR prediction. feature learning USED-FOR neural networks. practical data USED-FOR learning problems. neural networks USED-FOR problems. gradient descent USED-FOR neural networks. linear models USED-FOR data - independent features. polynomial sizes FEATURE-OF linear models. polynomial sizes FEATURE-OF data - independent features. polynomial algorithm PART-OF Statistical Query model. neural networks USED-FOR feature learning. OtherScientificTerm are class relevant patterns, background patterns, and structure of the input distribution. Material is synthetic and real data. ","This paper studies the problem of feature learning in neural networks on practical data. The authors propose a new statistical query model based on a polynomial algorithm. The main idea is to learn a set of class relevant patterns from the input data, and then use gradient descent to train neural networks to solve these problems. The paper shows that linear models can learn data-independent features that are polynomially larger than the polynometric sizes of the linear models.  The paper also shows that the structure of the input distribution can be used to learn the features for prediction. ",This paper proposes a new statistical query model based on a polynomial algorithm. The main idea is to learn a set of class relevant patterns that are independent of the input distribution. These patterns are then used to train a classifier that predicts the class of the data. The authors show that the proposed model is able to predict the class-specific patterns in the input data. 
8025,SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,robustness EVALUATE-FOR machine learning models. machine learning models USED-FOR adversarial examples. robustness FEATURE-OF adversarial examples. test - time adversaries FEATURE-OF adversarial examples. data distribution CONJUNCTION attacker constraints. attacker constraints CONJUNCTION data distribution. lower bounds USED-FOR model. bounds USED-FOR arbitrary classification functions. architectures CONJUNCTION models. models CONJUNCTION architectures. neural networks HYPONYM-OF architectures. neural networks HYPONYM-OF models. methodology USED-FOR robustness. robustness EVALUATE-FOR classifier. methodology USED-FOR fixed feature extractors. robustness EVALUATE-FOR fixed feature extractors. bounds USED-FOR classifier. bounds FEATURE-OF robustness. it USED-FOR classifier. method USED-FOR collisions. closed - form expressions USED-FOR collision finding. bespoke algorithm USED-FOR arbitrary feature extractors. closed - form expressions USED-FOR linear feature extractors. convex program USED-FOR bespoke algorithm. Method is training methods. ,This paper studies the robustness of machine learning models against adversarial examples in the presence of test-time adversaries. The authors provide lower bounds for the model’s robustness against arbitrary classification functions. They also provide a methodology for training fixed feature extractors with closed-form expressions for collision finding. They show that the proposed method is robust to collisions and that it can be used to train a classifier with better robustness than existing training methods. ,"This paper studies the robustness of machine learning models against adversarial examples in the presence of test-time adversaries. The authors provide lower bounds for arbitrary classification functions with respect to the data distribution and the attacker constraints. They also provide a methodology to improve robustness for fixed feature extractors. The proposed method is based on a convex program, and it can be applied to any classifier. The method is shown to be robust to collisions and closed-form expressions for collision finding. "
8050,SP:874b5fa51924cbcceed490d98a0ea80f74586b32,RL USED-FOR real - world problems. Offline reinforcement learning ( RL ) USED-FOR RL. regularization or constraints USED-FOR extrapolation error. regularization or constraints USED-FOR offline RL algorithms. framework USED-FOR V -function. learning procedure PART-OF offline dataset. optimal value learning CONJUNCTION behavior cloning. behavior cloning CONJUNCTION optimal value learning. conservatism FEATURE-OF offline learning. Expectile V -Learning ( EVL ) USED-FOR generalization. implicit planning USED-FOR V -values. offline trajectories USED-FOR implicit planning. Value - based Episodic Memory ( VEM ) HYPONYM-OF offline method. D4RL benchmark EVALUATE-FOR method. sparse - reward tasks HYPONYM-OF tasks. tasks EVALUATE-FOR method. sparse - reward tasks EVALUATE-FOR method. D4RL benchmark EVALUATE-FOR VEM method. OtherScientificTerm is Q - function. ,"Offline reinforcement learning (RL) is an important problem in RL for real-world problems. Offline RL algorithms rely on regularization or constraints to reduce the extrapolation error. The authors propose a framework to learn a V-function from the learning procedure in an offline dataset. The Q-function is learned by implicit planning on the V-values from the offline trajectories. Expectile V-Learning (EVL) is used to improve generalization. The proposed method, Value-based Episodic Memory (VEM), is evaluated on three tasks: optimal value learning, behavior cloning, and sparse-reward tasks.","This paper proposes an offline reinforcement learning (RL) algorithm for real-world problems. Offline RL algorithms are based on regularization or constraints to prevent extrapolation error. The authors propose a framework to learn a V-function that is invariant to the learning procedure in an offline dataset. The Q-function is a function of the Q-value function. The generalization is based on Expectile V-Learning (EVL) and the conservatism of offline learning. The implicit planning of the V-values is done using offline trajectories. The proposed method, Value-based Episodic Memory (VEM), is evaluated on three tasks: optimal value learning, behavior cloning, and sparse-reward tasks. The results show that the proposed method outperforms the D4RL benchmark."
8086,SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"robust accuracy CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robust accuracy. neural network classifiers USED-FOR adversarial perturbations. robustness FEATURE-OF neural network classifiers. adversarial training USED-FOR adversarial perturbations. adversarial training framework USED-FOR robust generalization. importance weight USED-FOR parametric function. bilevel optimization problem USED-FOR weighted adversarial training. approach COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE approach. approach COMPARE techniques. techniques COMPARE approach. clean and robust accuracy EVALUATE-FOR techniques. techniques CONJUNCTION state - of - the - art baselines. state - of - the - art baselines CONJUNCTION techniques. clean and robust accuracy EVALUATE-FOR state - of - the - art baselines. clean and robust accuracy EVALUATE-FOR approach. OtherScientificTerm are class - conditioned margin, and sample ’s multi - class margin. Method are MAML - based approaches, and robust classifier. Generic is upper - level task. ",This paper proposes a new adversarial training framework for robust generalization in the presence of adversarial perturbations in neural network classifiers trained with robustness. The authors propose to use the importance weight of the parametric function of the class-conditioned margin to train the robust classifier. They show that this approach can achieve better clean and robust accuracy compared to state-of-the-art baselines and other MAML-based approaches. They also provide a bilevel optimization problem for weighted adversarial learning.,This paper proposes a new adversarial training framework for robust generalization. The authors propose to use neural network classifiers trained with adversarial perturbations to improve the robustness of the class-conditioned margin. The proposed approach is evaluated on clean and robust accuracy compared to state-of-the-art baselines and compared to other MAML-based approaches. The main contribution of the paper is to use the importance weight as a parametric function to train the robust classifier. The paper also introduces a bilevel optimization problem for weighted adversarial learning. The results show that the proposed approach outperforms other techniques in terms of both the number of samples and the clean-and-robust accuracy. 
8122,SP:3ad36be6b6900aabe43da043461cf178ce977082,"force CONJUNCTION velocity. velocity CONJUNCTION force. position CONJUNCTION force. force CONJUNCTION position. velocity CONJUNCTION spin. spin CONJUNCTION velocity. computational physics CONJUNCTION chemistry. chemistry CONJUNCTION computational physics. spin HYPONYM-OF covariant information. velocity HYPONYM-OF covariant information. position HYPONYM-OF covariant information. force HYPONYM-OF covariant information. vectors HYPONYM-OF covariant information. geometric and physical information USED-FOR message and update functions. steerable MLPs PART-OF model. geometric and physical information PART-OF model. MLPs USED-FOR activation functions. activation functions USED-FOR steerable feature fields. MLPs USED-FOR steerable feature fields. components PART-OF SEGNNs. non - linear message aggregation CONJUNCTION linear ( steerable ) point convolutions. linear ( steerable ) point convolutions CONJUNCTION non - linear message aggregation. invariant messages FEATURE-OF equivariant graph networks. equivariant graph networks USED-FOR steerable messages. non - linear message aggregation HYPONYM-OF components. non - linear message aggregation PART-OF SEGNNs. computational physics CONJUNCTION chemistry. chemistry CONJUNCTION computational physics. chemistry EVALUATE-FOR method. computational physics EVALUATE-FOR method. OtherScientificTerm are node and edge attributes, invariant scalars, and steerable node attributes. Method is equivariant non - linear convolutions. ","This paper proposes a new method for equivariant non-linear convolutions. The proposed model combines geometric and physical information in the message and update functions of steerable MLPs in a model with steerable node and edge attributes. The model is composed of two components: (1) a vector of covariant information (e.g., force, velocity, position) and (2) vector of vector of vectors of the vector of the position. The vector is used to represent the node and edges as invariant scalars. The activation functions are used to encode the steerable feature fields in the MLPs.  The authors show that the proposed method can achieve state-of-the-art performance in terms of computational physics, chemistry, and computational physics. The authors also demonstrate that the equivariance of SEGNNs can be achieved by non-logarithmic non-langevin message aggregation and linear (steerable) point convolutions, and that the invariant message aggregation can be combined with linear (non-linear message aggregation. ","This paper proposes a new model that combines geometric and physical information in the message and update functions of SEGNNs with steerable MLPs. The model consists of two components: non-linear message aggregation and linear (steerable) point convolutions. The authors show that the equivariant nonlinear convolutions are invariant to node and edge attributes, and that the invariant scalars are steerable node attributes. The activation functions for steerable feature fields can be computed using MLPs, and the covariant information can be represented as vectors. The invariant messages can be used to generate steerable messages. The proposed method is evaluated on computational physics and chemistry. "
8158,SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"physics models CONJUNCTION gradient - based learning. gradient - based learning CONJUNCTION physics models. model explicability CONJUNCTION data efficiency. data efficiency CONJUNCTION model explicability. Differentiable physics modeling USED-FOR model explicability. gradient - based learning PART-OF Differentiable physics modeling. physics models PART-OF Differentiable physics modeling. It USED-FOR dynamics. It USED-FOR inverse problems. It USED-FOR design. inverse problems CONJUNCTION design. design CONJUNCTION inverse problems. dynamics CONJUNCTION inverse problems. inverse problems CONJUNCTION dynamics. rigid bodies CONJUNCTION deformable sheets. deformable sheets CONJUNCTION rigid bodies. rigid bodies HYPONYM-OF physics models. deformable sheets HYPONYM-OF physics models. material structures CONJUNCTION force interactions. force interactions CONJUNCTION material structures. Fine - grained models USED-FOR material structures. gradient - based learning USED-FOR Fine - grained models. Fine - grained models USED-FOR force interactions. gradient - based learning USED-FOR force interactions. individual yarn physics CONJUNCTION yarn - to - yarn interactions. yarn - to - yarn interactions CONJUNCTION individual yarn physics. differentiable fabrics model USED-FOR composite materials. cloths HYPONYM-OF composite materials. differentiable forces USED-FOR gradient - based learning. differentiable forces PART-OF empirical physics. forces USED-FOR cloths. complex physical structures CONJUNCTION heterogeneous materials. heterogeneous materials CONJUNCTION complex physical structures. data - efficiency CONJUNCTION high - fidelity. high - fidelity CONJUNCTION data - efficiency. model USED-FOR physical parameters. high - fidelity USED-FOR subtle dynamics. model USED-FOR subtle dynamics. high - fidelity EVALUATE-FOR model. data - efficiency EVALUATE-FOR model. OtherScientificTerm are complex physical phenomena, and granularity of yarns. Material is physical systems. ","This paper studies the problem of differentiable physics modeling with gradient-based learning. Differentiable physics models include physics models such as rigid bodies, deformable sheets, and rigid bodies with force interactions such as individual yarn physics, yarn-to-yarn interactions, and force interactions between material structures. The authors propose a differentiable fabrics model for composite materials such as cloths, where the physical systems are complex physical phenomena and the granularity of yarns can be very differentiable. Fine-grained models are used to model the material structures and the force interactions. It is also used to learn the dynamics and design for inverse problems and inverse problems. The model learns the physical parameters of the physical system, and the model is trained with high-fidelity and high-efficient data-efficiency. The experiments show that the proposed model is able to capture the subtle dynamics and the design. ","This paper presents a theoretical analysis of physics models and gradient-based learning in the context of Differentiable physics modeling. The authors show that physics models, such as rigid bodies, deformable sheets, and deformable bodies, can be seen as physical systems. They also show that Fine-grained models can be used to model material structures and force interactions. They show that differentiable physics models can also be used for model explicability, data efficiency, and data-efficiency. Finally, the authors propose a differentiable fabrics model for composite materials such as cloths. The model is able to capture the physical parameters of the physical system and the dynamics of the system. It is also able to solve inverse problems, design, and inverse problems. Experiments show that the model can capture subtle dynamics and high-fidelity for subtle dynamics. "
8194,SP:2c8358c095b10981d3015b9f6c75765419a9480d,"logical composition USED-FOR framework. logical composition USED-FOR reinforcement learning. OtherScientificTerm are task - specific skill, optimal policy, Boolean expression, unknown distribution, and task distribution. Generic are algorithm, distribution, approach, and tasks. Method are transferred policy, transfer learning, and transfer learning approach. ","This paper proposes a new framework based on logical composition for reinforcement learning based on the assumption that the task-specific skill can be represented as a Boolean expression. The authors propose a transfer learning approach to learn the optimal policy from the unknown distribution, where the goal is to transfer the learned policy to a new task. The proposed algorithm is based on an algorithm that learns the distribution of the new task from the known distribution. The approach is evaluated on a variety of tasks and shows that the proposed approach outperforms other transfer learning approaches.","This paper proposes a new framework based on logical composition for reinforcement learning, where the goal is to learn a task-specific skill. The framework is based on the idea of learning the optimal policy that maximizes the value of the task's specific skill. This is done by learning the distribution of the learned policy. The authors propose a transfer learning approach to learn the distribution. The distribution is learned by learning a Boolean expression of the unknown distribution, and then applying the learned distribution to the task distribution. They show that the proposed algorithm outperforms the state-of-the-art. They also show that their approach can be applied to other tasks. "
8230,SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"machine and deep learning solutions USED-FOR multivariate time series classification ( MTSC ). prediction accuracy EVALUATE-FOR complex models. accuracy EVALUATE-FOR solutions. ROCKET HYPONYM-OF MTSC solution. random convolutional kernels USED-FOR ROCKET. random convolutional kernels USED-FOR MTSC solution. distributed solution USED-FOR MTSC. LightWaveS HYPONYM-OF distributed solution. solution COMPARE deep learning solutions. deep learning solutions COMPARE solution. wavelet scattering transformation CONJUNCTION distributed feature selection. distributed feature selection CONJUNCTION wavelet scattering transformation. distributed feature selection USED-FOR solution. accuracy EVALUATE-FOR deep learning solutions. wavelet scattering transformation USED-FOR solution. wavelet scattering transformation USED-FOR time series. ROCKET features USED-FOR solution. accuracy EVALUATE-FOR solution. nodes CONJUNCTION channels. channels CONJUNCTION nodes. nodes USED-FOR LightWaveS. channels USED-FOR LightWaveS. it USED-FOR MTSC problem. inference speedup CONJUNCTION scalability. scalability CONJUNCTION inference speedup. accuracy CONJUNCTION inference speedup. inference speedup CONJUNCTION accuracy. training time CONJUNCTION accuracy. accuracy CONJUNCTION training time. training time EVALUATE-FOR algorithm. ROCKET USED-FOR inference. speedup EVALUATE-FOR ROCKET. speedup EVALUATE-FOR inference. edge device USED-FOR inference. datasets EVALUATE-FOR speedup. datasets EVALUATE-FOR inference. OtherScientificTerm are real - world environments, and features. Metric are prediction speed, and inference time. Task is training. ","This paper studies the problem of multivariate time series classification (MTSC) in the context of machine and deep learning solutions. The authors propose a distributed solution to the MTSC problem, called LightWaveS, which uses random convolutional kernels instead of the popular ROCKET. The proposed solution is based on wavelet scattering transformation, distributed feature selection, and distributed learning. The paper shows that the proposed solution achieves better prediction accuracy than the state-of-the-art in many real-world environments. ","This paper proposes a new method for multivariate time series classification (MTSC) based on machine and deep learning solutions. The main idea is to use a distributed solution for MTSC, LightWaveS, which is based on random convolutional kernels. The authors show that the proposed solution outperforms deep learning solution in terms of prediction accuracy and scalability. The proposed method is evaluated on two real-world environments, where the prediction speed is measured by the number of samples in the time series, and the prediction accuracy of complex models. The algorithm is shown to outperform the state-of-the-art in both training time and inference time. The speedup of the proposed method, called ROCKET, is shown on two datasets, where it outperforms the state of the art in both inference speedup and speedup on the edge device. The paper also shows that the solution can be combined with distributed feature selection and wavelet scattering transformation for time series. "
8266,SP:db43614ca016280a79448f44a97c81c8ff5ba981,"AMOS USED-FOR text encoders. Mixture Of Signals USED-FOR auxiliary generators. Mixture Of Signals USED-FOR Adversarial learning curriculum. Adversarial learning curriculum USED-FOR text encoders. discriminator USED-FOR replaced tokens. discriminator USED-FOR encoder. encoder USED-FOR replaced tokens. auxiliary masked language models ( MLMs ) USED-FOR replaced tokens. MLMs USED-FOR training signals. mixture weights USED-FOR discriminator loss. gradient PART-OF discriminator. mixture weights USED-FOR auxiliary MLMs ’ outputs. Gumbel - Softmax USED-FOR gradient. MLMs PART-OF unified auxiliary model. AMOS COMPARE pretrained models. pretrained models COMPARE AMOS. AMOS COMPARE ELECTRA. ELECTRA COMPARE AMOS. GLUE and SQuAD benchmarks EVALUATE-FOR BERT base - sized models. ELECTRA COMPARE pretrained models. pretrained models COMPARE ELECTRA. BERT base - sized models EVALUATE-FOR pretrained models. GLUE and SQuAD benchmarks EVALUATE-FOR pretrained models. GLUE and SQuAD benchmarks EVALUATE-FOR AMOS. Method are ELECTRA - style pretraining, and MLM. Metric is pretraining efficiency. ","This paper proposes a novel Adversarial learning curriculum for text encoders based on Mixture Of Signals for auxiliary generators. The proposed AMOS is a unified auxiliary model that combines MLMs in a unified manner. The discriminator is trained to predict the replaced tokens from the encoder, and the discriminator uses a discriminator trained with mixture weights to train an encoder to predict replaced tokens using auxiliary masked language models (MLMs). The authors show that AMOS performs better than pretrained models on GLUE and SQuAD benchmarks compared to ELECTRA and BERT base-sized models in terms of pretraining efficiency.","This paper proposes a new adversarial learning curriculum for text encoders based on Mixture Of Signals for auxiliary generators. The authors propose to use auxiliary masked language models (MLMs) to generate replaced tokens for the encoder and the discriminator for the replaced tokens. The discriminator is based on Gumbel-Softmax, which is a unified auxiliary model that uses MLMs to generate “auxiliary MLMs’ outputs”. The proposed method is evaluated on the GLUE and SQuAD benchmarks and compared to other pretrained models such as ELECTRA and BERT base-sized models. The results show that the proposed method improves the pretraining efficiency."
8302,SP:db3825633ab5d0671340390b23ab655838cc38b2,"pre - trained language models USED-FOR relational knowledge. clozestyle sentence USED-FOR pre - trained language models. clozestyle sentence USED-FOR relational knowledge. language models COMPARE knowledge graphs. knowledge graphs COMPARE language models. precision EVALUATE-FOR language models. adaptive fine - tuning USED-FOR fill - mask task. pre - trained language model USED-FOR fill - mask task. pre - trained language model USED-FOR adaptive fine - tuning. complex prompting techniques CONJUNCTION adaptive fine - tuning. adaptive fine - tuning CONJUNCTION complex prompting techniques. adaptive fine - tuning COMPARE baselines. baselines COMPARE adaptive fine - tuning. transfer learning capabilities FEATURE-OF language model. Task is relational fact extraction task. OtherScientificTerm are knowledge graph facts, and knowledge graph. Generic are model, and approach. Metric is knowledge extraction quality. ","This paper studies the relational fact extraction task, where the goal is to extract relational knowledge from a set of knowledge graph facts. The authors propose to use a clozestyle sentence as a pre-trained language model to learn relational knowledge, and then use adaptive fine-tuning on the fill-mask task to improve the precision of the language models. The model is trained using a combination of complex prompting techniques as well as adaptive fine -tuning. The approach is evaluated on a variety of tasks, and the results show that the proposed language model has better transfer learning capabilities than other baselines.","This paper proposes a new relational fact extraction task, where the goal is to extract relational knowledge from a set of knowledge graph facts. The key idea is to use a clozestyle sentence as a pre-trained language models for relational knowledge. The paper shows that the precision of language models is better than that of knowledge graphs, and that adaptive fine-tuning for the fill-mask task can be done with a pre -trained language model. The authors also show that the model can be trained with complex prompting techniques and adaptive fine -tuning. The main contribution of the paper is the proposed approach to improve the knowledge extraction quality of the language model, which is based on transfer learning capabilities."
8311,SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"multi - relations FEATURE-OF Knowledge bases. symmetry CONJUNCTION inversion. inversion CONJUNCTION symmetry. inversion CONJUNCTION composition. composition CONJUNCTION inversion. symmetry HYPONYM-OF properties. composition HYPONYM-OF properties. inversion HYPONYM-OF properties. Euclidean embedding models USED-FOR properties. hyperbolic space USED-FOR transitivity. representation learning framework USED-FOR relation properties. geometric spaces FEATURE-OF knowledge base embeddings. out - of - taxonomy entity typing task EVALUATE-FOR aligned embeddings. knowledge graph USED-FOR entities. datasets EVALUATE-FOR approach. low dimensions CONJUNCTION small training rates. small training rates CONJUNCTION low dimensions. low dimensions EVALUATE-FOR approach. YAGO3 USED-FOR datasets. small training rates EVALUATE-FOR approach. OtherScientificTerm are Euclidean space, and tree - like properties. Method is manifold alignment. ","This paper studies the problem of multi-relations in Knowledge bases. The authors propose a representation learning framework to learn relation properties such as symmetry, inversion, and composition. The properties are derived from Euclidean embedding models, and the authors show that the hyperbolic space can be used to model the transitivity of the embeddings in geometric spaces. The proposed approach is evaluated on two datasets, YAGO3 and CIFAR-10, and shows that the proposed approach can achieve good performance with low dimensions and small training rates. ","This paper studies the properties of multi-relations of Knowledge bases in the Euclidean space. The authors propose a representation learning framework to learn relation properties in the hyperbolic space. These properties include symmetry, inversion, composition, and inversion inversion. In particular, the authors propose to learn the properties from Euclideans embedding models. The proposed approach is evaluated on two datasets, YAGO3 and CIFAR-10, and shows that aligned embeddings perform better on the out-of-taxonomy entity typing task. "
8320,SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,frequency distribution FEATURE-OF real - world knowledge graphs. approaches USED-FOR static knowledge graphs. one - shot learning framework USED-FOR link prediction. temporal knowledge graphs USED-FOR one - shot learning framework. temporal knowledge graphs USED-FOR link prediction. self - attention mechanism USED-FOR temporal interactions between entities. network USED-FOR similarity score. self - attention mechanism CONJUNCTION network. network CONJUNCTION self - attention mechanism. network USED-FOR method. self - attention mechanism USED-FOR method. algorithm COMPARE baselines. baselines COMPARE algorithm. algorithm USED-FOR sparse relations. Method is low - shot learning methods. Task is temporal settings. OtherScientificTerm is data scarcity. ,"This paper proposes a new one-shot learning framework for link prediction on temporal knowledge graphs with frequency distribution. The proposed method uses a self-attention mechanism and a network to learn temporal interactions between entities. The authors show that the proposed algorithm is able to learn sparse relations in the presence of data scarcity, which is an important problem in many low-shooting learning methods. ",This paper proposes a one-shot learning framework for link prediction on temporal knowledge graphs with frequency distribution. The authors propose to use a self-attention mechanism to learn temporal interactions between entities and a network to compute the similarity score between entities. The proposed algorithm is evaluated on a number of datasets and shows better performance than baselines in terms of sparse relations. 
8336,SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"solver USED-FOR task. neural module USED-FOR solver. module USED-FOR task. module USED-FOR modules. visual reasoning tasks EVALUATE-FOR model. model COMPARE attention - based baseline. attention - based baseline COMPARE model. human judges USED-FOR reasoning process. Generic is tasks. OtherScientificTerm are Lower modules, and forgetting. ",This paper proposes a new solver based on a neural module to solve a given task. The proposed model is evaluated on a variety of visual reasoning tasks. The authors show that the proposed model performs better than the attention-based baseline. The main contribution of the paper is the use of human judges to guide the reasoning process. ,This paper proposes a new model for visual reasoning tasks. The main idea is to use a neural module to learn a solver to solve a given task. The module is then used to learn modules to solve the task. Lower modules are used to train the solver and upper modules to learn the modules. The model is compared to an attention-based baseline and is shown to outperform it. The reasoning process is performed by human judges. The authors also show that the model is able to learn more complex tasks than the original model. 
8345,SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"channel - selectivity FEATURE-OF convolutional layer. Selective Convolutional Unit ( SCU ) HYPONYM-OF architectural unit. parameter efficiency EVALUATE-FOR CNNs. architectural unit USED-FOR CNNs. Selective Convolutional Unit ( SCU ) HYPONYM-OF CNNs. bottlenecks FEATURE-OF CNNs. parameter efficiency EVALUATE-FOR architectural unit. SCU USED-FOR channel - selectivity. SCU USED-FOR training. pruning unimportant channels USED-FOR SCU. SCU - based models COMPARE baselines. baselines COMPARE SCU - based models. model compression EVALUATE-FOR baselines. postprocessing USED-FOR SCU - based models. model compression EVALUATE-FOR SCU - based models. OtherScientificTerm are identity ( e.g., residual ) connection, identity connection, pruned parameters, rewired parameters, and convolutional kernels. Method is deep convolutional neural networks ( CNN ). ","This paper studies the channel-selectivity of a convolutional layer. The authors propose a new architectural unit, Selective Convolutional Unit (SCU), to improve the parameter efficiency of CNNs with bottlenecks. SCU is trained by pruning unimportant channels in training, and the identity (e.g. residual) connection between the pruned parameters and the original parameters is updated during postprocessing. Experiments show that SCU-based models perform better than baselines in terms of model compression. ","This paper proposes a new architectural unit for CNNs with bottlenecks, called Selective Convolutional Unit (SCU). SCU improves channel-selectivity of convolutional layer by pruning the identity (e.g., residual) connection between the pruned parameters and the rewired parameters. SCU is used for training and postprocessing, and is shown to improve parameter efficiency of CNNs. The authors also show that SCU-based models achieve better model compression than baselines. "
8354,SP:2d80fa4bc440061be2234b5070503d3fa056baed,positive data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION positive data. positive data USED-FOR binary classifier. unlabeled data USED-FOR binary classifier. labeled positive data COMPARE unlabeled positive data. unlabeled positive data COMPARE labeled positive data. selection bias FEATURE-OF labeling process. it USED-FOR selection bias. PU learning USED-FOR Bayes optimal classifier. method USED-FOR classifier. algorithm USED-FOR scoring function. algorithm USED-FOR classifier. threshold USED-FOR classifier. method COMPARE methods. methods COMPARE method. methods USED-FOR PU learning. method USED-FOR PU learning. real - world datasets EVALUATE-FOR PU learning. real - world datasets EVALUATE-FOR method. real - world datasets EVALUATE-FOR methods. OtherScientificTerm is class posterior. ,"This paper studies the problem of PU learning for Bayes optimal classifier with positive data and unlabeled data. The authors propose a new algorithm to learn the scoring function for the classifier, which is based on a threshold. They show that the proposed method outperforms existing methods in PU learning on real-world datasets. ","This paper proposes a new method for learning a Bayes optimal classifier from positive data and unlabeled data. The proposed method is based on PU learning, where positive data is used to train a binary classifier, and the labeled positive data are used to improve the performance of the classifier. The authors show that the proposed method outperforms other methods in PU learning on real-world datasets. They also show that it can reduce the selection bias in the labeling process. The paper also proposes an algorithm for scoring function, where the class posterior is computed using a threshold."
8363,SP:5f312626b0613d2e07c59214c5f00db208a98717,"auxiliary losses USED-FOR representations. approach USED-FOR statistical inefficiency. statistical inefficiency FEATURE-OF neural networks. auxiliary losses USED-FOR approach. auxiliary loss USED-FOR main loss. reinforcement learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION reinforcement learning. multi - task supervised learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION multi - task supervised learning. Atari games USED-FOR reinforcement learning. gridworld USED-FOR reinforcement learning. domains EVALUATE-FOR algorithm. ImageNet USED-FOR multi - task supervised learning. multi - task supervised learning HYPONYM-OF domains. reinforcement learning HYPONYM-OF domains. reinforcement learning HYPONYM-OF domains. OtherScientificTerm are auxiliary task, and adaptive weight. ","This paper proposes a new approach to reduce the statistical inefficiency of neural networks by using auxiliary losses to improve the representations. The auxiliary loss is used to replace the main loss in the auxiliary task. The proposed algorithm is evaluated on three domains: multi-task supervised learning, reinforcement learning, and reinforcement learning on Atari games. ","This paper proposes an approach to reduce the statistical inefficiency of neural networks. The main idea is to use auxiliary losses to improve the representations. The auxiliary loss is used as the main loss, while the auxiliary task is used to increase the adaptive weight. The proposed algorithm is evaluated on three domains: reinforcement learning, multi-task supervised learning, and reinforcement learning on Atari games. "
8372,SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"Adversarial examples HYPONYM-OF machine learning models. geometric framework USED-FOR high - dimensional geometry of adversarial examples. manifold reconstruction literature USED-FOR geometric framework. low - dimensional data manifolds PART-OF high - dimensional space. decision boundary USED-FOR low - dimensional data manifold. decision boundary USED-FOR Adversarial examples. nearest neighbor classifiers CONJUNCTION ball - based adversarial training. ball - based adversarial training CONJUNCTION nearest neighbor classifiers. robustness EVALUATE-FOR norms. sufficient sampling conditions USED-FOR nearest neighbor classifiers. sufficient sampling conditions USED-FOR ball - based adversarial training. OtherScientificTerm are misclassifications, codimension, adversarial examples, and manifold. Method is adversarial training. ",This paper proposes a geometric framework for learning the high-dimensional geometry of adversarial examples in machine learning models. Adversarial examples are defined as low-dimensional data manifolds in the manifold reconstruction literature. The decision boundary between the two manifolds is defined as the difference between the nearest neighbor classifiers and the ball-based adversarial training with sufficient sampling conditions.  The authors show that the decision boundary for the low-dimensions of the manifold can be used to learn the misclassifications. The authors also provide a theoretical analysis of the robustness of the norms. ,"This paper proposes a geometric framework for learning the high-dimensional geometry of adversarial examples in machine learning models. Adversarial examples are defined as low-dimensional data manifolds in the manifold reconstruction literature. The decision boundary is defined in the lower-dimensional space, and the misclassifications are computed in the upper-dimension of the manifold. The codimension is defined as the difference between the two representations of the same manifold.  The authors show that the proposed norms are robust to adversarial training with sufficient sampling conditions for nearest neighbor classifiers and ball-based adversarial learning. "
8381,SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"human cognition USED-FOR high - dimensional spaces. interpretable low - dimensional representations USED-FOR areas. representation learning algorithms USED-FOR time series data. interpretable discrete dimensionality reduction CONJUNCTION deep generative modeling. deep generative modeling CONJUNCTION interpretable discrete dimensionality reduction. deep generative modeling USED-FOR representation learning framework. interpretable discrete dimensionality reduction USED-FOR representation learning framework. framework USED-FOR discrete representations of time series. discrete representations of time series USED-FOR smooth and interpretable embeddings. non - differentiability FEATURE-OF discrete representation learning. way USED-FOR non - differentiability. self - organizing map algorithm COMPARE original. original COMPARE self - organizing map algorithm. representation space FEATURE-OF Markov model. Markov model USED-FOR probabilistic interpretation of our method. model USED-FOR natural representation of uncertainty. model USED-FOR temporal transition structure. model USED-FOR clustering. static ( Fashion-)MNIST data CONJUNCTION time series of linearly interpolated ( Fashion-)MNIST images. time series of linearly interpolated ( Fashion-)MNIST images CONJUNCTION static ( Fashion-)MNIST data. clustering CONJUNCTION interpretability. interpretability CONJUNCTION clustering. eICU data set FEATURE-OF real world medical time series application. macro states FEATURE-OF chaotic Lorenz attractor system. clustering EVALUATE-FOR model. real world medical time series application EVALUATE-FOR model. interpretability EVALUATE-FOR model. static ( Fashion-)MNIST data EVALUATE-FOR model. Material are High - dimensional time series, and real world data. OtherScientificTerm is data features. Generic are representation, method, and representations. ","This paper proposes a new representation learning framework based on interpretable discrete dimensionality reduction and deep generative modeling for high-dimensional time series. The authors propose a new Markov model for probabilistic interpretation of our method. The proposed method is based on the self-organizing map algorithm, which is an extension of the original self-organized map algorithm. The main contribution of the paper is the use of discrete representations of time series to learn smooth and interpretable embeddings. The paper also proposes a way to improve the non-differentiability of discrete representation learning in the presence of data features. Experimental results on the eICU data set demonstrate the effectiveness of the proposed model in terms of clustering, interpretability, and clustering performance.","This paper proposes a new representation learning framework based on interpretable discrete dimensionality reduction and deep generative modeling for time series data. The proposed framework is based on discrete representations of time series for smooth and interpretable embeddings. The authors show that the proposed representation learning algorithms can be used to learn discrete representations for high-dimensional time series, while the proposed method can be applied to low-dimensional spaces. The main contribution of the paper is the use of a Markov model for the probabilistic interpretation of our method, which is an extension of the original self-organizing map algorithm. The method is evaluated on the eICU data set for real world medical time series application, which includes static (Fashion-)MNIST data and time series of linearly interpolated (Fabian-MNIST images. The model is shown to be able to learn a natural representation of uncertainty and to learn the temporal transition structure of the macro states of a chaotic Lorenz attractor system. The non-differentiability of discrete representation learning is studied in a different way, and the authors also show that their method is able to achieve better non -differentiability than the original. "
8390,SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"multidimensional probability distributions USED-FOR latent space prior distributions. latent space prior distributions USED-FOR implicit generative models. linear interpolations USED-FOR latent space. random latent vectors USED-FOR decoding linear interpolations. non - linear interpolations USED-FOR distribution mismatch. latent probability distribution USED-FOR distribution mismatch. multidimensional Cauchy distribution USED-FOR prior distribution. OtherScientificTerm are latent distribution, finite mean, and latent distributions. ","This paper proposes a multidimensional probability distributions for latent space prior distributions for implicit generative models. The authors show that linear interpolations on the latent space of random latent vectors can lead to distribution mismatch between the prior distribution and the latent probability distribution of the latent distribution. They also show that non-linear interpolations can be used to reduce the distribution mismatch. Finally, they provide a finite mean for the latent distributions. ","This paper presents multidimensional probability distributions for latent space prior distributions for implicit generative models. The authors show that linear interpolations in the latent space can be decomposed into random latent vectors, which can be used for decoding linear interpolation and non-linear interpolations for distribution mismatch. They also show that the distribution mismatch is due to the latent probability distribution of the latent distribution, which is a finite mean of the two latent distributions. To address this, the authors propose a prior distribution based on the multidsimensional Cauchy distribution. "
8399,SP:19b63ca635712f1509ca6e0141303c192f2709e0,"hyperbolic space FEATURE-OF shallow networks. embeddings USED-FOR ubiquitous attention mechanisms. ubiquitous attention mechanisms USED-FOR neural networks architectures. hyperbolic geometry FEATURE-OF embeddings. hyperbolic geometry COMPARE Euclidean geometry. Euclidean geometry COMPARE hyperbolic geometry. generalization EVALUATE-FOR neural machine translation. WMT’14 FEATURE-OF neural machine translation. learning on graphs EVALUATE-FOR method. synthetic and real - world graph tasks EVALUATE-FOR learning on graphs. visual question answering ( CLEVR ) tasks EVALUATE-FOR method. neural machine translation EVALUATE-FOR method. generalization EVALUATE-FOR method. Generic are approaches, and model. Method are geometry of embedding of object representations, and neural representations. OtherScientificTerm are embedding space, and semantic distance. Material is graphs. ","This paper studies the geometry of embedding of object representations in shallow networks in the hyperbolic space of shallow networks with embeddings in the embedding space of ubiquitous attention mechanisms. The authors propose a new approach to learn representations of objects that are embeddable in the space of the embedded space. The proposed approach is based on the idea that the embeds of objects can be represented as a set of nodes in a graph, and that the semantic distance between nodes in the graph can be computed as a function of the distance between the nodes.  The proposed method is evaluated on a variety of synthetic and real-world graph tasks, and achieves state-of-the-art performance on neural machine translation in WMT’14.","This paper proposes a new approach to learn embeddings for shallow networks in the hyperbolic space. The key idea is to learn the geometry of embedding of object representations in the embedding space, and then use ubiquitous attention mechanisms to train neural networks architectures. The authors propose two approaches to this problem: 1) to learn a model that can learn the embedded space and 2) to use the embeded space to train the neural representations. The proposed method is evaluated on both synthetic and real-world graph tasks, and shows better generalization to neural machine translation on WMT’14. "
8408,SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"attacks USED-FOR architecture information. attacks USED-FOR deep neural networks ( DNN ). architecture information FEATURE-OF deep neural networks ( DNN ). cache side - channels FEATURE-OF DNN fingerprinting attacks. threat model USED-FOR attacks. attack USED-FOR architecture. Flush+Reload HYPONYM-OF cache side - channel technique. DeepRecon HYPONYM-OF attack. cache side - channel technique USED-FOR internal information. Flush+Reload USED-FOR internal information. internal information USED-FOR attack. VGG19 CONJUNCTION ResNet50. ResNet50 CONJUNCTION VGG19. forward propagation USED-FOR complex networks. ResNet50 HYPONYM-OF complex networks. VGG19 HYPONYM-OF complex networks. meta - model USED-FOR pretrained model. transfer learning setting FEATURE-OF pretrained model. empirical security analysis USED-FOR DNNs ’ vulnerability. cache side - channel attacks FEATURE-OF DNNs ’ vulnerability. OtherScientificTerm are black - box networks, shared framework, network architecture, and architecture attributes. Method are victim model, co - located process, deep learning ( DL ) system, framework - level defense techniques, and DNNs. Task is fingerprinting process. ","This paper studies attacks on the architecture information of deep neural networks (DNN). The authors propose a new attack called DeepRecon, which is based on a cache side-channel technique, Flush+Reload, to extract the internal information from the DNN fingerprinting attacks. The authors show that this attack can be used to attack the architecture of a DNN by using a threat model to learn the architecture attributes of the victim model. They also show that the attack can also be used in the transfer learning setting, where a pretrained model is trained using a meta-model. The paper also provides empirical security analysis on DNNs’ vulnerability against cache side -channel attacks.    The paper proposes a new framework-level defense techniques to defend against the fingerprinting process in deep learning (DL) system. The main idea is to train black-box networks with a shared framework, and then use the shared framework to train the network architecture. The proposed framework is then applied to a variety of complex networks, such as VGG19, ResNet50, and ResNet100, and is shown to be robust to forward propagation.","This paper studies attacks on the architecture information of deep neural networks (DNN). The attack is based on the cache side-channels of DNN fingerprinting attacks. The authors propose DeepRecon, an attack based on Flush+Reload to extract the internal information of the black-box networks. The proposed method is evaluated on VGG19 and ResNet50."
8417,SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"representational hierarchy USED-FOR predicting future video frames. spatiotemporal memories PART-OF representational hierarchy. hierarchical network model USED-FOR spatiotemporal memories. Hierarchical Prediction Network ( HPNet ) HYPONYM-OF hierarchical network model. feedforward, feedback and lateral recurrent circuits PART-OF mammalian hierarchical visual system. feedforward, feedback and lateral recurrent circuits USED-FOR model. recurrent connections USED-FOR spatiotemporal memories. feed - forward path CONJUNCTION feedback path. feedback path CONJUNCTION feed - forward path. feed - forward path USED-FOR spatiotemporal features. feed - forward path PART-OF model. feedback path PART-OF model. feed - forward path CONJUNCTION feedback path. feedback path CONJUNCTION feed - forward path. feedback path PART-OF recurrent gated circuit. benchmark datasets EVALUATE-FOR long range video sequence predictions. hierarchical interaction PART-OF network. predictive self - supervised learning USED-FOR representational learning. visual cortex FEATURE-OF representational learning. OtherScientificTerm are hierarchy, internal memory states, prediction errors, frame - to - frame basis, memories of global movement patterns, and early visual cortex. Generic is level. ","This paper proposes a hierarchical network model for predicting future video frames. The model is based on feedforward, feedback and lateral recurrent circuits in a mammalian hierarchical visual system. The spatiotemporal memories in the representational hierarchy are encoded by recurrent connections. The feed-forward path and the feedback path are used to encode spatotemporal features in the model. The hierarchical interaction in the network is also used to learn the internal memory states. The prediction errors are computed on a frame-to-frame basis. The authors show that the proposed model is able to perform long range video sequence predictions on benchmark datasets. ","This paper proposes a hierarchical network model for learning spatiotemporal memories in a representational hierarchy for predicting future video frames. The Hierarchical Prediction Network (HPNet) is a model based on feedforward, feedback and lateral recurrent circuits in a mammalian hierarchical visual system. The model consists of a feed-forward path, a feedback path, and a recurrent gated circuit. The recurrent connections are used to encode spatiotemeporal memories, which are then used to predict future frames in a hierarchical hierarchy. The network is trained using predictive self-supervised learning, which is an extension of representational learning in the visual cortex. The authors show that the network is able to learn the hierarchy of internal memory states, and that the prediction errors can be reduced to a frame-to-frame basis. The paper also shows that the model can learn the spatial features of the input video frames, and the feedback path can be used to learn spatioteemporal features of input frames. Experiments are conducted on several benchmark datasets for long range video sequence predictions, showing that the proposed model can achieve state-of-the-art performance. "
8426,SP:fb74e57f35666742caf651e6da33b5defcf259a8,continuous embeddings USED-FOR kmers. method USED-FOR continuous embeddings. raw RNA - seq data USED-FOR continuous embeddings. raw RNA - seq data USED-FOR kmers. DNA sequence similarity CONJUNCTION DNA sequence abundance. DNA sequence abundance CONJUNCTION DNA sequence similarity. model USED-FOR DNA sequence similarity. model USED-FOR DNA sequence abundance. DNA sequence abundance FEATURE-OF embedding latent space. latent space USED-FOR exon information. them COMPARE known gene sub - structures. known gene sub - structures COMPARE them. acute myeloid leukemia patients FEATURE-OF raw RNA - Seq data. raw RNA - Seq data USED-FOR exon information. latent space USED-FOR detection of genomic abnormalities. visualization CONJUNCTION analysis. analysis CONJUNCTION visualization. representation space USED-FOR visualization. representation space USED-FOR analysis. translocations CONJUNCTION patient - specific mutations. patient - specific mutations CONJUNCTION translocations. patient - specific mutations HYPONYM-OF detection of genomic abnormalities. translocations HYPONYM-OF detection of genomic abnormalities. Generic is vectors. OtherScientificTerm is genomic abnormalities. ,"This paper proposes a method to learn continuous embeddings for kmers from raw RNA-seq data. The authors use a model to learn DNA sequence similarity and DNA sequence abundance in the embedding latent space, and then use the latent space to extract exon information from the raw RNA - Seq data for the classification of kmers in acute myeloid leukemia patients. They show that the learned vectors can be used to detect genomic abnormalities in the presence of translocations and patient-specific mutations. They also provide a visualization and analysis of the learned representation space for the analysis and the detection of genomic abnormalities.","This paper proposes a method for learning continuous embeddings for kmers from raw RNA-seq data. The key idea is to use a model to measure DNA sequence similarity and DNA sequence abundance in the embedding latent space. The authors show that the learned vectors are more similar to known gene sub-structures than to them. They also show that their model is able to measure exon information in the latent space, which can be used for the detection of genomic abnormalities such as translocations and patient-specific mutations. They evaluate their method on a set of acute myeloid leukemia patients. The visualization and analysis are performed in the representation space."
8435,SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"approach USED-FOR model compression. weight or filter space FEATURE-OF network. architecture space USED-FOR approach. 1 - D CNN encoder / decoder USED-FOR mapping. continuous embedding CONJUNCTION back. back CONJUNCTION continuous embedding. embedding USED-FOR parameter count. dataset EVALUATE-FOR architecture. gradient descent USED-FOR compression objective function. accuracy CONJUNCTION parameter count. parameter count CONJUNCTION accuracy. accuracy EVALUATE-FOR compression objective function. parameter count FEATURE-OF compression objective function. continuous space FEATURE-OF gradient descent. gradient descent USED-FOR compression phase. continuous feature USED-FOR discrete architecture. decoder USED-FOR discrete architecture. FMNIST CONJUNCTION SVHN. SVHN CONJUNCTION FMNIST. CIFAR-10/100 CONJUNCTION FMNIST. FMNIST CONJUNCTION CIFAR-10/100. CIFAR-10 EVALUATE-FOR compression. visual recognition tasks EVALUATE-FOR approach. compression EVALUATE-FOR approach. SVHN HYPONYM-OF visual recognition tasks. CIFAR-10/100 HYPONYM-OF visual recognition tasks. FMNIST HYPONYM-OF visual recognition tasks. Method are Architecture Compression, and model compression methods. OtherScientificTerm is discrete architecture space. ","This paper proposes a new approach for model compression based on Architecture Compression. The proposed approach is based on a 1-D CNN encoder/decoder, which maps the network to a weight or filter space. The key idea is to use continuous embedding and back to compute the parameter count of the embedding. The compression objective function is computed by gradient descent in the continuous space of the gradient descent during the compression phase. The authors show that the proposed architecture is able to compress the dataset on CIFAR-10/100, FMNIST, SVHN, and SVNIST on visual recognition tasks. ","This paper proposes a new approach for model compression, called Architecture Compression. The proposed approach is based on the architecture space of the network in the weight or filter space. The mapping is done using a 1-D CNN encoder/decoder, and the parameter count is computed using a continuous embedding and back. The compression phase is performed by gradient descent in the continuous space, where the discrete architecture space is represented by a continuous feature. The authors evaluate the proposed architecture on a dataset of CIFAR-10/100, FMNIST, SVHN, and several visual recognition tasks. They show that the proposed compression objective function has better accuracy and parameter count compared to other model compression methods."
8444,SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"local model - based control CONJUNCTION global value function learning. global value function learning CONJUNCTION local model - based control. global value function learning CONJUNCTION exploration. exploration CONJUNCTION global value function learning. local trajectory optimization USED-FOR value function learning. approximate value functions USED-FOR policies. approximate value functions USED-FOR planning horizon. trajectory optimization USED-FOR temporally coordinated exploration. estimating uncertainty USED-FOR value function approximation. trajectory optimization CONJUNCTION estimating uncertainty. estimating uncertainty CONJUNCTION trajectory optimization. temporally coordinated exploration CONJUNCTION estimating uncertainty. estimating uncertainty CONJUNCTION temporally coordinated exploration. humanoid locomotion CONJUNCTION dexterous in - hand manipulation. dexterous in - hand manipulation CONJUNCTION humanoid locomotion. components USED-FOR control tasks. humanoid locomotion HYPONYM-OF control tasks. dexterous in - hand manipulation HYPONYM-OF control tasks. Method are plan online and learn offline ” framework, and internal model. OtherScientificTerm are value function, and local solutions. ","This paper proposes a new “plan online and learn offline” framework, where the goal is to learn a value function that can be used to guide exploration and local model-based control as well as global value function learning. The key idea is to use approximate value functions to learn policies that cover the planning horizon, and then use trajectory optimization for temporally coordinated exploration and estimating uncertainty for value function approximation. The internal model is trained to predict the value function, and the local solutions are then used to train the internal model. The authors show that the proposed components can be applied to control tasks such as humanoid locomotion and dexterous in-hand manipulation.","The paper proposes a “plan online and learn offline” framework that combines local model-based control, global value function learning, and exploration. The key idea is to use approximate value functions for policies to guide the planning horizon. The value function is learned by estimating uncertainty for the value function approximation, and then using trajectory optimization for temporally coordinated exploration and estimating uncertainty in estimating uncertainty. The authors also propose an internal model to learn local solutions. Experiments are conducted on control tasks such as humanoid locomotion and dexterous in-hand manipulation."
8453,SP:771494fda4702cd8c7efbf225b19028f91b449b9,"parallel data USED-FOR Neural Machine Translation ( NMT ) systems. zero - shot and dual learning PART-OF approach. reinforcement learning USED-FOR duality of the machine translation task. reinforcement learning USED-FOR latter. UN corpus EVALUATE-FOR zero - shot dual system. zero - shot dual system COMPARE NMT system. NMT system COMPARE zero - shot dual system. NMT system USED-FOR zero - shot translation. English - French and English - Spanish USED-FOR zero - shot dual system. SpanishFrench EVALUATE-FOR NMT system. zero - shot dual method COMPARE LSTM - based unsupervised NMT system. LSTM - based unsupervised NMT system COMPARE zero - shot dual method. en− →fr task EVALUATE-FOR LSTM - based unsupervised NMT system. en− →fr task EVALUATE-FOR zero - shot dual method. Material are low - resource languages, monolingual data, and newstest2014. Task are unsupervised and semi - supervised methods, machine translation task, and fr− →en task. ","This paper proposes a novel approach to unsupervised and semi-supervised machine translation (MTT) based on parallel data. The proposed approach is based on zero-shot and dual learning, where the goal is to learn the duality of the machine translation task using reinforcement learning. The authors show that the proposed NMT system is able to achieve better performance on the UN corpus than the standard NMT based on English-French and English-Spanish. The paper also shows that the new method can achieve better results on the en− →fr task than the LSTM-based UNsupervised DMT system. ","This paper proposes a novel approach to learning parallel data for Neural Machine Translation (NNMT) systems. The approach is based on zero-shot and dual learning, where the former is used to learn the duality of the machine translation task, and the latter is learned using reinforcement learning. The authors show that the proposed NMT system is able to learn zero-shoot translation in both low-resource languages (English-French and English-Spanish) as well as monolingual data (French-German). The authors also show that their zero-Shot dual system outperforms the LSTM-based unsupervised and semi-supervised methods on the en− →fr task. The paper is well-written and well-motivated. "
8462,SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"framework USED-FOR Information - Retrieval ( IR ). IRGAN USED-FOR Information - Retrieval ( IR ). framework USED-FOR IRGAN. generator USED-FOR distribution. minimax loss function USED-FOR generator. adversarial fashion USED-FOR models. Method is Generative Adversarial Networks. Material is multiple domains. Generic are task, and model. OtherScientificTerm are conditional probability distribution, adversarial formulation, loss curves, loss functions, and co - training like setup. ","This paper proposes a framework for Information-Retrieval (IRGAN) based on IRGAN. The framework is based on Generative Adversarial Networks (GANs), which can be applied to multiple domains. The main idea is to learn a conditional probability distribution over the input data, and then use a generator to predict the distribution of the distribution. The generator is trained using a minimax loss function, and the loss curves are learned using adversarial formulation. The authors show that the proposed models can be trained in adversarial fashion in a co-training like setup, where each task is represented by a different model.","This paper proposes a framework for Information-Retrieval (IRGAN) based on Generative Adversarial Networks (GANs). IRGAN is an existing framework for IRGAN, where the goal is to learn a conditional probability distribution over multiple domains. The authors propose an adversarial formulation of the distribution, where a generator is trained to predict the distribution over the multiple domains, and a minimax loss function is used to train the generator. The generator is then used to estimate the distribution of the target domain, and the loss curves of the generator are used as input to the model. The models are trained in adversarial fashion in a co-training like setup."
8471,SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"Variational auto - encoders ( VAEs ) USED-FOR approximate inference. approximate inference USED-FOR intractable generative models. representations USED-FOR auxiliary tasks. VAEs USED-FOR latent codes. human interpretation HYPONYM-OF auxiliary tasks. classification HYPONYM-OF auxiliary tasks. variational auto - encoders CONJUNCTION sparse coding. sparse coding CONJUNCTION variational auto - encoders. sparsity FEATURE-OF latent space. latent space FEATURE-OF VAE. Spike and Slab prior distribution USED-FOR latent space. Spike and Slab prior distribution USED-FOR sparsity. evidence lower bound USED-FOR approximate posterior inference. approximate posterior inference COMPARE VAE case. VAE case COMPARE approximate posterior inference. discrete mixture recognition function USED-FOR approximate posterior inference. discrete mixture recognition function USED-FOR evidence lower bound. approach USED-FOR sparse representations. intractable non - linear probabilistic models USED-FOR sparse representations. sparse representations COMPARE VAE representations. VAE representations COMPARE sparse representations. classification accuracy CONJUNCTION robustness. robustness CONJUNCTION classification accuracy. robustness EVALUATE-FOR sparse representations. classification accuracy EVALUATE-FOR sparse representations. sparse elements USED-FOR subjectively understandable sources of variation. OtherScientificTerm are interpretability, and latent dimensions. Material is MNIST. ",This paper studies the problem of approximate inference for intractable generative models with variational auto-encoders (VAEs) for approximate inference in the context of interpretability. The authors propose a new approach to learn sparse representations for sparse representations in the latent space of a VAE by using the Spike and Slab prior distribution to learn the sparsity of a latent space in the VAE. The sparsity is defined as the difference between the number of latent dimensions of the input and the latent code of the latent variable.  The authors show that the approximate posterior inference in this VAE case is more robust than the standard VAE in terms of classification accuracy and robustness. ,"This paper proposes a new approach to approximate inference for intractable generative models. The authors propose variational auto-encoders (VAEs) for approximate inference, which can be used for many auxiliary tasks such as human interpretation, classification, and sparse coding. The main idea is to use a Spike and Slab prior distribution for the latent space of a VAE, where the sparsity of the VAE is proportional to the number of latent dimensions of the input data, and the interpretability of the latent dimensions. The paper shows that the evidence lower bound for approximate posterior inference can be obtained by using a discrete mixture recognition function. The approach is evaluated on a variety of datasets, including MNIST, CIFAR-10, and ImageNet. The results show that the proposed approach is able to approximate sparse representations on the same level of robustness and classification accuracy compared to VAE representations on a number of datasets. "
8480,SP:06a22143186fa2948fbe324ccae96a62ff12064e,non - adversarial feature matching - based approach USED-FOR generative models. pretrained neural networks USED-FOR feature extraction. autoencoders CONJUNCTION ConvNet classifiers. ConvNet classifiers CONJUNCTION autoencoders. pretrained neural networks USED-FOR Generative Feature Matching Networks ( GFMN ). pretrained neural networks USED-FOR approach. ConvNet classifiers HYPONYM-OF pretrained neural networks. autoencoders HYPONYM-OF pretrained neural networks. ImageNet HYPONYM-OF challenging datasets. CIFAR10 CONJUNCTION STL10. STL10 CONJUNCTION CIFAR10. first order statistics USED-FOR approach. pretrained ImageNet classifiers USED-FOR features. challenging benchmarks EVALUATE-FOR approach. CIFAR10 HYPONYM-OF challenging benchmarks. STL10 HYPONYM-OF challenging benchmarks. ,"This paper proposes a non-adversarial feature matching-based approach for training generative models. The approach uses pretrained neural networks, such as autoencoders and ConvNet classifiers, to perform feature extraction. The proposed approach is based on first order statistics, and is evaluated on two challenging benchmarks, CIFAR10 and STL10.","This paper proposes a non-adversarial feature matching-based approach for generative models. The approach is based on pretrained neural networks (i.e., autoencoders and ConvNet classifiers) for feature extraction. The proposed approach is evaluated on three challenging datasets (CIFAR10, STL10, and ImageNet) with first order statistics. "
8489,SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,Graph Neural Networks ( GNNs ) USED-FOR representation learning of graphs. representation vector USED-FOR node. neighborhood aggregation scheme USED-FOR GNNs. node and graph classification tasks EVALUATE-FOR GNN variants. GNNs USED-FOR graph representation learning. GNNs USED-FOR graph structures. theoretical framework USED-FOR graph structures. theoretical framework USED-FOR GNNs. Graph Convolutional Networks CONJUNCTION GraphSAGE. GraphSAGE CONJUNCTION Graph Convolutional Networks. Graph Convolutional Networks HYPONYM-OF GNN variants. GraphSAGE HYPONYM-OF GNN variants. architecture COMPARE WeisfeilerLehman graph isomorphism test. WeisfeilerLehman graph isomorphism test COMPARE architecture. architecture COMPARE GNNs. GNNs COMPARE architecture. graph classification benchmarks EVALUATE-FOR model. Generic is they. ,"Graph Neural Networks (GNNs) are widely used for representation learning of graphs. However, they are not well-studied in practice. This paper proposes a novel neighborhood aggregation scheme for GNNs that can be applied to graph representation learning. The proposed GNN variants, such as Graph Convolutional Networks and GraphSAGE, are shown to perform well on node and graph classification tasks. The theoretical framework also shows that the GNN can be used to learn graph structures. The authors also show that the proposed architecture is able to outperform the WeisfeilerLehman graph isomorphism test.","Graph Neural Networks (GNNs) are used for representation learning of graphs. GNNs can be used for graph representation learning, but they can also be used to learn graph structures. The authors propose a theoretical framework for learning GNN's for graph structures, which is based on the neighborhood aggregation scheme. The proposed GNN variants are Graph Convolutional Networks and GraphSAGE. The architecture is compared to the WeisfeilerLehman graph isomorphism test, and the model is evaluated on several graph classification benchmarks."
8498,SP:51126f2dd37ce57d2614c9044ede1e43627f0829,framework USED-FOR interpretable continual learning ( ICL ). this USED-FOR ICL. ICL idea USED-FOR continual learning approaches. saliency maps USED-FOR metric. average classification accuracy EVALUATE-FOR overall continual learning performance. metric EVALUATE-FOR ICL. overall continual learning performance EVALUATE-FOR ICL. average classification accuracy EVALUATE-FOR ICL. Method is variational continual learning framework. OtherScientificTerm is catastrophic forgetting. ,"This paper proposes a new framework for interpretable continual learning (ICL) based on the variational continual learning framework. The ICL idea is similar to previous continual learning approaches, where the objective is to maximize the average classification accuracy of the ICL. The authors propose a metric based on saliency maps to measure the generalization performance of ICL, and show that ICL achieves the best overall continual learning performance. ","This paper proposes a framework for interpretable continual learning (ICL), which is a variational continual learning framework. ICL is an extension of the ICL idea for continual learning approaches. The authors propose a metric based on saliency maps to measure the generalization ability of ICL. The metric is evaluated on average classification accuracy and overall continual learning performance. "
8507,SP:27a565b3e5442b93d208652784051e640b0c1bfe,"perturbations USED-FOR model. evaluation framework USED-FOR adversarial attacks. adversarial attacks FEATURE-OF seq2seq models. constraints USED-FOR word - based MT systems. human and automatic evaluation EVALUATE-FOR they. adversarial training USED-FOR model. meaning - preserving attacks FEATURE-OF adversarial training. adversarial robustness EVALUATE-FOR model. Material is Adversarial examples. Metric is robustness. OtherScientificTerm are semantics, and meaning preservation. Task is machine translation ( MT ). Generic is methods. ",This paper proposes a new evaluation framework for adversarial attacks on seq2seq models. Adversarial examples are generated by perturbations to the model during adversarial training. The authors show that the robustness of the model against the adversarial robustness against the meaning-preserving attacks can be measured by human and automatic evaluation. The paper also shows that the constraints of word-based MT systems can be used to improve the performance of machine translation (MT). ,"This paper proposes a new evaluation framework for adversarial attacks on seq2seq models. The model is trained with perturbations to the model, and the adversarial examples are used to evaluate the robustness of the model to adversarial training on meaning-preserving attacks. The authors propose two methods to evaluate these methods, one based on human and automatic evaluation and the other based on constraints on word-based MT systems. The latter is based on the idea of meaning preservation. Experiments on machine translation (MT) show that the proposed model achieves better adversarial robustness."
8516,SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,rewards CONJUNCTION inverse ( negative ) rewards. inverse ( negative ) rewards CONJUNCTION rewards. rewards USED-FOR policies. inverse ( negative ) rewards USED-FOR policies. policies COMPARE policies. policies COMPARE policies. policies USED-FOR mis - actions. inverse rewards USED-FOR policies. deep Q - learning CONJUNCTION double Q - learning. double Q - learning CONJUNCTION deep Q - learning. double Q - learning CONJUNCTION on - policy actor - critic. on - policy actor - critic CONJUNCTION double Q - learning. hybrid polices COMPARE algorithms. algorithms COMPARE hybrid polices. rewards FEATURE-OF hybrid polices. on - policy actor - critic USED-FOR hybrid polices. deep Q - learning USED-FOR hybrid polices. double Q - learning USED-FOR hybrid polices. polices COMPARE policies. policies COMPARE polices. Method is reinforcement learning algorithms. OtherScientificTerm is inverse policies. Material is OpenAI gym. ,"This paper studies the problem of learning policies with inverse rewards and inverse (negative) rewards in reinforcement learning algorithms. The authors show that policies with these two rewards are more likely to be mis-actions than policies with policies with the inverse rewards. They also show that such policies can be learned with deep Q-learning and double Q-learned with on-policy actor-critic. They show that these two algorithms can learn hybrid polices with different rewards with the same rewards, and that these policies can learn better policies than policies without the inverse policies. ","This paper proposes a new way to evaluate the performance of reinforcement learning algorithms. The key idea is to use inverse policies and inverse (negative) rewards to evaluate policies. The authors show that policies with inverse rewards are more robust to mis-actions than policies with deep Q-learning, double Q-Learning, and on-policy actor-critic. They also show that the proposed algorithms outperform other existing algorithms in terms of performance on the OpenAI gym. Finally, they show that hybrid polices with inverse policies have better rewards than policies without inverse rewards."
8525,SP:89a732b57934d08b937c93560f391b7758e54f8a,"object parts CONJUNCTION hierarchical structure. hierarchical structure CONJUNCTION object parts. dynamics model USED-FOR object parts. hierarchical, disentangled object representation CONJUNCTION dynamics model. dynamics model CONJUNCTION hierarchical, disentangled object representation. formulation USED-FOR hierarchical, disentangled object representation. formulation USED-FOR dynamics model. unlabeled videos USED-FOR dynamics model. structural descriptor USED-FOR low - level concepts. structural descriptor USED-FOR hierarchical structure. layered image representation USED-FOR object parts. structural descriptor USED-FOR hierarchy. PSD model USED-FOR segmenting object parts. real and synthetic datasets EVALUATE-FOR PSD model. PSD model USED-FOR tasks. PSD model USED-FOR motion distributions. motion distributions HYPONYM-OF tasks. segmenting object parts HYPONYM-OF tasks. OtherScientificTerm is system dynamics. ","This paper proposes a new formulation for learning a hierarchical, disentangled object representation and a dynamics model from unlabeled videos. The object parts are represented as a layered image representation, and the hierarchical structure is represented by a structural descriptor. The PSD model is trained on both real and synthetic datasets, and is shown to perform well on two tasks: motion distributions and segmenting object parts. The structural descriptor is also used to learn low-level concepts such as system dynamics.","This paper proposes a new formulation for learning a hierarchical, disentangled object representation and a dynamics model from unlabeled videos. The object parts are represented as a layered image representation, and the hierarchical structure is modeled as a structural descriptor for low-level concepts. The PSD model is trained on both real and synthetic datasets, and is evaluated on three tasks: segmenting object parts, learning motion distributions, and learning system dynamics."
8534,SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,noisy training datasets USED-FOR deep neural networks. noisy ( incorrect ) class labels PART-OF Large - scale datasets. noisy datasets USED-FOR softmax neural classifier. Deep Determinantal Generative Classifier ( DDGC ) HYPONYM-OF inference method. softmax neural classifier USED-FOR decision boundary. hidden feature spaces PART-OF discriminative deep model. discriminative deep model USED-FOR generative classifier. hidden feature spaces USED-FOR generative classifier. minimum covariance determinant estimator USED-FOR generative classifier. DDGC USED-FOR adversarial perturbations. noisy labels USED-FOR DDGC. noisy labels CONJUNCTION adversarial samples. adversarial samples CONJUNCTION noisy labels. training techniques USED-FOR noisy labels. training techniques USED-FOR adversarial samples. learning models USED-FOR noisy labels. learning models USED-FOR adversarial samples. learning models USED-FOR DDGC. training techniques USED-FOR DDGC. training techniques USED-FOR learning models. test accuracy EVALUATE-FOR deep model. CIFAR10 dataset EVALUATE-FOR deep model. noisy training labels FEATURE-OF CIFAR10 dataset. noise - handling training method USED-FOR deep model. Metric is classification accuracy. OtherScientificTerm is large margin property. ,"This paper proposes a new inference method, Deep Determinantal Generative Classifier (DDGC), which uses noisy training datasets to train deep neural networks with noisy (incorrect) class labels in Large-scale datasets. The main idea is to use a softmax neural classifier trained with noisy datasets to learn the decision boundary of a discriminative deep model over hidden feature spaces in a generative classifier with a minimum covariance determinant estimator. The DDGC is then trained with the noise-handling training method to train a deep model with noisy labels and adversarial samples. Experiments on CIFAR10 dataset show that the proposed deep model achieves better test accuracy than the state-of-the-art in terms of classification accuracy with noisy training labels. ","This paper proposes a new inference method, Deep Determinantal Generative Classifier (DDGC), which uses noisy training datasets for training deep neural networks on noisy (incorrect) class labels in Large-scale datasets. The idea is to train a discriminative deep model on the hidden feature spaces of the generative classifier with a minimum covariance determinant estimator. The decision boundary of the softmax neural classifier is defined by the large margin property. The authors show that DDGC can learn adversarial perturbations on noisy labels and adversarial samples. They also show that the training techniques for learning models to learn noisy labels can be used to train DDGC. The paper also shows that the noise-handling training method can improve the test accuracy of the deep model in the CIFAR10 dataset."
8543,SP:0fa525cc708470b757a60117cb608bb2feaa2c50,approaches USED-FOR Reinforcement Learning ( RL ). huge state spaces CONJUNCTION sparse delayed reward feedback. sparse delayed reward feedback CONJUNCTION huge state spaces. approaches USED-FOR large - scale applications. huge state spaces FEATURE-OF large - scale applications. sparse delayed reward feedback USED-FOR large - scale applications. action selection policies USED-FOR Hierarchical Reinforcement Learning ( HRL ) methods. temporal abstraction FEATURE-OF action selection policies. skill policies USED-FOR subgoals. approaches USED-FOR subgoal discovery. subgoal discovery USED-FOR HRL. approaches USED-FOR HRL. internal reward signal USED-FOR subgoal attainment. internal reward signal USED-FOR skills. intrinsic motivation USED-FOR skills. model - free method USED-FOR subgoal discovery. incremental unsupervised learning USED-FOR model - free method. method USED-FOR subgoals. intrinsic motivation learning mechanism CONJUNCTION method. method CONJUNCTION intrinsic motivation learning mechanism. approach USED-FOR HRL. rooms environment CONJUNCTION ATARI 2600 game. ATARI 2600 game CONJUNCTION rooms environment. sparse delayed feedback FEATURE-OF RL problems. Montezuma ’s Revenge HYPONYM-OF ATARI 2600 game. RL problems EVALUATE-FOR method. ATARI 2600 game HYPONYM-OF RL problems. Montezuma ’s Revenge HYPONYM-OF RL problems. rooms environment HYPONYM-OF RL problems. OtherScientificTerm is Abstraction. Generic is model. ,"This paper proposes Hierarchical Reinforcement Learning (HRL) methods based on action selection policies with sparse delayed reward feedback for large-scale applications such as huge state spaces and sparse delayed feedback in RL. Abstraction is an important problem in HRL and existing approaches for subgoal discovery for HRL are limited in their ability to handle subgoals. The authors propose a model-free method that uses incremental unsupervised learning to learn skills from an internal reward signal to guide subgoal attainment. The skill policies are learned using a temporal abstraction, and the skills are learned via intrinsic motivation learning mechanism. The proposed approach is evaluated on a variety of RL problems such as Montezuma’s Revenge, ATARI 2600 game, and rooms environment.",This paper presents a new approach to Reinforcement Learning (RL) for large-scale applications with huge state spaces and sparse delayed reward feedback. The authors propose Hierarchical Reinforcement learning (HRL) methods based on action selection policies with temporal abstraction. Abstraction is used to encourage subgoals to be learned using skill policies. The key idea is to use an internal reward signal for subgoal attainment and skills. The proposed model is based on incremental unsupervised learning with a model-free method and an intrinsic motivation learning mechanism. Experiments on several RL problems (ATARI 2600 game and Montezuma’s Revenge) demonstrate the effectiveness of the proposed approach for HRL.
8552,SP:e5861538bc8bb9165cb33299bbf12dd875abf976,Representation Learning CONJUNCTION Formal Methods. Formal Methods CONJUNCTION Representation Learning. Neuro - Symbolic Methods HYPONYM-OF Formal Methods. neural framework USED-FOR Circuit Satisfiability problem. model USED-FOR SAT problem. rich embedding architecture USED-FOR problem structure. end - to - end differentiable training procedure USED-FOR Reinforcement Learning. rich embedding architecture CONJUNCTION end - to - end differentiable training procedure. end - to - end differentiable training procedure CONJUNCTION rich embedding architecture. rich embedding architecture USED-FOR framework. end - to - end differentiable training procedure USED-FOR framework. framework COMPARE NeuroSAT method. NeuroSAT method COMPARE framework. out - of - sample generalization EVALUATE-FOR framework. out - of - sample generalization EVALUATE-FOR NeuroSAT method. Method is rich neural architectures. ,This paper proposes a neural framework for the Circuit Satisfiability problem. The model is trained to solve the SAT problem using a rich embedding architecture and an end-to-end differentiable training procedure for Reinforcement Learning. The authors show that the proposed framework achieves better out-of-sample generalization than the NeuroSAT method. ,"This paper proposes a novel neural framework for solving the Circuit Satisfiability problem, which is an important problem in Representation Learning and Formal Methods such as Neuro-Symbolic Methods. The authors propose a model to solve the SAT problem using a rich embedding architecture that is based on the problem structure, and an end-to-end differentiable training procedure for Reinforcement Learning. The proposed framework is evaluated on out-of-sample generalization and the NeuroSAT method is compared to the original framework. "
8561,SP:ff3e5d44619df3825632b0b1a943add081364861,"Deep neuroevolution CONJUNCTION deep reinforcement learning ( deep RL ) algorithms. deep reinforcement learning ( deep RL ) algorithms CONJUNCTION Deep neuroevolution. approaches USED-FOR policy search. deep reinforcement learning ( deep RL ) algorithms USED-FOR policy search. deep reinforcement learning ( deep RL ) algorithms HYPONYM-OF approaches. Deep neuroevolution HYPONYM-OF approaches. Deep neuroevolution USED-FOR policy search. them PART-OF approach. ad hoc evolutionary algorithm CONJUNCTION goal exploration process. goal exploration process CONJUNCTION ad hoc evolutionary algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm HYPONYM-OF sample efficient off - policy deep RL algorithm. goal exploration process CONJUNCTION Deep Deterministic Policy Gradient ( DDPG ) algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm CONJUNCTION goal exploration process. ad hoc evolutionary algorithm CONJUNCTION Deep Deterministic Policy Gradient ( DDPG ) algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm CONJUNCTION ad hoc evolutionary algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm USED-FOR combinations. ad hoc evolutionary algorithm USED-FOR combinations. goal exploration process USED-FOR combinations. cross - entropy method ( CEM ) USED-FOR combination scheme. CEM - RL HYPONYM-OF method. sample efficiency EVALUATE-FOR CEM - RL. Generic are former, latter, and methods. OtherScientificTerm is hyper - parameter setting. Method are off - policy deep RL algorithm, and DDPG. Task is deep RL. ","This paper proposes a sample efficient off-policy deep RL algorithm, called Deep Deterministic Policy Gradient (DDPG) algorithm, which is an extension of the recently proposed sample efficient Off-Policy Deep RL algorithm DDPG. The main idea is to use Deep neuroevolution and deep reinforcement learning (deep RL) algorithms to perform policy search in the hyper-parameter setting. The authors propose to combine them into a single approach, and use them in a combination scheme, where the goal exploration process and goal exploration can be combined with an ad hoc evolutionary algorithm and a Deep deterministic policy Gradient. The proposed method is evaluated on CEM-RL, where it is shown that the sample efficiency of the proposed method improves the performance of CEM - RL. ","This paper presents a new approach to learning a policy search algorithm that combines Deep neuroevolution and deep reinforcement learning (deep RL) algorithms for policy search. The authors propose a sample efficient off-policy deep RL algorithm, called Deep Deterministic Policy Gradient (DDPG) algorithm, which is based on the idea of a hyper-parameter setting. The main difference between the former and the latter is that the former uses a DDPG, while the latter uses an ad hoc evolutionary algorithm and a goal exploration process. The proposed combination scheme is a cross-entropy method (CEM) and the proposed method is evaluated on CEM-RL, where the authors show that the sample efficiency of the proposed CEM improves over existing methods. "
8570,SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,forecasting EVALUATE-FOR model. multivariate time series USED-FOR predictive model. forecasting CONJUNCTION temporal and variable level importance interpretation. temporal and variable level importance interpretation CONJUNCTION forecasting. hidden state matrix CONJUNCTION update process. update process CONJUNCTION hidden state matrix. IMV - LSTM USED-FOR variableswise hidden states. hidden state matrix USED-FOR IMV - LSTM. update process USED-FOR IMV - LSTM. summarization methods USED-FOR temporal and variable importance. mixture attention mechanism CONJUNCTION summarization methods. summarization methods CONJUNCTION mixture attention mechanism. mixture attention mechanism USED-FOR temporal and variable importance. real datasets EVALUATE-FOR IMV - LSTM. IMV - LSTM COMPARE baselines. baselines COMPARE IMV - LSTM. real datasets EVALUATE-FOR baselines. end - to - end framework USED-FOR forecasting. end - to - end framework USED-FOR knowledge extraction. forecasting CONJUNCTION knowledge extraction. knowledge extraction CONJUNCTION forecasting. It USED-FOR knowledge extraction. It USED-FOR forecasting. It USED-FOR end - to - end framework. multi - variate data USED-FOR knowledge extraction. OtherScientificTerm is target and exogenous variables. Generic is it. ,"This paper proposes a new model for predicting the future of multivariate time series. The model is based on the IMV-LSTM, which uses a hidden state matrix and an update process to learn the variableswise hidden states. The authors show that it is able to perform better than existing baselines on both forecasting and knowledge extraction on multi-variate data. They also show that the mixture attention mechanism and summarization methods can be used to learn temporal and variable importance. It is also shown that the end-to-end framework for forecasting can be applied to knowledge extraction. ","This paper proposes a new model for forecasting and knowledge extraction from multivariate time series. The proposed model, IMV-LSTM, uses a mixture attention mechanism and a hidden state matrix to predict variableswise hidden states. The authors show that it is able to predict both target and exogenous variables. It is also able to learn the temporal and variable level importance interpretation and summarization methods. It can also be used as an end-to-end framework for forecasting, knowledge extraction, and multi-variate data. Experiments on several real datasets show that IMV - LSTM outperforms baselines."
8579,SP:1c26660569b579f060f7b4a31e321c6d2356b928,"adversarial examples USED-FOR defenses. defenses USED-FOR adversarial attacks. feature smoothing HYPONYM-OF data augmentation method. feature smoothing USED-FOR neural network. virtual training data USED-FOR neural network. interpolation of features USED-FOR neural network. feature smoothing USED-FOR virtual data points. logit squeezing USED-FOR feature smoothing. adversarial and clean accuracy EVALUATE-FOR feature smoothing. weight decay CONJUNCTION mix up. mix up CONJUNCTION weight decay. logit squeezing CONJUNCTION weight decay. weight decay CONJUNCTION logit squeezing. label smoothing CONJUNCTION logit squeezing. logit squeezing CONJUNCTION label smoothing. mix up CONJUNCTION feature smoothing. feature smoothing CONJUNCTION mix up. weight decay CONJUNCTION feature smoothing. feature smoothing CONJUNCTION weight decay. feature smoothing USED-FOR unbiased estimation of the decision boundary. label smoothing CONJUNCTION weight decay. weight decay CONJUNCTION label smoothing. symmetrical assumptions CONJUNCTION label smoothing. label smoothing CONJUNCTION symmetrical assumptions. estimated variance FEATURE-OF unbiased estimation of the decision boundary. weight decay HYPONYM-OF methods. OtherScientificTerm are computational overhead, computational burden, and decision boundary. Material is MNIST and CIFAR10 datasets. Method is data augmentation methods. Generic is unified framework. ","This paper proposes a new data augmentation method called feature smoothing, which is an extension of the recently proposed weight decay and logit squeezing. The main idea is to use virtual training data to train a neural network with interpolation of features from virtual data points. The authors show that the proposed method can improve the adversarial and clean accuracy of the trained model. The proposed method is evaluated on MNIST and CIFAR10 datasets.","This paper proposes a data augmentation method called feature smoothing, which is an extension of the popular deep learning-based adversarial examples for defenses against adversarial attacks. The main idea is to use virtual training data to train a neural network on the interpolation of features. The authors propose a unified framework where the computational overhead, computational burden, and computational burden of the decision boundary can be reduced. The paper shows that the adversarial and clean accuracy of the proposed features smoothing can be improved by the use of the unified framework. Experiments on MNIST and CIFAR10 datasets show that the proposed methods outperform other methods such as weight decay, logit squeezing, label smoothing and mix up. "
8588,SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"deep convolutional neural network ( DCNN ) HYPONYM-OF deep and locally connected nonlinear network. theoretical framework USED-FOR networks. ReLU nonlinearity FEATURE-OF networks. framework USED-FOR disentangled representations. framework USED-FOR data distribution. gradient descent rules USED-FOR data distribution. Batch Norm HYPONYM-OF regularization techniques. teacher - student setting USED-FOR framework. Gaussian inputs CONJUNCTION independence of activation. independence of activation CONJUNCTION Gaussian inputs. independence of activation HYPONYM-OF unrealistic assumptions. Gaussian inputs HYPONYM-OF unrealistic assumptions. disentangled representations PART-OF deep networks. OtherScientificTerm are projection nature, and teacher ’s computational graph. ","This paper proposes a theoretical framework for learning disentangled representations of a deep convolutional neural network (DCNN) with ReLU nonlinearity. The framework is based on gradient descent rules for learning the data distribution. The authors show that the proposed framework can be applied to any data distribution with Gaussian inputs and independence of activation. The proposed framework is also applicable to the teacher-student setting, where the teacher’s computational graph has a projection nature.  The authors also provide some regularization techniques, such as Batch Norm, that can be used to improve the performance of deep networks.","This paper proposes a deep convolutional neural network (DCNN) which is a deep and locally connected nonlinear network with ReLU nonlinearity. The authors propose a theoretical framework for training networks that are nonlinear in the projection nature. The framework is based on the teacher-student setting, where the teacher’s computational graph is a “student” graph and the data distribution is a set of gradient descent rules. The proposed framework can be used to learn disentangled representations of deep networks with Gaussian inputs and independence of activation. The theoretical framework is tested on a variety of regularization techniques such as Batch Norm."
8597,SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"Prefrontal cortex ( PFC ) USED-FOR behavior repertoire. connectivity CONJUNCTION human behavior formation process. human behavior formation process CONJUNCTION connectivity. Behavioral Module ( BM ) CONJUNCTION end - to - end training strategy. end - to - end training strategy CONJUNCTION Behavioral Module ( BM ). Behavioral Module ( BM ) PART-OF modular architecture of neural networks. end - to - end training strategy PART-OF modular architecture of neural networks. approach USED-FOR learning of behaviors. learning of behaviors CONJUNCTION preferences representation. preferences representation CONJUNCTION learning of behaviors. approach USED-FOR preferences representation. property USED-FOR user modeling. property USED-FOR recommendation tasks. user modeling CONJUNCTION recommendation tasks. recommendation tasks CONJUNCTION user modeling. video games playing EVALUATE-FOR method. independent learning of new behavior patterns USED-FOR network extendability. strategy USED-FOR transfer of newly learned BMs. Task are dialog agents, and personalized representations of different user states. OtherScientificTerm is BMs. ",This paper proposes a new approach to learn a behavior repertoire from the prefrontal cortex (PFC) of dialog agents. The authors propose a modular architecture of neural networks with a Behavioral Module (BM) and an end-to-end training strategy that combines connectivity and the human behavior formation process. The proposed approach combines the learning of behaviors and preferences representation with a new property that can be used for user modeling and recommendation tasks. The paper also proposes a strategy for transfer of newly learned BMs to other BMs by independent learning of new behavior patterns. Experiments on video games playing demonstrate the effectiveness of the proposed method.,"This paper proposes a modular architecture of neural networks with a Behavioral Module (BM) and an end-to-end training strategy for learning of behaviors and the human behavior formation process. The Behavioral Module consists of a prefrontal cortex (PFC) for learning the behavior repertoire, and a dialog agents for learning personalized representations of different user states. The authors show that the proposed approach can be used for learning a preferences representation and a learning of the property for user modeling and recommendation tasks. The proposed method is evaluated on video games playing and is shown to improve network extendability by independent learning of new behavior patterns. The paper also proposes a new strategy for transfer of newly learned BMs."
8606,SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,plastic changes in synaptic connectivity USED-FOR lifelong learning. neuromodulation USED-FOR changes. learning CONJUNCTION adaptation. adaptation CONJUNCTION learning. self - modifying abilities USED-FOR biological reinforcement learning. self - modifying abilities USED-FOR learning. self - modifying abilities USED-FOR adaptation. self - modifying abilities FEATURE-OF brain. brain USED-FOR learning. brain USED-FOR adaptation. neuromodulated plasticity USED-FOR artificial neural networks. gradient descent USED-FOR artificial neural networks. differentiable formulation USED-FOR neuromodulation of plasticity. neuromodulated plasticity USED-FOR neural networks. neuromodulated plasticity USED-FOR reinforcement learning and supervised learning tasks. neural networks USED-FOR reinforcement learning and supervised learning tasks. neuromodulated plastic LSTMs COMPARE LSTMs. LSTMs COMPARE neuromodulated plastic LSTMs. task EVALUATE-FOR LSTMs. task EVALUATE-FOR neuromodulated plastic LSTMs. task EVALUATE-FOR benchmark language modeling task. benchmark language modeling task EVALUATE-FOR neuromodulated plastic LSTMs. benchmark language modeling task EVALUATE-FOR LSTMs. differentiable neuromodulation of plasticity USED-FOR neural networks. Method is differentiable Hebbian plasticity. ,"This paper studies plastic changes in synaptic connectivity in lifelong learning. The authors propose a differentiable Hebbian plasticity, which can be used to model the changes in neuromodulation of plasticity in neural networks. They show that this differentiable formulation can be applied to neural networks trained with gradient descent. They also show that the self-modifying abilities of the brain can be leveraged for learning and adaptation in biological reinforcement learning. Finally, the authors show that neurmodulated plasticity can improve the performance of neural networks on a variety of reinforcement learning and supervised learning tasks. ","This paper proposes a differentiable Hebbian plasticity for lifelong learning, which is based on neuromodulation of synaptic connectivity. The authors show that the changes in synaptic connectivity in the brain can be controlled by self-modifying abilities in biological reinforcement learning and adaptation. They also show that this differentiable formulation of plasticity can be applied to artificial neural networks with gradient descent. They show that neurmodulated plasticity improves the performance of neural networks on a variety of RL tasks and on a benchmark language modeling task. "
8615,SP:1ab5d94d31e99351433436c026799c8aa597bf73,"quantization techniques USED-FOR inference latency / memory consumption. full precision model USED-FOR non - intrusive quantization technique. quantization training process COMPARE training process. training process COMPARE quantization training process. loss function USED-FOR reduced quantization error. binary quantization USED-FOR full precision accuracy. 2 bit quantization USED-FOR full precision accuracy. 1.5 bits hybrid model COMPARE TWN LSTM model. TWN LSTM model COMPARE 1.5 bits hybrid model. WikiText-2 EVALUATE-FOR TWN LSTM model. Method are Deep Neural Networks, and binary model. Generic is techniques. Material are CIFAR dataset, and ImageNet. ","This paper proposes a new quantization technique to reduce the inference latency/memory consumption of Deep Neural Networks. The authors propose a full precision model for non-intrusive quantization, where the loss function is a reduced quantization error. They show that the binary quantization improves the full precision accuracy over 2 bit quantization and the 1.5 bits hybrid model on WikiText-2. The proposed techniques are evaluated on CIFAR dataset and ImageNet. The quantization training process is shown to outperform the training process in terms of accuracy.","This paper proposes a novel quantization techniques for reducing inference latency/memory consumption. The authors propose a non-intrinsic quantization technique that uses a full precision model instead of a binary model. The proposed quantization training process is similar to the training process of Deep Neural Networks. The main difference between the two techniques is that the proposed loss function is a reduced quantization error, while the binary model is a loss function that maximizes the full precision accuracy. Experiments on CIFAR dataset and ImageNet show the proposed 2 bit quantization achieves better precision accuracy compared to the 1.5 bits hybrid model and the standard TWN LSTM model on WikiText-2."
8624,SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"class - irrelevant properties USED-FOR style. method USED-FOR content embedding. deep metric - learning technique USED-FOR method. deep metric - learning technique USED-FOR content embedding. content encoder PART-OF variational autoencoder ( VAE ). content encoder CONJUNCTION to - be - trained style encoder. to - be - trained style encoder CONJUNCTION content encoder. to - be - trained style encoder PART-OF variational autoencoder ( VAE ). auxiliary loss CONJUNCTION leakage filtering. leakage filtering CONJUNCTION auxiliary loss. style information USED-FOR reconstruction. style information PART-OF content representation. auxiliary loss PART-OF method. leakage filtering PART-OF method. content representation USED-FOR style representation. method USED-FOR data - set augmentation. pose CONJUNCTION expression. expression CONJUNCTION pose. expression CONJUNCTION hairstyle. hairstyle CONJUNCTION expression. lighting CONJUNCTION pose. pose CONJUNCTION lighting. decompositions USED-FOR classification. Recombinations USED-FOR creative exercise. Recombinations USED-FOR data set augmentation. approach USED-FOR content - style decomposition and recombination. specific domain knowledge USED-FOR approaches. human body pose HYPONYM-OF specific domain knowledge. leakage filtering USED-FOR STOC. objective PART-OF STOC. leakage filtering HYPONYM-OF objective. supervised training USED-FOR style and content representations. STOC USED-FOR content - style recombination. Material are visual domains, masterworks of art, and Open - Ended Content. Method are domain - independent method, and Decompositions. OtherScientificTerm are VAE reconstruction loss, content, within - class variation, betweenand within - class variation, and musical composition. Task are few - shot learning tasks, face - recognition task, and emotion - recognition task. ","This paper proposes a domain-independent method to learn a style embedding using a deep metric-learning technique. The method combines a content encoder with a variational autoencoder (VAE), a to-be-trained style encoder and an auxiliary loss. The VAE reconstruction loss is used to learn the style information in the reconstruction. The style information is then used in the content representation, and the style representation is used for the content-style decomposition and recombination. Decompositions are used for classification and for learning the composition of the content. Recombinations are also used for data set augmentation for data-set augmentation. The authors show that the proposed STOC is able to achieve state-of-the-art performance on several few-shot learning tasks, including a face-recognition task, and an emotion-recognization task. ","This paper proposes a domain-independent method for learning a style with class-irrelevant properties. The method is based on a deep metric-learning technique for content embedding. The proposed method consists of a content encoder, a variational autoencoder (VAE), a to-be-trained style encoder and an auxiliary loss. The VAE reconstruction loss is used to reconstruct the style from the original content, and the auxiliary loss is applied to the reconstructed style. The style information in the reconstruction is used for the reconstruction of the content representation. Decompositions are used for classification and decompositions for classification. Recombinations are used in the creative exercise for data set augmentation and data-set augmentation for few-shot learning tasks, such as face-recognition task, and emotion-recognization task. The authors show that the proposed approach outperforms existing approaches on content-style decomposition and recombination. "
8633,SP:d37e15cde7765fca87595a242f0a4511b3346d46,method USED-FOR deep reinforcement learning ( deep RL ) training. deep reinforcement learning ( deep RL ) training USED-FOR problems. state - action permissibility ( SAP ) FEATURE-OF problems. permissibility PART-OF SAP. deep RL algorithms USED-FOR state - action exploration. SAP property PART-OF deep RL algorithms. SAP property USED-FOR state - action exploration. SAP guidance USED-FOR training. ,This paper proposes a method for deep reinforcement learning (deep RL) training for problems with state-action permissibility (SAP). The main idea is to use the SAP property in deep RL algorithms to improve the performance of state-actions exploration. The authors propose to use SAP guidance to guide the training during the training. ,This paper proposes a method for deep reinforcement learning (deep RL) training for problems with state-action permissibility (SAP) in the context of reinforcement learning. The authors propose to use the permissible part of SAP to guide the training of deep RL algorithms for state-actions exploration. The main idea is to use SAP guidance to guide training. The proposed method is evaluated on a variety of problems.
8642,SP:20015d8b60e13300586b67c281858cbe28825c48,"random weights USED-FOR weight - tied multilayer vanilla autoencoders. random deep weight - tied autoencoder model USED-FOR approximate inference. deep autoencoders COMPARE shallow counterparts. shallow counterparts COMPARE deep autoencoders. layer - wise pre - training CONJUNCTION batch normalization. batch normalization CONJUNCTION layer - wise pre - training. batch normalization HYPONYM-OF techniques. layer - wise pre - training HYPONYM-OF techniques. tanh activation USED-FOR deep autoencoder. OtherScientificTerm are large dimensions, phase transition phenomena, reversibility, and Lipschitz activations. Task is training initialization practice. Method is analytical techniques. ","This paper studies the problem of training multilayer autoencoders with random weights with large dimensions. The authors propose a random deep weight-tied autoencoder model for approximate inference. They show that the training initialization practice can be improved by adding a layer-wise pre-training, batch normalization, and a tanh activation to the training process. They also provide a theoretical analysis of the phase transition phenomena and the reversibility of the Lipschitz activations. ","This paper proposes a random deep weight-tied autoencoder model for approximate inference with random weights. The authors propose two techniques: layer-wise pre-training and batch normalization. They show that deep autoencoders outperform shallow counterparts in terms of large dimensions, phase transition phenomena, reversibility, and Lipschitz activations. They also provide some analytical techniques to support their claims."
8651,SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"search problem USED-FOR construction of adversarial images. model evaluations USED-FOR sporadic feedback. low frequency component PART-OF discrete cosine transform ( DCT ). iterative principle USED-FOR search strategy. iterative principle USED-FOR algorithm. method USED-FOR targeted and untargeted attacks. query efficiency EVALUATE-FOR method. median queries USED-FOR Google Cloud Vision. algorithm USED-FOR adversarial black - box attacks. PyTorch code USED-FOR it. Generic is model. Task is Model evaluations. Metric is adversarial loss. Material are ResNet-50, and adversarial ImageNet image. ",This paper studies the search problem for the construction of adversarial images from model evaluations. The authors propose a new search strategy based on an iterative principle to find the low frequency component in a discrete cosine transform (DCT). The proposed method is able to find both targeted and untargeted attacks. The proposed algorithm is also able to detect adversarial black-box attacks. Experimental results on ResNet-50 demonstrate the effectiveness of the proposed method. ,This paper proposes a new search problem for the construction of adversarial images. The main idea is to use model evaluations for sporadic feedback. The adversarial loss is defined as the low frequency component of discrete cosine transform (DCT). The search strategy is based on an iterative principle. The proposed method can be applied to both targeted and untargeted attacks. The method is evaluated on Google Cloud Vision with median queries and PyTorch code.
8660,SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"temporal abstractions USED-FOR curse of dimensionality. Hierarchical Reinforcement Learning USED-FOR temporal abstractions. method USED-FOR temporal abstractions. options framework HYPONYM-OF hierarchical framework. heuristics USED-FOR Option discovery. method COMPARE discovering bottlenecks. discovering bottlenecks COMPARE method. method USED-FOR bottlenecks. Successor options HYPONYM-OF model. Successor representations USED-FOR Successor options. Successor representations USED-FOR model. pseudo - reward USED-FOR intra - option policies. primitive actions USED-FOR Successor representations. Incremental Successor options model USED-FOR options. grid worlds CONJUNCTION complex high dimensional environments. complex high dimensional environments CONJUNCTION grid worlds. complex high dimensional environments EVALUATE-FOR approach. Deepmind - Lab HYPONYM-OF complex high dimensional environments. grid worlds EVALUATE-FOR approach. OtherScientificTerm are task - agnostic transferable skills, bottleneck states, landmark ” sub - goals, well connected regions, and sub - goals. Task is discovering bottleneck states. ","This paper proposes a new method for learning temporal abstractions for understanding the curse of dimensionality in Hierarchical Reinforcement Learning. The authors propose a hierarchical framework called options framework, where the goal is to learn task-agnostic transferable skills. Option discovery is done by using heuristics based on the fact that the bottleneck states are not well connected regions, but rather “landmark” sub-goals. The proposed method is able to discover bottlenecks faster than existing methods for discovering bottleneck states. The model is based on Successor options, a model that learns Successor representations for options from primitive actions, and uses a pseudo-reward to learn intra-option policies. The approach is evaluated on grid worlds and complex high dimensional environments such as Deepmind-Lab.","This paper proposes Hierarchical Reinforcement Learning for learning temporal abstractions for solving the curse of dimensionality. The authors propose a hierarchical framework, called options framework, which is based on the idea of task-agnostic transferable skills. Option discovery is done by using heuristics to learn a set of “milestone” sub-goals, which are defined as well connected regions. The method is evaluated on grid worlds and complex high dimensional environments such as Deepmind-Lab. The proposed method is shown to outperform discovering bottlenecks in terms of finding bottleneck states. The model is built on Successor options, which consists of two steps: (1) a model that learns Successor representations for each sub-goal, and (2) a method that learns intra-option policies based on a pseudo-reward. The approach is evaluated in grid worlds, where the authors show that the approach outperforms discovering bottleneck states, and is also shown to perform well in complex high-dimensional environments. "
8669,SP:12a172c1e2892d016b37932acfc48dcb56874a89,"probabilistic distributions USED-FOR domain division. problem USED-FOR recognition tasks. Open Set Learning ( OSL ) HYPONYM-OF recognition tasks. probabilistic way USED-FOR decision boundary. domain division algorithm USED-FOR recognition tasks. domain USED-FOR recognition tasks. bootstrapping CONJUNCTION KolmogorovSmirnov ( K - S ) Test. KolmogorovSmirnov ( K - S ) Test CONJUNCTION bootstrapping. statistical tools USED-FOR decision boundary. KolmogorovSmirnov ( K - S ) Test USED-FOR decision boundary. KolmogorovSmirnov ( K - S ) Test HYPONYM-OF statistical tools. bootstrapping HYPONYM-OF statistical tools. uncertain domain PART-OF framework. OSL and G - ZSL benchmarks EVALUATE-FOR approach. Method are classifiers, and WSVM. OtherScientificTerm is known, unknown and uncertain domains. ","This paper studies the problem of domain division in probabilistic distributions. The authors propose a domain division algorithm for recognition tasks such as Open Set Learning (OSL) and G-ZSL, where the goal is to find the best classifiers for a given set of known, unknown and uncertain domains. The decision boundary is defined in an unsupervised way, and the probabilistically way is used to define the decision boundary. The proposed framework consists of two statistical tools: bootstrapping and KolmogorovSmirnov (K-S) Test. The experimental results show that the proposed approach outperforms the state-of-the-art on both OSL and G -ZSL benchmarks.","This paper proposes a probabilistic way to define a decision boundary between known, unknown and uncertain domains. The domain division algorithm can be applied to a variety of recognition tasks such as Open Set Learning (OSL) and G-ZSL benchmarks. The proposed framework consists of two parts: (1) classifiers, and (2) a WSVM. The decision boundary is defined using statistical tools such as bootstrapping and KolmogorovSmirnov (K-S) Test."
8678,SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"neural network USED-FOR classification and regression. softmax cross - entropy CONJUNCTION mean squared error. mean squared error CONJUNCTION softmax cross - entropy. maximum margin separation CONJUNCTION simplicity ( Occam ’s Razor ). simplicity ( Occam ’s Razor ) CONJUNCTION maximum margin separation. mean squared error HYPONYM-OF solutions. softmax cross - entropy HYPONYM-OF solutions. simplicity ( Occam ’s Razor ) HYPONYM-OF inductive structures. maximum margin separation HYPONYM-OF inductive structures. polar prototype networks HYPONYM-OF networks. polar prototypes USED-FOR structure. maximal separation FEATURE-OF they. angular distances USED-FOR training. training USED-FOR regression. higher - dimensional outputs USED-FOR regression. polar interpolation USED-FOR training. large margin separation CONJUNCTION semantic class structure. semantic class structure CONJUNCTION large margin separation. semantic class structure USED-FOR polar prototype networks. large margin separation USED-FOR polar prototype networks. classification COMPARE network methods. network methods COMPARE classification. regression CONJUNCTION classification. classification CONJUNCTION regression. OtherScientificTerm are layout structures, layout, polar prototype, hypersphere, semantic priors, class prototypes, prototypes, and output dimensions. Task is minimizing angular distances. ","This paper proposes a neural network for classification and regression. The main idea is to learn the layout structures of the neural network by minimizing angular distances between the input and output dimensions. The proposed solutions include softmax cross-entropy, mean squared error, simplicity (Occam’s Razor) and maximum margin separation of inductive structures such as polar prototype networks. The structure of these networks is based on polar prototypes, which are constructed from a set of polar prototypes. The authors show that they have maximal separation between the output dimensions of the prototypes and the output of the network. The paper also shows that training with angular distances can improve the performance of regression and classification with higher-dimensional outputs. Finally, the paper shows that polar interpolation can be used for training with training with large margin separation and semantic class structure in the case of the polar prototype network. ","This paper proposes a novel neural network for classification and regression. The authors propose two inductive structures, namely simplicity (Occam’s Razor) and maximum margin separation, which is a combination of softmax cross-entropy and mean squared error. The proposed networks are based on polar prototype networks, which are a family of networks with polar prototypes. The basic idea is to learn the layout structures of the polar prototype and the corresponding layout of the hypersphere. The layout is learned by minimizing angular distances between the prototypes and their output dimensions. The training is done using polar interpolation, and the training is performed on higher-dimensional outputs for regression and classification.  The authors show that the proposed network methods outperform other network methods in classification, regression, and semantic class structure. "
8687,SP:d1034342785d133cf8372b8624897963cc2ee83a,"Reinforcement learning ( RL ) agents USED-FOR features. reward function USED-FOR features. it EVALUATE-FOR idea. it USED-FOR proof - of - concept environments. proof - of - concept environments EVALUATE-FOR idea. Maximum Causal Entropy IRL USED-FOR algorithm. Generic are preferences, and robot. OtherScientificTerm are implicit preference information, and side effects. ","This paper proposes a new algorithm for learning the preferences of Reinforcement learning (RL) agents. The idea is to use implicit preference information as a reward function to learn features from the reward function. The algorithm is based on Maximum Causal Entropy IRL, and it is shown to work well in proof-of-concept environments. ","This paper proposes an algorithm based on Maximum Causal Entropy IRL. The idea is novel and it is shown to work well in proof-of-concept environments. The key idea is to use implicit preference information as a reward function for features learned by Reinforcement learning (RL) agents. The main idea is that the robot learns to predict the preferences of the environment, and the reward function is a function of the features learned in the environment. "
8696,SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,method USED-FOR dependency structure between latent variables. deep generative models CONJUNCTION probabilistic graphical models. probabilistic graphical models CONJUNCTION deep generative models. deep generative models PART-OF modeling and inference framework. probabilistic graphical models PART-OF modeling and inference framework. latent variable space FEATURE-OF variational autoencoder ( VAE ). flexible dependency structure FEATURE-OF Bayesian network. Bayesian network USED-FOR variational autoencoder ( VAE ). Bayesian network USED-FOR latent variable space. network parameters CONJUNCTION variational parameters. variational parameters CONJUNCTION network parameters. variational parameters CONJUNCTION latent topology. latent topology CONJUNCTION variational parameters. single objective USED-FOR latent topology. single objective USED-FOR variational parameters. single objective USED-FOR network parameters. latent variable values FEATURE-OF top - down and bottom - up reasoning. top - down and bottom - up reasoning USED-FOR Inference. sampling procedure USED-FOR Inference. MNIST CONJUNCTION Omniglot. Omniglot CONJUNCTION MNIST. Omniglot CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Omniglot. Omniglot EVALUATE-FOR framework. CIFAR-10 EVALUATE-FOR framework. MNIST EVALUATE-FOR framework. structured variational autoencoder baselines COMPARE model. model COMPARE structured variational autoencoder baselines. Method is deep latent variable models. OtherScientificTerm is latent variable structures. ,"This paper proposes a method to learn the dependency structure between latent variables between deep generative models and probabilistic graphical models in a modeling and inference framework. The latent variable space of a variational autoencoder (VAE) is modeled as a Bayesian network with a flexible dependency structure. The network parameters and the variational parameters are learned using a single objective. Inference is performed using top-down and bottom-up reasoning based on the latent variable values. The sampling procedure is used for Inference. The proposed framework is evaluated on MNIST, Omniglot, and CIFAR-10. The model is shown to outperform structured variational autooencoders baselines. ","This paper proposes a method to learn the dependency structure between latent variables. The authors propose a modeling and inference framework that combines deep generative models and probabilistic graphical models. The latent variable space of a variational autoencoder (VAE) is modeled as a Bayesian network with flexible dependency structure. The model is evaluated on MNIST, Omniglot, CIFAR-10, and MNIST. The proposed model outperforms structured variational auto-encoder baselines. Inference is performed using top-down and bottom-up reasoning on the latent variable values. The sampling procedure is based on top-downs and bott-ups reasoning. The variational parameters and the network parameters are modeled using a single objective for the latent topology and the variational parameter for the network. "
8705,SP:976dedab53e69610692a563382ada1dbb82c1e9d,"interconnected neurons PART-OF dynamical neural network. numerical solutions USED-FOR mathematical optimization or learning problems. computational properties FEATURE-OF It. it CONJUNCTION massively parallel computer architecture. massively parallel computer architecture CONJUNCTION it. massively parallel computer architecture USED-FOR power and throughput efficiency. local memory HYPONYM-OF local information. dynamical network USED-FOR gradients. top - down feedback CONJUNCTION contrastive learning. contrastive learning CONJUNCTION top - down feedback. dynamical network USED-FOR ` 1 - minimizing dictionary learning problem. top - down feedback USED-FOR dynamical network. contrastive learning USED-FOR dynamical network. gradients USED-FOR learning. spiking neurons USED-FOR dynamical network. OtherScientificTerm is state space. Method are computational system, and learning process. Task is dictionary learning problems. ","This paper proposes a dynamical neural network with interconnected neurons. It combines the computational properties of a massively parallel computer architecture with the power and throughput efficiency of a single dynamical network. The authors show that the dynamical networks can be trained with top-down feedback, contrastive learning, and spiking neurons.  The authors also show that their computational system can be used to solve dictionary learning problems. ",This paper proposes a dynamical neural network with interconnected neurons. It is a massively parallel computer architecture with computational properties similar to numerical solutions for mathematical optimization or learning problems. The main difference is that the computational system does not store local information in the state space but in the local memory. The dynamical network uses top-down feedback and contrastive learning to learn gradients for the learning process. Experiments on dictionary learning problems show that the dynamical networks can solve the `1-minimizing dictionary learning problem with spiking neurons.
8714,SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"spatial pyramid structure CONJUNCTION encoder - decoder structure. encoder - decoder structure CONJUNCTION spatial pyramid structure. semantic image segmentation CONJUNCTION lane detection. lane detection CONJUNCTION semantic image segmentation. spatial pyramid structure USED-FOR nets. encoder - decoder structure FEATURE-OF nets. nets USED-FOR lane detection. nets USED-FOR semantic image segmentation. weak visual appearance CONJUNCTION prior information. prior information CONJUNCTION weak visual appearance. multi - scale context CONJUNCTION pixel - level accuracy. pixel - level accuracy CONJUNCTION multi - scale context. network USED-FOR lane detection. encoder - decoders module USED-FOR lane detection. evaluation methods EVALUATE-FOR lane detection. Method are Convolutional neural networks ( CNNs ), and encoder - decoders nets. Task are lane detection task, and model - based lane detection. Generic is methods. ","This paper studies the problem of model-based lane detection in the presence of weak visual appearance and prior information. The authors propose to use Convolutional neural networks (CNNs) to learn a spatial pyramid structure and an encoder-decoder structure to train the nets for the task of lane detection and semantic image segmentation. The network is then used to train a network for lane detection using the encoder -decoders module. Empirically, the authors show that the proposed evaluation methods can achieve state-of-the-art performance in terms of pixel-level accuracy and multi-scale context. ",This paper proposes a novel approach to the lane detection task. The authors propose to use Convolutional neural networks (CNNs) to learn a spatial pyramid structure and an encoder-decoder structure for the nets for the task of lane detection and semantic image segmentation. The network is then used for lane detection in the multi-scale context and pixel-level accuracy. The paper also proposes a model-based lane detection based on this network. Experiments are conducted on three different evaluation methods for evaluating the performance of the proposed method. 
8723,SP:68b0a10ca06df74612d0753cc3f3ddddde806035,policy COMPARE off - policy training data. off - policy training data COMPARE policy. supervised learning and online learning settings COMPARE batch contextual bandit learning. batch contextual bandit learning COMPARE supervised learning and online learning settings. ad platforms CONJUNCTION recommendation systems. recommendation systems CONJUNCTION ad platforms. batch learning setting USED-FOR online and interactive systems. ad platforms HYPONYM-OF online and interactive systems. recommendation systems HYPONYM-OF online and interactive systems. Policy Optimizer USED-FOR Exponential Models ( POEM ). Inverse Propensity Scoring ( IPS ) CONJUNCTION Policy Optimizer. Policy Optimizer CONJUNCTION Inverse Propensity Scoring ( IPS ). Policy Optimizer HYPONYM-OF approaches. Inverse Propensity Scoring ( IPS ) HYPONYM-OF approaches. inverse propensity weights USED-FOR approaches. Maximum Likelihood Inverse Propensity Scoring ( MLIPS ) USED-FOR batch learning. approach USED-FOR batch learning. logged bandit feedback USED-FOR Maximum Likelihood Inverse Propensity Scoring ( MLIPS ). logged bandit feedback USED-FOR batch learning. historical policy USED-FOR inverse propensity weights. logged action - context pairs USED-FOR maximum likelihood surrogate policy. MLIPS COMPARE IPS. IPS COMPARE MLIPS. nonasymptotic mean squared error EVALUATE-FOR IPS. nonasymptotic mean squared error EVALUATE-FOR MLIPS. surrogate policy COMPARE historical policy. historical policy COMPARE surrogate policy. large - scale ad placement dataset EVALUATE-FOR MLIPS. multi - label classification problems CONJUNCTION large - scale ad placement dataset. large - scale ad placement dataset CONJUNCTION multi - label classification problems. multi - label classification problems EVALUATE-FOR MLIPS. surrogate policy technique COMPARE error reduction techniques. error reduction techniques COMPARE surrogate policy technique. surrogate policy technique USED-FOR approaches. OtherScientificTerm is logged feedback. Metric is mean squared error. ,"This paper proposes a new approach for batch learning based on logged bandit feedback. The proposed approach, Maximum Likelihood Inverse Propensity Scoring (MLIPS), is an extension of Exponential Models (POEM) to the batch contextual bandit learning setting, where the goal is to learn a policy that performs better than off-policy training data. The authors propose two approaches to this problem: Policy Optimizer, which is based on the Inverse propensity weights of the historical policy, as well as a new surrogate policy that uses logged action-context pairs to learn the maximum likelihood surrogate policy. The paper shows that MLIPS achieves better nonasymptotic mean squared error than IPS on multi-label classification problems and large-scale ad placement dataset. The surrogate policy technique is also shown to perform better than other error reduction techniques. ","This paper proposes a new approach for batch learning with logged bandit feedback, Maximum Likelihood Inverse Propensity Scoring (MLIPS) for batch contextual bandit learning in supervised learning and online learning settings. The proposed approach is based on the idea of using off-policy training data to train a policy in a batch learning setting, where the goal is to minimize the mean squared error between the policy and the logits of the action-context pairs. The authors propose two approaches: Policy Optimizer for Exponential Models (POEM) and Inverse Propositions Scoring for the proposed approaches, which are based on inverse propensity weights from historical policy. The results show that the proposed surrogate policy technique outperforms existing error reduction techniques and the historical policy technique in terms of mean squared errors. The paper also shows that MLIPS is nonasymptotic mean squared loss with respect to the logit error, and that it outperforms IPS on multi-label classification problems and large-scale ad placement dataset. "
8732,SP:8e0ed65c5dded23b34798499b2436b24422fd729,learning framework USED-FOR few - shot classification tasks. Meta - learning USED-FOR learning framework. Meta - learning USED-FOR few - shot classification tasks. meta - learner USED-FOR model optimization. parameter initialization CONJUNCTION similarity metric. similarity metric CONJUNCTION parameter initialization. model optimization CONJUNCTION parameter initialization. parameter initialization CONJUNCTION model optimization. meta - learner PART-OF meta - learning methods. individualized feature embedding USED-FOR classifying. feature embedding USED-FOR individualized feature space. kernel generator USED-FOR feature embedding. feature embedding USED-FOR query images. kernel generator USED-FOR meta - learner. meta - knowledge USED-FOR convolutional kernels. kernel generator USED-FOR convolutional kernels. training USED-FOR convolutional kernels. meta - knowledge USED-FOR kernel generator. few - shot classification data sets EVALUATE-FOR method. Omniglot CONJUNCTION miniImageNet. miniImageNet CONJUNCTION Omniglot. miniImageNet EVALUATE-FOR method. miniImageNet HYPONYM-OF few - shot classification data sets. Omniglot HYPONYM-OF few - shot classification data sets. Method is fine - tuning. ,"This paper proposes a new learning framework for few-shot classification tasks based on meta-learning. Meta-learning is a popular learning framework that is used in many meta-training methods. The main idea is to use individualized feature embedding for classifying the query images using a kernel generator, and then use a meta-learner for model optimization, parameter initialization, and similarity metric. The meta-knowledge is then used to train convolutional kernels with the help of the kernel generator. The authors demonstrate the effectiveness of the proposed method on a few-shoot classification data sets such as Omniglot and miniImageNet. ","This paper proposes a new learning framework for few-shot classification tasks. Meta-learning is an extension of meta-learning methods that combine parameter initialization and a similarity metric to improve model optimization. The key idea is to use individualized feature embedding for classifying the query images. The authors propose to use a kernel generator to generate the feature embeddings for each query image, and then use meta-knowledge to train convolutional kernels from the training data. The proposed method is evaluated on a variety of few-shoot classification data sets, including Omniglot and miniImageNet. "
8741,SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,backpropagation HYPONYM-OF gradient - based learning algorithms. gradient - based learning algorithms USED-FOR Deep artificial neural networks ( DNNs ). Q - learning CONJUNCTION policy gradients. policy gradients CONJUNCTION Q - learning. Evolution strategies ( ES ) COMPARE backprop - based algorithms. backprop - based algorithms COMPARE Evolution strategies ( ES ). policy gradients USED-FOR deep reinforcement learning ( RL ) problems. backprop - based algorithms USED-FOR deep reinforcement learning ( RL ) problems. policy gradients HYPONYM-OF backprop - based algorithms. Q - learning HYPONYM-OF backprop - based algorithms. it USED-FOR stochastic gradient descent. ES HYPONYM-OF gradient - based algorithm. finite - difference approximation of the gradient HYPONYM-OF operation. operation USED-FOR it. operation USED-FOR stochastic gradient descent. non - gradient - based evolutionary algorithms USED-FOR DNN scales. Atari CONJUNCTION humanoid locomotion. humanoid locomotion CONJUNCTION Atari. it USED-FOR hard deep RL problems. humanoid locomotion HYPONYM-OF hard deep RL problems. Atari HYPONYM-OF hard deep RL problems. Deep GA USED-FOR networks. free parameters FEATURE-OF networks. evolutionary algorithm USED-FOR neural networks. ES CONJUNCTION GA. GA CONJUNCTION ES. DNNs CONJUNCTION novelty search. novelty search CONJUNCTION DNNs. A3C CONJUNCTION ES. ES CONJUNCTION A3C. novelty search USED-FOR exploration. DQN CONJUNCTION A3C. A3C CONJUNCTION DQN. DNNs USED-FOR high - dimensional problem. reward - maximizing algorithms USED-FOR high - dimensional problem. GA HYPONYM-OF reward - maximizing algorithms. ES HYPONYM-OF reward - maximizing algorithms. DQN HYPONYM-OF reward - maximizing algorithms. A3C HYPONYM-OF reward - maximizing algorithms. A3C CONJUNCTION DQN. DQN CONJUNCTION A3C. ES CONJUNCTION A3C. A3C CONJUNCTION ES. Deep GA,"This paper studies the problem of backpropagation in gradient-based learning algorithms for Deep artificial neural networks (DNNs). The authors propose a new evolutionary algorithm for training neural networks with free parameters. Evolution strategies (ES) outperform backprop-based algorithms such as Q-learning, policy gradients, and policy gradient. The proposed operation is a finite-difference approximation of the gradient, and it allows it to be used for stochastic gradient descent. The authors show that the proposed evolutionary algorithm can be applied to a wide range of deep reinforcement learning (RL) problems such as Atari, humanoid locomotion, DQN, A3C, ES, and Deep GA. They also show that non-gradient-based evolutionary algorithms can be used to improve the DNN scales. ","This paper proposes a new evolutionary algorithm for learning deep neural networks. The main idea is to use a gradient-based learning algorithm, called Evolution strategies (ES), to learn a policy gradient for each layer of the network. The authors show that it is equivalent to stochastic gradient descent, and that it can be combined with a finite-difference approximation of the gradient. The proposed algorithm is evaluated on Atari, humanoid locomotion, and a variety of RL problems. The results show that the proposed algorithm outperforms the state-of-the-art backpropagation algorithms."
8750,SP:dfdbe3267a8160f24746884cdf5297993e424231,"rewards USED-FOR learning. episodic memory USED-FOR novelty bonus. episodic memory USED-FOR curiosity method. VizDoom CONJUNCTION DMLab. DMLab CONJUNCTION VizDoom. DMLab CONJUNCTION MuJoCo. MuJoCo CONJUNCTION DMLab. VizDoom FEATURE-OF visually rich 3D environments. visually rich 3D environments EVALUATE-FOR approach. agent COMPARE curiosity method. curiosity method COMPARE agent. agent COMPARE ICM. ICM COMPARE agent. navigational tasks EVALUATE-FOR agent. curiosity method COMPARE ICM. ICM COMPARE curiosity method. VizDoom CONJUNCTION DMLab. DMLab CONJUNCTION VizDoom. ant USED-FOR MuJoCo. curiosity module PART-OF ant. OtherScientificTerm are Rewards, sparsity, curious behaviour, real task reward, environment dynamics, and first - person - view curiosity. Method are reinforcement learning algorithms, and RL algorithms. ","This paper proposes a new reinforcement learning algorithm based on an episodic memory for learning a novelty bonus. The novelty bonus is based on the fact that the agent can be trained in a variety of visually rich 3D environments such as VizDoom, DMLab, and MuJoCo. The agent is trained on a set of navigational tasks where the environment dynamics are known and the agent is able to adapt to the environment. The paper also proposes a curiosity module in the ant that encourages the agent to explore the environment in a way that encourages sparsity. Experiments show that the proposed agent outperforms the curiosity method and the ICM. ","This paper proposes a novel reinforcement learning algorithms for learning from rewards. The key idea is to use an episodic memory for learning the novelty bonus. The novelty bonus is defined as the sparsity between the reward and the environment dynamics. The agent is trained on visually rich 3D environments (VizDoom, DMLab, and MuJoCo) and evaluated on navigational tasks. Experiments show that the agent outperforms the curiosity method and the ICM in terms of sparsity. The paper also shows that the curiosity module of the ant can be combined with the ant in order to improve the performance. "
8759,SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,representation USED-FOR transition models. complex uncertain domains FEATURE-OF transition models. relational rules USED-FOR representation. iterative greedy algorithm USED-FOR deictic references. Feed - forward neural networks USED-FOR transition distribution. strategy COMPARE monolithic transition model. monolithic transition model COMPARE strategy. simulated domain EVALUATE-FOR monolithic transition model. OtherScientificTerm is rule. ,This paper studies the problem of representation for transition models in complex uncertain domains. The authors propose an iterative greedy algorithm to learn deictic references for each transition distribution using Feed-forward neural networks. The key idea is to use relational rules to learn the representation for each rule. The proposed strategy outperforms a monolithic transition model in the simulated domain.,This paper proposes a novel representation for transition models in complex uncertain domains. The representation is based on relational rules. The authors propose an iterative greedy algorithm to generate deictic references for each rule. Feed-forward neural networks are used to compute the transition distribution. The proposed strategy outperforms a monolithic transition model in a simulated domain.
8768,SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"INVASE HYPONYM-OF instance - wise feature selection method. selector network CONJUNCTION predictor network. predictor network CONJUNCTION selector network. predictor network CONJUNCTION baseline network. baseline network CONJUNCTION predictor network. neural networks CONJUNCTION selector network. selector network CONJUNCTION neural networks. baseline network USED-FOR selector network. actor - critic methodology USED-FOR baseline network. actor - critic methodology USED-FOR INVASE. neural networks PART-OF INVASE. predictor network PART-OF INVASE. selector network PART-OF INVASE. baseline network PART-OF INVASE. methodology USED-FOR INVASE. INVASE COMPARE state - of - the - art benchmarks. state - of - the - art benchmarks COMPARE INVASE. synthetic and real data experiments EVALUATE-FOR INVASE. Material is big data. OtherScientificTerm is features. Task are global feature selection, and instance - wise feature selection. Generic is state - of - the - art methods. ","This paper proposes a new instance-wise feature selection method called INVASE, which is based on the actor-critic methodology. INVASE consists of two components: a selector network and a predictor network. The selector network is trained using the baseline network, and the predictor network is learned using the selector network. Experiments on synthetic and real data experiments show that INVASE outperforms state-of-the-art methods.","This paper proposes a new instance-wise feature selection method called INVASE, which is based on the actor-critic methodology. The key idea of INVASE is to use a baseline network, a predictor network, and a selector network to learn the features. The authors show that INVASE outperforms state-of-the-art methods on both synthetic and real data experiments. "
8777,SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"per - pixel annotations USED-FOR supervised models. semantic segmentation HYPONYM-OF Predicting structured outputs. convolutional neural networks HYPONYM-OF supervised models. per - pixel annotations USED-FOR Predicting structured outputs. annotations USED-FOR model finetuning. disentangled space USED-FOR discriminative feature representations of patches. label histograms USED-FOR discriminative feature representations of patches. adversarial learning scheme USED-FOR feature representations. representations USED-FOR guidance. global alignment process CONJUNCTION patch - level alignment. patch - level alignment CONJUNCTION global alignment process. global alignment process USED-FOR framework. semantic segmentation EVALUATE-FOR framework. patch - level alignment USED-FOR framework. Generic are models, and benchmark datasets. Task is annotation. Method is domain adaptation method. ",This paper proposes a domain adaptation method based on per-pixel annotations for supervised models such as convolutional neural networks. The proposed framework is based on a global alignment process and patch-level alignment. The annotations are used for model finetuning and are used to improve the performance of the models. The authors show that the proposed framework improves the performance in terms of semantic segmentation and predicting structured outputs. ,"This paper proposes a framework for learning per-pixel annotations for supervised models such as semantic segmentation and convolutional neural networks. The framework is based on a global alignment process and patch-level alignment. The annotations are used for model finetuning. The authors propose an adversarial learning scheme to learn discriminative feature representations of patches in the disentangled space. The proposed framework is evaluated on two benchmark datasets, where the authors show that the proposed annotation can improve the performance of the model. "
8786,SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"optimistic algorithms USED-FOR AMSGrad. AMSGrad CONJUNCTION Adam. Adam CONJUNCTION AMSGrad. optimistic algorithms USED-FOR Adam. predictability of gradients USED-FOR optimistic algorithms. momentum method CONJUNCTION adaptive gradient method. adaptive gradient method CONJUNCTION momentum method. algorithms USED-FOR OPTIMISTIC ONLINE LEARNING. adaptive gradient method CONJUNCTION algorithms. algorithms CONJUNCTION adaptive gradient method. algorithms USED-FOR algorithms. adaptive gradient method USED-FOR algorithms. momentum method USED-FOR algorithms. Method are optimization algorithms, and deep neural nets. OtherScientificTerm is mini - batch of stochastic gradients. Task is online learning literature. ",This paper studies the problem of online optimization algorithms for AMSGrad and Adam. The authors consider the mini-batch of stochastic gradients in deep neural nets. They show that optimistic algorithms for Adam can improve the predictability of gradients. They then propose two algorithms based on the momentum method and adaptive gradient method. The algorithms are evaluated on OPTIMISTIC ONLINE LEARNING.,"This paper presents a theoretical analysis of optimistic algorithms for AMSGrad and Adam. The authors show that optimistic algorithms for Adam and Adam are based on the predictability of gradients. They also show that these optimization algorithms can be applied to deep neural nets. The algorithms are evaluated on OPTIMISTIC ONLINE LEARNING, where the authors show the convergence of the algorithms using momentum method, adaptive gradient method, and adaptive gradient methods. The paper is well-written and well-motivated, and the online learning literature is well written. "
8795,SP:52228b48f2776d57dd422edb33b82e247f056b75,benchmarks USED-FOR image classifier robustness. classifiers USED-FOR safety - critical applications. benchmark USED-FOR corruption robustness topic. IMAGENET - C USED-FOR corruption robustness topic. IMAGENET - C HYPONYM-OF benchmark. dataset EVALUATE-FOR classifier. common perturbations FEATURE-OF classifier ’s robustness. IMAGENET - P HYPONYM-OF dataset. common corruptions CONJUNCTION perturbations. perturbations CONJUNCTION common corruptions. benchmark USED-FOR perturbations. perturbations COMPARE worst - case adversarial perturbations. worst - case adversarial perturbations COMPARE perturbations. common corruptions FEATURE-OF benchmark. AlexNet classifiers CONJUNCTION ResNet classifiers. ResNet classifiers CONJUNCTION AlexNet classifiers. relative corruption robustness EVALUATE-FOR ResNet classifiers. relative corruption robustness EVALUATE-FOR AlexNet classifiers. common perturbation robustness EVALUATE-FOR bypassed adversarial defense. Generic is networks. ,"This paper presents a new benchmark for image classifier robustness, IMAGENET-C, for the corruption robustness topic. The authors show that the classifier’s robustness under common perturbations can be affected by common corruptions as well as perturbation of the training data. They also show that by using the proposed benchmark, the classifiers can be more robust in safety-critical applications. ","This paper presents a new benchmark for image classifier robustness. The authors propose a new dataset, IMAGENET-C, for the corruption robustness topic. The dataset is designed to evaluate the classifier’s robustness to common perturbations, which are used to train classifiers for safety-critical applications.  The authors show that the proposed benchmark is more robust to perturbation attacks than the worst-case adversarial attacks.  They also show that adversarial examples are more robust against common corruptions than adversarial ones.  Finally, the authors provide a theoretical analysis of the performance of AlexNet classifiers, ResNet classifier, and ResNet with respect to the relative corrupt robustness of the networks. They also provide a study on bypassed adversarial defense."
8804,SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"MAP estimation USED-FOR dropout training. model PART-OF family. models USED-FOR power mean. lower bounds FEATURE-OF stochastic subvariants. sampled dropout masks USED-FOR power mean. models PART-OF family. deterministic dropout USED-FOR MC averaging. Task is dropout. Method are conditional models, and regularisation - heavy language modelling. OtherScientificTerm are dropout objective, and deterministic subvariant ’s bound. Generic is It. ","This paper studies the problem of dropout estimation in dropout training. The authors propose a new dropout objective based on the deterministic subvariant’s bound. They show that the lower bounds for stochastic subvariants have lower bounds on the power mean of the sampled dropout masks. They then propose a family of conditional models that can be used as a model to estimate the dropout mean of any given model in the family. They also propose a deterministic dropout for MC averaging. Finally, they show that their regularisation-heavy language modelling can improve the performance.","This paper proposes a new dropout training method based on MAP estimation. The dropout objective is based on conditional models. The authors propose a family of models for estimating the power mean of stochastic subvariants with lower bounds on the lower bounds. The family consists of two models, one for the deterministic subvariant’s bound and one for a deterministic dropout for MC averaging. It is a regularisation-heavy language modelling, and the authors propose to use sampled dropout masks."
8840,SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"redundant filters PART-OF Convolutional Neural Networks ( CNNs ). robust pruning method USED-FOR GSFP. soft pruning strategy USED-FOR GSFP. cumulative saliency strategy USED-FOR pruning. accuracy EVALUATE-FOR pruning. cumulative saliency strategy USED-FOR accuracy. pruning USED-FOR model recovery process. saliency FEATURE-OF filter. saliency FEATURE-OF filter. saliency USED-FOR pruning. pruning COMPARE local pruning. local pruning COMPARE pruning. normalization formula USED-FOR layers of filters. layers of filters PART-OF network. CNN architectures CONJUNCTION data sets. data sets CONJUNCTION CNN architectures. CNN architectures EVALUATE-FOR GSFP. data sets EVALUATE-FOR GSFP. GSFP USED-FOR global and soft pruning strategies. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 EVALUATE-FOR it. MNIST EVALUATE-FOR it. test accuracy EVALUATE-FOR it. compression ratio EVALUATE-FOR it. OtherScientificTerm are global redundancy, and excessive pruning rate. Generic is model. Task is pruning guidance. Method is pre - trained CNN model. ","This paper proposes a robust pruning method for GSFP, called GSFP. The main idea is to prune layers of filters in a convolutional Neural Networks (CNNs) by using a cumulative saliency strategy to improve the accuracy of the pruning. The authors show that the saliency of a filter depends on the number of layers in the network, and the global redundancy of the model. They then propose a soft pruning strategy for the GSFP that uses a normalization formula to normalize the weights of the layers of the network. They show that this pruning can improve the model recovery process. They also show that it improves the compression ratio and the test accuracy of GSFP on a variety of CNN architectures and data sets. ","This paper proposes a robust pruning method called GSFP for convolutional Neural Networks (CNNs). The authors propose a soft pruning strategy for GSFP, which is based on a cumulative saliency strategy to improve the accuracy of the pruning during the model recovery process. The authors also propose a normalization formula for the layers of filters in the network to reduce the global redundancy. The proposed GSFP is evaluated on MNIST, CIFAR10, and several other CNN architectures and data sets. The results show that it improves the test accuracy and the compression ratio of the pre-trained CNN model compared to local pruning. "
8849,SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,character - based embedder CONJUNCTION word - based classifier. word - based classifier CONJUNCTION character - based embedder. transfer learning scheme USED-FOR cross - lingual subword similarity. limited training data USED-FOR transfer learning scheme. character - based embedder USED-FOR transfer learning scheme. embedder USED-FOR vector representations. written forms USED-FOR vector representations. word vectors USED-FOR classifier. multi - task objective USED-FOR model. CACO models COMPARE cross - lingual word embedding models. cross - lingual word embedding models COMPARE CACO models. low - resource settings USED-FOR CACO models. related language pairs USED-FOR cross - lingual word embedding models. high - resource settings USED-FOR cross - lingual word embedding models. Task is Text classification. Method is joint character representation. Material is cross - lingual or monolingual resources. ,This paper proposes a novel transfer learning scheme for cross-lingual subword similarity. The key idea is to use a character-based embedder and a word-based classifier to learn the joint character representation. The embedder learns the vector representations from written forms and the word vectors are used to train the classifier. The model is trained with a multi-task objective and is evaluated on CACO models in low-resource settings and cross-linkingual word embedding models on related language pairs. The results show that the proposed model performs better than the state-of-the-art on cross-latent or monolingual resources.,"This paper proposes a novel transfer learning scheme for cross-lingual subword similarity. The key idea is to use a character-based embedder and a word-based classifier to learn the joint character representation. The authors propose a multi-task objective to train the model with limited training data. The embedder is used to learn vector representations from written forms, and the classifier is trained on the word vectors. The paper shows that the proposed CACO models outperform other cross-lengual word embedding models in low-resource settings, and cross-ingual words embeddings models on related language pairs. However, the paper does not show that cross-langual or monolingual resources can be used to train cross-latent word-embedding models. "
8858,SP:544e421f9c747640d949f433e3091763508b7237,"marginalized average aggregation ( MAA ) module USED-FOR MAAN. latent discriminative probabilities USED-FOR MAA. latent discriminative probabilities USED-FOR MAA module. MAAN USED-FOR dense and integral action regions. MAAN USED-FOR class activation sequences. algorithm USED-FOR MAA. algorithm USED-FOR complexity. large - scale video datasets EVALUATE-FOR MAAN. MAAN USED-FOR weakly - supervised temporal action localization. large - scale video datasets EVALUATE-FOR weakly - supervised temporal action localization. OtherScientificTerm are dense and integral regions, overestimation of the most salient regions, video snippet features, averaged subset features, and O(T ). Method is marginalized average attentional network ( MAAN ). ","This paper proposes a new method for improving the performance of the marginalized average aggregation (MAAN) module in MAAN. The main idea is to use the latent discriminative probabilities of the MAA module to improve the generalization performance of MAAN in dense and integral action regions. The authors show that MAAN can be used to learn the class activation sequences of a video snippet features, and then use the learned algorithm to reduce the complexity of MAA. They also show that the proposed algorithm can be applied to weakly-supervised temporal action localization on large-scale video datasets. ",The paper proposes a marginalized average aggregation (MAAN) module for MAAN. The MAA module is based on latent discriminative probabilities for dense and integral action regions. The authors propose an algorithm to reduce the complexity of MAA by minimizing the overestimation of the most salient regions. MAAN is applied to class activation sequences. The algorithm is evaluated on several large-scale video datasets for weakly-supervised temporal action localization. The paper shows that MAAN can achieve better performance than the marginalized average attentional network (MAAn). 
8867,SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"neural models USED-FOR Natural Language Processing. structureless distributed representations USED-FOR neural models. models COMPARE representational form. representational form COMPARE models. structures PART-OF wordlevel and chunk - level representations. HRR USED-FOR models. models USED-FOR crude linguistic roles. HRR USED-FOR structured compositional representation. OtherScientificTerm are linguistic structures, and syntax. Method are language models, and Holographic Reduced Representation ( HRR ). ","This paper studies the problem of neural models for Natural Language Processing with structureless distributed representations. The authors propose a new language models, called Holographic Reduced Representation (HRR), which is a structured compositional representation based on HRR. The proposed models are able to capture the structures in both wordlevel and chunk-level representations, and can be used as a representational form. The models can also be used to represent crude linguistic roles. ","This paper proposes a new model for natural language processing. The model is based on the Holographic Reduced Representation (HRR) framework. The authors propose to use structureless distributed representations to train neural models for Natural Language Processing. The models are trained in a representational form, where the structures of wordlevel and chunk-level representations are represented as a set of structured compositional representation. The proposed models are evaluated on a variety of crude linguistic roles, and the results show that the proposed models perform better than existing language models. "
8876,SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"Partially observable Markov decision processes ( POMDPs ) USED-FOR decision - making. perception decision CONJUNCTION planning decision. planning decision CONJUNCTION perception decision. greedy strategy USED-FOR observation selection. point - based value iteration algorithm USED-FOR near - optimal uncertainty reduction. greedy strategy USED-FOR near - optimal uncertainty reduction. sampled belief points USED-FOR near - optimal uncertainty reduction. greedy strategy PART-OF point - based value iteration algorithm. solver USED-FOR reachable subspace of belief simplex. computations USED-FOR perception. planning HYPONYM-OF computations. active perception CONJUNCTION planning. planning CONJUNCTION active perception. OtherScientificTerm are stochastic outcome, known distribution, real - world scenarios, and action space. Method are POMDP models, and selection process. Material is robotic scenarios. ","This paper studies the problem of partially observable Markov decision processes (POMDPs) for decision-making in robotic scenarios. The authors propose a greedy strategy for observation selection based on a point-based value iteration algorithm that uses sampled belief points to achieve near-optimal uncertainty reduction. The algorithm is based on POMDP models with stochastic outcome, where the goal is to find a known distribution that is close to the true distribution in real-world scenarios. To achieve this goal, the authors propose two computations: (1) a reachable subspace of belief simplex, and (2) a selection process that uses a solver to select the optimal action space. Experiments show that the proposed computations can achieve better performance in active perception, planning, and planning decision.","This paper proposes a new method for learning POMDPs. The main idea is to learn a stochastic Markov decision process (POMDP) model that can be used for decision-making. The authors propose a greedy strategy for observation selection and a point-based value iteration algorithm for near-optimal uncertainty reduction based on sampled belief points. The proposed method is evaluated on a number of real-world scenarios, including robotic scenarios. The results show that the proposed method outperforms the state-of-the-art methods. "
8885,SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"Curriculum learning USED-FOR network. data complexity CONJUNCTION network training. network training CONJUNCTION data complexity. internal covariate shift PART-OF network forward pass. representation loss USED-FOR low weighted samples. adaptive weight CONJUNCTION representation loss. representation loss CONJUNCTION adaptive weight. adaptive weight PART-OF curriculum loss. representation loss PART-OF curriculum loss. random sampling USED-FOR curriculum learning. curriculum loss CONJUNCTION stochastic algorithms. stochastic algorithms CONJUNCTION curriculum loss. curriculum loss COMPARE SGD. SGD COMPARE curriculum loss. SGD HYPONYM-OF stochastic algorithms. Method are Deep neural networks, top layers, and learning of top layers. OtherScientificTerm are distribution changes in weight of top layers, backward pass, hard examples, noisy gradients, embedding space, fluctuation of top layers, and hard samples. Task are distribution shifting, and training. Material are Low - weighted data, and benchmark datasets. ",This paper studies the problem of distribution shifting in weight of top layers in Deep neural networks. The authors propose a new curriculum learning for training a network. The main idea is to use the internal covariate shift in the network forward pass as a representation loss for low weighted samples to reduce the data complexity and improve the network training. The proposed curriculum loss is a combination of adaptive weight and representation loss. The theoretical results show that the curriculum loss performs better than standard stochastic algorithms such as SGD. ,"This paper proposes a new method for training deep neural networks with distribution shifting. The authors propose a new curriculum learning method to improve the performance of the network in terms of data complexity and network training. The main idea is to learn the distribution changes in weight of top layers during the forward pass of a network forward pass, and then use an internal covariate shift during the backward pass. This is done by learning the top layers in the embedding space, which is then used as a representation loss for low weighted samples.  The authors show that the proposed curriculum loss outperforms other stochastic algorithms, such as SGD, on two benchmark datasets. "
8894,SP:8b555b9f24044bc68c204169d6a37e262361d706,"heuristics USED-FOR combinatorial optimization problems. REINFORCE USED-FOR baseline. value function USED-FOR baseline. deterministic greedy rollout USED-FOR baseline. attention layers USED-FOR model. REINFORCE USED-FOR model. heuristics USED-FOR Travelling Salesman Problem ( TSP ). heuristics USED-FOR Vehicle Routing Problem ( VRP ). hyperparameters USED-FOR heuristics. Orienteering Problem ( OP ) HYPONYM-OF Vehicle Routing Problem ( VRP ). Generic are it, models, problems, and baselines. Method are Pointer Network, and Prize Collecting TSP ( PCTSP ). ","This paper studies the problem of combinatorial optimization problems with hyperparameters. The authors propose a Pointer Network to solve the Travelling Salesman Problem (TSP). The model is based on REINFORCE with attention layers. The baseline is a deterministic greedy rollout with a value function, and it is shown that the model is able to converge to a good baseline with the same number of attention layers as the baseline. The heuristics are then applied to the Vehicle Routing Problem (VRP) and the Orienteering Problem (OP) and show that it can converge to good baselines. ","This paper proposes a new heuristics for combinatorial optimization problems. The main idea is to use a Pointer Network to solve the Travelling Salesman Problem (TSP) and the Orienteering Problem (OP) of the Vehicle Routing Problem (VRP). The model is based on REINFORCE, which is a deterministic greedy rollout with attention layers. The baseline is a value function, and the model is trained on the value function. The authors show that it outperforms the baselines on the two problems. "
8903,SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"time and space complexity EVALUATE-FOR neural network inference. network quantization USED-FOR neural network inference. limited computational and memory resources FEATURE-OF embedded and mobile devices. differentiable neural architecture search ( DNAS ) framework USED-FOR exponential search space. gradient - based optimization USED-FOR differentiable neural architecture search ( DNAS ) framework. neural architecture search problem USED-FOR problem. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR ResNet. ImageNet EVALUATE-FOR ResNet. quantized models COMPARE full precision models. full precision models COMPARE quantized models. model size CONJUNCTION computational cost. computational cost CONJUNCTION model size. computational cost EVALUATE-FOR full precision models. model size EVALUATE-FOR quantized models. computational cost EVALUATE-FOR quantized models. Method is quantization methods. OtherScientificTerm are design space, and bit - widths. ","This paper studies the time and space complexity of neural network inference with network quantization in the context of embedded and mobile devices with limited computational and memory resources. The authors propose a differentiable neural architecture search (DNAS) framework for the exponential search space based on gradient-based optimization. The problem is formulated as a novel neural network search problem, where the design space is represented as a set of embeddings, and the goal is to find the optimal solution for each embedding in this set. The proposed quantization methods are evaluated on CIFAR-10 and ImageNet, and show that the proposed quantized models perform better than full precision models in terms of model size and computational cost.","This paper studies the time and space complexity of neural network inference with network quantization. The authors propose a differentiable neural architecture search (DNAS) framework for the exponential search space, which is based on gradient-based optimization. The problem is formulated as an optimization problem with limited computational and memory resources for embedded and mobile devices, and the authors propose two quantization methods: (1) quantize the design space, and (2) optimize the bit-widths. The experiments on CIFAR-10 and ImageNet show that the proposed quantized models outperform full precision models in terms of model size and computational cost."
8912,SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"attention USED-FOR neural architectures. attention USED-FOR decoding stage. posterior attention distribution USED-FOR attention. posterior attention models COMPARE attention models. attention models COMPARE posterior attention models. morphological inflection tasks EVALUATE-FOR posterior attention models. alignment accuracy EVALUATE-FOR attention models. translation EVALUATE-FOR posterior attention models. translation CONJUNCTION morphological inflection tasks. morphological inflection tasks CONJUNCTION translation. BLEU score EVALUATE-FOR attention models. BLEU score CONJUNCTION alignment accuracy. alignment accuracy CONJUNCTION BLEU score. morphological inflection tasks EVALUATE-FOR attention models. translation EVALUATE-FOR attention models. alignment accuracy EVALUATE-FOR posterior attention models. BLEU score EVALUATE-FOR posterior attention models. Method are attention architectures, and Posterior Attention Models. Generic is architecture. ","This paper proposes Posterior Attention Models (PAM), a new architecture for neural architectures that uses attention in the decoding stage. Posterior attention models are based on the posterior attention distribution, where the attention is learned in a hierarchical fashion. The authors show that posterior attention models achieve better BLEU score and alignment accuracy compared to other attention models on translation and morphological inflection tasks.","This paper proposes a new architecture for neural architectures, called Posterior Attention Models (PAM). The main idea is to use attention for the decoding stage. The attention is based on the posterior attention distribution. The authors show that posterior attention models outperform other attention models on translation, morphological inflection tasks, and alignment accuracy. "
8921,SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"artifacts CONJUNCTION degenerated transformations. degenerated transformations CONJUNCTION artifacts. smoothness term USED-FOR harmonic functions. harmonic functions USED-FOR consistent mappings. smoothness term PART-OF sample graph. HarmonicGAN USED-FOR bi - directional translations. similarity - consistency USED-FOR inherent selfconsistency property. histogram CONJUNCTION CNN. CNN CONJUNCTION histogram. features FEATURE-OF Distance metrics. CNN HYPONYM-OF Distance metrics. histogram HYPONYM-OF Distance metrics. CNN HYPONYM-OF features. histogram HYPONYM-OF features. HarmonicGAN COMPARE state of the art. state of the art COMPARE HarmonicGAN. CycleGAN COMPARE HarmonicGAN. HarmonicGAN COMPARE CycleGAN. interpretability EVALUATE-FOR HarmonicGAN. object transfiguration CONJUNCTION semantic labeling. semantic labeling CONJUNCTION object transfiguration. medical imaging CONJUNCTION object transfiguration. object transfiguration CONJUNCTION medical imaging. medical imaging HYPONYM-OF applications. semantic labeling HYPONYM-OF applications. object transfiguration HYPONYM-OF applications. tasks EVALUATE-FOR methods. method USED-FOR medical imaging task. Task are unpaired image - to - image translation, manifold view of the problem, and translation. Generic is it. OtherScientificTerm are pixel - to - pixel supervision, manual inputs, and mean - squared error. Metric is training - time cost. ","This paper studies the problem of unpaired image-to-image translation in the manifold view of the problem. The authors propose a new method, called CycleGAN, for bi-directional translations. The main idea is to use the smoothness term in the sample graph of the harmonic functions to learn consistent mappings between the artifacts and degenerated transformations. The paper shows that the similarity-consistency property of these mappings can be used to improve the interpretability of the resulting model. The proposed method is evaluated on a variety of applications, including object transfiguration, semantic labeling, and medical imaging. The results show that CycleGAN outperforms the state of the art in terms of interpretability.","This paper proposes a new method for unpaired image-to-image translation. The authors propose a manifold view of the problem, where each pixel is represented as a set of points in a manifold, and each point in the manifold is represented by a single point in a different manifold. The main idea is to use a smoothness term in the sample graph of the manifold to represent the harmonic functions of the consistent mappings between the points. This is done by minimizing the similarity-consistency property of the corresponding points, and the authors show that it leads to better interpretability compared to the state of the art in terms of interpretability of the features of the Distance metrics (e.g., histogram, CNN, etc.). The authors also propose a method for bi-directional translations, which is based on HarmonicGAN. The proposed method is evaluated on medical imaging task, object transfiguration, semantic labeling, and medical imaging. The results show that the proposed method outperforms CycleGAN on all three tasks. "
8930,SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"EVGP USED-FOR gradient components. stochastic algorithm ( h - detach ) USED-FOR LSTM optimization. stochastic algorithm ( h - detach ) USED-FOR problem. linear path ( cell state ) PART-OF LSTM computational graph. long term dependencies FEATURE-OF components. LSTM USED-FOR dependencies. seed CONJUNCTION learning rate. learning rate CONJUNCTION seed. convergence speed CONJUNCTION robustness. robustness CONJUNCTION convergence speed. robustness CONJUNCTION learning rate. learning rate CONJUNCTION robustness. robustness CONJUNCTION seed. seed CONJUNCTION robustness. benchmark datasets EVALUATE-FOR generalization. benchmark datasets EVALUATE-FOR LSTM gradient. convergence speed EVALUATE-FOR vanilla LSTM gradient based training. LSTM gradient USED-FOR generalization. Method are Recurrent neural networks, and LSTMs. Task is exploding and vanishing gradient problem ( EVGP ). OtherScientificTerm are LSTM weights, and gradients. Generic is path. ","This paper studies exploding and vanishing gradient problem (EVGP) in Recurrent neural networks. The problem is formulated as a stochastic algorithm (h-detach) for LSTM optimization, where the gradient components of an EVGP are a linear path (cell state) in LSTMs. The authors show that the components have long term dependencies, and that the dependencies can be computed by an L STM.  The authors also show that, under certain conditions, the gradients of the path can be approximated by a simple L-divergence.  Finally, the authors provide convergence speed and robustness guarantees for vanilla LSTm gradient based training on several benchmark datasets. ","This paper studies the exploding and vanishing gradient problem (EVGP) of Recurrent neural networks. The problem is formulated as a stochastic algorithm (h-detach) for LSTM optimization, where the gradient components of the LSTMs are defined as a linear path (cell state) in the cell state. The authors show that this path can be decomposed into two components: (1) a linear component of the path, and (2) a non-linear component. The two components have long term dependencies, and the authors prove that the two components are independent of each other. They also show that the gradients of the paths can be computed as a function of the length of the cell. They show that for the linear component, the learning rate and the seed are the most important factors in the convergence speed, robustness, and generalization. The paper also shows that the generalization performance of the proposed L-STM gradient on several benchmark datasets is better than that of vanilla L STM gradient based training. "
8939,SP:9aaff3777321347d1194884af5690b0b5185eff9,posterior distribution FEATURE-OF binary weights. Bayesian deep learning perspective USED-FOR real binary weight networks. reinforcement learning scheme USED-FOR policy network. policy network USED-FOR posterior distribution. binary weights USED-FOR burn - after - reading style. binary weight instances USED-FOR recognition architecture. policy network USED-FOR binary weight instances. policy network USED-FOR recognition architecture. policy network USED-FOR neural network architecture. nested parameter structure FEATURE-OF policy network. nested parameterization USED-FOR joint posterior distribution of binary weights. ImageNet HYPONYM-OF visual recognition tasks. visual recognition tasks EVALUATE-FOR SnapQuant. ImageNet EVALUATE-FOR SnapQuant. Task is point estimation. Generic is method. ,This paper presents a Bayesian deep learning perspective on real binary weight networks from the perspective of the posterior distribution of binary weights. The authors propose a reinforcement learning scheme to train a policy network to learn the posterior distribution of the binary weights in a burn-after-reading style. The recognition architecture is based on the policy network that learns the binary weight instances from a set of nested parameter structure. The proposed method is evaluated on a variety of visual recognition tasks such as ImageNet and SnapQuant.,"This paper proposes a Bayesian deep learning perspective for real binary weight networks. The authors propose a reinforcement learning scheme to learn the posterior distribution of the binary weights of a binary weights in a burn-after-reading style. The posterior distribution is defined as the posterior distribution of binary weights with respect to a set of binary weight instances, and the recognition architecture is based on a policy network with a nested parameter structure. The proposed method is evaluated on several visual recognition tasks, including ImageNet and SnapQuant. "
8948,SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,Bayesian nonparametric framework USED-FOR federated learning with neural networks. inference approach USED-FOR global network. supervision CONJUNCTION data pooling. data pooling CONJUNCTION supervision. federated learning problems EVALUATE-FOR approach. image classification datasets USED-FOR federated learning problems. OtherScientificTerm is local neural network weights. Generic is framework. ,This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The inference approach is based on a global network with local neural network weights. The authors show that the proposed framework can achieve state-of-the-art performance on image classification datasets. The proposed approach is also able to solve several federated training problems with supervision and data pooling.,"This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The framework is based on an inference approach to learn a global network with local neural network weights. The proposed approach is evaluated on image classification datasets, and is shown to improve the performance of the proposed approach on a number of federated training problems, including supervision and data pooling."
8957,SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"GANs CONJUNCTION intrinsic curiosity. intrinsic curiosity CONJUNCTION GANs. GANs CONJUNCTION multi - agent RL. multi - agent RL CONJUNCTION GANs. intrinsic curiosity CONJUNCTION multi - agent RL. multi - agent RL CONJUNCTION intrinsic curiosity. differentiable games USED-FOR learning methods. approach USED-FOR learning dynamics. Opponent shaping USED-FOR learning dynamics. Opponent shaping HYPONYM-OF approach. learning dynamics FEATURE-OF games. approach USED-FOR games. theoretical guarantees FEATURE-OF algorithms. LOLA CONJUNCTION stable variant. stable variant CONJUNCTION LOLA. method USED-FOR LOLA. Stable Opponent Shaping ( SOS ) HYPONYM-OF method. LookAhead HYPONYM-OF stable variant. LookAhead USED-FOR equilibria. strict saddles PART-OF differentiable games. Method are Opponent - Learning Awareness ( LOLA ), LOLA agents, and SOS. Generic is algorithm. OtherScientificTerm are cooperation, Iterated Prisoner ’s Dilemma, ‘ arrogant ’ behaviour, and learning of opponents. ","This paper studies the problem of learning from adversarial examples in differentiable games. The authors propose a new algorithm called Opponent-Learning Awareness (LOLA), which is based on Stable Opponent Shaping (SOS). The authors show that SOS improves the performance of LOLA in a variety of differentiable game settings. They also provide theoretical guarantees for their algorithms.","This paper proposes a novel approach to learning dynamics in games with differentiable games. The approach is based on Opponent-Learning Awareness (LOLA), which is a method to learn the dynamics of games with GANs and multi-agent RL with intrinsic curiosity. The authors show that the proposed algorithm is stable under the Iterated Prisoner’s Dilemma, and that the learning of opponents leads to ‘arrogant’ behaviour. The proposed algorithms also provide theoretical guarantees on the equilibria of the learned dynamics. Experiments are conducted on LOLA and a stable variant called Stable Opponent Shaping (SOSP)."
8966,SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"learning system USED-FOR rare events. feature space FEATURE-OF classifiers / regressors. shape feature HYPONYM-OF prior information. segmentation algorithms USED-FOR it. shape feature USED-FOR feature space. Variational Auto - Encoder(VAE ) USED-FOR segmentation result. loss function USED-FOR shape feature. ground truth masks USED-FOR VAE. VAE USED-FOR shapes. representation USED-FOR qualities of segmentation results. one - dimensional feature space FEATURE-OF representation. segmentation algorithms USED-FOR medical segmentation task. medical segmentation task EVALUATE-FOR alarm system. segmentation algorithms EVALUATE-FOR alarm system. OtherScientificTerm are low dimensional feature space, bad shapes, and loss value. Generic is system. ","This paper proposes a learning system for rare events in the feature space of classifiers/regressors. The key idea is to learn the shape feature of the prior information of the classifier/regressor. The shape feature is defined as a function of the number of classes in the low dimensional feature space, and the loss function is used to learn a segmentation result from the Variational Auto-Encoder(VAE). The VAE uses ground truth masks to encode the shapes of the shapes. The representation of the shape is then used to represent the qualities of segmentation results in the one-dimensional feature space. The paper shows that the proposed alarm system is able to achieve state-of-the-art performance on the medical segmentation task using segmentation algorithms.","This paper proposes a learning system for rare events where the feature space of classifiers/regressors is a low dimensional feature space and the prior information is a shape feature. The authors propose a Variational Auto-Encoder(VAE) to learn the segmentation result from the shape feature, which is a function of the loss function. The VAE learns the shapes from ground truth masks. The paper shows that the VAE is able to learn shapes with good shapes and bad shapes with the same loss value, and it is shown that it can be learned using segmentation algorithms. The representation of segmentation results is learned in a one-dimensional feature space, and the authors show that the proposed representation can capture the qualities of segmentations results. The proposed alarm system is evaluated on a medical segmentation task, and shows that it is more robust to the loss value."
8975,SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"denoising CONJUNCTION inpainting. inpainting CONJUNCTION denoising. inpainting CONJUNCTION reconstruction. reconstruction CONJUNCTION inpainting. Deep neural networks USED-FOR compressing images. Deep neural networks USED-FOR inverse problems. compressing images USED-FOR inverse problems. reconstruction HYPONYM-OF inverse problems. convolutional neural networks HYPONYM-OF Deep neural networks. denoising HYPONYM-OF inverse problems. few and noisy measurements USED-FOR reconstruction. inpainting HYPONYM-OF inverse problems. tools COMPARE imagegenerating deep neural networks. imagegenerating deep neural networks COMPARE tools. wavelets HYPONYM-OF tools. deep neural network USED-FOR natural images. deep decoder HYPONYM-OF untrained simple image model. deep decoder USED-FOR images. network weights COMPARE wavelet - based thresholding. wavelet - based thresholding COMPARE network weights. underparameterization USED-FOR deep decoder. deep decoder USED-FOR denoising. underparameterization USED-FOR overfitting. ReLU activation CONJUNCTION channelwise normalization. channelwise normalization CONJUNCTION ReLU activation. pixel - wise linear combination of channels CONJUNCTION ReLU activation. ReLU activation CONJUNCTION pixel - wise linear combination of channels. upsampling unit CONJUNCTION pixel - wise linear combination of channels. pixel - wise linear combination of channels CONJUNCTION upsampling unit. them USED-FOR signal representations. neural networks USED-FOR signal representations. it USED-FOR neural networks. neural networks USED-FOR them. theoretical analysis USED-FOR network. OtherScientificTerm are output dimension, weight parameters, convolutions, and output dimensionality. Material is large datasets. ","This paper studies the problem of inverse problems such as denoising, inpainting, reconstruction, and reconstruction with few and noisy measurements. Deep neural networks such as convolutional neural networks are commonly used for these inverse problems. The authors propose two tools, wavelets and deep decoder, for compressing images. The main contribution of the paper is to show that underparameterization can lead to overfitting in deep decoders, which is a common problem in untrained simple image model. The paper also proposes a theoretical analysis of the output dimensionality of the neural networks and how it affects the performance of neural networks trained with them. The theoretical analysis shows that the network weights are more sensitive to the input dimension than wavelet-based thresholding, and that the weight parameters of the convolutions are more robust to noise. Finally, the paper proposes a new upsampling unit and a pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. The experimental results show that the proposed tools perform better than existing imagegenerating deep neural networks on natural images.","This paper presents a theoretical analysis of convolutional neural networks, which is a family of Deep neural networks for inverse problems such as denoising, inpainting, and reconstruction. The authors show that the output dimension, weight parameters, and output dimensionality of the convolutions are important factors in the performance of convolutions. They also show that underparameterization of the deep decoder can lead to overfitting, and show that it can be applied to natural images as well as images generated by a deep neural network for natural images. Finally, they show that these tools, such as wavelets, outperform other imagegenerating deep neural networks on large datasets. "
8984,SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"natural language ( NL ) USED-FOR Program synthesis. SAPS HYPONYM-OF end - to - end neural network. end - to - end neural network USED-FOR multi - sentence NL specifications. pretrained word embedding CONJUNCTION bi - directional multi - layer LSTM. bi - directional multi - layer LSTM CONJUNCTION pretrained word embedding. bi - directional multi - layer LSTM USED-FOR processing of word sequences. abstract syntax trees CONJUNCTION pretrained word embedding. pretrained word embedding CONJUNCTION abstract syntax trees. pretrained word embedding USED-FOR processing of word sequences. pretrained word embedding USED-FOR architecture. bi - directional multi - layer LSTM USED-FOR architecture. abstract syntax trees USED-FOR architecture. neural components USED-FOR architecture. signal propagation schemes CONJUNCTION soft attention mechanism. soft attention mechanism CONJUNCTION signal propagation schemes. doubly - recurrent LSTM CONJUNCTION signal propagation schemes. signal propagation schemes CONJUNCTION doubly - recurrent LSTM. signal propagation schemes USED-FOR decoder. soft attention mechanism USED-FOR decoder. doubly - recurrent LSTM USED-FOR decoder. SAPS COMPARE method. method COMPARE SAPS. NL analyzer CONJUNCTION source code generator. source code generator CONJUNCTION NL analyzer. methods COMPARE it. it COMPARE methods. fixed - dimensional latent representation USED-FOR NL analyzer. post - processing USED-FOR it. fixed - dimensional latent representation USED-FOR it. Task are software development, and end - user programming. ","This paper proposes SAPS, an end-to-end neural network based on SAPS for Program synthesis in natural language (NL) for multi-sentence NL specifications. The proposed architecture combines abstract syntax trees, pretrained word embedding, bi-directional multi-layer LSTM, and a soft attention mechanism for the decoder. The authors show that the proposed method outperforms SAPS in terms of performance on a variety of tasks, including NL analyzer, source code generator, and end-user programming. ","This paper proposes an end-to-end neural network for multi-sentence NL specifications, called SAPS, for Program synthesis. The architecture is based on abstract syntax trees, pretrained word embedding, bi-directional multi-layer LSTM, and signal propagation schemes. The authors also propose a soft attention mechanism for the decoder and a doubly-rewarded LSTMs for the signal propagation scheme. The proposed method is evaluated on two tasks: software development, and end-user programming. The NL analyzer is trained using a fixed-dimensional latent representation, and the source code generator is trained with post-processing. The results show that the proposed method outperforms existing methods."
8993,SP:d2ec231bb6153a303e5110e671dea14c2721e636,"deep neural networks USED-FOR tiny input perturbations. MNIST HYPONYM-OF computer vision. deep neural networks USED-FOR MNIST. L0 robustness EVALUATE-FOR undefended networks. adversarial robustness EVALUATE-FOR MNIST. class - conditional data distributions USED-FOR robust classification model. adversarial attacks USED-FOR model. robustness EVALUATE-FOR MNIST. MNIST EVALUATE-FOR approach. robustness EVALUATE-FOR approach. Method are neural network model, L∞ defense, input binarization, and decision - based attack. OtherScientificTerm are adversarial perturbations, L2 perturbations, Lp norms, and L0, L2 and L∞ perturbations. Material are unrecognizable images, and adversarial examples. Generic is attack. ","This paper studies the problem of adversarial perturbations of MNIST, a popular model for computer vision. The authors propose a novel attack on the neural network model, which is based on the L∞ defense, where the input binarization is replaced by a decision-based attack. They show that the L0 robustness of undefended networks with deep neural networks is better than the L2 robustness for MNIST with class-conditional data distributions. They also show that adversarial attacks can be used to improve the model's robustness against MNIST. ","This paper proposes a new adversarial attack on MNIST, a model that is designed to defend against tiny input perturbations. The attack is based on the L∞ defense, where the input binarization of the neural network model is used. The authors show that the L0 robustness of undefended networks is better than the L2 robustness. They also show that adversarial robustness for MNIST can be improved by using class-conditional data distributions. "
9002,SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"spectra of weight matrices PART-OF discriminator. spectra of weight matrices USED-FOR GANs. framework USED-FOR GANs. weight matrices PART-OF discriminator. slow singular value decays FEATURE-OF weight matrices. regularizers CONJUNCTION constraints. constraints CONJUNCTION regularizers. reparameterization approach USED-FOR GANs. reparameterization approach USED-FOR weight matrices. regularizers USED-FOR spectra of the weight matrices. constraints USED-FOR spectra of the weight matrices. spectrum control USED-FOR GANs. methods COMPARE method. method COMPARE methods. CIFAR-10, STL-10, and ImgaeNet datasets EVALUATE-FOR method. spectral normalization USED-FOR method. Method are Generative Adversarial Networks ( GANs ), and singular value decompositions. OtherScientificTerm is slow singular value decay. ","This paper proposes a framework to train GANs based on spectra of weight matrices in the discriminator. The authors propose a reparameterization approach to learn the weight matrix of GAN with slow singular value decays, which is an important problem in the context of Generative Adversarial Networks (GANs). The main idea is to use regularizers and constraints to generate spectra for the spectra, which are then used to train a discriminator based on the learned spectra. The proposed method is evaluated on CIFAR-10, STL-10 and ImgaeNet datasets and shows that the proposed method performs better than existing methods. ","This paper proposes a framework for training GANs with the spectra of weight matrices. The authors propose a reparameterization approach to train the discriminator with the weight matrix. The proposed method is evaluated on CIFAR-10, STL-10 and ImgaeNet datasets. The method outperforms other methods in terms of spectral normalization and constraints. The paper also shows that the slow singular value decay can be reduced to singular value decompositions. "
9011,SP:8115fd9b681198d62100c36794926fb57dc0a4f5,Acceleration USED-FOR reinforcement learning methods. Anderson acceleration technique USED-FOR value iteration. Anderson Accelerated Value Iteration ( A2VI ) HYPONYM-OF accelerated value iteration algorithm. method USED-FOR Deep Q - learning algorithm. approach USED-FOR approximation of the policy evaluation. approximate method USED-FOR policy evaluation. A2VI COMPARE policy iteration. policy iteration COMPARE A2VI. policy iteration HYPONYM-OF approximate method. toy problems CONJUNCTION Atari games. Atari games CONJUNCTION toy problems. Material is historical data. Generic is algorithm. ,"This paper proposes a new accelerated value iteration algorithm, Anderson Accelerated Value Iteration (A2VI), which uses the Anderson acceleration technique to accelerate value iteration in reinforcement learning methods. A2VI is an approximate method for policy evaluation, which can be applied to both toy problems and Atari games. The proposed method can be used as a Deep Q-learning algorithm. The authors show that the proposed algorithm can achieve state-of-the-art performance on both toy and Atari tasks.","This paper proposes an accelerated value iteration algorithm called Anderson Accelerated Value Iteration (A2VI) for reinforcement learning methods. A2VI is based on the Anderson acceleration technique for value iteration. The proposed method can be applied to any Deep Q-learning algorithm. The authors propose an approach for the approximation of the policy evaluation, which is an approximate method for policy evaluation. The algorithm is evaluated on toy problems and Atari games, and is compared to historical data."
9020,SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,method USED-FOR catastrophic forgetting problem. SupportNet USED-FOR catastrophic forgetting problem. class incremental learning scenario FEATURE-OF catastrophic forgetting problem. deep learning CONJUNCTION support vector machine ( SVM ). support vector machine ( SVM ) CONJUNCTION deep learning. support vector machine ( SVM ) PART-OF SupportNet. deep learning PART-OF SupportNet. SVM PART-OF SupportNet. consolidation regularizers USED-FOR model. SupportNet COMPARE deep learning model. deep learning model COMPARE SupportNet. SupportNet COMPARE incremental learning methods. incremental learning methods COMPARE SupportNet. tasks EVALUATE-FOR SupportNet. tasks EVALUATE-FOR method. OtherScientificTerm is catastrophic forgetting. ,This paper proposes a method for the catastrophic forgetting problem in the class incremental learning scenario. The proposed SupportNet combines deep learning and support vector machine (SVM) in the SupportNet. The model is based on consolidation regularizers. The authors show that the proposed method outperforms existing incremental learning methods on several tasks. ,This paper proposes a method for the catastrophic forgetting problem in the class incremental learning scenario. The proposed model is based on consolidation regularizers. SupportNet is a combination of deep learning and support vector machine (SVM). The proposed method is evaluated on several tasks and compared to other deep learning model and incremental learning methods.
9029,SP:d228d213f79716774043cea253305fecece659ec,"methods USED-FOR representations. methods USED-FOR unit selectivity. unit selectivity USED-FOR representations. neural networks ( NNs ) USED-FOR representations. measures PART-OF AlexNet. localist selectivity HYPONYM-OF measures. precision CONJUNCTION class - conditional mean activity selectivity CCMAS. class - conditional mean activity selectivity CCMAS CONJUNCTION precision. precision and CCMAS measures USED-FOR selectivity. fc6 CONJUNCTION conv5. conv5 CONJUNCTION fc6. units PART-OF conv5. units PART-OF fc6. RNNs COMPARE AlexNet. AlexNet COMPARE RNNs. AlexNet USED-FOR localist representations. RNNs USED-FOR localist representations. Generic is measure. Metric are top - class selectivity, and selectivity measures. Method are recurrent neural networks ( RNNs ), activation maximization ( AM ) images, fc8, and NNs. OtherScientificTerm are hidden layers, and selective units. ","This paper proposes a new measure of top-class selectivity for recurrent neural networks (RNNs), which is based on activation maximization (AM) images. The proposed measure, called localist selectivity, is a combination of two measures: precision and class-conditional mean activity selectivity CCMAS. The authors show that the proposed measures improve the performance of AlexNet on fc6 and conv5.  ","This paper proposes a new measure of unit selectivity for recurrent neural networks (RNNs), which is based on activation maximization (AM) images. The proposed measure is a combination of top-class selectivity and class-conditional mean activity selectivity (CCMAS) measures. The authors show that the proposed measures improve the performance of AlexNet on fc6, conv5, and fc8. They also show that AlexNet is able to learn localist representations with fewer hidden layers than RNNs. "
9038,SP:b9deae0392e0160b400d76c549d382e235196f8c,"spectral methods CONJUNCTION posterior inference. posterior inference CONJUNCTION spectral methods. probabilistic graphical models USED-FOR posterior inference. graphs USED-FOR Community detection. spectral methods USED-FOR Community detection. posterior inference USED-FOR Community detection. signal - to - noise ratio FEATURE-OF statistical and computational detection thresholds. stochastic block model HYPONYM-OF random graph families. graphs USED-FOR node - wise classification problem. node - wise classification problem USED-FOR community detection. learning perspective USED-FOR it. Graph Neural Networks ( GNNs ) USED-FOR community detection problems. supervised learning setting USED-FOR community detection problems. belief propagation algorithm USED-FOR binary and multiclass stochastic block models. they COMPARE belief propagation algorithm. belief propagation algorithm COMPARE they. line graph of edge adjacencies FEATURE-OF non - backtracking operator. non - backtracking operator USED-FOR GNNs. real - world datasets EVALUATE-FOR GNNs. linear ) GNNs USED-FOR community detection problems. linear ) GNNs USED-FOR optimization landscape. Generic is approaches. Method is generative models. OtherScientificTerm are computational threshold, local minimum, and global minimum / minima. ","This paper studies the problem of community detection with graph neural networks. Community detection is a node-wise classification problem with graphs, where the goal is to find the node with the highest signal-to-noise ratio. The authors consider the problem in a supervised learning setting, where each node is represented as a family of random graph families (e.g., a stochastic block model). Community detection can be done using spectral methods or posterior inference with probabilistic graphical models. The main contribution of this paper is to propose a belief propagation algorithm that can be applied to binary and multiclass Stochastic block models. In particular, it uses a learning perspective to learn a non-backtracking operator that predicts the local minimum and global minimum/minima of the computational threshold. The results show that they are more efficient than the belief propagation method, and that they can be used to train GNNs on real-world datasets. ","This paper proposes a new method for community detection based on Graph Neural Networks (GNNs) that can be used in conjunction with spectral methods and posterior inference. Community detection is a node-wise classification problem with graphs, where the computational threshold is a function of the signal-to-noise ratio between the statistical and computational detection thresholds. The authors propose a stochastic block model for random graph families, which is based on probabilistic graphical models. They show that it can be learned from a learning perspective. They also show that GNNs with a non-backtracking operator can be trained on real-world datasets, and they show that they are more robust than the belief propagation algorithm used in binary and multiclass Stochastic block models. Finally, the authors show that the optimization landscape of (non-backwarding operator) can be modeled as a line graph of edge adjacencies, and that the global minimum/minima of generative models can be computed."
9047,SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"sparse weights PART-OF linear combination. provable algorithms USED-FOR dictionary learning. provable dictionary learning methods USED-FOR coefficient recovery. linear and non - linear operations PART-OF it. algorithm COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE algorithm. Task are dictionary learning problem, optimization, and recovery of the dictionary. OtherScientificTerm are dictionary, coefficients, and geometric rate. Method are linear model, NOODL, and neural architectures. ","This paper studies the dictionary learning problem, where the goal is to learn a dictionary with sparse weights in a linear combination of the coefficients of the dictionary. The authors propose two provable algorithms for dictionary learning with provable dictionary learning methods for coefficient recovery. In particular, the authors propose a linear model, called NoODL, which is a combination of linear and non-linear operations. The optimization is based on the assumption that the coefficients are linear in the dictionary, and that the geometric rate is constant. The algorithm is shown to outperform state-of-the-art techniques in terms of the number of iterations and the performance of neural architectures.","This paper studies the dictionary learning problem, where the goal is to learn a dictionary with sparse weights. The authors propose two provable algorithms for dictionary learning. The first algorithm is a linear combination of linear and non-linear operations. The second algorithm is an extension of existing provable dictionary learning methods for coefficient recovery. The main idea is to use a linear model to recover the coefficients of the dictionary. The optimization is done by minimizing the geometric rate of the coefficients. The algorithm is shown to outperform state-of-the-art techniques on the NoODL and neural architectures."
9056,SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"differentiable model CONJUNCTION similarity function. similarity function CONJUNCTION differentiable model. loss function USED-FOR binary hash codes. differentiable model USED-FOR binary hash codes. loss function COMPARE prior methods. prior methods COMPARE loss function. log likelihood loss USED-FOR prior methods. log likelihood loss USED-FOR loss function. multi - indexing USED-FOR hashes. techniques USED-FOR similarity search tasks. ImageNet CONJUNCTION SIFT 1 M. SIFT 1 M CONJUNCTION ImageNet. information retrieval tasks EVALUATE-FOR SIFT 1 M. ImageNet USED-FOR information retrieval tasks. OtherScientificTerm are Hamming distance target, loss terms, and minibatch. Method is training scheme. Metric are MAP, and query cost. ","This paper proposes a new training scheme for learning binary hash codes with a differentiable model and a similarity function. The loss function is based on a log likelihood loss, which is used to learn binary hash code. The authors show that the proposed loss function performs better than prior methods. The paper also shows that multi-indexing can be used to find hashes that are close to the Hamming distance target. The proposed techniques are tested on several similarity search tasks on ImageNet and SIFT 1 M.","The paper proposes a new training scheme for binary hash codes. The loss function is a differentiable model and a similarity function. The authors show that the proposed loss function outperforms prior methods based on log likelihood loss and multi-indexing for hashes. The paper also shows that the Hamming distance target is the same as the MAP, and that the query cost is similar to that of the loss terms. Experiments on ImageNet and SIFT 1 M show the effectiveness of the proposed techniques for similarity search tasks. "
9065,SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,Neural architecture search ( NAS ) USED-FOR task - specific neural network topology. networks USED-FOR search. Graph HyperNetwork ( GHN ) USED-FOR search cost. graph neural network USED-FOR inference. regular hypernetworks CONJUNCTION premature early stopping. premature early stopping CONJUNCTION regular hypernetworks. GHNs USED-FOR architecture. GHNs USED-FOR network. GHNs COMPARE regular hypernetworks. regular hypernetworks COMPARE GHNs. GHNs COMPARE premature early stopping. premature early stopping COMPARE GHNs. validation accuracy EVALUATE-FOR networks. validation accuracy EVALUATE-FOR surrogate search signal. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. they COMPARE random search methods. random search methods COMPARE they. GHNs COMPARE random search methods. random search methods COMPARE GHNs. ImageNet EVALUATE-FOR random search methods. CIFAR-10 EVALUATE-FOR random search methods. networks COMPARE manual designs. manual designs COMPARE networks. GHNs USED-FOR anytime prediction setting. speed - accuracy tradeoff EVALUATE-FOR networks. Method is manual architecture designs. Generic is it. Task is NAS. ,"This paper studies the problem of neural architecture search (NAS) for task-specific neural network topology. The authors propose Graph HyperNetwork (GHN) to reduce the search cost by using graph neural network for inference. They show that GHNs are able to learn the architecture of a network faster than regular hypernetworks and premature early stopping. They also show that they can achieve better validation accuracy than random search methods such as CIFAR-10 and ImageNet. Finally, they show that the speed-accuracy tradeoff between networks can be improved by using GHNs in the anytime prediction setting. ","This paper proposes a new architecture search (NAS) for task-specific neural network topology. The authors propose a Graph HyperNetwork (GHN) to reduce the search cost of the network. The architecture is based on the Graph Neural Network (GNN), which is a graph neural network for inference. The paper shows that the proposed networks can achieve better validation accuracy than other networks for search, and the authors show that the surrogate search signal can be used to improve the validation accuracy of the networks. They also show that they are faster than other random search methods such as CIFAR-10 and ImageNet, and that the speed-accuracy tradeoff between the networks and the manual architecture designs can be reduced. Finally, they show that GHNs outperform other regular hypernetworks and premature early stopping in the anytime prediction setting."
9074,SP:65ccf43cd4e033d22239069057f5200d49f33724,Imitation learning USED-FOR optimal policy. expert demonstrations USED-FOR Imitation learning. expert demonstrations USED-FOR optimal policy. expert demonstrations USED-FOR deep learning. method USED-FOR generative adversarial imitation learning. multiclass classification USED-FOR discriminator functions. method USED-FOR multiclass classification. method USED-FOR discriminator functions. method COMPARE generative adversarial imitation learning baseline. generative adversarial imitation learning baseline COMPARE method. continuous control tasks EVALUATE-FOR method. method USED-FOR policies. continuous control tasks EVALUATE-FOR generative adversarial imitation learning baseline. OtherScientificTerm is non - expert demonstrations. ,"This paper proposes a method for generative adversarial imitation learning based on expert demonstrations. The idea is to use expert demonstrations to learn the optimal policy from expert demonstrations for Imitation learning. The method is based on multiclass classification, where the discriminator functions are learned using the method. The proposed method is evaluated on a variety of continuous control tasks and shows that the proposed method can learn policies with better performance than the generative imitation learning baseline. ","This paper proposes a method for generative adversarial imitation learning with expert demonstrations. The idea is to learn an optimal policy from expert demonstrations, and then use the expert demonstrations for deep learning. The proposed method is evaluated on multiclass classification and discriminator functions. The method is shown to outperform the generative imitation learning baseline on continuous control tasks. The authors also show that the proposed method can learn policies with non-expert demonstrations."
9083,SP:e8427949a98effbd37ce7604fa11f240e2342196,"natural science HYPONYM-OF applications. neural networks USED-FOR task. Invertible Neural Networks ( INNs ) HYPONYM-OF neural networks. neural networks USED-FOR ambiguous inverse problem. INNs USED-FOR forward process. neural networks COMPARE INNs. INNs COMPARE neural networks. latent output variables USED-FOR INNs. model USED-FOR inverse process. invertibility USED-FOR model. medicine CONJUNCTION astrophysics. astrophysics CONJUNCTION medicine. INNs USED-FOR unrecoverable parameters. INNs USED-FOR multi - modalities. INNs USED-FOR parameter correlations. parameter space FEATURE-OF multi - modalities. OtherScientificTerm are hidden system parameters, posterior parameter distribution, observed measurement, and distribution of the latent variables. Task is inverse problem. Generic is ambiguity. Method is INN. Material is artificial data. ","This paper studies the inverse problem in natural science, where the hidden system parameters are unknown. The authors propose to use neural networks to solve the task by using the Invertible Neural Networks (INNs) to learn the latent output variables of the INNs. The INNs are trained to predict the forward process of a model in the inverse process by using invertibility of the latent variables. The paper shows that INNs can learn the unrecoverable parameters in the parameter space of multi-modalities in parameter space.  The paper also shows that the INN is able to recover the parameter correlations between the parameters of the model. ","This paper studies the inverse problem in natural science. Invertible Neural Networks (INNs) are a family of neural networks that can be used for any task where the hidden system parameters are not known. In particular, INNs are used to solve the ambiguous inverse problem where the posterior parameter distribution is not known, but the observed measurement of the distribution of the latent variables is known. The authors propose to use INNs to learn the forward process using the latent output variables of the INNs. The model learns the inverse process using invertibility. In addition, the authors show that INNs can learn the unrecoverable parameters in the parameter space of multi-modalities in parameter space. "
9092,SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"ensemble of NNs COMPARE Bayesian NNs. Bayesian NNs COMPARE ensemble of NNs. scoring rule USED-FOR ensemble of NNs. finite mixture model USED-FOR ensemble method. uniform mixing weights USED-FOR finite mixture model. adaptive, input - dependent distribution USED-FOR fixed mixing weights. NN USED-FOR adaptive, input - dependent distribution. model COMPARE approaches. approaches COMPARE model. uncertainty estimates EVALUATE-FOR model. Method are deep neural networks ( NNs ), mixture model approach, mixture density networks, and compound density networks. OtherScientificTerm are prediction uncertainty, and mixture components. Material is adversarial examples. ","This paper proposes a mixture model approach for deep neural networks (NNs). The ensemble method is based on a scoring rule for the ensemble of NNs, where the prediction uncertainty depends on the mixture components. The authors propose a finite mixture model based on uniform mixing weights. The adaptive, input-dependent distribution is used to replace the fixed mixing weights in the NN. The proposed model is shown to have better uncertainty estimates than other approaches. The experimental results show that the proposed mixture density networks are more robust than compound density networks.","The paper proposes an ensemble of deep neural networks (NNs), where the ensemble of NNs is composed of Bayesian NNs with a scoring rule. The ensemble method is based on a finite mixture model with uniform mixing weights. The authors propose a mixture model approach, where the prediction uncertainty is modeled by mixture density networks, and the mixture components are represented by compound density networks. The proposed model is evaluated on adversarial examples, and is shown to outperform other approaches on uncertainty estimates. The adaptive, input-dependent distribution of the fixed mixing weights is also compared to the NN."
9101,SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"energy consumption CONJUNCTION communication bandwidth. communication bandwidth CONJUNCTION energy consumption. communication bandwidth CONJUNCTION storage requirements. storage requirements CONJUNCTION communication bandwidth. deep neural networks HYPONYM-OF model class. model size reduction PART-OF deep learning. pruning CONJUNCTION quantization. quantization CONJUNCTION pruning. techniques USED-FOR Shannon - style coding schemes. Shannon - style coding schemes USED-FOR empirical weight distribution. quantization HYPONYM-OF techniques. pruning HYPONYM-OF techniques. full variational distribution USED-FOR coding schemes. compression rates EVALUATE-FOR coding schemes. KullbackLeibler divergence FEATURE-OF sampled variational distribution. random sample USED-FOR network weights. constraint USED-FOR compression rate. constraint FEATURE-OF Kullback - Leibler divergence. encoding scheme COMPARE information - theoretical lower bound. information - theoretical lower bound COMPARE encoding scheme. variational family USED-FOR information - theoretical lower bound. variational family USED-FOR encoding scheme. it COMPARE approaches. approaches COMPARE it. method USED-FOR neural network compression. VGG-16 / CIFAR-10 EVALUATE-FOR approach. fixed memory budget EVALUATE-FOR approach. compression rates EVALUATE-FOR approach. compression rates EVALUATE-FOR it. OtherScientificTerm are memory footprint, deterministic weights, weight determinism, encoding distribution, and expected loss. Method is bits - back argument. ","This paper studies the problem of model size reduction in deep learning, where the memory footprint of a model class (e.g., deep neural networks) is large and the communication bandwidth and storage requirements are high. The authors propose Shannon-style coding schemes that use a full variational distribution over the empirical weight distribution of the model class, which is a combination of techniques such as pruning, quantization, and pruning. They show that the KullbackLeibler divergence between the sampled variational distributions of the sampled weights and the weights of a random sample of the network weights is a constraint on the compression rate of the encoded network weights, and that the compression rates of these coding schemes are close to the optimal compression rates in terms of the number of bits-back argument. They then propose a method for neural network compression using VGG-16/CIFAR-10, and show that it achieves better compression rates than existing approaches on a fixed memory budget than the encoding scheme with a variational family. ","This paper proposes a method for neural network compression. The main idea is to reduce the memory footprint of deep neural networks by reducing the number of parameters in the model class. The model size reduction in deep learning is an important part of deep learning. Previous techniques such as pruning, quantization, and various variants of Shannon-style coding schemes are used to approximate the empirical weight distribution of a model class, which is a full variational distribution of the weights. The authors propose to use a KullbackLeibler divergence between the sampled variational distributions of the network weights and a random sample of the original network weights. This constraint is used to control the compression rate, and the authors show that the proposed compression rate can be reduced to a fixed memory budget. The encoding scheme is based on the variational family of the information-theoretic lower bound, and it outperforms other approaches in terms of compression rates, storage requirements, and communication bandwidth. "
9110,SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,neural network architectures USED-FOR Neural architecture search ( NAS ). architectures USED-FOR large - scale tasks. ImageNet HYPONYM-OF large - scale tasks. GPU hours EVALUATE-FOR Differentiable NAS. continuous representation of network architecture USED-FOR Differentiable NAS. continuous representation of network architecture USED-FOR GPU hours. proxy tasks USED-FOR they. architectures USED-FOR proxy tasks. large - scale target tasks CONJUNCTION hardware platforms. hardware platforms CONJUNCTION large - scale target tasks. architectures USED-FOR large - scale target tasks. ProxylessNAS USED-FOR architectures. ProxylessNAS USED-FOR large - scale target tasks. GPU hours CONJUNCTION GPU memory. GPU memory CONJUNCTION GPU hours. computational cost EVALUATE-FOR regular training. GPU hours HYPONYM-OF computational cost. GPU memory HYPONYM-OF computational cost. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR model. test error EVALUATE-FOR model. ImageNet EVALUATE-FOR model. model COMPARE MobileNetV2. MobileNetV2 COMPARE model. ImageNet EVALUATE-FOR MobileNetV2. top-1 accuracy EVALUATE-FOR MobileNetV2. GPU latency EVALUATE-FOR model. top-1 accuracy EVALUATE-FOR model. ProxylessNAS USED-FOR neural architectures. Neural architecture search ( NAS ) USED-FOR neural network architecture design. neural network architecture design USED-FOR deep learning tasks. Neural architecture search ( NAS ) USED-FOR deep learning tasks. image recognition CONJUNCTION language modeling. language modeling CONJUNCTION image recognition. image recognition HYPONYM-OF deep learning tasks. language modeling HYPONYM-OF deep learning tasks. models USED-FOR task. NAS USED-FOR large - scale task. ImageNet HYPONYM-OF large - scale task. building blocks USED-FOR proxy tasks. top - performing blocks USED-FOR large - scale target task. paradigm USED-FOR NAS algorithms. blocks USED-FOR proxy tasks. latency HYPONYM-OF hardware metrics. methods USED-FOR transferability. Proxy,"This paper proposes a new architecture search method, Differentiable NAS, for large-scale target tasks. The proposed method is based on ProxylessNAS, which is an extension of Proxyless NAS. The main idea is to use a continuous representation of the network architecture to compute the GPU hours and the GPU memory. The authors show that the proposed method performs better than MobileNetV2 on CIFAR-10, ImageNet, and ImageNet with top-1 accuracy. ","This paper proposes a new paradigm for NAS algorithms, called Differentiable NAS, which is an extension of Neural architecture search (NAS) to large-scale tasks. The authors propose to use a continuous representation of network architecture to compute the GPU hours and GPU memory of a differentiable NAS. The proposed architectures are based on ProxylessNAS, which can be applied to a number of different architectures, and the authors show that they are transferable across different architectures and different hardware platforms. They also show that the proposed methods improve transferability in terms of test error and computational cost for regular training. "
9119,SP:e5b70d43d301d1980fae02623ea711976b429c14,"Lagrangian dual FEATURE-OF problem. additive linear penalties USED-FOR Lagrangian dual. non - convex settings FEATURE-OF problem. training procedure USED-FOR non - convex, large - data settings. second - order ones FEATURE-OF linear penalties. secondorder penalties USED-FOR penalized objective. penalty coefficient USED-FOR penalized objective. method USED-FOR gradients. second - order penalties FEATURE-OF gradients. algorithm USED-FOR classifier. Metric is fairness. OtherScientificTerm are linear constraints, constrained objective, Lagrangian, deterministic saddle - point equilibrium, instability, and stochastic mini - batch settings. Method is two - player min - max games. ","This paper studies the problem of Lagrangian dual in non-convex settings with additive linear penalties. The authors propose a training procedure to solve this problem in both non-vex, large-data settings. The main idea is to use secondorder penalties to penalize the penalized objective with a penalty coefficient that depends on the dimension of the data. The paper shows that the second-order ones are better than the linear penalties in the case of linear constraints.  The paper also provides an algorithm to train the classifier in the stochastic mini-batch settings, where the constrained objective is a deterministic saddle-point equilibrium. The algorithm is shown to improve the performance of the two-player min-max games.","This paper studies the problem of Lagrangian dual in non-convex settings with additive linear penalties. The main contribution of the paper is to propose a training procedure to tackle the problem in both non- convex, large-data settings and in the case of linear constraints with deterministic saddle-point equilibrium. The proposed objective is a constrained objective where the objective is to maximize the penalty coefficient of the penalized objective with respect to the secondorder penalties of the linear penalties, which are the second-order ones of the original linear penalties in the original problem. The authors show that the proposed method can achieve better gradients of the gradients in the first-order case with second-orders penalties, and the proposed algorithm can be used to train a classifier in the second order case. The paper also provides a theoretical analysis of the fairness of the algorithm in two-player min-max games, showing that the algorithm is more robust to instability in stochastic mini-batch settings."
9128,SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"Sampling discrete latent variables USED-FOR highvariance gradient estimators. continuous - relaxation methods USED-FOR latter. control - variate schemes USED-FOR former. branch paths PART-OF model. control - variate schemes CONJUNCTION continuous - relaxation methods. continuous - relaxation methods CONJUNCTION control - variate schemes. control - variate schemes USED-FOR state - of - the - art methods. state - of - the - art methods USED-FOR discrete latent - variable models. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. models CONJUNCTION inference networks. inference networks CONJUNCTION models. importance weighted autoencoder COMPARE RWS. RWS COMPARE importance weighted autoencoder. RWS USED-FOR inference networks. RWS USED-FOR models. RWS USED-FOR deep generative models. Method are Discrete latent - variable models, and continuous latentvariable models. OtherScientificTerm is pathwise derivative. ","This paper studies the problem of sampling discrete latent variables for highvariance gradient estimators. Discrete latent-variable models have been studied in the literature for a long time, and it has been shown that the former can be approximated by control-variate schemes and continuous-relaxation methods, while the latter can only approximate the latter. The authors propose a pathwise derivative, where the branch paths in the model are sampled from a set of continuous latent variable models, and the paths are computed using a RWS. They show that the importance weighted autoencoder (RWS) can be used to train the models and inference networks. They also show that it can be combined with state-of-the-art methods for discrete latent-variance models.","This paper proposes a new way of sampling discrete latent variables for highvariance gradient estimators. Discrete latent-variable models can be seen as a special case of continuous latent variable models. The authors propose to use control-variate schemes for the former and continuous-relaxation methods for the latter. The former is based on branch paths in the model, while the latter uses a pathwise derivative. They show that the importance weighted autoencoder (RWS) outperforms the state-of-the-art methods for discrete latent-variance models. They also show that RWS can be applied to deep generative models and inference networks."
9137,SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"human knowledge CONJUNCTION non - differentiable pipelines. non - differentiable pipelines CONJUNCTION human knowledge. non - differentiable pipelines USED-FOR scalar reward function. human knowledge USED-FOR scalar reward function. scalar reward function USED-FOR tasks. truncated randomized search USED-FOR structured prediction energy networks ( SPENs ). truncated randomized search USED-FOR reward function. structured prediction energy networks ( SPENs ) USED-FOR test - time inference. gradient - based search USED-FOR structured prediction energy networks ( SPENs ). gradient - based search USED-FOR test - time inference. truncated randomized search USED-FOR unknown local improvements. supervision USED-FOR SPENs. truncated randomized search USED-FOR supervision. truncated randomized search USED-FOR reward function. Task are structured output prediction tasks, and structured prediction. OtherScientificTerm are output space, and score landscape. Material is labeled training data. ","This paper studies the problem of structured output prediction tasks, where the output space is a set of labeled training data and the goal is to learn a scalar reward function based on human knowledge and non-differentiable pipelines. The authors propose to use a truncated randomized search for the reward function of structured prediction energy networks (SPENs) for test-time inference using gradient-based search. They show that SPENs can be trained with supervision and that the learned reward function can be used for unknown local improvements. ",This paper proposes a novel approach to learning structured output prediction tasks. The authors propose a scalar reward function based on human knowledge and non-differentiable pipelines. The reward function is based on a truncated randomized search for structured prediction energy networks (SPENs) for test-time inference using gradient-based search. The paper also proposes to use supervision for SPENs to improve the performance of the reward function. The proposed approach is evaluated on labeled training data.
9146,SP:638c1bc09992029b78bd83f0127594dcccb96c06,"It USED-FOR transferring policies. simulation environment FEATURE-OF transferring policies. these USED-FOR robust policies. active learning based framework USED-FOR model parameters. EffAcTS HYPONYM-OF active learning based framework. framework USED-FOR method. sample efficiency EVALUATE-FOR approach. EPOpt HYPONYM-OF method. continuous control tasks EVALUATE-FOR approach. Multi - Task Learning perspective USED-FOR Robust Policy Search. framework COMPARE Multi - Task Learning. Multi - Task Learning COMPARE framework. Task is learning policies. OtherScientificTerm are environment model parameters, and policies. Generic is approaches. ","This paper proposes an active learning based framework, EffAcTS, for learning policies in a simulation environment. The proposed method is based on the Multi-Task Learning perspective, where the goal is to learn a set of environment model parameters that are robust to changes in the environment. These parameters are then used to learn robust policies that are transferable across different environments. The authors show that the proposed framework can improve sample efficiency and reduce the cost of the proposed method, EPOpt. Empirically, the proposed approach is shown to perform well on continuous control tasks. ","This paper proposes an active learning based framework, EffAcTS, for learning policies in a simulation environment. The proposed method is based on the Multi-Task Learning perspective for Robust Policy Search. The key idea is to learn environment model parameters that are robust to perturbations in the environment, and then use these to learn robust policies. The authors show that the proposed framework improves sample efficiency and is more robust than other approaches. The approach is evaluated on continuous control tasks."
9155,SP:491c239713a6489f0b1790ca26db54a1813c67ae,"policy evaluation CONJUNCTION control. control CONJUNCTION policy evaluation. value function USED-FOR policy evaluation. value function USED-FOR control. fixed basis CONJUNCTION fixed representation. fixed representation CONJUNCTION fixed basis. algorithms USED-FOR linear function approximation. fixed basis USED-FOR linear function approximation. fixed representation USED-FOR linear function approximation. temporal difference learning CONJUNCTION Q - learning. Q - learning CONJUNCTION temporal difference learning. extensions USED-FOR nonlinear function approximation. Q - learning HYPONYM-OF methods. temporal difference learning HYPONYM-OF methods. nonlinear gradient temporal difference learning HYPONYM-OF nonlinear function approximation. two - timescale network ( TTN ) architecture USED-FOR linear methods. algorithms USED-FOR nonlinear value estimates. algorithms USED-FOR linear setting. data - efficient least - squares methods CONJUNCTION eligibility traces. eligibility traces CONJUNCTION data - efficient least - squares methods. linear policy evaluation algorithms USED-FOR nonlinear value estimates. eligibility traces CONJUNCTION linear policy evaluation algorithms. linear policy evaluation algorithms CONJUNCTION eligibility traces. algorithms USED-FOR approach. linear policy evaluation algorithms HYPONYM-OF algorithms. data - efficient least - squares methods HYPONYM-OF algorithms. eligibility traces HYPONYM-OF algorithms. dependent features FEATURE-OF linear component. nonlinear value function approximation algorithms USED-FOR policy evaluation and control. TTNs COMPARE nonlinear value function approximation algorithms. nonlinear value function approximation algorithms COMPARE TTNs. TTNs USED-FOR policy evaluation and control. Method are reinforcement learning agents, and nonlinear representation. ","This paper proposes a two-timescale network (TTN) architecture for learning nonlinear value function approximations for policy evaluation and control. The main idea is to use a fixed basis and a fixed representation to learn a linear function approximation for the value function in the form of a value function, which is then used to train the reinforcement learning agents. The authors propose two extensions for nonlinear function approximation: nonlinear gradient temporal difference learning and Q-learning. The paper shows that the proposed algorithms can be used in the linear setting, where the dependent features of the linear component are replaced by a nonlinear representation. The proposed approach is evaluated on a variety of data-efficient least-squares methods as well as linear policy evaluation algorithms and eligibility traces. The results show that TTNs are able to achieve better performance than other existing linear value function approximation algorithms in both the linear and nonlinear setting.","This paper proposes a two-timescale network (TTN) architecture for learning nonlinear value function approximations for reinforcement learning agents. The authors propose two extensions for nonlinear function approximation: nonlinear gradient temporal difference learning and Q-learning. The fixed basis is used for the linear function approximation and the fixed representation for the value function for policy evaluation and control. The nonlinear representation is based on dependent features of the linear component. The proposed approach is evaluated on two datasets and compared to existing algorithms in the linear setting: data-efficient least-squares methods and linear policy evaluation algorithms. The results show that the proposed TTNs outperform existing nonlinear values function approximation algorithms in both policy evaluation, control, and eligibility traces."
9164,SP:327d606cf3813b00a009a7785e08ef9e11f89493,"intrinsic semantic regularities PART-OF man - made environments. multi - target sub - policy CONJUNCTION Bayesian model. Bayesian model CONJUNCTION multi - target sub - policy. visual inputs USED-FOR multi - target sub - policy. semantic structures USED-FOR Bayesian model. Bayesian model PART-OF LEArning and Planning with Semantics ( LEAPS ). multi - target sub - policy PART-OF LEArning and Planning with Semantics ( LEAPS ). House3D HYPONYM-OF 3D environment. real - world objects FEATURE-OF human - designed indoor scenes. human - designed indoor scenes PART-OF 3D environment. House3D USED-FOR visual navigation tasks. LEAPS COMPARE baselines. baselines COMPARE LEAPS. semantic content USED-FOR baselines. Method are deep reinforcement learning agents, semantic model, and sub - policy. Task is AI. ","This paper studies the problem of learning deep reinforcement learning agents with intrinsic semantic regularities in man-made environments. The authors propose LEArning and Planning with Semantics (LEAPS), a multi-target sub-policy and a Bayesian model that uses visual inputs to learn the semantic structures of the sub-policies. The semantic model is trained to predict the actions of sub-problems in the environment. The paper shows that LEAPS performs better than other baselines in terms of semantic content compared to existing baselines. The main contribution of the paper is the use of House3D, a 3D environment with real-world objects in human-designed indoor scenes, to train visual navigation tasks.","This paper proposes a new approach to learn deep reinforcement learning agents that can be applied to man-made environments with intrinsic semantic regularities. Specifically, the authors propose LEArning and Planning with Semantics (LEAPS), which combines a multi-target sub-policy and a Bayesian model with visual inputs. The semantic model consists of two sub-problems: (1) learning a semantic model that predicts the sub-goal, and (2) learning the semantic structures that govern the behavior of the sub -policy. The authors show that LEAPS outperforms other baselines on visual navigation tasks in House3D, a 3D environment with real-world objects and human-designed indoor scenes."
9173,SP:d7c26f43bc68d160095b1f50447528843d79edbd,"multi - task perception - related basic knowledge CONJUNCTION driving knowledge. driving knowledge CONJUNCTION multi - task perception - related basic knowledge. perception module CONJUNCTION driving module. driving module CONJUNCTION perception module. perception module PART-OF driving model. driving module PART-OF driving model. driving knowledge USED-FOR it. multi - task perception - related basic knowledge USED-FOR it. segmentation map CONJUNCTION depth map. depth map CONJUNCTION segmentation map. control commands USED-FOR difficult driving task. depth map USED-FOR easier drivingrelated perception problems. depth map CONJUNCTION pixel level understanding of images. pixel level understanding of images CONJUNCTION depth map. generalization CONJUNCTION accident explanation ability. accident explanation ability CONJUNCTION generalization. multitask perception knowledge USED-FOR accident explanation ability. multitask perception knowledge USED-FOR generalization. method COMPARE benchmark method. benchmark method COMPARE method. average sucess rate EVALUATE-FOR navigation tasks. average sucess rate EVALUATE-FOR benchmark method. trained weather CONJUNCTION untrained weathers. untrained weathers CONJUNCTION trained weather. method USED-FOR navigation tasks. average sucess rate EVALUATE-FOR method. Method are deep learning driving models, and driving models. OtherScientificTerm is unobserved driving environment. Material is diversity of training driving dataset. ","This paper studies the problem of training deep learning driving models in an unobserved driving environment. The authors propose a new method that combines multi-task perception-related basic knowledge with driving knowledge. The driving model consists of a perception module, a driving module, and a vision module. In the vision module, the depth map and the pixel level understanding of images are used to train the driving model. The depth map is used to learn the control commands for a difficult driving task. The vision module is used for the easier drivingrelated perception problems. The proposed method achieves a better average sucess rate than the benchmark method on a variety of navigation tasks. The generalization and accident explanation ability are improved by using multitask perception knowledge and the driving module. ","This paper proposes a novel approach to improve the generalization of deep learning driving models. The key idea is to combine multi-task perception-related basic knowledge with driving knowledge. The driving model consists of a perception module, a driving module, and the driving module consists of two parts: a segmentation map and a depth map. The depth map is used for easier drivingrelated perception problems, while the vision module is used to learn control commands for a difficult driving task. The authors show that the proposed method achieves better generalization and average sucess rate for navigation tasks compared to the benchmark method. The proposed method is evaluated on a diversity of training driving dataset and on untrained weathers. "
9182,SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,adversarial robustness CONJUNCTION generalization. generalization CONJUNCTION adversarial robustness. accuracy EVALUATE-FOR model. adversarial perturbations FEATURE-OF robustness. robustness EVALUATE-FOR model. robust classifiers COMPARE classifiers. classifiers COMPARE robust classifiers. robust classifiers USED-FOR feature representations. feature representations COMPARE classifiers. classifiers COMPARE feature representations. salient data characteristics CONJUNCTION human perception. human perception CONJUNCTION salient data characteristics. robust models USED-FOR features. ,"This paper studies the robustness of a model trained with adversarial perturbations against adversarial robustness and generalization. The authors show that the accuracy of the model is not affected by the perturbation, and that robust classifiers are able to learn feature representations that are more robust than classifiers. They also show that robust models can learn features from salient data characteristics and human perception.",This paper studies the relationship between adversarial robustness and generalization in the context of adversarial perturbations. The authors show that the robustness of a model is correlated with the accuracy of the model. They also show that robust classifiers can be used to learn feature representations that are more robust than classifiers trained on the same set of features. The paper also shows that robust models can learn features that are similar to human perception.
9191,SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"backpropagation HYPONYM-OF reverse - mode automatic differentiation. reverse - mode automatic differentiation USED-FOR Deep neural networks. method USED-FOR gradient - based training of neural networks. Equilibrium Propagation USED-FOR gradient - based training of neural networks. local learning rules USED-FOR gradient - based training of neural networks. local learning rules USED-FOR method. iterative optimization of neural activations USED-FOR inference. iterative inference procedure USED-FOR Equilibrium propagation. feedforward network USED-FOR iterative inference procedure. feedforward network USED-FOR Initialized Equilibrium Propagation. local learning rule USED-FOR feed - forward network. initializing network USED-FOR inference. initializing network USED-FOR feedforward network. network COMPARE Equilibrium propagation. Equilibrium propagation COMPARE network. backpropagation USED-FOR deep networks. Method is Biological networks. OtherScientificTerm are gradients, neurons, and error gradient. ","This paper proposes a method for reverse-mode automatic differentiation in Deep neural networks, called backpropagation. The proposed method is based on local learning rules for gradient-based training of neural networks. The authors propose an iterative optimization of neural activations for inference using an initializing network and a feedforward network for Initialized Equilibrium Propagation with a local learning rule for the feed-forward network. Experimental results show that the proposed network performs better than Equilibrium propagation with the same number of iterations. ","This paper proposes a new method for gradient-based training of neural networks based on local learning rules. The proposed method is called backpropagation, which is a reverse-mode automatic differentiation for Deep neural networks. The key idea is to use iterative optimization of neural activations for inference. Initialized Equilibrium Propagation is an iterative inference procedure for Equilibrium propagation using a feedforward network with a local learning rule. The initializing network is used for inference, while the feed-forward network can be used for Initialized equilibria. The authors show that the proposed network outperforms the original network in terms of error gradient. "
9200,SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,gradient - free operations CONJUNCTION signSGD. signSGD CONJUNCTION gradient - free operations. gradient - free operations PART-OF ZO - signSGD. signSGD PART-OF ZO - signSGD. latter COMPARE SGD - type algorithms. SGD - type algorithms COMPARE latter. convergence speed EVALUATE-FOR SGD - type algorithms. convergence speed EVALUATE-FOR latter. sign information of gradient estimates USED-FOR latter. ZO - signSGD COMPARE signSGD. signSGD COMPARE ZO - signSGD. gradient estimators USED-FOR ZO - signSGD. gradient estimators USED-FOR convergence. ZO - signSGD CONJUNCTION black - box adversarial attacks. black - box adversarial attacks CONJUNCTION ZO - signSGD. ZO - signSGD USED-FOR robust deep learning. black - box adversarial attacks USED-FOR robust deep learning. ZO - signSGD USED-FOR generation of adversarial examples. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. image classification datasets EVALUATE-FOR ZO - signSGD. CIFAR-10 HYPONYM-OF image classification datasets. black - box neural networks USED-FOR generation of adversarial examples. Metric is convergence rate. OtherScientificTerm is optimization variables. ,"This paper studies the convergence rate of ZO-signSGD with gradient-free operations and signSGD. The authors show that the latter has a higher convergence speed than SGD-type algorithms, and that the sign information of gradient estimates can be used to improve the latter. They also show that gradient estimators can improve the convergence of ZOO-signSVGD with sign SGD. Finally, they show that ZO is able to achieve better performance than ZO on MNIST and CIFAR-10 on image classification datasets. ","This paper proposes a new algorithm for robust deep learning, ZO-signSGD, which combines gradient-free operations and signSGD. The latter is based on the sign information of gradient estimates. The authors show that the latter has a better convergence speed than SGD-type algorithms. They also show that gradient estimators can be used to improve the convergence rate. The paper also provides a theoretical analysis of the optimization variables. Experiments are conducted on MNIST and CIFAR-10 on image classification datasets, showing the effectiveness of the proposed algorithm. "
9209,SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"Deep learning USED-FOR artificial intelligence applications. energy - limited edge device USED-FOR complex neural network model. optimization method USED-FOR convolutional neural networks. multiply - accumulate ( MAC ) operations PART-OF convolutional filter. checkpoint USED-FOR MAC process. fine - tuning process USED-FOR accuracy drop. CIFAR-10 example model CONJUNCTION Network in Network. Network in Network CONJUNCTION CIFAR-10 example model. MAC operations EVALUATE-FOR method. CIFAR-10 and CIFAR-100 datasets EVALUATE-FOR Network in Network. accuracy drop EVALUATE-FOR method. method COMPARE method. method COMPARE method. CIFAR-100 dataset EVALUATE-FOR method. CIFAR-10 dataset EVALUATE-FOR method. CIFAR-10 dataset EVALUATE-FOR method. Method is convolutional operations. OtherScientificTerm are activation or pooling layers, filter, and checkpoints. ","This paper proposes a new optimization method for convolutional neural networks with energy-limited edge device. The proposed method is based on the multiplicative multiply-accentuate (MAC) operations in the convolutionan filter. The authors propose a new checkpoint for the MAC process, which is a fine-tuning process to improve the accuracy drop. The method is evaluated on the CIFAR-10 example model and the Network in Network. The results show that the proposed method achieves better accuracy drop than the state-of-the-art method.","This paper proposes a new optimization method for convolutional neural networks with an energy-limited edge device for training a complex neural network model. The proposed method is based on multiply-accelerate (MAC) operations in the convolutionals filter. The authors propose to use activation or pooling layers in the filter, and then apply a checkpoint to the MAC process. They show that the proposed method can achieve better accuracy drop than the standard fine-tuning process. The method is evaluated on CIFAR-10 and CifAR-100 datasets, and on Network in Network. "
9218,SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"adversarial examples USED-FOR neural network models. unique data properties USED-FOR learning principles. temporal dependency USED-FOR adversarial examples. temporal dependency USED-FOR discriminate power. temporal dependency FEATURE-OF audio data. automatic speech recognition ( ASR ) tasks CONJUNCTION audio adversarial attacks. audio adversarial attacks CONJUNCTION automatic speech recognition ( ASR ) tasks. temporal dependency USED-FOR discriminative power. image adversarial defense USED-FOR input transformation. robustness EVALUATE-FOR ASR systems. domain - specific data properties USED-FOR adversarial examples. OtherScientificTerm are adversarial inputs, audio adversarial examples, and adaptive attacks. ","This paper studies the problem of learning adversarial examples for neural network models with unique data properties. The authors propose to use temporal dependency on the audio data as a measure of the discriminative power, which is used to measure the ability of the adversarial inputs to discriminate power. They show that using domain-specific data properties can be used to learn adversarial example with domain-dependent data properties, which can be useful for learning principles. They also provide an image adversarial defense against input transformation, and show that adaptive attacks can improve the robustness of ASR systems. ","This paper studies adversarial examples for neural network models. The authors propose to use unique data properties for learning principles. The key idea is to use temporal dependency on adversarial example to improve the discriminative power, which is a measure of the temporal dependency of audio data with respect to adversarial inputs. This is done by using domain-specific data properties. The paper also proposes to use image adversarial defense to prevent input transformation and adaptive attacks. Experiments on automatic speech recognition (ASR) tasks and audio adversarial attacks show that the proposed ASR systems achieve better robustness."
9227,SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"They USED-FOR representations. images USED-FOR approaches. core inductive biases USED-FOR approaches. generator USED-FOR GAN. composition USED-FOR images. real - world images USED-FOR generative model. multi - object image datasets EVALUATE-FOR approach. generative model USED-FOR images. reference distribution FEATURE-OF images. Method are Deep generative models, and object representations. OtherScientificTerm is representational level. ","This paper proposes a new generative model for generating images from real-world images. Deep generative models can be used to learn representations from images, but they can be biased by core inductive biases. The proposed approach is based on a GAN with a generator that generates the images from a composition of images. The generated images are then passed through a generative network to generate the object representations. The resulting representations are then used to train the GAN. The approach is evaluated on multi-object image datasets. The results show that the generated images have a reference distribution that is similar to the reference distribution of the original images. ","This paper proposes a new approach to learn representations from images. Deep generative models can be seen as an extension of GANs. The key idea is to learn a generative model that generates images from real-world images, and then use this generator to generate images from the composition of the images. The approach is evaluated on multi-object image datasets, and the results show that the proposed approach outperforms other approaches that rely on core inductive biases. "
9236,SP:fb59990b8da0e95d8202383478a456667de60449,"Learning disentangled representations USED-FOR computer vision tasks. visual data USED-FOR Learning disentangled representations. referencebased disentangling HYPONYM-OF learning setting. deep generative model USED-FOR weak supervisory signal. reference - based variational autoencoders HYPONYM-OF deep generative model. reference set USED-FOR weak supervisory signal. adversarial learning USED-FOR objective function. adversarial learning USED-FOR variational inference framework. variational inference framework USED-FOR training. model USED-FOR disentangled representations. feature learning CONJUNCTION conditional image generation. conditional image generation CONJUNCTION feature learning. conditional image generation CONJUNCTION attribute transfer. attribute transfer CONJUNCTION conditional image generation. tasks EVALUATE-FOR model. minimal supervision USED-FOR model. attribute transfer HYPONYM-OF tasks. feature learning HYPONYM-OF tasks. conditional image generation HYPONYM-OF tasks. minimal supervision USED-FOR disentangled representations. OtherScientificTerm are high - level generative factors, target factors, supervision, and factors of interest. Method is Supervised approaches. Generic is representation. ","This paper studies the problem of learning disentangled representations for computer vision tasks from visual data. The authors propose a deep generative model, reference-based variational autoencoders, for this learning setting. The proposed model is able to learn disenangled representations with minimal supervision. The objective function is learned by adversarial learning. The model is evaluated on three tasks: feature learning, conditional image generation, and attribute transfer.","This paper proposes a new learning setting, referencebased disentangling, for learning disentangled representations from visual data. The key idea is to use a deep generative model with reference-based variational autoencoders to generate a weak supervisory signal from the reference set. The authors propose a variational inference framework for training the model to generate disenangled representations. The objective function is learned by adversarial learning. The model is evaluated on three tasks: feature learning, conditional image generation, and attribute transfer. Supervised approaches are used to learn the representation of the high-level generative factors, and the target factors are used for supervision. "
9245,SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"Deep neural network models USED-FOR rapid online adaptation. method USED-FOR continual online learning. deep neural network models USED-FOR method. mixture of models USED-FOR non - stationary task distributions. expectation maximization algorithm USED-FOR mixture of models. stochastic gradient descent USED-FOR model parameters. expectation maximization algorithm USED-FOR non - stationary task distributions. Chinese restaurant process USED-FOR expectation maximization algorithm. stochastic gradient descent USED-FOR online learning procedure. models COMPARE models. models COMPARE models. meta - learning USED-FOR model. SGD USED-FOR online adaptation. meta - learning USED-FOR online learning ( MOLe ) approach. motor failures CONJUNCTION unexpected disturbances. unexpected disturbances CONJUNCTION motor failures. varying terrains CONJUNCTION motor failures. motor failures CONJUNCTION varying terrains. MOLe COMPARE prior methods. prior methods COMPARE MOLe. MOLe USED-FOR continuous adaptation. continuous adaptation USED-FOR non - stationary task distributions. predictive model USED-FOR control. meta - learning USED-FOR model - based reinforcement learning. online learning ( MOLe ) approach USED-FOR model - based reinforcement learning. unexpected disturbances HYPONYM-OF non - stationary task distributions. varying terrains HYPONYM-OF non - stationary task distributions. motor failures HYPONYM-OF non - stationary task distributions. Method are predictive models, and large function approximators. OtherScientificTerm is real - world phenomena. ","This paper proposes a method for continual online adaptation using deep neural network models for rapid online adaptation. The proposed method is based on meta-learning, where a mixture of models is trained on non-stationary task distributions with the expectation maximization algorithm based on the Chinese restaurant process. The online learning procedure uses stochastic gradient descent to learn the model parameters, and then uses SGD to perform online adaptation via SGD. The authors show that the proposed MOLe outperforms prior methods in continuous adaptation for non-stable task distributions such as motor failures, unexpected disturbances, and varying terrains. They also show that predictive models can be used to control the performance of the predictive model. ","This paper proposes a method for continual online learning with deep neural network models for rapid online adaptation. The proposed method is based on meta-learning. The authors propose an expectation maximization algorithm for the mixture of models for non-stationary task distributions with varying terrains and motor failures. The model parameters are learned by stochastic gradient descent with a Chinese restaurant process. The online learning procedure uses SGD for the online adaptation, and the predictive models are trained with large function approximators. The predictive model is used for control. The paper shows that the proposed MOLe outperforms prior methods for continuous adaptation on non-stochastic task distributions such as unexpected disturbances, motor failures, and varying terains. "
9254,SP:5665e5f006f84927beb0440e145f476e02538077,"distributed prioritized experience replay USED-FOR RNN - based RL agents. representational drift CONJUNCTION recurrent state staleness. recurrent state staleness CONJUNCTION representational drift. parameter lag USED-FOR representational drift. single network architecture CONJUNCTION hyperparameters. hyperparameters CONJUNCTION single network architecture. single network architecture USED-FOR agent. Recurrent Replay Distributed DQN HYPONYM-OF agent. It HYPONYM-OF agent. human - level performance EVALUATE-FOR It. human - level performance EVALUATE-FOR agent. Atari games EVALUATE-FOR It. Method are distributed training of RL agents, and training strategy. Material are Atari-57, and DMLab-30. ","This paper studies distributed prioritized experience replay for RNN-based RL agents. The authors consider the problem of distributed training of RL agents, and propose a training strategy called Recurrent Replay Distributed DQN. It combines a single network architecture with hyperparameters and a parameter lag to reduce representational drift and recurrent state staleness. It is shown to improve the human-level performance on Atari games. ","This paper proposes a distributed prioritized experience replay for RNN-based RL agents. It is based on Recurrent Replay Distributed DQN, where the agent is trained using a single network architecture and a set of hyperparameters. The authors show that the representational drift and recurrent state staleness are caused by the parameter lag. They also show that It improves the human-level performance of the agent on Atari games. "
9263,SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,sequential generative models USED-FOR coordinated multi - agent trajectory behavior. offensive basketball gameplay HYPONYM-OF coordinated multi - agent trajectory behavior. hierarchical models USED-FOR long - term coordination. hierarchical models USED-FOR settings. intermediate variables USED-FOR hierarchical models. intermediate variables USED-FOR high - level behavioral semantics. hierarchical framework USED-FOR sequential generative models. programmatically produced weak labels USED-FOR spatiotemporal regime. programmatically produced weak labels USED-FOR approach. framework USED-FOR complex interactions between basketball players. framework USED-FOR realistic multi - agent trajectories of basketball gameplay. quantitative and qualitative evaluations EVALUATE-FOR approach. OtherScientificTerm is synthetic settings. ,This paper proposes a hierarchical framework for learning sequential generative models for coordinated multi-agent trajectory behavior in offensive basketball gameplay. The approach is based on programmatically produced weak labels for the spatiotemporal regime. The authors show that hierarchical models can be used to learn long-term coordination between agents in these settings. The intermediate variables are then used to train hierarchical models for high-level behavioral semantics. Experiments on synthetic settings demonstrate the effectiveness of the proposed approach on both quantitative and qualitative evaluations. ,This paper proposes a hierarchical framework for learning sequential generative models for coordinated multi-agent trajectory behavior in offensive basketball gameplay. The approach is based on programmatically produced weak labels for the spatiotemporal regime. The authors show that hierarchical models can learn long-term coordination in these settings with intermediate variables for high-level behavioral semantics. Experiments on synthetic settings show that the proposed framework can learn complex interactions between basketball players. The proposed approach is evaluated on both quantitative and qualitative evaluations.
9272,SP:1a90cdf028068528b0559e7d44bf26dda20310bd,vision model USED-FOR interacting agents. method USED-FOR temporal information. ambiguous visual information USED-FOR dynamics model. dynamics model USED-FOR method. method COMPARE baselines. baselines COMPARE method. one CONJUNCTION one. one CONJUNCTION one. one EVALUATE-FOR method. one EVALUATE-FOR method. soccer game engine USED-FOR one. real basketball trajectories USED-FOR one. one HYPONYM-OF sports datasets. one HYPONYM-OF sports datasets. sports datasets EVALUATE-FOR method. sports datasets EVALUATE-FOR baselines. ,This paper proposes a new vision model for interacting agents. The proposed method uses ambiguous visual information to extract temporal information from the dynamics model. The method is evaluated on two sports datasets: one with real basketball trajectories and one with a soccer game engine. The results show that the proposed method outperforms baselines on both.,"This paper proposes a vision model for interacting agents. The authors propose a method to capture temporal information from ambiguous visual information. The method is based on a dynamics model. The proposed method is evaluated on two sports datasets, one with a soccer game engine, and one with real basketball trajectories. The results show that the proposed method outperforms baselines. "
9281,SP:8392f04b7265f665ba6d44d297bca245d44b4708,"Deep neural networks USED-FOR approximating complicated functions. gradient descent methods USED-FOR Deep neural networks. neural network USED-FOR functionality. method USED-FOR end - to - end training. base neural network USED-FOR end - to - end training. method USED-FOR base neural network. differentiable neural network USED-FOR black - box functionality. differentiable estimator CONJUNCTION external blackbox non - differentiable counterpart. external blackbox non - differentiable counterpart CONJUNCTION differentiable estimator. neural network USED-FOR input to blackbox functionality. Estimate and Replace ” paradigm USED-FOR neural network. black - box function USED-FOR integrated model. integrated model COMPARE fully differentiable model. fully differentiable model COMPARE integrated model. black - box function USED-FOR inference. black - box function USED-FOR fully differentiable model. integrated model COMPARE RL - based methods. RL - based methods COMPARE integrated model. Task are training, and end - to - end optimization process. Generic is task. OtherScientificTerm are black - box functions, blackbox functions, black - box function interface, and intermediate labels. Method is base network. ","This paper proposes a method for end-to-end training of a base neural network for approximating complicated functions in Deep neural networks with gradient descent methods. The main idea is to use a differentiable neural network to learn the functionality of the input to blackbox functionality using a neural network trained on the base network. The black-box functions are defined as functions that can be approximated by a blackbox function interface. The authors propose a new “Estimate and Replace” paradigm for training the neural network, which is based on the “differentiable estimator” and its external blackbox non-differentiable counterpart. The integrated model is shown to perform better than a fully differentiable model trained with a black -box function for inference and inference with the integrated model. ","This paper proposes a method for end-to-end training of deep neural networks for approximating complicated functions. Deep neural networks have been widely used in gradient descent methods for training, and the authors propose a new way to train a base neural network for the purpose of approximating the functionality of the black-box functions. The main idea is to use a differentiable estimator and an external blackbox non-differentiable counterpart to estimate the functionality, and then use a neural network to learn the input to blackbox functionality. The authors propose to use the “Estimate and Replace” paradigm to train the neural network, where the blackbox functions are replaced with intermediate labels. The proposed integrated model is shown to outperform the fully differentiable model with the same number of parameters and the same amount of training data, while the integrated model outperforms RL-based methods in inference. "
9290,SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,learning USED-FOR task. data - driven inductive bias USED-FOR learning. gradient - based meta - learning CONJUNCTION hierarchical Bayes. hierarchical Bayes CONJUNCTION gradient - based meta - learning. function approximator USED-FOR mixture of hierarchical Bayesian models. neural network HYPONYM-OF function approximator. stochastic expectation maximization procedure USED-FOR parameter initializations. parameter initializations USED-FOR gradient descent. stochastic expectation maximization procedure USED-FOR gradient descent. stochastic expectation maximization procedure USED-FOR latent assignment of tasks. initializations USED-FOR latent assignment of tasks. approach USED-FOR diversity of training tasks. inductive biases PART-OF hyperparameters. miniImageNet benchmark EVALUATE-FOR 1 - shot classification. miniImageNet benchmark EVALUATE-FOR generalization. method USED-FOR task distribution. non - parametric variant USED-FOR task distribution. method USED-FOR non - parametric variant. few - shot regression tasks EVALUATE-FOR non - parametric variant. OtherScientificTerm is transfer. ,"This paper studies the problem of learning a task with data-driven inductive bias. The authors propose a function approximator for the mixture of hierarchical Bayesian models trained on a neural network. The main idea is to use stochastic expectation maximization procedure for parameter initializations for gradient descent and hierarchical Bayes. The proposed approach is shown to improve the diversity of training tasks and improve generalization on the miniImageNet benchmark for 1-shot classification. Finally, the authors show that the proposed method improves the task distribution on a few-shot regression tasks. ",This paper proposes a new method for learning a task with data-driven inductive bias. The authors propose a mixture of hierarchical Bayesian models with a function approximator on the input of a neural network. The main idea is to use parameter initializations for gradient descent with a stochastic expectation maximization procedure for the latent assignment of tasks. This approach is shown to improve the diversity of training tasks and improve generalization on the miniImageNet benchmark for 1-shot classification. The paper also proposes a non-parametric variant of the proposed method for the task distribution on few-shot regression tasks. 
9299,SP:a410144dbe19713a06c63da87d9fb58b999a7492,Auxiliary learning USED-FOR principal task. domain knowledge USED-FOR manually - defined auxiliary tasks. auxiliary tasks USED-FOR auxiliary tasks. Meta Auxiliary Learning ( MAXL ) USED-FOR image classification. hierarchical sub - class image classification HYPONYM-OF auxiliary task. meta learner USED-FOR sub - class target labels. meta learner USED-FOR multi - task evaluator. MAXL COMPARE baseline auxiliary learning methods. baseline auxiliary learning methods COMPARE MAXL. CIFAR datasets EVALUATE-FOR MAXL. MAXL COMPARE method. method COMPARE MAXL. CIFAR datasets EVALUATE-FOR baseline auxiliary learning methods. human - defined sub - class hierarchies USED-FOR method. MAXL USED-FOR automated generalisation. OtherScientificTerm is human knowledge. ,"This paper proposes Meta Auxiliary Learning (MAXL) for image classification, a hierarchical sub-class image classification task. The main idea of MAXL is to use a meta learner to learn the sub-classes of the principal task, and then use the learned representations to train a multi-task evaluator. The authors show that MAXL outperforms baseline auxiliary learning methods on CIFAR datasets. ","This paper proposes Meta Auxiliary Learning (MAXL) for image classification, which is a hierarchical sub-class image classification task. The main idea of MAXL is to use domain knowledge for manually-defined auxiliary tasks. The authors propose a meta learner to learn sub-classes of the principal task, and a multi-task evaluator to evaluate the performance of the proposed method. The proposed method is evaluated on CIFAR datasets, and compared to baseline auxiliary learning methods. MAXL outperforms the baseline in terms of automated generalisation, and is able to generalize to human knowledge. "
9308,SP:76248e1c914c60ce69de244fe7ec62488d01e161,neural network based representation USED-FOR open set recognition problem. datasets EVALUATE-FOR approaches. Generic is representation. ,This paper proposes a neural network based representation for the open set recognition problem. The proposed representation is based on the NeurIPS framework. The authors show that the proposed representation can achieve better performance than the state-of-the-art on a variety of datasets. ,This paper proposes a neural network based representation for the open set recognition problem. The authors show that the proposed representation is more robust to adversarial attacks. The proposed approaches are evaluated on three datasets.
9317,SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"ResNet-34 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-34. ResNet-50 CONJUNCTION ResNet-152. ResNet-152 CONJUNCTION ResNet-50. Inception - v3 CONJUNCTION densenet161. densenet161 CONJUNCTION Inception - v3. densenet161 CONJUNCTION VGG-16bn networks. VGG-16bn networks CONJUNCTION densenet161. ResNet-152 CONJUNCTION Inception - v3. Inception - v3 CONJUNCTION ResNet-152. accuracy EVALUATE-FOR full - precision baseline networks. finetuning USED-FOR full - precision baseline networks. ImageNet classification benchmark EVALUATE-FOR VGG-16bn networks. ResNet-18 CONJUNCTION ResNet-34. ResNet-34 CONJUNCTION ResNet-18. accuracy EVALUATE-FOR full - precision baseline networks. stochastic gradient descent USED-FOR training error. pretrained fp32 precision baseline networks CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pretrained fp32 precision baseline networks. pretrained fp32 precision baseline networks USED-FOR solution distance. matched learning rate annealing USED-FOR combat noise. techniques USED-FOR low - precision networks. techniques CONJUNCTION activation function range calibration. activation function range calibration CONJUNCTION techniques. activation function range calibration USED-FOR low - precision networks. Task is embedded deep network inference. Metric are energy and area efficiency, and energy. Method are pretrained models, and fp32 precision baseline networks. Generic are 4 - bit models, baseline networks, noise, and they. OtherScientificTerm are cosine similarity, gradient noise, quantization, and maximum variance of the gradient estimates. ","This paper studies the problem of embedded deep network inference. The authors propose a new method for training 4-bit models with cosine similarity. The main idea is to learn a set of baseline networks that are fp32 precision, and then fine-tune the training error using stochastic gradient descent. Theoretical results show that the finetuning improves the accuracy of full-precision baseline networks, and the performance of VGG-16bn networks on the ImageNet classification benchmark. ","This paper proposes a new method for embedded deep network inference. The main idea is to use 4-bit models as baseline networks, where the cosine similarity between the training data and the test data is computed. The authors show that the energy and area efficiency of the baseline networks can be improved by quantizing the noise, and they also show that they can improve the accuracy of full-precision baseline networks by finetuning. They show that fp32 precision baseline networks are better than pretrained models, and that training error can be reduced by stochastic gradient descent.  The authors also propose two techniques to improve the performance of low-principality networks: activation function range calibration and fine-tuning.  "
9326,SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,approach USED-FOR surface properties. model USED-FOR post - bounce trajectories. bouncing restitution CONJUNCTION effective collision normals. effective collision normals CONJUNCTION bouncing restitution. model USED-FOR physical properties. sensor inputs USED-FOR post - bounce trajectories. physical properties USED-FOR bouncing restitution. physical properties USED-FOR effective collision normals. sensor inputs USED-FOR model. Physics Inference Module ( PIM ) CONJUNCTION Visual Inference Module ( VIM ). Visual Inference Module ( VIM ) CONJUNCTION Physics Inference Module ( PIM ). modules PART-OF model. Bounce PART-OF model. Visual Inference Module ( VIM ) HYPONYM-OF modules. Physics Inference Module ( PIM ) HYPONYM-OF modules. Visual Inference Module ( VIM ) PART-OF model. PIM USED-FOR physical interactions. PIM USED-FOR prediction task. VIM USED-FOR physical parameters. physical parameters CONJUNCTION observed pre - collision 3D trajectories. observed pre - collision 3D trajectories CONJUNCTION physical parameters. physical interactions USED-FOR prediction task. physical parameters USED-FOR PIM. observed pre - collision 3D trajectories USED-FOR PIM. dataset EVALUATE-FOR model. dataset EVALUATE-FOR baselines. predicting post - bounce trajectories CONJUNCTION physical properties. physical properties CONJUNCTION predicting post - bounce trajectories. model COMPARE baselines. baselines COMPARE model. model USED-FOR predicting post - bounce trajectories. model USED-FOR physical properties. trajectory fitting USED-FOR post - bounce trajectories. Newtonian physics USED-FOR trajectory fitting. trajectory fitting HYPONYM-OF baselines. Material is Bounce Dataset. OtherScientificTerm is physics simulations. ,"This paper proposes a new approach to learn surface properties from sensor inputs for post-bounce trajectories. The proposed model consists of three modules: Physics Inference Module (PIM), Visual Inference module (VIM) and Bounce. The PIM learns the physical parameters and the physical properties for bouncing restitution and effective collision normals. The VIM learns physical parameters for the physical interactions and the Bounce learns the physics simulations. The model is evaluated on the Bounce Dataset and shows that the proposed model outperforms the baselines on the dataset. ","This paper proposes a new approach to learn surface properties. The proposed model is a combination of three modules: Physics Inference Module (PIM) and Visual Inference module (VIM). The PIM is used to learn physical interactions between the physical parameters of the PIM and the observed pre-collision 3D trajectories, while the VIM learns the physical properties of the bouncing restitution and the effective collision normals. The model is then used to predict post-bounce trajectories based on the sensor inputs for post-boom trajectories. Experiments on the Bounce Dataset show that the proposed model outperforms baselines in terms of trajectory fitting and Newtonian physics. "
9335,SP:010bd055310c363d3cb0fbe0e11546de58220e15,"neural networks USED-FOR adversarial images. gradients USED-FOR adversarial vulnerability. ` 1 - norm FEATURE-OF gradients. OtherScientificTerm are targeted but imperceptible image perturbations, image size, and network ’s weight distribution. Method is network architectures. Generic is nets. ","This paper studies the problem of adversarial image perturbations in neural networks. The authors consider the case where the target image is imperceptible to perturbation and the network architectures are not robust. They show that the gradients of the network’s weight distribution have a `1-norm’, which can be used to improve the adversarial vulnerability. They also provide a theoretical analysis of the effect of these gradients on the image size. ","This paper studies adversarial vulnerability of neural networks to adversarial images. The authors show that adversarial networks are vulnerable to targeted but imperceptible image perturbations. They show that the gradients of the network’s weight distribution are the `1-norm’ of the adversarial vulnerabilities. They also show that these gradients are a function of the image size, and that the network architectures can be seen as a weighted sum of the weights of these nets."
9351,SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"agent modeling USED-FOR mind model. probing USED-FOR agent modeling. pure curiosity - driven reinforcement learning USED-FOR probing policy. imitation learning USED-FOR approximated agent model. pure curiosity - driven reinforcement learning HYPONYM-OF learning processes. learning processes PART-OF framework. pure curiosity - driven reinforcement learning PART-OF framework. imitation learning PART-OF framework. imitation learning HYPONYM-OF learning processes. tasks EVALUATE-FOR approach. collaboration CONJUNCTION competition. competition CONJUNCTION collaboration. passive observation CONJUNCTION random probing. random probing CONJUNCTION passive observation. agent model COMPARE ones. ones COMPARE agent model. distilling optimal planning CONJUNCTION collaboration. collaboration CONJUNCTION distilling optimal planning. random probing CONJUNCTION curiositydriven approaches. curiositydriven approaches CONJUNCTION random probing. distilling optimal planning CONJUNCTION policy net. policy net CONJUNCTION distilling optimal planning. curiositydriven approaches USED-FOR ones. random probing USED-FOR agent model. approach USED-FOR agent model. competition HYPONYM-OF applications. distilling optimal planning HYPONYM-OF applications. passive observation USED-FOR agent model. passive observation USED-FOR ones. random probing USED-FOR ones. collaboration HYPONYM-OF applications. Method are interactive agent modeling scheme, and probing agent. ","This paper proposes a new agent modeling for mind model. The framework consists of two learning processes: pure curiosity-driven reinforcement learning for probing policy and imitation learning for approximating an approximated agent model. In the interactive agent modeling scheme, the probing agent is trained with a probing agent. The proposed approach is evaluated on a variety of tasks. The agent model is shown to perform better than existing ones in terms of distilling optimal planning, collaboration, competition, and policy net. ","This paper proposes an approach to improve the performance of an agent modeling for mind model. The framework consists of two learning processes: pure curiosity-driven reinforcement learning for the probing policy and imitation learning for an approximated agent model. Experiments are conducted on three different tasks to demonstrate the effectiveness of the approach. The agent model is shown to outperform existing ones in terms of performance, including ones based on passive observation, random probing, and curiositydriven approaches. The paper also presents an interactive agent modeling scheme where the probing agent is trained with a policy net. The proposed approach is evaluated on three applications: collaboration, competition, and distilling optimal planning."
9367,SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"modification USED-FOR Artificial Neural Networks ( ANNs ). Artificial Neural Networks ( ANNs ) USED-FOR ANNs. firing modes FEATURE-OF biological neuron. peripheral factors FEATURE-OF biological neuron. neuromodulators HYPONYM-OF peripheral factors. modification USED-FOR ANN nodes. ANN nodes USED-FOR activation sensitivities. Convolutional Neural Networks CONJUNCTION Long Short - Term Memory networks. Long Short - Term Memory networks CONJUNCTION Convolutional Neural Networks. modification COMPARE ANN nodes. ANN nodes COMPARE modification. ANN nodes USED-FOR Convolutional Neural Networks. ANN nodes USED-FOR Long Short - Term Memory networks. OtherScientificTerm are biological neurons, Biological neurons, biological neuromodulators, modulators, and slope of the activation function. ","This paper proposes a modification to Neural Networks (ANNs) to improve the activation sensitivities of biological neurons. The modification is based on the fact that the firing modes of a biological neuron have multiple peripheral factors (e.g., neuromodulators). The authors show that by modifying the activation function of the biological neurons, the modulators can be used to modify the parameters of the ANN nodes. The authors also show that the proposed modification improves the performance of Convolutional Neural Networks and Long Short-Term Memory networks.","This paper proposes a modification to Artificial Neural Networks (ANNs) to improve the performance of ANNs. The authors propose to modify the activation sensitivities of the ANN nodes in order to improve their performance. The modification is based on the observation that biological neurons have multiple firing modes, and that the firing modes of a biological neuron depend on the peripheral factors of the biological neuron (e.g., neuromodulators). The authors show that the modification improves the performance on Convolutional Neural Networks and Long Short-Term Memory networks. "
9383,SP:287a577834fd2820a939a1113b39146a22727491,voice CONJUNCTION pitch. pitch CONJUNCTION voice. neural analysis and synthesis ( NANSY ) framework USED-FOR voice. neural analysis and synthesis ( NANSY ) framework USED-FOR pitch. information bottleneck USED-FOR analysis features. analysis features USED-FOR controllable synthesis. information perturbation USED-FOR training strategy. formant CONJUNCTION pitch. pitch CONJUNCTION formant. pitch CONJUNCTION frequency response. frequency response CONJUNCTION pitch. reconstruction quality CONJUNCTION controllability. controllability CONJUNCTION reconstruction quality. controllability EVALUATE-FOR it. reconstruction quality EVALUATE-FOR it. wav2vec feature CONJUNCTION pitch feature. pitch feature CONJUNCTION wav2vec feature. Yingram USED-FOR self - supervised training. pitch feature CONJUNCTION Yingram. Yingram CONJUNCTION pitch feature. analysis features USED-FOR NANSY. pitch feature HYPONYM-OF analysis features. wav2vec feature HYPONYM-OF analysis features. Yingram HYPONYM-OF analysis features. selfsupervised training USED-FOR NANSY. NANSY USED-FOR multilingual setting. multilingual dataset USED-FOR NANSY. multilingual dataset USED-FOR it. zero - shot voice conversion CONJUNCTION pitch shift. pitch shift CONJUNCTION zero - shot voice conversion. pitch shift CONJUNCTION time - scale modification. time - scale modification CONJUNCTION pitch shift. NANSY USED-FOR zero - shot voice conversion. NANSY USED-FOR applications. NANSY USED-FOR pitch shift. NANSY USED-FOR time - scale modification. time - scale modification HYPONYM-OF applications. zero - shot voice conversion HYPONYM-OF applications. pitch shift HYPONYM-OF applications. Method is synthesis networks. OtherScientificTerm is bottleneck structures. Material is speech data. ,"This paper proposes a neural analysis and synthesis (NANSY) framework for voice, pitch, and frequency response. NANSY is based on the idea of self-supervised training with information perturbation, which is a popular training strategy in the multilingual setting. The key idea is to use the information bottleneck as an information bottleneck to train the analysis features for controllable synthesis. The authors show that the performance of synthesis networks with these bottleneck structures can be improved by selfsupervised learning. The main contribution of the paper is to show that NANSy can be used in a multilingual dataset for a variety of applications such as zero-shot voice conversion, pitch shift, and time-scale modification. The paper also shows that it can improve the reconstruction quality and controllability. ","This paper proposes a neural analysis and synthesis (NANSY) framework for voice and pitch. NANSY is based on the idea that the information bottleneck in the synthesis networks affects the performance of the analysis features. The authors propose a training strategy based on information perturbation to improve the reconstruction quality and controllability of the input speech data. Experiments are conducted on a multilingual dataset and on self-supervised training. The analysis features are used for controllable synthesis, and a wav2vec feature, pitch feature, and Yingram are used in the multilingual setting. "
9408,SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"gradient - based ) bilevel programming framework USED-FOR hyperparameter optimization. overfitting FEATURE-OF validation set. expectation bound USED-FOR cross - validation algorithm. gradient - based algorithms COMPARE cross - validation. cross - validation COMPARE gradient - based algorithms. regularization terms USED-FOR overfitting problem. regularization terms USED-FOR gradient - based algorithms. overfitting problem PART-OF gradient - based algorithms. outer and inner levels FEATURE-OF regularization terms. feature learning CONJUNCTION data reweighting. data reweighting CONJUNCTION feature learning. data reweighting USED-FOR noisy labels. OtherScientificTerm are optimization properties, and uniform stability. Task is generalization. Method is bilevel programming. ",This paper proposes a new (gradient-based) bilevel programming framework for hyperparameter optimization. The authors consider the overfitting problem in the validation set of a validation set where the optimization properties are not uniform. They show that gradient-based algorithms with regularization terms on the outer and inner levels are more robust to overfitting than cross-validation algorithm with expectation bound. They also show that feature learning and data reweighting can be used to improve the generalization performance of noisy labels. ,"This paper proposes a new (gradient-based) bilevel programming framework for hyperparameter optimization. The authors show that the overfitting in the validation set is due to overfitting of the optimization properties, and that the cross-validation algorithm is based on the expectation bound. They show that gradient-based algorithms with regularization terms that are similar to those of cross-validation are more robust to this overfitting problem. They also show that with the same number of parameters, the regularization term of the regularized version of the gradient is the same as the one of the original version. They further show that under certain assumptions on the outer and inner levels, the cross validation algorithm converges to the one with uniform stability. Finally, they show that feature learning and data reweighting can be used to detect noisy labels. "
9433,SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"knowledge distillation approach USED-FOR transfer of dark knowledge. student models USED-FOR methods. algorithm USED-FOR student - friendly representations. algorithm USED-FOR student branches. knowledge distillation methods USED-FOR student models. accuracy CONJUNCTION convergence speed. convergence speed CONJUNCTION accuracy. approach USED-FOR teacher models. technique USED-FOR student models. technique USED-FOR knowledge distillation methods. convergence speed EVALUATE-FOR student models. knowledge distillation techniques EVALUATE-FOR algorithm. teacher and student models USED-FOR knowledge distillation techniques. accuracy EVALUATE-FOR algorithm. Task are knowledge transfer, and knowledge distillation procedure. Method are teacher model, and teacher networks. ","This paper proposes a knowledge distillation approach for the transfer of dark knowledge from teacher to student. The idea is to distill the knowledge from the teacher model to the student model, and then use the student models to learn the student-friendly representations. The authors propose an algorithm to learn student branches from the student branches. The algorithm is based on the idea that the teacher and student models can be trained in a similar way. The proposed approach is evaluated on a variety of datasets and shows that the proposed technique improves the performance of the teacher models and the resulting student models in terms of accuracy and convergence speed. ","This paper proposes a knowledge distillation approach for the transfer of dark knowledge from teacher model to student model. The key idea is to use student models to generate student-friendly representations of the teacher model, and then use these methods on the student models. The authors propose an algorithm to learn student branches of the original teacher model and then apply the algorithm to the student branches. The algorithm is evaluated on several datasets and shows better accuracy and convergence speed compared to the state-of-the-art teacher and student models on both knowledge distillations techniques. "
9458,SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"Generalization PART-OF machine learning. invariant features USED-FOR algorithms. invariance USED-FOR OOD generalization. generalization USED-FOR out - of - distribution. expansion function USED-FOR OOD generalization. model selection module PART-OF OOD learning algorithm. model selection criterion FEATURE-OF theory. model selection criterion COMPARE baselines. baselines COMPARE model selection criterion. benchmark OOD datasets EVALUATE-FOR model selection criterion. benchmark OOD datasets EVALUATE-FOR baselines. Task are extracting invariant features, OOD, and OOD problem. OtherScientificTerm is OOD generalization error bounds. ","This paper studies the problem of extracting invariant features in machine learning. The authors consider the OOD problem, where the out-of-distribution (OOD) distribution is a function of the model selection module in an OOD learning algorithm. The invariance of OOD generalization can be used to improve the generalization error bounds of the algorithms. The main contribution of the paper is to provide a new model selection criterion for the theory. The paper shows that the proposed new benchmark OOD datasets outperform existing baselines.","This paper proposes a new invariant features for OOD generalization in machine learning. The invariance is a key property of OOD, and the authors propose two algorithms for extracting invariant feature from the out-of-distribution (OOD) distribution. The OOD problem is formulated as a model selection module in an OOD learning algorithm, where the generalization is achieved by an expansion function. The authors show that the proposed model selection criterion outperforms the baselines on three benchmark OOD datasets. "
9483,SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"stationary distribution USED-FOR meta - learning. Dynamic Gaussian Mixture Model USED-FOR meta - parameters. Dynamic Gaussian Mixture Model USED-FOR VC - BML. Chinese Restaurant Process USED-FOR number of component distributions. Dynamic mixtures USED-FOR meta - parameter level. Dynamic mixtures USED-FOR negative knowledge transfer problem. Dynamic mixtures USED-FOR diverse and dissimilar tasks. structured variational inference USED-FOR avoiding forgetting knowledge. posterior approximation method USED-FOR avoiding forgetting knowledge. point estimation method COMPARE posterior approximation method. posterior approximation method COMPARE point estimation method. structured variational inference HYPONYM-OF posterior approximation method. point estimation method USED-FOR posteriors of model parameters. VC - BML USED-FOR catastrophic forgetting. tasks EVALUATE-FOR VC - BML. non - stationary distributions FEATURE-OF tasks. Task is online setting. OtherScientificTerm are non - stationary distribution, and parameter space. ","This paper studies the problem of meta-learning with a stationary distribution in the online setting. The authors propose a Dynamic Gaussian Mixture Model for the meta-parameters of VC-BML, which is based on the Chinese Restaurant Process. Dynamic mixtures can be used to represent the number of component distributions in the parameter space, which can be then used as a proxy for the true non-stationary distribution of the model parameters.  The authors show that the proposed Dynamic Mixture model can be applied to the negative knowledge transfer problem, where the goal is to learn a negative representation of the true parameter space. The paper also shows that the point estimation method for the posteriors of model parameters can be combined with structured variational inference for avoiding forgetting knowledge. ","This paper proposes a new method for meta-learning with a non-stationary distribution. The authors propose a Dynamic Gaussian Mixture Model for the meta-parameters of VC-BML, which is based on the Chinese Restaurant Process. Dynamic mixtures are used at the Meta-parameter level to reduce the number of component distributions in the parameter space. The paper shows that the proposed point estimation method can be used to estimate posteriors of model parameters, and a posterior approximation method based on structured variational inference is used for avoiding forgetting knowledge in the online setting. The proposed method is evaluated on a variety of tasks with non-stochastic distributions, showing that VC-BML can avoid catastrophic forgetting. "
9508,SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"boundary conditions FEATURE-OF ordinary differential equations. it USED-FOR BVPs. Gauss – Markov prior CONJUNCTION it. it CONJUNCTION Gauss – Markov prior. linear time FEATURE-OF posterior distribution. mesh refinement CONJUNCTION hyperparameter adaptation. hyperparameter adaptation CONJUNCTION mesh refinement. uncertainty quantification CONJUNCTION mesh refinement. mesh refinement CONJUNCTION uncertainty quantification. model USED-FOR uncertainty quantification. model USED-FOR mesh refinement. model USED-FOR hyperparameter adaptation. probabilistic BVP solver COMPARE non - probabilistic algorithms. non - probabilistic algorithms COMPARE probabilistic BVP solver. algorithms USED-FOR ODE boundary value problems. first - order problems USED-FOR higher - order problems. manifold learning USED-FOR BVPs. numerical simulation CONJUNCTION probabilistic inference. probabilistic inference CONJUNCTION numerical simulation. lineartime complexity CONJUNCTION adaptive step - size selection. adaptive step - size selection CONJUNCTION lineartime complexity. adaptive step - size selection CONJUNCTION polynomial convergence rates. polynomial convergence rates CONJUNCTION adaptive step - size selection. adaptive step - size selection FEATURE-OF probabilistic solvers. lineartime complexity FEATURE-OF probabilistic solvers. probabilistic solvers USED-FOR initial value problems. Generic are algorithm, and scheme. Method are non - probabilistic methods, statistical modelling tool - chain, and Probabilistic numerical algorithms. Task are Boundary value problems, first - order boundary value problem, machine learning, and Neural Information Processing Systems. OtherScientificTerm are computational pipelines, leftand right - hand side boundary conditions, vector field, ODE knowledge, infectious disease, integration domain, and structured output uncertainty. ","This paper studies the problem of learning a probabilistic BVP solver that can solve ODE boundary value problems in linear time. The authors propose a new algorithm, called BVP, which is based on the Gauss-Markov prior and it is able to solve BVPs with manifold learning. The main idea is to learn the posterior distribution of a posterior distribution with linear time, and then use this posterior distribution as a model for mesh refinement, hyperparameter adaptation, and uncertainty quantification.  The authors show that the proposed algorithm can achieve better performance than non-probabilistic methods on the first-order boundary value problem, and also on the higher-order problems. They also show that BVP can achieve polynomial convergence rates and adaptive step-size selection. ","This paper proposes a new algorithm, called Probabilistic BVP solver, which is a variant of the Gauss-Markov prior. The main idea of the proposed algorithm is to learn the posterior distribution in linear time with respect to the leftand right-hand side boundary conditions of ordinary differential equations. The authors show that the proposed scheme is more efficient than non-probabilistic methods. The proposed algorithm can be viewed as a statistical modelling tool-chain, and the authors provide a theoretical analysis of the algorithm and its scheme. Experiments are conducted on a set of ODE boundary value problems, where the authors show the proposed algorithms can outperform existing algorithms on the ODE frontier value problems. "
9533,SP:86aac0c6b75fdc12f84bba342934865616f866d4,"partially observable system FEATURE-OF near optimal policy. episodic reinforcement learning USED-FOR reward - mixingMarkov decision process ( MDP ). reward models USED-FOR reward function. near optimal policy USED-FOR reward - mixing MDPs. algorithmic and analysis techniques USED-FOR problem. polynomial - time algorithm USED-FOR -optimal policy. observation space COMPARE latent state space. latent state space COMPARE observation space. Task is reinforcement learning. Method are reward model, and switching reward - models. OtherScientificTerm are dynamics, time - horizon, and partially observed environments. Generic are approaches, assumptions, and algorithm. ","This paper studies episodic reinforcement learning in the context of reward-mixingMarkov decision process (MDP). The goal is to learn a near optimal policy in a partially observable system. The reward model is learned by switching reward-models, and the reward models are trained to predict the reward function. The authors propose a polynomial-time algorithm for learning a near-optimal policy in the observation space and the latent state space. The algorithm is based on algorithmic and analysis techniques to solve the problem. The theoretical results show that the dynamics of the reward model can be modeled as a time-horizon, and that the proposed algorithm can be applied to a variety of partially observed environments. ","This paper proposes a new approach to reinforcement learning. The authors propose episodic reinforcement learning for reward-mixingMarkov decision process (MDP), where the goal is to learn a near optimal policy in a partially observable system. The reward model is learned by switching reward-models, and the reward function is learned using reward models. The problem is formulated as a polynomial-time algorithm, where the dynamics are modeled as a time-horizon. The proposed algorithm is evaluated on two partially observed environments. The results show that the proposed approach outperforms existing approaches in terms of performance. "
9558,SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"methods USED-FOR conditional average treatment effect estimation. two - step procedure USED-FOR multi - cause treatment effect. Single - cause Perturbation ( SCP ) USED-FOR multi - cause treatment effect. Single - cause Perturbation ( SCP ) HYPONYM-OF two - step procedure. covariate adjustment USED-FOR estimator. covariate adjustment USED-FOR augmented dataset. It USED-FOR estimator. covariate adjustment USED-FOR It. synthetic and semi - synthetic experiments EVALUATE-FOR SCP. Generic are applications, problem, and procedure. Task are multi - cause treatment effect problems, multi - cause problem, and causal inference. OtherScientificTerm are confounding bias, cause combination, and single - cause interventions. Material is observational dataset. ","This paper proposes a two-step procedure for conditional average treatment effect estimation based on Single-cause Perturbation (SCP) for multi-cause treatment effect. It is based on covariate adjustment for the augmented dataset, which is a simple yet effective way to improve the estimator. The main contribution of the paper is to provide a theoretical analysis of the confounding bias in the multi-cause treatment effect problem. The paper also provides a theoretical justification for the effectiveness of the proposed procedure. Experimental results on synthetic and semi-synthetic experiments show that the proposed SCP performs better than existing methods.","This paper proposes two methods for conditional average treatment effect estimation. Single-cause Perturbation (SCP) is a two-step procedure to estimate the multi-cause treatment effect. It is based on covariate adjustment on the augmented dataset, and the proposed estimator is a combination of the two. The proposed method is evaluated on synthetic and semi-synthetic experiments. "
9583,SP:247bc6675cce89d51558537daf63dadb0c4307f8,"multiwavelet - based neural operator learning scheme USED-FOR operator ’s kernel. fine - grained wavelets USED-FOR multiwavelet - based neural operator learning scheme. multiwavelet polynomial bases USED-FOR projection of the kernel. Burgers ’ equation CONJUNCTION Darcy Flow. Darcy Flow CONJUNCTION Burgers ’ equation. Darcy Flow CONJUNCTION Navier - Stokes equation. Navier - Stokes equation CONJUNCTION Darcy Flow. Korteweg - de Vries ( KdV ) equation CONJUNCTION Burgers ’ equation. Burgers ’ equation CONJUNCTION Korteweg - de Vries ( KdV ) equation. neural operator approaches COMPARE model. model COMPARE neural operator approaches. state - of - the - art EVALUATE-FOR model. accuracy EVALUATE-FOR neural operator approaches. accuracy EVALUATE-FOR model. relative L2 error EVALUATE-FOR Burgers ’ ( KdV ) equation. method USED-FOR time - varying equations. relative L2 error EVALUATE-FOR method. mappings between function spaces USED-FOR method. lower - resolution data USED-FOR method. OtherScientificTerm are inverse operator map, and complex dependencies. Method are inverse multiwavelet filters, projected kernel, and resolution - independent scheme. ","This paper proposes a multiwavelet-based neural operator learning scheme for learning an operator’s kernel from fine-grained wavelets. The projection of the kernel is based on multiwavelets polynomial bases, and the inverse operator map is computed using inverse multiwavelett filters. The proposed method is able to learn time-varying equations such as the Burgers’ equation, the Darcy Flow, the Navier-Stokes equation and the Korteweg-de Vries (KdV) equation. The model is shown to achieve state-of-the-art accuracy in terms of relative L2 error compared to neural operator approaches. The method also uses mappings between function spaces to learn a resolution-independent scheme. ","The paper proposes a multi-wavelet-based neural operator learning scheme for learning an operator’s kernel from fine-grained wavelets. The projection of the kernel is based on multiwavelet polynomial bases. The proposed model is evaluated on state-of-the-art neural operator approaches on lower-resolution data. The authors show that the proposed method can learn time-varying equations such as the Burgers’ equation, the Darcy Flow, the Navier-Stokes equation, and the Korteweg-de Vries (KdV) equation. They also show that their proposed model can achieve better accuracy than other neural operators approaches on the relative L2 error. "
9608,SP:1153785e6a016cfee2644952a772aa08927299b6,"Binary neural networks ( BNNs ) USED-FOR full - precision weights. 1 - bit with sign function USED-FOR Binary neural networks ( BNNs ). gradient FEATURE-OF sign function. approximate gradient USED-FOR optimization difficulty. sine functions USED-FOR BNNs. frequency domain approximation ( FDA ) HYPONYM-OF BNNs. Fourier frequency domain FEATURE-OF gradient of sign function. frequency domain approximation ( FDA ) HYPONYM-OF sine functions. low - frequency information FEATURE-OF sign function. noise adaptation module USED-FOR approximation error. benchmark datasets CONJUNCTION neural architectures. neural architectures CONJUNCTION benchmark datasets. method USED-FOR binary network. Method is back - propagation. Generic are approximations, and approach. OtherScientificTerm are factual gradient, and high - frequency coefficients. ","This paper studies the problem of learning full-precision weights for Binary neural networks (BNNs) with 1-bit with sign function. The authors consider the optimization difficulty of BNNs with sine functions such as frequency domain approximation (FDA) and the gradient of sign function in the Fourier frequency domain. They show that the approximate gradient of the sign function is a good approximation of the true sign function with low-frequency information. They then propose a noise adaptation module to reduce the approximation error. They also show that back-propagation improves the performance of the approximations. Finally, they propose a method to learn a binary network with high-frequency coefficients.",This paper proposes a method for learning full-precision weights for Binary neural networks (BNNs) with 1-bit with sign function. Binary neural networks are a family of BNNs with sine functions (e.g. frequency domain approximation (FDA) and Fourier frequency domain). The authors show that the gradient of sign function in the sign function is a function of low-frequency information. The authors also show that this gradient can be approximated by back-propagation. The paper also shows that the approximation error can be reduced by a noise adaptation module. 
9633,SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,cortical areas USED-FOR tasks. Recurrent neural networks ( RNNs ) USED-FOR cortical areas. Recurrent neural networks ( RNNs ) USED-FOR neuroscience - based tasks. cortical area USED-FOR tasks. multi - area RNNs USED-FOR multi - area computation. neuroscience - inspired architecture constraints FEATURE-OF multi - area RNNs. Dale ’s Law USED-FOR networks. full observability FEATURE-OF RNNs. full observability USED-FOR output - relevant information. modular computation USED-FOR minimal sufficient representations of task information. cortex USED-FOR minimal sufficient representations of task information. modular computation USED-FOR cortex. constrained multi - area RNNs USED-FOR computations. distributed computation PART-OF neural systems. OtherScientificTerm is coordination of multiple brain areas. Generic is computation. ,"This paper studies the problem of multi-area RNNs with neuroscience-inspired architecture constraints. The authors propose to use Recurrent neural networks (RNNs) to represent the cortical areas of the brain in neuroscience-based tasks. The main idea is to use a modular computation to compute the minimal sufficient representations of task information in the cortex for each cortical area for each task. The proposed networks are based on Dale’s Law, which allows for the coordination of multiple brain areas. Theoretically, the authors show that the proposed computations are more efficient than the standard constrained multi- area RNN with the same number of neurons. ","This paper proposes a novel architecture for multi-area RNNs with neuroscience-inspired architecture constraints. The proposed architecture is based on Recurrent Neural Networks (RNNs), which can be seen as an extension of the Dale’s Law. The authors show that the proposed architecture can be applied to a wide range of tasks in the cortical area for tasks that require coordination of multiple brain areas. The paper also shows that the full observability of an RNN can be improved by incorporating the output-relevant information from the cortex into the computation. The main contribution of the paper is to propose a modular computation for the minimal sufficient representations of task information in the cortex. The computations can be performed by a set of constrained multi- area RNN with the same number of neurons. "
9658,SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,Saliency maps USED-FOR convolutional neural networks ( CNNs ). convolutional neural networks ( CNNs ) USED-FOR image classification. saliency map USED-FOR image of interest. maps USED-FOR classification. confidence EVALUATE-FOR classifier. structured attention graphs ( SAGs ) USED-FOR attention maps. compact and representative SAG USED-FOR visualization. approach USED-FOR compact and representative SAG. diverse sampling USED-FOR approach. diverse sampling USED-FOR compact and representative SAG. diverse sampling USED-FOR visualization. SAGs COMPARE saliency maps. saliency maps COMPARE SAGs. SAGs USED-FOR comparative counterfactual questions. saliency maps USED-FOR comparative counterfactual questions. user study USED-FOR comparative counterfactual questions. user study EVALUATE-FOR SAGs. image classifications FEATURE-OF comparative counterfactual questions. SAGs COMPARE saliency map baselines. saliency map baselines COMPARE SAGs. user accuracy EVALUATE-FOR SAGs. user accuracy EVALUATE-FOR saliency map baselines. Method is beam search algorithm. OtherScientificTerm is image regions. ,This paper studies the problem of learning saliency maps for convolutional neural networks (CNNs) for image classification. The authors propose to use structured attention graphs (SAGs) to learn attention maps for each image of interest. These maps are then used to train a classifier with high confidence. The proposed approach is based on diverse sampling to learn a compact and representative SAG for visualization. The beam search algorithm is then applied to learn the saliency map of the image regions. The experiments show that the proposed SAGs perform better than standard SAGEs in terms of user accuracy and user study for comparative counterfactual questions on image classifications.,"This paper proposes to use saliency maps for convolutional neural networks (CNNs) for image classification. The idea is to use structured attention graphs (SAGs) to learn attention maps for each image of interest. These maps are then used for classification, and the confidence of the classifier is computed using a beam search algorithm. The proposed approach is based on diverse sampling for visualization and compact and representative SAG for visualization. The authors show that the proposed SAGs outperform other saliency map baselines in terms of user study for comparative counterfactual questions on image classifications."
9683,SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,loss functions CONJUNCTION regularizers. regularizers CONJUNCTION loss functions. loss functions USED-FOR image classification tasks. image classification tasks EVALUATE-FOR regularizers. test accuracy EVALUATE-FOR loss functions. test accuracy EVALUATE-FOR regularizers. loss functions USED-FOR representations. representations USED-FOR downstream tasks. loss functions USED-FOR downstream tasks. transferability EVALUATE-FOR hidden representations of convolutional neural networks. ImageNet USED-FOR hidden representations of convolutional neural networks. fixed feature extractors USED-FOR downstream tasks. networks USED-FOR tasks. objectives COMPARE vanilla softmax cross - entropy. vanilla softmax cross - entropy COMPARE objectives. ImageNet accuracy EVALUATE-FOR vanilla softmax cross - entropy. ImageNet accuracy EVALUATE-FOR objectives. centered kernel alignment USED-FOR hidden representations of networks. objectives CONJUNCTION hyperparameter combinations. hyperparameter combinations CONJUNCTION objectives. hyperparameter combinations USED-FOR class separation. objectives USED-FOR class separation. features USED-FOR downstream tasks. accuracy EVALUATE-FOR task. class separation FEATURE-OF Representations. task EVALUATE-FOR Representations. accuracy EVALUATE-FOR Representations. learning invariant features CONJUNCTION features. features CONJUNCTION learning invariant features. features USED-FOR transfer tasks. learning invariant features USED-FOR task. OtherScientificTerm is loss. Generic is network. ,"This paper studies the transferability of hidden representations of convolutional neural networks trained on ImageNet with centered kernel alignment. The authors show that the test accuracy of loss functions and regularizers on image classification tasks can be improved by using these representations as representations for downstream tasks using fixed feature extractors. They also show that these objectives can improve ImageNet accuracy over vanilla softmax cross-entropy and hyperparameter combinations for class separation. Finally, they demonstrate that these features can be used for transfer tasks by learning invariant features and features for the downstream tasks.",This paper studies the transferability of hidden representations of convolutional neural networks with centered kernel alignment. The authors propose two objectives: 1) improve the test accuracy of loss functions for image classification tasks and 2) improve test accuracy for regularizers. The paper shows that the proposed objectives outperform vanilla softmax cross-entropy and ImageNet accuracy on two tasks: class separation and learning invariant features for transfer tasks. 
9708,SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"spatial sampling CONJUNCTION temporal frequency of sampling. temporal frequency of sampling CONJUNCTION spatial sampling. neural network training strategy USED-FOR deep generative models of latent dynamics. selective backpropagation through time ( SBTT ) HYPONYM-OF neural network training strategy. SBTT USED-FOR sequential autoencoders. electrophysiological and calcium imaging data USED-FOR neural population dynamics. SBTT USED-FOR inference of neuronal population dynamics. electrophysiology USED-FOR inference of neuronal population dynamics. SBTT USED-FOR electrophysiology. interface bandwidths FEATURE-OF inference of neuronal population dynamics. SBTT USED-FOR high - frequency temporal structure. high - frequency temporal structure FEATURE-OF neural population activity. SBTT USED-FOR neural population activity. SBTT USED-FOR two - photon calcium imaging. limited, highbandwidth sampling USED-FOR pretrain dynamics models. SBTT USED-FOR models. models USED-FOR sparsely - sampled data. OtherScientificTerm are neural interfaces, brain circuits, bandwidth limits, latent low - dimensional population dynamics, latent dynamics, neuronal population dynamics, and implanted neuroelectronic interfaces. Task is Neural Information Processing Systems. ","This paper proposes a neural network training strategy called selective backpropagation through time (SBTT) for deep generative models of latent dynamics. SBTT is a sequential autoencoders that uses both electrophysiological and calcium imaging data for the inference of neuronal population dynamics using interface bandwidths. The authors show that SBTT can capture the high-frequency temporal structure of neural population activity and the spatial sampling and temporal frequency of sampling in neural interfaces. They also show that with limited, highbandwidth sampling, SBTT improves the performance of pretrain dynamics models trained with SBTT on sparsely-sampled data. ","This paper proposes a novel neural network training strategy for deep generative models of latent dynamics, selective backpropagation through time (SBTT). SBTT is an extension of sequential autoencoders. The authors show that SBTT can improve the inference of neuronal population dynamics in both electrophysiological and calcium imaging data for neural population dynamics with different interface bandwidths. They show that neural interfaces can be represented as neural interfaces in the brain circuits, and that the bandwidth limits of neural low-dimensional population dynamics can be reduced to those of neural high-dimensionality population dynamics. They also show that the high-frequency temporal structure of neural population activity can be approximated by SBTT.  The authors propose to use limited, highbandwidth sampling to train pretrain dynamics models on sparsely-sampled data, and use SBTT to train models for sparsely sampled data with implanted neuroelectronic interfaces."
9733,SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"Sequence - to - sequence learning USED-FOR sequence prediction tasks. neural networks USED-FOR Sequence - to - sequence learning. approach USED-FOR local distribution. neural network USED-FOR approach. neural network USED-FOR local distribution. hierarchical approach USED-FOR sequence - to - sequence learning. quasi - synchronous grammars USED-FOR hierarchical approach. style transfer CONJUNCTION small - scale machine translation. small - scale machine translation CONJUNCTION style transfer. compositional generalization ( SCAN ) CONJUNCTION style transfer. style transfer CONJUNCTION compositional generalization ( SCAN ). it COMPARE baselines. baselines COMPARE it. latent neural grammar USED-FOR domains. latent neural grammar USED-FOR diagnostic language navigation task. diagnostic language navigation task EVALUATE-FOR compositional generalization ( SCAN ). diagnostic language navigation task EVALUATE-FOR small - scale machine translation. diagnostic language navigation task HYPONYM-OF domains. style transfer HYPONYM-OF diagnostic language navigation task. compositional generalization ( SCAN ) HYPONYM-OF domains. small - scale machine translation HYPONYM-OF domains. style transfer HYPONYM-OF domains. Generic is models. Task is compositional generalization. Method are neural parameterization of the grammar, and manual feature engineering. OtherScientificTerm is combinatorial space of derivation rules. ","This paper proposes a hierarchical approach to sequence-to-sequence learning for sequence prediction tasks using neural networks. The approach uses a neural network to predict the local distribution of the input sequence using the neural parameterization of the grammar. The hierarchical approach is based on quasi-synchronous grammars. The authors propose a combinatorial space of derivation rules that can be used to train the models. The proposed approach is evaluated on three domains: style transfer, small-scale machine translation, and compositional generalization (SCAN) on a diagnostic language navigation task with latent neural grammar.","This paper proposes a hierarchical approach to sequence-to-sequence learning for sequence prediction tasks. The approach is based on a neural network that predicts the local distribution of the input sequence. The authors also propose a neural parameterization of the grammar. The proposed approach is evaluated on three domains: compositional generalization (SCAN), style transfer, and small-scale machine translation. The results show that the proposed approach outperforms baselines in all three domains. "
9769,SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,Feature Selection CONJUNCTION Functional Data Analysis. Functional Data Analysis CONJUNCTION Feature Selection. algorithm USED-FOR function - on - scalar feature selection. algorithm USED-FOR Group Elastic Net. Group Elastic Net USED-FOR function - on - scalar feature selection. scalar predictors USED-FOR functional response. algorithm USED-FOR Group Elastic Net. sparsity structure FEATURE-OF Augmented Lagrangian. algorithm USED-FOR ultrahigh dimensional settings. algorithm USED-FOR sparsity structure. ultrahigh dimensional settings FEATURE-OF Group Elastic Net. algorithm USED-FOR function - on - scalar regression framework. Functional Principal Components USED-FOR algorithm. approach COMPARE competitors. competitors COMPARE approach. simulations EVALUATE-FOR approach. Genome Wide Association Study USED-FOR application. Task is analysis of large and complex data sets. OtherScientificTerm is computational burden. ,This paper proposes a new algorithm for function-on-scalar feature selection in Group Elastic Net. The proposed algorithm is based on the Functional Principal Components of the Augmented Lagrangian. The authors show that the proposed algorithm can be used to improve the sparsity structure of Group ElasticNet in the ultrahigh dimensional settings. They also show that their approach outperforms competitors in simulations. ,"The paper proposes a new algorithm for function-on-scalar feature selection and Functional Data Analysis. The algorithm is based on Group Elastic Net, which is an extension of the Group Elastic Network. The main idea is to use the sparsity structure of the Augmented Lagrangian of the group of scalar predictors to predict the functional response of the input data set. The proposed algorithm is evaluated on two ultrahigh dimensional settings, where the computational burden is high. The authors show that the proposed algorithm outperforms competitors in terms of computational burden. The paper also shows that the algorithm can be applied to a function-onscale regression framework with Functional Principal Components. The application is evaluated in a Genome Wide Association Study. "
9805,SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,functional principal component analysis ( FPCA ) USED-FOR model estimation. real data analyses EVALUATE-FOR framework. Material is Structured point process data. Generic is matrix. OtherScientificTerm is log - Gaussian Cox processes. ,This paper proposes a functional principal component analysis (FPCA) for model estimation. Structured point process data is used as a matrix. The matrix is composed of log-Gaussian Cox processes. The authors show that the proposed framework can be applied to real data analyses. ,"This paper proposes a functional principal component analysis (FPCA) for model estimation. The framework is based on Structured point process data, where the matrix is a log-Gaussian Cox processes. Experiments on real data analyses show the effectiveness of the proposed framework."
9841,SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"online multi - task learning approach USED-FOR adaptive nonlinear control. adversarial disturbance CONJUNCTION unknown environmentdependent nonlinear dynamics. unknown environmentdependent nonlinear dynamics CONJUNCTION adversarial disturbance. unknown environmentdependent nonlinear dynamics FEATURE-OF nonlinear system. adversarial disturbance FEATURE-OF nonlinear system. shared representation USED-FOR environmentdependent dynamics. approach USED-FOR robot control. unified framework USED-FOR control - theoretic and learning - theoretic guarantees. non - asymptotic endto - end convergence guarantee USED-FOR multi - task nonlinear control. OMAC CONJUNCTION deep representation learning. deep representation learning CONJUNCTION OMAC. OMAC COMPARE adaptive control approaches. adaptive control approaches COMPARE OMAC. Method are Online Meta - Adaptive Control ( OMAC ), online representation learning, and control theory. Task is robotic system. ","This paper proposes Online Meta-Adaptive Control (OMAC), an online multi-task learning approach for adaptive nonlinear control with adversarial disturbance and unknown environmentdependent nonlinear dynamics in a nonlinear system. The authors propose a unified framework for learning control-theoretic and learning-thoroughness guarantees for the robotic system. OMAC uses a shared representation to learn the environmentdependent dynamics and the environment dependent dynamics in the shared representation. They show that OMAC achieves a non-asymptotic endto-end convergence guarantee for multi-tasks non-linear control. They also show that the proposed approach can be applied to robot control. Finally, the authors show the superiority of OMAC over adaptive control approaches and deep representation learning.","This paper proposes Online Meta-Adaptive Control (OMAC), an online multi-task learning approach for adaptive nonlinear control with adversarial disturbance and unknown environmentdependent nonlinear dynamics in the nonlinear system. The authors propose a unified framework for control-theoretic and learning-thruthic guarantees for the robotic system. They propose a shared representation for the environmentdependent dynamics, and a non-asymptotic endto-end convergence guarantee for the multi-tasks nonlinearity. They show that OMAC outperforms other adaptive control approaches, including deep representation learning."
9877,SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"bound propagation based certified robust training methods USED-FOR neural networks. certifiable robustness guarantees FEATURE-OF neural networks. interval bound propagation ( IBP ) CONJUNCTION CROWN - IBP. CROWN - IBP CONJUNCTION interval bound propagation ( IBP ). interval bound propagation ( IBP ) HYPONYM-OF SOTA ) methods. CROWN - IBP PART-OF SOTA ) methods. weight initialization method USED-FOR IBP training. regularization USED-FOR ReLU activation states. regularization USED-FOR certified bounds. BN USED-FOR ReLU activation states. Batch Normalization ( BN ) USED-FOR model. regularization USED-FOR certified training. verified error CONJUNCTION verified error. verified error CONJUNCTION verified error. verified error FEATURE-OF TinyImageNet. network architecture USED-FOR SOTA. Metric is per - batch training complexity. Method are neural network training, and Fast - Certified - Robust - Training. Generic are they, and methods. OtherScientificTerm are exploded bounds, long warmup schedules, and training schedules. Task is wamrup. Material is CIFAR-10. ","This paper studies the problem of certifiable robustness guarantees for neural networks with bound propagation based certified robust training methods. The authors propose two methods: interval bound propagation (IBP) and CROWN-IBP, which are both SOTA (SOTA) methods. They show that they are robust to exploding bounds and that they can be used to improve the per-batch training complexity of neural network training. The main contribution of the paper is to propose a weight initialization method for IBP training, which can be combined with Batch Normalization (BN) for ReLU activation states, and regularization for certified bounds.  The authors also propose Fast-Certified-Robust-Training, which uses BN to regularize the model and improve the certified error of TinyImageNet. The proposed SOTA is based on the network architecture of SOTA, and it is shown that it can achieve a wamrup of CIFAR-10. ","This paper proposes a new method for certifiable robustness guarantees for neural networks. The method is based on the idea of Fast-Certified-Robust-Training (FCCR), which is an extension of SOTA (SOTA) methods such as interval bound propagation (IBP) and CROWN-IBP. The authors propose a weight initialization method for IBP training and a regularization for ReLU activation states. The proposed method is evaluated on CIFAR-10 and TinyImageNet. The results show that the proposed method outperforms SOTA in terms of per-batch training complexity and verified error. "
9913,SP:18ffeb199a670fb2b1f4417b8653479001944dab,"change point detection method USED-FOR adversaries. Huber ε - contamination framework USED-FOR adversarial attacks. phase transition phenomenon FEATURE-OF change point detection. minimax lower bound USED-FOR computationally - feasible method. Task are Change point detection, and univariate mean change point detection problem. Method are theoretically - justified methods, and robust change point detection methods. OtherScientificTerm are model violations, heavy - tailed noise distribution, isolate outliers, systematic contamination, spurious change points, contamination distributions, detection boundary, contamination proportion ε, contamination proportion, and logarithmic factors. Metric is minimax - rate optimal localisation error rate. ","This paper proposes a new change point detection method for adversaries. The authors propose a Huber ε-contamination framework for adversarial attacks. The proposed method is based on the phase transition phenomenon of change points detection, which is a well-studied problem in the literature. The main contribution of the paper is to provide a minimax lower bound for the computationally-feasible method. The paper also provides theoretical justification for the proposed method. ","This paper proposes a change point detection method to detect adversaries. The authors propose a Huber ε-contamination framework to detect adversarial attacks. The proposed method is based on a minimax lower bound for the computationally-feasible method. The main idea is to minimize the minimax-rate optimal localisation error rate of the detection boundary, which is a function of the contamination proportion ε. The paper provides theoretical justification for the proposed method. "
9949,SP:d03617b5fc446768809cf015c9234b0c9386a690,"differentiable model CONJUNCTION neural network. neural network CONJUNCTION differentiable model. batch Gradient Descent ( GD ) USED-FOR empirical loss. batch Gradient Descent ( GD ) USED-FOR learning. paradigms USED-FOR learning problems. GD USED-FOR learning. SGD USED-FOR learning. SGD CONJUNCTION GD. GD CONJUNCTION SGD. precision ρ FEATURE-OF gradient calculations. statistical queries ( SQ ) USED-FOR learning. SGD USED-FOR sample - based learning algorithm. learning power EVALUATE-FOR PAC learning. SGD USED-FOR SQ learning. fine enough precision COMPARE minibatch size. minibatch size COMPARE fine enough precision. GD USED-FOR sample - based learning algorithm. fine enough precision USED-FOR GD. GD USED-FOR PAC learning. SGD USED-FOR PAC learning. SGD CONJUNCTION GD. GD CONJUNCTION SGD. SGD COMPARE SQ learning. SQ learning COMPARE SGD. OtherScientificTerm are population loss, bρ, ρ, and mini - batch size. ","The paper proposes a new empirical loss based on batch Gradient Descent (GD) for learning the empirical loss. The empirical loss is based on statistical queries (SQ) and SGD. The authors show that SGD and GD can be used to improve the performance of PAC learning with fine enough precision compared to GD and GD with minibatch size.  The authors also show that the population loss can be improved by SGD, GD, and GD.  ","This paper proposes batch Gradient Descent (GD) for empirical loss, which is a differentiable model and a neural network. GD is used for learning in the context of paradigms for learning problems where the population loss is large. The authors show that SGD and GD can be used to improve the performance of learning with statistical queries (SQ) for learning with small batch size. They also show that GD with fine enough precision outperforms minibatch size and SGD with small sample-based learning algorithm with SGD for SQ learning. They show that the precision ρ of the gradient calculations is larger than the precision of the bρ, and that the mini-batch size is smaller than that of GD. Finally, they show that PAC learning with GD can achieve better learning power than PAC learning without SGD. "
9985,SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"machine learning CONJUNCTION inverse problems. inverse problems CONJUNCTION machine learning. model probability distribution USED-FOR discrete data. discrete data USED-FOR inverse problems. discrete data USED-FOR machine learning. Wasserstein distance FEATURE-OF model distribution. uniform probability distribution USED-FOR Wasserstein distance. convergence FEATURE-OF Lloyd - type algorithm. ambient space FEATURE-OF point cloud. point cloud USED-FOR algorithm. Poliak - Łojasiewicz inequality USED-FOR Wasserstein distance cost. Task is minimization problem. Method are Lloyd ’s algorithm, and gradient descent. OtherScientificTerm are Voronoi cells, Power cells, spurious critical points, error term, and discrete distribution. Metric is Wasserstein error. Generic are problem, and bounds. ","This paper studies the minimization problem of the “Lloyd’s algorithm” in the context of machine learning and inverse problems with discrete data. The authors consider the problem of minimizing the Wasserstein error of the model distribution of a model probability distribution over a set of discrete data with Voronoi cells. Power cells are considered as spurious critical points, and the goal is to minimize the error term.  The authors propose a new algorithm based on the point cloud in the ambient space, which is a variant of the Lloyd-type algorithm with a uniform probability distribution.   The main contribution of the paper is to show that the proposed algorithm converges to a point cloud with a Poliak-Łojasiewicz inequality, which leads to a significant reduction in the WASSERSTEIN distance cost.  In addition, the authors also provide bounds on the gradient descent of the algorithm.","This paper studies the minimization problem of the “Lloyd’s algorithm”, where the goal is to minimize the Wasserstein error of the Voronoi cells. The problem is formulated as a linear combination of two problems: (1) minimizing the number of spurious critical points, and (2) finding the optimal solution of the problem. The authors propose a “lloyd-type” algorithm, which is based on gradient descent. The main idea of the algorithm is to learn a point cloud in the ambient space. The point cloud consists of a set of discrete data, and the model probability distribution of the discrete data is used to solve the inverse problems for machine learning and for inverse problems with discrete data. The paper shows the convergence of the proposed Lloyd-type algorithm under the assumption of uniform probability distribution for the model distribution. The error term is defined as the difference between the discrete distribution with respect to the Waderstein distance and the Wateredstein distance of the point cloud under Poliak-Łojasiewicz inequality. "
10021,SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"Convolution HYPONYM-OF feature transform. Convolution HYPONYM-OF neural networks. feature transform PART-OF neural networks. convolution layers CONJUNCTION self - attention blocks. self - attention blocks CONJUNCTION convolution layers. convolution layers PART-OF Transformer networks. dynamic transforms USED-FOR video understanding. correspondence relations USED-FOR representation. motion information HYPONYM-OF correspondence relations. self - attention HYPONYM-OF dynamic transforms. relational kernels CONJUNCTION relational contexts. relational contexts CONJUNCTION relational kernels. rich structures of spatio - temporal relations USED-FOR relational feature transform. relational kernels USED-FOR rich structures of spatio - temporal relations. relational self - attention ( RSA ) HYPONYM-OF relational feature transform. Diving48 CONJUNCTION FineGym. FineGym CONJUNCTION Diving48. Something - Something - V1&V2 CONJUNCTION Diving48. Diving48 CONJUNCTION Something - Something - V1&V2. RSA network COMPARE convolution and self - attention counterparts. convolution and self - attention counterparts COMPARE RSA network. motion - centric benchmarks USED-FOR video action recognition. Something - Something - V1&V2 HYPONYM-OF video action recognition. Diving48 HYPONYM-OF video action recognition. motion - centric benchmarks EVALUATE-FOR RSA network. FineGym HYPONYM-OF motion - centric benchmarks. Something - Something - V1&V2 HYPONYM-OF motion - centric benchmarks. Diving48 HYPONYM-OF motion - centric benchmarks. Method are deep learning, stationary convolution kernels, and dynamic feature transforms. ","This paper proposes a new feature transform called relational self-attention (RSA) for deep learning. The relational feature transform is based on rich structures of spatio-temporal relations between relational kernels and relational contexts. The key idea is to use correspondence relations between the representation and the motion information in the representation. The authors show that dynamic transforms can be used to improve video understanding by using convolution layers in Transformer networks. The proposed RSA network outperforms the convolution and self -attention counterparts on motion-centric benchmarks such as Diving48, FineGym and Something-Something-V1&V2.","This paper proposes a relational self-attention (RSA) model for video understanding. The key idea is to learn a relational feature transform from relational kernels and relational contexts. The authors show that the relational features can be represented as stationary convolution kernels, and that the representation can be decomposed into correspondence relations between the motion information and the representation of the relational kernels. The proposed model is evaluated on three motion-centric benchmarks: Diving48, FineGym, and Something-Something-V1&V2. The RSA network is shown to outperform the convolution and self-Attention counterparts. "
10057,SP:2c2530069d5cab485629090243da464d107feadd,"mean field theory FEATURE-OF multilayer neural networks. mean field limit USED-FOR learning dynamics. infinite - width limit FEATURE-OF random fluctuation. large - width expansion USED-FOR random fluctuation. formulation USED-FOR stochastic dependency. fluctuation FEATURE-OF multilayer networks. system of dynamical equations USED-FOR limiting fluctuation distribution. second - order mean field limit HYPONYM-OF system of dynamical equations. stochasticity CONJUNCTION nonlinear time evolution. nonlinear time evolution CONJUNCTION stochasticity. nonlinear time evolution FEATURE-OF limiting fluctuation. cross - layer dependency CONJUNCTION nonlinear time evolution. nonlinear time evolution CONJUNCTION cross - layer dependency. cross - layer dependency FEATURE-OF stochasticity. large - width networks USED-FOR fluctuation. vanishing fluctuation FEATURE-OF output function. squared loss FEATURE-OF empirical risk minimization setting. empirical risk minimization setting FEATURE-OF shallow networks. loss function FEATURE-OF multilayer networks. squared loss FEATURE-OF shallow networks. OtherScientificTerm are infinite - width scaling, network depth, complex interaction, limit theorem, large - width regime, training trajectory, and global optimum. Material is multilayer case. Method are neuronal embedding framework, and gradient descent mean field training. Generic are it, and network. ","This paper studies the mean field theory of multilayer neural networks. The authors propose a new mean field limit for learning dynamics in multilayers. The main contribution of the paper is a new formulation for the stochastic dependency between the network depth and the number of layers. Theoretically, the authors show that the limiting fluctuation distribution of a system of dynamical equations, the second-order mean field, can be defined as a function of the size of the network, and that it can be used to define a new loss function for shallow networks with a squared loss in the empirical risk minimization setting. The paper also provides a theoretical analysis for the multilayered case.","This paper proposes a new mean field theory for multilayer neural networks. The main idea is to use the mean field limit for learning dynamics. The authors propose a new formulation for stochastic dependency and nonlinear time evolution, which is based on a system of dynamical equations, namely the second-order meanfield limit. They show that under infinite-width scaling, the limiting fluctuation distribution can be defined as a function of the network depth and the number of layers. They also show that it is not necessary to use gradient descent mean field training, but it is possible to use large-width expansion for random fluctuation. They prove the limit theorem, which shows that in the large-wide regime, the training trajectory converges to the global optimum. They then show that the loss function of shallow networks with a squared loss is the same as the empirical risk minimization setting. Finally, the authors show that in multilayers case, the limit of the output function is the vanishing fluctuation of the input function. "
10093,SP:a3d927854d9d7fd39b8d05a79666810d585d5062,inductive biases USED-FOR predictive extrapolation. Hamiltonian / Lagrangian form USED-FOR structure. inductive biases USED-FOR Forecasting of time - series data. dissipative brackets PART-OF metriplectic dynamical systems. metriplectic dynamical systems USED-FOR parameterization of dissipative brackets. process USED-FOR generalized Casimirs. generalized Casimirs USED-FOR entropy. dynamics COMPARE penalty - based approaches. penalty - based approaches COMPARE dynamics. time - series data USED-FOR dynamical system. data - driven modeling USED-FOR physical systems. data - driven modeling CONJUNCTION machine learning ( ML ) tasks. machine learning ( ML ) tasks CONJUNCTION data - driven modeling. learnable dynamics FEATURE-OF dynamical system. physics - based structure USED-FOR architectures. minimal bias FEATURE-OF black - box model form. approaches USED-FOR structure preserving models of reversible dynamics. structure preserving models of reversible dynamics USED-FOR inductive bias. approaches USED-FOR inductive bias. algebraic structure of Hamiltonian / Lagrangian dynamics USED-FOR flow map. energy FEATURE-OF flow map. symplectic structure FEATURE-OF flow map. approaches USED-FOR reversible systems. entropy HYPONYM-OF generalized Casimirs. framework USED-FOR Physical systems. thermodynamic consistency FEATURE-OF mimetic properties. first and second laws of thermodynamics HYPONYM-OF mimetic properties. fluctuation dissipation theorem ( FDT ) USED-FOR closed stochastic systems. model USED-FOR metriplectic systems. algebraic structure FEATURE-OF system. first principles modeling USED-FOR system. system USED-FOR multiscale problems. time history USED-FOR multiscale problems. training strategy USED-FOR NODEs. metriplectic system USED-FOR time - series data. training strategy USED-FOR algebraic objects. internal entropy CONJUNCTION temperature. temperature CONJUNCTION internal entropy. non - observable states FEATURE-OF dissipative systems. internal entropy HYPONYM-OF non - observable states. temperature HYPONYM-OF non - observable states. null - spaces USED-FOR reversible and irreversible components of the dynamics. dissipative chaotic systems USED-FOR science and engineering problems. latent dimension FEATURE-OF,"This paper proposes a framework for learning a dynamical system from time-series data with inductive biases for predictive extrapolation. The proposed framework is based on the Hamiltonian/Lagrangian form of the structure of a Hamiltonian / Lagrangian dynamics, where the energy of the flow map is defined by the symplectic structure of the flows. The process of learning a generalized Casimirs is then used to estimate the entropy of the generalized Casimiros, which is used as a parameterization of dissipative brackets in metriplectic dynamical systems. The authors show that the inductive bias of the proposed framework can be reduced to a minimal bias in a black-box model form, which can be used to improve the performance of the system. The system is evaluated on a variety of data-driven modeling and machine learning (ML) tasks, where it is shown that the learned dynamics can outperform the state-of-the-art penalty-based approaches. ","This paper proposes a framework for modeling physical systems with inductive biases for predictive extrapolation. The framework is based on the Hamiltonian/Lagrangian form of the structure of a dynamical system with learnable dynamics. The authors propose a parameterization of dissipative brackets in metriplectic dynamical systems, which is a generalization of the generalized Casimirs, a process that can be applied to a number of different types of systems. The proposed framework is evaluated on a set of time-series data, where the authors show that the proposed framework outperforms the state-of-the-art in terms of time history. The paper also shows that the inductive bias in the black-box model form can be reduced to a minimal bias. The system is also evaluated on multiscale problems with time history, where it is shown to outperform the state of the art. "
10129,SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"Fairness CONJUNCTION robustness. robustness CONJUNCTION Fairness. robustness PART-OF Trustworthy AI. Fairness PART-OF Trustworthy AI. Fairness CONJUNCTION robustness. robustness CONJUNCTION Fairness. sample selection - based algorithm USED-FOR fair and robust training. combinatorial optimization problem USED-FOR unbiased selection of samples. greedy algorithm USED-FOR optimization problem. algorithm COMPARE state - of - the - art technique. state - of - the - art technique COMPARE algorithm. fairness CONJUNCTION robustness. robustness CONJUNCTION fairness. robustness EVALUATE-FOR state - of - the - art technique. fairness EVALUATE-FOR state - of - the - art technique. robustness EVALUATE-FOR algorithm. fairness EVALUATE-FOR algorithm. synthetic and benchmark real datasets EVALUATE-FOR algorithm. synthetic and benchmark real datasets EVALUATE-FOR state - of - the - art technique. fair and robust training baselines COMPARE algorithm. algorithm COMPARE fair and robust training baselines. sampling step USED-FOR batch selection. sampling step USED-FOR algorithm. clean data USED-FOR algorithm. Method are unbiased model, and training algorithm. OtherScientificTerm is data corruption. ","This paper proposes a sample selection-based algorithm for fair and robust training in Trustworthy AI. Fairness and robustness are two important components of the unbiased model, and the goal of the training algorithm is to ensure that the unbiased selection of samples is not corrupted by data corruption. The authors propose a combinatorial optimization problem for unbiased selection, where the optimization problem is solved by a greedy algorithm. The algorithm is evaluated on synthetic and benchmark real datasets, and shows that the proposed algorithm achieves better fairness than the state-of-the-art technique, as well as robustness. The sampling step for batch selection is also used to improve the performance of the algorithm.","This paper proposes a sample selection-based algorithm for fair and robust training in Trustworthy AI. The authors propose a combinatorial optimization problem for unbiased selection of samples, where the objective is to minimize the data corruption. The optimization problem is formulated as a greedy algorithm, and the algorithm is compared to a state-of-the-art technique that combines Fairness and robustness. The algorithm is evaluated on synthetic and benchmark real datasets, where it outperforms both fair-and-robust training baselines in terms of fairness, robustness, and batch selection. The sampling step for batch selection is performed on clean data. The training algorithm is also evaluated on a synthetic dataset."
10165,SP:991127729bf067fe27fdd7ed360aab39e4df5921,"Neural network models USED-FOR hidden data biases. function space FEATURE-OF inductive biases. inductive biases USED-FOR models. periodic activation functions USED-FOR Bayesian neural networks. triangular wave CONJUNCTION periodic ReLU activation functions. periodic ReLU activation functions CONJUNCTION triangular wave. deep neural networks USED-FOR out - of - domain detection. periodic activation functions USED-FOR deep neural networks. in - domain data EVALUATE-FOR periodic activation functions. Generic is them. OtherScientificTerm are network weights, translation - invariant, stationary Gaussian process priors, sinusoidal ( Fourier ) activations, and perturbed inputs. ","This paper studies the problem of out-of-domain detection with deep neural networks. The authors propose a new class of neural network models with hidden data biases in the function space. The models are based on inductive biases that are defined in terms of function space, where the network weights are determined by a translation-invariant, stationary Gaussian process priors. They show that the authors can use periodic activation functions such as triangular wave and periodic ReLU activation functions to improve the performance of Bayesian neural networks on in-domain data. They also show that these activations are invariant to sinusoidal (Fourier) activations, and that perturbed inputs are not.","This paper studies the problem of learning hidden data biases in neural network models. The authors show that the inductive biases in the function space of the models are invariant to translation-invariant, stationary Gaussian process priors. They also show that they can be modeled as periodic activation functions in Bayesian neural networks, such as triangular wave and periodic ReLU activation functions. They show that in-domain data can be represented as a set of perturbed inputs, and they show that these perturbations do not affect the network weights. They further show that deep neural networks can be used for out-of-domain detection."
10201,SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,user interaction CONJUNCTION complex dynamic systems. complex dynamic systems CONJUNCTION user interaction. complex dynamic systems FEATURE-OF programs. user interaction FEATURE-OF programs. mouse based games HYPONYM-OF complex dynamic systems. autonomous methods USED-FOR feedback. unit tests USED-FOR interactive programs. feedback USED-FOR interactive programs. classifying Markov Decision Processes ( MDPs ) USED-FOR feedback. dynamics and reward model USED-FOR MDP. agent USED-FOR differential trajectories. agent CONJUNCTION autoregressive model. autoregressive model CONJUNCTION agent. differential trajectories PART-OF MDP. agent USED-FOR cooperative objective. autoregressive model USED-FOR cooperative objective. method USED-FOR automatic feedback system. automatic feedback system USED-FOR interactive code assignments. anonymized student submissions FEATURE-OF dataset. Task is coding education. Method is classifier. Material is hand - coded bug labels. ,"This paper studies the problem of coding education with interactive programs with user interaction and complex dynamic systems (e.g., mouse based games). The authors propose two autonomous methods for generating feedback for interactive programs based on unit tests. The first method is based on classifying Markov Decision Processes (MDPs) and using a dynamics and reward model to learn the MDP. The second method uses an agent and an autoregressive model to train a cooperative objective. The authors show that the proposed method is able to generate an automatic feedback system for interactive code assignments with anonymized student submissions. ",This paper proposes a new method to train an automatic feedback system for interactive code assignments. The idea is to use a dynamics and reward model to train a MDP. This is done by classifying Markov Decision Processes (MDPs) and then using autonomous methods to train the feedback. The agent is trained with differential trajectories and an autoregressive model. Experiments are conducted on mouse based games and complex dynamic systems. The authors show that the proposed method is able to improve the performance of coding education. 
10237,SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"superpixels CONJUNCTION attentions. attentions CONJUNCTION superpixels. attentions CONJUNCTION saliency maps. saliency maps CONJUNCTION attentions. superpixels USED-FOR low - level input features. high - level latent object features USED-FOR approach. disentangled representation USED-FOR high - level latent object features. identifiable latent representation USED-FOR independent factors of variation. mimic tree USED-FOR DRL action values. identifiable latent representation USED-FOR Represent And Mimic ( RAMi ) framework. fidelity EVALUATE-FOR mimic tree. Minimum Description Length ( MDL ) objective EVALUATE-FOR mimic tree. Information Bottleneck ( IB ) principle USED-FOR Minimum Description Length ( MDL ) objective. mimic tree COMPARE baseline models. baseline models COMPARE mimic tree. decision rules CONJUNCTION causal impacts. causal impacts CONJUNCTION decision rules. latent traversals CONJUNCTION decision rules. decision rules CONJUNCTION latent traversals. causal impacts CONJUNCTION human evaluation results. human evaluation results CONJUNCTION causal impacts. latent traversals FEATURE-OF mimic tree. decision rules PART-OF mimic tree. Task is Interpreting Deep Reinforcement Learning ( DRL ) models. OtherScientificTerm are transparency regulations, latent features, IB - optimal mimic tree, and nodes. Method is DRL model. ","This paper proposes a new approach to learn low-level input features from superpixels and attentions. The authors propose a Represent And Mimic (RAMi) framework that uses an identifiable latent representation to represent the independent factors of variation in a DRL action values. The key idea is to learn a disentangled representation of high-level latent object features, which is then used to train a mimic tree to predict the DRL actions. The IB-optimal mimic tree is trained using the Information Bottleneck (IB) principle. The paper shows that the mimic tree achieves better fidelity than baseline models in terms of decision rules, latent traversals, causal impacts, and human evaluation results.","This paper proposes a new approach to learn low-level input features from superpixels and attentions. The authors propose a Represent And Mimic (RAMi) framework, which uses an identifiable latent representation of the high-level latent object features, which is a disentangled representation. The key idea of the approach is to learn the independent factors of variation between the super pixels and the attentions, and then use an IB-optimal mimic tree to predict the DRL action values. The paper also proposes an Information Bottleneck (IB) principle for the Minimum Description Length (MDL) objective, which improves the fidelity of the mimic tree compared to baseline models. Experiments show that the proposed mimic tree outperforms baseline models in terms of decision rules, latent traversals, causal impacts, and human evaluation results."
10273,SP:84560de78af979354fff83d1370d8675c1e9191f,"weather forecasts CONJUNCTION political prognostications. political prognostications CONJUNCTION weather forecasts. political prognostications CONJUNCTION financial projections. financial projections CONJUNCTION political prognostications. Bayesian framework USED-FOR structure of dynamic predictions. GLIM HYPONYM-OF Bayesian framework. Gaussian latent information martingale HYPONYM-OF Bayesian framework. historical data USED-FOR latent process of information flow. martingale structure CONJUNCTION volatility. volatility CONJUNCTION martingale structure. approach USED-FOR probability paths. volatility HYPONYM-OF probability paths. martingale structure HYPONYM-OF probability paths. GLIM COMPARE baseline methods. baseline methods COMPARE GLIM. metrics USED-FOR estimated posterior probability path distributions. estimated posterior probability path distributions EVALUATE-FOR GLIM. estimated posterior probability path distributions EVALUATE-FOR baseline methods. metrics EVALUATE-FOR baseline methods. metrics EVALUATE-FOR GLIM. Task are probability estimates of future binary outcomes, and time series analysis. Generic are first, second, former, and trajectories. OtherScientificTerm is dynamic structure of predictions. ","This paper proposes a Bayesian framework for learning the structure of dynamic predictions using Gaussian latent information martingale. The authors propose to use historical data to learn the latent process of information flow. The first, second, and third stages of the process are modeled as trajectories, and the second stage is modeled as a sequence of probability estimates of future binary outcomes.  The authors show that the proposed approach can learn probability paths such as martingales structure, volatility, and political prognostications as well as weather forecasts and financial projections. The proposed GLIM outperforms baseline methods on several metrics for estimating posterior probability path distributions. ","This paper proposes a Bayesian framework for learning the structure of dynamic predictions. The proposed framework, GLIM, is based on Gaussian latent information martingale, where the first, second, and third steps of the latent process of information flow are learned from historical data. The authors propose to use probability estimates of future binary outcomes as a proxy for time series analysis. They show that the proposed approach is able to learn probability paths with respect to the martingales structure, volatility, and political prognostications. They also show that GLIM outperforms baseline methods on several metrics for estimating posterior probability path distributions. "
10309,SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"fixed confidence FEATURE-OF active pure exploration. generic stochastic bandit environments USED-FOR active pure exploration. instance - specific lower bounds FEATURE-OF expected sample complexity. instance - specific lower bounds USED-FOR problem. proportions USED-FOR optimization problem. tractability FEATURE-OF optimization problem. algorithm USED-FOR pure exploration problems. Frank - Wolfe algorithm USED-FOR lower - bound optimization problem. Frank - Wolfe algorithm USED-FOR it. FWS USED-FOR pure exploration tasks. arm identification HYPONYM-OF pure exploration tasks. FWS COMPARE state - of - art algorithms. state - of - art algorithms COMPARE FWS. OtherScientificTerm are sampling budget, structural properties of the environment, and lower bounds. Method are Oracle algorithm, and learning algorithms. Metric is sample complexity. ","This paper studies the problem of active pure exploration in generic stochastic bandit environments with fixed confidence. The problem is formulated as an optimization problem with proportions that depend on the sampling budget and the structural properties of the environment. The authors derive instance-specific lower bounds for the expected sample complexity of the problem, and propose an Oracle algorithm to solve it using the Frank-Wolfe algorithm for the lower-bound optimization problem. They show that the proposed FWS outperforms state-of-the-art algorithms for pure exploration tasks such as arm identification. ","This paper studies active pure exploration with fixed confidence in generic stochastic bandit environments. The authors propose a novel Oracle algorithm, which is based on the Frank-Wolfe algorithm, to solve the lower-bound optimization problem with instance-specific lower bounds on the expected sample complexity of the environment. The optimization problem is formulated in terms of proportions of the optimization problem and tractability of the sampling budget. They show that FWS outperforms state-of-the-art algorithms on several pure exploration tasks such as arm identification and arm identification. They also show that their algorithm can be applied to other pure exploration problems. "
10345,SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"sequences CONJUNCTION trees. trees CONJUNCTION sequences. trees CONJUNCTION graphs. graphs CONJUNCTION trees. graphs HYPONYM-OF optimizing combinatorial spaces. sequences HYPONYM-OF optimizing combinatorial spaces. trees HYPONYM-OF optimizing combinatorial spaces. black - box function evaluations USED-FOR combinatorial spaces. Bayesian optimization ( BO ) USED-FOR problems. framework USED-FOR problems. BO approach USED-FOR combinatorial spaces. deep generative models ( DGMs ) USED-FOR latent representation of structures. discrete structure USED-FOR function evaluation. latent space USED-FOR surrogate model. DGM USED-FOR surrogate model. LADDER HYPONYM-OF approach. latent space representation USED-FOR surrogate modeling. structural information PART-OF decoded structures. structural information PART-OF structure - coupled kernel. real - world benchmarks EVALUATE-FOR LADDER. LADDER COMPARE BO. BO COMPARE LADDER. LADDER COMPARE state - of - the - art methods. state - of - the - art methods COMPARE LADDER. LADDER COMPARE latent space method. latent space method COMPARE LADDER. real - world benchmarks EVALUATE-FOR BO. BO COMPARE latent space method. latent space method COMPARE BO. real - world benchmarks EVALUATE-FOR state - of - the - art methods. Task is drug design. OtherScientificTerm are physical lab experiments, continuous spaces, continuous space, inductive bias, and black - box function. ","This paper studies the problem of drug design, where the goal is to design a drug that can be used in physical lab experiments. The authors propose Bayesian optimization (BO) to solve the problems of optimizing combinatorial spaces such as sequences, trees, and graphs. The main idea is to use deep generative models (DGMs) to learn a latent representation of structures, which can then be used as a surrogate model for the surrogate model trained with a DGM. The surrogate model is trained using the latent space representation of the surrogate modeling, which is then used to evaluate the function evaluation on the discrete structure of the function with respect to a discrete structure.  The authors show that the proposed approach, LADDER, outperforms BO on a variety of real-world benchmarks, and outperforms LADER on a number of state-of-the-art methods. ","This paper proposes a new framework for solving combinatorial optimization problems in the context of drug design. The authors propose Bayesian optimization (BO) to solve these problems. The main idea is to use deep generative models (DGMs) to learn a latent representation of structures. This latent space representation is then used for surrogate modeling, where the surrogate model is trained on the DGM. The proposed approach is evaluated on several real-world benchmarks and compared to LADDER, BO, and state-of-the-art methods."
10381,SP:37adabdc6615c5199a481553c8ccc06d57363614,"representation of state - action value functions USED-FOR regret minimization. constant regret FEATURE-OF MDP. linear reward function FEATURE-OF MDP. low - rank MDPs CONJUNCTION zero inherent Bellman error. zero inherent Bellman error CONJUNCTION low - rank MDPs. condition USED-FOR problems. LSVI - UCB CONJUNCTION ELEANOR. ELEANOR CONJUNCTION LSVI - UCB. constant regret bound USED-FOR optimistic algorithms. LSVI - UCB HYPONYM-OF optimistic algorithms. ELEANOR HYPONYM-OF optimistic algorithms. algorithm USED-FOR representation selection. representations CONJUNCTION them. them CONJUNCTION representations. constant regret EVALUATE-FOR it. representations USED-FOR it. OtherScientificTerm are linear structure, universally spanning optimal features ( UNISOFT ), Bellman closure assumption, and UNISOFT condition. Generic is representation. ","This paper proposes a new representation of state-action value functions for regret minimization based on the representation of the linear structure of the MDP with a linear reward function. The main idea is to use the universally spanning optimal features (UNISOFT) as the Bellman closure assumption. The authors show that this condition can be applied to a variety of problems, including low-rank MDPs, zero inherent Bellman error, and optimistic algorithms such as LSVI-UCB and ELEANOR. The proposed algorithm can be used for representation selection, and it is shown that it has a constant regret bound for optimistic algorithms. ",This paper proposes a new representation of state-action value functions for regret minimization. The main idea is to use the linear structure of the MDP as a linear reward function. The authors propose a Bellman closure assumption and show that the constant regret bound for optimistic algorithms such as LSVI-UCB and ELEANOR are equivalent to the one for low-rank MDPs and zero inherent Bellman error. They also show that this condition can be applied to other problems such as universally spanning optimal features (UNISOFT). The authors also propose an algorithm for representation selection based on the proposed algorithm. They show that it is equivalent to a constant regret under the UNISOFT condition and that it can be used to select the best representations for them.
10417,SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"energy conservation FEATURE-OF dynamics. Lagrangian or Hamiltonian dynamics PART-OF neural network architecture. differential equations USED-FOR approaches. legged robots CONJUNCTION robotic manipulators. robotic manipulators CONJUNCTION legged robots. contacts CONJUNCTION collisions. collisions CONJUNCTION contacts. contacts PART-OF physical systems. collisions PART-OF physical systems. robotic manipulators HYPONYM-OF physical systems. legged robots HYPONYM-OF physical systems. differentiable contact model USED-FOR contact mechanics. frictionless / frictional CONJUNCTION elastic / inelastic. elastic / inelastic CONJUNCTION frictionless / frictional. frictionless / frictional HYPONYM-OF differentiable contact model. elastic / inelastic HYPONYM-OF differentiable contact model. elastic / inelastic HYPONYM-OF contact mechanics. frictionless / frictional HYPONYM-OF contact mechanics. model USED-FOR inequality constraints. contact model USED-FOR Lagrangian and Hamiltonian neural networks. simultaneous learning of contact and system properties USED-FOR contact model. coefficients of restitution FEATURE-OF 2D and 3D physical systems. 2D and 3D physical systems EVALUATE-FOR framework. differentiable physics simulator USED-FOR downstream gradient - based optimization tasks. dynamics USED-FOR differentiable physics simulator. dynamics USED-FOR downstream gradient - based optimization tasks. planning and control HYPONYM-OF downstream gradient - based optimization tasks. OtherScientificTerm are inductive bias, and joint angles. ","This paper proposes a differentiable contact model for contact mechanics (e.g., frictionless/frictional, elastic/inelastic, etc.) in physical systems such as legged robots, robotic manipulators, and physical systems with contacts. The contact model is based on the Lagrangian or Hamiltonian dynamics in a neural network architecture. The authors show that the energy conservation of the dynamics depends on the inductive bias of the contact model, which is a result of the simultaneous learning of contact and system properties. The proposed approaches are based on differential equations, where the joint angles between the contact and the system are defined as the coefficients of restitution. The dynamics of the differentiable physics simulator can be used to learn the dynamics for downstream gradient-based optimization tasks such as planning and control. The model can also be used for inequality constraints. The experimental results on 2D and 3D physical systems demonstrate the effectiveness of the proposed framework.","This paper proposes a novel approach to learning the dynamics of physical systems with respect to energy conservation. The key idea is to use Lagrangian or Hamiltonian dynamics in a neural network architecture. The authors propose two approaches to learn differential equations for these approaches. The first approach is based on a differentiable contact model that combines frictionless/frictional and elastic/inelastic. The second approach relies on simultaneous learning of contact and system properties. The proposed framework is evaluated on 2D and 3D physical systems, including legged robots, robotic manipulators, and physical systems where the inductive bias is low and the joint angles are high. The model is shown to be robust to inequality constraints and can be applied to downstream gradient-based optimization tasks such as planning and control. "
10453,SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"Lipschitz constant USED-FOR parameter trajectory. 1st layer bias FEATURE-OF NNs. bounded complexity EVALUATE-FOR NNs. Task is Benevolent Training Hypothesis ( BTH ). Metric is complexity. Method are deep neural network ( NN ), and stochastic training procedure. OtherScientificTerm are training dynamics, BTH, NN ’s Lipschitz constant, input space, Dropout, and trainingand datadependent generalization bound. ","This paper studies the Benevolent training Hypothesis (BTH) in the context of deep neural network (NN). BTH is a stochastic training procedure in which the training dynamics of the NN are perturbed during training. The authors show that the complexity of NNs with 1st layer bias is bounded by the Lipschitz constant of the parameter trajectory. They also show that NNs have bounded complexity under the assumption that the input space is a dropout. Finally, they provide a training and datadependent generalization bound for NNs.",This paper studies the Benevolent Training Hypothesis (BTH) of deep neural network (NN) with a stochastic training procedure. The main idea of BTH is to use the Lipschitz constant of the parameter trajectory in the input space as a measure of the complexity of the training dynamics of the NN. The authors show that the 1st layer bias of NNs with bounded complexity can be reduced to a 1st-layer bias. They also show that BTH can be used to reduce the training and datadependent generalization bound. Dropout is used to improve the performance of Dropout. 
10489,SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"operation USED-FOR distribution. Forster transform HYPONYM-OF operation. disjoint mixture of few distributions USED-FOR distribution. polynomial - time algorithm USED-FOR distribution - independent PAC learning of halfspaces. distribution - independent PAC learning of halfspaces PART-OF Massart noise model. polynomial sample complexity FEATURE-OF polynomial - time algorithm. algorithms USED-FOR learning problem. sample complexity EVALUATE-FOR algorithms. OtherScientificTerm are anticoncentration properties, and bit complexity. ","This paper studies the problem of distribution-independent PAC learning of halfspaces in the Massart noise model. The authors propose a new operation called Forster transform, where the distribution is a disjoint mixture of few distributions, and the anticoncentration properties of the distribution are learned. The polynomial-time algorithm is proposed to solve the distribution-dependent PAC learning problem.  The authors show that the polynomially sample complexity of the Poisson-Time algorithm is O(1/\sqrt{T})^2, which is a significant improvement over existing algorithms in the sample complexity. The bit complexity is also improved.",The paper proposes a new operation for learning a distribution from a disjoint mixture of few distributions. The operation is called Forster transform. The authors propose a polynomial-time algorithm for distribution-independent PAC learning of halfspaces in the Massart noise model. The main idea is to use anticoncentration properties of the distribution to reduce the bit complexity of the learning problem. The proposed algorithms are shown to have better sample complexity than existing algorithms.
10525,SP:e5229305af00067ae2dbabd903e585964aec8928,"models USED-FOR graph - based learning tasks. Graph neural networks USED-FOR graph - based learning tasks. Graph neural networks HYPONYM-OF models. adversarial attacks USED-FOR graph - level classification. biochemistry and social network analysis HYPONYM-OF real - life applications. Bayesian optimisation - based attack method USED-FOR graph classification models. graph properties CONJUNCTION constraints. constraints CONJUNCTION graph properties. constraints CONJUNCTION modes of attack. modes of attack CONJUNCTION constraints. graph properties FEATURE-OF graph classification tasks. graph classification tasks EVALUATE-FOR method. adversarial robustness EVALUATE-FOR graph classification models. Task is node - level classification tasks. OtherScientificTerm are unrealistic setups, perturbation, and adversarial samples. ","This paper proposes a Bayesian optimisation-based attack method to improve the robustness of graph classification models trained with Graph neural networks. The proposed method is based on Bayesian optimization, and it is shown that the proposed method improves the adversarial robustness on several graph classification tasks. The authors also show that their method can be applied to a variety of real-life applications such as biochemistry and social network analysis.","This paper proposes a Bayesian optimisation-based attack method for graph classification models. Graph neural networks are popular models for graph-based learning tasks, but they are vulnerable to adversarial attacks in real-life applications such as biochemistry and social network analysis. The proposed method is able to improve adversarial robustness on graph classification tasks with graph properties, constraints, and modes of attack. The authors show that under unrealistic setups, perturbation can be used to generate adversarial samples."
10561,SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"distribution shifts FEATURE-OF Machine learning models. adaptation USED-FOR label distribution shift. adaptation USED-FOR online setting. online learning USED-FOR online label shift adaptation. Follow The Leader ( FTL ) CONJUNCTION Online Gradient Descent ( OGD ). Online Gradient Descent ( OGD ) CONJUNCTION Follow The Leader ( FTL ). Online Gradient Descent ( OGD ) USED-FOR adaptation algorithms. online learning techniques USED-FOR adaptation algorithms. Online Gradient Descent ( OGD ) HYPONYM-OF online learning techniques. Follow The Leader ( FTL ) HYPONYM-OF online learning techniques. OGD USED-FOR label shift scenarios. OtherScientificTerm are test - time label distribution, regret bounds, and simulated and real world label distribution shifts. Generic is model. Task is estimation of the expected test loss. ","This paper studies the problem of online label shift adaptation in the online setting, where the test-time label distribution changes over time. The authors propose two adaptation algorithms, Follow The Leader (FTL) and Online Gradient Descent (OGD), which are based on online learning techniques. The main contribution of the paper is the estimation of the expected test loss. The regret bounds are established for both simulated and real world label distribution shifts, and the model is shown to be robust to label shift scenarios.","This paper studies the problem of label distribution shifts in Machine learning models. The authors propose an adaptation for the online setting, where the test-time label distribution is not available. They propose two adaptation algorithms, Follow The Leader (FTL) and Online Gradient Descent (OGD), which are based on online learning for online label shift adaptation. The main contribution of the model is the estimation of the expected test loss. The regret bounds are derived for both simulated and real world label distributions shifts. They show that OGD is more robust to label shift scenarios. "
10597,SP:806515ae07fb1c9d02773592005d53d4158ef102,distribution FEATURE-OF detection and localization of gradual changes. time - ordered observations USED-FOR distribution. time - ordered observations USED-FOR detection and localization of gradual changes. discontinuity jump in distribution USED-FOR abrupt setting. method USED-FOR detecting and localizing gradual changes. features FEATURE-OF distribution. prior knowledge FEATURE-OF distribution. prior knowledge FEATURE-OF features. detection CONJUNCTION localization. localization CONJUNCTION detection. method USED-FOR detection. method USED-FOR localization. Method is data generating model. ,This paper proposes a method for detecting and localizing gradual changes in the presence of discontinuity jump in the data generating model. The proposed method is based on time-ordered observations of the distribution of the time-order observations for detection and localization of gradual changes. The authors show that the features of a distribution with prior knowledge can be used as a proxy for the prior knowledge of the features in the distribution. The method is then applied to both detection and the localization. The experimental results demonstrate the effectiveness of the proposed method.,"This paper proposes a new method for detecting and localizing gradual changes in the context of time-ordered observations of gradual changes. The proposed method is based on a data generating model, where the distribution of the distribution is learned from prior knowledge. The authors show that the proposed method improves detection and localization in an abrupt setting with discontinuity jump in distribution. "
10633,SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"brain USED-FOR blind source separation ( BSS ) problems. linear BSS problems PART-OF signal processing. Independent Component Analysis ( ICA ) USED-FOR linear BSS problems. neural architecture CONJUNCTION synaptic learning rules. synaptic learning rules CONJUNCTION neural architecture. objective function USED-FOR biologically plausible NN. objective function USED-FOR ICA. neural architecture PART-OF biologically plausible NN. synaptic learning rules PART-OF biologically plausible NN. synaptic plasticity USED-FOR algorithm. extracellular calcium CONJUNCTION local field potential. local field potential CONJUNCTION extracellular calcium. local field potential CONJUNCTION nitric oxide. nitric oxide CONJUNCTION local field potential. neuromodulators CONJUNCTION extracellular calcium. extracellular calcium CONJUNCTION neuromodulators. OtherScientificTerm are biological circuit, biophysical variables, and synapse. Method are ICA neural network ( NN ), and NN. Task is synaptic weight update. ","This paper studies the problem of blind source separation (BSS) problems in the brain. The authors consider linear BSS problems in signal processing with Independent Component Analysis (ICA). The authors show that the ICA neural network (NN) is biologically plausible, and that the neural architecture and synaptic learning rules of a biologically plausible NN can be used as an objective function for ICA.  The authors then propose an algorithm based on synaptic plasticity, which is able to learn biophysical variables in the neural circuit. The NN is then trained by synaptic weight update. The proposed algorithm is shown to perform well on both extracellular calcium and the local field potential of nitric oxide.","This paper proposes a novel approach to solve blind source separation (BSS) problems in the brain. The main idea is to use Independent Component Analysis (ICA) for linear BSS problems in signal processing, where the biological circuit is represented as a set of biophysical variables. The authors propose an ICA neural network (NN), which is a biologically plausible NN with a neural architecture and synaptic learning rules. The proposed algorithm is based on synaptic plasticity, and the authors show that the ICA can be applied to any biologically plausible nN with the same objective function. The paper also shows that the proposed algorithm can be used to compute the local field potential, the extracellular calcium, the neuromodulators and the nitric oxide. "
10669,SP:22f8b517a3df65144412938f5891c463d7bae0ab,"neural activity USED-FOR task - related behavior. Recurrent Neural Networks ( RNNs ) USED-FOR neural activity. Recurrent Neural Networks ( RNNs ) USED-FOR task - related behavior. neuroscience CONJUNCTION machine learning. machine learning CONJUNCTION neuroscience. space of solutions FEATURE-OF task. RNNs COMPARE neural data. neural data COMPARE RNNs. space of solutions USED-FOR tasks. two - neuron network USED-FOR task. discrete dynamical regimes USED-FOR diversity. Delayed discrimination CONJUNCTION Interval discrimination. Interval discrimination CONJUNCTION Delayed discrimination. Interval discrimination CONJUNCTION Time reproduction. Time reproduction CONJUNCTION Interval discrimination. Delayed discrimination HYPONYM-OF neuroscience - inspired tasks. Time reproduction HYPONYM-OF neuroscience - inspired tasks. Interval discrimination HYPONYM-OF neuroscience - inspired tasks. neural activity FEATURE-OF networks. extrapolation patterns USED-FOR dynamical objects. tool USED-FOR reduced dynamics of networks. compact directed graph USED-FOR tool. compact directed graph USED-FOR reduced dynamics of networks. Machine learning CONJUNCTION Neuroscience. Neuroscience CONJUNCTION Machine learning. Method is machine learning algorithms. OtherScientificTerm are underspecification, hidden structure, and neural features. Generic is representation. ","This paper studies the problem of unsupervised learning with recurrent neural networks (RNNs). The authors show that RNNs are able to learn a task-related behavior with neural activity in the space of solutions for a given task. The authors then propose a two-neuron network to solve the task using discrete dynamical regimes, and show that the diversity of the learned representations can be used to improve the performance of the RNN. They also show that under certain conditions, the representation learned by RNN can be better than the original neural data. ","This paper proposes a novel representation learning framework for deep neural networks. The key idea is to use a two-neuron network to model the dynamics of the neural network. The proposed framework is based on the notion of ""diversity"", which is defined as the number of neurons in the network that are active in a given task. The authors show that the diversity of the network is a function of the size of the task space, and that it depends on the depth of the space of solutions for the task.  The authors also show that this diversity can be used to improve the performance of RNNs on a variety of tasks, including Delayed discrimination, Interval discrimination, and Time reproduction."
10705,SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"Modeling distributions of covariates PART-OF unsupervised learning. density estimation PART-OF unsupervised learning. density estimation HYPONYM-OF Modeling distributions of covariates. arbitrary conditional density estimation USED-FOR conditional distribution. covariates FEATURE-OF conditional distribution. arbitrary conditional density estimation HYPONYM-OF problem. prior knowledge USED-FOR inference. unobserved features CONJUNCTION observed features xo. observed features xo CONJUNCTION unobserved features. ACE USED-FOR complexity. learning one - dimensional conditionals USED-FOR problem. energy function USED-FOR densities. approach COMPARE prior methods. prior methods COMPARE approach. arbitrary conditional likelihood estimation CONJUNCTION data imputation. data imputation CONJUNCTION arbitrary conditional likelihood estimation. ACE USED-FOR arbitrary conditional likelihood estimation. ACE USED-FOR data imputation. state - of - the - art USED-FOR arbitrary conditional likelihood estimation. state - of - the - art EVALUATE-FOR ACE. benchmarks EVALUATE-FOR state - of - the - art. benchmarks EVALUATE-FOR data imputation. benchmarks EVALUATE-FOR ACE. OtherScientificTerm are distributions of covariates, joint distribution, and one - dimensional conditionals. Generic is method. Method is Arbitrary Conditioning with Energy ( ACE ). ","This paper studies the problem of density estimation of covariates in unsupervised learning. The authors propose a new method called Arbitrary Conditioning with Energy (ACE) to solve the problem. ACE is based on the notion of arbitrary conditional density estimation, where the conditional distribution is defined as the sum of the covariates of the joint distribution. The densities are computed by learning one-dimensional conditionals, and then the energy function is used to estimate the densities. ACE uses prior knowledge to guide the inference, and is shown to reduce the complexity. The proposed approach is also shown to perform better than prior methods on several benchmarks for data imputation and arbitrary conditional likelihood estimation. ","The paper proposes a new method for learning distributions of covariates in unsupervised learning, called Arbitrary Conditioning with Energy (ACE). ACE is an extension of density estimation, which is an important problem in the context of modeling distributions of covariances. The authors propose to learn a conditional distribution with covariates that depends on the joint distribution of the unobserved features and the observed features xo, and then use prior knowledge to perform inference on the conditional distribution. The problem is formulated as learning one-dimensional conditionals for the problem, where the densities are computed using an energy function. ACE is shown to reduce the complexity of the problem. The proposed approach is evaluated on several benchmarks for data imputation and arbitrary conditional likelihood estimation."
10741,SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"MSE or L1 loss function USED-FOR low - level vision. single image super - resolution ( SISR ) HYPONYM-OF low - level vision. texture and edge areas COMPARE smooth areas. smooth areas COMPARE texture and edge areas. smooth areas PART-OF photographic images. adaptive weighted loss USED-FOR deep networks. adaptive weighted loss USED-FOR SISR. adaptive weighted loss USED-FOR situations. SISR USED-FOR deep networks. textured and edge pixels HYPONYM-OF situations. variance estimation USED-FOR SISR solutions. sparsity prior USED-FOR regularizing SISR solutions. uncertainty estimation USED-FOR regularizing SISR solutions. visual quality EVALUATE-FOR SISR. uncertainty - driven loss COMPARE MSE or L1 loss. MSE or L1 loss COMPARE uncertainty - driven loss. uncertainty - driven loss COMPARE loss functions. loss functions COMPARE uncertainty - driven loss. SISR networks EVALUATE-FOR uncertainty - driven loss. computation EVALUATE-FOR loss functions. OtherScientificTerm are visual information, pixel - by - pixel basis, high - resolution image ( mean ), and uncertainty ( variance ). Task is spatial adaptation. Method is network architectures. ","This paper proposes a new MSE or L1 loss function for low-level vision, called single image super-resolution (SISR) in the context of spatial adaptation. SISR is an adaptive weighted loss for deep networks that can be used in situations such as textured and edge pixels, where the visual information is not available in the pixel-by-pixel basis. The paper shows that the smooth areas in the photographic images are more informative than the texture and edge areas. The authors also show that the variance estimation is a good way to regularize the SisR solutions with a sparsity prior, and that the uncertainty estimation can be useful for regularizing SISRs with high-resolution image (mean) and low-resolution images (mean). Finally, the authors show that their uncertainty-driven loss performs better than MSE/L1 loss in terms of visual quality and computation compared to other loss functions.  ","This paper proposes a new MSE or L1 loss function for low-level vision, single image super-resolution (SISR). The key idea is to regularize the pixel-by-pixel basis of the high-resolution image (mean) and the low-resolution image (variance). This is done by using a sparsity prior. The authors show that the adaptive weighted loss for SISR can be used in two situations: textured and edge pixels. The smooth areas in the photographic images are more similar to the texture and edge areas. The paper also shows that the uncertainty-driven loss outperforms other loss functions in computation. "
10777,SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"PAC - Bayesian generalization bounds USED-FOR adversarial robustness. PACBayesian framework USED-FOR averaged risk. perturbations FEATURE-OF averaged risk. majority votes FEATURE-OF perturbations. robust model USED-FOR attacks. adversarial attacks HYPONYM-OF attacks. Generic is model. Method are worst - case analysis, theoretically founded analysis, and PAC - Bayesian framework. Task is learning phase. ",This paper studies PAC-Bayesian generalization bounds for adversarial robustness. The authors propose a PACBayesian framework for estimating the averaged risk of perturbations in the learned model under the worst-case analysis. The main contribution of the paper is a theoretically founded analysis that shows that the robust model can be used to defend against attacks such as adversarial attacks. The paper also provides theoretical guarantees for the learning phase.,"This paper proposes PAC-Bayesian generalization bounds for adversarial robustness. The PACBayesian framework is used to estimate the averaged risk of perturbations in the majority votes. The authors show that the model is robust to adversarial attacks. They also provide a theoretically founded analysis of the learning phase. Finally, the authors propose a robust model for attacks."
10813,SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,Logical reasoning USED-FOR querying mechanism. large and incomplete databases USED-FOR querying mechanism. Knowledge Graphs ( KGs ) USED-FOR Logical reasoning. spatial geometries USED-FOR query representations. spatial geometries USED-FOR approaches. boxes HYPONYM-OF spatial geometries. transformation tricks USED-FOR unions. Probabilistic Entity Representation Model ( PERM ) USED-FOR entities. semantic position CONJUNCTION smooth decision boundary. smooth decision boundary CONJUNCTION semantic position. Multivariate Gaussian density USED-FOR semantic position. Multivariate Gaussian density USED-FOR smooth decision boundary. mean and covariance parameters USED-FOR semantic position. Multivariate Gaussian density USED-FOR Probabilistic Entity Representation Model ( PERM ). mean and covariance parameters FEATURE-OF Multivariate Gaussian density. Multivariate Gaussian density USED-FOR entities. intersection CONJUNCTION union. union CONJUNCTION intersection. projection CONJUNCTION intersection. intersection CONJUNCTION projection. projection HYPONYM-OF closed logical operations. union HYPONYM-OF closed logical operations. intersection HYPONYM-OF closed logical operations. end - to - end objective function USED-FOR union. end - to - end objective function USED-FOR closed logical operations. PERM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PERM. logical query reasoning problem EVALUATE-FOR PERM. logical query reasoning problem EVALUATE-FOR state - of - the - art methods. evaluation metrics EVALUATE-FOR public benchmark KG datasets. public benchmark KG datasets EVALUATE-FOR state - of - the - art methods. evaluation metrics EVALUATE-FOR state - of - the - art methods. public benchmark KG datasets EVALUATE-FOR PERM. evaluation metrics EVALUATE-FOR PERM. work COMPARE methods. methods COMPARE work. F1 EVALUATE-FOR methods. F1 EVALUATE-FOR work. COVID-19 drugrepurposing case study EVALUATE-FOR PERM ’s competence. low - dimensional visualization of the Gaussian representations USED-FOR query answering process. Task is logical operations of projection and intersection. OtherScientific,"This paper proposes a new querying mechanism based on Knowledge Graphs (KGs) for large and incomplete databases. Logical reasoning is an important problem in many domains. The authors propose a query answering process based on low-dimensional visualization of the Gaussian representations of the query representations. The query representations are based on spatial geometries such as boxes, boxes, and boxes. The proposed approach is based on the Probabilistic Entity Representation Model (PERM) which is a Multivariate Gaussian density to represent the entities and the semantic position of the entities. The main contribution of the paper is a new end-to-end objective function for the union, the projection, and the intersection of the two logical operations of projection and intersection. The paper also proposes two transformation tricks for the unions. The experimental results show that PERM outperforms state-of-the-art methods on F1 and public benchmark KG datasets. PERM also performs well on the logical query reasoning problem in the COVID-19 drugrepurposing case study.","This paper proposes a new querying mechanism for large and incomplete databases. Logical reasoning on Knowledge Graphs (KGs) is an important topic in the literature. The authors propose a query answering process based on low-dimensional visualization of the Gaussian representations of the query representations. The query representations are based on spatial geometries (e.g., boxes, boxes, etc.). The authors introduce two approaches to learn these query representations: (1) a Probabilistic Entity Representation Model (PERM) that is based on Multivariate Gaussian density for entities, (2) a Probabilistic Population Model (PPM) based on the mean and covariance parameters for the semantic position and the smooth decision boundary, and (3) an end-to-end objective function for learning closed logical operations such as projection, union, and intersection. The proposed method is evaluated on the COVID-19 drugrepurposing case study, and compared to state-of-the-art methods on several public benchmark KG datasets and evaluation metrics such as F1. PerM outperforms the state of the art methods on the logical query reasoning problem, and is competitive with other methods."
10849,SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"memory scaling CONJUNCTION gradient degradation issues. gradient degradation issues CONJUNCTION memory scaling. Gradient - based hyperparameter optimization USED-FOR few - shot meta - learning. algorithm USED-FOR memory scaling issues. forward - mode differentiation USED-FOR memory scaling issues. noise reduction properties EVALUATE-FOR algorithm. theoretical guarantees FEATURE-OF algorithm. noise reduction properties FEATURE-OF theoretical guarantees. greedy gradientbased alternatives COMPARE black - box methods. black - box methods COMPARE greedy gradientbased alternatives. hyperparameter search ranges FEATURE-OF CIFAR-10. Generic is tasks. OtherScientificTerm are hyperparameters, and greediness. Method is unrolled optimization. ","This paper proposes a new algorithm for few-shot meta-learning based on Gradient-based hyperparameter optimization. The main idea is to use forward-mode differentiation to solve the memory scaling issues and gradient degradation issues in memory scaling. The proposed algorithm has theoretical guarantees on the noise reduction properties of the proposed algorithm, as well as on the greedy gradientbased alternatives compared to black-box methods. The authors also provide a theoretical analysis of the hyperparameters of the algorithm. The theoretical analysis shows that the algorithm achieves better performance on CIFAR-10 in terms of the number of iterations and the hyper parameter search ranges. ","This paper proposes Gradient-based hyperparameter optimization for few-shot meta-learning. The main idea is to use forward-mode differentiation to address memory scaling and gradient degradation issues. The authors provide theoretical guarantees on the noise reduction properties of the proposed algorithm, as well as empirical results on CIFAR-10 in terms of the number of hyperparameters and the greediness of the algorithm. The proposed algorithm outperforms greedy gradientbased alternatives and black-box methods in the proposed tasks. "
10885,SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"systems PART-OF Human reasoning. Neural sequence models USED-FOR structured tasks. neural sequence model USED-FOR candidate generations. symbolic reasoning module USED-FOR logical consistency. neural System 1 CONJUNCTION logical System 2. logical System 2 CONJUNCTION neural System 1. neural inference USED-FOR neural System 1. neural inference USED-FOR approach. story generation CONJUNCTION grounded instruction - following. grounded instruction - following CONJUNCTION story generation. accuracy EVALUATE-FOR neurally - based generations. coherence CONJUNCTION accuracy. accuracy CONJUNCTION coherence. coherence EVALUATE-FOR neurally - based generations. approach USED-FOR neurally - based generations. coherence EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Method are System 1, System 1 - like sequence models, and System 2 - inspired logical reasoning. Generic is they. ",This paper proposes a new neural sequence model for candidate generations for structured tasks. The proposed approach is based on neural inference for neural System 1 and logical System 2. The authors propose a symbolic reasoning module to improve the logical consistency of the candidate generations. They show that the proposed approach improves the coherence and accuracy of neurally-based generations in terms of both story generation and grounded instruction-following. ,"This paper proposes a novel approach to learn a sequence model for learning a sequence of candidate generations. The idea is to use neural sequence models for structured tasks such as story generation, grounded instruction-following, and logical system 2-inspired logical reasoning. The proposed approach is based on neural inference on neural System 1 and logical System 2. The authors show that the proposed approach improves the coherence and accuracy of neurally-based generations. "
10921,SP:d77d046095e4c8336c0c76ac48cb046923230753,"off - policy evaluation ( OPE ) USED-FOR continuous treatment settings. personalized dose - finding HYPONYM-OF continuous treatment settings. decision rule USED-FOR historical data. discrete treatment settings FEATURE-OF OPE. estimation method USED-FOR OPE. estimation method USED-FOR continuous treatments. deep jump learning USED-FOR estimation method. deep learning CONJUNCTION multiscale change point detection. multiscale change point detection CONJUNCTION deep learning. OPE methods USED-FOR continuous treatments. OPE methods USED-FOR discrete treatments. OtherScientificTerm are treatment decision rule, and treatment space. Generic is method. Method is deep discretization. Task is Warfarin Dosing. ","This paper proposes an off-policy evaluation (OPE) for continuous treatment settings such as personalized dose-finding, where the treatment decision rule is based on historical data. The OPE is applicable to discrete treatment settings in the context of OPE. The authors propose an estimation method based on deep jump learning to estimate the OPE for continuous treatments. The proposed method uses deep discretization to learn the treatment space. The experiments show the effectiveness of the proposed OPE methods for discrete treatments and multiscale change point detection. ","This paper proposes an off-policy evaluation (OPE) for continuous treatment settings such as personalized dose-finding, where the treatment decision rule is based on historical data. The main idea of the proposed method is to use deep discretization to learn a decision rule for historical data, and then use an estimation method for OPE for continuous treatments in discrete treatment settings. The authors show that the proposed deep learning and multiscale change point detection outperform other OPE methods for discrete treatments in the treatment space. "
10957,SP:4d085e57286fdd36143108a002d16914222c239a,natural sciences CONJUNCTION engineering applications. engineering applications CONJUNCTION natural sciences. modeling framework USED-FOR inference in time - series data. Switching dynamical systems USED-FOR modeling framework. inference in time - series data USED-FOR engineering applications. inference in time - series data USED-FOR natural sciences. biology CONJUNCTION discrete - event systems. discrete - event systems CONJUNCTION biology. subordinated diffusion process FEATURE-OF Markov jump process. continuous time FEATURE-OF areas. Markov jump process USED-FOR model. biology HYPONYM-OF areas. discrete - event systems HYPONYM-OF areas. evolution equations USED-FOR prior and posterior marginal densities. Gaussian process approximation CONJUNCTION posterior inference. posterior inference CONJUNCTION Gaussian process approximation. posterior inference USED-FOR Markov jump processes. Gaussian process approximation USED-FOR diffusion level. posterior inference PART-OF continuous - time variational inference algorithm. Gaussian process approximation PART-OF continuous - time variational inference algorithm. path - wise Kullback - Leibler divergence USED-FOR Bayesian latent state estimates. variational expectation maximization USED-FOR point estimates of unknown system parameters. real - world examples EVALUATE-FOR algorithm. Material is time - series data. OtherScientificTerm is real axis. ,This paper proposes a new modeling framework for inference in time-series data for natural sciences and engineering applications. The model is based on a Markov jump process with a subordinated diffusion process. The authors use evolution equations to model prior and posterior marginal densities. The diffusion level is approximated by a Gaussian process approximation and posterior inference. The Bayesian latent state estimates are obtained by path-wise Kullback-Leibler divergence. The point estimates of unknown system parameters are obtained using variational expectation maximization. The proposed algorithm is evaluated on real-world examples.,This paper proposes a new modeling framework for inference in time-series data for natural sciences and engineering applications. The model is based on a Markov jump process with a subordinated diffusion process. The diffusion level is approximated by a Gaussian process approximation and posterior inference. The prior and posterior marginal densities are derived from evolution equations. The authors also propose a path-wise Kullback-Leibler divergence for Bayesian latent state estimates. The proposed algorithm is evaluated on real-world examples. 
10993,SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"A HYPONYM-OF linear mapping. compressed sensing CONJUNCTION phase retrieval. phase retrieval CONJUNCTION compressed sensing. model USED-FOR signal processing problems. nonlinear processing function USED-FOR model. phase retrieval HYPONYM-OF signal processing problems. compressed sensing HYPONYM-OF signal processing problems. spectrum of sensing matrices HYPONYM-OF sensing matrices. expectation propagation algorithm ( EP ) HYPONYM-OF recovery methods. spikiness FEATURE-OF spectrum. measure USED-FOR EP. EP USED-FOR recovery. spikiness of the spectrum USED-FOR EP recovery. Task are nonlinear inverse problem, phase - retrieval problems, and 1 - bit compressed sensing problems. Method are componentwise nonlinear transformation, and optimal sensing systems. OtherScientificTerm are f, spikier spectrums, and sub - Gaussian and orthogonal matrices. Generic is framework. ","This paper studies the problem of nonlinear inverse problem. The authors propose a nonlinear mapping from linear mapping to nonlinear transformation. The model is based on the nonlinear processing function, and the proposed model can be applied to signal processing problems such as compressed sensing, phase retrieval, and phase-retrieval problems. The main contribution of the paper is a new recovery methods based on expectation propagation algorithm (EP) and spikiness of sensing matrices (e.g., the spectrum of sensing matrix). The authors show that EP recovers the EP with spikier spectrums, which is a good measure for recovery. The paper also shows that EP can be used to recover the recovered EP from the spectrum. ","The paper proposes a new framework for nonlinear inverse problem. The framework is based on the idea of componentwise nonlinear transformation (C.A.A), which is a linear mapping from f to f. The model is applied to signal processing problems such as compressed sensing, phase retrieval, and phase-retrieval problems. The proposed model uses a nonlinear processing function. The authors show that the proposed framework is able to recover sub-Gaussian and orthogonal matrices of the f, and the spikier spectrums of the spectrum of sensing matrices. The paper also shows that the recovery methods are based on expectation propagation algorithm (EP) and spikiness in the spectrum. The main contribution of the paper is the proposed measure for EP for recovery, which is based upon the spiking of the spectral spectrum. "
11029,SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,auxiliary semantic information USED-FOR Generalized Zero - Shot Learning ( GZSL ). category attributes HYPONYM-OF auxiliary semantic information. cross - domain transferability CONJUNCTION category discriminability. category discriminability CONJUNCTION cross - domain transferability. category discriminability EVALUATE-FOR visual representations. cross - domain transferability EVALUATE-FOR visual representations. prototypes USED-FOR prototypical visual patterns. attribute prototypes USED-FOR DPPN. DPPN USED-FOR attribute - related local regions. attribute prototypes USED-FOR attribute - region correspondence. DPPN USED-FOR attribute - region correspondence. attribute prototypes USED-FOR DPPN. DPPN USED-FOR visual representations. semantic - visual alignment CONJUNCTION representation transferability. representation transferability CONJUNCTION semantic - visual alignment. attribute localization ability FEATURE-OF visual representations. DPPN USED-FOR visual representations. progressive attribute localization CONJUNCTION DPPN. DPPN CONJUNCTION progressive attribute localization. category prototypes USED-FOR DPPN. unifed framework USED-FOR visual representations. DPPN USED-FOR visual representations. unifed framework USED-FOR attribute and category prototypes. DPPN USED-FOR domain shift problem. DPPN USED-FOR GZSL. domain shift problem FEATURE-OF GZSL. Generic is approach. Method is Dual Progressive Prototype Network ( DPPN ). ,"This paper proposes a new approach to learn the auxiliary semantic information for Generalized Zero-Shot Learning (GZSL). The proposed approach is called Dual Progressive Prototype Network (DPPN) and is based on attribute prototypes. The proposed DPPN uses attribute prototypes to generate prototypical visual patterns that can be used as attribute-related local regions. The authors show that the cross-domain transferability and category discriminability of the visual representations can be improved by using the attribute prototypes in DPPNs. The paper also shows that the attribute localization ability of visual representations is improved by the use of the unifed framework for visual representations. Finally, the authors demonstrate that the proposed approach can be applied to the domain shift problem of GZSL. ","This paper proposes a new approach to learn auxiliary semantic information for Generalized Zero-Shot Learning (GZSL). The proposed approach is based on the Dual Progressive Prototype Network (DPPN), which is an unifed framework for learning both attribute and category prototypes for prototypical visual patterns. DPPN is able to learn attribute-related local regions and attribute prototypes for attribute-region correspondence. The cross-domain transferability and category discriminability of visual representations can be used to improve the performance of GZSL in terms of semantic-visual alignment, representation transferability, and attribute localization ability. Experiments show the effectiveness of the proposed approach. "
11065,SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"Defocus blur HYPONYM-OF blur effects. blur effects PART-OF images. end - to - end deep learning approach USED-FOR removing defocus blur. all - in - focus image USED-FOR consequent vision tasks. end - to - end deep learning approach USED-FOR all - in - focus image. accuracy EVALUATE-FOR models. linear parametric form FEATURE-OF spatially variant defocus blur kernels. fixed - point iteration USED-FOR GKM - based deblurring. fixed - point iteration USED-FOR deep neural network. GKMNet HYPONYM-OF deep neural network. scale - recurrent attention module USED-FOR mixing coefficients. GKM USED-FOR defocus deblurring. mixing coefficients PART-OF GKM. lightweight scale - recurrent architecture CONJUNCTION scale - recurrent attention module. scale - recurrent attention module CONJUNCTION lightweight scale - recurrent architecture. mixing coefficients USED-FOR defocus deblurring. scale - recurrent attention module USED-FOR GKMNet. lightweight scale - recurrent architecture USED-FOR GKMNet. model complexity CONJUNCTION computational efficiency. computational efficiency CONJUNCTION model complexity. GKMNet COMPARE defocus deblurring methods. defocus deblurring methods COMPARE GKMNet. computational efficiency EVALUATE-FOR GKMNet. model complexity EVALUATE-FOR GKMNet. OtherScientificTerm are spatially variant amount, and defocus blur. ",This paper studies the problem of removing defocus blur from images. The authors propose an end-to-end deep learning approach for removing the all-in-focus image from a deep neural network trained with GKMNet. They show that the accuracy of models trained with this approach can be improved by using a fixed-point iteration for GKMs. They also provide a spatially variant amount of mixing coefficients for the mixing coefficients in GKMS and a scale-recurrent attention module to improve the performance of the deep network. ,"This paper proposes a novel method for removing defocus blur in images. Defocus blur is defined as the spatially variant amount of the blur effects in an image. The authors propose an end-to-end deep learning approach to remove the all-in-focus image for consequent vision tasks. In particular, the authors propose a GKM-based deblurring based on a fixed-point iteration for the deep neural network, which is a variant of GKN with mixing coefficients and a scale-reward attention module. The paper shows that the proposed models achieve better accuracy and computational efficiency compared to existing models. "
11101,SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"models USED-FOR SSVRL. visual content PART-OF videos. RGB frames CONJUNCTION motion vectors. motion vectors CONJUNCTION RGB frames. motion vectors USED-FOR low - resolution optical flows. compressed videos USED-FOR motion vectors. supervision signals FEATURE-OF motion vectors. multi - instance InfoNCE loss USED-FOR cross guidance contrastive learning algorithm. downstream tasks EVALUATE-FOR MVCGC. MVCGC COMPARE competitors. competitors COMPARE MVCGC. Generic are methods, and method. OtherScientificTerm is mutual information. Metric is storage and computation efficiency. ","This paper proposes a new model for SSVRL based on the multi-instance InfoNCE loss. The proposed method is based on a cross guidance contrastive learning algorithm, which is a cross-encoder-decoder architecture. The main idea is to learn the mutual information between the RGB frames and the motion vectors in low-resolution optical flows from compressed videos with visual content. The authors show that the proposed MVCGC outperforms the state-of-the-art methods on several downstream tasks.","This paper proposes a new model for SSVRL, called Multi-instance InfoNCE loss (MVCGC), which is a cross guidance contrastive learning algorithm. The proposed method is based on mutual information between RGB frames and motion vectors in low-resolution optical flows. The motion vectors are learned from compressed videos with visual content. The authors show that MVCGC outperforms other methods in terms of storage and computation efficiency on downstream tasks."
11137,SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"Bayesian treatment USED-FOR overconfidence. Bayesian treatment USED-FOR ReLU nets. overconfidence FEATURE-OF ReLU nets. features FEATURE-OF BNN. ReLU features USED-FOR Bayesian linear models. it USED-FOR BNNs. model COMPARE BNNs. BNNs COMPARE model. infinite ReLU features FEATURE-OF finite ReLU BNNs. GP USED-FOR finite ReLU BNNs. model USED-FOR GP posterior. it USED-FOR ReLU BNN. Method are ReLU Bayesian neural networks ( BNNs ), and Gaussian process ( GP ). OtherScientificTerm are infinite - width limit, and asymptotic overconfidence. ","This paper studies the problem of overconfidence in ReLU Bayesian neural networks (BNNs) under the infinite-width limit. The authors propose a Bayesian treatment for the overconfidence of ReLU nets, which is based on the Gaussian process (GP). The authors show that the features of a BNN with infinite ReLU features are asymptotically better than those of Bayesian linear models. They also show that it can be used to train BNNs with finite GP. Finally, they show that their model can be applied to the GP posterior of a ReLU BNN. ","This paper studies the overconfidence of ReLU Bayesian neural networks (BNNs). The authors propose a Bayesian treatment to reduce overconfidence in ReLU nets. The authors show that ReLU features of a BNN with infinite-width limit are more robust to overconfidence than those of Bayesian linear models. They also show that the Gaussian process (GP) of a ReLU BNN is more robust than the GP of BNNs, and they show that it can be used to improve the performance of the BNN. They further show that this model can improve the GP posterior of the ReLU bNN, and that it is able to achieve asymptotic overconfidence. "
11173,SP:e77276f61626e896f6a985296f1d832129242cdf,"tools USED-FOR finite - sample confidence bounds. LUCB CONJUNCTION Successive Elimination. Successive Elimination CONJUNCTION LUCB. tools USED-FOR estimation of potentially complex nuisance functions. finite - sample confidence bounds USED-FOR estimation of potentially complex nuisance functions. finite - sample confidence bounds USED-FOR asymptotic variance. Successive Elimination USED-FOR best - arm - identification algorithms. LUCB USED-FOR best - arm - identification algorithms. bounds USED-FOR best - arm - identification algorithms. sample complexity EVALUATE-FOR upper bounds. upper bounds EVALUATE-FOR method. sample complexity EVALUATE-FOR method. OtherScientificTerm are data collection mechanism, and arm. Method is bestarm - identification bandit framework. Material is artificially generated data. ",This paper proposes a new bestarm-identification bandit framework. The authors propose two tools for finite-sample confidence bounds for the estimation of potentially complex nuisance functions: LUCB and Successive Elimination. The main contribution of the paper is to provide asymptotic variance bounds for best-arm identification algorithms based on the bounds obtained by the bounds derived by LucB. The proposed method is shown to have lower sample complexity than existing methods and achieves better upper bounds than existing best-armed-identifying algorithms. ,"This paper proposes a new bestarm-identification bandit framework. The authors propose two tools for finite-sample confidence bounds for estimation of potentially complex nuisance functions, LUCB and Successive Elimination. The main idea is to use the data collection mechanism to estimate the asymptotic variance between the best arm and the worst arm. The bounds for best-arm-imitation algorithms are based on the bounds for the best-armed-inference algorithms. The proposed method is shown to have better upper bounds on the sample complexity and lower bounds on sample complexity on artificially generated data."
11209,SP:471361588bfc6c6033631509d1e43e77fd9721ce,"scalability EVALUATE-FOR distributed learning. communication FEATURE-OF gradient. algorithm USED-FOR biased compression. variance FEATURE-OF stochastic gradient. moving average USED-FOR history gradients. moving average USED-FOR variance. compression error USED-FOR ErrorCompensatedX. asymptotic convergence rate EVALUATE-FOR ErrorCompensatedX. unified theoretical analysis framework USED-FOR variance reduced algorithms. Metric are Communication cost, communication cost, and convergence speed. Method are stochastic gradient descent, training without compression, and error compensation. ",This paper studies the scalability of distributed learning with stochastic gradient descent. The authors consider the communication cost of the gradient with respect to the number of iterations of training without compression. They show that the algorithm with biased compression has a compression error of $\epsilon$ and that the variance of the stochedastic gradient is bounded by a moving average of the history gradients. The convergence speed of the asymptotic convergence rate of ErrorCompensatedX is shown by a unified theoretical analysis framework for variance reduced algorithms. ,"This paper studies the scalability of distributed learning with stochastic gradient descent. The authors propose a new algorithm for biased compression, ErrorCompensatedX, where the communication cost of the gradient depends on the communication of the history gradients, and the variance of the observed gradient is computed using a moving average of the moving average. The paper provides a unified theoretical analysis framework for variance reduced algorithms, and provides convergence speed and asymptotic convergence rate. The main contribution of the paper is the analysis of training without compression and error compensation."
11245,SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"graph USED-FOR model. local explainability CONJUNCTION global explainability. global explainability CONJUNCTION local explainability. performant paradigm USED-FOR multi - grained explainability. pre - training and fine - tuning idea USED-FOR explainer. pre - training and fine - tuning idea USED-FOR multi - grained explanations. explainer USED-FOR multi - grained explanations. synthetic and real - world datasets EVALUATE-FOR explainer. AUC EVALUATE-FOR baselines. explainer COMPARE baselines. baselines COMPARE explainer. explaining graph classification EVALUATE-FOR baselines. AUC EVALUATE-FOR explaining graph classification. synthetic and real - world datasets EVALUATE-FOR baselines. explaining graph classification EVALUATE-FOR explainer. AUC EVALUATE-FOR explainer. Method are graph neural network ( GNN ), explainers, pre - training phase, and fine - tuning phase. Metric is explainability. Generic is approaches. OtherScientificTerm are class - wise patterns, local context, and class - wise characteristics. ","This paper studies the problem of explainability of graph neural network (GNN) explainers. The authors propose a performant paradigm for multi-grained explainability based on a pre-training and fine-tuning idea to improve the explainer’s local explainability and global explainability. The model is trained on a graph, where the class-wise patterns are represented by a local context and the local context is represented by the global context. The explainer is trained using the pre-train phase, and the fine-tune phase is used to improve explainer's performance on both synthetic and real-world datasets. The proposed explainer outperforms existing baselines in explaining graph classification on AUC.","This paper proposes a new approach to improve the explainability of graph neural network (GNN) models. The authors propose a performant paradigm for multi-grained explainability, which is based on the pre-training and fine-tuning idea. The model is trained on a graph, where the class-wise patterns are learned in the local context, and the global context is learned in a global context. The proposed approach is evaluated on both synthetic and real-world datasets, showing that the proposed explainer outperforms existing baselines on explaining graph classification and AUC. "
11281,SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"subgraph USED-FOR methods. method USED-FOR counterfactual explanations. GNNs USED-FOR counterfactual explanations. GNNs USED-FOR common decision logic. common decision boundaries USED-FOR GNN. GNN USED-FOR they. common decision boundaries USED-FOR they. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are noise, human intuition, explanations, and edges. ","This paper proposes a new method for providing counterfactual explanations for graph neural networks (GNNs). The proposed methods are based on a subgraph, where the noise is added to the input graph and the explanations are computed using human intuition. The authors show that GNNs can be used to provide common decision logic for GNN with common decision boundaries, and they can also be used as a GNN to provide explanations for graphs with edges.","This paper proposes a new method for generating counterfactual explanations for graph neural Networks (GNNs). The method is based on the idea that GNNs can be used to generate explanations for common decision logic. The methods are based on a subgraph, where the noise is generated by human intuition and the explanations are generated by GNN. The authors show that they can generate explanations that are close to common decision boundaries of a GNN, and that they are also close to the edges of the graph."
11317,SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"information bottleneck CONJUNCTION adversarial feedback. adversarial feedback CONJUNCTION information bottleneck. information bottleneck USED-FOR VoiceMixer. adversarial feedback USED-FOR VoiceMixer. self - supervised representation learning USED-FOR information bottleneck. self - supervision USED-FOR model. adversarial feedback USED-FOR discriminator. voice style FEATURE-OF generalization. content and style discriminator PART-OF discriminator. generalization EVALUATE-FOR model. self - supervision USED-FOR content and style discriminator. transfer EVALUATE-FOR model. content information USED-FOR audio quality. audio quality EVALUATE-FOR model. Task is voice conversion. Material is converted voice. OtherScientificTerm are converted speech containing source speech style, and source speech content. ","This paper studies the problem of voice conversion in the context of self-supervised representation learning. The authors propose VoiceMixer, an information bottleneck based on adversarial feedback to improve the generalization of the discriminator in the presence of content and style discriminator. The proposed model is evaluated on the transfer of audio quality and audio quality of the converted speech containing source speech style. ","This paper proposes a new method for voice conversion, called VoiceMixer, which is based on self-supervised representation learning with an information bottleneck and adversarial feedback. The key idea is to learn the converted speech containing source speech style. The discriminator is a combination of a content and style discriminator. The model is evaluated on transfer and generalization of the voice style, and on audio quality. "
11353,SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"Siamese voxel - to - BEV tracker USED-FOR tracking. sparse 3D point clouds FEATURE-OF tracking. Siamese shape - aware feature learning network CONJUNCTION voxel - to - BEV target localization network. voxel - to - BEV target localization network CONJUNCTION Siamese shape - aware feature learning network. Siamese shape - aware feature learning network PART-OF it. voxel - to - BEV target localization network PART-OF it. Siamese shape - aware feature learning network USED-FOR discriminative features. Siamese shape - aware feature learning network USED-FOR 3D shape information. dense 3D shape USED-FOR shape information. 2D center CONJUNCTION z - axis center. z - axis center CONJUNCTION 2D center. 2D center CONJUNCTION z - axis center. z - axis center CONJUNCTION 2D center. voxelized point cloud USED-FOR dense BEV feature map. max pooling USED-FOR dense BEV feature map. max pooling USED-FOR voxelized point cloud. KITTI and nuScenes datasets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. Task is 3D object tracking in point clouds. Material is point clouds. OtherScientificTerm are dynamic environments, sparse point clouds, and template ’s feature. Method are template feature embedding, and voxel - toBEV target localization network. ","This paper proposes a Siamese voxel-to-BEV tracker for tracking in sparse 3D point clouds. The tracking is based on sparse point clouds, where the shape information is encoded in a dense 3D shape, and the template feature embedding is learned from the template’s feature. The paper proposes to use the Siamesa shape-aware feature learning network and a voxell-to -BEV target localization network to learn discriminative features. The proposed method is evaluated on KITTI and nuScenes datasets and shows that it outperforms state-of-the-art methods.","The paper proposes a Siamese voxel-to-BEV tracker for tracking in sparse 3D point clouds. The tracking is done in dynamic environments with sparse point clouds, where the point clouds are generated by a template feature embedding. The paper proposes to use the Siameses shape-aware feature learning network and a voxell-to -BEV target localization network to learn discriminative features for 3D shape information. The shape information is encoded in a dense 3D form with a 2D center, z-axis center, and a z- axis center.  The paper shows that the proposed method outperforms state-of-the-art methods on KITTI and nuScenes datasets.  "
11389,SP:8b788c78680a54c453a04f4551436763ee57585e,Positional encoding USED-FOR attention - based deep model architectures. Transformer HYPONYM-OF attention - based deep model architectures. learnable Fourier features USED-FOR positional encoding method. multi - layer perceptron USED-FOR trainable encoding. learnable Fourier feature mapping USED-FOR trainable encoding. representation USED-FOR spatial multi - dimensional position. L2 distances CONJUNCTION positional relationships. positional relationships CONJUNCTION L2 distances. image FEATURE-OF pixel positions. image HYPONYM-OF spatial multi - dimensional position. pixel positions HYPONYM-OF spatial multi - dimensional position. learnable Fourier feature representation USED-FOR multi - dimensional positional encoding. learnable Fourier feature representation COMPARE methods. methods COMPARE learnable Fourier feature representation. faster convergence EVALUATE-FOR methods. accuracy EVALUATE-FOR learnable Fourier feature representation. accuracy EVALUATE-FOR methods. Method is Attentional mechanisms. ,"This paper proposes a new positional encoding method based on learnable Fourier features for attention-based deep model architectures such as Transformer. The proposed multi-layer perceptron is used to trainable encoding using learnable Fourier feature mapping. The representation maps the spatial multi-dimensional position (e.g., image, pixel positions) to L2 distances and positional relationships. The authors show that the proposed methods achieve faster convergence compared to existing methods, and can be used to learn a more accurate Fourier feature representation. ","This paper proposes a new positional encoding method based on learnable Fourier features for attention-based deep model architectures such as Transformer. Specifically, the authors propose a multi-layer perceptron for trainable encoding, which is based on a learnable Fourier feature mapping. The proposed representation is used to encode a spatial multi-dimensional position (e.g., image, pixel positions) and positional relationships. The authors show that the proposed methods achieve faster convergence and better accuracy compared to other methods. Attentional mechanisms are also proposed."
11425,SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"latent variables CONJUNCTION selection bias. selection bias CONJUNCTION latent variables. causal MAG FEATURE-OF system. observational data USED-FOR system. observational data USED-FOR causal MAG. Constraint - based methods USED-FOR problem. latter USED-FOR CI tests. computational complexity EVALUATE-FOR former. lower bound USED-FOR constraint - based method. lower bound USED-FOR CI tests. CI tests USED-FOR constraint - based method. upper bound CONJUNCTION approach. approach CONJUNCTION upper bound. approach COMPARE state of the art. state of the art COMPARE approach. synthetic and real - world structures EVALUATE-FOR state of the art. synthetic and real - world structures EVALUATE-FOR approach. Generic are methods, and technique. OtherScientificTerm are large graphs, completeness guarantees, structure, and conditional independence ( CI ) tests. Method is recursive constraint - based method. ","This paper proposes a recursive constraint-based method to solve the problem of large graphs. The system is modeled as a causal MAG with observational data. The problem is formulated as a problem where the latent variables and the selection bias of the system are determined by the causal MAG. Constraint-based methods are used to solve this problem. The authors show that the former has a lower computational complexity than the latter, and that the latter has a better upper bound on the number of CI tests required for CI tests. They also show that their approach is more efficient than the state of the art in terms of both synthetic and real-world structures. Finally, they provide completeness guarantees for their technique.","This paper proposes a recursive constraint-based method for large graphs. The system is based on causal MAG, where the system is trained on observational data. The problem is formulated as a combination of latent variables and selection bias. Constraint-based methods are used to solve the problem. The authors show that the proposed technique can reduce the computational complexity of the former and the latter can be used for CI tests. The completeness guarantees are provided for the structure, and conditional independence (CI) tests are also provided. The proposed approach is evaluated on both synthetic and real-world structures. The upper bound and the lower bound for the constraint -based method are shown to be better than the state of the art."
11461,SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,information parallelism USED-FOR online decision making problems. stochastic multi - arm bandit CONJUNCTION linear contextual bandit. linear contextual bandit CONJUNCTION stochastic multi - arm bandit. batch Thompson Sampling framework USED-FOR canonical online decision making problems. linear contextual bandit HYPONYM-OF canonical online decision making problems. stochastic multi - arm bandit HYPONYM-OF canonical online decision making problems. asymptotic ) regret bound EVALUATE-FOR batch Thompson Sampling policy. batch policy USED-FOR exploration - exploitation trade - off. batch policy USED-FOR exponential reduction. dynamic batch allocation COMPARE natural baselines. natural baselines COMPARE dynamic batch allocation. static batch allocations HYPONYM-OF natural baselines. ,"This paper proposes a new batch Thompson Sampling framework for online decision making problems with information parallelism. The main idea is to learn a batch policy for exploration-exploitation trade-off, where the goal is to minimize the (asymptotic) regret bound of a batch Thompson sampling policy. The authors show that this batch policy can achieve exponential reduction in the number of iterations of the batch policy. They also show that the dynamic batch allocation performs better than natural baselines such as static batch allocations.","This paper proposes a batch Thompson Sampling framework for online decision making problems with information parallelism. The authors show that the (asymptotic) regret bound of the batch Thompson Ssampling policy is the same as the asymptotic regret bound for the stochastic multi-arm bandit and the linear contextual bandit. They also show that dynamic batch allocation is equivalent to natural baselines (e.g., static batch allocations) in terms of exponential reduction, and that the batch policy can be used for exploration-exploitation trade-off. "
11497,SP:653a519e3c799c25e0d0b4240322642040b121a3,"multiple source DA CONJUNCTION domain generalization ( DG ) settings. domain generalization ( DG ) settings CONJUNCTION multiple source DA. upper - bounds USED-FOR domain - invariant representations. upper - bounds USED-FOR target general loss. Task is Domain adaptation ( DA ). Method is domain - invariant representation. Generic are representations, them, and theory. ","This paper studies the problem of Domain Adaptation (DA). Domain adaptation (DA) is an important problem where the goal is to learn representations that are invariant to domain-invariant perturbations. The authors consider the problem in multiple source DA and domain generalization (DG) settings, and propose to use upper-bounds on the target general loss as upper-bound for domain- invariant representations. They show that these representations are not always invariant, and that they can be used to improve the performance of DA. They also provide a theoretical analysis of their theory.","This paper proposes Domain adaptation (DA), a method for learning a domain-invariant representation that is invariant to changes in the input domain. Domain adaptation is an extension of Domain Generalization (DG), which is a generalization of Domain Adaptation (DA). The main idea of DA is to use multiple source DA and domain generalization (GDA) settings. The authors provide upper-bound for the target general loss, which is based on upper-bounds for the domain- invariant representations. They show that the representations are invariant, and show that they can be used in theory."
11533,SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"lightweight architectures USED-FOR SR methods. neural architecture search CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION neural architecture search. memory and computation resources USED-FOR model compression techniques. knowledge distillation HYPONYM-OF model compression techniques. neural architecture search HYPONYM-OF model compression techniques. network pruning HYPONYM-OF model compression technique. it USED-FOR SR networks. filter pruning USED-FOR residual blocks. L2 regularization USED-FOR sparsity. L2 regularization USED-FOR scale parameters. L2 regularization USED-FOR aligned structured sparsity learning ( ASSL ). weight normalization layer PART-OF aligned structured sparsity learning ( ASSL ). sparsity structure alignment penalty term USED-FOR norm of soft mask gram matrix. layers FEATURE-OF pruned filter locations. sparsity structure alignment penalty term USED-FOR pruned filter locations. aligned structured sparsity learning strategy USED-FOR image SR network. model size CONJUNCTION computation. computation CONJUNCTION model size. ASSLN HYPONYM-OF image SR network. ASSLN COMPARE methods. methods COMPARE ASSLN. OtherScientificTerm are moderate model size, and network parameters. Generic is state - of - the - art methods. Method is lightweight SR networks. ","This paper proposes a new model compression technique called network pruning based on L2 regularization to reduce sparsity in SR networks. The proposed method is based on an aligned structured sparsity learning strategy, where the weight normalization layer is added to align the network parameters. The authors also propose a sparsity structure alignment penalty term to ensure that the norm of soft mask gram matrix is aligned with the pruned filter locations in the layers. The experimental results show the effectiveness of the proposed method.","This paper proposes a new model compression technique called network pruning, which is a variant of network compression techniques such as neural architecture search and knowledge distillation. The key idea is to use memory and computation resources to reduce the sparsity of model compression techniques. The authors show that it can be used to improve the performance of SR networks by using filter pruning to reduce residual blocks. The paper also proposes an aligned structured sparsity learning strategy for image SR network, which uses L2 regularization to reduce sparsity. The weight normalization layer of aligned structured Sparsity learning (ASSL) consists of a sparsity structure alignment penalty term that penalizes the norm of soft mask gram matrix between layers of the pruned filter locations. The proposed method is compared to state-of-the-art methods on a variety of lightweight SR networks. The results show that the proposed methods outperform other methods in terms of model size and computation."
11569,SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"exploration USED-FOR complex coordination problems. EMC HYPONYM-OF Episodic Multi - agent reinforcement learning. reward backpropagation USED-FOR centralized training. individual utility functions USED-FOR local execution. individual utility functions HYPONYM-OF induced "" individual Q - values. episodic memory USED-FOR policy training. episodic memory USED-FOR explored informative experience. intrinsic rewards USED-FOR coordinated exploration. intrinsic reward USED-FOR coordinated exploration. method COMPARE MARL baselines. MARL baselines COMPARE method. StarCraft II micromanagement benchmark FEATURE-OF tasks. didactic examples USED-FOR method. tasks EVALUATE-FOR MARL baselines. tasks EVALUATE-FOR method. StarCraft II micromanagement benchmark EVALUATE-FOR MARL baselines. Method is factorized MARL algorithms. OtherScientificTerm are embeddings of local actionobservation histories, and individual Q - value function. ","This paper studies the problem of exploration in multi-agent reinforcement learning with EMC. The authors propose a factorized MARL algorithms, where the embeddings of local actionobservation histories are used to train the agent to explore the environment. The goal is to maximize the mutual information between the agent and the environment, and the agent is encouraged to explore as much as possible. The agent is trained with an episodic memory for policy training, which is then used to learn the ""induced"" individual Q-values (e.g., individual utility functions) for local execution. The intrinsic rewards are used for coordinated exploration. The proposed method is evaluated on a variety of tasks, including StarCraft II micromanagement benchmark, and compared to other MARL baselines. ","This paper proposes an extension of EMC to Episodic Multi-agent reinforcement learning (EMC) to the context of exploration in complex coordination problems. The authors propose to use intrinsic rewards to encourage coordinated exploration, which can be used as a reward backpropagation for centralized training. The intrinsic reward is a function of the embeddings of local actionobservation histories, where the ""induced"" individual Q-values are the ""individual utility functions"" that are used for local execution, and the ""independent"" Q-value function is used for policy training. Experiments on three tasks on the StarCraft II micromanagement benchmark show that the proposed method outperforms MARL baselines on all three tasks. The proposed method is evaluated on two didactic examples, where it outperforms factorized MARL algorithms."
11605,SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"Gaussian covariates USED-FOR linear regression model. Statistical Query ( SQ ) lower bound USED-FOR problem. upper bounds USED-FOR task. SQ lower bound COMPARE algorithms. algorithms COMPARE SQ lower bound. Task is list - decodable linear regression. OtherScientificTerm are noise distribution, hypothesis vectors, and regression vector. ",This paper studies the problem of list-decodable linear regression with Gaussian covariates. The authors propose a new Statistical Query (SQ) lower bound for the problem. The upper bounds for this task are based on upper bounds on the noise distribution of the hypothesis vectors of the regression vector. They show that the SQ lower bound is better than existing algorithms. ,"This paper studies the problem of list-decodable linear regression with Gaussian covariates. The authors propose a Statistical Query (SQ) lower bound for the problem, which is based on the assumption that the noise distribution of the hypothesis vectors is Gaussian. They show that the proposed lower bound is equivalent to the upper bounds for the task. They also show that their algorithms outperform other algorithms. "
11641,SP:7b258252a9063514348f5fa8d9c85afd85748747,"expert domain knowledge USED-FOR ML models. patient health status CONJUNCTION disease progression. disease progression CONJUNCTION patient health status. pharmacology USED-FOR domain knowledge. systems of Ordinary Differential Equations ( ODEs ) USED-FOR Pharmacological models. expert - designed ODEs CONJUNCTION machine - learned Neural ODEs. machine - learned Neural ODEs CONJUNCTION expert - designed ODEs. expert and latent variables USED-FOR observable quantities. synthetic data EVALUATE-FOR LHM. Task is Modeling a system ’s temporal behaviour. Method are Machine Learning ( ML ) approaches, and latent hybridisation model ( LHM ). OtherScientificTerm is small sample regime. Generic are application, models, variables, and system. ","This paper studies the problem of learning a system’s temporal behaviour in a small sample regime. The authors propose two Machine Learning (ML) approaches to this problem. The first is based on systems of Ordinary Differential Equations (ODEs), which can be used to learn ML models with expert domain knowledge from pharmacology. The second is a latent hybridisation model (LHM), which combines expert-designed ODEs with machine-learned Neural ODE. The proposed LHM is evaluated on synthetic data and shows that it is able to learn observable quantities from expert and latent variables. ","This paper presents a new application of expert domain knowledge to ML models. The domain knowledge is based on pharmacology, and the authors propose to use systems of Ordinary Differential Equations (ODEs) for Pharmacological models, including expert-designed ODEs and machine-learned Neural ODE. The authors propose two Machine Learning (ML) approaches: (1) a latent hybridisation model (LHM) that learns a system’s temporal behaviour from expert and latent variables, and (2) a small sample regime where the models are trained to predict the observable quantities of the system. Experiments on synthetic data show that the LHM is able to predict observable quantities from both the expert and the latent variables."
11677,SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,Representation learning USED-FOR meta - learning. Representation learning USED-FOR rapid learning of new tasks. meta - learning USED-FOR rapid learning of new tasks. works USED-FOR task - specific representations. MAML USED-FOR task - specific representations. MAML HYPONYM-OF works. fine - tuning - based objective HYPONYM-OF per - task adaptation. per - task adaptation USED-FOR representation. representation USED-FOR task - specific representations. theoretical framework USED-FOR MAML - like algorithm. risk bounds FEATURE-OF predictors. shared structure USED-FOR method. finetuning USED-FOR risk bounds. finetuning USED-FOR predictors. gradient descent USED-FOR finetuning. gradient descent USED-FOR predictors. logistic regression and neural network settings EVALUATE-FOR bounds. OtherScientificTerm is frozen representation ” objective. Generic is algorithm. Method is few - shot learning. ,"This paper studies the problem of representation learning for meta-learning in the context of rapid learning of new tasks. The authors propose two works, MAML and fine-tuning-based objective, for per-task adaptation. The main idea is to learn a representation for each task by per-tasks, which is then used as a “frozen representation” objective. The proposed algorithm is based on the theoretical framework of few-shot learning, and the proposed method uses a shared structure to learn the shared representations. The paper shows that the proposed risk bounds for the predictors obtained by finetuning the predicted predictors using gradient descent are tight in both logistic regression and neural network settings.","This paper proposes a new framework for meta-learning for rapid learning of new tasks. The main idea is to use representation learning to improve the generalization ability of the learned representations. The authors propose two works, MAML and a fine-tuning-based objective for per-task adaptation. The proposed algorithm is based on the “frozen representation” objective, which is a theoretical framework for few-shot learning. The method uses a shared structure and finetuning to optimize the risk bounds of the predictors in the form of gradient descent. The bounds are evaluated on logistic regression and neural network settings. The paper is well-written and easy to follow. "
11713,SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"paired images CONJUNCTION texts. texts CONJUNCTION paired images. lexicalist approach USED-FOR compositional and grounded meaning representation of language. grounded data USED-FOR compositional and grounded meaning representation of language. paired images HYPONYM-OF grounded data. texts HYPONYM-OF grounded data. neural network embedding USED-FOR shiny objects. symbolic form FEATURE-OF neuro - symbolic semantic program. lexical meanings USED-FOR neuro - symbolic program. syntax USED-FOR lexical meanings. joint parsing CONJUNCTION expected execution algorithm. expected execution algorithm CONJUNCTION joint parsing. exponentiallygrowing compositional space FEATURE-OF learning. expected execution algorithm USED-FOR learning. joint parsing USED-FOR learning. visual reasoning CONJUNCTION language - driven navigation. language - driven navigation CONJUNCTION visual reasoning. language - driven navigation EVALUATE-FOR G2L2. visual reasoning EVALUATE-FOR G2L2. domains EVALUATE-FOR G2L2. language - driven navigation HYPONYM-OF domains. visual reasoning HYPONYM-OF domains. G2L2 USED-FOR compositions of words. OtherScientificTerm are syntactic type, syntactic type of adjective, and local marginalization. Method is meaning programs. Metric is training time. ","This paper proposes a new lexicalist approach to learn a compositional and grounded meaning representation of language from grounded data such as paired images and texts. The authors propose a neuro-symbolic semantic program in symbolic form, where the syntactic type of adjective is represented by a neural network embedding, and the semantic program is represented as a neuro - symbolic semantic program with lexical meanings. The paper shows that the learned meaning programs can be used to learn compositional compositions of words, and that the training time is exponential in the exponentiallygrowing compositional space. The proposed joint parsing and expected execution algorithm are used for learning the learning of the joint parsing. The experimental results show that the proposed G2L2 can achieve state-of-the-art performance in two domains: visual reasoning and language-driven navigation.","This paper proposes a lexicalist approach to learn a compositional and grounded meaning representation of language from grounded data such as paired images and texts. The authors propose a neuro-symbolic semantic program in symbolic form, where the syntactic type of adjective corresponds to the semantic type of noun, and a neural network embedding is used to represent shiny objects. The lexical meanings are learned from the syntax, and the learning is performed using joint parsing and an expected execution algorithm in an exponentiallygrowing compositional space. The paper shows that G2L2 is able to learn compositions of words that are compositional in the sense that local marginalization is applied to the meaning programs. Experiments are conducted on two domains: visual reasoning and language-driven navigation."
11749,SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,stochastic Newton algorithm USED-FOR homogeneous distributed stochastic convex optimization. stochastic gradients CONJUNCTION stochastic Hessian - vector products. stochastic Hessian - vector products CONJUNCTION stochastic gradients. stochastic gradients FEATURE-OF population objective. method COMPARE methods. methods COMPARE method. convergence guarantees FEATURE-OF quasi - self - concordant objectives. method USED-FOR communication rounds. communication rounds COMPARE methods. methods COMPARE communication rounds. logistic regression HYPONYM-OF quasi - self - concordant objectives. OtherScientificTerm is stochastic computations. ,"This paper proposes a stochastic Newton algorithm for homogeneous distributed stochastically convex optimization. The proposed method achieves better convergence guarantees than existing methods for communication rounds and quasi-self-concordant objectives (e.g., logistic regression). The main contribution of the paper is to show that the population objective can be approximated by stochedastic gradients and stochy Hessian-vector products. The authors also provide a theoretical analysis of the convergence of the proposed method. ",This paper proposes a stochastic Newton algorithm for homogeneous distributed stochastically convex optimization. The population objective is defined as a population objective that is defined by stochastastic gradients and stochangastic Hessian-vector products. The authors show that the proposed method outperforms existing methods for communication rounds and other quasi-self-concordant objectives such as logistic regression. 
11785,SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"Chamfer Distance ( CD ) CONJUNCTION Earth Mover ’s Distance ( EMD ). Earth Mover ’s Distance ( EMD ) CONJUNCTION Chamfer Distance ( CD ). Chamfer Distance ( CD ) HYPONYM-OF metrics. Earth Mover ’s Distance ( EMD ) HYPONYM-OF metrics. global distribution USED-FOR EMD. them USED-FOR consistent evaluation. Density - aware Chamfer Distance ( DCD ) HYPONYM-OF similarity measure. it USED-FOR disparity of density distributions. it COMPARE EMD. EMD COMPARE it. it COMPARE CD. CD COMPARE it. it USED-FOR detailed structures. CD USED-FOR It. DCD USED-FOR point cloud completion task. CD CONJUNCTION EMD. EMD CONJUNCTION CD. DCD USED-FOR local geometric details. DCD USED-FOR training loss. metrics EVALUATE-FOR model. CD loss USED-FOR model. CD CONJUNCTION EMD. EMD CONJUNCTION CD. DCD USED-FOR it. OtherScientificTerm are mismatched local density, fidelity of detailed structures, unbounded value range, outliers, and bounded value range. Method is point discriminator module. Task are guided downsampling step, and point cloud similarity evaluation. ","This paper proposes a new similarity measure called Density-aware Chamfer Distance (CD) and Earth Mover’s Distance (EMD) to measure the distance between two points in a point cloud. The two metrics are based on the notion of mismatched local density. The authors show that the global distribution of the EMD can be used as a global distribution for the CD, and that it is able to capture the disparity of density distributions. It is also shown that the CD can capture more detailed structures than EMD. The paper also shows that the DCD can be applied to the point cloud completion task.","The paper proposes two metrics, Chamfer Distance (CD) and Earth Mover’s Distance (EMD), which measure the disparity of density distributions between two points in a point cloud. The authors propose to use them for consistent evaluation. The main contribution of the paper is a guided downsampling step, where the point cloud similarity evaluation is performed by a point discriminator module. The proposed metrics are based on the Density-aware Chamfer distance (DCD) which is a similarity measure based on a global distribution. It is shown that it is more robust to mismatched local density than CD and EMD, and it can be used to measure the fidelity of detailed structures. The paper also proposes a training loss based on DCD to improve the performance of the model. The model is evaluated on three metrics and compared to CD, EMD and CD for point cloud completion task. "
11821,SP:e4b302009520770814ff2c096020b779a9fc38fe,Knowledge distillation USED-FOR small student network. small student network USED-FOR teacher model. ensemble of networks HYPONYM-OF teacher model. knowledge distillation USED-FOR student generalization. dataset USED-FOR distillation. Generic is it. OtherScientificTerm is predictive distributions. Task is optimization. ,"This paper proposes to use knowledge distillation to improve the performance of a small student network in a teacher model (e.g. ensemble of networks). The idea is to distill the knowledge of the teacher model into a small set of predictive distributions, and then use it to optimize the student generalization. The authors propose a dataset for distillation, and show that it can achieve better performance than the state-of-the-art. ",This paper proposes to use knowledge distillation for small student network for teacher model. The teacher model is an ensemble of networks. The idea is to distill the student generalization from the dataset to the predictive distributions. The authors show that it leads to better optimization.
11857,SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"algorithm USED-FOR ( k, ε)-coreset. decision trees PART-OF machine learning. decision trees CONJUNCTION partition trees. partition trees CONJUNCTION decision trees. computational geometry FEATURE-OF partition trees. sklearn CONJUNCTION lightGBM. lightGBM CONJUNCTION sklearn. coresets USED-FOR random forests. computation time EVALUATE-FOR random forests. random forests CONJUNCTION parameter tuning. parameter tuning CONJUNCTION random forests. lightGBM EVALUATE-FOR coresets. sklearn EVALUATE-FOR coresets. computation time EVALUATE-FOR parameter tuning. real - world data - sets EVALUATE-FOR random forests. coresets USED-FOR parameter tuning. computation time EVALUATE-FOR coresets. accuracy EVALUATE-FOR parameter tuning. real - world data - sets EVALUATE-FOR coresets. accuracy EVALUATE-FOR coresets. Method is k - tree. OtherScientificTerm are axis - parallel rectangles, error parameter, tree, optimal k - tree, and coreset. Metric is regression or classification loss. Generic is loss. ","This paper proposes a new algorithm for learning a (k, ε)-coreset of decision trees in the context of machine learning. The coreset is composed of a k-tree and a partition tree. The partition trees are based on the computational geometry of partition trees, and the decision trees are the axis-parallel rectangles. The error parameter of the partition tree is computed as a function of the number of nodes in the tree.  The goal of the algorithm is to learn an optimal k-trees, which is then used as a regression or classification loss. The authors show that the resulting coresets can be used for random forests and parameter tuning, as well as sklearn and lightGBM. The performance of the coresets is evaluated on real-world data-sets, showing that the accuracy and computation time of these coresets are comparable to those of random forests. ","This paper proposes a new algorithm for learning a (k, ε)-coreset of decision trees in machine learning. The key idea is to learn a k-tree with axis-parallel rectangles and partition trees of the same computational geometry. The error parameter of the tree is computed as a function of the value of the decision trees. The loss is a regression or classification loss. The authors show that the optimal k-trees can be learned from the coreset. The coresets can be used for random forests, sklearn, lightGBM, and parameter tuning. The paper also shows that the coresets have better computation time and better accuracy on real-world data-sets."
11893,SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"δ - correct algorithm USED-FOR Top - m identification problem. sample complexity EVALUATE-FOR δ - correct algorithm. tractable lower bound USED-FOR δ - correct algorithm. sample complexity EVALUATE-FOR tractable lower bound. algorithm USED-FOR setting. sample complexity FEATURE-OF upper bound. algorithm COMPARE baselines. baselines COMPARE algorithm. synthetic and real - world data EVALUATE-FOR algorithm. Method are fixed - confidence Top - m identification ), misspecified linear bandit models, and linear models. Generic are problem, and algorithms. Task is medicine and recommendation systems. OtherScientificTerm are linearity, structure of the problem, misspecification, and lower bound. ","This paper studies the problem of top-m identification in medicine and recommendation systems. The authors propose a new algorithm, called δ-correct algorithm, for the Top-M identification problem. The problem is formulated as a fixed-confidence Top-mu identification (TMA) problem with misspecified linear bandit models. The main idea of the problem is to learn a linearity of the structure of the problems, which is then used as a tractable lower bound on the sample complexity of the upper bound. The proposed algorithm is tested on both synthetic and real-world data, and is shown to outperform existing algorithms in both settings.",The paper proposes a new algorithm for the Top-m identification problem. The problem is formulated as a fixed-confidence Top-M identification problem with misspecified linear bandit models. The authors provide a tractable lower bound for the δ-correct algorithm with respect to the sample complexity of the upper bound. The algorithm is tested on both synthetic and real-world data and is shown to outperform existing algorithms in both medicine and recommendation systems. 
11929,SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"self - supervised learning USED-FOR graph neural networks ( GNNs ). self - supervised learning USED-FOR representation of graph - structure data. self - supervised learning methods USED-FOR GNNs. self - supervised learning USED-FOR disentangled graph representations. Disentangled Graph Contrastive Learning ( DGCL ) method USED-FOR disentangled graph - level representations. self - supervision USED-FOR disentangled graph - level representations. factorized representations USED-FOR latent and disentangled aspect. latent factor FEATURE-OF graph. factorized representations USED-FOR expressive information. contrastive learning manner FEATURE-OF factor - wise discrimination objective. latent factors USED-FOR expressive information. synthetic and real - world datasets EVALUATE-FOR method. method COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE method. synthetic and real - world datasets EVALUATE-FOR state - of - the - art baselines. OtherScientificTerm are real - world graph, and entanglement of the latent factors. Generic is learned representations. Task is Learning disentangled graph representations. ",This paper proposes a new self-supervised learning method for disentangled graph representation learning. The proposed method is based on the Disentangled Graph Contrastive Learning (DGCL) method. The authors propose a new contrastive learning objective based on a factor-wise discrimination objective. The method is evaluated on both synthetic and real-world datasets and compared to state-of-the-art baselines.,"This paper proposes a novel method for learning disentangled graph representations. The authors propose a new method called Disentangled Graph Contrastive Learning (DGCL) method to disentangle graph-level representations with self-supervised learning for graph neural networks (GNNs). The key idea is to learn a representation of graph-structured data that is independent of the real-world graph. The disentanglement of the latent factors is achieved by using factorized representations of the graph, which are learned in a contrastive learning manner. The proposed method outperforms state-of-the-art baselines on both synthetic and real - world datasets. "
11965,SP:0a7edbbdabab11273689c40c517001eb46491113,"robustness EVALUATE-FOR network. Statistical Reliability Engineering FEATURE-OF stochastic simulation. stochastic simulation USED-FOR network. stochastic simulation USED-FOR robustness. statistical hypothesis test USED-FOR robustness assessment. Importance Splitting simulation USED-FOR procedure. OtherScientificTerm are theoretical guarantees, sample size, and network function. Method is large scale networks. Generic is method. ","This paper studies the robustness of large scale networks under stochastic simulation in the context of Statistical Reliability Engineering. The authors propose a statistical hypothesis test for robustness assessment, which is a popular technique in the literature. The proposed procedure is based on Importance Splitting simulation. Theoretical guarantees are provided for the theoretical guarantees. The empirical results show that the proposed method can achieve better robustness than existing methods.",This paper proposes a stochastic simulation for robustness of a network based on Statistical Reliability Engineering (SRE). The authors provide theoretical guarantees on the sample size and sample size of the network function. The proposed procedure is based on the Importance Splitting simulation. The authors also provide a statistical hypothesis test to evaluate the robustness assessment. The method is evaluated on large scale networks.
12001,SP:c1db485ff1ff9573daa421e167225654babb55ac,Generative modeling USED-FOR machine learning. Deep polynomial neural networks ( PNNs ) USED-FOR unsupervised image generation. PNNs USED-FOR conditional generation tasks. super - resolution HYPONYM-OF conditional generation tasks. noise variable CONJUNCTION conditional variable. conditional variable CONJUNCTION noise variable. single - variable polynomial expansions USED-FOR PNNs. conditional variable HYPONYM-OF two - variable inputs. noise variable HYPONYM-OF two - variable inputs. framework USED-FOR autoand cross - correlations. framework USED-FOR polynomial expansion. input variables USED-FOR CoPE. edges - to - image translation CONJUNCTION image - to - image translation. image - to - image translation CONJUNCTION edges - to - image translation. inverse problems CONJUNCTION edges - to - image translation. edges - to - image translation CONJUNCTION inverse problems. class - conditional generation CONJUNCTION inverse problems. inverse problems CONJUNCTION class - conditional generation. image - to - image translation CONJUNCTION attributeguided generation. attributeguided generation CONJUNCTION image - to - image translation. class - conditional generation CONJUNCTION edges - to - image translation. edges - to - image translation CONJUNCTION class - conditional generation. tasks EVALUATE-FOR CoPE. class - conditional generation EVALUATE-FOR CoPE. attributeguided generation HYPONYM-OF tasks. class - conditional generation HYPONYM-OF tasks. inverse problems HYPONYM-OF tasks. image - to - image translation HYPONYM-OF tasks. edges - to - image translation HYPONYM-OF tasks. CoPE USED-FOR conditional generation tasks. CoPE USED-FOR polynomial_nets_for_conditional_generation. OtherScientificTerm is noise. Material is synthesized image. ,"This paper studies the problem of unsupervised image generation with Deep polynomial neural networks (PNNs) in the context of generative modeling in machine learning. PNNs are commonly used for conditional generation tasks such as super-resolution and image-to-image translation. The authors propose a new framework called CoPE to learn a two-variable inputs: a noise variable and a conditional variable. The noise variable is a function of the input variables, and the conditional variable is the output of the PNN. The two-variance inputs are learned by single-variable Polynomial expansions. The proposed framework is able to learn auto-and cross-correlation between the two input variables. CoPE is shown to perform well on a variety of tasks including attributeguided generation, class-conditional generation, and inverse problems. ","Generative modeling is an important problem in machine learning. Deep polynomial neural networks (PNNs) are used for unsupervised image generation. PNNs are commonly used for conditional generation tasks such as super-resolution, image-to-image translation, and class-conditional generation. However, most of these tasks are computationally expensive. The authors propose to use single-variable Polynomial expansions to improve the performance of the PNN. The framework is based on autoand cross-correlation, and the authors show that the proposed framework can be applied to polynomials of two-variable inputs (i.e., noise variable and the conditional variable). The authors also show that CoPE can be used to learn the input variables of the poleomial expansion. CoPE is evaluated on a number of tasks including class-conditioned generation, inverse problems, and attributeguided generation. "
12037,SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,neural tangent kernel ( NTK ) CONJUNCTION MMD. MMD CONJUNCTION neural tangent kernel ( NTK ). approach USED-FOR MMD statistic. connection USED-FOR approach. memory and computational complexity EVALUATE-FOR MMD statistic. MMD statistic USED-FOR online implementation. approach USED-FOR NTK based two - sample tests. connection USED-FOR NTK based two - sample tests. theories USED-FOR kernel MMD. connection USED-FOR NTK test statistic properties. Type - I error CONJUNCTION testing power. testing power CONJUNCTION Type - I error. testing power HYPONYM-OF NTK test statistic properties. Type - I error HYPONYM-OF NTK test statistic properties. synthetic and real - world datasets EVALUATE-FOR theory. synthetic and real - world datasets EVALUATE-FOR NTK - MMD statistic. OtherScientificTerm is two - sample test. ,This paper proposes a new approach to estimate the MMD statistic for NTK based two-sample tests using a neural tangent kernel (NTK) and MMD. The approach is based on the connection between NTK test statistic properties such as Type-I error and testing power. Theoretical results show that the proposed MMD statistics improve the memory and computational complexity of the online implementation. Empirical results on synthetic and real-world datasets demonstrate the effectiveness of the theory. ,This paper proposes a novel approach to perform NTK based two-sample tests using neural tangent kernel (NTK) and MMD. The approach is based on the connection between the NTK test statistic properties (Type-I error and testing power) and the MMD statistic (memory and computational complexity). The paper also proposes an online implementation of the proposed approach. Experiments on synthetic and real-world datasets validate the theory and show that the proposed kernel MMD outperforms existing theories. 
12073,SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"minimum necessary information USED-FOR neural net D ( · ). class - disentanglement USED-FOR variational autoencoder G ( · ). former COMPARE latter. latter COMPARE former. variational autoencoder G ( · ) USED-FOR class - dependent information. class - disentanglement USED-FOR class - dependent information. classification PART-OF x − G(x ). latter USED-FOR classification. clean images CONJUNCTION adversarial images. adversarial images CONJUNCTION clean images. it USED-FOR adversarial images. it USED-FOR clean images. adversarial attacks USED-FOR perturbations. class - dependent part USED-FOR perturbations. adversarial detection CONJUNCTION adversarial defense. adversarial defense CONJUNCTION adversarial detection. adversarial defense USED-FOR G(x ). detection CONJUNCTION defense. defense CONJUNCTION detection. approach USED-FOR adversarial attacks. defense USED-FOR adversarial attacks. approach USED-FOR defense. detection EVALUATE-FOR approach. Task are detection and defense of adversarial attacks, and classification and attack models. ",This paper proposes a new method for detecting and defense of adversarial attacks. The proposed method is based on a variational autoencoder G (G) with class-disentanglement. The main idea is to learn the minimum necessary information for a neural net D (D) using the class-dependent information from the variational auto-encoder G. The authors show that the proposed method outperforms the former by a large margin. The paper also shows that it can detect clean images and adversarial images.,"This paper proposes a novel approach to improve the detection and defense of adversarial attacks. The authors propose a variational autoencoder G (G(x) with class-disentanglement, which learns the minimum necessary information for a neural net D (D) to be robust to adversarial perturbations. The classification of x − G(x), which consists of the classification of clean images and adversarial images, and the latter of the class-dependent information of the latter. The proposed approach is shown to improve adversarial detection and the defense of G(X) on clean images, but it is not shown that it improves the detection on adversarial image. "
12109,SP:2789874561620ba7894c4672f935056bb911e919,Bayesian optimization ( BO ) USED-FOR federated learning ( FL ) setting. federated Thompson sampling ( FTS ) algorithm USED-FOR applications. federated hyperparameter tuning HYPONYM-OF applications. federated Thompson sampling ( FTS ) algorithm USED-FOR Bayesian optimization ( BO ). federated Thompson sampling ( FTS ) algorithm USED-FOR federated learning ( FL ) setting. privacy guarantee FEATURE-OF FL. privacy guarantee FEATURE-OF FTS. differential privacy ( DP ) USED-FOR deep neural networks. DP USED-FOR iterative algorithms. DP USED-FOR user - level privacy. FTS USED-FOR user - level privacy. DP USED-FOR FTS. local modeling USED-FOR BO. DP framework USED-FOR parameter vectors. utility EVALUATE-FOR algorithm. local modeling USED-FOR algorithm. distributed exploration ( DE ) USED-FOR utility. distributed exploration ( DE ) USED-FOR algorithm. privacy CONJUNCTION utility. utility CONJUNCTION privacy. theoretical guarantees FEATURE-OF privacy. theoretical guarantees FEATURE-OF utility. theoretical guarantees FEATURE-OF differentially private FTS. privacy CONJUNCTION utility. utility CONJUNCTION privacy. utility CONJUNCTION privacy guarantee. privacy guarantee CONJUNCTION utility. real - world experiments EVALUATE-FOR DP - FTS - DE. utility EVALUATE-FOR DP - FTS - DE. OtherScientificTerm is privacy - utility trade - off. ,"This paper proposes a federated Thompson sampling (FT) algorithm for Bayesian optimization (BO) in the federated learning (FL) setting. The authors show that the privacy guarantee of FTS has a differentially private FTS with differential privacy (DP) for deep neural networks, and that DP can be used to improve user-level privacy for iterative algorithms. The proposed algorithm is based on local modeling for BO, where the DP framework is used to learn the parameter vectors. The utility of the proposed algorithm, DP-FTS-DE, is evaluated on real-world experiments. The theoretical guarantees of the utility and privacy of DP-FT-DE are shown. ","This paper proposes a federated Thompson sampling (FT) algorithm for applications such as federated hyperparameter tuning. The privacy guarantee of FL is based on differential privacy (DP) in deep neural networks. The DP framework is applied to parameter vectors, and the privacy-utility trade-off between DP and FTS is used for user-level privacy. The proposed algorithm is evaluated on real-world experiments with distributed exploration (DE) and privacy guarantees of differentially private FTS. "
12145,SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"it USED-FOR real - world problems. data annotation USED-FOR MLC models. informative samples USED-FOR cost - effective annotation. BM USED-FOR label correlations. mixture component USED-FOR global pattern of label correlations. Bayesian Bernoulli mixture of label clusters USED-FOR BM. Bayesian Bernoulli mixture of label clusters USED-FOR label correlations. predictive GP USED-FOR feature - component - label mapping. BM CONJUNCTION predictive GP. predictive GP CONJUNCTION BM. BM USED-FOR feature - component - label mapping. predictive GP USED-FOR data features. AL USED-FOR sparse labels. BM USED-FOR sparse labels. GP USED-FOR mixture components. auxiliary variable based variational inference algorithm USED-FOR non - conjugacy. mapping process USED-FOR end - to - end posterior inference. predictive distribution USED-FOR label prediction. model USED-FOR predictive distribution. feature uncertainty CONJUNCTION label covariance. label covariance CONJUNCTION feature uncertainty. label covariance USED-FOR data sampling. BM ) USED-FOR data sampling. label covariance CONJUNCTION BM ). BM ) CONJUNCTION label covariance. GP USED-FOR feature uncertainty. real - world multi - label datasets EVALUATE-FOR model. AL EVALUATE-FOR model. real - world multi - label datasets EVALUATE-FOR AL. Task is Multi - label classification ( MLC ). OtherScientificTerm are correlated ( hence non - exclusive ) labels, sparse label space, correlated label space, inductive bias, and label covariance matrix. ","This paper studies the problem of Multi-label classification (MLC) in the sparse label setting, where the data is not available to the model. The authors propose a new model, called AL, that is able to learn sparse labels from sparse data. The main idea is to use a Bayesian Bernoulli mixture of label clusters to learn the global pattern of label correlations. The mixture component is then used to predict the label correlations between the two clusters. The proposed model is evaluated on real-world multi-label datasets.","This paper proposes a new model for Multi-label classification (MLC), which is an important problem in real-world problems. The main idea is to use data annotation to improve the performance of MLC models. The authors propose to use informative samples for cost-effective annotation, and use a Bayesian Bernoulli mixture of label clusters to map the global pattern of label correlations to a mixture component of the correlated (semi-exclusive) labels. This mixture component is then used to predict the data features, and a predictive GP is used for feature-component-label mapping. The proposed model is evaluated on a set of real-World multi-label datasets, where the model is shown to outperform AL on sparse labels and BM for sparse labels.  The authors also propose an auxiliary variable based variational inference algorithm for non-conjugacy, which can be used for end-to-end posterior inference without inductive bias. "
12181,SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"wedge - shaped point cloud sectors COMPARE point cloud. point cloud COMPARE wedge - shaped point cloud sectors. end - to - end latency EVALUATE-FOR lidar perception models. lidars HYPONYM-OF streaming data source. cartesian coordinate systems USED-FOR methods. multi - scale padding USED-FOR spatial context. feature undistortion CONJUNCTION range stratified convolutions. range stratified convolutions CONJUNCTION feature undistortion. feature undistortion USED-FOR core polar convolutional architecture. range stratified convolutions USED-FOR core polar convolutional architecture. nuScenes dataset EVALUATE-FOR streaming based methods. OtherScientificTerm are sectors, and rectangular regions. Method are polar coordinate system, and non - streaming methods. ","This paper proposes a new streaming data source called lidar perception models with end-to-end latency. The main idea is to use cartesian coordinate systems to learn the spatial context of a point cloud, and then use a polar coordinate system to map the point cloud to a set of rectangular regions. The spatial context is learned by multi-scale padding. The authors show that the proposed methods are able to perform well on the nuScenes dataset. The core polar convolutional architecture is based on feature undistortion and range stratified convolutions. ","This paper proposes a new method for learning a polar coordinate system. The proposed methods are based on cartesian coordinate systems. The key idea is to use a streaming data source (e.g. lidars) as the input, and then to learn a set of sectors (i.e. point cloud) that correspond to the point cloud. The spatial context is represented by multi-scale padding. The authors show that the proposed core polar convolutional architecture with feature undistortion and range stratified convolutions can achieve better end-to-end latency than non-streaming methods on the nuScenes dataset. "
12217,SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"Structured latent variables USED-FOR deep learning models. prior knowledge PART-OF deep learning models. variables USED-FOR learning. differentiable surrogate USED-FOR training. learning approach USED-FOR latent variable. Gumbel - Max trick USED-FOR distributions. structured domains FEATURE-OF distributions. score function estimators USED-FOR optimization. score function estimators USED-FOR differentiable surrogates. stochastic invariant HYPONYM-OF recursive algorithms. gradient estimates CONJUNCTION control variates. control variates CONJUNCTION gradient estimates. feature USED-FOR gradient estimates. feature USED-FOR control variates. structured latent variable models COMPARE relaxation - based counterparts. relaxation - based counterparts COMPARE structured latent variable models. OtherScientificTerm are surrogate, and biased gradients. Generic is model. ",This paper studies the problem of learning latent variables for deep learning models with prior knowledge in the presence of biased gradients. The authors propose a learning approach to learn the latent variable using a differentiable surrogate to guide the training. The surrogate is trained with a Gumbel-Max trick to learn distributions in structured domains. The score function estimators are used for optimization and differentiable surrogates are used to train the model. The main contribution of the paper is the formulation of recursive algorithms such as stochastic invariant. The feature is used to improve the gradient estimates and control variates of structured latent variable models compared to relaxation-based counterparts.,"This paper proposes to use structured latent variables to train deep learning models with prior knowledge. The key idea is to use a differentiable surrogate to guide the training of the latent variable in the learning approach. The surrogate is a Gumbel-Max trick to learn distributions over structured domains. The authors show that differentiable surrogates can be learned using score function estimators for optimization. They also show that the learned distributions are invariant to biased gradients. They show that recursive algorithms with stochastic invariant are also invariant. Finally, the authors propose a new feature to improve the gradient estimates and control variates of the model. The proposed structured latent variable models outperform relaxation-based counterparts."
12253,SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"Deep convolutional neural networks ( CNNs ) USED-FOR image denoising. large datasets USED-FOR Deep convolutional neural networks ( CNNs ). noisy image USED-FOR denoisers. models USED-FOR features. large datasets USED-FOR CNN models. convolutional layers PART-OF CNN. multiplicative scaling parameter USED-FOR GainTuning. GainTuning COMPARE CNNs. CNNs COMPARE GainTuning. denoising EVALUATE-FOR GainTuning. image - denoising benchmarks EVALUATE-FOR CNNs. image - denoising benchmarks EVALUATE-FOR GainTuning. noise level CONJUNCTION image type. image type CONJUNCTION noise level. adaptive GainTuning USED-FOR transmission - electronmicroscope images. synthetic data USED-FOR CNN. CNN USED-FOR adaptive GainTuning. GainTuning USED-FOR structure of catalytic nanoparticles. methodology COMPARE GainTuning. GainTuning COMPARE methodology. low signal - to - noise ratios FEATURE-OF data. data USED-FOR GainTuning. data USED-FOR structure of catalytic nanoparticles. Generic are they, and them. OtherScientificTerm is overfitting. ","This paper studies the problem of image denoising in deep convolutional neural networks (CNNs) with large datasets. The authors propose a new method, GainTuning, which uses a multiplicative scaling parameter to improve the performance of CNNs. The main idea is to use a noisy image to train denoisers, and then use the learned models to extract features from the noisy image. The proposed method is evaluated on a variety of image-denoising benchmarks and shows that it outperforms the state-of-the-art CNNs on these benchmarks.","The paper proposes a new method for image denoising, called GainTuning, which is based on a multiplicative scaling parameter. The main idea is to use a noisy image as input to denoisers, and then train CNN models on large datasets to learn the features of the noisy image. The authors show that the proposed method outperforms other methods on several image-denoising benchmarks. "
12289,SP:90afa1102683b456bc72a54abef466326827546a,convolutional neural network CONJUNCTION asymmetric multiway cut problem solver. asymmetric multiway cut problem solver CONJUNCTION convolutional neural network. fully differentiable architecture USED-FOR simultaneous semantic and instance segmentation. panoptic segmentation PART-OF fully differentiable architecture. convolutional neural network PART-OF fully differentiable architecture. asymmetric multiway cut problem solver PART-OF fully differentiable architecture. latter USED-FOR combinatorial optimization problem. combinatorial optimization problem USED-FOR panoptic labeling. semantic and boundary predictions USED-FOR panoptic labeling. semantic and boundary predictions PART-OF combinatorial optimization problem. formulation USED-FOR smooth surrogate of the panoptic quality metric. gradient USED-FOR optimization problem. Cityscapes and COCO datasets EVALUATE-FOR approaches. combinatorial optimization USED-FOR panoptic segmentation ( COPS ). optimization USED-FOR large scale real - world problem. optimization CONJUNCTION deep learning. deep learning CONJUNCTION optimization. deep learning USED-FOR large scale real - world problem. approach USED-FOR combinatorial optimization. optimization USED-FOR approach. Generic is architecture. ,This paper proposes a fully differentiable architecture for simultaneous semantic and instance segmentation with panoptic segmentation. The proposed architecture consists of a convolutional neural network and an asymmetric multiway cut problem solver. The latter is used to solve the combinatorial optimization problem for panoptical labeling with semantic and boundary predictions. The optimization problem is formulated as a gradient-based optimization problem where the objective is to find a smooth surrogate of the panoptimistic quality metric. The authors show that the proposed approach can be used to perform large scale real-world problem with optimization and deep learning. The experimental results on Cityscapes and COCO datasets demonstrate the effectiveness of the proposed approaches.,This paper proposes a fully differentiable architecture for simultaneous semantic and instance segmentation. The authors propose a convolutional neural network and an asymmetric multiway cut problem solver. The latter is used to solve the combinatorial optimization problem for panoptic labeling with semantic and boundary predictions. The paper also proposes a formulation for the smooth surrogate of the panoptics quality metric. The proposed approach is evaluated on Cityscapes and COCO datasets. The optimization problem is solved with gradient. The results show that the proposed approach can be applied to large scale real-world problem with optimization and deep learning.
12325,SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"Probabilistic context - free grammars ( PCFGs ) CONJUNCTION dynamic Bayesian networks ( DBNs ). dynamic Bayesian networks ( DBNs ) CONJUNCTION Probabilistic context - free grammars ( PCFGs ). dynamic Bayesian networks ( DBNs ) HYPONYM-OF sequence models. Probabilistic context - free grammars ( PCFGs ) HYPONYM-OF sequence models. PCFGs USED-FOR nested hierarchical dependencies ( tree structures ). continuous latent variables USED-FOR DBNs. PCFGs CONJUNCTION DBNs. DBNs CONJUNCTION PCFGs. PCFGs USED-FOR Recursive Bayesian Networks ( RBNs ). RBNs USED-FOR joint distribution. discrete or continuous latent variables FEATURE-OF tree - structured Bayesian networks. tree - structured Bayesian networks USED-FOR joint distribution. exponential number of possible structures CONJUNCTION continuous variables. continuous variables CONJUNCTION exponential number of possible structures. exponential number of possible structures USED-FOR joint inference. maximum posterior estimates USED-FOR continuous latent variables. PCFGs USED-FOR inside and outside probabilities. inside and outside probabilities USED-FOR RBNs. gradient descent USED-FOR maximum posterior estimates. robust parameter optimisation CONJUNCTION Bayesian inference. Bayesian inference CONJUNCTION robust parameter optimisation. marginal data likelihood ( evidence ) CONJUNCTION marginal posterior distribution. marginal posterior distribution CONJUNCTION marginal data likelihood ( evidence ). change point detection CONJUNCTION hierarchical clustering. hierarchical clustering CONJUNCTION change point detection. RBNs USED-FOR segmentation. RBNs COMPARE change point detection. change point detection COMPARE RBNs. RBNs COMPARE hierarchical clustering. hierarchical clustering COMPARE RBNs. noisy sequences USED-FOR RBNs. examples EVALUATE-FOR RBNs. musical data USED-FOR hierarchical music analysis. raw note level USED-FOR hierarchical music analysis. OtherScientificTerm are dependencies, latent variables, nested hierarchical dependency structure, mixed discrete - continuous case, network structures, and expert annotations. Generic is neither. Method is Gaussian RBNs. Material is synthetic data. ","This paper proposes Recursive Bayesian Networks (RBNs), a family of sequence models based on Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs). PCFGs and DBNs can be viewed as nested hierarchical dependencies (tree structures). The authors show that continuous latent variables can be used to represent the joint distribution of a tree-structured Bayesian network, which can be either discrete or continuous. The maximum posterior estimates are obtained by gradient descent on the continuous latent variable. The authors also show that the exponential number of possible structures and the number of continuous variables is sufficient for joint inference, robust parameter optimisation, and Bayesian inference.    The authors demonstrate that the performance of RBNs on a variety of tasks, including change point detection, hierarchical clustering, and segmentation with noisy sequences, is comparable to the performance on synthetic data. ","This paper presents Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs) for sequence models. PCFGs and DBNs are used to model nested hierarchical dependencies (tree structures) and continuous latent variables. The authors show that the joint distribution of tree-structured Bayes networks with discrete or continuous latent variable is a mixed discrete-continuous case, where the dependencies are defined as a function of the nested hierarchical dependency structure. They also show that maximum posterior estimates for continuous latent models are obtained by gradient descent, which is a generalization of Gaussian RBNs.  The authors also provide an empirical evaluation of the performance of the RBN on synthetic data and on musical data at the raw note level.  "
12361,SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"objective functions FEATURE-OF deep neural networks. Backward propagation of errors ( backpropagation ) USED-FOR objective functions. loss functions HYPONYM-OF objective functions. pseudo - Lagrange multiplier method USED-FOR constrained backpropagation ( CBP ) algorithm. two - bit shift weight constraints HYPONYM-OF constraints. binary HYPONYM-OF constraints. ResNet-18 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-18. ResNet-50 CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION ResNet-50. AlexNet CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION AlexNet. CBP USED-FOR AlexNet. CBP USED-FOR GoogLeNet. CBP USED-FOR ResNet-18. posttraining method USED-FOR CBP. GoogLeNet PART-OF ImageNet. ImageNet USED-FOR CBP. backpropagation USED-FOR CBP. ResNet-18 CONJUNCTION ResNet50. ResNet50 CONJUNCTION ResNet-18. ResNet50 CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION ResNet50. algorithm COMPARE methods. methods COMPARE algorithm. top-1 accuracy EVALUATE-FOR ResNet-18. binary weights USED-FOR GoogLeNet. ImageNet EVALUATE-FOR methods. ImageNet EVALUATE-FOR algorithm. top-1 accuracy EVALUATE-FOR algorithm. CBP USED-FOR learning algorithm. constraint functions USED-FOR learning algorithm. constraint functions USED-FOR CBP. OtherScientificTerm are weight precision, objective function, and minimal performance loss. Generic is algorithms. Method is CBP algorithm. ","This paper proposes a pseudo-Lagrange multiplier method for constrained backpropagation (CBP) algorithm. The objective functions of deep neural networks are known to be objective functions with backward propagation of errors (backpropagating) and the loss functions of objective functions are known as loss functions with weight precision.  The authors propose a posttraining method for CBP by using the constraint functions of the learning algorithm: two-bit shift weight constraints and binary weights. The authors show that the proposed CBP can be combined with GoogLeNet, ResNet-18, and GoogleNet in ImageNet and achieves better top-1 accuracy than existing methods.   ","This paper proposes a constrained backpropagation (CBP) algorithm based on a pseudo-Lagrange multiplier method. Backward propagation of errors (backpropagating) is used to learn objective functions for deep neural networks. The objective function is defined as a function of the weight precision, objective function, and loss functions. Two-bit shift weight constraints are used to define the constraints: one-bit shifts weight constraints and binary constraints. The authors also propose a posttraining method for CBP. The proposed CBP algorithm is evaluated on ImageNet, ResNet-18, AlexNet, and GoogLeNet. The results show that the proposed algorithm achieves better top-1 accuracy compared to other methods. "
12397,SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"acquisition function USED-FOR data / label efficiency. acquisition function USED-FOR Active learning. discrete instance set ( pool - based scenario CONJUNCTION continuous instance space ( query synthesis scenario ). continuous instance space ( query synthesis scenario ) CONJUNCTION discrete instance set ( pool - based scenario. active learning scenarios USED-FOR Gaussian Process Classification ( GPC ). active learning strategies USED-FOR classification error. active learning strategies USED-FOR Estimated Error Reduction ( EER ). gradient - based optimization techniques USED-FOR continuous instance space. continuous instance space USED-FOR query synthesis. it COMPARE gradient - based optimization techniques. gradient - based optimization techniques COMPARE it. gradient - based optimization techniques USED-FOR query synthesis. algorithms USED-FOR EER - based active learning. GPC USED-FOR EER - based active learning. one - dimensional integral USED-FOR joint predictive distribution of label pairs. gradient chain rule USED-FOR gradient of the acquisition function. query synthesis active learning algorithm USED-FOR EER - based strategies. algorithms COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE algorithms. sampling efficiency EVALUATE-FOR state - of - the - art algorithms. synthetic and real - world datasets EVALUATE-FOR state - of - the - art algorithms. sampling efficiency EVALUATE-FOR algorithms. synthetic and real - world datasets EVALUATE-FOR algorithms. Task is labeling. Method is onestep - look - ahead manner. OtherScientificTerm are EER - based acquisition functions, and EER. Metric is computational overhead. ",This paper proposes a new acquisition function to improve the data/label efficiency in Active learning. The acquisition function is based on the Gaussian Process Classification (GPC) framework. Active learning strategies are used to reduce the classification error by using active learning strategies for Estimated Error Reduction (ER). The authors propose a query synthesis active learning algorithm that uses gradient-based optimization techniques for query synthesis in a continuous instance space and a discrete instance set (pool-based scenario). The EER-based acquisition functions are trained in an onestep-look-ahead manner. The authors show that the gradient of the acquisition function can be computed using a gradient chain rule. The one-dimensional integral is used to learn the joint predictive distribution of label pairs. The proposed algorithms are evaluated on synthetic and real-world datasets and show that it performs better than state-of-the-art algorithms in terms of sampling efficiency. ,"This paper proposes a novel acquisition function for improving data/label efficiency. Active learning is an important problem in the context of labeling. The authors propose two active learning scenarios for Gaussian Process Classification (GPC): discrete instance set (pool-based scenario) and continuous instance space (query synthesis scenario). Active learning strategies are used to reduce the classification error by estimating the Estimated Error Reduction (EER). The authors show that it is more efficient than gradient-based optimization techniques for query synthesis in the discrete instance space, and it is also more efficient in the query synthesis scenario. The main contribution of the paper is to propose an onestep-look-ahead manner to estimate the EER-based acquisition functions. This is done by computing the joint predictive distribution of label pairs using a one-dimensional integral. The gradient of the acquisition function is computed using a gradient chain rule. Experimental results show that the proposed algorithms outperform state-of-the-art algorithms on both synthetic and real-world datasets. "
12433,SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"energy functions USED-FOR bounded gradients. gradients USED-FOR numerical instabilities. regularization FEATURE-OF autoencoder - based architectures. low - dimensional manifold FEATURE-OF data. natural images HYPONYM-OF data. natural images HYPONYM-OF low - dimensional manifold. VAE models HYPONYM-OF autoencoder - based architectures. over - regularization CONJUNCTION underregularization. underregularization CONJUNCTION over - regularization. infinite gradients FEATURE-OF autoencoder - based energy function. Method are continuous variational autoencoder ( VAE ) models, and VAE energy function. OtherScientificTerm are parameter gradients, unbounded gradients, posterior collapse, overand under - regularization, and large gradients. Generic is model. Task is suboptimal feature selection. ","This paper studies the problem of continuous variational autoencoder (VAE) models where the parameter gradients are unbounded gradients. The authors consider the case where the data is a low-dimensional manifold (e.g., natural images). They show that the energy functions of bounded gradients can lead to numerical instabilities when the number of parameters in the model is large. They then propose a VAE energy function that can be used to regularize the parameters of the model. The paper also shows that the over-regularization and underregularization can be combined to improve the performance of the autoencoders. ","This paper proposes a continuous variational autoencoder (VAE) models, where the parameters of the model are parameter gradients. The authors show that the energy functions of bounded gradients are invariant to numerical instabilities, and that under-regularization and underregularization of the gradients can lead to the posterior collapse of the models. The paper also shows that the authors can generalize the results of VAE models to autoencoders-based architectures with regularization. "
12469,SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"graph feedback USED-FOR bandit problem. directed graph USED-FOR bandit problem. min - max regret EVALUATE-FOR graph. fractional weak domination number CONJUNCTION k - packing independence number. k - packing independence number CONJUNCTION fractional weak domination number. linear program USED-FOR them. fractional vertex packing set HYPONYM-OF linear program. strong duality theorem USED-FOR regret upper bound. integrality gap FEATURE-OF dual linear program. trees CONJUNCTION graphs. graphs CONJUNCTION trees. bounded integrality gap FEATURE-OF vertex packing problem. graphs FEATURE-OF vertex packing problem. trees FEATURE-OF vertex packing problem. bounded degree FEATURE-OF graphs. OtherScientificTerm are bandit arms, lower bound, and optimal regret. Metric is regret. Generic are notions, and bounds. ","This paper studies graph feedback in the bandit problem, where the goal is to find a directed graph with min-max regret. The authors propose a dual linear program with fractional weak domination number and k-packing independence number to solve them. The regret upper bound is based on a strong duality theorem. The lower bound relies on the fact that the optimal regret is bounded by the bounded degree of the graphs. ","This paper studies graph feedback in the bandit problem, where the goal is to learn a directed graph that maximizes the min-max regret of the graph. The authors propose two notions of regret: (1) the fractional weak domination number and (2) the k-packing independence number. The lower bound is based on the strong duality theorem, and the regret upper bound relies on strong dualities theorem. They show that the two notions are equivalent to each other, and they show that they can be approximated by a linear program with a fractional vertex packing set, which is equivalent to a dual linear program. They also provide bounds on the bounded integrality gap of the vertex packing problem of graphs with trees and graphs with a bounded degree. "
12505,SP:e50dec57af337839cbde4b65fb7b431785fda44d,"Shapley values USED-FOR model agnostic feature attributions. global population distribution USED-FOR feature absence. neighbourhood reference distributions USED-FOR Shapley values. Nadaraya - Watson estimator HYPONYM-OF kernel regressor. self - normalised importance sampling estimator USED-FOR Nadaraya - Watson estimator. Neighbourhood Shapley values USED-FOR sparse feature relevance attributions. on - manifold explainability CONJUNCTION robustness. robustness CONJUNCTION on - manifold explainability. robustness EVALUATE-FOR adversarial classifiers. They USED-FOR adversarial classifiers. robustness EVALUATE-FOR They. on - manifold explainability EVALUATE-FOR They. OtherScientificTerm are global population, and local model behaviour. Method is Shapley analysis. ","This paper studies the problem of model agnostic feature attributions based on Shapley values for sparse feature relevance attributions. The authors propose a self-normalised importance sampling estimator based on the Nadaraya-Watson estimator, which is a kernel regressor with neighbourhood reference distributions. They show that the Shapley analysis improves the on-manifold explainability and robustness of adversarial classifiers. ","The paper proposes to use Shapley values for model agnostic feature attributions. The idea is that the global population distribution of feature absence can be used as a proxy for feature absence in the feature absence. The authors use neighbourhood reference distributions to estimate the Shapley value of a kernel regressor (Nadaraya-Watson estimator), which is based on a self-normalised importance sampling estimator. They show that They improve the on-manifold explainability and robustness of adversarial classifiers. They also show that the local model behaviour is similar to the global model behaviour. "
12541,SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"feature representations USED-FOR deep reinforcement learning ( RL ). them USED-FOR feature learning. state - action sequences HYPONYM-OF un - experienced or less - experienced trajectories. data efficiency EVALUATE-FOR RL feature representation learning. backward dynamics model USED-FOR trajectory cycle. dynamics model USED-FOR PlayVirtual. actions USED-FOR virtual state - action trajectories. cycle consistency constraint FEATURE-OF trajectory. Atari and DeepMind Control Suite benchmarks EVALUATE-FOR designs. benchmarks EVALUATE-FOR method. Method is RL. OtherScientificTerm are data inefficiency, cycle - consistent virtual trajectories, latent space, and groudtruth state supervision. ","This paper studies the problem of feature representations for deep reinforcement learning (RL) and proposes to use them for feature learning in order to improve the data efficiency of RL feature representation learning. The authors propose to use a backward dynamics model to model the trajectory cycle of a trajectory in PlayVirtual, which is composed of state-action sequences, i.e., un-experienced or less-empirical trajectories (e.g. state-actions). The authors show that this trajectory has a cycle consistency constraint, and that the dynamics model can be used to learn the dynamics of the trajectory. The proposed method is evaluated on the Atari and DeepMind Control Suite benchmarks and shows that the proposed method achieves better performance than existing designs. ","This paper proposes a novel way to learn feature representations for deep reinforcement learning (RL). The key idea is to use them for feature learning in un-experienced or less-experimental trajectories (e.g., state-action sequences). In RL, data inefficiency is a major issue in RL feature representation learning due to the high data efficiency. To address this issue, the authors propose to learn cycle-consistent virtual trajectories in the latent space. The authors use a backward dynamics model to model the trajectory cycle in PlayVirtual. The trajectory is constrained by a cycle consistency constraint. The proposed method is evaluated on Atari and DeepMind Control Suite benchmarks. The results show that the proposed method outperforms the baselines in terms of data efficiency and performance. "
12577,SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,Noisy labels FEATURE-OF large real - world datasets. noisy labels FEATURE-OF robustness. robustness EVALUATE-FOR network ’s architecture. framework USED-FOR robustness. robustness EVALUATE-FOR network. architecture CONJUNCTION target / noise functions. target / noise functions CONJUNCTION architecture. predictive power EVALUATE-FOR representations. predictive power EVALUATE-FOR network ’s robustness. representations USED-FOR linear model. clean labels USED-FOR linear model. architecture COMPARE noise. noise COMPARE architecture. architecture USED-FOR network. predictive power COMPARE methods. methods COMPARE predictive power. predictive power USED-FOR representations. predictive power COMPARE noisy - label - training methods. noisy - label - training methods COMPARE predictive power. representations COMPARE noisy - label - training methods. noisy - label - training methods COMPARE representations. test accuracy EVALUATE-FOR noisy - label - training methods. test accuracy EVALUATE-FOR predictive power. clean labels USED-FOR methods. Method is neural network architectures. ,"This paper studies the robustness of neural network architectures with noisy labels on large real-world datasets. The authors propose a framework to measure the robusts of a network’s architecture against noisy labels. The proposed architecture and target/noise functions are trained with a linear model with clean labels. They show that the proposed representations have better predictive power and better test accuracy than other representations trained with noisy-label-training methods, and that their architecture is more robust against noise than other methods. ","This paper proposes a framework to measure the robustness of a network’s architecture to noisy labels in large real-world datasets. The authors show that the proposed architecture and target/noise functions are more robust to noise than a linear model trained with clean labels. They also show that their proposed architecture is more robust against noise than other neural network architectures. Finally, they show that predictive power of the proposed representations is better than other methods in terms of test accuracy. "
12613,SP:903727fe028684623a8ccadec210e641ecffc685,"RL algorithm USED-FOR reward function. method USED-FOR value function. transitions USED-FOR value function. hyperparameters USED-FOR method. data - driven Bellman equation USED-FOR method. approach COMPARE prior methods. prior methods COMPARE approach. Method are Reinforcement learning ( RL ) algorithms, RL algorithms, and control algorithm. Generic are process, and two - stage process. OtherScientificTerm are intermediate reward function, and reward function term. ","This paper studies the problem of Reinforcement learning (RL) algorithms. The authors propose a method to learn a reward function using RL algorithm with hyperparameters. The proposed method is based on a data-driven Bellman equation, where the intermediate reward function is a function of the number of transitions in the two-stage process, and the goal is to learn the value function. The paper shows that the proposed approach can achieve better performance than prior methods. ","This paper proposes a new method for learning the value function of an RL algorithm for learning a reward function. The method is based on Reinforcement learning (RL) algorithms, where the intermediate reward function is computed by a two-stage process. The main idea of the proposed method is to use transitions to learn the reward function term. The proposed method uses hyperparameters and a data-driven Bellman equation. The authors show that the proposed approach outperforms prior methods. They also show that their approach is more efficient than prior methods in terms of the number of transitions. The paper also shows that their method is more computationally efficient than existing RL algorithms. "
12649,SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"convex and non - convex settings FEATURE-OF differentially private stochastic optimization. algorithm USED-FOR l2 setting. differentially private algorithms USED-FOR general convex losses. algorithm USED-FOR optimal excess population risk. near - linear time FEATURE-OF algorithm. super - linear time FEATURE-OF differentially private algorithms. algorithm USED-FOR l1 setting. algorithm USED-FOR dimension dependent lower bound. nearly - optimal excess population risk FEATURE-OF algorithm. algorithms USED-FOR differentially private non - convex setting. smooth losses CONJUNCTION polyhedral constraint. polyhedral constraint CONJUNCTION smooth losses. smooth losses FEATURE-OF l1 - case. polyhedral constraint FEATURE-OF l1 - case. linear time FEATURE-OF nearly dimension independent rate. smooth losses FEATURE-OF constrained l2 - case. linear - time algorithm USED-FOR constrained l2 - case. method USED-FOR non - smooth weakly convex stochastic optimization. method COMPARE non - private algorithm. non - private algorithm COMPARE method. method USED-FOR l2 - case. non - convex l2 setting CONJUNCTION lp setting. lp setting CONJUNCTION non - convex l2 setting. Material is convex case. OtherScientificTerm are general non - smooth convex losses, and polylogarithmic ( in the dimension ) overhead. ","This paper studies the problem of differentially private stochastic optimization in convex and non-convex settings. The authors propose a new algorithm for the l2 setting with near-linear time. The proposed algorithm is based on the general non-smooth convex losses, and is able to obtain a dimension dependent lower bound on the nearly-optimal excess population risk. The algorithm is also able to converge to the optimal excess risk in the l1 setting. The paper also shows that the proposed method can be applied to the non-strongly convex l2-case and the lp setting.","This paper studies the problem of differentially private stochastic optimization in convex and non-convex settings. The authors propose a new algorithm for the l2 setting with near-linear time. The proposed algorithm is based on a dimension dependent lower bound on the nearly-optimal excess population risk, which is a result of the polylogarithmic (in the dimension) overhead. The algorithm can be applied to general convex losses in the convex case as well as general non-smooth convex loss in the non-Convex setting. The main contribution of the paper is the theoretical analysis of the proposed algorithm in the l1 setting and the polyhedral constraint of the L2-case. The method is shown to outperform the existing non-private algorithm in both the constrained l1-case with smooth losses and the lp setting without smooth losses. "
12685,SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"cooperative bandit problem USED-FOR large - scale decision - making. arbitrary corruptions CONJUNCTION delays. delays CONJUNCTION arbitrary corruptions. arbitrary corruptions FEATURE-OF stochastic networks. stochastic networks USED-FOR communication. cooperative bandit learning HYPONYM-OF real - world communication scenarios. adversarially corrupted rewards FEATURE-OF message - passing. random delays FEATURE-OF network. byzantine communication PART-OF message - passing. network USED-FOR instantaneous rewardsharing. stochastic time - varying networks USED-FOR message - passing. message - passing HYPONYM-OF real - world communication scenarios. instantaneous rewardsharing HYPONYM-OF real - world communication scenarios. message - passing HYPONYM-OF real - world communication scenarios. near - optimal guarantees FEATURE-OF incurred group regret. decentralized algorithms CONJUNCTION near - optimal guarantees. near - optimal guarantees CONJUNCTION decentralized algorithms. decentralized algorithms USED-FOR environments. delayed - update algorithm COMPARE state - of - the - art. state - of - the - art COMPARE delayed - update algorithm. network topologies EVALUATE-FOR delayed - update algorithm. network topologies EVALUATE-FOR state - of - the - art. tight network - dependent minimax lower bounds FEATURE-OF group regret. Generic are problem, and algorithms. OtherScientificTerm is perfect communication. Task is real - world distributed settings. ",This paper studies the cooperative bandit problem in large-scale decision-making. The authors consider the problem of communication between stochastic networks with arbitrary corruptions and delays. They show that message-passing with adversarially corrupted rewards with random delays can incur group regret with tight network-dependent minimax lower bounds. They also show that decentralized algorithms can achieve near-optimal guarantees in these environments. ,"This paper studies the cooperative bandit problem for large-scale decision-making. The authors consider the problem in the context of stochastic networks with arbitrary corruptions and delays. They propose two algorithms to solve the problem. The first algorithm is based on stochastically time-dynamically varying networks. The second algorithm uses a byzantine communication part of message-passing with random delays. The proposed algorithms are evaluated on real-world distributed settings, including instantaneous rewardsharing and message-solving. The paper shows that the proposed delayed-update algorithm outperforms state-of-the-art in terms of network topologies and near-optimal guarantees on group regret under tight network-dependent minimax lower bounds."
12721,SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"transformer USED-FOR computer vision applications. architectures USED-FOR feature representations. vision transformers USED-FOR feature representations. convolutional neural networks COMPARE vision transformers. vision transformers COMPARE convolutional neural networks. architectures FEATURE-OF vision transformers. mobile devices USED-FOR feature representations. post - training quantization algorithm USED-FOR vision transformers. optimal low - bit quantization intervals USED-FOR quantization task. ranking loss USED-FOR quantization objective. quantization loss CONJUNCTION feature diversity. feature diversity CONJUNCTION quantization loss. nuclear norm FEATURE-OF attention map. nuclear norm USED-FOR mixedprecision quantization scheme. method COMPARE posttraining quantization algorithms. posttraining quantization algorithms COMPARE method. benchmark models EVALUATE-FOR method. top-1 accuracy EVALUATE-FOR DeiT - B model. ImageNet dataset EVALUATE-FOR DeiT - B model. Method is attention mechanism. OtherScientificTerm are self - attention, and quantization. ","This paper proposes a new transformer for computer vision applications. The key idea is to use a post-training quantization algorithm to improve the performance of vision transformers trained on mobile devices. The authors propose a quantization task with optimal low-bit quantization intervals. The quantization objective is based on a ranking loss, and the attention mechanism is a weighted sum of the quantization loss and the feature diversity. The attention map is a nuclear norm of the attention map. The proposed mixedprecision quantization scheme is evaluated on the ImageNet dataset and shows that the proposed method improves the top-1 accuracy of the DeiT-B model.","This paper proposes a new transformer for computer vision applications. The key idea is to use different architectures for different feature representations in different mobile devices. The authors propose a post-training quantization algorithm for vision transformers, which outperforms convolutional neural networks. The quantization objective is based on a ranking loss, and the attention mechanism relies on self-attention. The main contribution of the paper is to propose a mixedprecision quantization scheme based on the nuclear norm of the attention map. The proposed method is evaluated on three benchmark models and achieves top-1 accuracy on ImageNet dataset. "
12757,SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"Double Q - learning USED-FOR overestimation issue of Q - learning. polynomial learning rate USED-FOR analysis. polynomial learning rate USED-FOR slower convergence rate. analytical tools USED-FOR convergence rate. sampling strategy USED-FOR asynchronous double Q - learning. synchronous double Q - learning USED-FOR global optimum. time complexity EVALUATE-FOR asynchronous algorithm. fast convergence FEATURE-OF double - Q learning. Method are Q - learning, double Q - learning, and finite - time analysis. OtherScientificTerm are constant learning rate, state - action space, and discount factor. ",This paper studies the overestimation issue of Q-learning in the context of synchronous double-Q-learning. The authors propose a sampling strategy to achieve the global optimum in asynchronous double-learning with a polynomial learning rate. The convergence rate is shown to be faster than the constant learning rate under the assumption that the state-action space is a finite-time analysis. The paper also shows that the asynchronous algorithm has a lower time complexity than the synchronous algorithm. ,"This paper studies the overestimation issue of Q-learning. The authors propose a sampling strategy for asynchronous double Q-Learning, which is an extension of the previous work on double Q - learning. The main contribution of the paper is a polynomial learning rate for the analysis of the convergence rate of the slower convergence rate, which can be computed using analytical tools. They show that the constant learning rate can be used to estimate the state-action space, and that the global optimum is obtained by synchronous double-learning, which has a lower time complexity than the asynchronous algorithm. They also show that double-Q learning has fast convergence in terms of the discount factor, and they provide a finite-time analysis."
12793,SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"unlabeled and test data COMPARE labeled data. labeled data COMPARE unlabeled and test data. SSL algorithms USED-FOR real - world applications. semi - supervised OOD detection HYPONYM-OF setting. labeled data CONJUNCTION in - distribution data. in - distribution data CONJUNCTION labeled data. approach STEP USED-FOR OOD detection. technique USED-FOR approach STEP. Structure - Keep Unzipping HYPONYM-OF technique. It USED-FOR representation space. representation space USED-FOR OOD samples. STEP approach COMPARE methods. methods COMPARE STEP approach. OOD detection benchmarks EVALUATE-FOR STEP approach. benchmarks EVALUATE-FOR detection. detection EVALUATE-FOR STEP approach. benchmarks EVALUATE-FOR STEP approach. Task are semi - supervised learning ( SSL ) studies, OOD detection settings, and training. OtherScientificTerm are distribution of labeled data, and unknown distribution. Method is optimization algorithm. ","This paper studies semi-supervised learning (SSL) studies in OOD detection settings, where the distribution of labeled data is different from that of the unlabeled and test data. In this setting, SSL algorithms can be applied to real-world applications. The authors propose a technique called Structure-Keep Unzipping (STU) to improve the performance of the proposed approach STEP in the setting where the labeled data and in-distribution data are not available. STU is based on the optimization algorithm from [1]. It uses a representation space to represent the OOD samples and the unknown distribution. Experiments show that the proposed STEP approach performs better than other methods in detection.","This paper proposes a new semi-supervised learning (SSL) studies for OOD detection settings, where the distribution of labeled data is different from that of unlabeled and test data. In this setting, SSL algorithms can be applied to real-world applications. The authors propose a technique called Structure-Keep Unzipping to improve the performance of the proposed approach STEP. The proposed optimization algorithm is based on the idea of minimizing the difference between the unknown distribution and the labeled data. It is shown that the representation space of the OOD samples is similar to the original representation space. Experiments show that the proposed STEP approach outperforms other methods in detection on several benchmarks."
12829,SP:6bf8b94483b26033795b0eda9649518027f5e1c2,phrase localization CONJUNCTION referring expression comprehension / segmentation. referring expression comprehension / segmentation CONJUNCTION phrase localization. visual grounding USED-FOR visual reasoning. referring expression comprehension / segmentation HYPONYM-OF visual grounding. phrase localization HYPONYM-OF visual grounding. approaches USED-FOR referring expression comprehension ( REC ). approaches USED-FOR segmentation ( RES ). referring expression comprehension ( REC ) CONJUNCTION segmentation ( RES ). segmentation ( RES ) CONJUNCTION referring expression comprehension ( REC ). one - stage multi - task framework USED-FOR visual grounding tasks. modalities PART-OF visual - lingual encoder. modalities PART-OF transformer architecture. model USED-FOR contextualized lingual queries. segmentation mask USED-FOR referred regions. model USED-FOR decoder. contextualized model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE contextualized model. REC and RES tasks EVALUATE-FOR contextualized model. REC and RES tasks EVALUATE-FOR state - of - the - art methods. external dataset EVALUATE-FOR pre - training schedule. contextualized information CONJUNCTION multi - task training. multi - task training CONJUNCTION contextualized information. contextualized information USED-FOR model. multi - task training USED-FOR model. Generic is two - stage setup. Method is complex task - specific one - stage architectures. OtherScientificTerm is bounding box. ,"This paper proposes a two-stage setup for visual grounding for visual reasoning. The main idea is to use a one-stage multi-task framework to learn visual grounding tasks such as phrase localization and referring expression comprehension/segmentation. The proposed transformer architecture consists of two modalities: a visual-lingual encoder and a decoder. The model learns contextualized lingual queries from contextualized information, and then uses the learned model to generate contextualized expressions for the decoder to use as a bounding box. The learned segmentation mask is used to represent the referred regions. The contextualized model is compared to state-of-the-art methods on both REC and RES tasks. The pre-training schedule is also evaluated on an external dataset. ",This paper proposes a two-stage multi-task framework for visual grounding for visual reasoning. The main idea is to combine approaches for referring expression comprehension (REC) and segmentation (RES) with phrase localization. The proposed transformer architecture consists of two modalities: a visual-lingual encoder and a decoder. The model learns contextualized lingual queries from the contextualized information and the decoder uses a segmentation mask to map the referred regions to the bounding box. The pre-training schedule is evaluated on an external dataset. The contextualized model is compared to state-of-the-art methods on both REC and RES tasks. The results show that the model is able to learn contextualized and contextualized-specific one-stage architectures.
12865,SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"Boosting HYPONYM-OF algorithmic approach. weak learner HYPONYM-OF agnostic PAC learner. classification loss EVALUATE-FOR agnostic PAC learner. boosting algorithm USED-FOR weak hypotheses. booster CONJUNCTION weak learner. weak learner CONJUNCTION booster. OtherScientificTerm are weak and moderately inaccurate hypotheses, and weak - learner calls. Method are multiclass boosting, Multiclass boosting, boosting, and AdaBoost. Metric is weak learner ’s accuracy parameter. ","This paper studies the problem of multiclass boosting, an algorithmic approach to improve the performance of the agnostic PAC learner (e.g. weak learner). Multiclass boosting is a well-studied and well-motivated problem in the literature. The main contribution of this paper is a new boosting algorithm for weak hypotheses. The authors propose AdaBoost, a boosting algorithm that can be applied to both weak and moderately inaccurate hypotheses. They show that the weak-learner calls are more accurate than the strong learner calls. They also show that AdaBoost can improve the classification loss of an agnosticPAC learner.","This paper proposes a new algorithmic approach called Boosting, which is an extension of multiclass boosting. Multiclass boosting is a variant of boosting, where the weak learner’s accuracy parameter is a function of the classification loss of the agnostic PAC learner (e.g., the weak learners). The authors propose a boosting algorithm for weak hypotheses, where weak and moderately inaccurate hypotheses are considered. The weak-learner calls are called AdaBoost. The authors show that boosting is equivalent to a weak learners’ accuracy parameter, and that the strong learner calls can be seen as weak-learner calls. "
12901,SP:f63b050773871338c48b778c362172e4b72477a4,"methods USED-FOR unsupervised object segmentation. methods USED-FOR interpretable object - centric scene generation. unsupervised object segmentation CONJUNCTION interpretable object - centric scene generation. interpretable object - centric scene generation CONJUNCTION unsupervised object segmentation. limited visual complexity FEATURE-OF simulated and real - world datasets. simulated and real - world datasets EVALUATE-FOR methods. RNNs USED-FOR object representations. paradigms COMPARE embedding - based approach. embedding - based approach COMPARE paradigms. clustering procedure USED-FOR randomly ordered object representations. iterative refinement COMPARE clustering procedure. clustering procedure COMPARE iterative refinement. RNNs CONJUNCTION iterative refinement. iterative refinement CONJUNCTION RNNs. GENESIS - V2 USED-FOR variable number of object representations. GENESIS - V2 HYPONYM-OF model. RNNs USED-FOR variable number of object representations. iterative refinement USED-FOR variable number of object representations. GENESIS - V2 COMPARE baselines. baselines COMPARE GENESIS - V2. unsupervised image segmentation CONJUNCTION object - centric scene generation. object - centric scene generation CONJUNCTION unsupervised image segmentation. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. GENESIS - V2 USED-FOR unsupervised image segmentation. GENESIS - V2 USED-FOR object - centric scene generation. baselines USED-FOR unsupervised image segmentation. synthetic datasets USED-FOR unsupervised image segmentation. object - centric scene generation EVALUATE-FOR baselines. real - world datasets USED-FOR object - centric scene generation. synthetic datasets USED-FOR object - centric scene generation. synthetic datasets EVALUATE-FOR GENESIS - V2. real - world datasets EVALUATE-FOR GENESIS - V2. real - world datasets EVALUATE-FOR baselines. Method are unsupervised learning of object - representations, and stochastic stick - breaking process. OtherScientificTerm is unnatural ordering. ","This paper proposes a new unsupervised learning of object-representations. The proposed model, GENESIS-V2, is based on a clustering procedure for randomly ordered object representations. The authors show that the proposed method outperforms existing methods on both simulated and real-world datasets with limited visual complexity.   The authors also show that GENESis-v2 is able to learn a variable number of object representations using RNNs and iterative refinement, which is more efficient than existing paradigms such as the embedding-based approach. ","The authors propose two methods for unsupervised object segmentation for interpretable object-centric scene generation. The proposed methods are evaluated on simulated and real-world datasets with limited visual complexity. The main contribution of the paper is to propose a new model, GENESIS-V2, which is an extension of the GENIS-VAE model. The key idea is to use RNNs and iterative refinement to learn the object representations. The authors propose a clustering procedure for randomly ordered object representations, which can be seen as a variant of the embedding-based approach. The idea is interesting and novel. However, the paper suffers from a lack of experimental results. "
12937,SP:408deb9e5577ee7118b836fee77135df641fe545,"black box method USED-FOR point predictions. conformal inference USED-FOR framework. conformal inference methods COMPARE adaptive approach. adaptive approach COMPARE conformal inference methods. coverage frequency EVALUATE-FOR adaptive approach. learning problem USED-FOR distribution shift. adaptive conformal inference HYPONYM-OF method. real world datasets EVALUATE-FOR adaptive conformal inference. real world datasets EVALUATE-FOR method. Generic is methods. OtherScientificTerm are data generating distribution, data generating process, and distribution shifts. ","This paper proposes a black box method for point predictions. The framework is based on conformal inference, where the data generating distribution is sampled from a data generating process. The learning problem is to learn the distribution shift in the data generated by the distribution shifts. The authors show that the adaptive approach has a better coverage frequency than conformal in terms of the number of samples. The proposed method is evaluated on a variety of real world datasets.","This paper proposes a black box method for point predictions. The framework is based on conformal inference, where the data generating distribution is sampled from a data generating process. The authors show that the proposed adaptive approach is more robust to distribution shifts in coverage frequency than conventional conformal inference methods. The distribution shift is caused by a learning problem where the learning problem is solved by minimizing the distribution shift. The proposed method is evaluated on two real world datasets."
12973,SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"crowded scenes USED-FOR Multi - person pose estimation. bounding box detection CONJUNCTION keypoint grouping. keypoint grouping CONJUNCTION bounding box detection. bounding box detection PART-OF direct pose - level inference strategy. keypoint grouping PART-OF direct pose - level inference strategy. Pose - level Inference Network ( PINet ) USED-FOR complete pose cues. visible body parts USED-FOR complete pose cues. Part - based Pose Generation ( PPG ) USED-FOR coarse poses. PINet USED-FOR coarse poses. Part - based Pose Generation ( PPG ) USED-FOR PINet. pose priors USED-FOR Pose Refinement module. Pose Refinement module USED-FOR coarse poses. visual body cues USED-FOR global pose cues. visual body cues USED-FOR PINet. discriminative body parts USED-FOR PINet. crowded scenes pose estimation benchmarks EVALUATE-FOR PINet. AP EVALUATE-FOR it. OCHuman dataset EVALUATE-FOR it. OtherScientificTerm are overlapping and occlusions, person bounding boxes, and pose cues. Method is Pose Fusion module. ","This paper studies the problem of multi-person pose estimation in crowdsed scenes. The authors propose Pose-level Inference Network (PINet), a direct pose-level inference strategy based on bounding box detection and keypoint grouping. The PINet learns complete pose cues from visible body parts as well as visual body cues for global pose cues. The Pose Refinement module uses pose priors to refine coarse poses using Part-based Pose Generation (PPG) and PINet for coarse poses with overlapping and occlusions. PINet is trained with discriminative body parts and the Pose Fusion module is used to learn the pose cues for each person bounding boxes. The proposed PINet achieves state-of-the-art performance on several crowded scenes pose estimation benchmarks. ","This paper proposes Pose-level Inference Network (PINet), a direct pose-level inference strategy based on bounding box detection and keypoint grouping. PINet learns complete pose cues from visible body parts and global pose cues based on visual body cues. The Pose Fusion module is a Pose Refinement module that uses pose priors to generate coarse poses using Part-based Pose Generation (PPG) for coarse poses with overlapping and occlusions. The PINet is evaluated on several crowded scenes pose estimation benchmarks on the OCHuman dataset. "
13022,SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,Robust Markov decision processes ( RMDPs ) PART-OF robust reinforcement learning algorithms. algorithm USED-FOR Bellman operator. Bellman operator USED-FOR S - rectangular robust Markov decision processes. L∞-constrained rectangular ambiguity sets FEATURE-OF S - rectangular robust Markov decision processes. homotopy continuation method CONJUNCTION bisection method. bisection method CONJUNCTION homotopy continuation method. algorithm USED-FOR S - rectangular ambiguity. bisection method USED-FOR S - rectangular ambiguity. homotopy continuation method USED-FOR S - rectangular ambiguity. quasi - linear time FEATURE-OF S - rectangular ambiguity. bisection method PART-OF algorithm. homotopy continuation method PART-OF algorithm. cubic time FEATURE-OF leading general linear programming methods. leading general linear programming methods USED-FOR algorithm. cubic time EVALUATE-FOR algorithm. it COMPARE leading commercial optimization package. leading commercial optimization package COMPARE it. Generic is method. ,This paper proposes a new algorithm for Bellman operator for S-rectangular robust Markov decision processes (RMDPs) in robust reinforcement learning algorithms. The proposed algorithm is based on the homotopy continuation method and bisection method. The authors show that the proposed algorithm has quasi-linear time in terms of S-ratio ambiguity in the form of cubic time. They also show that it can be combined with a leading commercial optimization package to achieve better cubic time than leading general linear programming methods. ,"This paper proposes a novel algorithm for learning robust Markov decision processes (RMDPs) in robust reinforcement learning algorithms. The algorithm is based on the Bellman operator, which can be used to learn S-rectangular robust decision processes in L∞-constrained rectangular ambiguity sets. The proposed algorithm consists of a homotopy continuation method, a bisection method, and an algorithm for solving S-triangular ambiguity in quasi-linear time. Experiments show that the proposed algorithm achieves better cubic time than leading general linear programming methods in terms of cubic time, and it outperforms the leading commercial optimization package. "
13071,SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,machine - learned predictions USED-FOR online algorithms. generalized one - way trading CONJUNCTION two - stage online knapsack. two - stage online knapsack CONJUNCTION generalized one - way trading. competitive ratio EVALUATE-FOR online algorithms. Task is online knapsack problem. OtherScientificTerm is upper and lower bound. ,"This paper studies the online knapsack problem. The authors propose two online algorithms based on machine-learn predictions. The first is generalized one-way trading, and the second is two-stage onlineknapsack. The competitive ratio of the online algorithms is studied. The upper and lower bound are provided.","This paper studies the online knapsack problem with machine-learned predictions for online algorithms. The authors propose two online algorithms, generalized one-way trading and two-stage onlineknapsack. The upper and lower bound of the competitive ratio of the online algorithms is given."
13120,SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"Episodic control USED-FOR reinforcement learning. model - based episodic memory of trajectories USED-FOR episodic control. memory USED-FOR agent. memory USED-FOR complementary learning model. model - based, episodic and habitual learning PART-OF architecture. dynamic hybrid control CONJUNCTION model - based, episodic and habitual learning. model - based, episodic and habitual learning CONJUNCTION dynamic hybrid control. dynamic hybrid control USED-FOR complementary learning model. model - based, episodic and habitual learning USED-FOR complementary learning model. model COMPARE reinforcement learning agents. reinforcement learning agents COMPARE model. OtherScientificTerm are episodic memory, and stochastic and non - Markovian settings. ","This paper proposes a new episodic control for reinforcement learning based on a model-based episodic memory of trajectories. The proposed architecture combines dynamic hybrid control with model- based, episodic and habitual learning in order to train a complementary learning model that uses the memory to guide the agent. The authors show that the proposed model outperforms existing reinforcement learning agents in both stochastic and non-Markovian settings.",This paper proposes a novel architecture that combines model-based episodic memory of trajectories for reinforcement learning with dynamic hybrid control for episodic control. The key idea is to use the memory of the agent to guide the learning of the complementary learning model. Experiments on stochastic and non-Markovian settings show that the proposed model outperforms other reinforcement learning agents.
13169,SP:551174c1266b5f4b6aaf5432a4c713386f90898c,labeled data USED-FOR deep learning. Semi - supervised learning ( SSL ) USED-FOR unlabeled data. pseudo labels USED-FOR unlabeled data. data programming ( DP ) scheme USED-FOR probabilistic labels. probabilistic labels USED-FOR unlabeled data. DP - SSL HYPONYM-OF SSL method. data programming ( DP ) scheme USED-FOR SSL method. LFs PART-OF SSL style. DP methods USED-FOR initial labeling functions ( LFs ). human experts USED-FOR DP methods. noisy labels USED-FOR label model. probabilistic labels USED-FOR unlabeled samples. LFs USED-FOR noisy labels. DP - SSL COMPARE SSL methods. SSL methods COMPARE DP - SSL. classification EVALUATE-FOR SSL methods. SSL benchmarks EVALUATE-FOR DP - SSL. test sets EVALUATE-FOR SSL methods. DP - SSL USED-FOR unlabeled data. test sets EVALUATE-FOR classification. classification EVALUATE-FOR DP - SSL. test sets EVALUATE-FOR DP - SSL. CIFAR-10 EVALUATE-FOR DP - SSL. classification accuracy EVALUATE-FOR test data. unlabeled data EVALUATE-FOR DP - SSL. test data EVALUATE-FOR DP - SSL. classification accuracy EVALUATE-FOR DP - SSL. annotation accuracy EVALUATE-FOR DP - SSL. Method is SSL. Material is labeled samples. ,"This paper proposes a new SSL method called DP-SSL, which is based on the data programming (DP) scheme for learning probabilistic labels for unlabeled data. The authors propose to use pseudo labels to represent unlabeling data as pseudo labels, and use DP methods to learn initial labeling functions (LFs). The DP methods are based on human experts, and can be trained using DP methods. The proposed SSL method is evaluated on a variety of SSL benchmarks, and the results show that DP -SSL outperforms SSL methods in terms of classification accuracy and annotation accuracy on test data. ","This paper proposes a new SSL method called DP-SSL, which is a variant of Semi-supervised learning (SSL) for unlabeled data. The SSL method is based on the data programming (DP) scheme, which uses probabilistic labels to learn unlabeling data from pseudo labels. The DP methods are based on initial labeling functions (LFs) that are learned by human experts. The authors show that DP methods outperform SSL methods on several SSL benchmarks, including CIFAR-10, and on several test sets for classification and annotation accuracy. They also show that noisy labels can be used to improve the performance of the label model. "
13218,SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"Multi - view Pose transformer ( MvP ) USED-FOR estimating multi - person 3D poses. multi - view images USED-FOR Multi - view Pose transformer ( MvP ). multi - view images USED-FOR estimating multi - person 3D poses. MvP USED-FOR multi - person 3D poses. volumetric representation USED-FOR per - person 3D pose. volumetric representation USED-FOR estimating 3D joint locations. detected 2D poses USED-FOR per - person 3D pose. query embeddings USED-FOR MvP. query embeddings USED-FOR skeleton joints. accuracy EVALUATE-FOR pipeline. hierarchical scheme USED-FOR query embeddings of multi - person skeleton joints. accuracy EVALUATE-FOR MvP. hierarchical scheme USED-FOR MvP. geometrically guided attention mechanism USED-FOR cross - view information. MvP USED-FOR geometrically guided attention mechanism. projective attention HYPONYM-OF geometrically guided attention mechanism. feature representations USED-FOR projective attention. view - dependent camera geometry PART-OF feature representations. RayConv operation USED-FOR MvP. MvP model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MvP model. it COMPARE approach. approach COMPARE it. Panoptic dataset EVALUATE-FOR AP25. AP25 EVALUATE-FOR it. Panoptic dataset EVALUATE-FOR it. MvP USED-FOR recovering human mesh. MvP USED-FOR modeling multi - person body shapes. SMPL model USED-FOR modeling multi - person body shapes. SMPL model USED-FOR recovering human mesh. Generic is intermediate tasks. OtherScientificTerm are multi - view information, 3D joint locations, human mesh, and multi - person body shapes. Method is inputdependent query adaptation approach. ","This paper proposes a multi-view Pose transformer (MvP) for estimating multi-person 3D poses from multi-views images. MvP uses a geometrically guided attention mechanism, called projective attention, to capture cross-view information. The proposed pipeline is evaluated on the Panoptic dataset, and the accuracy of MvP is shown to be better than state-of-the-art methods. The authors also propose a hierarchical scheme to learn query embeddings of multi -person skeleton joints from query embedding of the MvP. The SMPL model is then used for recovering human mesh from the reconstructed human mesh. ","This paper proposes a multi-view Pose transformer (MvP) for estimating multi-person 3D poses. Multi-view images are used to generate multi- view images. The authors propose a geometrically guided attention mechanism for cross-view information, which is based on projective attention. The proposed pipeline is evaluated on the Panoptic dataset and on AP25. MvP is trained using a hierarchical scheme for query embeddings of multi-Person skeleton joints and query embedding of skeleton joints of a human mesh. The MvP model outperforms state-of-the-art methods on both intermediate tasks. The paper also proposes an inputdependent query adaptation approach. The main contribution of the paper is the use of feature representations based on view-dependent camera geometry.  "
13267,SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"sparse vectors PART-OF family. error - free responses USED-FOR sparse vectors. support recovery CONJUNCTION approximate recovery problems. approximate recovery problems CONJUNCTION support recovery. approximate recovery problems USED-FOR problems. support recovery USED-FOR problems. 1 - bit compressed sensing USED-FOR approximate recovery problems. 1 - bit compressed sensing USED-FOR problems. learning algorithms USED-FOR problem. learning algorithms USED-FOR problem. Task is learning problems. OtherScientificTerm are noisy responses, and unknown vectors. Method is learning model. Metric is query complexity. ","This paper studies the problem of support recovery and approximate recovery problems with 1-bit compressed sensing with sparse vectors in the family of sparse vectors. The authors consider the setting where the input is noisy responses and the goal is to recover sparse vectors with error-free responses. The problem is formulated as a learning problem with unknown vectors, and the learning algorithms are used to solve the problem. The learning model is trained to predict the query complexity.","This paper studies the problem of learning problems with sparse vectors, i.e., the family of sparse vectors with error-free responses. The authors propose a learning model with noisy responses, where the input is a set of unknown vectors, and the output of the learning model is the input of the unknown vectors. The problem is formulated as a 1-bit compressed sensing problem, with support recovery and approximate recovery problems, and learning algorithms are used to solve the problem. The query complexity of the query complexity is measured in terms of the number of queries."
13316,SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"sensors USED-FOR detecting abrupt changes in temporal behavior patterns. detecting abrupt changes in temporal behavior patterns FEATURE-OF industrial and security applications. sensors USED-FOR industrial and security applications. information - theoretic lower bound USED-FOR finitely parameterized probability distributions. information - theoretic lower bound FEATURE-OF detection delay. bounds COMPARE information - theoretic lower bounds. information - theoretic lower bounds COMPARE bounds. expected delay bounds USED-FOR scheme. synthetic and real datasets EVALUATE-FOR method. OtherScientificTerm are abrupt changes in temporal behavior patterns, abrupt changes, sensing actions, and exploitation of querying informative actions. Task is bandit quickest changepoint detection problem. Method is online sensing scheme. Metric is false alarm rates. ",This paper studies the bandit quickest changepoint detection problem. The authors propose an online sensing scheme based on an information-theoretic lower bound on the detection delay of finitely parameterized probability distributions. The proposed scheme is based on the expected delay bounds on the number of abrupt changes in temporal behavior patterns. The paper shows that the proposed method can be applied to both synthetic and real datasets. ,"This paper proposes an online sensing scheme for detecting abrupt changes in temporal behavior patterns in sensors for industrial and security applications. The authors propose an information-theoretic lower bound for the detection delay of finitely parameterized probability distributions. The proposed scheme is based on the expected delay bounds for the scheme, and the authors show that the proposed bounds are better than the known bounds. The paper also shows that the bandit quickest changepoint detection problem can be solved with the proposed method on both synthetic and real datasets. "
13365,SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"stochastic bilevel CONJUNCTION min - max. min - max CONJUNCTION stochastic bilevel. min - max CONJUNCTION compositional optimization. compositional optimization CONJUNCTION min - max. Stochastic nested optimization USED-FOR machine learning applications. compositional optimization HYPONYM-OF Stochastic nested optimization. stochastic bilevel HYPONYM-OF Stochastic nested optimization. min - max HYPONYM-OF Stochastic nested optimization. nested structure FEATURE-OF problems. SGD - type updates USED-FOR nested problems. they COMPARE non - nested problems. non - nested problems COMPARE they. convergence rate EVALUATE-FOR they. SGD - type updates USED-FOR stochastic nested problems. ALternating Stochastic gradient dEscenT ( ALSET ) method HYPONYM-OF SGD approach. ALSET USED-FOR stochastic nested problems. hidden smoothness FEATURE-OF problem. SGD - type algorithms USED-FOR stochastic nested problems. Method is problem - specific algorithms and analyses. Generic are analysis, and it. OtherScientificTerm are nested problem, and regularity conditions. Metric is sample complexity. ","This paper studies the problem-specific algorithms and analyses of Stochastic nested optimization in machine learning applications. The authors consider two problems: stochastic bilevel and min-max. The main contribution of the paper is a new analysis of the convergence rate of SGD-type updates for nested problems with nested structure. They show that they converge faster than non-nested problems, and that the sample complexity of a nested problem is bounded by the number of iterations of the nested problem. They also show that the problem has a hidden smoothness of the problem, which is a result of the regularity conditions. ","This paper presents a new analysis of Stochastic nested optimization for machine learning applications, namely, stochastic bilevel and min-max. The analysis is based on problem-specific algorithms and analyses. The main idea of the analysis is to study the nested structure of problems with nested structure. The authors show that they converge faster than non-nested problems with SGD-type updates for nested problems with hidden smoothness. They also show that the convergence rate of they is faster than that of non-neighboring algorithms with regularity conditions. Finally, the authors propose a new SGD approach, the ALternating Stochastastic gradient dEscenT (ALSET) method, which is able to solve stochastically nested problems. "
13414,SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"supervised learning USED-FOR transformer - based model. siamese sampling mechanism USED-FOR sparse and similar clips. interdependent knowledge PART-OF network. reasoning strategy USED-FOR interdependent knowledge. interdependent knowledge PART-OF network inference. siamese clips HYPONYM-OF sparse and similar clips. Siamese Sampling and Reasoning ( SiaSamRea ) approach USED-FOR interdependent knowledge. reasoning strategy PART-OF Siamese Sampling and Reasoning ( SiaSamRea ) approach. reasoning strategy USED-FOR network inference. siamese sampling mechanism PART-OF Siamese Sampling and Reasoning ( SiaSamRea ) approach. siamese knowledge reasoning USED-FOR soft label. siamese knowledge reasoning HYPONYM-OF modules. siamese knowledge generation HYPONYM-OF modules. modules PART-OF reasoning strategy. siamese knowledge reasoning PART-OF reasoning strategy. siamese knowledge generation PART-OF reasoning strategy. SiaSamRea USED-FOR multimodal reasoning paradigm. ActivityNet - QA CONJUNCTION How2QA. How2QA CONJUNCTION ActivityNet - QA. How2QA CONJUNCTION TGIF - QA. TGIF - QA CONJUNCTION How2QA. MSVD - QA CONJUNCTION ActivityNet - QA. ActivityNet - QA CONJUNCTION MSVD - QA. MSRVTT - QA CONJUNCTION MSVD - QA. MSVD - QA CONJUNCTION MSRVTT - QA. VideoQA benchmarks EVALUATE-FOR SiaSamRea. Task is VideoQA ) task. OtherScientificTerm are inter - relationship, and soft labels. ","This paper proposes a transformer-based model for supervised learning. The authors propose a reasoning strategy based on Siamese Sampling and Reasoning (SiaSamRea) approach to learn interdependent knowledge in a network. The reasoning strategy consists of three modules: siamese knowledge generation, siameses sampling mechanism for sparse and similar clips, and soft label reasoning. The paper shows the effectiveness of SiaSia on VideoQA benchmarks. ","This paper proposes a transformer-based model for supervised learning. The authors propose a Siamese Sampling and Reasoning (SiaSamRea) approach for learning interdependent knowledge in a network. The reasoning strategy consists of two modules: siamese knowledge generation and siameses sampling mechanism for sparse and similar clips (e.g., siamesed clips). The authors also propose a multimodal reasoning paradigm, where soft labels are learned by siamesE knowledge reasoning for each soft label. Experiments are conducted on VideoQA benchmarks, including MSVD-QA, How2QA and ActivityNet-qA."
13463,SP:160022e2cd61159da92f92e85520b7062a337a8d,"Structured distributions USED-FOR latent probabilistic representations. observed data USED-FOR latent probabilistic representations. computational and memory complexity EVALUATE-FOR latent representations. Hidden Markov Models ( HMMs ) CONJUNCTION Probabilistic Context - Free Grammars ( PCFGs ). Probabilistic Context - Free Grammars ( PCFGs ) CONJUNCTION Hidden Markov Models ( HMMs ). Probabilistic Context - Free Grammars ( PCFGs ) HYPONYM-OF models. Hidden Markov Models ( HMMs ) HYPONYM-OF models. computational and memory complexity EVALUATE-FOR structured models. approach USED-FOR structured models. computational and memory complexity EVALUATE-FOR approach. rank USED-FOR speed. matrix - vector product USED-FOR central inference step. polyphonic music modeling CONJUNCTION unsupervised grammar induction. unsupervised grammar induction CONJUNCTION polyphonic music modeling. language modeling CONJUNCTION polyphonic music modeling. polyphonic music modeling CONJUNCTION language modeling. neural parameterized structured models USED-FOR language modeling. unsupervised grammar induction CONJUNCTION video modeling. video modeling CONJUNCTION unsupervised grammar induction. neural parameterized structured models USED-FOR polyphonic music modeling. accuracy EVALUATE-FOR models. approach COMPARE models. models COMPARE approach. neural parameterized structured models EVALUATE-FOR approach. neural parameterized structured models USED-FOR unsupervised grammar induction. unsupervised grammar induction EVALUATE-FOR approach. large state spaces FEATURE-OF models. accuracy EVALUATE-FOR approach. OtherScientificTerm are combinatorial spaces, hidden states, and low - rank constraint. ","This paper proposes a new approach to learn structured distributions for latent probabilistic representations from observed data. The proposed models are based on Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs). The proposed approach reduces the computational and memory complexity of learning the latent representations of the observed data and the number of hidden states in the combinatorial spaces. The central inference step is based on a matrix-vector product, where the rank of the hidden states is used as the speed. The authors show that the proposed approach achieves better performance than existing structured models on large state spaces and unsupervised grammar induction with neural parameterized structured models. The performance of the proposed models is also shown to be competitive with other models in terms of accuracy. ","This paper proposes a new approach for learning structured distributions for latent probabilistic representations from observed data. The authors propose two models: Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs). The proposed approach reduces the computational and memory complexity of the latent representations, and the speed of the central inference step, which is based on the matrix-vector product of the hidden states.  The authors also propose a low-rank constraint on the number of hidden states and the rank, which allows for faster speed. They show that the proposed approach outperforms existing models on large state spaces, and on unsupervised grammar induction and neural parameterized structured models for language modeling and polyphonic music modeling. "
13512,SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"exploration USED-FOR Reinforcement Learning. Thompson Sampling HYPONYM-OF Bayesian exploration strategies. technique USED-FOR complex environments. computational intractability FEATURE-OF probability distributions. deep neural network models CONJUNCTION approximate posterior methods. approximate posterior methods CONJUNCTION deep neural network models. approximation techniques USED-FOR exploration - exploitation trade - offs. Sample Average Uncertainty ( SAU ) HYPONYM-OF uncertainty measure. uncertainty measure USED-FOR contextual bandits. SAU HYPONYM-OF frequentist approach. Bayesian approaches USED-FOR outcomes uncertainty. Thompson Sampling HYPONYM-OF Bayesian approaches. SAU USED-FOR uncertainty measure. SAU USED-FOR deep contextual bandits. drop - in replacement USED-FOR epsilongreedy exploration. drop - in replacement USED-FOR deep contextual bandits. SAU - based exploration COMPARE deep Bayesian bandit methods. deep Bayesian bandit methods COMPARE SAU - based exploration. modest computation cost EVALUATE-FOR deep Bayesian bandit methods. real - world datasets EVALUATE-FOR deep Bayesian bandit methods. real - world datasets EVALUATE-FOR SAU - based exploration. Task is exploration - exploitation dilemma. OtherScientificTerm are action - value function, value predictions, and regret bounds. Method are outcome models, and outcome model. Metric is complexity. Material is deep bandit scenario. ","This paper studies the exploration-exploration dilemma in Reinforcement Learning. The authors propose Thompson Sampling, a new Bayesian exploration strategies based on the technique from Bayesian learning. The main idea is to use the Sample Average Uncertainty (SAU) as an uncertainty measure for contextual bandits, which is a frequentist approach. The uncertainty measure is based on Bayesian approaches to estimate the outcomes uncertainty, and the authors show that the computational intractability of probability distributions with respect to the action-value function is a major factor in the regret bounds of outcome models. They also show that approximation techniques can be used to improve the exploration -exploration trade-offs. Finally, the authors provide a drop-in replacement for epsilongreedy exploration, and show that SAU-based exploration has a modest computation cost compared to deep Bayesian bandit methods on real-world datasets.","This paper proposes a novel exploration-exploration framework for Reinforcement Learning. The authors propose a new uncertainty measure, Sample Average Uncertainty (SAU), which is based on Thompson Sampling, one of the most popular Bayesian exploration strategies for complex environments. The main idea of SAU is to learn an action-value function that is independent of the value predictions of the outcome models. The paper shows that SAU improves the computational intractability of probability distributions with respect to the complexity of the decision-making process in the deep bandit scenario, and the authors also show that the uncertainty measure can be applied to contextual bandits (e.g., drop-in replacement for epsilongreedy exploration). The authors also provide a frequentist approach to learn outcomes uncertainty, which is similar to Bayesian approaches for learning outcomes uncertainty. They also provide some approximation techniques for learning exploration-expertise trade-offs, and show that their proposed SAU-based exploration has a modest computation cost compared to deep Bayesian bandit methods on real-world datasets."
13561,SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"behavior CONJUNCTION neural activity. neural activity CONJUNCTION behavior. deep learning USED-FOR automated analysis of behavior. computer vision CONJUNCTION deep learning. deep learning CONJUNCTION computer vision. computer vision USED-FOR automated analysis of behavior. Disentangled Behavior Embedding ( DBE ) USED-FOR robust behavioral embeddings. DBE CONJUNCTION stochastic temporal model. stochastic temporal model CONJUNCTION DBE. end - to - end approach USED-FOR discrete behavior representations. models USED-FOR consistent behavior representations. dynamic behavioral factors ( pose ) PART-OF deep autoencoder. temporal structures of pose dynamics USED-FOR models. fine - grained behavioral motif generation CONJUNCTION behavior decoding. behavior decoding CONJUNCTION fine - grained behavioral motif generation. approaches COMPARE DBE. DBE COMPARE approaches. approaches COMPARE VDBE. VDBE COMPARE approaches. DBE CONJUNCTION VDBE. VDBE CONJUNCTION DBE. DBE USED-FOR tasks. tasks EVALUATE-FOR VDBE. tasks EVALUATE-FOR approaches. behavior decoding HYPONYM-OF tasks. fine - grained behavioral motif generation HYPONYM-OF tasks. Material are neuroscience, large and high - quality video datasets, and interpretable behavioral videos. Task is motor task. ","This paper studies the problem of learning robust behavioral embeddings from video. The authors propose Disentangled Behavior Embedding (DBE), a method to learn a set of discrete behavior representations from video by using an end-to-end approach. The main idea is to use dynamic behavioral factors (pose) in a deep autoencoder to encode the dynamic behavior factors (poses) in the video, and then use a stochastic temporal model to predict the pose dynamics of the pose. The proposed method is evaluated on two tasks: fine-grained behavioral motif generation and behavior decoding. The results show that DBE outperforms VDBE on both tasks.","This paper proposes Disentangled Behavior Embedding (DBE), a method for learning robust behavioral embeddings from large and high-quality video datasets. The authors propose an end-to-end approach to learn discrete behavior representations from a motor task. The model is based on dynamic behavioral factors (pose) in a deep autoencoder. The models are trained to generate consistent behavior representations based on temporal structures of pose dynamics. Experiments show that DBE outperforms other approaches on three tasks: fine-grained behavioral motif generation, behavior decoding, and VDBE. "
13610,SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"DMTET HYPONYM-OF deep 3D conditional generative model. user guides USED-FOR DMTET. coarse voxels HYPONYM-OF user guides. hybrid 3D representation USED-FOR implicit and explicit 3D representations. DMTET USED-FOR reconstructed surface. implicit approaches COMPARE DMTET. DMTET COMPARE implicit approaches. deep 3D generative models USED-FOR explicit representations. deep 3D generative models COMPARE model. model COMPARE deep 3D generative models. meshes HYPONYM-OF explicit representations. deformable tetrahedral grid USED-FOR discretized signed distance function. implicit signed distance representation CONJUNCTION explicit surface mesh representation. explicit surface mesh representation CONJUNCTION implicit signed distance representation. discretized signed distance function CONJUNCTION differentiable marching tetrahedra layer. differentiable marching tetrahedra layer CONJUNCTION discretized signed distance function. differentiable marching tetrahedra layer USED-FOR implicit signed distance representation. deformable tetrahedral grid CONJUNCTION differentiable marching tetrahedra layer. differentiable marching tetrahedra layer CONJUNCTION deformable tetrahedral grid. deformable tetrahedral grid PART-OF DMTET. differentiable marching tetrahedra layer PART-OF DMTET. reconstruction CONJUNCTION adversarial losses. adversarial losses CONJUNCTION reconstruction. surface mesh USED-FOR adversarial losses. adversarial losses USED-FOR generation of the hierarchy of subdivisions. reconstruction USED-FOR generation of the hierarchy of subdivisions. approach USED-FOR conditional shape synthesis. coarse voxel inputs USED-FOR conditional shape synthesis. OtherScientificTerm are signed distance values, finer geometric details, arbitrary topology, and hierarchy of subdivisions. Material is complex 3D animal shapes. ","This paper proposes a deep 3D conditional generative model called DMTET, which is a hybrid 3D representation that combines implicit and explicit 3D representations. The user guides are composed of coarse voxels, and the reconstructed surface is represented by DMTTE. The model is shown to outperform existing implicit approaches in terms of reconstruction and adversarial losses. The paper also shows that the model is able to learn complex 3D animal shapes. ","This paper proposes a deep 3D conditional generative model called DMTET, which is a hybrid 3D representation of implicit and explicit 3D representations. The proposed model is evaluated on complex 3D animal shapes. The reconstructed surface of the reconstructed surface is modeled using user guides on coarse voxels, where the signed distance values are used to represent finer geometric details, arbitrary topology, and a differentiable tetrahedral grid is used for the discretized signed distance function and the differentiable marching tetrahedra layer for the implicit signed distance representation and the explicit surface mesh representation. Experiments show that the proposed model outperforms other implicit approaches, and outperforms the state-of-the-art in terms of reconstruction and adversarial losses on the surface mesh. The paper also presents a new approach for conditional shape synthesis using coarse vxel inputs. "
13659,SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"information theory CONJUNCTION statistics. statistics CONJUNCTION information theory. statistics CONJUNCTION machine learning. machine learning CONJUNCTION statistics. statistical dependence FEATURE-OF Mutual information ( MI ). structural properties FEATURE-OF it. sliced MI ( SMI ) USED-FOR surrogate measure of dependence. it USED-FOR structural properties. scalable computation CONJUNCTION estimation. estimation CONJUNCTION scalable computation. structural properties FEATURE-OF MI. estimation EVALUATE-FOR it. scalable computation EVALUATE-FOR it. MI COMPARE SMI. SMI COMPARE MI. deterministic transformations USED-FOR SMI. SMI USED-FOR feature extraction. processing functions of raw data USED-FOR it. independence testing CONJUNCTION feature extraction. feature extraction CONJUNCTION independence testing. MI USED-FOR high - dimensional inference. SMI COMPARE MI. MI COMPARE SMI. SMI USED-FOR high - dimensional inference. independence testing USED-FOR theory. feature extraction USED-FOR theory. Task is estimation of highdimensional MI. OtherScientificTerm are statistical scalability, and one - dimensional random projections. ","This paper studies the statistical dependence of Mutual Information (MI) between two data points. Mutual information (MI), a surrogate measure of dependence between data points, is a well-studied topic in the literature, and it is well-known that it has structural properties such as information theory, statistics, and machine learning. The authors propose a sliced MI (SMI) to measure the statistical scalability of MI. The SMI is based on deterministic transformations, and is trained using processing functions of raw data. The theoretical results show that SMI outperforms MI in terms of both the estimation and the estimation of high-dimensional MI. Moreover, it is shown that it is able to capture structural properties of MI, such as scalable computation, estimation, and estimation. Finally, the authors show that the SMI can be used for feature extraction, independence testing, and feature extraction for theory. ","This paper proposes a surrogate measure of dependence, Mutual information (MI), which is a measure of statistical dependence between two sets of data points. Mutual information is defined as the sum of information theory and statistics in machine learning, and it can be used to measure structural properties of MI, such as statistical scalability. The paper shows that MI is more robust to deterministic transformations than SMI, and that it performs better in terms of scalable computation and estimation compared to SMI.  The paper also shows that SMI performs better than MI for feature extraction and independence testing, which is an important contribution of the theory.  "
13708,SP:e220b348901b476c2afd95f97630fb5400582f40,"query efficiency EVALUATE-FOR myopic methods. non - myopic Bayesian optimization COMPARE myopic methods. myopic methods COMPARE non - myopic Bayesian optimization. expected improvement HYPONYM-OF myopic methods. query efficiency EVALUATE-FOR non - myopic Bayesian optimization. unreliable bruteforce derivative - free optimization USED-FOR Monte Carlo rollout acquisition function. unreliable bruteforce derivative - free optimization USED-FOR multi - step lookahead constrained BO method. sample average approximation CONJUNCTION infinitesimal perturbation analysis. infinitesimal perturbation analysis CONJUNCTION sample average approximation. reparameterization trick USED-FOR Methods. likelihoodratio - based unbiased estimator USED-FOR acquisition function optimization. 2 - OPT - C COMPARE methods. methods COMPARE 2 - OPT - C. query efficiency EVALUATE-FOR methods. query efficiency EVALUATE-FOR 2 - OPT - C. Metric is computational cost. Method is unconstrained BO methods. Material is unconstrained setting. OtherScientificTerm are constraints, sampled acquisition function surface, feasible and infeasible regions, and tight constraints. Task are constrained problems, and sequential and batch settings. ",This paper proposes a new multi-step lookahead constrained BO method based on unreliable bruteforce derivative-free optimization for the Monte Carlo rollout acquisition function. The authors show that the proposed method achieves better query efficiency than myopic methods in terms of expected improvement compared to non-myopic Bayesian optimization. The main contribution of the paper is a reparameterization trick to improve the performance of unconstrained BO methods in the unconstrain setting. The paper also provides a theoretical analysis of the constraints on the sampled acquisition function surface and the number of feasible and infeasible regions.  The authors also provide a likelihoodratio-based unbiased estimator for acquisition function optimization and show that their methods perform better than 2-OPT-C and other methods. ,This paper proposes a multi-step lookahead constrained BO method based on unreliable bruteforce derivative-free optimization for the Monte Carlo rollout acquisition function. The authors show that the proposed method can achieve better query efficiency compared to non-myopic Bayesian optimization with expected improvement. They also show that their method is more computationally efficient compared to unconstrained BO methods. The paper also provides a reparameterization trick to improve the computational cost of the proposed methods. 
13757,SP:51fbd861422647912f275b48861ea3c4812afdc8,scalar value functions PART-OF value network. distributional RL USED-FOR return distribution. return distribution COMPARE scalar value. scalar value COMPARE return distribution. hybrid reward architectures ( HRA ) USED-FOR source - specific value functions. hybrid reward architectures ( HRA ) USED-FOR RL. source - specific value functions USED-FOR reward. distributional RL CONJUNCTION hybrid reward architectures. hybrid reward architectures CONJUNCTION distributional RL. Multi - Dimensional Distributional DQN ( MD3QN ) USED-FOR joint return distribution. distributional RL USED-FOR joint return distribution. distributional RL USED-FOR Multi - Dimensional Distributional DQN ( MD3QN ). MD3QN USED-FOR randomness in returns. MD3QN USED-FOR rich reward correlation. joint return distribution CONJUNCTION Bellman target. Bellman target CONJUNCTION joint return distribution. Maximum Mean Discrepancy FEATURE-OF joint return distribution. Maximum Mean Discrepancy USED-FOR empirical algorithm. method USED-FOR joint return distribution. method COMPARE RL methods. RL methods COMPARE method. multi - dimensional reward functions USED-FOR control setting. richly correlated reward functions FEATURE-OF joint return distribution. control setting EVALUATE-FOR RL methods. multi - dimensional reward functions USED-FOR method. multi - dimensional reward functions USED-FOR RL methods. Method is joint distribution modeling. OtherScientificTerm is joint distributional Bellman operator. ,"This paper studies joint distribution modeling in RL. The authors propose a new distributional RL for the joint return distribution, where the return distribution is computed as a scalar value functions in the value network, and the reward is computed using distributional reinforcement learning (DQN). The authors use hybrid reward architectures (HRA) to learn the source-specific value functions for the reward, which are then used to train a joint distributional Bellman operator. The empirical algorithm is based on Maximum Mean Discrepancy (MMD3QN), which is an extension of the Multi-Dimensional Distributional DQN [1]. The authors show that the proposed method is able to obtain a rich return distribution with richly correlated reward functions in a control setting with multi-dimensional reward functions. ","This paper proposes a new method for joint distribution modeling. The authors propose to use distributional RL and hybrid reward architectures (HRA) to model the return distribution of a value network, where the scalar value functions in the value network are the source-specific value functions of the reward. The joint return distribution is modeled by a Multi-Dimensional Distributional DQN (MD3QN), which is a variant of the joint distributional Bellman operator, and the authors propose an empirical algorithm based on Maximum Mean Discrepancy for the proposed empirical algorithm. The proposed method is shown to outperform existing RL methods with multi-dimensional reward functions in a control setting. The main contribution of the paper is to propose a method for estimating the joint returns distribution with richly correlated reward functions, which is based on MD3Qn for rich reward correlation."
13806,SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"CorticalFlow HYPONYM-OF geometric deep - learning model. diffeomorphic transformations USED-FOR model. numeric conditions USED-FOR manifoldness. discrete resolution USED-FOR topological errors. numeric conditions USED-FOR topological errors. CorticalFlow USED-FOR brain cortical surface reconstruction. its USED-FOR brain cortical surface reconstruction. its EVALUATE-FOR CorticalFlow. computation time EVALUATE-FOR CorticalFlow. CorticalFlow USED-FOR generation of anatomically plausible surfaces. Material is 3 - dimensional image. OtherScientificTerm are template mesh ’s topological properties, and GPU memory footprint. Method are flow Ordinary Differential Equation ( ODE ) framework, and surface reconstruction methods. Task is generation of surfaces. ","This paper proposes a geometric deep-learning model called CorticalFlow, a flow Ordinary Differential Equation (ODE) framework. The model is based on diffeomorphic transformations, where the template mesh’s topological properties are represented by a discrete resolution. Theoretical results show that the manifoldness of the manifold under the numeric conditions of the topological errors can be approximated by the discrete resolution, and that its performance on brain cortical surface reconstruction is comparable to its state-of-the-art counterpart in terms of computation time. The paper also shows that the generation of surfaces from the surface reconstruction methods is computationally efficient. ","This paper proposes a geometric deep-learning model called CorticalFlow, which is based on the flow Ordinary Differential Equation (ODE) framework. The model is built on diffeomorphic transformations, where the topological properties of a 3-dimensional image are represented by a template mesh. The manifoldness of the manifold is modeled by discrete resolution, and topological errors are modeled by numeric conditions on the manifoldness. The authors show that the proposed model is computationally efficient in terms of computation time, and its use in brain cortical surface reconstruction is shown to improve the generation of anatomically plausible surfaces. "
13855,SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"computational cost EVALUATE-FOR models. differential privacy CONJUNCTION max information. max information CONJUNCTION differential privacy. deletion guarantees CONJUNCTION deletion guarantees. deletion guarantees CONJUNCTION deletion guarantees. deletion guarantees FEATURE-OF non - adaptive sequences. deletion guarantees FEATURE-OF adaptive sequences. provable deletion guarantees FEATURE-OF adaptive deletion sequences. attack USED-FOR SISA algorithm. non - convex models USED-FOR adaptive deletion sequences. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. Method is Data deletion algorithms. Task is non - convex setting. OtherScientificTerm are update sequence, non - adaptive deletion sequences, and training methodologies. ","This paper studies the problem of data deletion algorithms in the non-convex setting, where the update sequence is a non-adaptive deletion sequence. The authors study the computational cost of models with differential privacy and max information. They show that adaptive deletion sequences with provable deletion guarantees are more computationally efficient than those with deletion guarantees. They also show that the SISA algorithm can be used as an attack against non-vex models. They then propose two training methodologies: MNIST and Fashion-MNIST.","This paper proposes a novel attack on Data deletion algorithms in the non-convex setting. The authors show that models with differential privacy and max information are more computationally cost efficient than models with non-adaptive deletion sequences. The attack is based on the SISA algorithm, and the authors also show that the adaptive deletion sequences generated by non-convolutional models are provable deletion guarantees, and that the deletion guarantees of non-adversarial sequences are more robust than those of adaptive sequences. Experiments are conducted on MNIST, Fashion-MNIST, CIFAR-10, and MNIST with different training methodologies."
13904,SP:7150006590e268ab732c9be6c9048f67a377f956,epistemic uncertainty CONJUNCTION aleatoric uncertainty. aleatoric uncertainty CONJUNCTION epistemic uncertainty. prior distribution FEATURE-OF MDPs. policy optimising CVaR USED-FOR setting. aleatoric uncertainty CONJUNCTION inherent stochasticity of MDPs. inherent stochasticity of MDPs CONJUNCTION aleatoric uncertainty. prior distribution FEATURE-OF epistemic uncertainty. Monte Carlo tree search CONJUNCTION Bayesian optimisation. Bayesian optimisation CONJUNCTION Monte Carlo tree search. two - player stochastic game USED-FOR problem. Monte Carlo tree search USED-FOR approximate algorithm. Bayesian optimisation USED-FOR approximate algorithm. approach COMPARE baseline approaches. baseline approaches COMPARE approach. baseline approaches USED-FOR problem. approach USED-FOR problem. Task is risk - averse Bayes - adaptive reinforcement learning. OtherScientificTerm is conditional value at risk ( CVaR ). ,"This paper studies the problem of risk-averse Bayes-adaptive reinforcement learning in the setting where the prior distribution of MDPs with epistemic uncertainty and aleatoric uncertainty is unknown. In this setting, the conditional value at risk (CVaR) is used for policy optimising CVaR. The authors consider the problem in a two-player stochastic game and propose an approach to solve the problem using Monte Carlo tree search and Bayesian optimisation. The proposed approach outperforms baseline approaches in this problem.","This paper studies risk-averse Bayes-adaptive reinforcement learning, where the goal is to learn a conditional value at risk (CVaR) that maximizes the epistemic uncertainty and aleatoric uncertainty of MDPs with a prior distribution over the prior distribution. The setting is modeled by policy optimising CVaR. The authors propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation on a two-player stochastic game. The proposed approach outperforms baseline approaches on the problem."
13953,SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"binary classification data USED-FOR shallow ReLU networks. logistic loss USED-FOR shallow ReLU networks. gradient descent USED-FOR logistic loss. gradient descent USED-FOR shallow ReLU networks. sigmoid mapping USED-FOR conditional distribution. gradient descent USED-FOR population risk. early stopping USED-FOR gradient descent. complexity measure EVALUATE-FOR conditional model. local interpolation property FEATURE-OF univariate classifier. Deep networks USED-FOR arbitrary prediction problems. gradient descent USED-FOR Deep networks. constant step size FEATURE-OF vanilla gradient descent. vanilla gradient descent USED-FOR inner ( inputfacing ) weights. shallow ReLU networks HYPONYM-OF networks. induced conditional model COMPARE model. model COMPARE induced conditional model. optimality FEATURE-OF population misclassification rate. sigmoid mapping USED-FOR induced conditional model. gradient descent USED-FOR restricted conditional models. network nodes CONJUNCTION gradient descent iterations. gradient descent iterations CONJUNCTION network nodes. optimal test error FEATURE-OF noisy distributions. optimal test error EVALUATE-FOR univariate predictor. local interpolation property FEATURE-OF univariate predictor. multiplicative error property FEATURE-OF logistic loss. technique USED-FOR large network width. empirical logistic risk CONJUNCTION logistic risk. logistic risk CONJUNCTION empirical logistic risk. logistic loss CONJUNCTION empirical logistic risk. empirical logistic risk CONJUNCTION logistic loss. compactly - supported marginal FEATURE-OF Borel measure. gradient descent USED-FOR optimal test error. it USED-FOR learning task. universal approximation properties FEATURE-OF neural networks. Bayes ( convex ) risk USED-FOR conditional model. agnostic learning setting HYPONYM-OF predictors. shallow ReLU networks USED-FOR predictors. gradient descent USED-FOR shallow ReLU networks. OtherScientificTerm are data distribution, joint distribution, training time, measurable functions, training risk, data simplicity, distribution, finite sample, sphere, function f, R, and computational and statistical obstructions. Metric are Bayes risk, logistic and misclassification losses, calibration, Bayes risk R, and misclassification loss. Method are stalwart methods, infinite - width random feature model, classification calibration, and universal","This paper studies the problem of binary classification data for shallow ReLU networks. Deep networks are used to solve arbitrary prediction problems where the data distribution, joint distribution, and the training time are unknown. The authors propose a new method for estimating the logistic and misclassification losses. The proposed method is based on gradient descent with early stopping.   The authors show that gradient descent can be used to estimate the population risk of a population misclassifying rate, which is a measure of the optimality of a given population. The paper also shows that the proposed method has a compactly-supported marginal. ","This paper presents a new method for estimating the population misclassification rate of deep neural networks. The main idea is to use a stochastic gradient descent method to estimate the population risk. The authors show that the proposed method is more robust to misclassifications than existing stalwart methods, and that it can be applied to any learning task where the data distribution is not known. They also show that their method can be used in the agnostic learning setting. "
14002,SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"misinformation campaigns USED-FOR social outcomes. misinformation campaigns USED-FOR coordinated accounts. social media FEATURE-OF coordinated accounts. coordinated group detection USED-FOR misinformation. methodology USED-FOR misinformation. methodology USED-FOR coordinated group detection. social media USED-FOR misinformation. social media FEATURE-OF sparsity of account activities. limited expressive power EVALUATE-FOR detectors. prior knowledge USED-FOR detectors. temporal logic CONJUNCTION pre - defined filtering functions. pre - defined filtering functions CONJUNCTION temporal logic. prior knowledge FEATURE-OF neural temporal point process. neural temporal point process PART-OF coordination detection framework. pre - defined filtering functions HYPONYM-OF prior knowledge. temporal logic HYPONYM-OF prior knowledge. account embedding space CONJUNCTION prior knowledge. prior knowledge CONJUNCTION account embedding space. theoretically guaranteed variational inference approach USED-FOR mean - field approximation. mean - field approximation USED-FOR it. theoretically guaranteed variational inference approach USED-FOR it. real - world dataset EVALUATE-FOR method. method COMPARE model. model COMPARE method. real - world dataset EVALUATE-FOR model. unsupervised and semi - supervised settings EVALUATE-FOR model. unsupervised and semi - supervised settings EVALUATE-FOR method. COVID-19 Vaccine Tweets dataset EVALUATE-FOR model. COVID-19 vaccines FEATURE-OF spreading misinformation. Method is deep learning based coordination detectors. Generic are they, and distribution. OtherScientificTerm is Gibbs distribution of group assignment. Task is detection. ","This paper proposes a new methodology for coordinated group detection for misinformation in social media. The proposed method is based on deep learning based coordination detectors. The detectors are based on prior knowledge from the temporal logic, pre-defined filtering functions, and the neural temporal point process in the coordination detection framework. The authors show that the detectors have limited expressive power due to the sparsity of account activities on social media in the presence of misinformation campaigns on coordinated accounts. The method is evaluated on the COVID-19 Vaccine Tweets dataset and on a real-world dataset and shows that it performs better than a theoretically guaranteed variational inference approach in terms of mean-field approximation. ","This paper proposes a methodology for coordinated group detection for misinformation in social media. The authors propose a new method for detecting misinformation in the context of social media, where the sparsity of account activities on social media can lead to social outcomes. The proposed method is based on deep learning based coordination detectors. The detectors are trained with limited expressive power, which is achieved by using prior knowledge in the form of temporal logic, pre-defined filtering functions, and prior knowledge of the neural temporal point process in the coordination detection framework. The method is evaluated on the COVID-19 Vaccine Tweets dataset and on a real-world dataset, where it outperforms the proposed model in both unsupervised and semi-supervised settings. The paper also proposes a theoretically guaranteed variational inference approach for the mean-field approximation of the detectors. "
14051,SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"binary classification task HYPONYM-OF model problem. unit sphere FEATURE-OF smooth curves. structure USED-FOR model problem. deep fully - connected neural network USED-FOR binary classification task. network depth COMPARE geometric properties. geometric properties COMPARE network depth. generalization guarantee EVALUATE-FOR deep networks. nonlinear data USED-FOR deep networks. fitting resource USED-FOR classification problem. network depth HYPONYM-OF fitting resource. neural tangent kernel ( NTK ) regime FEATURE-OF reduction to dynamics. convergence CONJUNCTION generalization. generalization CONJUNCTION convergence. decay properties FEATURE-OF NTK. fine - grained control USED-FOR decay properties. fine - grained control USED-FOR NTK. manifolds FEATURE-OF translationally invariant operator. smooth functions USED-FOR NTK. translationally invariant operator USED-FOR NTK. OtherScientificTerm are low - dimensional nonlinear structure, mild regularity conditions, network width, intrinsic data properties, and network. Task is engineering and scientific problems. Method is randomly - initialized gradient descent. ","This paper studies the binary classification task, a model problem where the structure of the model problem is a low-dimensional nonlinear structure. The authors consider a deep fully-connected neural network trained on nonlinear data, where the network width is bounded by a unit sphere, and the network depth is a function of the number of nodes in the unit sphere. They show that the generalization guarantee of deep networks trained on this nonlinear dataset is a result of a fitting resource called network depth, which can be applied to any classification problem with smooth curves. They also show that under mild regularity conditions, the reduction to dynamics in the neural tangent kernel (NTK) regime can be approximated by a translationally invariant operator on manifolds. The convergence and generalization properties of NTK are obtained by fine-grained control on the decay properties of the NTK. ","This paper studies the problem of binary classification task with a deep fully-connected neural network, which is a model problem with a low-dimensional nonlinear structure with mild regularity conditions. The authors propose a fitting resource for the classification problem, where the structure of the model problem is defined as a unit sphere. They show that the network depth and geometric properties of the unit sphere are equivalent to the network width, and that the generalization guarantee of deep networks on nonlinear data can be obtained by randomly-initialized gradient descent. They also show that under the neural tangent kernel (NTK) regime, the reduction to dynamics of the NTK is equivalent to convergence and generalization under the fine-grained control of the decay properties of NTK under the smooth functions of the network. Finally, the authors provide a translationally invariant operator for NTK on manifolds. "
14100,SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"class information PART-OF GAN. auxiliary classifier GAN HYPONYM-OF cGANs. softmax cross - entropy loss ( ACGAN ) FEATURE-OF auxiliary classifier GAN. relational information FEATURE-OF class - labeled dataset. Tiny - ImageNet CONJUNCTION CUB200. CUB200 CONJUNCTION Tiny - ImageNet. CIFAR10 CONJUNCTION Tiny - ImageNet. Tiny - ImageNet CONJUNCTION CIFAR10. CUB200 CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION CUB200. Tiny - ImageNet EVALUATE-FOR ReACGAN. CUB200 EVALUATE-FOR ReACGAN. CIFAR10 EVALUATE-FOR ReACGAN. ImageNet datasets EVALUATE-FOR ReACGAN. D2D - CE CONJUNCTION StyleGAN2 architecture. StyleGAN2 architecture CONJUNCTION D2D - CE. differentiable augmentations USED-FOR ReACGAN. software package USED-FOR representative cGANs. Model weights CONJUNCTION software package. software package CONJUNCTION Model weights. Method are Conditional Generative Adversarial Networks ( cGAN ), ACGAN, and classifier. OtherScientificTerm are diversity, and unit hypersphere. ","This paper studies Conditional Generative Adversarial Networks (cGAN), a family of cGANs with auxiliary classifier GANs with softmax cross-entropy loss (ACGAN). The main idea is to use class information in the GAN to improve the diversity of the class-labeled dataset. The relational information of a class-labelled dataset is then used to train the classifier. The authors show that ReACGAN on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets with differentiable augmentations can achieve state-of-the-art performance. Model weights and the software package can be used to learn representative cGAN. The proposed D2D-CE and StyleGAN2 architecture are also shown to perform well. ","This paper proposes Conditional Generative Adversarial Networks (cGAN), which is an extension of the auxiliary classifier GAN of cGANs with softmax cross-entropy loss (ACGAN) to the class information in GAN. The main idea of the ACGAN is that the diversity of a class-labeled dataset can be reduced to a unit hypersphere, and the relational information can be used to improve the performance of the classifier. ReACGAN is evaluated on three ImageNet datasets (CIFAR10, CUB200, and Tiny-ImageNet) and two ImageNet architectures (D2D-CE and StyleGAN2 architecture) with differentiable augmentations. Model weights and the software package are used to train representative cGAN."
14149,SP:080e80746a87228b156408ff649ab7a17f44e92d,Policy Space Response Oracles ( PSRO ) HYPONYM-OF reinforcement learning ( RL ) algorithm. reinforcement learning ( RL ) algorithm USED-FOR two - player zero - sum games. large games FEATURE-OF approximate Nash equilibria. PSRO USED-FOR continuous actions. PSRO USED-FOR approximate Nash equilibrium. Extensive - Form Double Oracle ( XDO ) HYPONYM-OF extensive - form double oracle algorithm. extensive - form double oracle algorithm USED-FOR two - player zero - sum games. PSRO COMPARE XDO. XDO COMPARE PSRO. best responses USED-FOR XDO. deep RL USED-FOR Neural XDO ( NXDO ). deep RL USED-FOR best response. XDO COMPARE PSRO. PSRO COMPARE XDO. XDO USED-FOR approximate Nash equilibrium. XDO COMPARE CFR. CFR COMPARE XDO. Leduc poker game CONJUNCTION Oshi - Zumo. Oshi - Zumo CONJUNCTION Leduc poker game. exploitability EVALUATE-FOR CFR. exploitability EVALUATE-FOR XDO. NXDO COMPARE PSRO. PSRO COMPARE NXDO. NXDO COMPARE NFSP. NFSP COMPARE NXDO. PSRO CONJUNCTION NFSP. NFSP CONJUNCTION PSRO. sequential multidimensional continuous - action game EVALUATE-FOR NFSP. sequential multidimensional continuous - action game EVALUATE-FOR PSRO. sequential multidimensional continuous - action game EVALUATE-FOR NXDO. NXDO HYPONYM-OF deep RL method. deep RL method USED-FOR approximate Nash equilibrium. NXDO USED-FOR approximate Nash equilibrium. high - dimensional continuous - action sequential games FEATURE-OF approximate Nash equilibrium. OtherScientificTerm is infostates. Material is Leduc poker. ,"This paper proposes a new reinforcement learning (RL) algorithm called Policy Space Response Oracles (PSRO) for two-player zero-sum games. PSRO is based on the extensive-form double oracle algorithm, Extensive-Form Double Oracle (XDO), which is a deep RL method for approximate Nash equilibrium in large games with infostates. The authors show that PSRO can achieve approximate Nash equilibria in high-dimensional continuous-action sequential games with large games. They also show that the best responses obtained by XDO are better than the best response obtained by Neural XDO (NXDO) in deep RL. Finally, they compare PSRO with NFSP and NFSP in a sequential multidimensional continuous-actions game and show that XDO has better exploitability than PSRO and CFR. ","Policy Space Response Oracles (PSRO) is a reinforcement learning (RL) algorithm for two-player zero-sum games. PSRO is an extension of the extensive-form double oracle algorithm, Extensive-Form Double Oracle (XDO) which is used in the original RL algorithm. The main difference between PSRO and XDO is that PSRO can be applied to continuous actions, while XDO can be used for continuous actions. The authors show that XDO achieves approximate Nash equilibria in large games, while PSRO does not. They also show that the best responses of XDO are better than those of PSRO in terms of exploitability.  The authors propose Neural XDO (NXDO), a deep RL method for approximate Nash equilibrium in high-dimensional continuous-action sequential games. They evaluate XDO on the Leduc poker game, Oshi-Zumo, and a sequential multidimensional sequential-action game. They compare XDO with PSRO, NFSP, and NFSP."
14198,SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"graph structured data USED-FOR deep neural networks. node - level unsupervised learning HYPONYM-OF nodeor graph - level supervised learning. node clustering HYPONYM-OF node - level unsupervised learning. representation complexity EVALUATE-FOR graphs. adjacency matrices USED-FOR graphs. permutation - invariant variational autoencoder USED-FOR graph structured data. model USED-FOR node order. model USED-FOR graph reconstruction. extracted representations USED-FOR downstream graph - level classification and regression. Method are graph - level unsupervised representation learning, and graph matching. ","This paper studies the problem of graph structured data in deep neural networks. The authors propose a new graph-level unsupervised representation learning method, called node clustering, which uses a permutation-invariant variational autoencoder to learn graphs with adjacency matrices. The proposed model is able to learn the node order of a graph, which can then be used to train a model for graph reconstruction. The extracted representations are then used for downstream graphs-level classification and regression. ",This paper proposes a new method for graph-level unsupervised representation learning. The main idea is to use graph structured data as input for deep neural networks. The authors propose a permutation-invariant variational autoencoder to generate graphs with adjacency matrices. The model is trained to predict the node order and the node clustering. The proposed model is then used for graph reconstruction and graph matching. The extracted representations are used for downstream graph -level classification and regression. The paper shows that the proposed method is able to reduce the representation complexity of graphs.
14247,SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"limited scalability EVALUATE-FOR Graph Neural Networks ( GNNs ). subgraph USED-FOR GNN. bounded - size scope FEATURE-OF localized subgraph. critical neighbors PART-OF subgraph. GNN USED-FOR informative representation. GNN USED-FOR local neighborhood. function approximation ( GraphSAGE ) CONJUNCTION topological learning ( GIN ). topological learning ( GIN ) CONJUNCTION function approximation ( GraphSAGE ). graph signal processing ( GCN ) CONJUNCTION function approximation ( GraphSAGE ). function approximation ( GraphSAGE ) CONJUNCTION graph signal processing ( GCN ). decoupling USED-FOR GNN expressive power. graphs CONJUNCTION backbone GNN architectures. backbone GNN architectures CONJUNCTION graphs. backbone GNN architectures EVALUATE-FOR design. graphs EVALUATE-FOR design. OtherScientificTerm are graph and model sizes, model depth, receptive field, degraded expressivity, oversmoothing, neighborhood explosion, node, and global graph. Task is expensive computation. Method is GNNs. ","This paper studies the limited scalability of Graph Neural Networks (GNNs) with limited graph and model sizes. The authors show that the subgraph of a GNN with bounded-size scope can be represented as a subgraph with critical neighbors in the receptive field of the GNN. The GNN can be used to learn an informative representation of the local neighborhood of a node, and the critical neighbors are then used to represent the global neighborhood of the node.  The authors also show that GNNs with limited expressivity can suffer from oversmoothing due to the large number of critical neighbors, which leads to expensive computation. To address this issue, the authors propose to use function approximation (GraphSAGE) and topological learning (GIN) as well as graph signal processing (GCN) to improve the expressive power of GNN by decoupling the graph and the global graph. The design of the proposed design is evaluated on graphs and backbone GNN architectures.","This paper studies the limited scalability of Graph Neural Networks (GNNs) in the context of graph and model sizes. The authors show that the subgraph of a GNN with bounded-size scope can be represented as a subgraph with critical neighbors in the receptive field of the GNN. The subgraph can be decomposed into a local neighborhood and a global neighborhood. The GNN can be used to learn an informative representation of the local neighborhood, and the global neighborhood can be computed by GNNs. The paper also shows that the local subgraph has bounded-sized scope, and that the global subgraph does not have bounded-sizes.  The authors also show that in the case of expensive computation, oversmoothing can lead to degraded expressivity due to the neighborhood explosion, and propose to use decoupling to improve GNN expressive power. The proposed design is evaluated on graphs and backbone GNN architectures. "
14296,SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"Normalizing flows HYPONYM-OF latent - variable generative models. tractable likelihood FEATURE-OF latent - variable generative models. Jacobian FEATURE-OF latent - to - observable - variable transformation. linear time FEATURE-OF likelihood. nearly - singular Jacobian FEATURE-OF networks. affine couplings USED-FOR regular distributions. well - conditioned affine - coupling flows USED-FOR log - concave distribution. underdamped Langevin dynamics CONJUNCTION Hénon maps. Hénon maps CONJUNCTION underdamped Langevin dynamics. Hénon maps HYPONYM-OF structured dynamical system. underdamped Langevin dynamics HYPONYM-OF stochastic differential equation. affine coupling architectures CONJUNCTION underdamped Langevin dynamics. underdamped Langevin dynamics CONJUNCTION affine coupling architectures. symplectic diffeomorphisms FEATURE-OF structured dynamical system. iid Gaussians USED-FOR padded version of the input distribution. Gaussian padding USED-FOR normalizing flows. Method is Affine - coupling models. Generic is architecture. OtherScientificTerm are representational power, ill - conditioned Jacobians, well - conditioned affine coupling flows, and Gibbs measures. Task are universal approximation, likelihood - based training, and training affine couplings. ","This paper studies the problem of normalizing flows in latent-variable generative models with tractable likelihood in linear time. Affine-coupling models are known to have representational power, but they are often ill-conditioned Jacobians. The authors propose a new architecture, called Normalizing flows, where the Jacobian of the latent-to-observable-variable transformation is replaced by a nearly-singular Jacobian, and the network is trained with affine couplings. The main idea is to train the affine coupling flows on the log-concave distribution of the input distribution, and then use the well-conditional affine-cooupling flows to normalize the regular distributions.  The authors show that underdamped Langevin dynamics and Hénon maps of a structured dynamical system with symplectic diffeomorphisms can be approximated by affine coupled architectures, and that the iid Gaussians can be used to approximate a padded version of the output distribution.   ","This paper studies the tractable likelihood of latent-variable generative models such as Normalizing flows and Affine-coupling models. The authors propose a new architecture that is based on the Jacobian of the latent-to-observable-variable transformation with linear time. The Jacobian is a nearly-singular Jacobian in networks with representational power, and the authors show that affine couplings can be used to approximate regular distributions of the log-concave distribution with well-conditioned affine-Coupling flows. They also show that underdamped Langevin dynamics and Hénon maps with symplectic diffeomorphisms of the structured dynamical system (e.g., underdamping Langevin Dynamics with stochastic differential equation) can be approximated with affine coupling architectures and underdamps. The main contribution of the paper is to provide a universal approximation of the likelihood-based training, and to provide Gibbs measures for the distribution of the input distribution with Gaussian padding. "
14345,SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"Lagrangian problem USED-FOR coupons allocation. method USED-FOR coupons allocation policy. λ - generalization method USED-FOR policy learning process. λ USED-FOR policy learning process. offline reinforcement learning method CONJUNCTION off - policy evaluation algorithm. off - policy evaluation algorithm CONJUNCTION offline reinforcement learning method. policy learning CONJUNCTION policy evaluation. policy evaluation CONJUNCTION policy learning. offline reinforcement learning method USED-FOR policy learning. off - policy evaluation algorithm USED-FOR policy learning. off - policy evaluation algorithm USED-FOR policy evaluation. offline reinforcement learning method USED-FOR policy evaluation. simulation platform CONJUNCTION real - world e - commerce market. real - world e - commerce market CONJUNCTION simulation platform. real - world e - commerce market EVALUATE-FOR approach. simulation platform EVALUATE-FOR approach. Task are Coupons allocation, and online e - commerce environment. Material are e - commerce market, and e - commerce platform. OtherScientificTerm are coupons, Lagrangian multiplier variable λ, and policy space. Method are coupons allocation policy learning, and λ - generalization ( BCORLE(λ ) ) framework. Metric are computation overhead, and users ’ retention rate. Generic are policy, and problem. ","This paper studies the problem of coupons allocation policy learning in an online e-commerce environment. The authors propose a method to learn a coupon allocation policy by solving a Lagrangian problem in the form of a Lagrangian multiplier variable. The proposed method is based on the BCORLE(λ) framework, where the goal is to find a policy that maximizes the user’s “retention rate” in the policy space. The paper proposes an offline reinforcement learning method, an off-policy evaluation algorithm, and a policy learning process based on λ-generalization method. The approach is evaluated on a simulation platform and a real-world e -commerce market.","The paper proposes a method for learning a coupons allocation policy. The idea is to learn a Lagrangian problem for coupons allocation, where the goal is to maximize the computation overhead. The paper proposes to solve the problem using the λ-generalization (BCORLE(λ) ) framework, which is an offline reinforcement learning method and an off-policy evaluation algorithm for policy learning and policy evaluation. The proposed approach is evaluated on a simulation platform and a real-world e-commerce market. "
14394,SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"local affinity FEATURE-OF label consistency. local affinity USED-FOR intrinsic structure. self regularization loss USED-FOR noisy neighbors. inherent structure USED-FOR domain adaptation. local neighbors CONJUNCTION reciprocal neighbors. reciprocal neighbors CONJUNCTION local neighbors. reciprocal neighbors CONJUNCTION expanded neighborhood. expanded neighborhood CONJUNCTION reciprocal neighbors. reciprocal neighbors USED-FOR local structure. local neighbors USED-FOR local structure. Task is Domain adaptation ( DA ). Method are DA methods, source pretrained model, and source domain classifier. Generic is method. OtherScientificTerm are affinity, and expanded neighborhoods. Material is 2D image and 3D point cloud recognition datasets. ","This paper studies domain adaptation (DA) in the context of domain generalization. The authors propose a new method called Domain Domain Adaptation (DDA), which is based on the notion of local affinity between the source and target domains. The idea is to learn the intrinsic structure of the source domain and the target domain by using a self regularization loss on the noisy neighbors. The proposed method is evaluated on 2D image and 3D point cloud recognition datasets. The results show that the proposed method outperforms existing DA methods.","This paper proposes a novel method for Domain adaptation (DA) that uses local affinity to improve label consistency. The idea is to learn the intrinsic structure of the source domain using local affinity, and then use DA methods to train a source pretrained model. The authors propose a self regularization loss to prevent noisy neighbors from being added to the original source domain classifier. The proposed method is evaluated on 2D image and 3D point cloud recognition datasets. The paper shows that the inherent structure of domain adaptation can be learned using local neighbors, reciprocal neighbors, and an expanded neighborhood. "
14443,SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,graph learning CONJUNCTION image / video recognition. image / video recognition CONJUNCTION graph learning. image / video recognition CONJUNCTION object detection. object detection CONJUNCTION image / video recognition. point cloud processing CONJUNCTION graph learning. graph learning CONJUNCTION point cloud processing. Learning representations from sets USED-FOR point cloud processing. point cloud processing CONJUNCTION image / video recognition. image / video recognition CONJUNCTION point cloud processing. geometrically - interpretable and generic pooling mechanism USED-FOR fixed - dimensional representation. geometrically - interpretable and generic pooling mechanism USED-FOR features. end - to - end trainable Euclidean embedding USED-FOR sliced - Wasserstein distance. end - to - end trainable Euclidean embedding USED-FOR set - structured data. method COMPARE set representation learning approaches. set representation learning approaches COMPARE method. pooling method COMPARE method. method COMPARE pooling method. point - cloud HYPONYM-OF set - structured data. set - structured data EVALUATE-FOR pooling method. OtherScientificTerm is probability distribution. ,"This paper proposes a new pooling method for learning representations from sets. The proposed method is based on the end-to-end trainable Euclidean embedding for sliced-Wasserstein distance between two sets of data points. The key idea is to use a geometrically-interpretable and generic pooling mechanism to learn a fixed-dimensional representation of the data points, which is then used to learn the features of the set. The authors show that the proposed method outperforms existing set representation learning approaches on set-structured data such as point-cloud, graph learning, and image/video recognition.","The paper proposes a new method for learning representations from sets for point cloud processing, graph learning, and image/video recognition. The key idea is to use a geometrically-interpretable and generic pooling mechanism to learn a fixed-dimensional representation of the features. The proposed method is evaluated on set-structured data (e.g., point-cloud) and on a set-structure data with end-to-end trainable Euclidean embedding for sliced-Wasserstein distance. The paper shows that the proposed method outperforms set representation learning approaches."
14492,SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"training stability FEATURE-OF recurrent neural networks ( RNNs ). SBO - RNN HYPONYM-OF RNNs. stochastic bilevel optimization ( SBO ) USED-FOR RNNs. feedforward and backpropagation USED-FOR lower and upper - level optimization. stochastic gradient descent ( SGD ) USED-FOR SBO problem. stochastic gradient descent ( SGD ) USED-FOR RNN. RNN USED-FOR SBO problem. benchmark datasets EVALUATE-FOR approach. OtherScientificTerm are hidden states, hyperparameters, and vanishing or exploding gradient. Material is training data. ",This paper studies the problem of training stability of recurrent neural networks (RNNs) with SBO-RNN. The authors propose a stochastic bilevel optimization (SBO) for RNNs with feedforward and backpropagation. The SBO problem is formulated as a simple optimization problem with hidden states and hyperparameters. The main contribution of the paper is to propose a new RNN that can solve the RNN problem with stochedastic gradient descent (SGD) in an SBO setting. The proposed approach is evaluated on several benchmark datasets.,"This paper studies the problem of training stability of recurrent neural networks (RNNs) with hidden states. The authors propose an extension of stochastic bilevel optimization (SBO) to RNNs called SBO-RNN, which is a variant of SBO. The main idea is to use feedforward and backpropagation for both lower and upper-level optimization. The proposed approach is evaluated on several benchmark datasets and shows promising results. "
14541,SP:d3a4300e21ca215334f256f0467a428470548fe4,"online problem USED-FOR minimizing power consumption. algorithm USED-FOR power - saving states. energy consumption CONJUNCTION wake - up costs. wake - up costs CONJUNCTION energy consumption. wake - up costs FEATURE-OF power - saving states. energy consumption FEATURE-OF power - saving states. predicted lengths of the idle periods USED-FOR learning - augmented online algorithm. worst - case guarantee EVALUATE-FOR algorithm. algorithm USED-FOR online ski rental problem. learning augmented setting USED-FOR online ski rental problem. OtherScientificTerm is prediction error. Generic are problem, and approach. ",This paper studies the online problem of minimizing power consumption in the online ski rental problem. The authors propose a new algorithm for minimizing power-saving states with energy consumption and wake-up costs. The algorithm is based on a learning augmented online algorithm with predicted lengths of the idle periods. The paper shows that the proposed algorithm has the worst-case guarantee of $O(\sqrt{T})$ with respect to the prediction error. The proposed algorithm is shown to be efficient in the learning augmented setting. ,"This paper studies the online problem of minimizing power consumption. The authors propose an algorithm for minimizing power-saving states with respect to energy consumption and wake-up costs. The problem is formulated as an online ski rental problem in a learning augmented setting, where the goal is to minimize the prediction error. The proposed learning-augmented online algorithm is based on the predicted lengths of the idle periods. The algorithm is shown to have the worst-case guarantee, and the proposed approach is evaluated on a variety of environments."
14590,SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,sample sizes FEATURE-OF tasks. task similarities CONJUNCTION sample complexity. sample complexity CONJUNCTION task similarities. mathematical framework USED-FOR transferability. sample complexity EVALUATE-FOR learning models. transferability FEATURE-OF multi - source transfer learning problems. optimal combining coefficients USED-FOR transferability. models USED-FOR tasks. models USED-FOR task. model complexity CONJUNCTION similarities. similarities CONJUNCTION model complexity. sample sizes CONJUNCTION model complexity. model complexity CONJUNCTION sample sizes. analytical expression USED-FOR transferability measure. sample sizes FEATURE-OF analytical expression. model complexity FEATURE-OF analytical expression. sample sizes FEATURE-OF transferability measure. model complexity FEATURE-OF transferability measure. analyses USED-FOR practical learning tasks. parameterized model USED-FOR quantifiable transferability measure. deep neural networks USED-FOR multi - source transfer learning tasks. alternating iterative algorithm USED-FOR deep neural networks. approach COMPARE transfer learning algorithms. transfer learning algorithms COMPARE approach. image classification tasks EVALUATE-FOR approach. image classification tasks EVALUATE-FOR transfer learning algorithms. transfer learning algorithms USED-FOR multi - source and few - shot scenarios. multi - source and few - shot scenarios EVALUATE-FOR approach. Method is transfer learning algorithm designs. Task is knowledge transferring mechanism. ,This paper studies the transferability of multi-source transfer learning problems with large sample sizes. The authors propose a mathematical framework to measure transferability between different learning models with different sample sizes and task similarities. They show that the optimal combining coefficients for transferability can be obtained by optimizing the model complexity and the sample sizes of different models for different tasks. They also provide a quantifiable transferability measure based on a parameterized model. The proposed approach is evaluated on image classification tasks and transfer learning algorithms in both multi- source and few-shot scenarios.,This paper proposes a mathematical framework for quantifying the transferability of learning models on different tasks with different sample sizes and different task similarities. The authors propose an alternating iterative algorithm for learning deep neural networks for multi-source transfer learning tasks. They show that the quantifiable transferability measure is based on the analytical expression of the model complexity and sample sizes. They also show that optimal combining coefficients can be used to measure transferability. The proposed approach is evaluated on several image classification tasks and compared to other transfer learning algorithms in both multi- source and few-shot scenarios. 
14639,SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"asymmetry FEATURE-OF search tasks. search image USED-FOR computational model. eccentricity - dependent visual recognition CONJUNCTION target - dependent top - down cues. target - dependent top - down cues CONJUNCTION eccentricity - dependent visual recognition. eccentricity - dependent visual recognition USED-FOR model. model COMPARE human behavior. human behavior COMPARE model. human behavior USED-FOR paradigmatic search tasks. asymmetry FEATURE-OF paradigmatic search tasks. paradigmatic search tasks EVALUATE-FOR model. model USED-FOR search asymmetry. ImageNet USED-FOR model. developmental diet USED-FOR model. classical perceptual properties FEATURE-OF neural network models. Task are Visual search, and visual search. OtherScientificTerm are eye movements, polarity of search asymmetry, and VisualSearchAsymmetry. Method is task - specific training. Material is natural images. ","This paper studies the problem of visual search in the presence of asymmetry in the search image. The authors propose a new computational model based on a search image that can be used to train a computational model on a given search image, which is then used to learn a task-specific training. The model is trained on ImageNet, where it is trained with eccentricity-dependent visual recognition and target-dependent top-down cues. The proposed model is evaluated on a variety of paradigmatic search tasks with asymmetry, and it is shown that the model is able to learn search asymmetry better than human behavior. Visual search asymptotically outperforms other neural network models in terms of classical perceptual properties. ","This paper studies the problem of learning a search image from a computational model. The authors propose VisualSearchAsymmetry, an approach to learn a search model from an image. The model is based on a combination of eccentricity-dependent visual recognition and target-dependent top-down cues. The proposed model is trained on ImageNet with a developmental diet, and is evaluated on a set of paradigmatic search tasks with different asymmetry of search tasks. The experiments show that the model is able to learn search asymmetry with better performance than human behavior. "
14688,SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"adversarial examples USED-FOR certifiably robust models. tightness of the upper bound USED-FOR certifiably robust models. Interval Bound Propagation ( IBP ) training COMPARE models. models COMPARE Interval Bound Propagation ( IBP ) training. looser bounds USED-FOR Interval Bound Propagation ( IBP ) training. tighter bounds USED-FOR models. loss landscapes FEATURE-OF linear relaxation - based methods. tightness CONJUNCTION smoothness. smoothness CONJUNCTION tightness. tightness USED-FOR method. smoothness USED-FOR method. Method are Certifiable training, certifiable training, and certifiable training method. OtherScientificTerm are worst - case loss, and loss landscape. Generic is state - of - the - arts method. ",This paper studies the problem of certifiable training in the presence of adversarial examples. The authors show that the tightness of the upper bound for certifiably robust models is tighter than that of the best-case loss. They also show that models trained with looser bounds for Interval Bound Propagation (IBP) training are more robust than models trained on the same loss landscapes. They then propose a state-of-the-arts method that combines tightness and smoothness to improve the performance of the proposed method. ,This paper studies the problem of certifiably robust models in the presence of adversarial examples. The authors propose a state-of-the-arts method that uses the tightness of the upper bound on the worst-case loss of the certifiable training. They show that models trained with these tighter bounds outperform models trained using looser bounds in Interval Bound Propagation (IBP) training. The proposed method is based on tightness and smoothness. They also show that linear relaxation-based methods with loss landscapes with tighter bounds are more robust to adversarial attacks. 
14737,SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"stochastic setting FEATURE-OF online linear regression. online ridge regression CONJUNCTION forward algorithm. forward algorithm CONJUNCTION online ridge regression. high probability regret bounds USED-FOR online ridge regression. high probability regret bounds USED-FOR forward algorithm. robustness FEATURE-OF regularization parameter. ridge USED-FOR forward algorithm. it PART-OF algorithms. linear function approximation PART-OF algorithms. it USED-FOR regret bounds. modification USED-FOR linear bandit settings. Method is online regression algorithms. OtherScientificTerm are bounded observations, boundedness assumption, and theoretical bounds. ","This paper studies the problem of online linear regression in the stochastic setting with bounded observations. The authors consider the online regression algorithms in the online ridge regression and forward algorithm with high probability regret bounds. They show that the robustness of the regularization parameter of the forward algorithm is a function of the ridge, and that it can be used to improve the regret bounds of the algorithms with linear function approximation. They also show that this modification can be applied to the linear bandit settings. ","This paper studies the stochastic setting of online linear regression in the context of online regression algorithms in the setting of bounded observations. The authors propose two algorithms: online ridge regression and a forward algorithm based on high probability regret bounds. The forward algorithm is based on a regularization parameter that ensures robustness to bounded observations, while the online ridge is based upon a boundedness assumption. The regret bounds are based on the linear function approximation of the algorithms, and it is shown that it is equivalent to the regret bounds in the linear bandit settings. "
14786,SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"generative adversarial network CONJUNCTION adversarial training. adversarial training CONJUNCTION generative adversarial network. method USED-FOR setting. nonconvex - nonconcave setting FEATURE-OF minimax problems. adversarial training HYPONYM-OF minimax problems. generative adversarial network HYPONYM-OF minimax problems. two - time - scale variant PART-OF EG. slowO(1 / k ) rate FEATURE-OF squared gradient norm. smooth structured nonconvexnonconcave setting FEATURE-OF two - time - scale variant. EG+ HYPONYM-OF EG. EG+ HYPONYM-OF two - time - scale variant. slowO(1 / k ) rate EVALUATE-FOR two - time - scale variant. O(1 / k ) rate FEATURE-OF squared gradient norm. anchoring technique USED-FOR EG. EG+ CONJUNCTION EAG. EAG CONJUNCTION EG+. fast O(1 / k ) rate FEATURE-OF squared gradient norm. squared gradient norm FEATURE-OF smooth structured nonconvex - nonconcave problems. EG+ USED-FOR two - time - scale EG. negative comonotonicity condition FEATURE-OF saddle - gradient operator. fast extragradient ( FEG ) HYPONYM-OF two - time - scale EG. FEG - A HYPONYM-OF backtracking line - search version. Method are extragradient ( EG ) method, extra anchored gradient ( EAG ), and FEG. OtherScientificTerm are smooth convex - concave setting, and problem parameters. ","This paper proposes a new extragradient (EG) method for minimax problems in the nonconvex-nonconcave setting. The authors propose a two-time-scale variant of EG with slowO(1/k) rate for the squared gradient norm in the smooth structured non-convexus nonconcavity setting, and an extra anchored gradient (EG+) for the smooth convex-concaves setting. EG+ is an extension of EG+ that uses the anchoring technique in EG. FEG-A is a backtracking line-search version of the backtracking version of FEG. The theoretical results show that EG+ and EAG are more efficient than EG+ with fast O(1 / k) rate, and FEG is more efficient with a negative comonotonicity condition in the saddle-gradient operator. ","This paper proposes a two-time-scale variant of the extragradient (EG) method, called extra anchored gradient (EG). EG is an extension of EG, which uses an anchoring technique to improve the performance of EG in the setting of nonconvex-nonconcave setting. EG+ is a variant of EG with a slower O(1/k) rate than the squared gradient norm in the smooth structured nonconvxnoncalave setting, and FEG-A is a backtracking line-search version with a negative comonotonicity condition on the saddle-gradient operator. "
14835,SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"statistical data USED-FOR uniformity testing. rankings FEATURE-OF statistical data. uniform distribution COMPARE Mallows model. Mallows model COMPARE uniform distribution. pairwise statistics USED-FOR uniform distribution. pairwise statistics USED-FOR Mallows model. uniformity testing algorithm USED-FOR local DP scenario. ranking data USED-FOR binary statistics. binary statistics USED-FOR it. OtherScientificTerm are alternative class, large domain, uniformity, ✏ 0, and privacy budget parameter. Method are Mallows models, central DP algorithm, and uniformity testing algorithms. Task is Testing ranking data. ",This paper studies the problem of uniformity testing on statistical data with rankings. The authors consider the local DP scenario where the alternative class is a large domain and the uniform distribution is a pairwise statistics. The central DP algorithm is based on the Mallows models. The main contribution of the paper is to propose a uniformity test algorithm for the localDP scenario. The paper shows that it is possible to train a Mallows model with pairwise statistical statistics and a uniform distribution that is close to the one learned by the MallOWS model. The algorithm is also shown to be computationally efficient.   ,"This paper proposes a new uniformity testing algorithm for the local DP scenario, where the alternative class is a large domain, and the distribution of the ranking data is not uniform. The authors propose to use pairwise statistics for the uniform distribution instead of the standard Mallows model. The main idea is to use a central DP algorithm, which is a combination of the two. The privacy budget parameter is the difference between the uniformity, ✏ 0, and that of the Mallows models. The paper also proposes to use the binary statistics instead of ranking data for binary statistics. Experiments are conducted on ranking data. "
14884,SP:99a835191a3ba8372e391b6d3316e9b68e543295,Greedy algorithms USED-FOR learning graphical models. worst - case exponential runtime EVALUATE-FOR greedy algorithms. greedy algorithms USED-FOR learning directed acyclic graphs. greedy scorebased algorithm USED-FOR learning DAGs. edge - greedy algorithms COMPARE approach. approach COMPARE edge - greedy algorithms. vertex - greedy USED-FOR approach. GES and hill - climbing algorithms HYPONYM-OF edge - greedy algorithms. score evaluations USED-FOR approach. polynomial - time algorithms USED-FOR learning DAG models. polynomial - time algorithms PART-OF algorithm. score - based algorithms USED-FOR order - based algorithms. Bregman divergences CONJUNCTION exponential families. exponential families CONJUNCTION Bregman divergences. score functions CONJUNCTION optimality conditions. optimality conditions CONJUNCTION score functions. algorithm USED-FOR score. Task is learning statistical models with sparse structure. Generic is they. OtherScientificTerm is DAGs. Method is DAG models. Metric is sample and computational complexity bounds. ,"This paper studies the problem of learning statistical models with sparse structure. The authors propose a greedy scorebased algorithm for learning DAGs with directed acyclic graphs. The proposed approach is based on vertex-greedy, and is shown to have the worst-case exponential runtime for greedy algorithms. The paper also shows that the proposed algorithm can be combined with polynomial-time algorithms in order-based algorithms to improve the sample and computational complexity bounds. ",This paper proposes a greedy scorebased algorithm for learning directed acyclic graphs. The approach is based on GES and hill-climbing algorithms with worst-case exponential runtime. The authors show that the proposed approach outperforms edge-greedy algorithms in terms of sample and computational complexity bounds. They also show that their algorithm can be combined with polynomial-time algorithms for learning DAG models with sparse structure. 
14933,SP:b60989706296b963b6671c01f22384978a334be1,"accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION accuracy. adversarial robustness EVALUATE-FOR CNNs. adversarial training USED-FOR CNNs. adversarial training USED-FOR adversarial robustness. adversarial robustness FEATURE-OF backbone CNNs. dilation architecture USED-FOR backbone CNN. accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION accuracy. real - world datasets CONJUNCTION benchmark neural networks. benchmark neural networks CONJUNCTION real - world datasets. real - world datasets EVALUATE-FOR algorithm. benchmark neural networks EVALUATE-FOR algorithm. adversarial robustness EVALUATE-FOR algorithm. accuracy EVALUATE-FOR algorithm. Method are convolutional neural networks ( CNNs ), and neural architecture dilation algorithm. Generic is they. OtherScientificTerm are adversarial attacks, and minimal computational overhead. ",This paper studies the problem of adversarial robustness of convolutional neural networks (CNNs). The authors propose a neural architecture dilation algorithm to improve the performance of backbone CNNs. The authors show that the dilation architecture improves the accuracy and adversarial perturbations of the backbone CNN. The proposed algorithm is evaluated on several real-world datasets and benchmark neural networks.,"This paper proposes a neural architecture dilation algorithm to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. Specifically, the authors propose a dilation architecture to train a backbone CNN with adversarial training to improve CNNs’ accuracy and adversarial robustness. The algorithm is evaluated on two real-world datasets and two benchmark neural networks. The authors show that the proposed algorithm improves the accuracy and robustness against attacks with minimal computational overhead."
14982,SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"linear function approximation USED-FOR episodic Markov decision processes ( MDPs ). model - based reward - free reinforcement learning USED-FOR episodic Markov decision processes ( MDPs ). linear function approximation USED-FOR model - based reward - free reinforcement learning. transition probability kernel PART-OF MDP. feature mappings FEATURE-OF linear function. Linear Mixture MDP assumption USED-FOR algorithm. linear function USED-FOR transition probability kernel. reward function USED-FOR ε - optimal policy. UCRL - RFE USED-FOR ε - optimal policy. Bernstein - type bonus USED-FOR UCRL - RFE. upper bound COMPARE lower bound. lower bound COMPARE upper bound. Task are planning phase, and exploration phase. Generic is policy. OtherScientificTerm is feature mapping. Method are linear Mixture MDPs, and reward - free algorithm. ","This paper proposes a new linear function approximation for episodic Markov decision processes (MDPs) for model-based reward-free reinforcement learning. The main idea is to use the linear Mixture MDP assumption in the MDP as a transition probability kernel in the planning phase, and then use the reward function as a reward function in the exploration phase to learn the ε-optimal policy. The proposed algorithm is based on UCRL-RFE with Bernstein-type bonus. The authors show that the proposed upper bound is better than the lower bound. ",This paper proposes a linear function approximation for episodic Markov decision processes (MDPs) for model-based reward-free reinforcement learning. The main idea is to use the linear Mixture MDP assumption for the algorithm. The authors show that the transition probability kernel of an MDP is a linear combination of the feature mappings of the policy and the reward function of the MDP. The planning phase is the exploration phase. The reward function is the ε-optimal policy with UCRL-RFE with Bernstein-type bonus. The upper bound of the lower bound is the same as the upper bound in the paper. 
15031,SP:28563ba0975f56ddb662cd46e85de78bb6024d36,seasonal patterns FEATURE-OF data stream of events. Shifting Seasonal Matrix Factorization approach USED-FOR seasonal patterns. SSMF HYPONYM-OF Shifting Seasonal Matrix Factorization approach. it USED-FOR regime shifts. regime shifts PART-OF seasonal patterns. regime shifts USED-FOR it. lossless data compression scheme USED-FOR it. lossless data compression scheme USED-FOR method. algorithm COMPARE baseline methods. baseline methods COMPARE algorithm. real - world data streams EVALUATE-FOR baseline methods. real - world data streams EVALUATE-FOR algorithm. OtherScientificTerm is human intervention. ,"This paper proposes a Shifting Seasonal Matrix Factorization approach called SSMF. The proposed method is based on a lossless data compression scheme, and it uses regime shifts in the data stream of events to learn the seasonal patterns of events. The authors show that the proposed algorithm performs better than baseline methods on real-world data streams. ","This paper proposes a Shifting Seasonal Matrix Factorization approach to learn seasonal patterns in a data stream of events. The main idea is to use SSMF, which is a lossless data compression scheme, to learn the seasonal patterns. The proposed algorithm is evaluated on real-world data streams and compared to baseline methods. The authors show that it is able to learn regime shifts in the data stream, and it is also able to identify regime shifts that are not caused by human intervention."
15080,SP:e4bb07033001be4d04695ef058f426d49fe440be,"Assignment HYPONYM-OF informatics. exact solvers USED-FOR assignment problems. objective functions CONJUNCTION prior assumptions. prior assumptions CONJUNCTION objective functions. algorithms USED-FOR real problems. WeaveNet HYPONYM-OF neural network architecture. feature weaving layer PART-OF core module. strongly NP - hard settings USED-FOR stable matching. stable matching HYPONYM-OF non - linear assignment problems. OtherScientificTerm is NP - hardness or incomplete input. Method are approximation algorithms, learning - based 7 method, learning - based baselines, and algorithmic method. Task are real - world assignment problems, and combinatorial problem of assignment. Generic is model. ","This paper studies the problem of assignment in informatics, i.e., how to solve a real-world assignment problems in the presence of NP-hardness or incomplete input. The authors propose to use exact solvers to solve assignment problems, where the objective functions and prior assumptions are unknown. The main contribution of the paper is a new neural network architecture called WeaveNet. The core module consists of a feature weaving layer and a learning-based 7 method. The proposed algorithms can be applied to real problems such as stable matching and non-linear assignment problems. The paper also proposes a new approximation algorithms for these approximation algorithms. The model is evaluated on a combinatorial problem and shows that the proposed algorithmic method is able to achieve state-of-the-art performance.","This paper studies the problem of assignment in informatics, i.e., how to solve real-world assignment problems with NP-hardness or incomplete input. The authors propose to use exact solvers to solve assignment problems, which can be approximated by approximation algorithms. They propose a learning-based 7 method, which is based on WeaveNet, a neural network architecture with a feature weaving layer in the core module. They show that the objective functions and prior assumptions of the learned objective functions can be used to solve the real problems. They also propose an algorithmic method to solve non-linear assignment problems such as stable matching in strongly NP-Hard settings. "
15129,SP:8a559e21d45661eef427b310e5fe8488d5749137,3D point cloud data USED-FOR safety - critical applications. autonomous driving HYPONYM-OF safety - critical applications. robustness EVALUATE-FOR 3D deep learning models. adversarial attacks FEATURE-OF 3D deep learning models. threat models USED-FOR 3D point clouds. self - supervised learning proxy tasks USED-FOR threat models. adversarial training USED-FOR self - supervised learning proxy tasks. adversarial training USED-FOR threat models. MLP - based ( PointNet ) CONJUNCTION convolution - based ( DGCNN ). convolution - based ( DGCNN ) CONJUNCTION MLP - based ( PointNet ). convolution - based ( DGCNN ) CONJUNCTION transformer - based ( PCT ) 3D architectures. transformer - based ( PCT ) 3D architectures CONJUNCTION convolution - based ( DGCNN ). self - supervision USED-FOR 3D point cloud recognition. self - supervision COMPARE adversarial training baseline. adversarial training baseline COMPARE self - supervision. robustness EVALUATE-FOR 3D point cloud recognition. robustness EVALUATE-FOR self - supervision. it USED-FOR adversarial propagation. local feature learning USED-FOR adversarial robustness. local feature learning USED-FOR point clouds. adversarial robustness FEATURE-OF point clouds. DGCNN CONJUNCTION jigsaw proxy task. jigsaw proxy task CONJUNCTION DGCNN. jigsaw proxy task USED-FOR 3D adversarial robustness. 3D adversarial robustness EVALUATE-FOR DGCNN. OtherScientificTerm is point - level input perturbations. ,This paper studies the robustness of 3D deep learning models against adversarial attacks on 3D point cloud data in safety-critical applications such as autonomous driving. The authors propose to use adversarial training on self-supervised learning proxy tasks to train threat models to predict the point clouds from the threat models. The proposed DGCNN and convolution-based (DGCNN) as well as transformer-based and PCT) 3D architectures are evaluated on a jigsaw proxy task and 3D adversarial robustness for 3D Point cloud recognition. The results show that the proposed method is able to achieve better robustness against 3D perturbations compared to self-smoothing and adversarial learning baseline. ,"This paper studies the robustness of 3D deep learning models against adversarial attacks on 3D point cloud data for safety-critical applications such as autonomous driving. The authors show that adversarial training for self-supervised learning proxy tasks for threat models can be used to improve robustness against 3D adversarial perturbations on point clouds. They also show that it can be applied to adversarial propagation on point-level input perturbation. They further show that 3D convolution-based (DGCNN) and transformer-based(PCT) 3D architectures can improve 3D robustness on a jigsaw proxy task and on a DGCNN with local feature learning. They show that the proposed adversarial robustness outperforms the adversarial learning baseline in terms of robustness to point clouds, and the proposed robustness is also competitive with self-smoothing in terms robustness for 3D Point cloud recognition. "
15178,SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"FISTA CONJUNCTION mirror descent. mirror descent CONJUNCTION FISTA. projected Newton ’s method CONJUNCTION FISTA. FISTA CONJUNCTION projected Newton ’s method. near - optimal regret bounds CONJUNCTION convergence rates. convergence rates CONJUNCTION near - optimal regret bounds. projected Newton ’s method CONJUNCTION mirror descent. mirror descent CONJUNCTION projected Newton ’s method. O(T ) regret EVALUATE-FOR online mirror descent. near - optimal regret bounds FEATURE-OF Optimization algorithms. mirror descent HYPONYM-OF Optimization algorithms. projected Newton ’s method HYPONYM-OF Optimization algorithms. FISTA HYPONYM-OF Optimization algorithms. conditional gradient variants USED-FOR linear optimization. O(T ) regret HYPONYM-OF suboptimal rates. toolkit USED-FOR projections. discrete and continuous perspectives USED-FOR toolkit. discrete and continuous perspectives USED-FOR projections. early termination USED-FOR away - step Frank - Wolfe algorithm. runtime EVALUATE-FOR Bregman projections. OtherScientificTerm are computational bottleneck, iterative projections, and cardinality based submodular polytopes. Metric is runtime v / s convergence rates. ","This paper studies the problem of online mirror descent with O(T) regret, where the goal is to minimize the number of iterations needed to converge to the optimal solution. The authors propose a new optimization algorithm, called Frank-Wolfe, which is based on the FISTA and mirror descent algorithms. The main contribution of the paper is to provide near-optimal regret bounds on the convergence rates and convergence rates of these two algorithms. They also provide a theoretical analysis of the runtime v/s convergence rates for these algorithms. ","The paper proposes a new toolkit for learning projections from discrete and continuous perspectives. Optimization algorithms such as FISTA, mirror descent, and projected Newton’s method are proposed. The paper provides near-optimal regret bounds on the convergence rates and O(T) regret for online mirror descent with conditional gradient variants for linear optimization with early termination. The authors show that the runtime v/s convergence rates for Bregman projections are better than for iterative projections with cardinality based submodular polytopes. They also show that an away-step Frank-Wolfe algorithm can be used to reduce the runtime of an iterative projection."
15227,SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"natural parameters PART-OF k - parameter minimal exponential family. i.i.d. samples USED-FOR natural parameters. maximum likelihood estimator USED-FOR exponential family. finite sample guarantees USED-FOR parameter estimation. maximum likelihood estimation USED-FOR re - parameterized distribution. exponential family FEATURE-OF re - parameterized distribution. maximum likelihood estimation USED-FOR method. re - parameterized distribution USED-FOR method. Generic are it, and estimator. Metric are sample complexity, and computational complexity. ","This paper studies the problem of parameter estimation with finite sample guarantees for k-parameter minimal exponential family with natural parameters, i.e. i.i.d. samples. The authors propose a new estimator based on maximum likelihood estimator for the exponential family, and show that it is computationally efficient. The proposed method uses maximum likelihood estimation for the re-parametrized distribution of the original exponential family. They also show that the sample complexity of the estimator is bounded.","This paper proposes a method to estimate the natural parameters of the k-parameter minimal exponential family, i.i.d. samples. The natural parameters are defined as a set of samples with finite sample guarantees, and the parameter estimation is based on the maximum likelihood estimator of the exponential family. The authors show that it is computationally efficient to compute the estimator, and it is also computationally expensive to compute it. The proposed method uses maximum likelihood estimation of the re-parametrized distribution of an exponential family in the same exponential family as the original estimator. The paper also shows that the sample complexity of the proposed method is much lower than the computational complexity of existing methods."
15276,SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"differentiable renderers USED-FOR predicting intrinsic object properties. learning - based approaches USED-FOR inverse graphics. rasterization - based renderers USED-FOR learning - based approaches. rasterization CONJUNCTION ray - tracing. ray - tracing CONJUNCTION rasterization. DIBR++ HYPONYM-OF hybrid differentiable renderer. speed CONJUNCTION realism. realism CONJUNCTION speed. hybrid differentiable renderer USED-FOR photorealistic effects. ray - tracing PART-OF hybrid differentiable renderer. direct estimation CONJUNCTION spherical basis functions. spherical basis functions CONJUNCTION direct estimation. renderer USED-FOR light transport. environmental lighting and spatially - varying material models PART-OF renderer. direct estimation USED-FOR light transport. spherical basis functions USED-FOR light transport. learning frameworks USED-FOR geometry, reflectance and lighting prediction. physics - based differentiable renderers COMPARE DIB - R++. DIB - R++ COMPARE physics - based differentiable renderers. compact and expressive shading model USED-FOR DIB - R++. path tracing USED-FOR physics - based differentiable renderers. approach COMPARE rasterization - based approaches. rasterization - based approaches COMPARE approach. material editing CONJUNCTION relighting. relighting CONJUNCTION material editing. approach USED-FOR artistic applications. material and lighting disentanglement FEATURE-OF synthetic and real data. rasterization - based approaches USED-FOR artistic applications. material and lighting disentanglement EVALUATE-FOR rasterization - based approaches. relighting HYPONYM-OF artistic applications. material editing HYPONYM-OF artistic applications. material and lighting disentanglement EVALUATE-FOR approach. synthetic and real data EVALUATE-FOR approach. Method is naive lighting and material models. OtherScientificTerm are non - Lambertian, specular reflections, and ground - truth. ","This paper proposes a hybrid differentiable renderer, DIBR++, for predicting intrinsic object properties in inverse graphics. DIB-R++ is based on a compact and expressive shading model. The renderer uses environmental lighting and spatially-differential material models to generate photorealistic effects. The authors provide direct estimation for light transport and spherical basis functions for direct estimation of light transport. The proposed approach is evaluated on synthetic and real data with both material and lighting disentanglement. The experimental results show that the proposed approach outperforms existing rasterization-based approaches in various artistic applications such as relighting, material editing and relighting.","This paper proposes a hybrid differentiable renderer, DIBR++, for predicting intrinsic object properties. The authors show that DIB-R++ is able to capture the photorealistic effects of lighting and ray-tracing in inverse graphics, which is an important application of learning-based approaches for inverse graphics. The main contribution of the paper is a compact and expressive shading model, which allows for non-Lambertian, specular reflections, and ground-truth. The renderer is based on environmental lighting and spatially-dense material models, and is trained on a set of naive lighting and material models. The proposed approach is evaluated on synthetic and real data with both material and lighting disentanglement, showing that the proposed approach outperforms other rasterization-based renderers in terms of speed, realism, and spherical basis functions for light transport. The approach is also compared to physics-based differentiable rendererers based on path tracing, and it is shown that the approach outperform rasterized versions of these renderers. The paper also shows that the learned frameworks can be used for geometry, reflectance and lighting prediction, and the approach can be applied to a variety of artistic applications such as material editing, relighting, and relighting."
15325,SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"Soft - argmax operation USED-FOR detection - based methods. soft - argmax USED-FOR neural network. sampling - argmax HYPONYM-OF differentiable training method. continuous formulation USED-FOR output distribution. continuous formulation USED-FOR differentiable sampling process. continuous formulation USED-FOR expectation. sampling - argmax USED-FOR localization tasks. soft - argmax operation USED-FOR localization tasks. sampling - argmax COMPARE soft - argmax operation. soft - argmax operation COMPARE sampling - argmax. OtherScientificTerm are differentiable manner, probability map, pixel - wise supervision, map, implicit constraints, and expectation of the localization error. Generic are model, and method. Metric is average error. ","This paper proposes a differentiable training method called sampling-argmax, which is based on a continuous formulation of the output distribution of the differentiable sampling process. The main idea is to use a soft- argmax operation to train the neural network. The model is trained in the same differentiable manner, but the probability map is differentiable, and pixel-wise supervision is added to the map. The proposed method is evaluated on a variety of localization tasks, and the results show that the proposed sampling -argmax operation performs better than the soft-argmax operation on most of the localization tasks. The authors also provide an upper bound on the average error of the proposed method.","The paper proposes a differentiable training method called sampling-argmax, which is a continuous formulation of the output distribution of a neural network. The idea is to learn the distribution of the parameters of the model in the differentiable manner, where the probability map is learned by pixel-wise supervision. The paper shows that the proposed method is more robust to the average error of the network than sampling-aggmax. The main contribution of the paper is to introduce implicit constraints on the expectation of the localization error. The authors also show that sampling-agnostic sampling process is equivalent to the continuous formulation. Experiments are conducted on localization tasks where the proposed soft- argmax operation is shown to outperform sampling-agmax operation on some localization tasks."
15374,SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"Graph Contrastive Learning ( GCL ) USED-FOR generalizable representations. contrastive views USED-FOR generalizable representations. data augmentation USED-FOR contrastive views. incomplete structure information USED-FOR models learning. directional structure FEATURE-OF directed graphs. hand - picking parameters FEATURE-OF predefined contrastive views. data augmentation USED-FOR contrastive information. directional structure HYPONYM-OF intrinsic graph structural information. data augmentation USED-FOR graph structure. predefined contrastive views USED-FOR GCL. hand - picking parameters USED-FOR GCL. it USED-FOR contrastive information. Laplacian perturbation HYPONYM-OF directed graph data augmentation method. contrastive views USED-FOR directed graph contrastive learning framework. Laplacian perturbation USED-FOR contrastive views. multi - task curriculum learning USED-FOR it. model COMPARE GCL models. GCL models COMPARE model. structural features of directed graphs EVALUATE-FOR model. benchmarks EVALUATE-FOR state - of - the - art approaches. Method is message passing scheme. OtherScientificTerm are graph changing action, directed graph structure, and easy - to - difficult contrastive views. ","Graph Contrastive Learning (GCL) aims to learn generalizable representations from contrastive views. In GCL, the data augmentation is used to augment the contrastive information in the graph structure, such as directional structure of directed graphs. The authors propose a message passing scheme, where the graph changing action is passed through the directed graph structure. They show that it can improve the performance of GCL by using hand-picking parameters in the predefined contrastive view. They also demonstrate that it is possible to use Laplacian perturbation to improve the quality of directed graph contrastive learning framework. Finally, they show that the proposed model is able to learn structural features of the directed graphs better than other GCL models.","Graph Contrastive Learning (GCL) aims to learn generalizable representations from incomplete structure information in models learning. GCL uses predefined contrastive views with hand-picking parameters to learn the contrastive information. The authors propose a directed graph data augmentation method based on Laplacian perturbation. They show that the proposed model can learn the structural features of directed graphs better than state-of-the-art approaches. They also provide a message passing scheme to encourage the graph changing action to be directed graph structure, and show that it can learn easy-to-difficult contrastive images. The proposed model is evaluated on multi-task curriculum learning."
15423,SP:85b383d2f722f7bff438840e423f5cb4c67d5980,common interface FEATURE-OF grounded language learning environments. RTFM CONJUNCTION Messenger. Messenger CONJUNCTION RTFM. grid - world environments CONJUNCTION symbolic counterparts of visual worlds. symbolic counterparts of visual worlds CONJUNCTION grid - world environments. interpreting rich natural language USED-FOR complex scenes. interpreting rich natural language USED-FOR symbolic counterparts of visual worlds. RTFM HYPONYM-OF grid - world environments. ALFWorld HYPONYM-OF complex scenes. Messenger HYPONYM-OF grid - world environments. grid - world environments PART-OF SILG. symbolic counterparts of visual worlds PART-OF SILG. action space CONJUNCTION language specification. language specification CONJUNCTION action space. richness of observation space CONJUNCTION action space. action space CONJUNCTION richness of observation space. language specification CONJUNCTION plan complexity. plan complexity CONJUNCTION language specification. shared model architecture USED-FOR RL. recurrent state - tracking CONJUNCTION entity - centric attention. entity - centric attention CONJUNCTION recurrent state - tracking. egocentric local convolution CONJUNCTION recurrent state - tracking. recurrent state - tracking CONJUNCTION egocentric local convolution. entity - centric attention CONJUNCTION pretrained LM. pretrained LM CONJUNCTION entity - centric attention. shared model architecture USED-FOR environments. SILG USED-FOR pretrained LM. shared architecture COMPARE environment - specific architectures. environment - specific architectures COMPARE shared architecture. SILG EVALUATE-FOR models. SILG USED-FOR language grounding. Method is unified models. OtherScientificTerm is entities. Material is multi - environment benchmark. ,"This paper studies the common interface of grounded language learning environments with a common interface. The authors propose SILG, a shared model architecture for RL with grid-world environments such as RTFM and Messenger. The SILG is composed of symbolic counterparts of visual worlds in the action space, language specification, and plan complexity, and is able to handle complex scenes such as ALFWorld, which is a multi-environment benchmark.  The authors show that SILG can improve the performance of language grounding in these environments by using the shared architecture in environments where the entities are represented as a set of entities.  They also show that the proposed shared architecture can be used to train a pretrained LM, recurrent state-tracking, entity-centric attention, and egocentric local convolution. ","This paper proposes a common interface for grounded language learning environments, where the goal is to learn unified models across multiple entities. The authors propose a shared model architecture for RL, which can be applied to both grid-world environments and symbolic counterparts of visual worlds (e.g., RTFM, Messenger, etc.). The authors show that the shared architecture outperforms environment-specific architectures in terms of richness of observation space, action space, language specification, plan complexity, and recurrent state-tracking (egocentric local convolution, egocentric global convolution). The authors also provide a multi-environment benchmark, where they compare the performance of the proposed models with the state-of-the-art SILG for language grounding. "
15472,SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"Vision MoE ( V - MoE ) COMPARE dense networks. dense networks COMPARE Vision MoE ( V - MoE ). Vision MoE ( V - MoE ) HYPONYM-OF Vision Transformer. V - MoE COMPARE networks. networks COMPARE V - MoE. V - MoE USED-FOR image recognition. extension USED-FOR adaptive per - image compute. routing algorithm USED-FOR extension. V - MoE USED-FOR scale vision models. V - MoE USED-FOR 15B parameter model. ImageNet EVALUATE-FOR 15B parameter model. Task are Natural Language Processing, and Computer Vision. Method is vision models. ","This paper proposes Vision MoE (V-MoE), a Vision Transformer, which is a variant of Vision-based Neural Language Processing (NLP). The authors propose a routing algorithm to improve the performance of V-MoEs in the context of Natural Language Processing. The authors show that the proposed extension can be used for adaptive per-image compute, and that it can be combined with existing networks such as V-MOE for image recognition. The proposed extension is also applied to scale vision models such as Computer Vision. The experimental results on ImageNet show that V-moE is able to outperform the standard 15B parameter model in terms of performance.","This paper proposes Vision MoE (V-MoE), a Vision Transformer, which is an extension of Vision MoEs (V) that is more efficient than dense networks. The authors show that V-MoEs outperform networks on image recognition and image classification. The extension is based on adaptive per-image compute, and the authors propose a routing algorithm to improve the efficiency of the extension. Experiments on ImageNet show that the proposed 15B parameter model can outperform the state-of-the-art in terms of performance on the image classification task. The paper also shows that the extension can be applied to other scale vision models. "
15521,SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"benign optimization landscape FEATURE-OF loss function. n ( sample size ) neurons FEATURE-OF 1 - hidden - layer networks. zero training loss FEATURE-OF global minimizer. local - min or saddle points FEATURE-OF nice local region. global minimizer FEATURE-OF KKT point. projected gradient methods USED-FOR KKT points. SGD USED-FOR narrow neural nets. projected gradient methods USED-FOR narrow neural nets. projected gradient methods COMPARE SGD. SGD COMPARE projected gradient methods. projected gradient methods USED-FOR constrained formulation. Method are neural networks, narrow networks, and gradient descent. Generic is network. OtherScientificTerm are activation, expressivity, and feasible region. Task is constrained optimization formulation. ","This paper studies the problem of constrained optimization of neural networks with n (sample size) neurons in 1-hidden-layer networks. The authors propose a new loss function for the loss function in the benign optimization landscape. The global minimizer is a zero training loss for the KKT point of the network. The local-min or saddle points are the nice local region in the network, and the activation is a function of the number of neurons.  The authors show that SGD can be used to train narrow neural nets with projected gradient methods for KKT points, and that the constrained formulation of SGD outperforms the state-of-the-art in the constrained optimization formulation.   The main contribution of the paper is a theoretical analysis of gradient descent for narrow networks. ","This paper proposes a new loss function for a particular loss function in the benign optimization landscape. The authors propose to use n (sample size) neurons in 1-hidden-layer networks, where the activation, expressivity, and local-min or saddle points of the network are defined as a function of the number of neurons in the network. The paper shows that the global minimizer of the KKT point is a zero training loss, and that the proposed constrained formulation of the constrained optimization formulation is equivalent to SGD for narrow neural nets, and the proposed projected gradient methods for KKT points outperform SGD in the constrained formulation. "
15570,SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"risk measures USED-FOR risk - aware multi - armed bandit models. variance HYPONYM-OF risk measures. correlated options FEATURE-OF real - world online decision making problems. learner PART-OF CMCB. full - bandit feedback HYPONYM-OF feedback settings. full - information HYPONYM-OF feedback settings. logarithmic factors FEATURE-OF optimal regrets. matching lower bounds USED-FOR algorithms. optimal regrets FEATURE-OF algorithms. option correlation FEATURE-OF risk - aware bandits. analytical techniques USED-FOR bandit analysis. analytical techniques USED-FOR concentration. estimated covariance USED-FOR concentration. analytical techniques USED-FOR risk of selected actions. analytical techniques USED-FOR estimated covariance. sampling strategy properties USED-FOR bandit analysis. sampling strategy properties USED-FOR analytical techniques. Generic is they. OtherScientificTerm are weight vectors, random feedback, option covariance, reward observation scenarios, and covariance structures. ","This paper studies the problem of risk-aware multi-armed bandit models with variance. The authors propose a new risk measures, called option correlation, which is a measure of the variance of the risk of a bandit model. Theoretically, the authors show that the variance is a function of the number of options in the bandit training set, and that it depends on the weight vectors of the training set. The paper also shows that the covariance structure of the CMCB can be used to estimate the concentration of the option covariance of the optimal bandit regret.  The authors then propose two algorithms with matching lower bounds on the optimal regrets of the algorithms. The algorithms are based on the sampling strategy properties of bandit analysis, where the analytical techniques are used to compute the concentration and the estimated covariance for each of the selected actions.","This paper presents risk measures for risk-aware multi-armed bandit models. The risk measures are based on variance HYPONYM of risk measures. The authors show that in real-world online decision making problems with correlated options, including full-bandit feedback and feedback settings with full-information, the optimal regrets are defined as the logarithmic factors of the optimal regret of the weight vectors of the reward observation scenarios. They also show that they can be approximated by matching lower bounds on the algorithms. They show that under certain conditions (e.g., random feedback, option covariance, etc.), the algorithms can achieve optimal regrets in terms of optimal regrets. They further show that the proposed CMCB is a learner that learns to learn the covariance structures of the target and the target. They provide analytical techniques to estimate the concentration of selected actions and the estimated covariance of the selected actions. Finally, they provide sampling strategy properties for bandit analysis."
15619,SP:472a90bb175b0286765c5a47b040e1a58f594a05,"r × r - dimensional PSD matrices PART-OF Positive Semidefinite ( PSD ) factorization. PSD factorizations USED-FOR semidefinite programs. quantum resources USED-FOR information theory. Nonnegative Matrix Factorization ( NMF ) problem USED-FOR PSD factorization task. algorithm USED-FOR NMFs. Multiplicative Update algorithm HYPONYM-OF algorithm. positive diagonal matrices USED-FOR non - negativity. non - commutative extension USED-FOR PSD factorizations. Lee - Seung ’s algorithm USED-FOR non - commutative extension. Matrix Multiplicative Update ( MMU ) algorithm HYPONYM-OF non - commutative extension. multiplicative update algorithm USED-FOR NMF. squared loss objective EVALUATE-FOR update scheme. blockdiagonal PSD factorizations CONJUNCTION tensor PSD factorizations. tensor PSD factorizations CONJUNCTION blockdiagonal PSD factorizations. MMU algorithm USED-FOR blockdiagonal PSD factorizations. MMU algorithm USED-FOR tensor PSD factorizations. MMU algorithm USED-FOR primitive. primitive USED-FOR blockdiagonal PSD factorizations. primitive USED-FOR tensor PSD factorizations. real and synthetic data EVALUATE-FOR method. OtherScientificTerm are r - dimensional non - negative vectors, and matrix geometric mean of appropriate PSD matrices. Generic are problem, and it. Method are PSD factorization, MajorizationMinimization framework, and Lieb ’s Concavity Theorem. ","This paper studies the nonnegative Matrix Factorization (NMF) problem in the PSD factorization task, where the objective is to find r × r-dimensional PSD matrices in Positive Semidefinite (PSD) factorization. The problem is formulated as a matrix geometric mean of appropriate PSD matrixrices in the form of positive diagonal matrices, and the goal is to minimize the matrix $\mathcal{O}(\sqrt{T})$ matrix $\epsilon$ of the matrix. The authors propose a non-commutative extension of the non-computative extension, the Matrix Multiplicative Update (MMU) algorithm, which is based on Lee-Seung’s algorithm. The proposed algorithm is shown to improve the performance of NMFs in terms of the squared loss objective. The paper also shows that the proposed method can be applied to both real and synthetic data. ",This paper studies the nonnegative Matrix Factorization (NMF) problem for the PSD factorization task. The NMF problem is formulated as an extension of Positive Semidefinite (PSD) factorization with r × r-dimensional PSD matrices. The authors propose a new algorithm for NMFs based on the Multiplicative Update algorithm. The non-commutative extension is based on Lee-Seung’s algorithm for non-computative extension. The paper shows that the non-negativity of positive diagonal matrices can be reduced to non-negative vectors.  The authors show that the NMF can be solved with a multiplicative update algorithm with a squared loss objective. The proposed method is evaluated on both real and synthetic data.  
15668,SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"Domain Generalization ( DG ) USED-FOR model. DG approaches USED-FOR domaininvariant information. DG approaches USED-FOR generalization capability. features PART-OF latent space. domain - specific representation USED-FOR generalization. meta - learning framework USED-FOR domain - specific representation. mDSDI COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE mDSDI. state - of - the - art techniques USED-FOR DG. mDSDI USED-FOR DG. domain - specific COMPARE domain - invariant. domain - invariant COMPARE domain - specific. Background - Colored - MNIST HYPONYM-OF dataset. OtherScientificTerm are domain - specific information, invariance view, and domain - invariant and domainspecific features. Generic is framework. Method is unified framework. ","This paper proposes Domain Generalization (DG) as a new model for learning domain invariant representations. The authors propose a meta-learning framework to learn a domain-specific representation for generalization. The proposed framework is based on the idea of domain-invariant information, which can be represented as a set of features in the latent space.  The authors provide a unified framework for learning the domain invariance view and show that the proposed mDSDI outperforms state-of-the-art techniques in terms of generalization capability. ","This paper proposes Domain Generalization (DG) to improve the generalization capability of a model. The authors propose a meta-learning framework to learn a domain-specific representation for generalization. The framework is based on the idea of domain invariant and domain-invariant features, which is a unified framework.  The authors show that DG approaches are able to capture domaininvariance information in the latent space, while state-of-the-art techniques do not. They also show that mDSDI is able to generalize better than DG, and is more robust to domain-variant and domainspecific features.  "
15717,SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"diffusion models COMPARE generative models. generative models COMPARE diffusion models. image sample quality EVALUATE-FOR generative models. image sample quality EVALUATE-FOR diffusion models. architecture USED-FOR unconditional image synthesis. diversity CONJUNCTION fidelity. fidelity CONJUNCTION diversity. method USED-FOR diversity. gradients USED-FOR classifier. sample quality USED-FOR conditional image synthesis. classifier guidance USED-FOR sample quality. classifier USED-FOR method. gradients USED-FOR method. classifier guidance USED-FOR conditional image synthesis. classifier guidance CONJUNCTION upsampling diffusion models. upsampling diffusion models CONJUNCTION classifier guidance. FID EVALUATE-FOR classifier guidance. Material are ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512. Method is BigGAN - deep. ","This paper proposes a new architecture for unconditional image synthesis based on the BigGAN-deep framework. The proposed method uses gradients to improve the sample quality of diffusion models and generative models. The authors show that the proposed method improves the diversity and fidelity of the generated images by using the gradients in the classifier to improve sample quality. The method is evaluated on ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512. The experimental results show the effectiveness of the proposed classifier guidance and upsampling diffusion models on FID.","This paper proposes a new architecture for unconditional image synthesis. The proposed method is based on BigGAN-deep. The authors show that diffusion models with high image sample quality outperform generative models in terms of diversity and fidelity. The method uses gradients to guide the classifier to improve sample quality and classifier guidance to improve the sample quality for conditional image synthesis using FID. The experiments are conducted on ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512. "
15766,SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"out - of - distribution samples USED-FOR few - shot learning. unlabeled samples HYPONYM-OF out - of - distribution samples. out - of - distribution samples USED-FOR classifier. query data HYPONYM-OF in - distribution samples. approach USED-FOR inductive and transductive settings. method COMPARE pretrained networks. pretrained networks COMPARE method. architectures FEATURE-OF pretrained networks. OtherScientificTerm are irrelevant features, prototypes, and feature extractors. Task is pre - training. ","This paper proposes a new method for few-shot learning with out-of-distribution samples. The proposed method is based on unlabeled samples (i.e., query data) and is able to extract irrelevant features from the data. The key idea is to learn a classifier from out- of-dispersion samples and then use the out-distributed samples to train the classifier. The authors show that the proposed approach can be used in inductive and transductive settings. They also show that their method outperforms existing pretrained networks on a variety of architectures. ","This paper proposes to use out-of-distribution samples for few-shot learning with unlabeled samples. The idea is to learn a classifier that can be trained with out- of-distributions samples. In particular, the classifier is trained on query data, which is a subset of the in-distributed samples. This approach is applied to inductive and transductive settings. The authors show that the proposed method outperforms pretrained networks on a variety of architectures. "
15815,SP:b1f65724926f136979829b7a6c870bc31f38f591,experience replay USED-FOR reinforcement learning. Prioritized sampling USED-FOR samples. recentness CONJUNCTION corrective feedback. corrective feedback CONJUNCTION recentness. TD error CONJUNCTION recentness. recentness CONJUNCTION TD error. criteria PART-OF prioritization. TD error HYPONYM-OF criteria. recentness HYPONYM-OF prioritization. corrective feedback HYPONYM-OF criteria. corrective feedback PART-OF prioritization. recentness HYPONYM-OF criteria. optimal prioritization strategy USED-FOR Bellman update. on - policiness CONJUNCTION Q value. Q value CONJUNCTION on - policiness. methods USED-FOR prioritization weight. ReMERN CONJUNCTION ReMERT. ReMERT CONJUNCTION ReMERN. ReMERT HYPONYM-OF methods. ReMERN HYPONYM-OF methods. ReMERT USED-FOR temporal ordering of states. ReMERN HYPONYM-OF error network. MuJoCo CONJUNCTION Atari. Atari CONJUNCTION MuJoCo. Atari CONJUNCTION Meta - World. Meta - World CONJUNCTION Atari. methods COMPARE prioritized sampling algorithms. prioritized sampling algorithms COMPARE methods. RL benchmarks EVALUATE-FOR prioritized sampling algorithms. RL benchmarks EVALUATE-FOR methods. Meta - World HYPONYM-OF RL benchmarks. MuJoCo HYPONYM-OF RL benchmarks. Atari HYPONYM-OF RL benchmarks. Metric is regret minimization objective. OtherScientificTerm is hindsight TD error. Task is sampling. Generic is strategy. ,"This paper studies the problem of experience replay in reinforcement learning. Prioritized sampling is a popular technique for sampling samples from a dataset. The authors propose two criteria for prioritization: TD error and recentness. TD error is a measure of the regret minimization objective, while recentness measures the recentness and corrective feedback are two criteria that are used in the prioritization.  The authors show that the optimal prioritization strategy for Bellman update can be obtained by maximizing the TD error of the optimal strategy. They also show that this strategy can be combined with ReMERT for temporal ordering of states and ReMERN for the error network. Finally, they show that these methods are able to achieve better prioritization weight than existing methods in terms of on-policiness and Q value. They show that their methods outperform other prioritized sampling algorithms on several RL benchmarks such as Meta-World, MuJoCo, Atari, and Atari.","This paper proposes a new approach to learning from experience replay in reinforcement learning. Prioritized sampling is used to sample samples that are more likely to be of high quality. The authors propose two criteria for prioritization: TD error and recentness. TD error is a measure of the regret minimization objective, while recentness is the measure of TD error in hindsight TD error. The Bellman update is an optimal prioritization strategy that maximizes TD error while minimizing recentness and corrective feedback. The proposed strategy is evaluated on a variety of RL benchmarks, including MuJoCo, Atari, Meta-World, ReMERT, and ReMERN. The results show that the proposed methods can achieve better prioritization weight and on-policiness and Q value compared to other methods. The paper also shows that the prioritized sampling algorithms outperform other prioritized samplers in terms of temporal ordering of states. "
15864,SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,nonstationary environment FEATURE-OF expert advice. nonstationary environment FEATURE-OF sequential prediction. expert advice USED-FOR sequential prediction. regret bounds USED-FOR linear - time algorithm. relative entropy projection step PART-OF algorithm. projection COMPARE weight - sharing approaches. weight - sharing approaches COMPARE projection. implicit costs FEATURE-OF weight updates. algorithm USED-FOR projection step. linear time FEATURE-OF projection step. OtherScientificTerm is long - term memory guarantees. Task is portfolio optimization. ,"This paper studies the problem of sequential prediction in a nonstationary environment with expert advice. The authors propose a linear-time algorithm with relative entropy projection step, where the projection step has linear time and the implicit costs of weight updates are bounded by long-term memory guarantees. They show that the proposed projection is more efficient than weight-sharing approaches in portfolio optimization.",This paper proposes a linear-time algorithm with regret bounds for sequential prediction in a nonstationary environment. The algorithm is based on the relative entropy projection step in the algorithm. The authors provide long-term memory guarantees for the linear time. They show that the proposed projection step is more efficient than weight-sharing approaches in terms of implicit costs of weight updates. They also provide a theoretical analysis of portfolio optimization.
15913,SP:b2439973063e827b3cbe92306a2fdee3286b6b44,navigational engines CONJUNCTION recommendation systems. recommendation systems CONJUNCTION navigational engines. routing applications USED-FOR recommendation systems. routing applications USED-FOR navigational engines. contextual linear bandits USED-FOR routing applications. routing applications USED-FOR variant. contextual linear bandits USED-FOR variant. algorithms USED-FOR problem. O(d log d ) regret CONJUNCTION list size poly(d ). list size poly(d ) CONJUNCTION O(d log d ) regret. O(d log d ) regret FEATURE-OF algorithm. list size poly(d ) FEATURE-OF algorithm. nearly tight algorithms USED-FOR problem. Steiner ’s formula USED-FOR centroid of a convex set. algorithmic techniques USED-FOR convex geometry. Steiner ’s formula HYPONYM-OF algorithmic techniques. OtherScientificTerm is hidden d - dimensional value. Method is cutting - plane algorithms. Metric is regret. ,"This paper studies the problem of learning a convex geometry with a hidden d-dimensional value. The authors propose two algorithms to solve the problem. The first algorithm, called Steiner’s formula, is based on the O(d log d) regret and the list size poly(d) regret. The second algorithm, named “Steiner”, uses routing applications from contextual linear bandits to learn a variant of the routing applications in navigational engines and recommendation systems. Both algorithms are nearly tight algorithms for solving the problem, and the regret of the proposed algorithm is comparable to that of the cutting-plane algorithms. The main contribution of the paper is to provide a theoretical analysis of the regret for the two algorithms. ","This paper proposes a novel algorithm for learning convex geometry. The proposed algorithm is based on the Steiner’s formula for learning the centroid of a convex set. The authors propose two algorithms for solving the problem: O(d log d) regret and list size poly(d) regret. The main idea of the algorithm is to learn the hidden d-dimensional value of the convex geometries. The algorithm is evaluated on a variety of routing applications for navigation systems and recommendation systems, including contextual linear bandits. Experiments show that the proposed algorithm outperforms nearly tight algorithms for the problem. "
15962,SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"Automated machine learning ( AutoML ) USED-FOR data scientists. combinators USED-FOR compositional code. orthogonal combinators USED-FOR machinelearning operators. orthogonal combinators USED-FOR pipelines. machinelearning operators PART-OF pipelines. search spaces USED-FOR AutoML optimizers. hyperparameter schemas CONJUNCTION search spaces. search spaces CONJUNCTION hyperparameter schemas. translation scheme USED-FOR search spaces. hyperparameter schemas USED-FOR translation scheme. pipelines USED-FOR translation scheme. Lale HYPONYM-OF sklearn - compatible AutoML library. user study EVALUATE-FOR it. Method are machine learning, AutoML, and functional programming. OtherScientificTerm is non - compositional code changes. ","This paper studies the problem of automated machine learning (AutoML) in the context of data scientists. The authors propose two pipelines that use orthogonal combinators to learn compositional code. The main idea is that the search spaces of AutoML optimizers can be represented as hyperparameter schemas, which can be used as a translation scheme between two different search spaces. The two pipelines are then used to train the machinelearning operators in these two pipelines. The paper shows that the two pipelines can be combined in a sklearn-compatible AutoML library called Lale, which is a combination of Lale and AutoML.   The paper also shows that, when combined with the hyperparameters of the two search spaces, the proposed translation scheme can achieve state-of-the-art results in terms of performance on the user study.","This paper proposes a new framework for learning compositional code for data scientists. The idea is to use orthogonal combinators to learn compositional codes for each data point. The authors propose two pipelines, which are composed of two machinelearning operators, one for learning the compositional operators and the other for learning a set of search spaces for AutoML optimizers. The main contribution of the paper is to propose a translation scheme based on hyperparameter schemas and search spaces. The proposed pipeline is evaluated on Lale, a sklearn-compatible AutoML library, and it is shown to perform better than the state-of-the-art in terms of user study. "
16011,SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"small datasets USED-FOR neural network weights. problem - byproblem basis FEATURE-OF pattern of sparsity. generalization CONJUNCTION interference. interference CONJUNCTION generalization. interference FEATURE-OF few - shot and continual learning problems. generalization EVALUATE-FOR selective sparsity. meta - learning USED-FOR adaptable features. inductive bias USED-FOR meta - learning systems. Method are weight initialization, learning algorithm, sparse learning, and sparse gradient descent. Metric is generalization error. OtherScientificTerm are patterned sparsity, and learning rates. ","This paper studies the problem of weight initialization for neural network weights on small datasets. The authors consider the problem-by-problem basis of the pattern of sparsity in the training data. The generalization error is defined as the difference between the generalization of the weight initialization and the learning algorithm. The paper shows that in the few-shot and continual learning problems with patterned sparsity, both generalization and interference can be improved by meta-learning with adaptable features. The main contribution of the paper is to study the inductive bias of meta-learned neural network systems in the presence of selective sparsity. In particular, the authors show that sparse learning can lead to better generalization than sparse learning with sparse gradient descent.","This paper studies the problem of weight initialization for neural network weights on small datasets. The authors propose a new learning algorithm based on sparse learning. They show that the pattern of sparsity on the problem-byproblem basis is a generalization error on a few-shot and continual learning problems. They also show that this patterned sparsity does not depend on the size of the training set, but on the number of training samples, and that it depends on the learning rates of the weights. They further show that meta-learning is able to learn adaptable features with inductive bias, and show that selective sparsity leads to better generalization. "
16060,SP:05037e1850003a725a466b64d3e32aa2aed458fb,"shared response modeling HYPONYM-OF multi - view learning problem. multi - set canonical correlation analysis USED-FOR unmixing matrices. sampling noise USED-FOR Multiset CCA. joint diagonalization USED-FOR approach. joint diagonalization USED-FOR Multiset CCA. ShICA - ML HYPONYM-OF maximum - likelihood method. non - Gaussianity USED-FOR ShICA - J. maximum - likelihood method USED-FOR non - Gaussianity. maximum - likelihood method USED-FOR ShICA - J. second - order statistics USED-FOR ShICA - J. method USED-FOR shared components estimation. ShICA USED-FOR shared components estimation. ShICA USED-FOR method. ShICA COMPARE alternatives. alternatives COMPARE ShICA. fMRI and MEG datasets EVALUATE-FOR ShICA. OtherScientificTerm are common components, shared independent components, additive Gaussian noise, and noise variances. Method is Shared Independent Component Analysis ( ShICA ). Generic is model. ",This paper studies the problem of shared response modeling in multi-view learning problem. The authors propose a multi-set canonical correlation analysis for unmixing matrices with sampling noise. The approach is based on joint diagonalization between the common components and the shared independent components. The proposed model is called Shared Independent Component Analysis (ShICA). The authors show that the proposed method can achieve state-of-the-art performance on both fMRI and MEG datasets.   ,"This paper proposes a new multi-view learning problem called shared response modeling, which is an extension of multi-set canonical correlation analysis for unmixing matrices. The authors propose a new model called Shared Independent Component Analysis (ShICA), which is based on joint diagonalization of the common components and the shared independent components. Multiset CCA is defined by sampling noise from the sampling noise, and the authors propose to use additive Gaussian noise to reduce the noise variances. The proposed method is evaluated on fMRI and MEG datasets, and compared to other alternatives. ShICA is shown to outperform other methods for shared components estimation. The maximum-likelihood method, ShICA-ML, is used to improve the non-Gaussianity of the model by using second-order statistics."
16109,SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"self - play ( SP ) CONJUNCTION population play ( PP ). population play ( PP ) CONJUNCTION self - play ( SP ). multi - agent reinforcement learning techniques USED-FOR agents. population play ( PP ) HYPONYM-OF multi - agent reinforcement learning techniques. self - play ( SP ) HYPONYM-OF multi - agent reinforcement learning techniques. model USED-FOR human - aware ” agents. behavioral cloning play ” CONJUNCTION BCP. BCP CONJUNCTION behavioral cloning play ”. behavioral cloning play ” HYPONYM-OF human - aware ” agents. BCP HYPONYM-OF human - aware ” agents. behavioral cloning USED-FOR human model. generalization EVALUATE-FOR agents. agents USED-FOR human co - players. approach USED-FOR agents. generalization EVALUATE-FOR approach. multi - agent approaches USED-FOR competitive domains. self - play agents USED-FOR agent partner. FCP agents COMPARE SP. SP COMPARE FCP agents. SP CONJUNCTION PP. PP CONJUNCTION SP. PP CONJUNCTION BCP. BCP CONJUNCTION PP. FCP agents COMPARE BCP. BCP COMPARE FCP agents. FCP agents COMPARE PP. PP COMPARE FCP agents. Material is human data. Generic are it, and method. Method are Fictitious Co - Play ( FCP ), and two - player collaborative cooking simulator. OtherScientificTerm is subjective preference. ",This paper proposes a new multi-agent reinforcement learning technique called Fictitious Co-Play (FCP) that combines self-play (SP) with population play (PP) and behavioral cloning play (BCP) to learn agents that are “human-aware” agents such as BCP and “behavioral cloning play”. The authors propose a two-player collaborative cooking simulator and show that it can be used to train agents with human co-players. They also show that their approach improves the generalization of agents in competitive domains. ,"This paper proposes a new multi-agent reinforcement learning technique, called Fictitious Co-Play (FCP), which combines self-play (SP) with population play (PP) and behavioral cloning play (BCP) to improve the generalization performance of agents in competitive domains. The authors propose a model for “human-aware” agents (e.g., BCP, PP) and human-aware “agents” (i.e., SP, BCP). The authors show that the proposed approach improves generalization of agents to human co-players. They also show that it can be applied to competitive domains where the agent partner can be a human. The method is evaluated on a two-player collaborative cooking simulator, and it is shown that it outperforms FCP agents in terms of generalization. "
16158,SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"method USED-FOR cooperative multi - agent reinforcement learning. discrete and continuous action spaces FEATURE-OF cooperative multi - agent reinforcement learning. deep deterministic policy gradients USED-FOR policies. approach USED-FOR policies. MADDPG HYPONYM-OF multi - agent actor - critic method. deep deterministic policy gradients USED-FOR approach. QMIX HYPONYM-OF multi - agent Q - learning algorithm. FACMAC USED-FOR centralised but factored critic. per - agent utilities CONJUNCTION joint action - value function. joint action - value function CONJUNCTION per - agent utilities. joint action - value function FEATURE-OF centralised but factored critic. per - agent utilities PART-OF centralised but factored critic. non - linear monotonic function USED-FOR centralised but factored critic. non - linear monotonic function USED-FOR joint action - value function. it USED-FOR tasks. representational capacity USED-FOR it. monolithic, or monotonically factored critics USED-FOR tasks. joint action space FEATURE-OF centralised policy gradient estimator. centralised policy gradient estimator USED-FOR FACMAC. multi - agent MuJoCo benchmark CONJUNCTION StarCraft II micromanagement tasks. StarCraft II micromanagement tasks CONJUNCTION multi - agent MuJoCo benchmark. multi - agent particle environments CONJUNCTION multi - agent MuJoCo benchmark. multi - agent MuJoCo benchmark CONJUNCTION multi - agent particle environments. multi - agent MuJoCo benchmark EVALUATE-FOR FACMAC. multi - agent particle environments EVALUATE-FOR FACMAC. StarCraft II micromanagement tasks EVALUATE-FOR FACMAC. FACMAC COMPARE baselines. baselines COMPARE FACMAC. FACMAC COMPARE MADDPG. MADDPG COMPARE FACMAC. MADDPG COMPARE baselines. baselines COMPARE MADDPG. OtherScientificTerm are critic, and coordinated policy changes. Method are nonmonotonic factorisation, and centralised critic. ","This paper proposes a method for cooperative multi-agent reinforcement learning in discrete and continuous action spaces. The proposed approach is based on deep deterministic policy gradients to estimate the policies. The centralised policy gradient estimator is a non-linear monotonic function, and the centralised but factored critic is a joint action-value function that combines per-agent utilities and the joint action -value function. The authors show that the proposed MADDPG outperforms the state-of-the-art multi-actor actor-critic method (MADDPG) by a large margin. The main contribution of the paper is the use of nonmonotonic factorisation to improve the representation of the critic. The paper also shows that it can be applied to a variety of tasks with monolithic, or monotonically factored critics. ","This paper proposes a method for cooperative multi-agent reinforcement learning in discrete and continuous action spaces. The approach is based on deep deterministic policy gradients for policies. The centralised but factored critic is composed of per-agent utilities and a joint action-value function. The proposed method is called MADDPG, which is an actor-critic method with nonmonotonic factorisation. It is shown that it outperforms existing baselines on a number of tasks with monolithic, or monotonically factored critics. The authors also show that it has better representational capacity than other baselines, and that it is more robust to coordinated policy changes. Experiments are conducted on a single-agent Q-learning algorithm called QMIX, where the centralised critic is a non-linear monotonic function in the joint action space, and a centralised policy gradient estimator is used."
16207,SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,Hebbian plasticity USED-FOR storage. Hebbian plasticity CONJUNCTION attractor dynamics. attractor dynamics CONJUNCTION Hebbian plasticity. storage CONJUNCTION attractor dynamics. attractor dynamics CONJUNCTION storage. Hopfield networks PART-OF neuroscience. memory - augmented neural networks USED-FOR machine learning. key - value mechanism USED-FOR memory - augmented neural networks. augmented networks COMPARE variants. variants COMPARE augmented networks. three - factor plasticity rules USED-FOR basic key - value memory. network parameters USED-FOR rules. heteroassociative memory CONJUNCTION sequence learning. sequence learning CONJUNCTION heteroassociative memory. continual recall CONJUNCTION heteroassociative memory. heteroassociative memory CONJUNCTION continual recall. network COMPARE Hopfield networks. Hopfield networks COMPARE network. network USED-FOR continual recall. network USED-FOR heteroassociative memory. Hopfield networks USED-FOR autoassociative memory tasks. network USED-FOR sequence learning. autoassociative memory tasks EVALUATE-FOR network. Hopfield network USED-FOR model of biological long - term memory. ,"This paper proposes a new key-value mechanism for memory-augmented neural networks for machine learning. The key-values are learned using three-factor plasticity rules, which are based on Hebbian plasticity for storage and attractor dynamics for storage. These rules are learned by modifying the network parameters. The proposed network is evaluated on a variety of autoassociative memory tasks, including continual recall, continual recall with heteroscedastic memory, and sequence learning. Compared to existing variants of augmented networks, the proposed network performs better than Hopfield networks in most of these tasks. The authors also show that the proposed Hopfield network can be used as a model of biological long-term memory.","This paper proposes a key-value mechanism for memory-augmented neural networks for machine learning. The key idea is to use Hebbian plasticity for storage and attractor dynamics for storage. The authors propose three-factor plasticity rules for the basic key-values memory, which are based on the network parameters. The proposed network is evaluated on several autoassociative memory tasks (continual recall, heteroassociation memory, and sequence learning). The authors show that the proposed network outperforms Hopfield networks on continual recall and heteroasciative memory. The paper also presents a model of biological long-term memory using a Hopfield network."
16256,SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"Pairwise learning HYPONYM-OF learning tasks. bipartite ranking CONJUNCTION metric learning. metric learning CONJUNCTION bipartite ranking. It USED-FOR machine learning tasks. metric learning HYPONYM-OF machine learning tasks. bipartite ranking HYPONYM-OF machine learning tasks. approach USED-FOR streaming data. streaming data USED-FOR pairwise learning. online gradient descent ( OGD ) algorithm USED-FOR approach. stochastic and online gradient descent methods USED-FOR pairwise learning. optimization CONJUNCTION generalization error bounds. generalization error bounds CONJUNCTION optimization. stability results CONJUNCTION optimization. optimization CONJUNCTION stability results. generalization error bounds USED-FOR smooth and nonsmooth problems. generalization error bounds USED-FOR convex and nonconvex. convex and nonconvex CONJUNCTION smooth and nonsmooth problems. smooth and nonsmooth problems CONJUNCTION convex and nonconvex. optimization CONJUNCTION generalization analysis. generalization analysis CONJUNCTION optimization. techniques USED-FOR optimization. techniques USED-FOR generalization analysis. generalization bounds USED-FOR OGD. buffering set USED-FOR OGD. buffering set USED-FOR generalization bounds. algorithms USED-FOR differentially private SGD algorithms. differentially private SGD algorithms USED-FOR pairwise learning. stability analysis USED-FOR differentially private SGD algorithms. algorithms CONJUNCTION stability analysis. stability analysis CONJUNCTION algorithms. OtherScientificTerm are loss function, scalability issue, and gradient direction. Metric is storage and computational complexity. ","This paper proposes a new approach for streaming data for pairwise learning with stochastic and online gradient descent methods. It focuses on two machine learning tasks: bipartite ranking and metric learning. The proposed approach is based on the online gradient ascent (OGD) algorithm. Theoretical results show that OGD can achieve better generalization error bounds for convex and nonconvex problems as well as smooth and nonsmooth problems. The theoretical results also show that the loss function of OGD has a scalability issue, and that the gradient direction is not optimal. The paper also shows that the generalization bounds obtained by OGD on the buffering set are better than the ones obtained by differentially private SGD algorithms and other techniques for optimization and stability analysis. ","This paper proposes a new approach for streaming data for pairwise learning with stochastic and online gradient descent methods for machine learning tasks such as bipartite ranking and metric learning. The proposed approach is based on the Online gradient descent (OGD) algorithm, where the loss function is a scalability issue, and the gradient direction is a function of the storage and computational complexity. The authors provide generalization bounds for OGD on the buffering set and generalization error bounds for convex and nonsmooth problems, optimization and stability results, and techniques for optimization, stability analysis, and differentially private SGD algorithms are proposed to improve the generalization performance of OGD."
16305,SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"REDO HYPONYM-OF class - agnostic framework. class - agnostic framework USED-FOR Dynamic Objects. RGBD or calibrated videos USED-FOR REDO. RGBD or calibrated videos USED-FOR class - agnostic framework. rigid motion CONJUNCTION non - rigid motion. non - rigid motion CONJUNCTION rigid motion. non - rigid motion CONJUNCTION articulation. articulation CONJUNCTION non - rigid motion. occlusion CONJUNCTION camera settings. camera settings CONJUNCTION occlusion. unified framework USED-FOR problem setting. articulation HYPONYM-OF object dynamics. rigid motion HYPONYM-OF object dynamics. non - rigid motion HYPONYM-OF object dynamics. aggregated temporal visual cues USED-FOR canonical 4D implicit function. 4D transformation module USED-FOR object dynamics. REDO COMPARE dynamic reconstruction methods. dynamic reconstruction methods COMPARE REDO. Generic are modules, and component. Material is real - world video data 3DPW. ","This paper proposes REDO, a class-agnostic framework for Dynamic Objects based on RGBD or calibrated videos. The proposed modules are based on a unified framework for the problem setting. The core component is a canonical 4D implicit function based on aggregated temporal visual cues. The object dynamics, such as rigid motion, non-rigid motion, articulation, and occlusion, are modeled by a 4D transformation module. Experiments on real-world video data 3DPW show that REDO outperforms dynamic reconstruction methods.","This paper proposes REDO, a class-agnostic framework for Dynamic Objects, which is based on RGBD or calibrated videos. REDO consists of two modules: 1) a 4D transformation module that learns object dynamics such as rigid motion, non-rigid motion, articulation, and occlusion, 2) a unified framework for the problem setting. The first component learns a canonical 4D implicit function based on aggregated temporal visual cues. The second component is a real-world video data 3DPW. Experiments show that REDO outperforms other dynamic reconstruction methods."
16354,SP:8ae97752e74b4395774575009031abcb6ba5cea7,"fixed stepsize FEATURE-OF linear stochastic approximation ( LSA ) algorithms. methods USED-FOR machine learning tasks. high probability bounds USED-FOR LSA. covariance matrices PART-OF central limit theorems. Method are random estimates, polynomial concentration bounds, and Gaussian or exponential high probability bounds. OtherScientificTerm are products of matrices, stepsize, and random matrices. Generic is bounds. ","This paper studies the problem of linear stochastic approximation (LSA) algorithms with fixed stepsize. The authors consider the case where the products of matrices are random estimates and the stepsize is fixed. The central limit theorems are covariance matrices, and the authors show that random estimates have polynomial concentration bounds. They also provide high probability bounds for LSA with high stepsize, and show that these bounds can be extended to Gaussian or exponential high probability.  The authors also provide some theoretical guarantees for these bounds.  Finally, the authors provide some numerical experiments to demonstrate the effectiveness of the proposed methods in machine learning tasks.","This paper studies the fixed stepsize of linear stochastic approximation (LSA) algorithms. The authors propose two new methods for machine learning tasks that are based on high probability bounds on the covariance matrices of the central limit theorems. The main idea is to use random estimates of the products of matrices, and then to use polynomial concentration bounds to approximate the polynomials of these products. The proposed bounds are a generalization of Gaussian or exponential high-probability bounds. "
16403,SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"options framework USED-FOR temporal abstraction. options framework USED-FOR reinforcement learning. temporal abstraction USED-FOR reinforcement learning. discounted Markov decision processes ( MDPs ) CONJUNCTION average - reward MDPs. average - reward MDPs CONJUNCTION discounted Markov decision processes ( MDPs ). samplebased planning variants PART-OF learning algorithms. intra - option algorithms CONJUNCTION samplebased planning variants. samplebased planning variants CONJUNCTION intra - option algorithms. algorithms CONJUNCTION convergence proofs. convergence proofs CONJUNCTION algorithms. those USED-FOR algorithms. those USED-FOR convergence proofs. Four - Room domain EVALUATE-FOR algorithms. OtherScientificTerm is option - interrupting behavior. Method are discounted, and average - reward formulation. ","This paper proposes a new options framework for temporal abstraction in reinforcement learning. The authors propose discounted Markov decision processes (MDPs) and average-reward MDPs, which can be combined with samplebased planning variants in learning algorithms such as intra-option algorithms and samplebased planner variants. They show that those can achieve better convergence proofs in the Four-Room domain than existing algorithms. They also show that the option-interrupting behavior of the discounted MDP can be reduced to the average reward formulation.","The paper proposes an options framework for temporal abstraction in reinforcement learning. The authors propose discounted Markov decision processes (MDPs), average-reward MDPs, and samplebased planning variants in different learning algorithms. They show that those can achieve better convergence proofs than other algorithms in the Four-Room domain. They also show that the option-interrupting behavior can be reduced to a discounted, and the authors also propose a new algorithm based on the average -reward formulation."
16452,SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"Visual Transformers ( VTs ) COMPARE Convolutional networks ( CNNs ). Convolutional networks ( CNNs ) COMPARE Visual Transformers ( VTs ). CNNs COMPARE VTs. VTs COMPARE CNNs. VTs USED-FOR global relations between image elements. representation capacity FEATURE-OF they. models COMPARE common CNNs. common CNNs COMPARE models. local properties FEATURE-OF visual domain. local properties USED-FOR VTs. local properties PART-OF CNN architectural design. small training set regime FEATURE-OF robustness. robustness EVALUATE-FOR VTs. images USED-FOR auxiliary selfsupervised task. VTs USED-FOR spatial relations. task USED-FOR VT training. task USED-FOR VTs. task CONJUNCTION ( supervised ) training. ( supervised ) training CONJUNCTION task. it PART-OF VTs. accuracy EVALUATE-FOR VTs. VTs EVALUATE-FOR method. accuracy EVALUATE-FOR method. OtherScientificTerm are convolutional inductive bias, and computational overhead. Material is ImageNet. Method is VTs - Drloc. ","This paper studies the problem of learning representations that are robust to convolutional inductive bias. The authors propose to use Visual Transformers (VTs) instead of Convolutional networks (CNNs) to learn the global relations between image elements. They show that VTs have better representation capacity than CNNs in terms of their local properties in the visual domain. They also show that the robustness of VTs in the small training set regime is better than common CNNs. Finally, the authors propose an auxiliary selfsupervised task that uses images from the VTs to learn spatial relations between images. The proposed method is evaluated on ImageNet and shows that it achieves better accuracy than VTs on the task and (supervised) training.","This paper presents a new method for learning the global relations between image elements. The authors propose to use Visual Transformers (VTs) instead of Convolutional networks (CNNs) in order to reduce convolutional inductive bias. They show that VTs have better representation capacity than common CNNs in terms of the number of layers, and that they have better robustness in the small training set regime. They also show that the local properties of a CNN architectural design can be used to improve the robustness of VTs in the visual domain. Finally, the authors propose an auxiliary selfsupervised task to learn the spatial relations between images. The proposed method is called VTs-Drloc, and it consists of two steps: (1) a task for learning spatial relations, and (2) (supervised) training. The paper shows that the proposed method improves the accuracy of the VTs compared to other methods. "
16501,SP:0132ef17585e293b23e9dc45189c0989d829b52a,datasets USED-FOR Label - free alignment. Hyperbolic spaces USED-FOR informative representations of hierarchical data. geometric approach USED-FOR label - free alignment of hierarchical datasets. translation CONJUNCTION scaling. scaling CONJUNCTION translation. scaling CONJUNCTION rotation. rotation CONJUNCTION scaling. Riemannian geometry USED-FOR Lorentz model of hyperbolic space. Riemannian geometry USED-FOR HPA. theoretical properties CONJUNCTION stability. stability CONJUNCTION theoretical properties. stability CONJUNCTION computational efficiency. computational efficiency CONJUNCTION stability. theoretical properties EVALUATE-FOR HPA. gene expression and mass cytometry data FEATURE-OF batch correction tasks. batch correction tasks EVALUATE-FOR its. methods USED-FOR label - free alignment in hyperbolic spaces. data USED-FOR unsupervised batch effect removal. Method is hyperbolic Procrustes analysis ( HPA ). Generic is components. Task is alignment. OtherScientificTerm is hyperbolic spaces. ,"This paper proposes a geometric approach for label-free alignment of hierarchical datasets. The authors propose to use Hyperbolic spaces to learn informative representations of hierarchical data. The key idea is to use the Riemannian geometry of the Lorentz model of hyperbolic space as a basis for the HPA. The proposed HPA has theoretical properties such as stability, computational efficiency, and translation and scaling. Experiments on gene expression and mass cytometry data show its effectiveness on batch correction tasks. ","This paper proposes a geometric approach to label-free alignment of hierarchical datasets. Hyperbolic spaces are useful for learning informative representations of hierarchical data. The authors propose a hyperbolic Procrustes analysis (HPA), which is based on the Riemannian geometry of the Lorentz model of hyperbola space. The key components of the HPA are: (1) the rotation, (2) translation, (3) scaling, (4) stability, and (5) computational efficiency. Experiments are conducted on gene expression and mass cytometry data for batch correction tasks, where the authors show that the proposed HPA outperforms existing methods for label-based alignment in hyperboid spaces. The paper also shows that the data can be used for unsupervised batch effect removal."
16550,SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"Privacy - protected microdata USED-FOR differentially private algorithm. logarithmic factor FEATURE-OF accuracy. accuracy EVALUATE-FOR differentially private query answering systems. sum query CONJUNCTION point queries. point queries CONJUNCTION sum query. noisy answers USED-FOR sum query. noisy answers USED-FOR point queries. log(d ) factor COMPARE O(d ) factor. O(d ) factor COMPARE log(d ) factor. log(d ) factor USED-FOR point queries. O(d ) factor USED-FOR sum query. lower bounds USED-FOR pure, approximate, and concentrated differential privacy. Material are microdata, and benchmark datasets. Method are uncertainty principle, pure differential privacy, and mitigation strategies. OtherScientificTerm is microdata requirement. ","This paper studies the problem of differentially private query answering systems with a logarithmic factor of the accuracy. The authors consider the uncertainty principle, which is a well-studied problem in the literature. The main contribution of this paper is to propose a new algorithm based on privacy-protected microdata. The key idea is to use the log(d) factor for the sum query and the point queries, and to use noisy answers for the point query. Theoretical results show that the proposed lower bounds are better than the ones for pure, approximate, and concentrated differential privacy. ","This paper proposes a differentially private algorithm that uses privacy-protected microdata for answering queries. The authors show that the uncertainty principle of the proposed algorithm can be extended to the case of pure differential privacy. The paper also provides lower bounds for pure, approximate, and concentrated differential privacy, as well as mitigation strategies. The proposed algorithm is evaluated on two benchmark datasets, where the accuracy of the accuracy is measured by the logarithmic factor of the error of the query answering systems. The log(d) factor is used for sum query and point queries with noisy answers, and the O(d ) factor for point queries."
16599,SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"sparse reward CONJUNCTION inefficient exploration. inefficient exploration CONJUNCTION sparse reward. inefficient exploration FEATURE-OF long - horizon tasks. RL CONJUNCTION planning. planning CONJUNCTION RL. path - planner CONJUNCTION RL agent. RL agent CONJUNCTION path - planner. dense feedback USED-FOR curriculum of tree - structured sub - tasks. RL agent USED-FOR dense feedback. dense feedback USED-FOR path - planner. planner USED-FOR long - horizon task. easy - to - hard curriculum USED-FOR planner. bottom - up traversal of the tree USED-FOR RL agent. RL agent CONJUNCTION planner. planner CONJUNCTION RL agent. mutual training USED-FOR CO - PILOT. SAC CONJUNCTION PPO. PPO CONJUNCTION SAC. RL CONJUNCTION planning ( RRT *. planning ( RRT * CONJUNCTION RL. CO - PILOT COMPARE RL. RL COMPARE CO - PILOT. CO - PILOT COMPARE combination ( SoRB ). combination ( SoRB ) COMPARE CO - PILOT. navigation and continuous control tasks EVALUATE-FOR combination ( SoRB ). SAC HYPONYM-OF RL. navigation and continuous control tasks EVALUATE-FOR CO - PILOT. PPO HYPONYM-OF RL. success rate CONJUNCTION sample efficiency. sample efficiency CONJUNCTION success rate. sample efficiency EVALUATE-FOR CO - PILOT. success rate EVALUATE-FOR CO - PILOT. Method are Goal - conditioned reinforcement learning ( RL ), Planning, environment model, and planning policy. OtherScientificTerm are dense reward / guidance, tree of sub - tasks, sub - tasks, and RRT *. Generic is task. ","This paper studies Goal-conditioned reinforcement learning (RL) in the context of long-horizon tasks with sparse reward and inefficient exploration. The authors propose a curriculum of tree-structured sub-tasks where dense feedback is used to guide the path-planner and the RL agent to learn dense feedback. The planner is trained using an easy-to-hard curriculum, and the agent is trained with a bottom-up traversal of the tree. Planning, the environment model, is trained to learn the optimal planning policy for each sub-task.  The authors show that CO-PILOT outperforms SAC and PPO in both navigation and continuous control tasks, and outperforms RL and planning (RRT). The success rate and sample efficiency are also improved. ","This paper proposes Goal-conditioned reinforcement learning (RL), which is a method to learn a planning policy for long-horizon tasks with sparse reward and inefficient exploration. Planning is a curriculum of tree-structured sub-tasks with dense reward/guidance, where each sub-task has its own environment model, and the goal is to learn the environment model to guide the planning policy. The RL agent is trained with dense feedback from the environment, and a path-planner is learned from dense feedback. The planner is trained on a long-hierarchical task with an easy-to-hard curriculum, where the planner is learned using a bottom-up traversal of the tree. The planning policy is trained using mutual training. The authors show that CO-PILOT outperforms both RL and planning (RRT*) on both navigation and continuous control tasks, and outperforms the combination (SoRB) and the RL agent. The success rate and sample efficiency of CO-PIILOT are also compared to RL and PPO. "
16648,SP:9911693a04a300b5a93634fb0267ef83e5489d77,"black box explanations USED-FOR model credibility. techniques USED-FOR explanations. hyper - parameter tuning USED-FOR methods. Bayesian framework USED-FOR generating local explanations. LIME CONJUNCTION KernelSHAP. KernelSHAP CONJUNCTION LIME. credible intervals USED-FOR feature importances. real world datasets CONJUNCTION user studies. user studies CONJUNCTION real world datasets. OtherScientificTerm are local explanations, and feature importance. Generic are framework, and uncertainty. ",This paper proposes a Bayesian framework for generating local explanations for black box explanations for model credibility. The proposed framework is based on the idea that local explanations can be generated by hyper-parameter tuning. The authors propose two techniques for generating explanations based on these techniques. The first is to use LIME and KernelSHAP. The second is to learn the feature importances using credible intervals. The experimental results show the effectiveness of the proposed framework. ,"This paper proposes a Bayesian framework for generating local explanations for black box explanations for model credibility. The framework is based on Bayesian Bayes framework. The authors propose two techniques to generate explanations for explanations based on hyper-parameter tuning. The main idea is to use local explanations to reduce the uncertainty of the local explanations. The key idea of the framework is to consider the feature importance of the explanations, which is then used to estimate the feature importances based on credible intervals. Experiments on real world datasets and user studies show the effectiveness of LIME and KernelSHAP."
16697,SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"energy - efficient neural networks CONJUNCTION hardware accelerations. hardware accelerations CONJUNCTION energy - efficient neural networks. multiplications PART-OF convolutional neural networks ( CNNs ). Adder neural networks ( ANNs ) USED-FOR low energy cost. ANNs COMPARE CNNs. CNNs COMPARE ANNs. accuracy EVALUATE-FOR ANNs. accuracy EVALUATE-FOR CNNs. ANNs CONJUNCTION CNNs. CNNs CONJUNCTION ANNs. knowledge distillation HYPONYM-OF training tricks. filters CONJUNCTION features. features CONJUNCTION filters. similarity measurement FEATURE-OF features. similarity measurement FEATURE-OF filters. similarity measurement USED-FOR property difference. unordered heavy tails PART-OF ANNs. classification EVALUATE-FOR ANNs. feature distributions PART-OF loss function. method USED-FOR heavy tails. angle - based constraint USED-FOR diversity of tails. method USED-FOR ANNs. heavy tails PART-OF ANNs. angle - based constraint USED-FOR distribution parameters. classifier USED-FOR method. approach USED-FOR ANNs. benchmarks EVALUATE-FOR approach. benchmarks EVALUATE-FOR distributions. OtherScientificTerm are fatter tails, feature space, Multivariate Skew Laplace distributions, and ANN features. ","This paper studies the problem of energy-efficient neural networks and hardware accelerations with multiplications in convolutional neural networks (CNNs). The authors propose a new approach to reduce the low energy cost of ANNs by learning fatter tails in the feature space. The main idea is to use an angle-based constraint on the diversity of tails to encourage the distribution parameters of the weights to be more diverse. The authors show that the proposed method is able to learn heavy tails in ANNs with unordered heavy tails and unordered weights in CNNs.  The authors also show that their method can be used to train heavy tails with filters, features, and similarity measurement for property difference between the filters and the features. The method is evaluated on a variety of benchmarks and shows that their approach can improve the performance of ANN's on classification. ","This paper proposes a new approach to improve the performance of convolutional neural networks (CNNs) with multiplications in order to reduce the low energy cost. The authors propose two training tricks, namely knowledge distillation and the use of an angle-based constraint on the diversity of tails in the feature space. They show that the proposed method can improve the accuracy of ANNs and CNNs with unordered heavy tails in comparison to CNNs that have fatter tails. The proposed method is based on the idea of using feature distributions in the loss function, and the filters and features are computed by similarity measurement of the features and the property difference of the filters. The method is evaluated on three different benchmarks to evaluate the distributions of the ANNs. "
16746,SP:cbccb65457564992d534504c0d060da44cafce8c,"gradient descent phenomenon USED-FOR learning proclivity. learning proclivity FEATURE-OF over - parameterized neural networks. features USED-FOR task. feature imbalances FEATURE-OF neural networks. learning dynamics USED-FOR imbalance. learning dynamics USED-FOR gradient descent. guarantees USED-FOR regularization method. formalism USED-FOR regularization method. regularization method USED-FOR feature learning dynamics. formalism USED-FOR guarantees. accuracy EVALUATE-FOR regularization method. robustness EVALUATE-FOR regularization method. OtherScientificTerm are Gradient Starvation, predictive features, statistical structure, and gradient starvation. Metric is cross - entropy loss. Method is Dynamical Systems theory. ","This paper studies the gradient descent phenomenon in learning proclivity in over-parameterized neural networks with feature imbalances. The authors propose Gradient Starvation, a regularization method based on a formalism that guarantees that the learning dynamics of gradient descent can be used to improve the performance of the feature learning dynamics on a task with different features. The theoretical analysis is based on the Dynamical Systems theory. The main contribution of the paper is the theoretical analysis of the cross-entropy loss, which shows that the predictive features of the gradients are not independent of the statistical structure, and that gradient starvation can lead to the imbalance. The paper also provides theoretical guarantees on the accuracy and robustness of the proposed regularized method.",This paper studies the gradient descent phenomenon in learning proclivity of over-parameterized neural networks. The authors show that feature imbalances of neural networks with different learning dynamics are due to the imbalance between the predictive features and the features of the task. They propose a regularization method based on a formalism that guarantees that the feature learning dynamics of gradient descent will not be affected by the cross-entropy loss of Gradient Starvation. They show that the proposed regularisation method improves the accuracy and robustness of the proposed method. They also provide a theoretical analysis of the statistical structure of the gradient starvation. The paper is based on Dynamical Systems theory.
16795,SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"Deep reinforcement learning USED-FOR superhuman AI. superhuman AI USED-FOR competitive games. Go and StarCraft HYPONYM-OF competitive games. learning techniques USED-FOR AI teammate. AI teammate USED-FOR human - machine collaborative games. AI teammates COMPARE those. those COMPARE AI teammates. subjective metrics of trust EVALUATE-FOR those. objective team performance EVALUATE-FOR AI teammates. AI agents PART-OF cooperative card game Hanabi. interpretability CONJUNCTION trust. trust CONJUNCTION interpretability. teamwork CONJUNCTION interpretability. interpretability CONJUNCTION teamwork. interpretability HYPONYM-OF subjective measures. trust HYPONYM-OF subjective measures. teamwork HYPONYM-OF subjective measures. AI design CONJUNCTION reinforcement learning benchmarking. reinforcement learning benchmarking CONJUNCTION AI design. subjective metrics of human - AI teaming CONJUNCTION objective task performance. objective task performance CONJUNCTION subjective metrics of human - AI teaming. Method are rule - based and learning - based agents, rule - based AI teammate ( SmartBot ), and learning - based agent. OtherScientificTerm are game score, and human - AI team performance. Metric is subjective metrics. ","This paper studies the problem of superhuman AI in competitive games with superhuman AI. The authors propose to use deep reinforcement learning to train a superhuman AI teammate in human-machine collaborative games such as Go and StarCraft. The AI teammate is trained using a combination of rule-based and learning-based agents. The learning techniques are used to train the AI teammate using different learning techniques. The goal is to improve the performance of the AI teammates compared to those trained with other AI teammates. The paper shows that the objective team performance of AI teammates is better than those trained using other subjective metrics such as interpretability, trust, and teamwork. ","This paper proposes a new way to train superhuman AI for competitive games using Deep reinforcement learning. The idea is to use learning techniques to train an AI teammate to play human-machine collaborative games such as Go and StarCraft. The authors propose to use rule-based and learning-based agents to train the AI teammate (SmartBot). The authors show that AI teammates are more trust-efficient than those trained with other learning techniques. They also show that human-AI team performance is better than other subjective metrics such as interpretability, trust, teamwork, and objective team performance. Finally, the authors conduct experiments on cooperative card game Hanabi, where AI agents are trained to play the game score. The results show that the proposed AI design and reinforcement learning benchmarking outperforms other AI design."
16844,SP:2a05e333fc1a14057515ef3addde9a40152373db,"visual question generation ( VQG ) USED-FOR human - like neural questions. image CONJUNCTION side information. side information CONJUNCTION image. side information USED-FOR human - like neural questions. image USED-FOR human - like neural questions. double visual and answer hints USED-FOR model. rule - based similarity matching method USED-FOR candidate visual hints. learning approach USED-FOR double - hints based VQG. weakly supervised learning problem USED-FOR learning approach. benchmark datasets EVALUATE-FOR method. automatic machine metrics CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic machine metrics. method COMPARE approaches. approaches COMPARE method. benchmark datasets EVALUATE-FOR approaches. automatic machine metrics HYPONYM-OF metrics. human evaluation HYPONYM-OF metrics. human evaluation EVALUATE-FOR method. metrics EVALUATE-FOR approaches. metrics EVALUATE-FOR method. automatic machine metrics EVALUATE-FOR method. OtherScientificTerm are uninformative and non - referential questions, visual hints, salient visual regions of interest, predicted salient visual regions of interest, and quality of predicted visual hints. Generic is they. Method is generation procedure. ","This paper proposes a new method for visual question generation (VQG) for human-like neural questions with image and side information. The model uses double visual and answer hints to train a model that can generate both uninformative and non-referential questions. The learning approach is based on a weakly supervised learning problem, where the candidate visual hints are generated by a rule-based similarity matching method. The proposed method is evaluated on several benchmark datasets and compared with other approaches on a variety of metrics such as automatic machine metrics and human evaluation.","This paper proposes a new method for visual question generation (VQG) for human-like neural questions with image and side information. The authors propose a learning approach for double-hints based VQG with double visual and answer hints. The idea is to use a rule-based similarity matching method to predict candidate visual hints from a set of uninformative and non-referential questions, and then use a weakly supervised learning problem to train the model. The proposed method is evaluated on three benchmark datasets and compared to other approaches on three metrics: automatic machine metrics, human evaluation, and the quality of predicted visual hints. "
16908,SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"Label noise CONJUNCTION class imbalance. class imbalance CONJUNCTION Label noise. label noise CONJUNCTION class imbalance. class imbalance CONJUNCTION label noise. Generalized Data Weighting ( GDW ) USED-FOR class imbalance. Generalized Data Weighting ( GDW ) USED-FOR label noise. class level FEATURE-OF gradients. gradients USED-FOR Generalized Data Weighting ( GDW ). GDW USED-FOR loss gradient. chain rule USED-FOR GDW. GDW COMPARE instance weighting methods. instance weighting methods COMPARE GDW. GDW USED-FOR class - level weights. computational cost EVALUATE-FOR instance weighting methods. gradient descent step USED-FOR class - level weights. class - level weights USED-FOR GDW. gradient descent step USED-FOR GDW. GDW COMPARE state - of - the - art methods. state - of - the - art methods COMPARE GDW. uniform noise setting FEATURE-OF CIFAR10. uniform noise setting EVALUATE-FOR GDW. CIFAR10 EVALUATE-FOR GDW. Material are real - world datasets, and clean and unbiased data. Generic is methods. OtherScientificTerm are class - level information, class - level gradients, and intermediate gradients. ","This paper studies the problem of label noise and class imbalance in the context of GDW. The authors propose a new method called Generalized Data Weighting (GDW) to address the class imbalance and label noise. GDW uses gradient descent step to learn the class-level weights for GDW, and then uses a chain rule to compute the loss gradient. The paper shows that GDW achieves better computational cost than instance weighting methods in the uniform noise setting of CIFAR10. ","This paper proposes a new loss function for class-level information. The authors propose to use gradient descent to reduce the computational cost of the loss function. The proposed loss function is based on the chain rule, and the authors show that the proposed method can be applied to both real-world datasets and synthetic data. "
16972,SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"it USED-FOR embodied agents. language USED-FOR embodied agents. sensorimotor modalities USED-FOR language. embodied agent FEATURE-OF spatio - temporal descriptions of behavioral traces. time - extended predicates CONJUNCTION spatio - temporal references. spatio - temporal references CONJUNCTION time - extended predicates. time - extended predicates PART-OF descriptions. spatio - temporal references PART-OF descriptions. architectural biases USED-FOR task. attention computations USED-FOR latter. multimodal Transformer architectures HYPONYM-OF models. generalization CONJUNCTION generalization. generalization CONJUNCTION generalization. generalization EVALUATE-FOR models. randomly held - out sentences USED-FOR generalization. generalization EVALUATE-FOR models. grammar primitives USED-FOR generalization. generalization HYPONYM-OF generalization. generalization HYPONYM-OF generalization. object identity FEATURE-OF attention computation. attention computation PART-OF Transformers. object identity USED-FOR generalization. code CONJUNCTION pretrained models. pretrained models CONJUNCTION code. OtherScientificTerm are Language, grounded language, spatio - temporal linguistic concepts, and truth function. Task are spatio - temporal language grounding task, and language - guided autonomous embodied agents. ","This paper studies the spatio-temporal language grounding task, where the goal is to learn a language that can be used by embodied agents to communicate with sensorimotor modalities. Language is a grounded language, and the goal of the task is to find a set of words that are useful for communicating with the embodied agent. The task is formulated as the task of learning a language to describe the behavior of an embodied agent in the presence of behavioral traces. The authors propose two models, multimodal Transformer architectures, which are based on attention computations to learn the latter. The proposed models are evaluated on a variety of tasks, and show that the proposed models achieve better generalization than the state-of-the-art in terms of randomly held-out sentences and grammar primitives. The attention computation in Transformers is based on object identity, and it is shown that the attention computation improves the generalization of Transformers.","This paper proposes a language-guided language grounding task for learning embodied agents. Language is a grounded language with sensorimotor modalities that can be used to guide the language of an embodied agent to generate spatio-temporal descriptions of behavioral traces. The task is formulated as an extension of the spatio -temporal linguistic concepts, where the descriptions are composed of time-extended predicates, spatio-, temporal references, and time-long predicates. The authors propose two models, multimodal Transformer architectures with attention computations for the latter. The proposed models are evaluated on three tasks: generalization, generalization with randomly held-out sentences, and generalization using grammar primitives. Transformers are evaluated using attention computation in the form of object identity. The results show that Transformers are able to generalize better than code and pretrained models. "
17036,SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"detecting CONJUNCTION tracking. tracking CONJUNCTION detecting. tracking USED-FOR Multiple object tracking and segmentation. detecting USED-FOR Multiple object tracking and segmentation. single frame predictions USED-FOR segmentation mask. temporal dimension USED-FOR association problem. approaches USED-FOR association problem. temporal dimension USED-FOR approaches. Prototypical Cross - Attention Network ( PCAN ) USED-FOR online multiple object tracking and segmentation. rich spatio - temporal information USED-FOR online multiple object tracking and segmentation. rich spatio - temporal information USED-FOR Prototypical Cross - Attention Network ( PCAN ). cross - attention USED-FOR rich information. cross - attention USED-FOR PCAN. prototypical appearance module USED-FOR contrastive foreground and background prototypes. PCAN USED-FOR contrastive foreground and background prototypes. prototypical appearance module USED-FOR PCAN. PCAN COMPARE video instance tracking and segmentation competition winners. video instance tracking and segmentation competition winners COMPARE PCAN. Youtube - VIS and BDD100 K datasets EVALUATE-FOR video instance tracking and segmentation competition winners. Youtube - VIS and BDD100 K datasets EVALUATE-FOR PCAN. OtherScientificTerm are space - time memory, and prototypes. ","This paper proposes Prototypical Cross-Attention Network (PCAN) for online multiple object tracking and segmentation using rich spatio-temporal information. PCAN is based on single frame predictions for the segmentation mask. The authors show that the cross-attention is able to capture the rich information in the space-time memory, which is useful for the task of multi-object tracking. They also show that PCAN can learn contrastive foreground and background prototypes using a prototypical appearance module. ",This paper proposes a Prototypical Cross-Attention Network (PCAN) for online multiple object tracking and segmentation. The authors propose to use rich spatio-temporal information for the multi-object tracking problem. The proposed approach is based on single frame predictions for the segmentation mask.  The authors show that PCAN is able to learn contrastive foreground and background prototypes with a prototypical appearance module.  PCAN outperforms video instance tracking and BDD100 K datasets. 
17100,SP:1175ad16382b349ab1a39895150172d266abe571,optimization USED-FOR deep learning. it USED-FOR gradient descent. approximate numerical solution USED-FOR initial value problem of gradient flow. curvature FEATURE-OF gradient flow trajectory. gradient descent USED-FOR initial value problem of gradient flow. gradient descent USED-FOR approximate numerical solution. homogeneous activations FEATURE-OF deep neural networks. favorable curvature FEATURE-OF gradient flow trajectories. gradient descent USED-FOR they. gradient descent USED-FOR global minimum. deep linear neural networks USED-FOR gradient flow. random initialization USED-FOR gradient descent. gradient descent COMPARE gradient flow. gradient flow COMPARE gradient descent. deep neural networks USED-FOR gradient descent. step size FEATURE-OF gradient descent. OtherScientificTerm is Gradient flow. Metric is computational efficiency. Method is gradient flows. ,"This paper studies the optimization of gradient flow in deep learning. The authors show that gradient flow has a favorable curvature of the gradient flow trajectory, and that it can be used as an approximate numerical solution for the initial value problem of gradient gradient flow. Gradient flow has homogeneous activations in deep neural networks, and they can be optimized using gradient descent. The paper also shows that gradient descent can achieve a global minimum using random initialization. Finally, the paper shows how gradient flow can be efficiently optimized using deep linear neural networks. ","This paper proposes a new optimization method for deep learning. The main idea is to use gradient descent as an approximate numerical solution for the initial value problem of gradient flow. Gradient flow is defined as the curvature of the gradient flow trajectory with respect to the homogeneous activations of deep neural networks. The authors show that gradient flow can be approximated by deep linear neural networks, and they show that it is equivalent to gradient descent for gradient descent with random initialization. They also show that they can approximate the global minimum of gradient descent using gradient descent. They show that the gradient flows are equivalent to the gradient descent in terms of step size, and that the computational efficiency is better than gradient flow, and the generalization of gradient flows is better."
17164,SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"multi - armed bandits USED-FOR delayed and longterm impact of actions. action history USED-FOR learning. regret EVALUATE-FOR algorithm. OtherScientificTerm are delayed impact of actions, arm rewards, feedback loop, and delayed impacts of historical actions. Generic are setting, and techniques. Task is bandit setting. Metric is matching regret lower bound. Method are bandit literature, and fair algorithms. ","This paper studies the problem of multi-armed bandits with delayed and longterm impact of actions. The setting is a bandit setting, where the goal is to maximize the matching regret lower bound. The authors consider the setting where the action history is not available for learning, and the goal of the bandit literature is to learn a fair algorithms. The algorithm is based on a feedback loop, and is able to learn delayed impacts of historical actions. ","This paper proposes a novel method for learning multi-armed bandits with delayed and longterm impact of actions. The authors propose a novel setting where the delayed impact of the actions is a function of the action history, and the long term impact is an action history of the arm rewards. This setting is similar to the bandit setting, but the authors propose two techniques to deal with the delayed impacts of historical actions: (1) a matching regret lower bound, and (2) a fair algorithms. The proposed algorithm is evaluated on the regret of the algorithm, and it is shown that the algorithm achieves better regret than fair algorithms in terms of regret."
17228,SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"end - to - end solution USED-FOR video instance segmentation ( VIS ). transformers USED-FOR end - to - end solution. per - clip pipeline COMPARE per - frame methods. per - frame methods COMPARE per - clip pipeline. per - clip models USED-FOR frame - to - frame communications. benchmark sets EVALUATE-FOR method. method USED-FOR near - online inference. Method are Inter - frame Communication Transformers ( IFC ), and offline inference. OtherScientificTerm are overhead, concise memory tokens, and features. Material is YouTube - VIS 2019 val set. ",This paper proposes a new end-to-end solution for video instance segmentation (VIS) using transformers. The proposed method is based on Inter-frame Communication Transformers (IFC) and is able to perform near-online inference without overhead. The authors show that the proposed per-clip pipeline performs better than per-frame methods in terms of performance on several benchmark sets. ,"This paper proposes an end-to-end solution for video instance segmentation (VIS) using transformers. The idea is to use Inter-frame Communication Transformers (IFC) to perform both offline inference and near-online inference. The authors show that the per-clip pipeline outperforms per-frame methods in terms of overhead, concise memory tokens, and features. The proposed method is evaluated on three benchmark sets. "
17292,SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"vector - space representation USED-FOR machine learning applications. vector - space representation USED-FOR graph analysis. graph analysis CONJUNCTION machine learning applications. machine learning applications CONJUNCTION graph analysis. Graph embedding USED-FOR vector - space representation. Graph embedding USED-FOR graph. sampling of context nodes USED-FOR graph embedding methods. random walks USED-FOR sampling of context nodes. random walks HYPONYM-OF biased sampler. degree FEATURE-OF random walks. residual2vec HYPONYM-OF graph embedding method. random walks ’ bias USED-FOR graph embedding. random graphs USED-FOR residual2vec. link prediction CONJUNCTION clustering. clustering CONJUNCTION link prediction. debiasing USED-FOR structural properties. structural properties PART-OF graph embedding. clustering EVALUATE-FOR debiasing. link prediction EVALUATE-FOR debiasing. OtherScientificTerm are structural properties of graphs, node, and structural biases in graphs. Task is graph representation learning. ","This paper proposes a new graph embedding method called residual2vec, which is based on the random walks’ bias to learn the structural properties of graphs. The authors show that the sampling of context nodes from random walks can be used as a biased sampler to train the embedding methods. The paper also shows that the degree of random walks in random walks has a significant impact on the performance of the proposed method. ","This paper proposes a new graph embedding method called residual2vec, which is based on random walks’ bias. The authors propose a biased sampler, where random walks are sampled from a set of context nodes, and a vector-space representation of the graph is used for graph analysis and machine learning applications. Graph embedding is used to represent the graph. The structural properties of graphs are learned by debiasing the structural properties in graphs, and the authors show that the proposed method is able to learn structural biases in graphs. The paper also shows that the random walks can be used to learn the degree of random walks in a graph. Experiments are conducted on link prediction and clustering, showing that the method can learn structural properties. "
17356,SP:851eac96135b577a5014166edcb43db6a190cf4b,"local differential privacy FEATURE-OF estimating non - linear functionals of discrete distributions. quadratic risk USED-FOR power sum functional. plug - in type estimators COMPARE MLE. MLE COMPARE plug - in type estimators. two - step procedure USED-FOR sequentially interactive case. α - LDP mechanisms CONJUNCTION estimators. estimators CONJUNCTION α - LDP mechanisms. private samples USED-FOR estimators. OtherScientificTerm are discrete distribution, non - interactive case, and privacy constraint. Method are privacy mechanisms ( PM ), multinomial model, and Gaussian model. ","This paper studies the problem of estimating non-linear functionals of discrete distributions with local differential privacy. The authors consider the non-interactive case, where the discrete distribution is a multinomial model and the power sum functional is a quadratic risk. They show that plug-in type estimators (e.g., MLE) are more sensitive to private samples than estimators based on α-LDP mechanisms. They also show that the privacy mechanisms (PM) can be applied to the non interactive case as well. Finally, they propose a two-step procedure to solve the sequentially interactive case. ","This paper studies the problem of estimating non-linear functionals of discrete distributions with local differential privacy. The authors propose privacy mechanisms (PM), a multinomial model where the discrete distribution is represented by a Gaussian model. The privacy constraint is based on the quadratic risk of the power sum functional in the non-interactive case. The paper shows that plug-in type estimators (e.g., MLE) outperform MLE with private samples, but not estimators with public private samples. In the sequentially interactive case, the authors propose a two-step procedure to solve the problem. "
17420,SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"directed graph USED-FOR learner ’s feedback. filtering CONJUNCTION label efficient classification. label efficient classification CONJUNCTION filtering. feedback graphs USED-FOR applications. label efficient classification HYPONYM-OF applications. filtering HYPONYM-OF applications. GAPPLETRON HYPONYM-OF online multiclass algorithm. arbitrary feedback graphs USED-FOR online multiclass algorithm. surrogate regret bounds USED-FOR algorithm. domination number HYPONYM-OF graph - theoretic parameter. full information case FEATURE-OF GAPPLETRON. surrogate regret EVALUATE-FOR GAPPLETRON. synthetic data EVALUATE-FOR algorithm. algorithm COMPARE baselines. baselines COMPARE algorithm. synthetic data EVALUATE-FOR baselines. feedback graphs EVALUATE-FOR algorithm. Task is online multiclass classification. OtherScientificTerm are bandit feedback, surrogate losses, prediction space, and time horizon. Generic are bounds, lower bound, and upper bounds. ","This paper studies the problem of online multiclass classification with directed graph. The authors propose a new algorithm called GAPPLETRON, which uses arbitrary feedback graphs to learn the learner’s feedback. They show that the surrogate regret bounds for the proposed algorithm can be derived from the graph-theoretic parameter known as the domination number. They also provide a lower bound for the surrogate losses. Finally, the authors show that their algorithm performs better on synthetic data than other baselines.","This paper proposes an online multiclass algorithm called GAPPLETRON, which is based on a directed graph for learning learner’s feedback. The proposed algorithm has surrogate regret bounds on the prediction space, which are based on the domination number of the graph-theoretic parameter. The lower bound is a linear combination of the upper bounds. The algorithm is evaluated on synthetic data and two applications: filtering and label efficient classification. "
17484,SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"threshold cut USED-FOR single dimension ( feature ). decision tree USED-FOR it. decision tree USED-FOR k - clustering. algorithm USED-FOR explainable clustering. O(k ) CONJUNCTION O(k ). O(k ) CONJUNCTION O(k ). Ω(k ) lower bound USED-FOR k - means. Ω(log k ) lower bound USED-FOR k - medians. Ω(log k ) lower bound CONJUNCTION Ω(k ) lower bound. Ω(k ) lower bound CONJUNCTION Ω(log k ) lower bound. upper bounds COMPARE O(k ). O(k ) COMPARE upper bounds. O(k ) HYPONYM-OF upper bounds. OtherScientificTerm are cluster, k - means objective, upper and lower bounds, and ` p - norms. Metric is k - medians objective. ","This paper proposes a new threshold cut for single dimension (feature) clustering. The key idea is to use a decision tree for k-clustering, where each feature is represented as a cluster and each cluster is represented by a k-means objective. The authors propose a new algorithm for explainable clustering based on this algorithm. The upper and lower bounds are based on the Ω(log k) lower bound on the k-medians objective, and the upper bounds are O(k) and O(K). ","This paper proposes an algorithm for explainable clustering with a threshold cut on the single dimension (feature). The key idea is to use a decision tree for k-clustering, where each cluster is represented by a k-means objective. The k-medians objective is defined as the sum of the upper and lower bounds of the k-mean objective, and the Ω(log k) lower bound for the k - medians. The upper bounds are O(k) and O(K), while the lower bounds are Ψ(k), O(log) and Ω(-k) lower bounds. The authors show that the upper bounds outperform the lower ones in terms of the p-norm."
17548,SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"pre - trained language model ( PrLM ) USED-FOR downstream natural language processing tasks. multilingual PrLM USED-FOR limited resources. language universality USED-FOR limited resources. limited resources USED-FOR low - resource languages. multilingual PrLM USED-FOR downstream natural language processing tasks. language universality USED-FOR multilingual PrLM. plain text USED-FOR multilingual PrLMs. monolingual linguistic structure knowledge USED-FOR PrLMs. explicit universal dependency parsing CONJUNCTION implicit language modeling. implicit language modeling CONJUNCTION explicit universal dependency parsing. multilingual PrLM USED-FOR explicit universal dependency parsing. multilingual PrLM USED-FOR implicit language modeling. learned representation USED-FOR model. universal dependency parse FEATURE-OF Syntax. model COMPARE multilingual PrLM. multilingual PrLM COMPARE model. model COMPARE multilingual - BERT. multilingual - BERT COMPARE model. model COMPARE approach. approach COMPARE model. linguistic structure parsing datasets EVALUATE-FOR multilingual PrLM. linguistic structure parsing datasets EVALUATE-FOR model. multilingual - BERT HYPONYM-OF multilingual PrLM. OtherScientificTerm are universal linguistic structure clues, and PrLM interpretability. ","This paper proposes a pre-trained language model (PrLM) for downstream natural language processing tasks with limited resources. The multilingual PrLM uses language universality to learn limited resources for low-resource languages. The model is based on multilingual-BERT, which is a learned representation of the language. The authors show that the model is able to perform better than multilingual prLM in terms of universal linguistic structure clues and PrLM interpretability. The paper also provides a theoretical analysis of the monolingual linguistic structure knowledge of PrLMs. ","This paper proposes a pre-trained language model (PrLM) for downstream natural language processing tasks. The multilingual PrLM uses language universality to learn limited resources for low-resource languages. PrLMs are trained with monolingual linguistic structure knowledge. The model is trained with a learned representation of the universal linguistic structure clues, and the model is evaluated on Syntax with universal dependency parse. The proposed model outperforms multilingual-BERT on several linguistic structure parsing datasets. "
17612,SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"deep architecture USED-FOR vehicle routing problems ( VRPs ). Transformer HYPONYM-OF deep architecture. Transformer USED-FOR vehicle routing problems ( VRPs ). positional encoding ( PE ) method USED-FOR representing VRP solutions. learning improvement models USED-FOR VRP. it USED-FOR learning improvement models. Dual - Aspect Collaborative Transformer ( DACT ) USED-FOR embeddings. embeddings USED-FOR node and positional features. Transformer USED-FOR symmetry of VRP solutions. cyclic positional encoding ( CPE ) method USED-FOR Transformer. cyclic sequences HYPONYM-OF symmetry of VRP solutions. cyclic positional encoding ( CPE ) method USED-FOR positional features. curriculum learning strategy USED-FOR sample efficiency. Proximal Policy Optimization USED-FOR DACT. traveling salesman problem ( TSP ) CONJUNCTION capacitated vehicle routing problem ( CVRP ). capacitated vehicle routing problem ( CVRP ) CONJUNCTION traveling salesman problem ( TSP ). DACT USED-FOR capacitated vehicle routing problem ( CVRP ). DACT USED-FOR traveling salesman problem ( TSP ). DACT COMPARE Transformer based improvement models. Transformer based improvement models COMPARE DACT. synthetic and benchmark instances EVALUATE-FOR DACT. OtherScientificTerm are VRP solutions, and incompatible correlations. Generic are them, and ones. ","This paper proposes a deep architecture for vehicle routing problems (VRPs) called Transformer, which is based on the Dual-Aspect Collaborative Transformer (DACT). The Transformer is an extension of the positional encoding (PE) method for representing VRP solutions, and it can be used to train learning improvement models for VRP. The key idea is to learn embeddings for both node and positional features from the Transformer. The proposed Transformer learns the symmetry of VRP solution (e.g., cyclic sequences) by using cyclic positional encoding as a way to encode the positional features. The authors also propose a curriculum learning strategy to improve sample efficiency. Proximal Policy Optimization is used to optimize the DACT. Experiments on synthetic and benchmark instances show that DACT outperforms Transformer based improvement models. ",This paper proposes a deep architecture for vehicle routing problems (VRPs) called Transformer. Transformer is an extension of the positional encoding (PE) method for representing VRP solutions. The authors propose Dual-Aspect Collaborative Transformer (DACT) to learn embeddings for both node and positional features. The proposed Transformer can be used to encode the symmetry of VRP solution (e.g. cyclic sequences) and to learn them. DACT is evaluated on both synthetic and benchmark instances. The results show that DACT outperforms Transformer based improvement models on the traveling salesman problem (TSP) and the capacitated vehicle routing problem (CVRP). The authors also show that the proposed curriculum learning strategy can improve sample efficiency and reduce incompatible correlations.
17676,SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"Bayes error FEATURE-OF generative models. normalizing flows USED-FOR generative models. invertible transformation USED-FOR Bayes error. it USED-FOR Gaussian base distributions. Bayes error EVALUATE-FOR flow models. Holmes - Diaconis - Ross integration USED-FOR it. it USED-FOR Bayes error. synthetic datasets COMPARE benchmark datasets. benchmark datasets COMPARE synthetic datasets. Bayes error FEATURE-OF synthetic datasets. approach USED-FOR classification models. method USED-FOR benchmark datasets. Task is data - driven classification problem. Metric is classification error. OtherScientificTerm are data distribution, and intractable quantity. Generic are technique, and models. ","This paper studies the Bayes error of generative models trained with normalizing flows. The authors consider the data-driven classification problem, where the data distribution is unknown and the goal is to estimate the classification error. In this setting, the authors propose a technique called Holmes-Diaconis-Ross integration, which is based on the invertible transformation. They show that it is able to recover Gaussian base distributions, and it is also able to reduce the Bayesian error of flow models. They also show that the proposed approach can improve the performance of classification models on synthetic datasets and benchmark datasets. ","This paper studies the Bayes error of generative models trained with normalizing flows. The authors propose a new technique, called Holmes-Diaconis-Ross integration (HDSR), which is based on invertible transformation. They show that it can be applied to Gaussian base distributions. They also show that the proposed method can be used on synthetic datasets as well as benchmark datasets. Finally, they evaluate the proposed approach on classification models. "
17740,SP:2896679f0472522bc3334178cd7574494cf12b7b,"language modeling CONJUNCTION computer vision. computer vision CONJUNCTION language modeling. neural architectures USED-FOR language modeling. neural architectures USED-FOR computer vision. hyper - parameter choices CONJUNCTION training instability. training instability CONJUNCTION hyper - parameter choices. automated and architecture agnostic method USED-FOR initializing neural networks. GradInit HYPONYM-OF automated and architecture agnostic method. GradInit USED-FOR initializing neural networks. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. norm FEATURE-OF network layer. heuristic USED-FOR GradInit. numerical scheme USED-FOR variables. GradInit USED-FOR convolutional architectures. skip connections USED-FOR convolutional architectures. learning rates CONJUNCTION momentum coefficients. momentum coefficients CONJUNCTION learning rates. Adam CONJUNCTION SGD. SGD CONJUNCTION Adam. Transformer architecture USED-FOR machine translation. It USED-FOR Transformer architecture. learning rate warmup FEATURE-OF it. Adam USED-FOR learning rate warmup. SGD USED-FOR learning rate warmup. SGD USED-FOR it. Adam USED-FOR it. Generic are architectures, and schemes. OtherScientificTerm are network parameters, hyperparameters, scalar multiplier variable, and normalization layers. Method are architecture - specific initialization schemes, and neural networks. ","This paper proposes GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a heuristic, where the network parameters are computed by a scalar multiplier variable, and the hyperparameters are learned by normalizing the network layer with a norm. The authors show that GradInit can be used to train convolutional architectures with skip connections, and that it can be combined with SGD, Adam, and SGD for learning rate warmup. It is also used for machine translation with a Transformer architecture.","This paper proposes an automated and architecture agnostic method for initializing neural networks. The authors propose GradInit, an extension of GradInit to the problem of learning neural architectures for language modeling and computer vision. GradInit is based on a heuristic, where the network parameters are learned by minimizing a scalar multiplier variable, and the hyperparameters of the network layer are learned via hyper-parameter choices and training instability. The main difference between the two architectures is that GradInit uses skip connections instead of normalization layers, and it uses SGD instead of Adam for learning rate warmup and SGD for it for machine translation.  The authors also propose two architecture-specific initialization schemes for learning neural networks, where they use a numerical scheme to learn the variables. "
17804,SP:f69731403592fa5bdd4ca327708582d615aa131c,"Linear mixed - effect models USED-FOR disease progression. interpretable parameters USED-FOR subject trajectories. diffeomorphism USED-FOR Euclidean metric. diffeomorphism USED-FOR metric. reproducible kernel Hilbert space FEATURE-OF radial basis functions. radial basis functions USED-FOR diffeomorphism. metric update USED-FOR forecasting of imaging and clinical biomarkers. TADPOLE challenge EVALUATE-FOR methods. Material is longitudinal data. Method are interpretable models, and ADNI. OtherScientificTerm are progression profiles, Riemannian manifold, patient - specific trajectories, and central geodesic. Generic is approach. Metric is interpretability. Task is Neural Information Processing Systems. ","This paper studies the problem of interpretability of linear mixed-effect models for disease progression in longitudinal data. The authors propose a new metric based on diffeomorphism for the Euclidean metric, based on the reproducible kernel Hilbert space. The main idea is to use interpretable parameters to represent the subject trajectories in the Riemannian manifold, and then use the interpretable models to predict the progression profiles. The proposed approach is based on ADNI, and the authors show that the proposed metric update improves the forecasting of imaging and clinical biomarkers in the TADPOLE challenge. ","This paper proposes a new metric for predicting disease progression based on linear mixed-effect models for disease progression. The proposed metric is based on diffeomorphism in the Euclidean metric of the Riemannian manifold. The main idea of the proposed approach is to use interpretable parameters to predict subject trajectories from interpretable models. The authors show that the radial basis functions in the reproducible kernel Hilbert space can be used to estimate the diffeomorphicism of the patient-specific trajectories. They also provide a metric update for the forecasting of imaging and clinical biomarkers based on the proposed metric update. Experiments are conducted on longitudinal data and on the TADPOLE challenge, and the proposed methods are shown to outperform existing methods in terms of interpretability."
17868,SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"routing - by - memory mechanism USED-FOR CNN architectures. memory head CONJUNCTION procedure. procedure CONJUNCTION memory head. memory head PART-OF PU. procedure PART-OF PU. procedures USED-FOR features. mechanism USED-FOR Networks. four - step training strategy USED-FOR mechanism. four - step training strategy USED-FOR Networks. VGGNet CONJUNCTION ResNet. ResNet CONJUNCTION VGGNet. ResNet CONJUNCTION EfficientNet ’s accuracies. EfficientNet ’s accuracies CONJUNCTION ResNet. Tiny ImageNet CONJUNCTION ImageNet. ImageNet CONJUNCTION Tiny ImageNet. ImageNet CONJUNCTION CIFAR-100 benchmarks. CIFAR-100 benchmarks CONJUNCTION ImageNet. Tiny ImageNet EVALUATE-FOR EfficientNet ’s accuracies. VGGNet EVALUATE-FOR method. ResNet EVALUATE-FOR method. EfficientNet ’s accuracies EVALUATE-FOR method. ImageNet EVALUATE-FOR method. CIFAR-100 benchmarks EVALUATE-FOR method. Tiny ImageNet EVALUATE-FOR method. Method are Convolutional Neural Networks ( CNNs ), parallel procedures, and parallel Procedural Units ( PUs ). OtherScientificTerm are semantic features, procedure sequence, intermediate features, and intermediate feature. Generic are specialized procedures, It, and network. ","This paper proposes a routing-by-memory mechanism for CNN architectures. The mechanism is based on a four-step training strategy to train Neural Networks with parallel procedures. The main idea is to learn the semantic features of each procedure sequence, and then use these features to train the network. The PU consists of a memory head, a procedure, and the memory head of the PU. The paper shows that these specialized procedures can be used to train specialized neural networks. The proposed method is evaluated on VGGNet, ResNet, and Tiny ImageNet on CIFAR-100 benchmarks, showing EfficientNet’s accuracies and the performance of the proposed method.","This paper proposes a routing-by-memory mechanism for CNN architectures. The main idea is to train Convolutional Neural Networks (CNNs) with parallel procedures. The paper introduces a PU, which consists of a memory head and a procedure. The memory head consists of the semantic features and the procedure sequence, while the procedure is the intermediate features. It can be seen as an extension of the parallel Procedural Units (PU). The paper also introduces specialized procedures for each of these features. The proposed mechanism can be applied to different Networks and a four-step training strategy can be used to train different Networks. Experiments on VGGNet, ResNet, and EfficientNet’s accuracies on CIFAR-100 benchmarks and Tiny ImageNet show that the proposed method outperforms the state-of-the-art in terms of accuracy. "
17932,SP:d240173080cd3647dbaa5173a6422396f226775b,"fundamental symmetries CONJUNCTION coordinate freedoms of physical law. coordinate freedoms of physical law CONJUNCTION fundamental symmetries. coordinate freedoms of physical law FEATURE-OF neural networks. fundamental symmetries FEATURE-OF neural networks. irreducible representations USED-FOR frameworks. rotation CONJUNCTION reflection ( parity ). reflection ( parity ) CONJUNCTION rotation. translation CONJUNCTION rotation. rotation CONJUNCTION translation. fundamental symmetries USED-FOR physical laws. scalar products CONJUNCTION scalar contractions. scalar contractions CONJUNCTION scalar products. scalar contractions HYPONYM-OF scalars. scalar products HYPONYM-OF scalars. OtherScientificTerm are high - order tensor objects, symmetry - enforcing constraints, classical physics, permutations, symmetries, and Euclidean, Lorentz, and Poincaré groups. Method are polynomial functions, and scalar - based method. Generic is theory. ","This paper studies the problem of learning high-order tensor objects with symmetry-enforcing constraints in classical physics. The authors propose two frameworks based on irreducible representations, which are based on fundamental symmetries and coordinate freedoms of physical law in neural networks. Theoretically, the authors show that these fundamental symmetsries can be used to learn physical laws with rotation, translation, and reflection (parity). The authors then propose a scalar-based method to learn polynomial functions. The scalars are scalar products, scalar contractions, and scalar product-based methods. Experimental results demonstrate the effectiveness of the theory. ","The paper proposes a new framework for learning high-order tensor objects. The framework is based on the notion of symmetry-enforcing constraints. The authors show that the fundamental symmetries of neural networks and the coordinate freedoms of physical law are invariant to permutations of the polynomial functions. The frameworks are based on irreducible representations. The paper also proposes a scalar-based method to learn the symmetry-encouraging constraints. Experiments are performed on Euclidean, Lorentz, and Poincaré groups. The scalars are scalar products, scalar contractions, rotation, and reflection (parity)."
17996,SP:72c0f47566904deb27d8157da30807ec1d6b5685,Bounding box ( bbox ) regression HYPONYM-OF computer vision. loss functions USED-FOR bbox regression. Intersection over Union ( IoU ) loss HYPONYM-OF loss functions. IoUbased losses USED-FOR power IoU losses. power IoU term CONJUNCTION power regularization term. power regularization term CONJUNCTION power IoU term. power parameter α FEATURE-OF power regularization term. power regularization term FEATURE-OF power IoU losses. power IoU term FEATURE-OF power IoU losses. order preservingness CONJUNCTION loss / gradient reweighting. loss / gradient reweighting CONJUNCTION order preservingness. α - IoU losses HYPONYM-OF losses. loss / gradient reweighting HYPONYM-OF properties. order preservingness HYPONYM-OF properties. α - IoU losses COMPARE IoU - based losses. IoU - based losses COMPARE α - IoU losses. small datasets CONJUNCTION noisy bboxes. noisy bboxes CONJUNCTION small datasets. object detection benchmarks CONJUNCTION models. models CONJUNCTION object detection benchmarks. bbox regression accuracy EVALUATE-FOR detectors. performance margin EVALUATE-FOR α - IoU losses. performance margin EVALUATE-FOR IoU - based losses. OtherScientificTerm is α. ,"This paper studies the problem of bbox regression in computer vision. The authors propose a new loss function, the Intersection over Union (IoU) loss, which is based on the power IoU term. The power parameter α is defined as the sum of the power regularization term of the IoU losses. The paper shows that IoUbased losses can achieve better performance than the state-of-the-art in terms of both order preservingness and loss/gradient reweighting. ","This paper proposes a new loss function for bbox regression, i.e., the Intersection over Union (IoU) loss, for computer vision. The power IoU term is defined as the power parameter α, and the power regularization term is the sum of the power of the two loss functions. The authors show that the proposed IoUbased losses are more robust to noise than the α-iOU losses. They also show the performance margin of the IoU-based losses in terms of order preservingness and loss/gradient reweighting. "
18060,SP:397125177d7007316d67194ec00d5dc57b44ac79,"imitation learning problem USED-FOR policy. Markov Decision Process ( MDP ) setting FEATURE-OF policy. imitation learning USED-FOR policy. policy USED-FOR problem. adversarial construction USED-FOR policy. DROIL CONJUNCTION Maximum Entropy Inverse Reinforcement Learning. Maximum Entropy Inverse Reinforcement Learning CONJUNCTION DROIL. framework USED-FOR generalized concept of entropy. generalized concept of entropy USED-FOR DROIL. framework USED-FOR DROIL. approach USED-FOR objective function. approach USED-FOR convex optimization problem. state and action spaces FEATURE-OF loss functions. convex optimization problem USED-FOR objective function. polynomial number of variables FEATURE-OF convex optimization problem. approach USED-FOR stationary and non - stationary policies. methods COMPARE it. it COMPARE methods. inner reinforcement learning problem USED-FOR it. synthetic data CONJUNCTION highway driving environment. highway driving environment CONJUNCTION synthetic data. optimization method USED-FOR DROIL. synthetic data EVALUATE-FOR DROIL. highway driving environment EVALUATE-FOR DROIL. synthetic data EVALUATE-FOR optimization method. highway driving environment EVALUATE-FOR optimization method. OtherScientificTerm are reward function, demonstrated behaviors, noisy demonstrations, and optimistic generalizations. Generic is task. Method is Distributionally Robust Imitation Learning ( DROIL ). ","This paper studies the problem of imitation learning a policy in the Markov Decision Process (MDP) setting in the imitation learning problem, where the goal is to learn a reward function that maximizes the performance of the policy in a given environment. The problem is formulated as a distributionally Robust Imitation Learning (DROIL) task where the policy is learned by adversarial construction. The authors propose a framework for DROIL based on the generalized concept of entropy in Maximum Entropy Inverse Reinforcement Learning, which is a convex optimization problem with polynomial number of variables. The proposed approach is able to learn an objective function that is convex with respect to the state and action spaces of the loss functions. The approach can also be applied to stationary and non-stationary policies. The optimization method is evaluated on synthetic data and a highway driving environment, and it outperforms existing methods. ","This paper proposes Distributionally Robust Imitation Learning (DROIL), an imitation learning problem for learning a policy in the Markov Decision Process (MDP) setting. The problem is formulated as an adversarial construction of a policy, where the goal is to learn a reward function that maximizes the likelihood of the observed behaviors. The authors propose a framework for DROIL based on the generalized concept of entropy. The loss functions in the state and action spaces are modeled as a convex optimization problem with polynomial number of variables. The proposed approach can be applied to stationary and non-stationary policies. Experiments on synthetic data and a highway driving environment show that DROIL outperforms existing methods. "
18124,SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"Post - processing USED-FOR algorithmic fairness. approach USED-FOR ML systems. Post - processing HYPONYM-OF approach. retraining USED-FOR it. post - processing algorithms USED-FOR individual fairness ( IF ). similarity graph USED-FOR fairness constraints. graph Laplacian regularization USED-FOR graph smoothing problem. graph smoothing problem USED-FOR IF post - processing problem. post - processing algorithms USED-FOR individual biases. post - processing algorithms USED-FOR large - scale NLP models. individual biases FEATURE-OF large - scale NLP models. accuracy EVALUATE-FOR post - processing algorithms. BERT HYPONYM-OF large - scale NLP models. Method is postprocessing. Generic is model. OtherScientificTerm are objective function, and individual fairness. ","This paper proposes a new approach for algorithmic fairness in ML systems, called Post-processing. The main idea is to use post-processing to improve the performance of the model by reducing the number of parameters in the objective function. To this end, it uses retraining on the similarity graph to enforce fairness constraints. The authors also propose a graph smoothing problem based on graph Laplacian regularization to solve the IF post-processing problem. The paper shows that the proposed post -processing algorithms can improve the individual fairness (IF) of large-scale NLP models such as BERT by reducing individual biases. ","This paper proposes a new approach to improve algorithmic fairness in ML systems. The approach is called Post-processing. The authors propose to use post-processing algorithms to improve individual fairness (IF) under similarity graph constraints. The main idea is to use graph Laplacian regularization to solve the graph smoothing problem, which is an important problem in the IF post-processing problem. The paper also proposes to use retraining to improve it. Experiments are conducted on BERT and on large-scale NLP models with different individual biases. The results show that the proposed postprocessing improves the accuracy of the models. "
18188,SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"Text - to - SQL task USED-FOR SQL queries. model USED-FOR database schemas. graph structure USED-FOR unified encoding model. unified encoding model USED-FOR natural language question and database schema. graph structure USED-FOR SADGA. question - graph CONJUNCTION schema - graph. schema - graph CONJUNCTION question - graph. unified modeling USED-FOR structure - aware aggregation method. Global Graph Linking CONJUNCTION Local Graph Linking. Local Graph Linking CONJUNCTION Global Graph Linking. Local Graph Linking CONJUNCTION DualGraph Aggregation Mechanism. DualGraph Aggregation Mechanism CONJUNCTION Local Graph Linking. Global Graph Linking USED-FOR structure - aware aggregation method. Local Graph Linking PART-OF structure - aware aggregation method. DualGraph Aggregation Mechanism PART-OF structure - aware aggregation method. Text - to - SQL benchmark Spider EVALUATE-FOR proposal. Task are Text - to - SQL, and cross - domain Text - to - SQL. Method are encoding method, and question - schema linking method. OtherScientificTerm is database schema. ","This paper proposes a unified encoding model for natural language question and database schema. The proposed encoding method, called SADGA, is based on a unified modeling of the question-graph and the schema-graph. The model is able to learn the database schemas from the query and the answer. The paper also proposes a structure-aware aggregation method, Local Graph Linking and DualGraph Aggregation Mechanism. The proposal is evaluated on the Text-to-SQL benchmark Spider.","This paper proposes a unified encoding model for natural language question and database schema. The model is able to encode database schemas into a question-graph and a schema-graph. The encoding method is based on the question-schema linking method. The proposed SADGA uses a graph structure to encode the query and the database schema, and a unified modeling for the structure-aware aggregation method based on Local Graph Linking and DualGraph Aggregation Mechanism. The proposal is evaluated on the Text-to-SQL benchmark Spider. "
18252,SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"models USED-FOR supervised and reinforcement learning. discrete and continuous model components USED-FOR models. approach USED-FOR discrete - continuous computation graphs. discrete probability distributions USED-FOR neural networks. stochastic softmax tricks USED-FOR neural networks. discrete component USED-FOR graph ’s execution paths. discrete component USED-FOR computation graphs. sequential discrete components USED-FOR stochastic computations graphs. small gradients CONJUNCTION local minima. local minima CONJUNCTION small gradients. scale parameter FEATURE-OF Gumbel noise perturbations. scale parameter USED-FOR learning behavior. dropout residual connections USED-FOR stochastic, discrete - continuous computation graphs. complex discrete - stochastic models COMPARE continuous counterparts. continuous counterparts COMPARE complex discrete - stochastic models. benchmark datasets EVALUATE-FOR complex discrete - stochastic models. benchmark datasets EVALUATE-FOR continuous counterparts. Method are discrete - continuous models, and complex discrete - continuous models. Generic is strategies. ","This paper proposes a new approach to learn discrete-continuous computation graphs from discrete probability distributions. The authors propose to use stochastic softmax tricks to improve the performance of neural networks with discrete and continuous model components in supervised and reinforcement learning. The main idea is to use a discrete component to represent the graph’s execution paths as sequential discrete components, and then use a continuous component to compute the computation graphs. The paper shows that the scale parameter of Gumbel noise perturbations can be used as a scale parameter for the learning behavior of the learned models. The proposed strategies are evaluated on several benchmark datasets and show that the complex discrete-stochastic models perform better than their continuous counterparts. ","This paper proposes a novel approach for learning discrete-continuous computation graphs with discrete probability distributions. The authors propose to use stochastic softmax tricks to train neural networks with discrete and continuous model components. The discrete component is used to model the graph’s execution paths, while the continuous component is applied to the computation graphs. The main contribution of the paper is to introduce sequential discrete components for learning stochastically computations graphs with small gradients and local minima. The proposed strategies are evaluated on two benchmark datasets, where the authors show that complex discrete-stochastic models outperform their continuous counterparts. They also show that the scale parameter of Gumbel noise perturbations can be used to improve the learning behavior of learning behavior with dropout residual connections."
18316,SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"Approximate Bayesian inference USED-FOR neural networks. Approximate Bayesian inference COMPARE training. training COMPARE Approximate Bayesian inference. high - fidelity approximate inference FEATURE-OF Bayesian neural networks ( BNNs ). full - batch Hamiltonian Monte Carlo USED-FOR high - fidelity approximate inference. covariate shift FEATURE-OF Bayesian model average. approximate inference procedures CONJUNCTION maximum a - posteriori ( MAP ) training. maximum a - posteriori ( MAP ) training CONJUNCTION approximate inference procedures. priors USED-FOR BNNs. robustness EVALUATE-FOR BNNs. robustness EVALUATE-FOR priors. Material is out - of - distribution data. Method is classical estimation. OtherScientificTerm are linear dependencies, features, and posterior contraction. ","This paper studies the problem of high-fidelity approximate inference in Bayesian neural networks (BNNs) with a full-batch Hamiltonian Monte Carlo. The authors show that the covariate shift of the Bayesian model average can be used to improve the performance of Approximate Bayesian inference compared to training. They also show that classical estimation can be improved by incorporating linear dependencies between features. Finally, they show that priors can improve the robustness of BNNs with approximate inference procedures and maximum a-posteriori (MAP) training.",This paper proposes a new approach to approximate Bayesian inference for neural networks. The authors propose to use a full-batch Hamiltonian Monte Carlo to perform high-fidelity approximate inference for Bayesian neural networks (BNNs) with out-of-distribution data. The main idea is to use the covariate shift of the Bayesian model average to estimate the linear dependencies between the features of the input and the output of the BNN. The paper also proposes to use classical estimation for the posterior contraction. The results show that Approximate Bayes inference outperforms training and maximum a-priori (MAP) training in terms of robustness to adversarial attacks. 
18380,SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"settings PART-OF meta - learning evaluation. in - distribution [ ID ] HYPONYM-OF settings. out - of - distribution [ OOD ] HYPONYM-OF settings. task distribution USED-FOR train and test tasks. metalearning theory CONJUNCTION FSL applications. FSL applications CONJUNCTION metalearning theory. they USED-FOR task generation. few - shot classification benchmarks EVALUATE-FOR OOD evaluation. ID setting USED-FOR metalearning theory. meta - learning methods USED-FOR ID setting. OOD datasets EVALUATE-FOR meta - learning methods. meta - learning method USED-FOR model selection. ID evaluation CONJUNCTION OOD evaluation. OOD evaluation CONJUNCTION ID evaluation. FSL benchmarks USED-FOR ID evaluation. FSL benchmarks USED-FOR OOD evaluation. benchmarks USED-FOR OOD evaluation. Task are OOD setting, and ID vs. OOD evaluation. Generic is methods. ","This paper studies the problem of meta-learning evaluation in two settings: in-distribution [ID] and out-of-disribution [OD] settings. In the ID setting, the task distribution is used for train and test tasks, and the OOD setting is used to evaluate the performance of the model in the ID vs. ID setting. The authors propose two methods to perform OOD evaluation on OOD datasets. First, they use metalearning theory and FSL applications to train the model selection, and then they use them for task generation. Second, they show that the performance on few-shot classification benchmarks is comparable to ID evaluation on FSL benchmarks. ","This paper proposes two new settings for meta-learning evaluation: in-distribution [ID] and out-of-Distribution [OD] settings. In the ID setting, the goal is to learn a task distribution for both train and test tasks. The OOD setting is to evaluate the performance of a model in the ID vs. ID setting in the context of metalearning theory and FSL applications, where they are used for task generation. The ID setting is used for training the model selection, and the OOD evaluation is done on few-shot classification benchmarks. The authors show that meta-learned methods can be used in ID setting to evaluate on OOD datasets. The paper also shows that the proposed methods are able to outperform other methods on ID evaluation and on FSL benchmarks."
18444,SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"rules PART-OF knowledge base ( KB ). language model ( LM)-based rule generation USED-FOR rules. KB - based rule induction CONJUNCTION LM - based rule generation. LM - based rule generation CONJUNCTION KB - based rule induction. data commonalities USED-FOR KB - based methods. LMs USED-FOR free text. rich expressive power FEATURE-OF LMs. methods USED-FOR canned ” rules. open rule induction problem USED-FOR open rules. Orion ( open rule induction ) system USED-FOR open rules. LMs USED-FOR open rules. automatically inducted rules COMPARE manually annotated rules. manually annotated rules COMPARE automatically inducted rules. open rules USED-FOR relation extraction. OtherScientificTerm are Rules, annotated rules, and supervision of annotated rules. Method are inference systems, rule induction systems, and LM - based methods. Generic is they. ","This paper studies the open rule induction problem in the context of language model (LM)-based rule generation, where rules are generated from a knowledge base (KB). The authors propose two methods for open rules: KB-based rule induction and LM-based rules generation. Both methods are based on data commonalities. The authors show that the open rules are more expressive than LMs for free text and have a rich expressive power. They also show that open rules can be used for relation extraction, and that they can be more efficient than manually annotated rules.   The authors also provide a theoretical analysis of the inference systems that are used to generate open rules. The main result is that the Orion (open rule induction) system is able to generate more open rules than the LMs. ","This paper proposes a new approach to learning rules in a knowledge base (KB). The authors propose a language model (LM)-based rule generation to generate rules from the rules in the knowledge base. The main idea is to learn rules from data commonalities, and then use these rules to train inference systems. The authors also propose two methods to generate “canned” rules. The first method is based on an open rule induction problem, where the authors propose to use an Orion (open rule induction) system to generate open rules that can be used for relation extraction. The second method uses LMs to generate free text with rich expressive power. Authors show that they outperform automatically inducted rules and manually annotated rules in terms of relation extraction, and they also show that their methods can be combined with other LM-based methods. "
18508,SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,Reinforcement Learning ( RL ) algorithms USED-FOR real - world scenarios. single - agent counterpart COMPARE offline multiagent RL. offline multiagent RL COMPARE single - agent counterpart. state and action space FEATURE-OF agents. agents PART-OF offline multiagent RL. offline RL algorithms USED-FOR multi - agent systems. offline RL algorithm USED-FOR extrapolation error. state - action pairs USED-FOR value estimation. Implicit Constraint Q - learning ( ICQ ) HYPONYM-OF offline RL algorithm. ICQ USED-FOR multi - agent tasks. implicit constraint USED-FOR joint - policy. OtherScientificTerm is accumulated extrapolation error. ,"This paper studies the problem of offline multi-agent reinforcement learning in real-world scenarios. The authors propose a new offline RL algorithm, called Implicit Constraint Q-learning (ICQ), which is able to recover the extrapolation error of a single-agent counterpart in the offline multiagent RL. The main idea is to learn a joint-policy with an implicit constraint on the number of agents in the state and action space of the agents. The value estimation is performed using the state-action pairs of the two agents.  The authors show that ICQ can achieve state-of-the-art performance on a variety of multi- agent tasks. ","This paper proposes a new offline RL algorithm to reduce the extrapolation error of multi-agent systems. The authors propose Implicit Constraint Q-learning (ICQ) which is a variant of the single-agent counterpart. ICQ can be applied to multiple agents in both the state and action space, which is useful for real-world scenarios. In particular, the authors propose a joint-policy with an implicit constraint on the value estimation of the state-action pairs. "
18572,SP:1939b24b68970c33ca16ce238deed257f76d009e,"machine learning models USED-FOR security related applications. real - world adversaries USED-FOR neural network based detectors. uniform norm - bounded perturbations USED-FOR adversarial examples ( AEs ). finance CONJUNCTION social networks. social networks CONJUNCTION finance. malware CONJUNCTION finance. finance CONJUNCTION malware. AEs USED-FOR domains. social networks HYPONYM-OF domains. malware HYPONYM-OF domains. finance HYPONYM-OF domains. semantically meaningful dependencies FEATURE-OF features. features USED-FOR applications. non - uniform perturbations USED-FOR feature dependencies. non - uniform perturbations USED-FOR adversarial training. malware classification CONJUNCTION credit risk prediction. credit risk prediction CONJUNCTION malware classification. credit risk prediction CONJUNCTION spam detection. spam detection CONJUNCTION credit risk prediction. approach USED-FOR real - world attacks. certification EVALUATE-FOR non - uniform bounds. non - uniform perturbation bounds USED-FOR robustness certification. Metric is imperceptibility. OtherScientificTerm are uniform perturbations, and empirical data distribution. ","This paper studies the problem of real-world adversaries against neural network based detectors. The authors propose uniform norm-bounded perturbations for adversarial examples (AEs) to improve the imperceptibility of machine learning models for security related applications. AEs are used in domains such as finance, social networks, malware, and credit risk prediction. The main idea is to use non-uniform perturbation bounds for the feature dependencies of the features in these applications, which are semantically meaningful dependencies on the empirical data distribution. The proposed approach improves the robustness certification of the proposed method against the proposed methods.","This paper proposes a new approach to detect real-world attacks on machine learning models for security related applications. The authors propose uniform norm-bounded perturbations for adversarial examples (AEs) to improve the imperceptibility of neural network based detectors. The main idea of the proposed approach is to learn features with semantically meaningful dependencies on the data distribution, and then use these features for applications such as credit risk prediction, malware classification, and social networks. AEs are applied to domains such as finance, social networks, and malware. The non-uniform perturbation bounds are used in adversarial training for robustness certification. "
18636,SP:417b30930b245667d777e5d90ee80dd41546760e,spectral filtering USED-FOR statistical properties. spectral filtering USED-FOR learning with kernels. learning with kernels USED-FOR statistical properties. regularization schemes COMPARE Tikhonov regularization. Tikhonov regularization COMPARE regularization schemes. faster convergence rates FEATURE-OF excess risk. regularization schemes USED-FOR excess risk. regularization schemes USED-FOR least squares. faster convergence rates EVALUATE-FOR regularization schemes. loss functions USED-FOR estimators. Tikhonov regularization USED-FOR generalized self concordant loss functions ( GSC ). logistic loss HYPONYM-OF generalized self concordant loss functions ( GSC ). proximal point method USED-FOR optimization. iterated Tikhonov regularization scheme CONJUNCTION proximal point method. proximal point method CONJUNCTION iterated Tikhonov regularization scheme. fast and optimal rates USED-FOR GSC. iterated Tikhonov regularization scheme USED-FOR fast and optimal rates. iterated Tikhonov regularization scheme USED-FOR GSC. OtherScientificTerm is source and capacity conditions. Task is learning task. ,"This paper studies spectral filtering for learning with kernels in the context of statistical properties. The authors show that the excess risk of existing regularization schemes (e.g., Tikhonov regularization) has faster convergence rates than those of the standard regularization scheme (i.e., the proximal point method) when the source and capacity conditions of the learning task are different. They then propose a generalized self concordant loss functions (GSC), such as logistic loss, which can be used as loss functions for estimators. The theoretical results show that this GSC can achieve fast and optimal rates for GSC with an iterated Tikhonoov regularisation scheme and a proximal method for optimization. ","This paper proposes spectral filtering for learning with kernels for statistical properties. The authors propose two regularization schemes for minimizing excess risk, one based on Tikhonov regularization and the other based on generalized self concordant loss functions (GSC). The authors show faster convergence rates for the excess risk in terms of the source and capacity conditions. The main contribution of the paper is to propose two loss functions for estimators, namely the logistic loss and the proximal point method for optimization. Experiments are conducted on a learning task where the authors show that the fast and optimal rates for GSC can be obtained by the iterated Tikhonsov regularisation scheme and proximal points method."
18700,SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"linear transform USED-FOR input - output dimensions. butterfly matrices USED-FOR linear transform. Deformable Butterfly ( DeBut ) HYPONYM-OF linear transform. It USED-FOR neural networks. sparsity FEATURE-OF DeBut layer. sparsity USED-FOR network compression. light weight CONJUNCTION inference complexity. inference complexity CONJUNCTION light weight. DeBut COMPARE fully connected and convolutional layers. fully connected and convolutional layers COMPARE DeBut. OtherScientificTerm are butterflies, and natural complexity - accuracy tradeoff. Method is neural network. Generic is it. Metric is accuracy. ","This paper proposes a new linear transform for input-output dimensions, Deformable Butterfly (DeBut), which is based on butterfly matrices. It can be used to compress neural networks with sparsity in the DeBut layer. The authors show that DeBut is more efficient than fully connected and convolutional layers in terms of light weight and inference complexity. They also show that it has a natural complexity-accuracy tradeoff. ","This paper proposes Deformable Butterfly (DeBut), a linear transform for input-output dimensions of neural networks. It is based on butterfly matrices. The authors show that DeBut is more efficient than fully connected and convolutional layers in terms of sparsity of the DeBut layer in network compression. They also show that it is more accurate than the fully connected neural network. The paper also provides a natural complexity-accuracy tradeoff between the light weight and inference complexity."
18764,SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"weights PART-OF network. method USED-FOR weight reusability. shared weights USED-FOR MARK. common Knowledge Base ( KB ) USED-FOR shared weights. metalearning approach USED-FOR weight reusability. metalearning approach USED-FOR KB. benchmarks EVALUATE-FOR MARK. average accuracy EVALUATE-FOR methods. MARK USED-FOR reusable knowledge. Method are artificial neural networks, and MetA Reusable Knowledge. Task are Catastrophic Forgetting ( CF ), and overwriting. OtherScientificTerm are forgetting of old information, trainable masks, and KB relevant weights. Generic are task, and model. Material is 20 - Split - MiniImageNet dataset. Metric is forgetfulness. ","This paper studies the problem of Catastrophic Forgetting (CF) in artificial neural networks. The authors propose a method for weight reusability based on the common Knowledge Base (KB) and propose a metalearning approach for KB. The proposed method, called MetA Reusable Knowledge (MARK), uses shared weights between the weights in the network to reduce the forgetting of old information. The paper shows that the proposed methods achieve better average accuracy than existing methods on a variety of benchmarks. ","This paper proposes a method for weight reusability in artificial neural networks. The authors propose a common Knowledge Base (KB) for shared weights in the network, and propose a metalearning approach to improve the KB. They also propose MetA Reusable Knowledge (MARK) to reduce the forgetting of old information. The proposed method is evaluated on two benchmarks and shows better average accuracy compared to other methods for reusable knowledge. "
18828,SP:722c52467e384058f8fdffa254d0e8db47440a64,"exact solvers USED-FOR Mixed Integer Programming ( MIP ). Primal heuristics USED-FOR exact solvers. MIP heuristics PART-OF solver. hard - coded rules USED-FOR solvers. rules USED-FOR problem. rules USED-FOR heuristics. data - driven framework USED-FOR scheduling heuristics. scheduling heuristics PART-OF exact MIP solver. data - driven framework USED-FOR exact MIP solver. algorithm USED-FOR schedule. Task are real - world applications, and learning task. Method are primal heuristics, problem - specific schedule of heuristics, and academic MIP solver. Metric is average primal integral. ","This paper studies the problem of Mixed Integer Programming (MIP) with primal heuristics, where the goal is to find exact solvers for the exact MIP. The problem is formulated as a set of hard-coded rules for the solvers to solve the problem. The authors propose a data-driven framework to learn the scheduling heuristic of the perfect MIP solver, which is a combination of MIP heuristic and the exact solver. The algorithm is based on an existing algorithm for learning the schedule of the optimal solution, and the authors show that this algorithm can be applied to a variety of real-world applications. ","This paper proposes to use primal heuristics for Mixed Integer Programming (MIP) to learn exact solvers. The authors propose to use hard-coded rules to train the solvers, and then use these rules to solve the problem. The main idea is to learn a solver based on MIP heuristic, which is an exact MIP solver with the MIP-specific schedule. The paper proposes a data-driven framework to learn the scheduling heuristic for the exact Mip solver. The algorithm is based on an existing algorithm for learning the schedule, and the algorithm is evaluated on real-world applications, and on a learning task. The results show that the average primal integral of the proposed algorithm is better than the one used in the academic paper. "
18892,SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"it COMPARE real - world applications. real - world applications COMPARE it. self - driving cars CONJUNCTION robotics. robotics CONJUNCTION self - driving cars. real - world applications PART-OF reinforcement learning. robotics HYPONYM-OF reinforcement learning. self - driving cars HYPONYM-OF reinforcement learning. robotics HYPONYM-OF real - world applications. self - driving cars HYPONYM-OF real - world applications. sublinear regret EVALUATE-FOR algorithm. unknown parametric model USED-FOR trajectory labels. Task are reinforcement learning ( RL ), RL practice, and learning. OtherScientificTerm are binary feedback, and reward signal. Generic is this. ","This paper studies the problem of reinforcement learning (RL) in the context of binary feedback, where the goal is to maximize the sublinear regret of the reward signal. The authors propose a new algorithm, which is based on an unknown parametric model that predicts the trajectory labels of trajectories in the environment. They show that it can achieve sublinear regrets in real-world applications such as self-driving cars, robotics, and reinforcement learning. They also show that their algorithm can be applied to RL practice.","This paper proposes a new algorithm for reinforcement learning (RL), which is an extension of RL practice where the goal is to improve the performance of the agent. The main idea is to use binary feedback as a reward signal to guide the learning. The authors show that it outperforms real-world applications such as self-driving cars, robotics, and reinforcement learning in terms of sublinear regret. The algorithm is based on an unknown parametric model that predicts the trajectory labels of trajectories."
18956,SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"node embedding CONJUNCTION graph pooling methods. graph pooling methods CONJUNCTION node embedding. Graph neural networks USED-FOR representing graph - structured data. edges PART-OF graph. edges PART-OF graph. edges USED-FOR discrimination. graph reconstruction and generation CONJUNCTION graph classification tasks. graph classification tasks CONJUNCTION graph reconstruction and generation. graph classification tasks HYPONYM-OF tasks. graph reconstruction and generation HYPONYM-OF tasks. nodes PART-OF hypergraph. edges PART-OF graph. Dual Hypergraph Transformation ( DHT ) USED-FOR edge representation learning framework. message - passing techniques USED-FOR node representations. message - passing techniques USED-FOR edges. dual hypergraph construction USED-FOR message - passing techniques. hypergraphs USED-FOR edge representations. method COMPARE graph representation learning methods. graph representation learning methods COMPARE method. edge representation learning method USED-FOR graph representation and generation. graph datasets USED-FOR graph representation and generation. hypergraphs USED-FOR edge representation learning method. graph datasets EVALUATE-FOR hypergraphs. graph datasets EVALUATE-FOR edge representation learning method. lossless compression of the nodes CONJUNCTION removal of irrelevant edges. removal of irrelevant edges CONJUNCTION lossless compression of the nodes. edge representation learning and pooling method COMPARE graph pooling methods. graph pooling methods COMPARE edge representation learning and pooling method. graph pooling methods USED-FOR graph classification. edge representation learning and pooling method USED-FOR graph classification. Material is graph - structured data. Generic is they. OtherScientificTerm is connectivity. Method are graph representation learning, holistic graph - level edge representations, and edge representation learning. ","This paper proposes Dual Hypergraph Transformation (DHT) as an edge representation learning framework for graph neural networks for representing graph-structured data. The proposed method is based on node embedding and graph pooling methods, where nodes in a hypergraph are represented as edges in a graph, and edges in the graph are used for discrimination. The message-passing techniques for node representations are based on dual hypergraph construction. The authors show that they can learn edge representations from hypergraphs that are more holistic graph-level edge representations. They also show that the proposed method performs better than other graph representation learning methods on several graph datasets for graph representation and generation and graph classification tasks. ","This paper proposes Dual Hypergraph Transformation (DHT), an edge representation learning framework for representing graph-structured data in Graph neural networks. The key idea is to learn holistic graph-level edge representations, where nodes in a hypergraph are represented as edges, and edges in a graph are represented by message-passing techniques. The authors show that the proposed method outperforms other graph representation learning methods in graph representation and generation, graph reconstruction and generation and graph classification tasks. The paper also shows that hypergraphs can be used to learn edge representations for graph datasets. "
19020,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information ( MI ) maximization USED-FOR learning representations of data. representations USED-FOR learning. representations USED-FOR reinforcement learning ( RL ). representations USED-FOR RL. MI objectives USED-FOR representations. samples of high - dimensional observations USED-FOR MI. state representation USED-FOR optimal policy. objectives USED-FOR insufficient representations. visual observations FEATURE-OF simulated game environment. OtherScientificTerm are irrelevant and redundant information, MI based objectives, and structure of the MDP. Task is control. Generic is methods. ","This paper studies mutual information (MI) maximization for learning representations of data in reinforcement learning (RL). The authors consider the problem of learning representations for learning in RL using MI objectives. The MI objectives are based on samples of high-dimensional observations from a simulated game environment, where irrelevant and redundant information is available. The authors propose to use MI based objectives to learn representations of the MDP. The state representation of the optimal policy is then used as a state representation for learning the optimal representations. The proposed methods are evaluated on a variety of simulated game environments with visual observations.",This paper proposes mutual information (MI) maximization for learning representations of data. The authors show that MI objectives can be used to learn representations for reinforcement learning (RL) that are more robust to irrelevant and redundant information. The MI objectives are learned from samples of high-dimensional observations. The paper also shows that MI based objectives can learn representations that are robust to insufficient representations.  The authors propose two methods for learning such representations. The first one is based on the structure of the MDP. The second one uses the state representation of the optimal policy to learn an optimal policy. Experiments are conducted on a simulated game environment with visual observations.
19084,SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"steerable convolution USED-FOR 3D semantic analysis. SS - Conv USED-FOR steerable convolution. sparse tensors USED-FOR steerable convolution. pipeline USED-FOR precise estimation of object poses. SS - Conv USED-FOR pipeline. Feature - Steering module USED-FOR pose refinement. SE(3)-equivariance USED-FOR Feature - Steering module. instance - level 6D pose estimation CONJUNCTION category - level 6D pose and size estimation. category - level 6D pose and size estimation CONJUNCTION instance - level 6D pose estimation. category - level 6D pose and size estimation CONJUNCTION categorylevel 6D pose tracking. categorylevel 6D pose tracking CONJUNCTION category - level 6D pose and size estimation. categorylevel 6D pose tracking HYPONYM-OF 3D object semantic analysis. instance - level 6D pose estimation HYPONYM-OF 3D object semantic analysis. category - level 6D pose and size estimation HYPONYM-OF 3D object semantic analysis. pipeline COMPARE methods. methods COMPARE pipeline. metrics EVALUATE-FOR tasks. tasks EVALUATE-FOR pipeline. tasks EVALUATE-FOR methods. metrics EVALUATE-FOR pipeline. metrics EVALUATE-FOR methods. SS - Conv USED-FOR pipeline. SS - Conv COMPARE convolutions. convolutions COMPARE SS - Conv. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. efficiency EVALUATE-FOR SS - Conv. accuracy EVALUATE-FOR SS - Conv. efficiency EVALUATE-FOR convolutions. accuracy EVALUATE-FOR convolutions. Method are SE(3)-equivariant deep feature learning, and Sparse Steerable Convolution ( SS - Conv ). Material is dense, volumetric data. Task is processing of 3D data. Generic are designs, and code. ","This paper proposes Sparse Steerable Convolution (SS-Conv) for 3D image classification. The authors propose to use sparse tensors to train a steerable convolution to perform 3D 3D semantic analysis. The proposed pipeline is based on the SE(3)-equivariant deep feature learning and uses the Feature-Steering module to perform pose refinement. The paper shows that the proposed pipeline outperforms existing methods on three different tasks: instance-level 6D pose estimation, category-level6D pose and size estimation, and 3D object semantic analysis on dense, volumetric data. The efficiency of the proposed SS-conv outperforms other convolutions in terms of accuracy and efficiency. ","This paper proposes a new approach to steerable convolution for 3D semantic analysis. The authors propose Sparse Steerable Convolution (SS-Conv) which uses sparse tensors to improve the performance of steerable neural networks. The proposed approach is based on SE(3)-equivariant deep feature learning. The main idea is to use a feature-steering module for the pose refinement. The Feature-Steering module is trained with SE(2) equivariance, which is a variant of Se(3) Equivariant Deep Feature Learning. The paper also proposes a pipeline for precise estimation of object poses. The pipeline is evaluated on three tasks: instance-level 6D pose estimation, category-level6D pose and size estimation, and 3D object semantic analysis, where the proposed pipeline outperforms existing methods on all three tasks. SS-conv outperforms other convolutions in terms of accuracy, efficiency, and other metrics. "
19148,SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"Attention USED-FOR vision transformers. informative tokens USED-FOR image recognition. dynamic token sparsification framework USED-FOR redundant tokens. lightweight prediction module USED-FOR importance score. features USED-FOR importance score. module USED-FOR redundant tokens. module PART-OF layers. layers USED-FOR redundant tokens. attention masking strategy USED-FOR prediction module. hierarchically pruning USED-FOR method. accuracy EVALUATE-FOR vision transformers. FLOPs EVALUATE-FOR method. accuracy EVALUATE-FOR method. throughput EVALUATE-FOR method. DynamicViT models COMPARE CNNs. CNNs COMPARE DynamicViT models. CNNs CONJUNCTION vision transformers. vision transformers CONJUNCTION CNNs. DynamicViT models COMPARE vision transformers. vision transformers COMPARE DynamicViT models. dynamic token sparsification framework USED-FOR DynamicViT models. ImageNet EVALUATE-FOR CNNs. ImageNet EVALUATE-FOR vision transformers. complexity / accuracy trade - offs EVALUATE-FOR DynamicViT models. Method are self - attention, and raoyongming. OtherScientificTerm is unstructured sparse tokens. Generic is framework. ",This paper proposes a new approach for self-attention in vision transformers. The authors propose a dynamic token sparsification framework to replace redundant tokens in the layers with informative tokens for image recognition. The proposed method is based on hierarchically pruning the layers to remove redundant tokens. The key idea is to use a lightweight prediction module to estimate the importance score based on the features of the features. The prediction module is trained using an attention masking strategy to mask the redundant tokens from the layers. The method is evaluated on ImageNet and FLOPs and shows that the proposed method improves the accuracy and throughput of the proposed DynamicViT models compared to CNNs and vision transformer models. ,"This paper proposes a novel approach to improve the performance of vision transformers by using self-attention to generate informative tokens for image recognition. The proposed framework is based on the dynamic token sparsification framework. The key idea is to use a lightweight prediction module to estimate the importance score based on features of the input images. The prediction module consists of two layers, one for generating redundant tokens and the other for generating unstructured sparse tokens. The attention masking strategy is used in the prediction module. The method is evaluated on FLOPs with hierarchically pruning. The authors show that the proposed method improves the throughput and accuracy on ImageNet compared to other CNNs and Vision transformers on complexity/accuracy trade-offs. "
19212,SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"cross - validation methods CONJUNCTION conformal prediction. conformal prediction CONJUNCTION cross - validation methods. holdout methods CONJUNCTION cross - validation methods. cross - validation methods CONJUNCTION holdout methods. inference USED-FOR regression function. methods USED-FOR predictive inference. distribution - free guarantees USED-FOR predictive inference. methods USED-FOR distribution - free guarantees. inference USED-FOR inference. inference USED-FOR conditional mean. inference HYPONYM-OF regression function. conformal prediction HYPONYM-OF methods. holdout methods HYPONYM-OF methods. cross - validation methods HYPONYM-OF methods. non - vanishing width FEATURE-OF confidence interval. inference USED-FOR E [ Y |X ]. finite setting CONJUNCTION continuous setting. continuous setting CONJUNCTION finite setting. Task is data analysis problems. OtherScientificTerm are distributional assumptions, inference guarantees, sample size, and vanishing - width confidence intervals. ","This paper studies the problem of data analysis problems with distributional assumptions. The authors propose two methods: cross-validation methods and conformal prediction. The proposed methods are based on distribution-free guarantees for predictive inference, which allows for inference of a regression function (e.g., inference on the conditional mean). The authors show that the proposed methods can achieve better performance than holdout methods, and that the inference guarantees can be extended to the finite setting and the continuous setting. The main contribution of the paper is that the confidence interval of a confidence interval with non-vanishing width can be used as a proxy for the sample size. The paper also provides some theoretical guarantees for the vanishing-width confidence intervals. Finally, the authors demonstrate the effectiveness of the proposed inference for E [Y |X] in a finite setting.","This paper studies the problem of data analysis problems with distributional assumptions. The authors propose two methods for obtaining distribution-free guarantees for predictive inference, namely, holdout methods and cross-validation methods. The methods are based on the assumption that the conditional mean of the inference is a function of the sample size and the confidence interval is non-vanishing width. The inference is then used to compute the regression function, which is then applied to the E [Y |X] in a finite setting and a continuous setting. "
19276,SP:123952325765c040c3078fc7dca2b6d370e55590,bias mitigation methods USED-FOR DNN models. learning debiased encoders USED-FOR bias mitigation methods. instance - level annotations USED-FOR sensitive attributes. fairness sensitive information PART-OF encoder. discrimination EVALUATE-FOR DNN models. task - specific classification head PART-OF DNN models. Representation Neutralization for Fairness ( RNF ) HYPONYM-OF mitigation technique. neutralized representations USED-FOR classification head. ground - truth label CONJUNCTION sensitive attributes. sensitive attributes CONJUNCTION ground - truth label. classification head PART-OF DNN model. neutralized representations USED-FOR DNN model. RNF USED-FOR classification head. fairness sensitive information PART-OF encoder representations. bias - amplified model USED-FOR proxy annotations. proxy annotations USED-FOR sensitive attributes. bias - amplified model USED-FOR low - resource settings. benchmark datasets EVALUATE-FOR RNF framework. RNF framework USED-FOR DNN models. benchmark datasets EVALUATE-FOR discrimination of DNN models. task - specific performance EVALUATE-FOR discrimination of DNN models. task - specific performance EVALUATE-FOR RNF framework. Method is biased representations. Task is fairness. OtherScientificTerm is sensitive attribute annotations. ,"This paper proposes Representation Neutralization for Fairness (RNF), a new mitigation technique for bias mitigation methods for DNN models trained with learning debiased encoders. The main idea is to use the fairness sensitive information in the encoder to improve the performance of the classification head in the task-specific classification head of the DNN model by using neutralized representations of the training data. The bias-amplified model is then used to learn proxy annotations for sensitive attributes from instance-level annotations. The RNF framework is evaluated on a variety of benchmark datasets to demonstrate the effectiveness of the proposed RNF for the discrimination of DNNs in terms of their performance on tasks with high and low-resource settings.","This paper proposes Representation Neutralization for Fairness (RNF), a new mitigation technique for learning debiased encoders for DNN models. The authors propose to use fairness sensitive information in the encoder and the sensitive attributes in the instance-level annotations to improve the fairness of the classification head of the DNN model. The proposed RNF framework is evaluated on two benchmark datasets for the discrimination of DNNs in terms of task-specific performance. The paper also proposes a bias-amplified model to learn proxy annotations for sensitive attributes. "
19340,SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,translations CONJUNCTION rotations. rotations CONJUNCTION translations. rotations FEATURE-OF learning models. translations FEATURE-OF learning models. learning models USED-FOR image analysis. Convolutional Neural Networks ( CNN ) USED-FOR image analysis. convolutions USED-FOR They. physics FEATURE-OF Bessel functions. Bessel functions USED-FOR convolutional layer. Task is medical imaging. Method is Bessel - CNNs ( B - CNNs ). OtherScientificTerm is rotation angles. ,This paper studies the problem of medical imaging with Bessel-CNNs. The authors propose to use convolutional Neural Networks (CNN) to perform image analysis with translations and rotations. They show that convolutions can be used to learn Bessel functions in physics. They also show that Bessel function can be applied to the convolution of the convronal layer. ,"This paper studies the problem of medical imaging with Bessel-CNNs (BCCNNs). The authors propose to use convolutional Neural Networks (CNN) for image analysis. They show that Bessel functions are equivalent to physics in terms of rotation angles. They also show that convolutions can be used to encode rotation angles, which is an important property of Bessels. The authors also provide a theoretical analysis of the Bessel function. "
19404,SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,large - scale solver USED-FOR kernel ridge regression. ParK HYPONYM-OF large - scale solver. ParK HYPONYM-OF kernel ridge regression. random projections CONJUNCTION iterative optimization. iterative optimization CONJUNCTION random projections. partitioning CONJUNCTION random projections. random projections CONJUNCTION partitioning. partitioning CONJUNCTION iterative optimization. iterative optimization CONJUNCTION partitioning. partitioning PART-OF approach. iterative optimization PART-OF approach. space and time complexity EVALUATE-FOR approach. random projections PART-OF approach. statistical accuracy EVALUATE-FOR approach. local effective dimension CONJUNCTION bias. bias CONJUNCTION local effective dimension. orthogonality FEATURE-OF local estimators. feature space COMPARE input space. input space COMPARE feature space. feature space FEATURE-OF partitions. statistical - computational tradeoff EVALUATE-FOR model. large - scale datasets EVALUATE-FOR method. ,"This paper proposes a large-scale solver for kernel ridge regression called ParK. The approach combines partitioning, random projections, iterative optimization, and partitioning. The authors show that the proposed approach improves both the statistical accuracy and the space and time complexity of the model. The paper also provides a statistical-computational tradeoff between the feature space and the input space. ","This paper proposes a large-scale solver for kernel ridge regression, called ParK. The approach consists of partitioning, random projections, iterative optimization, and orthogonality of local estimators. The proposed approach is evaluated on both space and time complexity. The model is shown to outperform the state-of-the-art in terms of statistical-computational tradeoff. Experiments are conducted on two large -scale datasets."
19468,SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"reinforcement learning settings USED-FOR Neural agents. discrete tokens USED-FOR Neural agents. one - hot vectors USED-FOR discrete communication tokens. zero - shot understanding HYPONYM-OF communication. natural language processing USED-FOR word embedding techniques. discrete tokens USED-FOR neural agent architectures. continuous space USED-FOR discrete tokens. technique USED-FOR communication. technique COMPARE one - hot tokens. one - hot tokens COMPARE technique. decision theoretic framework EVALUATE-FOR technique. Generic are techniques, and method. OtherScientificTerm are human communication, unlabeled emergent agent communication, and one - hot communication. ","This paper studies the problem of learning discrete tokens for Neural agents in reinforcement learning settings. The authors propose a new technique for learning discrete communication tokens in a continuous space. The key idea is to use one-hot vectors to encode discrete communication token in the continuous space, and then use these tokens to communicate with the agent. The proposed method is evaluated on a variety of tasks, including zero-shot understanding, zero-hot communication, and human communication. ","This paper proposes a novel method for learning discrete tokens for Neural agents in reinforcement learning settings. The key idea is to use one-hot vectors to encode discrete communication tokens in a continuous space. The authors propose to use natural language processing for word embedding techniques. The proposed technique is evaluated on zero-shot understanding of communication, where the goal is to learn human communication and unlabeled emergent agent communication. The technique is shown to improve communication in a decision theoretic framework."
19532,SP:8630ccc627534f9033bced04e2137a897ffef701,they COMPARE convolutional networks. convolutional networks COMPARE they. Transformers USED-FOR computer vision. generalization COMPARE convolutional networks. convolutional networks COMPARE generalization. model capacity FEATURE-OF Transformers. depthwise Convolution CONJUNCTION self - Attention. self - Attention CONJUNCTION depthwise Convolution. convolution layers CONJUNCTION attention layers. attention layers CONJUNCTION convolution layers. CoAtNets HYPONYM-OF hybrid models. capacity CONJUNCTION efficiency. efficiency CONJUNCTION capacity. generalization CONJUNCTION capacity. capacity CONJUNCTION generalization. coat ” nets HYPONYM-OF CoAtNets. relative attention USED-FOR hybrid models. relative attention USED-FOR depthwise Convolution. relative attention USED-FOR self - Attention. CoAtNets COMPARE CoAtNet. CoAtNet COMPARE CoAtNets. resource constraints FEATURE-OF CoAtNets. JFT-3B USED-FOR CoAtNet. top-1 accuracy EVALUATE-FOR CoAtNet. top-1 accuracy EVALUATE-FOR it. ImageNet EVALUATE-FOR it. OtherScientificTerm is inductive bias. Generic is architectures. Metric is ImageNet top-1 accuracy. Material is ImageNet-21 K. ,"This paper studies the inductive bias of Transformers for computer vision. The authors show that the model capacity of Transformers is much larger than that of convolutional networks, and that they are more efficient than they are in terms of generalization. They also show that CoAtNets, a class of hybrid models such as “coat” nets and “self-attention,” are better than convolution layers and attention layers. They show that relative attention improves the performance of depthwise Convolution and self-Attention by relative attention in the hybrid models. Finally, they show that JFT-3B improves CoAtNet's top-1 accuracy on ImageNet-21 K. ","This paper studies the generalization properties of Transformers for computer vision. The authors show that they are more general than convolutional networks in terms of model capacity and efficiency. They also show that CoAtNets and “coat” nets are better than other hybrid models such as relative attention for depthwise Convolution and self-Attention for self-attention, and that the inductive bias of these architectures is lower than that of other architectures. The paper also shows that the top-1 accuracy of CoAtNet is better than ImageNet-21 K. The main contribution of the paper is to show that the capacity and the efficiency are not correlated with the resource constraints. The experiments are conducted on JFT-3B and ImageNet."
19596,SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,second - order oracle bound USED-FOR expected risk. expected risk FEATURE-OF weighted majority vote. second - order oracle bound USED-FOR weighted majority vote. one - sided Chebyshev ’s ) HYPONYM-OF parametric form of the ChebyshevCantelli inequality. parametric form of the ChebyshevCantelli inequality USED-FOR bound. form USED-FOR optimization challenge. Chebyshev - Cantelli inequality CONJUNCTION C - bounds. C - bounds CONJUNCTION Chebyshev - Cantelli inequality. optimization challenge USED-FOR prior oracle bounds. Chebyshev - Cantelli inequality USED-FOR prior oracle bounds. it USED-FOR oracle bound. second order Markov ’s inequality USED-FOR it. second order Markov ’s inequality USED-FOR oracle bound. PAC - Bayesian bounding CONJUNCTION Bennett ’s inequality. Bennett ’s inequality CONJUNCTION PAC - Bayesian bounding. PAC - Bayes - Bennett HYPONYM-OF concentration of measure inequality. PAC - Bayesian bounding PART-OF it. Bennett ’s inequality PART-OF it. it USED-FOR empirical estimation of the oracle bound. PAC - Bayes - Bennett inequality COMPARE PAC - Bayes - Bernstein inequality. PAC - Bayes - Bernstein inequality COMPARE PAC - Bayes - Bennett inequality. ChebyshevCantelli inequality CONJUNCTION PAC - Bayes - Bennett inequality. PAC - Bayes - Bennett inequality CONJUNCTION ChebyshevCantelli inequality. parametric form USED-FOR concentration of measure. PAC - Bayes - Bennett inequality USED-FOR concentration of measure. parametric form CONJUNCTION PAC - Bayes - Bennett inequality. PAC - Bayes - Bennett inequality CONJUNCTION parametric form. parametric form FEATURE-OF ChebyshevCantelli inequality. Task is minimization. Generic is bounds. ,"This paper studies the second-order oracle bound for the expected risk of a weighted majority vote under a parametric form of the ChebyshevCantelli inequality (i.e., one-sided). The authors propose a new form for the optimization challenge to find prior oracle bounds. The authors show that the proposed form is equivalent to the PAC-Bayesian bounding and the Bennett’s inequality, and that it can be used as an empirical estimation of the oracle upper bound.  The authors also provide a theoretical analysis of the proposed bounds. ",The paper proposes a parametric form of the ChebyshevCantelli inequality (one-sided Cheyshev’s) which is a second-order oracle bound for the expected risk of weighted majority vote. The proposed form is used in the optimization challenge for minimizing the minimization. The authors show that the proposed form outperforms prior oracle bounds in terms of C-bounds and PAC-Bayesian bounding. The paper also shows that it outperforms PAC-Bennett inequality and the second order Markov ’s inequality in the empirical estimation of the oracle upper bound. 
19660,SP:5bac542a6532d43cf100e085398b4a4783719814,"audio - visual video parsing task USED-FOR audio or visual event categories. method USED-FOR audio or visual events. common and diverse event semantics USED-FOR audio or visual events. common and diverse event semantics USED-FOR method. method USED-FOR event co - occurrence. method COMPARE methods. methods COMPARE method. weakly - supervised audio - visual video parsing EVALUATE-FOR method. weakly - supervised audio - visual video parsing EVALUATE-FOR methods. OtherScientificTerm are audio and visual events, cross - modality co - occurrence, supervisory signals, and video - level annotations. Method is parsing model. ","This paper proposes a new audio-visual video parsing task for audio or visual event categories. The proposed method is based on common and diverse event semantics for both audio and visual events, where cross-modality co-occurrence is defined by supervisory signals. The authors show that the proposed method can achieve better performance than existing methods in weakly-supervised audio-video video parsing. They also show that their parsing model is more interpretable than previous methods. ",This paper proposes a new audio-visual video parsing task for audio or visual event categories. The proposed method is based on common and diverse event semantics to learn the audio/visual events. The authors propose a parsing model that learns the cross-modality co-occurrence between audio and visual events. They also propose supervisory signals that are used to predict the event co-event. They show that the proposed method outperforms other methods in weakly-supervised audio-video video parsing. 
19724,SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"federated learning ( FL ) USED-FOR global model. quantized and personalized FL algorithm USED-FOR collective ( personalized model compression ) training. knowledge distillation ( KD ) USED-FOR collective ( personalized model compression ) training. quantization parameters CONJUNCTION model dimensions / structures. model dimensions / structures CONJUNCTION quantization parameters. model dimensions / structures FEATURE-OF compressed personalized models. quantization parameters FEATURE-OF compressed personalized models. algorithm USED-FOR quantized models. relaxed optimization problem USED-FOR algorithm. knowledge distillation loss USED-FOR local client objectives. global model USED-FOR knowledge distillation loss. model dimension EVALUATE-FOR compressed model. knowledge distillation loss USED-FOR compressed personalization framework. alternating proximal gradient update USED-FOR compressed personalization problem. FedAvg CONJUNCTION local training of clients. local training of clients CONJUNCTION FedAvg. QuPeD COMPARE personalized FL methods. personalized FL methods COMPARE QuPeD. personalized FL methods CONJUNCTION FedAvg. FedAvg CONJUNCTION personalized FL methods. QuPeD COMPARE FedAvg. FedAvg COMPARE QuPeD. QuPeD COMPARE local training of clients. local training of clients COMPARE QuPeD. local training of clients HYPONYM-OF personalized FL methods. Method are FL algorithms, and ( federated ) learning process. Material is heterogeneous data. Task is personalization. OtherScientificTerm is quantization values. ","This paper proposes a federated learning (FL) algorithm for learning a global model from heterogeneous data. The authors propose a combination of a quantized and personalized FL algorithm for collective (personalized model compression) training using knowledge distillation (KD) in the context of collective (private) personalized model compression. The proposed algorithm is based on the relaxed optimization problem, and is able to compress quantized models with different quantization parameters and different model dimensions/structures. The compressed personalization framework is a compressed personalisation framework based on alternating proximal gradient update, which is a common technique in the (federated) learning process. The paper shows that the compressed model has a smaller model dimension than a standard compressed model with the same quantization values, and can achieve better performance than FedAvg and other personalized FL methods such as QuPeD and local training of clients.","This paper proposes a federated learning (FL) for learning a global model from heterogeneous data. The authors propose a quantized and personalized FL algorithm for collective (personalized model compression) training using knowledge distillation (KD). The authors show that the quantization parameters and the model dimensions/structures of compressed personalized models are similar to the quantized models, and that the (federated) learning process can be decomposed into a relaxed optimization problem. The proposed compressed personalization framework is based on alternating proximal gradient update, and the authors propose an algorithm that compresses the local client objectives using the global model. Experiments show that a compressed model with the same model dimension outperforms QuPeD, FedAvg, and local training of clients. "
19788,SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"Constrained clustering PART-OF machine learning. partially labeled data USED-FOR prior information. prior information USED-FOR it. partially labeled data USED-FOR it. framework USED-FOR constrained clustering. deep generative models USED-FOR framework. stochastic gradient variational inference USED-FOR framework. domain knowledge USED-FOR model ( DC - GMM ). probabilistic relations FEATURE-OF domain knowledge. DC - GMM COMPARE deep constrained clustering methods. deep constrained clustering methods COMPARE DC - GMM. robustness EVALUATE-FOR deep constrained clustering methods. data sets EVALUATE-FOR DC - GMM. data sets EVALUATE-FOR deep constrained clustering methods. robustness EVALUATE-FOR DC - GMM. real - world applications EVALUATE-FOR approach. Generic are model, and constraints. OtherScientificTerm are prior clustering preferences, and pairwise constraints. Task is clustering process. ","This paper proposes a new framework for constrained clustering in machine learning. The framework is based on stochastic gradient variational inference with partially labeled data, where the prior information is used to train a model (DC-GMM) with domain knowledge with probabilistic relations. The model is trained using deep generative models. The authors show that the proposed approach has better robustness on two data sets compared to deep constrained clustersering methods. The proposed approach is also shown to perform well in real-world applications.","This paper proposes a framework for constrained clustering in machine learning, where it uses partially labeled data for prior information. The framework is based on stochastic gradient variational inference. The authors propose a model (DC-GMM) based on domain knowledge with probabilistic relations between the domain knowledge and the prior clustering preferences. The model is trained using deep generative models. The proposed approach is evaluated on several real-world applications and shows better robustness compared to other deep-constrained clustering methods. "
19852,SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,Neural Tangent Kernel ( NTK ) USED-FOR infinitely - wide neural networks. least squares loss USED-FOR infinitely - wide neural networks. gradient descent USED-FOR least squares loss. gradient descent USED-FOR infinitely - wide neural networks. NTK regression COMPARE finitely - wide neural networks. finitely - wide neural networks COMPARE NTK regression. small - scale datasets USED-FOR finitely - wide neural networks. kernel methods USED-FOR large - scale learning tasks. computational complexity EVALUATE-FOR kernel methods. near input - sparsity time approximation algorithm USED-FOR NTK. near input - sparsity time approximation algorithm USED-FOR learning. polynomial expansions of arc - cosine kernels USED-FOR near input - sparsity time approximation algorithm. NTK USED-FOR learning. spectral approximation guarantee USED-FOR NTK matrix. random features CONJUNCTION sketching algorithm. sketching algorithm CONJUNCTION random features. random features PART-OF arc - cosine kernels. leverage score sampling USED-FOR random features. accuracy EVALUATE-FOR CNTK. linear regressor COMPARE CNTK. CNTK COMPARE linear regressor. accuracy EVALUATE-FOR linear regressor. speedup EVALUATE-FOR linear regressor. CNTK features USED-FOR linear regressor. speedup EVALUATE-FOR CNTK. large - scale regression and classification tasks EVALUATE-FOR methods. CIFAR-10 dataset EVALUATE-FOR CNTK. Method is convolutional counterpart of NTK ( CNTK ). OtherScientificTerm is linear runtime. ,"This paper proposes Neural Tangent Kernel (NTK), a convolutional counterpart of NTK (CNTK) for infinitely-wide neural networks with least squares loss. NTK regression can be viewed as an extension of the recent work on finitely-wide Neural networks with polynomial expansions of arc-cosine kernels with random features and leverage score sampling. The authors propose a near input-sparsity time approximation algorithm for NTK for learning with a near-optimal spectral approximation guarantee for the NTK matrix. Theoretically, the authors show that NTK improves the performance of the linear regressor and the CNTK features on the CIFAR-10 dataset. Empirical results on large-scale regression and classification tasks demonstrate the effectiveness of the proposed methods. ","This paper proposes Neural Tangent Kernel (NTK) for infinitely-wide neural networks with least squares loss, which is a convolutional counterpart of NTK (CNTK). NTK regression can be seen as a variant of finitely-wider neural networks on small-scale datasets. The authors propose a near input-sparsity time approximation algorithm for learning with polynomial expansions of arc-cosine kernels, where the random features are obtained by leverage score sampling and the sketching algorithm. The spectral approximation guarantee of the NTK matrix is also provided. The proposed methods are evaluated on large-scale regression and classification tasks on CIFAR-10 dataset. The computational complexity of the proposed kernel methods is comparable to that of other kernel methods. CNTK is shown to outperform the linear regressor in terms of speedup and accuracy. "
19916,SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"framework USED-FOR multi - person 3D motion trajectory prediction. local - range encoder CONJUNCTION global - range encoder. global - range encoder CONJUNCTION local - range encoder. global - range encoder USED-FOR social interactions. local - range encoder USED-FOR individual motion. local - range encoder PART-OF Multi - Range Transformers model. global - range encoder PART-OF Multi - Range Transformers model. Transformer decoder USED-FOR prediction. model COMPARE methods. methods COMPARE model. long - term 3D motion prediction EVALUATE-FOR methods. long - term 3D motion prediction EVALUATE-FOR model. OtherScientificTerm are human pose trajectory, and local and global - range encoder features. ","This paper proposes a framework for multi-person 3D motion trajectory prediction. The proposed Multi-Range Transformers model consists of a local-range encoder and a global-residual encoder to capture social interactions. The human pose trajectory is represented by a Transformer decoder, and the prediction is performed using the local and global-reward encoder features. Experiments show that the proposed model is able to achieve state-of-the-art performance on long-term motion prediction.","This paper proposes a framework for multi-person 3D motion trajectory prediction. The proposed model is based on the Multi-Range Transformers model, which consists of a local-range encoder for individual motion and a global-range decoder for social interactions. The human pose trajectory is represented by a Transformer decoder, and the prediction is performed using the Transformer encoder. The model outperforms other methods in long-term 3Dmotion prediction. "
19997,SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"reinforcement learning USED-FOR long - horizon planning problems. programs USED-FOR reinforcement learning. programs USED-FOR settings. strategy USED-FOR program. program synthesis USED-FOR guiding programs. generative model USED-FOR It. It USED-FOR program. model USED-FOR program. approach COMPARE non - program - guided approaches. non - program - guided approaches COMPARE approach. benchmarks EVALUATE-FOR non - program - guided approaches. 2D Minecraft - inspired environment HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR approach. program - guided reinforcement learning USED-FOR approach. Generic is approaches. Method are guiding program, model predictive program synthesis ( MPPS ), and handcrafted programs. Task is programming task. ","This paper studies the problem of long-horizon planning in reinforcement learning, where the goal is to learn a guiding program that can be used to solve a programming task. It proposes a generative model that learns a program from a set of training examples, and then uses a strategy to generate a program based on this strategy. It is shown that the proposed approach outperforms non-program-guided approaches on several benchmarks in the 2D Minecraft-inspired environment.","This paper proposes a novel approach for long-horizon planning problems. The approach is based on program-guided reinforcement learning. It uses a generative model to generate a program, which is then applied to a programming task. The guiding program is learned using model predictive program synthesis (MPPS). The authors show that the proposed approach outperforms non-program-guided approaches on several benchmarks in a 2D Minecraft-inspired environment. "
20078,SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"causal imitation learning USED-FOR Imitation learning. sequential settings USED-FOR causal imitation learning. graphical criterion USED-FOR causal imitation. algorithm USED-FOR imitability. Task are naïve imitation, and single - stage decision - making. OtherScientificTerm are sensors, imitator, demonstrator ’s behavior ( DO ), and demonstrator. Generic is theory. ","This paper studies causal imitation learning in sequential settings in the context of Imitation learning. The authors consider the problem of naïve imitation, where the goal is to imitate the imitator’s behavior (DO) in a single-stage decision-making. They propose a graphical criterion to measure the imitability of the demonstrator, and propose an algorithm to improve imitability. They show that the proposed algorithm can achieve better imitability than the state-of-the-art. ","This paper proposes a new method for causal imitation learning in sequential settings. The authors propose to use a graphical criterion to measure the imitability of the imitator’s behavior (DO) in the context of naïve imitation, which is an important problem in single-stage decision-making. The main idea of the paper is to use the graphical criterion as a surrogate for the causal imitation. The paper is well-written and well-motivated, and the theory is sound. However, the experimental results are not convincing. "
20159,SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,transition losses USED-FOR object - structured representation. object - structured representation COMPARE pixels. pixels COMPARE object - structured representation. transition losses COMPARE pixels. pixels COMPARE transition losses. transition losses USED-FOR model. object persistence CONJUNCTION object identity. object identity CONJUNCTION object persistence. alignment module USED-FOR model. object identity HYPONYM-OF transition models. object persistence HYPONYM-OF transition models. objectlevel loss CONJUNCTION object alignment. object alignment CONJUNCTION objectlevel loss. object occlusion CONJUNCTION re - appearance. re - appearance CONJUNCTION object occlusion. model COMPARE baseline. baseline COMPARE model. it USED-FOR object occlusion. it USED-FOR re - appearance. partially observable environments FEATURE-OF re - appearance. partially observable environments FEATURE-OF object occlusion. OtherScientificTerm is slot - wise object memory. ,"This paper studies the problem of object-structured representation learning with transition losses. The authors propose two transition models: object persistence and object identity. The object persistence model is based on the alignment module, and the object identity model uses the objectlevel loss and object alignment. The paper shows that the proposed model performs better than the baseline in terms of object occlusion and re-appearance in partially observable environments. ","This paper proposes a new model that combines transition losses for object-structured representation with pixels. The model consists of an objectlevel loss, an object alignment module, and an object persistence module. The authors show that the proposed model outperforms the baseline in terms of object occlusion and re-appearance in partially observable environments. The paper also shows that the model is more robust to slot-wise object memory."
20240,SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"classification CONJUNCTION regression. regression CONJUNCTION classification. regression CONJUNCTION off - policy policy learning. off - policy policy learning CONJUNCTION regression. classification CONJUNCTION off - policy policy learning. off - policy policy learning CONJUNCTION classification. Empirical risk minimization ( ERM ) PART-OF machine learning. classification HYPONYM-OF machine learning. adaptively collected data USED-FOR generic importance sampling weighted ERM algorithm. maximal inequality USED-FOR rates. exploration rate FEATURE-OF rates. fast rates USED-FOR regression. convexity of squared - error loss USED-FOR fast rates. regret guarantees USED-FOR policy learning. OtherScientificTerm are modelagnostic guarantees, hypothesis class, importance sampling structure, and exploration. Method is contextual bandit algorithm. Metric is fast convergence rates. Material is bandit - collected data. Generic is theory. ","This paper proposes a contextual bandit algorithm based on Empirical risk minimization (ERM) in machine learning (classification, regression, and off-policy policy learning). The authors propose a generic importance sampling weighted ERM algorithm with adaptively collected data. The authors show that fast rates for classification and regression with convexity of squared-error loss can be obtained with maximal inequality. The regret guarantees for policy learning with regret guarantees are also provided. The main contribution of the paper is a theoretical analysis of the fast convergence rates of the proposed algorithms. The theory is well-motivated and easy to follow. ","This paper proposes a contextual bandit algorithm for learning from bandit-collected data. Empirical risk minimization (ERM) is an important part of machine learning, such as classification, regression, and off-policy policy learning. The authors propose a generic importance sampling weighted ERM algorithm based on adaptively collected data with modelagnostic guarantees. The main idea of the theory is to use maximal inequality as a measure of the rates of the maximal inequality between two sets of rates with respect to the exploration rate of the first set of rates for classification and the second set for regression. The paper shows that the fast rates for regression and regression are due to the convexity of squared-error loss, while for regression the authors show that fast rates are related to the importance sampling structure. The regret guarantees for policy learning are based on the regret guarantees of the hypothesis class."
20321,SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,weighted regression model USED-FOR machine learning tasks. predictive power EVALUATE-FOR models. low sample sizes CONJUNCTION covariate perturbations. covariate perturbations CONJUNCTION low sample sizes. mitigation strategy USED-FOR problems. doubly non - negative matrix USED-FOR sample weights. log - determinant divergence CONJUNCTION Bures - Wasserstein distance. Bures - Wasserstein distance CONJUNCTION log - determinant divergence. uncertainty set FEATURE-OF weighting matrix. Bures - Wasserstein distance USED-FOR weighting matrix. log - determinant divergence USED-FOR weighting matrix. first - order methods USED-FOR adversarially reweighted estimate. Task is kernel - reweighted regression. Method is reweighting strategy. ,"This paper studies the problem of kernel-reweighted regression with low sample sizes and covariate perturbations in machine learning tasks. The authors propose a mitigation strategy to tackle these problems by reweighting the weighting matrix with a doubly non-negative matrix. The paper shows that the reweighted matrix has the same predictive power as the original models, but with a lower uncertainty set. The proposed weighting matrices are based on log-determinant divergence and Bures-Wasserstein distance, which is a well-studied and well-motivated idea. The main contribution of the paper is the use of first-order methods for adversarially reweight the estimate.",This paper proposes a new weighted regression model for machine learning tasks with low sample sizes and covariate perturbations. The authors propose a reweighting strategy to improve the predictive power of models. The main idea is to use a doubly non-negative matrix for the sample weights. The weighting matrix is defined as the log-determinant divergence and Bures-Wasserstein distance between the uncertainty set and the true uncertainty set. The proposed mitigation strategy can be applied to a variety of problems. The paper shows that the adversarially reweighted estimate can be obtained using first-order methods. 
20402,SP:fe12e13602925b9400fd596a987755beb10aa3d1,"discrete latent variables USED-FOR models. continuous relaxation USED-FOR low - variance reparameterization gradients. binary random variables USED-FOR it. importance sampling CONJUNCTION statistical couplings. statistical couplings CONJUNCTION importance sampling. importance sampling USED-FOR estimator. statistical couplings USED-FOR estimator. sequences of binary variables CONJUNCTION Rao - Blackwellization. Rao - Blackwellization CONJUNCTION sequences of binary variables. Rao - Blackwellization USED-FOR reparameterizing categorical variables. sequences of binary variables USED-FOR reparameterizing categorical variables. reparameterizing categorical variables USED-FOR gradient estimators. Rao - Blackwellization USED-FOR gradient estimators. estimators COMPARE REINFORCE. REINFORCE COMPARE estimators. leave - one - out - baseline estimator USED-FOR estimators. leave - one - out - baseline estimator USED-FOR REINFORCE. Method are unbiased gradient estimators, performant estimator, continuous relaxations, and categorical gradient estimators. Material is categorical setting. OtherScientificTerm is stick - breaking coupling. ","This paper studies the problem of low-variance reparameterization gradients with continuous relaxation. The authors propose a new estimator based on importance sampling, statistical couplings, and Rao-Blackwellization. They show that the estimator can be trained with binary random variables, and that it can be used to train models with discrete latent variables. They also show that their estimator performs better than REINFORCE with a leave-one-out-baseline estimator.","This paper proposes a new method for learning unbiased gradient estimators for categorical data. The authors propose to use continuous relaxation to learn low-variance reparameterization gradients for models with discrete latent variables. The main idea is to use a performant estimator with binary random variables to estimate the gradients. The estimator is based on importance sampling, statistical couplings, and Rao-Blackwellization. Experiments show that the proposed estimator outperforms other estimators with leave-one-out-basis estimator and REINFORCE with the help of continuous relaxations. "
20483,SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"predictors USED-FOR top architectures. search path USED-FOR high - performance sub - space. weaker predictors USED-FOR search path. strong predictor USED-FOR architecture space. predictor USED-FOR well - performed architectures. coarse - to - fine iteration USED-FOR ranking of sampling space. NAS - Bench-101 CONJUNCTION NAS - Bench-201. NAS - Bench-201 CONJUNCTION NAS - Bench-101. WeakNAS USED-FOR top - performance architectures. ImageNet MobileNet Search Space EVALUATE-FOR SOTA. SOTA EVALUATE-FOR WeakNAS. ImageNet MobileNet Search Space EVALUATE-FOR WeakNAS. Method are Neural Architecture Search ( NAS ), predictor - based NAS approaches, proxy accuracy predictor, and weak predictor. OtherScientificTerm are architecture - performance pairs, and weak predictors. Generic are architecture, and framework. ","This paper proposes a new architecture search method called Neural Architecture Search (NAS). The main idea is to use a proxy accuracy predictor to predict the architecture of a given architecture, and then use a search path to find the high-performance sub-space. The search path is based on the search path of weaker predictors that predict the top architectures. The paper shows that using a predictor that predicts the architecture space of a well-performed architecture is a good predictor for well-performing architectures.  The paper also shows that by using a coarse-to-fine iteration, the ranking of sampling space can be improved.  WeakNAS outperforms SOTA on ImageNet MobileNet Search Space and NAS-Bench-201. ","This paper proposes a new architecture search method, called WeakNAS, which is a variant of Neural Architecture Search (NAS). WeakNAS is based on the idea that the top architectures should be learned from a set of weak predictors, and the search path should be performed in a high-performance sub-space. WeakNAS uses a proxy accuracy predictor, which predicts the architecture-performance pairs of the top and bottom layers of the architecture. The search path uses weaker predictors for the architecture space, while the strong predictor is used for the low-performing architecture space. The framework is evaluated on ImageNet MobileNet Search Space and SOTA, and compared to other predictor-based NAS approaches. The results show that WeakNAS outperforms other top-performance architectures on NAS-Bench-101 and NAS-bench-201. The authors also show that a coarse-to-fine iteration improves the ranking of sampling space."
20564,SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"latent codes PART-OF globally consistent coordinate system. Entropic Desired Dynamics USED-FOR Intrinsic ConTrol ( EDDICT ). tractable learning CONJUNCTION interpretable latent space. interpretable latent space CONJUNCTION tractable learning. EDDICT ’s globally consistent codes USED-FOR it. prior methods COMPARE EDDICT ’s globally consistent codes. EDDICT ’s globally consistent codes COMPARE prior methods. state coverage CONJUNCTION unsupervised. unsupervised CONJUNCTION state coverage. hard exploration games EVALUATE-FOR unsupervised. Montezuma ’s Revenge HYPONYM-OF hard exploration games. OtherScientificTerm are local objective, and fixed additive latent dynamics. ","This paper studies the problem of learning a globally consistent coordinate system with latent codes. The authors propose to use Entropic Desired Dynamics (EDDICT) to learn the Intrinsic ConTrol (ICDICT). The key idea is to use EDDICT’s globally consistent codes in order to make it tractable with tractable learning and interpretable latent space. The local objective is to learn a fixed additive latent dynamics that is invariant to changes in the global objective. The proposed method is evaluated on a variety of hard exploration games, including Montezuma “s Revenge” and unsupervised. The results show that the proposed method performs better than prior methods.","This paper introduces Intrinsic ConTrol (EDDICT), a globally consistent coordinate system with latent codes. The key idea is to use Entropic Desired Dynamics to define the local objective. The authors show that it is equivalent to EDDICT’s globally consistent codes in tractable learning and interpretable latent space. They show that prior methods do not achieve better state coverage or unsupervised performance in hard exploration games such as Montezuma‘s Revenge. They also show that fixed additive latent dynamics can be used."
20645,SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"reinforcement learning ( RL ) USED-FOR drug design. reward scoring function USED-FOR RL. molecular docking program USED-FOR RL. molecular docking program HYPONYM-OF physical simulation. molecular docking program HYPONYM-OF reward scoring function. physical simulation USED-FOR protein - small molecule binding affinity. models USED-FOR chemically realistic and pharmacochemically acceptable molecules. local optima CONJUNCTION smooth surfaces. smooth surfaces CONJUNCTION local optima. docking score optimization HYPONYM-OF exploration problem. RL framework USED-FOR pharmacochemically acceptable molecules. docking scores FEATURE-OF pharmacochemically acceptable molecules. fragment - based generation method CONJUNCTION error - prioritized experience replay ( PER ). error - prioritized experience replay ( PER ) CONJUNCTION fragment - based generation method. Explorative Experience replay USED-FOR Drug design ( FREED ). Explorative Experience replay USED-FOR Fragment - based generative RL. de novo and scaffold - based schemes EVALUATE-FOR model. model COMPARE methods. methods COMPARE model. method USED-FOR model. predictive error - PER ( FREED(PE ) ) USED-FOR model. predictive error - PER ( FREED(PE ) ) HYPONYM-OF method. OtherScientificTerm are molecular structure, realistic and qualified chemical space, and drugs. ","This paper proposes a new reward scoring function for drug design in reinforcement learning (RL) based on the molecular docking program in physical simulation (MDP). In RL, the goal is to find a protein-small molecule binding affinity in a realistic and qualified chemical space. The exploration problem is formulated as docking score optimization, which is a well-studied exploration problem in the literature. The authors propose a new RL framework for finding pharmacochemically acceptable molecules with docking scores that are close to the docking scores of pharmacologically acceptable molecules. The proposed method is based on fragment-based generation method, error-priorized experience replay (PER), and Explorative Experience replay for Drug design (FREED). Experiments show that the proposed model performs better than previous methods on de novo and scaffold-based schemes. ","This paper proposes a new reinforcement learning (RL) method for drug design. The authors propose a molecular docking program for RL, which is based on the molecular docking programs in physical simulation for protein-small molecule binding affinity. The reward scoring function for RL is modeled as an exploration problem with docking score optimization, where the goal is to learn a molecular structure that is compatible with the realistic and qualified chemical space. The proposed model is evaluated on both de novo and scaffold-based schemes. The model outperforms existing methods in terms of predictive error-PER (PE) and error-priorized experience replay (PER) for Drug design (FREED). The authors also propose a fragment-based generation method for Fragment-based generative RL. "
20726,SP:b938bca513e7de1231212064caf8877a78d8b612,"complexity EVALUATE-FOR directed acyclic graphical models. observational data USED-FOR directed acyclic graphical models. local Markov boundary search procedure USED-FOR ancestral sets. ancestral sets PART-OF graphical model. local Markov boundary search procedure USED-FOR approach. forward greedy search algorithm USED-FOR Markov boundary. backward pruning phase PART-OF forward greedy search algorithm. forward greedy search algorithm USED-FOR graph ensembles. identifiability condition FEATURE-OF graph. finite - sample guarantees USED-FOR recovering Markov boundaries. results USED-FOR polytrees. polynomial time FEATURE-OF polytrees. minimal assumptions USED-FOR structure of directed graphical models. approach USED-FOR discrete or continuous distributions. OtherScientificTerm are distributional assumptions, nodes, and Markov boundaries. Metric is sample complexity. Generic is algorithm. ","This paper studies the complexity of directed acyclic graphical models on observational data. The authors propose a local Markov boundary search procedure to recover ancestral sets of a graphical model from ancestral sets. The proposed approach is based on a forward greedy search algorithm to recover the Markov boundaries of a graph ensembles with a backward pruning phase. Theoretically, the authors show that the resulting graph has an identifiability condition on the number of nodes in the graph, and that the sample complexity of the algorithm is bounded by a finite-sample guarantees. Empirical results on polytrees with polynomial time demonstrate that the proposed approach can recover discrete or continuous distributions with minimal assumptions.","This paper studies the sample complexity of directed acyclic graphical models on observational data. The authors propose a new approach to recover the Markov boundary of a graphical model from ancestral sets, which is based on a forward greedy search algorithm with a backward pruning phase. They show that this approach can recover discrete or continuous distributions with polynomial time. They also provide finite-sample guarantees for recovering Markov boundaries, and show that the proposed algorithm can recover polytrees with results that are close to the original results. The paper also shows that the structure of directed graphical models with minimal assumptions can be recovered from minimal assumptions. "
20807,SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"( "", ) DP algorithm USED-FOR privately learnable class. public randomness USED-FOR global stability. Task is learning with differential privacy ( DP ). OtherScientificTerm are privacy protection, probabilistic representation dimension, and nearly - matching lower bound. Method are "" -DP algorithms, local model, and correlated sampling strategy. ","This paper studies the problem of learning with differential privacy (DP). The authors propose a ("","") DP algorithm for privately learnable class with public randomness. The privacy protection is based on the probabilistic representation dimension of the local model. The authors provide a nearly-matching lower bound for ""DP algorithms"". The authors also provide a correlated sampling strategy to improve the global stability.","This paper proposes a private learning with differential privacy (DP) algorithm. The privacy protection is based on the probabilistic representation dimension. The authors propose a private learnable class with a ("", ) DP algorithm. They show that the global stability is achieved by minimizing the public randomness. They also propose a nearly-matching lower bound on the privacy protection. The ""-DP algorithms are based on a local model with a correlated sampling strategy."
20888,SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"Estimating the per - state expected cumulative rewards PART-OF reinforcement learning approaches. latent Markov decision process USED-FOR transition and reward models. gradient descent USED-FOR global optima. gradient descent USED-FOR linear parametrization. convergence rates USED-FOR cases. stochastic gradient descent ( SGD ) COMPARE explicit counterpart. explicit counterpart COMPARE stochastic gradient descent ( SGD ). implicit representation COMPARE explicit counterpart. explicit counterpart COMPARE implicit representation. implicit representation USED-FOR stochastic gradient descent ( SGD ). OtherScientificTerm are per - state expected cumulative rewards, and value predictions. Method are deep neural - network function - approximation methods, value iteration networks, and implicit representations of value functions. Generic is approach. ","This paper studies the problem of estimating the per-state expected cumulative rewards in reinforcement learning approaches that are based on the latent Markov decision process in transition and reward models. The authors propose a new deep neural-network function-approximation methods, where the value iteration networks are trained with implicit representations of value functions. The implicit representation is trained by stochastic gradient descent (SGD) and the explicit counterpart is learned by gradient descent for global optima. The convergence rates for the two cases are shown to be close to each other. ","This paper proposes a new approach to estimating the per-state expected cumulative rewards in reinforcement learning approaches. The authors propose to use deep neural-network function-approximation methods. The main idea is to use the latent Markov decision process in transition and reward models. The key idea of the approach is to train value iteration networks with implicit representations of value functions. The paper shows that the implicit representation of the value functions is better than the explicit counterpart of stochastic gradient descent (SGD) for global optima, and the gradient descent for linear parametrization. The convergence rates for cases with high convergence rates are also shown. "
20988,SP:992aa07d4f815d1c81f967374590eece933833b1,text sources USED-FOR Knowledge Graphs ( KGs ). embeddings USED-FOR inferring new facts. KG refinement task USED-FOR KGs. embeddings USED-FOR KGs. techniques USED-FOR KG refinement. inference rules USED-FOR techniques. ontological information USED-FOR embeddings. ontological information CONJUNCTION inferences rules. inferences rules CONJUNCTION ontological information. IterefinE HYPONYM-OF KG refinement framework. inferences rules USED-FOR one. ontological information USED-FOR one. ComplEx HYPONYM-OF KG embeddings. KG embeddings USED-FOR IterefinE. ontological information USED-FOR IterefinE. type - supervised embeddings USED-FOR KG. KG benchmarks EVALUATE-FOR embeddings. embeddings USED-FOR KG. PSL - KGI USED-FOR KG. overall weighted F1 score EVALUATE-FOR embeddings. Task is KG - based question answering. OtherScientificTerm is ontologies. Method is IterefinE framework. ,"This paper proposes a novel KG refinement task for KGs. The authors propose a new KG-based question answering framework based on the IterefinE framework. The key idea is to use embeddings from text sources for inferring new facts in Knowledge Graphs (KGs). The authors use two techniques to improve the KGS refinement: (1) using inferences rules and (2) using the ontological information in KG embedding. The proposed techniques are evaluated on a variety of KG benchmarks, including ComplEx and KG benchmark. ","This paper proposes a novel KG refinement task for inferring new facts from text sources. Knowledge Graphs (KGs) are a set of embeddings that can be used to generate new facts, which are then used for KG-based question answering. The proposed IterefinE framework is based on the Iterefine framework. The authors propose two techniques to perform KG refining, one based on ontological information and the other based on inferences rules. The paper shows that the proposed KG on ComplEx and ComplEx-KGI are better than the original KGs on the KG benchmarks. They also show that the type-supervision of the KGs can be improved by using PSL-GI to improve the overall weighted F1 score of a KG. "
20997,SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"binary predictions USED-FOR KBC quality. evaluation paradigm USED-FOR model selection criteria. real - world entities PART-OF KB. model COMPARE KB. KB COMPARE model. benchmark EVALUATE-FOR KB embeddings models. completion task EVALUATE-FOR ranking task. prediction separability FEATURE-OF KB embedding models. thresholding PART-OF TransE. classification F1 score EVALUATE-FOR TransE. Method is Knowledge base completion ( KBC ) methods. Material are knowledge base ( KB ), and FB14k - QAQ. Generic are method, and models. OtherScientificTerm are likelihood ranking, evaluation data structure, and KB queries. Task are ranking setting, and ranking - based and classification - based evaluation. ","This paper proposes a new evaluation paradigm for model selection criteria for Knowledge base completion (KBC) methods. The proposed method is based on binary predictions for KBC quality, where the likelihood ranking is a function of the evaluation data structure. The authors show that KB embeddings models have better prediction separability than KB embedding models with real-world entities in KB. They also show that the proposed method, called FB14k-QAQ, outperforms KB in a ranking task on a benchmark. The main contribution of the paper is the introduction of a new thresholding in TransE, which improves the classification F1 score of TransE by a factor of 1.5 for ranking-based and classification-based evaluation. ","This paper proposes a new evaluation paradigm for model selection criteria. Knowledge base completion (KBC) methods rely on binary predictions to evaluate the KBC quality. The authors propose a new method, called FB14k-QAQ, where the likelihood ranking is based on the evaluation data structure, and the models are evaluated on real-world entities. The proposed method is evaluated on a ranking setting, and is shown to outperform KB in terms of prediction separability of KB embedding models on a benchmark. TransE uses thresholding in the ranking task and classification F1 score in the classification-based evaluation. "
21006,SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,dialog system models USED-FOR tasks. human annotations USED-FOR dialog system models. language priors USED-FOR down - stream NLP tasks. BERT CONJUNCTION GPT-2. GPT-2 CONJUNCTION BERT. GPT-2 HYPONYM-OF large pre - trained language models. BERT HYPONYM-OF large pre - trained language models. pre - trained language models USED-FOR dialog response generation. Alternating Roles Dialog Model ( ARDM ) HYPONYM-OF framework. large pretrained language model USED-FOR ARDM. belief states CONJUNCTION dialog acts. dialog acts CONJUNCTION belief states. It USED-FOR conversations. supervision USED-FOR It. human annotations USED-FOR It. belief states HYPONYM-OF human annotations. dialog acts HYPONYM-OF human annotations. ARDM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE ARDM. CamRest676 CONJUNCTION MultiWOZ. MultiWOZ CONJUNCTION CamRest676. task - oriented dialog datasets EVALUATE-FOR state - of - the - art methods. task - oriented dialog datasets EVALUATE-FOR ARDM. MultiWOZ HYPONYM-OF task - oriented dialog datasets. CamRest676 HYPONYM-OF task - oriented dialog datasets. ARDM USED-FOR non - collaborative tasks. persuasion HYPONYM-OF non - collaborative tasks. ARDM USED-FOR human - like responses. ARDM USED-FOR persuasion tasks. ,"This paper proposes a framework called Alternating Roles Dialog Model (ARDM) to learn dialog system models for tasks with human annotations. The framework is based on language priors for down-stream NLP tasks where the goal is to generate dialog response generation using pre-trained language models such as BERT and GPT-2. ARDM is trained using a large pretrained language model and human annotations such as belief states, dialog acts, and dialog acts. It is also trained with supervision to learn the human-like responses to conversations. Experiments on task-oriented dialog datasets such as CamRest676 and MultiWOZ show that ARDM performs better than state-of-the-art methods on non-collaborative tasks like persuasion.","This paper presents a framework for learning dialog system models for tasks that require human annotations. The authors propose an Alternating Roles Dialog Model (ARDM) which is based on language priors for down-stream NLP tasks. ARDM uses a large pretrained language model (BERT or GPT-2) and human annotations (e.g., belief states, dialog acts) for dialog response generation and supervision. It can be used to generate human-like responses to conversations. Experiments on task-oriented dialog datasets such as CamRest676 and MultiWOZ show that ARDM outperforms state-of-the-art methods on non-collaborative tasks such as persuasion. "
21015,SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,deep neural network USED-FOR classification. softmax values FEATURE-OF network. implied loss FEATURE-OF uncertainty measure. confidence measures USED-FOR Top k. networks USED-FOR method. binning values USED-FOR confidence measures. Generic is values. ,"This paper proposes a deep neural network for classification with softmax values. The proposed method is based on the binning values of the confidence measures for Top k. The authors propose a new uncertainty measure, the implied loss, which is defined as the difference between the two values. They show that the proposed method can be applied to a wide range of networks.",This paper proposes a deep neural network for classification. The proposed method is based on confidence measures for Top k. The confidence measures are based on binning values of the network with softmax values. The uncertainty measure is defined as the implied loss. The authors show that the proposed values are more robust to changes in the values. 
21024,SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"architecture CONJUNCTION hyperparameters. hyperparameters CONJUNCTION architecture. generalization FEATURE-OF neural networks. wide neural networks USED-FOR gradient descent. spectrum FEATURE-OF NNGP kernel. kernel USED-FOR gradient descent. kernel USED-FOR Gaussian Processes. Neural Network Gaussian Process ( NNGP ) kernel HYPONYM-OF kernel. Neural Tangent Kernel ( NTK ) HYPONYM-OF kernel. NTK COMPARE NNGP kernel. NNGP kernel COMPARE NTK. spectrum COMPARE NNGP kernel. NNGP kernel COMPARE spectrum. spectrum FEATURE-OF NTK. Fully Connected Networks ( FCNs ) CONJUNCTION Convolutional Neural Networks ( CNNs ). Convolutional Neural Networks ( CNNs ) CONJUNCTION Fully Connected Networks ( FCNs ). Convolutional Neural Networks ( CNNs ) HYPONYM-OF architectures. Fully Connected Networks ( FCNs ) HYPONYM-OF architectures. CNNs COMPARE FCNs. FCNs COMPARE CNNs. learning dynamics FEATURE-OF CNNs. average pooling FEATURE-OF CNNs. pooling FEATURE-OF CNNs. Task is deep learning. OtherScientificTerm are wide network limit, large depth limit, and hyperparameter space. Method are random networks, and NNGP. Metric are trainability, and training accuracy. Material is real datasets. ","This paper studies the generalization of neural networks with different architectures and hyperparameters. The authors propose a new kernel called Neural Tangent Kernel (NTK) that can be used for gradient descent with wide neural networks. NTK has a spectrum that is much wider than the NNGP kernel, and can be applied to Gaussian Processes. They show that NTK can achieve better generalization than other architectures such as Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). The authors also show that CNNs with average pooling are better than FCNs in terms of learning dynamics. ","This paper studies the generalization of neural networks with different architectures and hyperparameters. The authors propose a new kernel for gradient descent, the Neural Network Gaussian Process (NNGP) kernel. The kernel is based on the Neural Tangent Kernel (NTK) kernel, which has a wide network limit and a large depth limit. They show that the NTK kernel is more general than the NNGP kernel in terms of the spectrum of the neural networks. They also show that NTK has a larger spectrum than the original NTK, and that it is more robust to changes in the hyperparameter space. Finally, the authors provide a theoretical analysis of the training dynamics of CNNs with different learning dynamics and show that FCNs and Convolutional Neural Networks (CNNs) have better average pooling compared to CNNs. "
21033,SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"computational methods USED-FOR protein folding. geometric invariance CONJUNCTION computational efficiency. computational efficiency CONJUNCTION geometric invariance. graph - based method USED-FOR protein models. GRAPHQA HYPONYM-OF graph - based method. GRAPHQA USED-FOR protein models. geometric invariance HYPONYM-OF favorable properties. representation learning HYPONYM-OF favorable properties. state - ofthe - art EVALUATE-FOR hand - engineered and representation - learning approaches. Material is Proteins. Task is biological processes. OtherScientificTerm are 3D structure, protein ’s structure, and sequential and 3D structure. Method is GRAPHQA components. ","This paper proposes a new graph-based method for protein folding, GRAPHQA, which combines geometric invariance and computational efficiency. The authors show that GRAPHA is able to learn a 3D structure of the protein’s structure, which can be used as a basis for learning a sequence of protein models. The paper also shows that the learned structure can be combined with existing hand-engineered and representation-learning approaches to achieve state-of-the-art performance. ","This paper proposes a novel graph-based method for protein models, GRAPHQA, that combines geometric invariance and computational efficiency. The authors show that the proposed method is able to learn a 3D structure that is invariant to changes in protein’s structure. They also show that it can learn a sequence of 3D structures that are orthogonal to each other. The paper also shows that it is possible to learn sequential and 3D protein structures. "
21042,SP:5188280131b58a35d3deda126a0754aea8fa6e58,"loss function FEATURE-OF neural network. geometry of the functional space CONJUNCTION parameterization of this space. parameterization of this space CONJUNCTION geometry of the functional space. network ’s weights USED-FOR parameterization of this space. pure critical points COMPARE spurious critical points. spurious critical points COMPARE pure critical points. loss function FEATURE-OF linear neural networks. determinantal variety HYPONYM-OF functional space. linear maps HYPONYM-OF determinantal variety. bounded rank FEATURE-OF linear maps. functional space USED-FOR network. loss functions CONJUNCTION parameterizations. parameterizations CONJUNCTION loss functions. loss functions FEATURE-OF linear networks. architectures USED-FOR linear maps. loss landscape FEATURE-OF linear networks. determinantal variety FEATURE-OF functional space. Generic is space. OtherScientificTerm are parameterization, determinantal varieties, smooth convex losses, filling architectures, quadratic loss, non - filling architectures, and architecture. Task is landscape of linear networks. ","This paper studies the landscape of linear networks in terms of the loss function of a neural network. The network’s weights are defined as the geometry of the functional space and the parameterization of this space. The paper shows that pure critical points are better than spurious critical points, and that the determinantal variety of linear neural networks (i.e., linear maps with bounded rank) is better than the pure critical point. The authors also show that the loss functions and parameterizations in linear networks are more general than in non-filling architectures. The main contribution of the paper is to show that linear networks with bounded loss landscape are more robust to smooth convex losses than linear networks that have quadratic loss. ","This paper studies the landscape of linear networks in terms of the loss function of a neural network in the functional space, which is defined as the geometric of the network’s weights and the parameterization of this space. The authors show that the space is a determinantal variety of the linear maps of linear maps, which are defined as a bounded rank of the parameters of a linear network. They also show that this space is more robust to spurious critical points than pure critical points. They show that linear networks with different loss functions and parameterizations have different loss landscape. They further show that for linear networks that have different architectures for linear maps (e.g., smooth convex losses) and for non-filling architectures with different filling architectures (i.e., quadratic loss), the landscape is similar."
21051,SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"Inductive and unsupervised graph learning USED-FOR predictive or information retrieval tasks. graph similarity evaluation USED-FOR learning processes. reconstruction error based loss functions USED-FOR learning processes. embedding of the subgraph vector distribution USED-FOR output vector representation. output vector representation USED-FOR graph. SEED framework USED-FOR subgraphs. reconstruction errors FEATURE-OF subgraphs. SEED CONJUNCTION graph isomorphism. graph isomorphism CONJUNCTION SEED. SEED framework COMPARE competitive baseline methods. competitive baseline methods COMPARE SEED framework. public benchmark datasets EVALUATE-FOR SEED framework. OtherScientificTerm are label information, subgraph samples, subgraph vectors, and subgraph vector distribution. Method is graph learning. Material is graph structured objects. ","This paper studies the problem of unsupervised graph learning in the context of predictive or information retrieval tasks. The authors propose a new graph similarity evaluation for learning processes based on reconstruction error based loss functions. The proposed SEED framework is based on the embedding of the subgraph vector distribution in the output vector representation of a graph. The subgraph vectors are then used as input to the embeddings of subgraph samples, and the output vectors are used as output vectors to generate the final graph.  The authors show that SEED can achieve state-of-the-art performance on several public benchmark datasets. ","This paper proposes a new framework for unsupervised graph learning for predictive or information retrieval tasks. The main idea is to use graph similarity evaluation for learning processes based on reconstruction error based loss functions. The authors propose a SEED framework for learning subgraphs with reconstruction errors, which is based on the embedding of the subgraph vector distribution in the output vector representation of the graph. The subgraph vectors are constructed from a set of subgraph samples, and the label information is extracted from the input subgraph. The proposed framework is evaluated on several public benchmark datasets, and is shown to outperform competitive baseline methods in terms of SEED and graph isomorphism. "
21060,SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,Counterfactual regret minimization ( CFR ) methods USED-FOR twoplayer zero - sum extensive games. imperfect information FEATURE-OF twoplayer zero - sum extensive games. vanilla CFR USED-FOR large - scale games. game tree USED-FOR vanilla CFR. Lazy - CFR HYPONYM-OF CFR algorithm. lazy update strategy USED-FOR CFR algorithm. Lazy - CFR COMPARE vanilla CFR. vanilla CFR COMPARE Lazy - CFR. regret EVALUATE-FOR Lazy - CFR. regret COMPARE regret. regret COMPARE regret. Lazy - CFR COMPARE regret. regret COMPARE Lazy - CFR. regret EVALUATE-FOR vanilla CFR. regret EVALUATE-FOR vanilla CFR. Lazy - CFR COMPARE CFR. CFR COMPARE Lazy - CFR. ,"This paper studies counterfactual regret minimization (CFRM) methods for twoplayer zero-sum extensive games with imperfect information. The authors propose a new CFR algorithm called Lazy-CFR, which is based on the lazy update strategy. The main idea is to use vanilla CFR for large-scale games where the game tree is not fully connected to the environment. The regret is shown to be lower than vanilla CFR, and Lazy -CFR is also shown to have a lower regret than CFR.","The paper proposes counterfactual regret minimization (CFR) methods for twoplayer zero-sum extensive games with imperfect information. The proposed CFR algorithm, Lazy-CFR, is based on the lazy update strategy. The authors show that vanilla CFR for large-scale games is equivalent to vanilla CFR on the game tree. They also show that the regret of Lazy -CFR is lower than that of vanilla CFR."
21069,SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"Unsupervised Domain Adaptation ( UDA ) methods USED-FOR transferable features. explicit feature distribution modeling USED-FOR UDA. Distribution Matching Prototypical Network ( DMPN ) USED-FOR deep features. Gaussian mixture distributions USED-FOR Distribution Matching Prototypical Network ( DMPN ). Gaussian mixture distributions USED-FOR deep features. domain discrepancy losses PART-OF DMPN. probabilistic interpretations FEATURE-OF domain discrepancy losses. one USED-FOR pseudo negative log likelihood. classification loss CONJUNCTION domain discrepancy losses. domain discrepancy losses CONJUNCTION classification loss. labeled source data CONJUNCTION domain discrepancy losses. domain discrepancy losses CONJUNCTION labeled source data. classification loss USED-FOR DMPN. labeled source data USED-FOR classification loss. DMPN USED-FOR discriminative and domain invariant features. domain discrepancy losses USED-FOR DMPN. Digits Image transfer task EVALUATE-FOR state - of - the - art approaches. Digits Image transfer task EVALUATE-FOR approach. mean accuracy EVALUATE-FOR DMPN. VisDA 2017 dataset EVALUATE-FOR DMPN. hyper - parameter sensitivity analysis EVALUATE-FOR approach. hyper - parameter changes FEATURE-OF approach. OtherScientificTerm are feature distribution discrepancy, feature distributions, Gaussian component means, and source feature distribution. Generic is methods. Task is UDA tasks. ","This paper studies the problem of unsupervised domain adaptation (UDA) methods for transferable features from source domain to target domain. The authors propose a new method called Distribution Matching Prototypical Network (DMPN) that uses Gaussian mixture distributions to learn deep features from labeled source data and domain discrepancy losses in the DMPN. The proposed method is based on explicit feature distribution modeling, where the feature distribution discrepancy between source and target domain is computed using Gaussian component means. The paper shows that the proposed method achieves better mean accuracy than existing state-of-the-art approaches on the Digits Image transfer task. ","This paper proposes a new unsupervised Domain Adaptation (UDA) methods for transferable features from one domain to another. The authors propose to use explicit feature distribution modeling in UDA. The key idea is to use Gaussian mixture distributions for the deep features, and then use Distribution Matching Prototypical Network (DMPN) for the classification loss and domain discrepancy losses in DMPN with probabilistic interpretations of the feature distribution discrepancy between the source and target domains. The proposed methods are evaluated on the Digits Image transfer task and compared to state-of-the-art approaches on the VisDA 2017 dataset. The approach is evaluated on hyper-parameter sensitivity analysis and mean accuracy. The paper shows that the proposed approach is able to learn discriminative and domain invariant features. "
21078,SP:40be996e8bb86e887077b762b87c7c34a786ac98,"deep generative models USED-FOR tasks. Continuous Normalizing Flows ( CNFs ) HYPONYM-OF deep generative models. conditional image generation CONJUNCTION downstream predictive tasks. downstream predictive tasks CONJUNCTION conditional image generation. CNFs USED-FOR conditional image generation. CNFs USED-FOR downstream predictive tasks. model USED-FOR highdimensional latent code. class - specific supervised code CONJUNCTION unsupervised code. unsupervised code CONJUNCTION class - specific supervised code. InfoCNF HYPONYM-OF conditional CNF. unsupervised code PART-OF conditional CNF. class - specific supervised code PART-OF conditional CNF. gating networks USED-FOR error tolerances. gating networks USED-FOR ordinary differential equation ( ODE ) solvers. partitioning strategy USED-FOR InfoCNF. error tolerances FEATURE-OF ordinary differential equation ( ODE ) solvers. InfoCNF USED-FOR error tolerances. gating networks USED-FOR InfoCNF. test accuracy EVALUATE-FOR baseline. InfoCNF COMPARE baseline. baseline COMPARE InfoCNF. NFEs FEATURE-OF CIFAR10. likelihood scores EVALUATE-FOR InfoCNF. CIFAR10 EVALUATE-FOR InfoCNF. test accuracy EVALUATE-FOR InfoCNF. partitioning strategy USED-FOR extrapolation. partitioning strategy USED-FOR InfoCNF. time - series data USED-FOR InfoCNF. time - series data EVALUATE-FOR partitioning strategy. Task is exact likelihood estimation. OtherScientificTerm are latent space, and labeled information. ","This paper proposes Continuous Normalizing Flows (CNFs), a family of deep generative models for tasks where the latent space is a highdimensional latent code. CNFs are useful for conditional image generation and downstream predictive tasks. The authors propose InfoCNF, a conditional CNF that combines the unsupervised code and the class-specific supervised code in a single conditional. The main idea is to use gating networks to improve the error tolerances of ordinary differential equation (ODE) solvers by partitioning the data into two parts: (1) a partitioning strategy for extrapolation, and (2) a Gating network for exact likelihood estimation.  The authors demonstrate the effectiveness of the proposed method on CIFAR10 with NFEs. The likelihood scores of the learned model are better than the baseline. ","This paper proposes Continuous Normalizing Flows (CNFs), a family of deep generative models for tasks where the model is trained on highdimensional latent code. CNFs are used for conditional image generation and downstream predictive tasks. The conditional CNF consists of a class-specific supervised code, an unsupervised code, and a conditional cNF. The authors propose a partitioning strategy for extrapolation based on time-series data, and use gating networks to improve the error tolerances of ordinary differential equation (ODE) solvers. Experiments on CIFAR10 and NFEs show that InfoCNF outperforms the baseline in terms of test accuracy and likelihood scores. "
21087,SP:97764e3393216106ff2ac3f550845acf4636119f,"nonlinear functions USED-FOR approximation of the value function. Temporal - Difference ( TD ) learning algorithm USED-FOR nonlinear functions. lazy training regime FEATURE-OF algorithm. non - lazy TD learning USED-FOR models. Generic are problem, regime, model, and convergence. OtherScientificTerm are approximating function, learning process, scaling, and exponential convergence. Method are lazy training, neural networks, and underand over - parametrized frameworks. ","This paper studies the problem of learning nonlinear functions for approximation of the value function. The authors propose a Temporal-Difference (TD) learning algorithm for learning the approximation of nonlinear function in a lazy training regime. The problem is formulated as an optimization problem, where the objective is to find an approximating function that is close to the true value function in the learning process. In the regime, the model is trained with non-lazy TD learning, and the goal is to learn a model that converges to a value function close to its true value. The main contribution of the paper is to show that the proposed algorithm achieves exponential convergence in a relatively simple and effective way. The paper also provides theoretical guarantees for the convergence of the algorithm.   The authors also provide a theoretical analysis of the effect of lazy training on the performance of neural networks trained with underand over-parametrized frameworks. ","This paper proposes a Temporal-Difference (TD) learning algorithm for learning nonlinear functions for approximation of the value function. The authors show that the proposed algorithm achieves exponential convergence in the lazy training regime under the assumption that the learning process is linear in the number of parameters. They also show that under the same regime, the model converges to a value function that is non-asymptotically close to the approximating function. Finally, they show that models trained with non-lazy TD learning can achieve exponential convergence under underand over-parametrized frameworks."
21096,SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"reinforcement learning problem USED-FOR hypothesis verification. agents USED-FOR problem. action sequence CONJUNCTION post - condition. post - condition CONJUNCTION action sequence. pre - condition CONJUNCTION action sequence. action sequence CONJUNCTION pre - condition. Generic are agent, and they. ","This paper studies the reinforcement learning problem for hypothesis verification. The problem is formulated as a problem where agents are given an action sequence, a pre-condition, and a post-condition. The agent has to decide whether to accept the action sequence or not. The goal is to minimize the number of times that the agent takes the action, and to make sure that they accept the post-conditions.","This paper proposes a reinforcement learning problem for hypothesis verification. The problem is formulated as a reinforcement learning problem, where the agent is given a set of agents, and the goal is to verify that they are the best agents for the given problem. The agent is trained on the set of actions, the action sequence, the post-condition, and a pre-condition."
21105,SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"neural networks USED-FOR approximate reasoning. fixed dimensional latent space USED-FOR approximate reasoning. latent space FEATURE-OF approximate reasoning. formula space CONJUNCTION latent space. latent space CONJUNCTION formula space. embeddings COMPARE predicted embeddings. predicted embeddings COMPARE embeddings. formula space FEATURE-OF rewrite steps. latent space FEATURE-OF rewrite steps. graph neural networks USED-FOR rewrite - success of statements. mathematical disciplines PART-OF corpus of mathematical formulas. OtherScientificTerm are transformations, semantic features, vector space, rewrite rule, and predicted latent representations. Generic are reasoning, and they. ","This paper studies the problem of approximate reasoning in the fixed dimensional latent space of graph neural networks. The authors propose a rewrite-success of statements in the latent space, which is a special case of the rewrite steps in the formula space and latent space. The rewrite rule is based on the rewrite rule in the vector space, and the authors show that the embeddings of the proposed embedding are better than those of the original and predicted latent representations. The paper also shows that the learned representations are more interpretable than the original representations. ","This paper proposes a novel approach to approximate reasoning in a fixed dimensional latent space. The authors propose to use graph neural networks to perform rewrite-success of statements in the latent space, where the transformations are based on semantic features. The proposed approach is based on a rewrite rule, which is a combination of two mathematical disciplines: (1) a corpus of mathematical formulas, and (2) a vector space, which consists of the embeddings of the words and the predicted latent representations. The rewrite steps are defined in the formula space and latent space of the rewrite steps. The paper shows that the proposed approach outperforms other approaches in terms of reasoning."
21114,SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"multi - view geometry USED-FOR methods. approach USED-FOR depth. images CONJUNCTION sparse depth measurements. sparse depth measurements CONJUNCTION images. images USED-FOR approach. sparse depth measurements USED-FOR depth. images USED-FOR depth. global - local network architecture USED-FOR inductive bias. model USED-FOR monocular dense depth estimation. sparse ground truth USED-FOR model. sparse ground truth USED-FOR monocular dense depth estimation. global parameters USED-FOR metric agent motion. network USED-FOR global parameters. Method are Natural intelligent agents, artificial systems, and natural agents. OtherScientificTerm are equations of projective geometry, visual and haptic feedback, and sparse supervision. ",This paper proposes a new approach to measure depth based on images and sparse depth measurements. The approach is based on a global-local network architecture that is able to capture the inductive bias of natural intelligent agents. The authors show that the global parameters of the network can be used to estimate the metric agent motion. The proposed method is evaluated on a variety of datasets.,"This paper proposes a new approach to measure depth based on images and sparse depth measurements. The approach is based on multi-view geometry, which is an extension of existing methods based on single-view geometries. Natural intelligent agents can be represented as a set of equations of projective geometry, while artificial systems can be modeled as a series of natural agents. The authors propose a global-local network architecture to reduce inductive bias. The proposed model is used for monocular dense depth estimation based on sparse ground truth. The global parameters of the metric agent motion are learned by the network, and the global parameters are used as input to the network. The network is trained with visual and haptic feedback, and sparse supervision."
21123,SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,word pieces USED-FOR machine learning tasks. word pieces USED-FOR natural language models. natural language models USED-FOR machine learning tasks. machine learning tasks USED-FOR opaque ids. hash functions USED-FOR hash tokens. multi - layer Transformer USED-FOR Bloom filter digests. multi - layer Transformer USED-FOR models. accuracy EVALUATE-FOR models. They COMPARE models. models COMPARE They. computational budget FEATURE-OF sampled softmax. sampled softmax USED-FOR models. multi - layer Transformer USED-FOR Bloom filter digests. method USED-FOR problems. this USED-FOR problems. this USED-FOR method. large vocabulary size FEATURE-OF problems. Method is Bloom filter. OtherScientificTerm is hashing. ,"This paper studies the problem of learning word pieces for machine learning tasks using natural language models. The authors propose Bloom filter, a multi-layer Transformer for Bloom filter digests. They show that this method can solve a variety of problems with large vocabulary size and large computational budget. They also show that models trained with sampled softmax can achieve better accuracy than models trained without hashing. ","This paper proposes a novel Bloom filter for word pieces in natural language models for machine learning tasks for opaque ids. The authors propose a multi-layer Transformer for Bloom filter digests, where hash functions are used to generate hash tokens. They show that the proposed models achieve better accuracy in terms of computational budget compared to models trained on a sampled softmax. They also show that this method can be applied to problems with large vocabulary size. "
21132,SP:745dd86d7f7bba79a02d27922003b764b620f83e,"grouping policy USED-FOR small part proposals. grouping policy USED-FOR learningbased agglomerative clustering framework. local context USED-FOR part - level features. largescale fine - grained 3D part dataset EVALUATE-FOR method. method USED-FOR knowledge of parts. PartNet HYPONYM-OF largescale fine - grained 3D part dataset. shape segmentation baselines COMPARE approach. approach COMPARE shape segmentation baselines. Task are discovering 3D parts, and contextual bandit problem. Generic is prior. Method is data - driven shape segmentation approaches. ","This paper proposes a learningbased agglomerative clustering framework based on a grouping policy for small part proposals. The proposed method is based on the largescale fine-grained 3D part dataset, PartNet. The key idea is to use a local context to learn the part-level features from the data. The authors show that the proposed approach outperforms existing shape segmentation baselines in terms of knowledge of parts. ",The paper proposes a learningbased agglomerative clustering framework with a grouping policy for small part proposals. The method is based on data-driven shape segmentation approaches. The key idea is to use local context to learn the part-level features in the context of the prior. The proposed method is evaluated on a largescale fine-grained 3D part dataset called PartNet. The authors show that the proposed approach outperforms the shape segmentsation baselines on the contextual bandit problem.
21141,SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"input / output datasets USED-FOR they. gender - related characteristics CONJUNCTION hair color. hair color CONJUNCTION gender - related characteristics. generative adversarial network ( GAN ) USED-FOR images of black - haired men. edit USED-FOR transformation. latent space FEATURE-OF transformation. autoencoder USED-FOR transformed data. editing transformation USED-FOR transformed data. transformation USED-FOR complex and non - linear transformations. latent trained space USED-FOR transformation. data domains CONJUNCTION modalities. modalities CONJUNCTION data domains. modalities CONJUNCTION applications. applications CONJUNCTION modalities. technique USED-FOR data domains. applications PART-OF biology. image transformations USED-FOR it. removal of batch artifacts HYPONYM-OF biology. removal of batch artifacts HYPONYM-OF applications. Method are generative neural networks, discriminator, generative models, and neuron editing. OtherScientificTerm are blond - haired men, source distribution, target distribution, manifold, distribution shifts, neuron ’s activations, unwanted noise, and drug treatments. Material is images of black - haired women. ","This paper proposes a generative adversarial network (GAN) to generate images of black-hired men from input/output datasets, where they can be used to train generative neural networks. The authors use a discriminator to learn the target distribution of the source distribution and target distribution, and then use an autoencoder to generate transformed data using an editing transformation on the transformed data. The transformation is based on a latent space in the latent trained space, which is used to generate complex and non-linear transformations. The paper shows that this technique can be applied to different data domains, modalities, and applications in biology (e.g., removal of batch artifacts, removal of drug treatments). ","This paper proposes a generative adversarial network (GAN) for generating images of black-hired men and blond-haired women, where they are trained on input/output datasets. The authors propose to use generative neural networks, where the discriminator is trained on the source distribution and the target distribution is a manifold. The paper proposes to use an autoencoder to generate transformed data using an editing transformation in the latent space of the transformed data. The transformation is a combination of complex and non-linear transformations on the latent trained space. The proposed technique is evaluated on three different data domains and three different modalities (e.g., removal of batch artifacts in biology, removal of drug treatments in biology) and it is shown that it can be applied to image transformations. "
21150,SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"training algorithms CONJUNCTION model architectures. model architectures CONJUNCTION training algorithms. reinforcement learning CONJUNCTION image semantic segmentation. image semantic segmentation CONJUNCTION reinforcement learning. few - shot image classification CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION few - shot image classification. model architectures USED-FOR few - shot domain. meta - learning approaches USED-FOR few - shot image classification. meta - learning approaches USED-FOR reinforcement learning. training algorithms USED-FOR few - shot domain. neural network representations USED-FOR meta - learning approaches. learning systems USED-FOR few - shot to many - shot settings. first - order meta - learning of initializations USED-FOR deep neural networks. first - order meta - learning of initializations USED-FOR dense, structured predictions. FOMAML CONJUNCTION Reptile. Reptile CONJUNCTION FOMAML. neural network architecture USED-FOR fast learning. generalization error EVALUATE-FOR meta - learning algorithms. small benchmark dataset EVALUATE-FOR meta - learning systems. meta - learning systems USED-FOR fewand many - shot settings. EfficientLab HYPONYM-OF neural network architecture. FP - k HYPONYM-OF small benchmark dataset. meta - learned initializations USED-FOR image segmentation. meta - learned initializations USED-FOR canonical few - shot learning problems. meta - learned initializations COMPARE random and ImageNet - trained initializations. random and ImageNet - trained initializations COMPARE meta - learned initializations. update routine USED-FOR tasks. FSS-1000 dataset EVALUATE-FOR network. Method are ensembling many models, relation networks, and MAML - type algorithms. OtherScientificTerm is initialization. Generic are task, and model. ","This paper studies the problem of few-shot image classification and reinforcement learning with meta-learning methods. The authors propose a new meta-training method for the few shot setting, which is based on the first-order meta-learned of initializations for deep neural networks. The main idea is to learn the initializations of the neural network by ensembling many models, and then use these initializations to perform dense, structured predictions. The paper shows that the proposed method is able to achieve better generalization error than the state-of-the-art on a small benchmark dataset, FP-k. ","This paper presents a meta-learning approach for few-shot image classification. The authors propose to use first-order meta-learning of initializations in deep neural networks for dense, structured predictions. This is done by ensembling many models into relation networks, and then using MAML-type algorithms for initialization. The main contribution of the paper is to propose a new neural network architecture, EfficientLab, for fast learning. The paper also presents a small benchmark dataset, FP-k, to evaluate the generalization error of meta-Learning systems in fewand many-shot settings. Experiments are conducted on the FSS-1000 dataset and show that the proposed network outperforms random and ImageNet-trained initializations for canonical few-shoot learning problems. "
21159,SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"unlabelled data USED-FOR few - shot learning. Prototypical Random Walk Networks(PRWN ) HYPONYM-OF SS - FSL approach. Prototypical Networks ( PN ) USED-FOR SS - FSL approach. Prototypical Networks ( PN ) USED-FOR Prototypical Random Walk Networks(PRWN ). random walk semi - supervised loss USED-FOR representations. random walk semi - supervised loss USED-FOR network. network USED-FOR representations. graph - based approaches USED-FOR few - shot learning. prototypical random walk notion USED-FOR compact and well - separated class representations. model COMPARE art. art COMPARE model. model COMPARE fully supervised prototypical networks. fully supervised prototypical networks COMPARE model. 1 - shot mini - Imagenet case EVALUATE-FOR it. accuracy EVALUATE-FOR it. robustness FEATURE-OF labelled / unlabelled class distribution mismatch. discriminative power test EVALUATE-FOR baseline. Task are human intelligence, transductive setting, and mini - Imagenet 5 - shot classification task. Method is AI models. OtherScientificTerm are graph - NN parameters, distractors, and unlabeled data. Material are collective test set, mini - Imagenet, and Omniglot. ","This paper proposes Prototypical Random Walk Networks (PRWN), a new SS-FSL approach called Prototypic Networks (PN) for few-shot learning with unlabeled data. Prototypically Networks (PN) is a SS-FSL approach based on Prototypicial Random Walk Network (PWN) that uses a random walk semi-supervised loss to learn representations from the unlabelled data. The network learns representations by using a prototypical random walk notion to learn compact and well-separated class representations using graph-NN parameters. The proposed model outperforms the state-of-the-art in the 1-shot mini-Imagenet case, and it also outperforms fully supervised prototypical networks in the 2-shot classification task. The paper also shows that the proposed model is more robust to distractors than the state of the art in the mini-imagenet setting. The authors also provide a discriminative power test to evaluate the performance of the proposed baseline.","This paper proposes a novel SS-FSL approach called Prototypical Random Walk Networks (PRWN) for few-shot learning on unlabelled data. The authors propose a novel network that uses a random walk semi-supervised loss to learn representations of the graph-NNN parameters. The proposed model is evaluated on the mini-Imagenet 5-shot classification task, where it outperforms the state-of-the-art in the 1-shot mini-imagenet case, and outperforms other graph-based approaches in the transductive setting. The paper also shows that the proposed model outperforms fully supervised prototypical networks on the discriminative power test, and the robustness of labelled/unlabelled class distribution mismatch is measured on Omniglot. "
21168,SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"machine learning USED-FOR remote sensing. labeled data USED-FOR machine learning. deep convolutional neural networks HYPONYM-OF models. selfsupervised learning approaches USED-FOR remote sensing domain. multi - sensor, multi - channel information USED-FOR remote sensing applications. Contrastive Sensor Fusion USED-FOR representations. Contrastive Sensor Fusion HYPONYM-OF self - supervised training objective. model USED-FOR representation. encoder USED-FOR representations. dataset USED-FOR encoder. unlabeled coterminous image triplets PART-OF dataset. remote sensing classification task EVALUATE-FOR representations. Material are unlabeled data, and coterminous data. Method is fused multi - sensor representations. Generic is method. ","This paper proposes a new self-supervised learning objective for remote sensing based on Contrastive Sensor Fusion. The proposed method is based on the idea of fused multi-sensor representations, which is an extension of the multi-channel information in multi-source sensing applications. The key idea is to use unlabeled data to train the model to learn a representation of the data, and then use the encoder to learn representations from this dataset. Experiments on the remote sensing classification task demonstrate the effectiveness of the proposed method.","This paper proposes a new self-supervised learning approach for remote sensing. The proposed method is based on contrastive sensor fusion, which is a novel approach for learning multi-sensor representations from unlabeled data. The authors show that the proposed method outperforms the state-of-the-art methods on the remote sensing classification task. "
21177,SP:4d8e054f07006b4f896721b5c24da805727d2c22,"fine - tuning HYPONYM-OF retraining technique. fine - tuning COMPARE retraining techniques. retraining techniques COMPARE fine - tuning. Learning rate rewinding USED-FOR unpruned weights. learning rate schedule USED-FOR weight rewinding. Learning rate rewinding COMPARE weight rewinding. weight rewinding COMPARE Learning rate rewinding. learning rate schedule USED-FOR Learning rate rewinding. rewinding techniques COMPARE fine - tuning. fine - tuning COMPARE rewinding techniques. accuracy CONJUNCTION compression ratios. compression ratios CONJUNCTION accuracy. rewinding techniques USED-FOR network - agnostic pruning algorithm. compression ratios EVALUATE-FOR network - agnostic pruning algorithm. accuracy EVALUATE-FOR network - agnostic pruning algorithm. Method are neural network pruning algorithms, and Weight rewinding. Generic is network. OtherScientificTerm are learning rate, and training schedule. ","This paper studies the problem of fine-tuning neural network pruning algorithms. The authors propose a new retraining technique, called Weight rewinding, which uses a learning rate schedule to re-weight the unpruned weights in the training process. They show that Weight Rewinding improves the accuracy and compression ratios of the proposed network-agnostic pruning algorithm. They also show that the proposed method is more efficient than other re-winding techniques.","This paper proposes a new retraining technique called fine-tuning, which is an extension of neural network pruning algorithms. The main idea is to re-weight the weights of the network during training. Weight rewinding is a regularization of the learning rate. The authors propose a learning rate schedule that rewinds the unpruned weights during training, and then re-weights the weights during the final training phase. The proposed network-agnostic pruning algorithm is shown to achieve better accuracy and compression ratios compared to other re-winding techniques."
21186,SP:3bb1c79f9482e09828eda45fbb2e654f37219365,normalized ) output margin CONJUNCTION generalization. generalization CONJUNCTION normalized ) output margin. output margin FEATURE-OF generalization. all - layer margin HYPONYM-OF margin. generalization EVALUATE-FOR deep models. theoretically inspired training algorithm USED-FOR all - layer margin. neural net USED-FOR adversarially robust setting. robust test error FEATURE-OF deep networks. Jacobian and hidden layer norms USED-FOR neural nets. Method is linear classifiers. Generic is algorithm. ,"This paper studies the problem of robustness of deep neural networks in the presence of adversarial attacks. The authors propose a theoretical inspired training algorithm to learn the all-layer margin, which is the (normalized) output margin of a neural network in the adversarially robust setting. The proposed algorithm is based on the Jacobian and hidden layer norms of neural nets. The theoretical results show that the proposed algorithm improves the robust test error of deep networks. ",This paper proposes an algorithm to improve the generalization of deep models with a normalized (or normalized) output margin and generalization with an all-layer margin. The authors propose a theoretically inspired training algorithm to optimize the output margin of the neural network in an adversarially robust setting. The proposed algorithm is based on Jacobian and hidden layer norms for neural nets with robust test error. Experiments are conducted on linear classifiers.
21195,SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"ungrounded dialogues CONJUNCTION unstructured documents. unstructured documents CONJUNCTION ungrounded dialogues. unstructured documents USED-FOR model. ungrounded dialogues USED-FOR model. limited training examples USED-FOR small parameters. benchmarks EVALUATE-FOR model. out - of - domain knowledge EVALUATE-FOR model. Task are intelligent conversational agent, and knowledge - grounded dialogue generation. Material is knowledge - grounded dialogues. Method are response generation model, disentangled response decoder, and generation model. ",This paper studies the problem of knowledge-grounded dialogue generation from ungrounded dialogues and unstructured documents. The authors propose a new response generation model based on a disentangled response decoder. The model is trained on a set of limited training examples with small parameters. The proposed model is evaluated on a variety of benchmarks with out-of-domain knowledge.,"This paper proposes an intelligent conversational agent for knowledge-grounded dialogue generation. The proposed model is based on unstructured dialogues, which are composed of both ungrounded dialogues (unstructured documents) as well as the unstructuring documents of the response generation model. The authors propose a disentangled response decoder, which generates a set of small parameters from limited training examples. The model is evaluated on several benchmarks on out-of-domain knowledge, and the model is shown to outperform the generation model in terms of accuracy."
21204,SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"parallel corpus USED-FOR neural machine translation models ( NMT ). non - parallel bilingual data USED-FOR decoding. non - parallel bilingual data USED-FOR training. training CONJUNCTION decoding. decoding CONJUNCTION training. non - parallel bilingual data USED-FOR Existing approaches. source to target translation model CONJUNCTION target to source translation model. target to source translation model CONJUNCTION source to target translation model. target to source translation model CONJUNCTION language models. language models CONJUNCTION target to source translation model. mirror - generative NMT ( MGNMT ) HYPONYM-OF single unified architecture. source to target translation model PART-OF single unified architecture. target to source translation model PART-OF single unified architecture. language models PART-OF single unified architecture. translation models CONJUNCTION language models. language models CONJUNCTION translation models. latent semantic space FEATURE-OF language models. non - parallel data USED-FOR translation directions. translation models CONJUNCTION language models. language models CONJUNCTION translation models. language models USED-FOR decoding. translation models USED-FOR decoding. MGNMT COMPARE approaches. approaches COMPARE MGNMT. Material are non - parallel corpora, and resource - rich and low - resource situations. ","This paper proposes a parallel corpus for neural machine translation models (NMT) with non-parallel bilingual data for training and decoding. Existing approaches are based on the use of parallel bilingual data to train the NMT on the non-preparable bilingual data. The authors propose a single unified architecture, mirror-generative NMT (MGNMT), which combines a source to target translation model with a target to source translation model and a language models. The proposed MGNMT is shown to perform better than existing approaches in both resource-rich and low-resource situations. The main contribution of the paper is to show that the translation models trained on the latent semantic space of language models and language models can be used for decoding and translation models for decoding. ","This paper presents a parallel corpus for training neural machine translation models (NNMT) on non-parallel bilingual data. Existing approaches are based on the use of a single unified architecture consisting of a source to target translation model, a target to source translation model and a language models. The paper proposes a new architecture called mirror-generative NMT (MGNMT) which is based on an existing architecture, mirror-genome-based NMT. MGNMT is shown to outperform existing approaches in both resource-rich and low-resource situations. The main contribution of the paper is to introduce a new language models in the latent semantic space, which can be used for translation directions, training and decoding. "
21213,SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,maximum entropy reinforcement learning algorithms USED-FOR Deep Reinforcement Learning ( DRL ). benchmarks EVALUATE-FOR sample efficiency. entropy term USED-FOR maximum entropy algorithms. entropy term USED-FOR bounded nature of the action spaces. entropy term PART-OF Soft Actor Critic ( SAC ). entropy term USED-FOR Mujoco benchmark. streamlined algorithms USED-FOR SAC. entropy maximization USED-FOR streamlined algorithms. non - uniform sampling method USED-FOR transitions. transitions PART-OF replay buffer. streamlined algorithm COMPARE SAC. SAC COMPARE streamlined algorithm. continuous control tasks EVALUATE-FOR streamlined algorithm. non - uniform sampling scheme USED-FOR streamlined algorithm. OtherScientificTerm is maximum entropy objective. Task is training. ,"This paper studies the sample efficiency of maximum entropy reinforcement learning algorithms for Deep Reinforcement Learning (DRL). The authors propose a new entropy term for maximum entropy algorithms for the bounded nature of the action spaces. The entropy term is used in Soft Actor Critic (SAC), which is a Mujoco benchmark. The authors show that the entropy maximization of SAC can be used to speed up the training of the SAC. The main contribution of the paper is a non-uniform sampling method for the transitions in the replay buffer. The proposed streamlined algorithm outperforms SAC on continuous control tasks.","This paper proposes a new benchmark for evaluating maximum entropy reinforcement learning algorithms for Deep Reinforcement Learning (DRL). The proposed benchmarks are based on the Mujoco benchmark, where the maximum entropy objective is defined as the bounded nature of the action spaces, and the entropy term is used to evaluate the sample efficiency of maximum entropy algorithms. The authors propose a Soft Actor Critic (SAC) algorithm that uses the same entropy term in the Soft actor Critic as in SAC. The main difference between SAC and other streamlined algorithms based on entropy maximization is that the authors propose to use a non-uniform sampling method for transitions in the replay buffer. The experiments show that the proposed streamlined algorithm outperforms SAC on continuous control tasks."
21222,SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"machine learning models USED-FOR adversarial attacks. industrial copyright detection tools USED-FOR web. industrial copyright detection tools USED-FOR adversarial attacks. neural net USED-FOR system. gradient methods USED-FOR system. AudioTag copyright detector CONJUNCTION YouTube ’s Content ID system. YouTube ’s Content ID system CONJUNCTION AudioTag copyright detector. Adversarial music USED-FOR industrial systems. YouTube ’s Content ID system HYPONYM-OF industrial systems. AudioTag copyright detector HYPONYM-OF industrial systems. Method are classifier, copyright detection systems, neural network based systems, and music identification method. Generic is they. OtherScientificTerm is attacks. Material is adversarial examples. ","This paper proposes a new classifier for the task of protecting the web from adversarial attacks on machine learning models. The proposed system is based on neural network based systems and uses gradient methods to train the system using a neural net. The authors show that the proposed method is able to identify the source of the attacks, and that they can be used to train a classifier that can detect the source and target of the attack. They also show that their method can be applied to other industrial systems such as AudioTag copyright detector and YouTube’s Content ID system. ","This paper proposes a new classifier for adversarial attacks on industrial copyright detection tools for the web. The authors propose to use neural network based systems. The system is based on gradient methods, where the classifier is trained on adversarial examples, and the system is trained using a neural net. The proposed method is evaluated on three industrial systems: AudioTag copyright detector, YouTube’s Content ID system, and a music identification method. "
21231,SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"visual explanation USED-FOR deep metric learning. model COMPARE classification. classification COMPARE model. framework USED-FOR metric learning applications. framework USED-FOR model. cross - view pattern discovery CONJUNCTION interactive retrieval. interactive retrieval CONJUNCTION cross - view pattern discovery. interactive retrieval HYPONYM-OF applications. cross - view pattern discovery HYPONYM-OF applications. Method are learning representation, and metric learning. OtherScientificTerm are overall activation map, and point - to - point activation intensity. ",This paper proposes a visual explanation for deep metric learning. The framework is based on the idea that the overall activation map of a learning representation can be represented as a set of point-to-point activation intensity. The model can be used for both cross-view pattern discovery and interactive retrieval. The authors show that the proposed framework can improve the performance of the model compared to classification. ,This paper proposes a visual explanation for deep metric learning. The learning representation is based on the overall activation map. The authors propose a framework to train the model and compare it to classification. The framework is applied to a variety of metric learning applications such as cross-view pattern discovery and interactive retrieval. The results show that the proposed metric learning can improve the point-to-point activation intensity.
21240,SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"learning control USED-FOR online lifelong learning scenario. they USED-FOR failure modes. computational resources USED-FOR model - based planning methods. model - based planning CONJUNCTION model - free learning. model - free learning CONJUNCTION model - based planning. planner CONJUNCTION model - free components. model - free components CONJUNCTION planner. Method are model - free policy learning methods, Adaptive Online Planning ( AOP ), AOP, continual learning agent, and reinforcement learning methods. OtherScientificTerm are compact networks, performance degradation, dynamics, constrained computation limits, and unpredictable changes in the world. Generic are setting, and algorithm. Task is planning. ","This paper proposes Adaptive Online Planning (AOP), an online lifelong learning scenario where the goal is to learn a continual learning agent that is able to adaptively adapt to new environments. The authors propose a new setting where the learning agent is a planner and the environment is a set of compact networks. The planner is a continuous learning agent, and the dynamics of the environment can be controlled by the planner. The algorithm is based on the idea that the planner can learn to adapt to the environment in a way that is consistent with the environment.  The authors show that AOP can adapt to different environments in a similar way to previous model-free policy learning methods. They also show that they can learn failure modes that are similar to the ones in previous models. ","This paper proposes Adaptive Online Planning (AOP), an online lifelong learning scenario where the goal is to learn a continual learning agent that is able to adaptively adapt to new environments. Adaptive online planning is an extension of model-free policy learning methods. The main idea is to use computational resources to learn model-based planning methods that are not constrained by compact networks. The authors show that they are able to learn failure modes that do not depend on the number of parameters of the compact networks, and that they do not require any performance degradation. They also show that the dynamics of the dynamics can be controlled by constrained computation limits. The setting is very similar to reinforcement learning methods, and the authors propose an algorithm that can be applied to this setting. The proposed algorithm is based on a planner, model, and a few other component of the planner. The algorithm is evaluated on a variety of environments, and it is shown that it can learn to adapt to unpredictable changes in the world. "
21249,SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"Visual attention mechanisms USED-FOR image captioning models. sparsemax CONJUNCTION Total - Variation Sparse Attention ( TVMAX ). Total - Variation Sparse Attention ( TVMAX ) CONJUNCTION sparsemax. sparsity - promoting transformations USED-FOR softmax attention mechanism. Total - Variation Sparse Attention ( TVMAX ) HYPONYM-OF sparsity - promoting transformations. sparsemax HYPONYM-OF sparsity - promoting transformations. sparsemax USED-FOR sparse attention weights. interpretability EVALUATE-FOR TVMAX transformation. humanrated caption quality CONJUNCTION attention relevance. attention relevance CONJUNCTION humanrated caption quality. TVMAX COMPARE attention mechanisms. attention mechanisms COMPARE TVMAX. attention relevance EVALUATE-FOR attention mechanisms. attention relevance EVALUATE-FOR TVMAX. humanrated caption quality EVALUATE-FOR attention mechanisms. humanrated caption quality EVALUATE-FOR TVMAX. OtherScientificTerm are image structure, relevant features, and sparsity. Material is Microsoft COCO and Flickr30k datasets. Method is softmax. ",This paper proposes a new softmax attention mechanism based on sparsity-promoting transformations such as sparsemax and Total-Variation Sparse Attention (TVMAX). The key idea is to use sparsemax for sparse attention weights and TVMAX for sparse features. The authors show that the TVMAX transformation improves interpretability and improves the humanrated caption quality and attention relevance compared to other attention mechanisms such as TVMAX. Experiments are conducted on the Microsoft COCO and Flickr30k datasets.,"This paper proposes a novel softmax attention mechanism based on sparsity-promoting transformations, namely sparsemax and Total-Variation Sparse Attention (TVMAX). The authors show that TVMAX improves interpretability and humanrated caption quality on the Microsoft COCO and Flickr30k datasets. TVMAX is shown to outperform other attention mechanisms in terms of attention relevance and attention quality. "
21258,SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,Neural networks USED-FOR structured data. Neural networks USED-FOR graphs. graphs HYPONYM-OF structured data. Predicting the evolution of dynamic graphs PART-OF graph mining. model USED-FOR evolution of dynamic graphs. graph neural network USED-FOR temporal evolution patterns of dynamic graphs. graph neural network CONJUNCTION recurrent architecture. recurrent architecture CONJUNCTION graph neural network. recurrent architecture USED-FOR temporal evolution patterns of dynamic graphs. generative model USED-FOR graph instance. graph instance USED-FOR topology. common network evolving dynamics FEATURE-OF artificial datasets. real - world datasets EVALUATE-FOR model. artificial datasets EVALUATE-FOR model. OtherScientificTerm is static graphs. Material is real - world networks. Generic is task. ,"This paper proposes a new model for predicting the evolution of dynamic graphs in graph mining. Neural networks are used to model structured data such as graphs. The authors propose a graph neural network and a recurrent architecture to learn temporal evolution patterns of dynamic graph. The graph instance is learned by a generative model, and the topology of the graph is learned from the graph instance. The model is evaluated on two real-world datasets and two artificial datasets with common network evolving dynamics. ",This paper proposes a novel model for predicting the evolution of dynamic graphs in graph mining. Neural networks are used to generate structured data (e.g. graphs) from graphs. The authors propose to use a graph neural network and a recurrent architecture to predict temporal evolution patterns of the dynamic graphs. They also propose a generative model to predict the topology of the graph instance. The model is evaluated on two real-world datasets and two artificial datasets with common network evolving dynamics. 
21267,SP:ff722957a1765c0568426ed88dd910a6b74054ef,"incomplete datasets USED-FOR machine learning applications. missing data imputation techniques USED-FOR filling missing values. method USED-FOR imputing missing features. method USED-FOR distribution of target assignments. incomplete data USED-FOR distribution of target assignments. generator network USED-FOR imputations. generator network USED-FOR imputations. predictor network USED-FOR classification uncertainties. generator network USED-FOR predictor network. imputed samples USED-FOR predictor network. CIFAR-10 image dataset CONJUNCTION real - world tabular classification datasets. real - world tabular classification datasets CONJUNCTION CIFAR-10 image dataset. real - world tabular classification datasets EVALUATE-FOR method. CIFAR-10 image dataset EVALUATE-FOR method. method USED-FOR generating imputations. class uncertainties FEATURE-OF classification task. OtherScientificTerm are missing values, distribution of missing values, missing features, and missingness rates. Method is discriminator network. ","This paper proposes a method for imputing missing features from incomplete datasets to machine learning applications. The missing data imputation techniques are commonly used for filling missing values. The authors propose a method to generate imputations from incomplete data using a generator network trained on the imputations generated by a discriminator network. The imputed samples are then used to train a predictor network that predicts the distribution of missing values, and the generator network is used to estimate the classification uncertainties. The method is evaluated on CIFAR-10 image dataset and real-world tabular classification datasets and shows that the proposed method can improve the performance of generating imputations in a classification task with class uncertainties.",This paper proposes a method for imputing missing features from incomplete datasets for machine learning applications. The missing data imputation techniques are used for filling missing values with the distribution of missing values. The proposed method uses a generator network to generate imputations from incomplete data and a predictor network to estimate the classification uncertainties from imputed samples. The discriminator network is trained to predict the missing features. The method is evaluated on CIFAR-10 image dataset and real-world tabular classification datasets. The results show that the proposed method can improve the class uncertainties of a classification task.
21276,SP:c051b0fe779d9e4131016970b7ba469b596f3009,"Off - policy estimation USED-FOR long - horizon problems. healthcare CONJUNCTION robotics. robotics CONJUNCTION healthcare. Off - policy estimation USED-FOR real - life applications. robotics HYPONYM-OF real - life applications. healthcare HYPONYM-OF real - life applications. curse of horizon FEATURE-OF importance - sampling - based methods. stationary distribution FEATURE-OF known behavior policy. estimator USED-FOR importance ratios of stationary distributions. Reproducing Kernel Hilbert Spaces ( RKHSs ) USED-FOR estimator. asymptotic consistency CONJUNCTION finite - sample generalization. finite - sample generalization CONJUNCTION asymptotic consistency. Method is high - fidelity simulators. Task is on - policy evaluation. Generic are approach, it, problem, and operator. Material is off - policy data. ",This paper studies the problem of on-policy evaluation in the context of high-fidelity simulators. The authors propose a new estimator based on Reproducing Kernel Hilbert Spaces (RKHSs) to estimate the importance ratios of stationary distributions of a known behavior policy over a stationary distribution. The estimator is based on the asymptotic consistency and finite-sample generalization of the estimator. The proposed approach is evaluated on two real-life applications: healthcare and robotics. The results show that the proposed estimator outperforms existing importance-sampling-based methods in terms of curse of horizon.,This paper proposes a new approach for long-horizon problems. The main idea is to use an estimator based on Reproducing Kernel Hilbert Spaces (RKHSs) to estimate the importance ratios of stationary distributions of the known behavior policy over the curse of horizon. The authors show that this estimator is more robust to asymptotic consistency and finite-sample generalization than other importance-sampling-based methods. The proposed approach is evaluated on high-fidelity simulators. 
21285,SP:065c900843011a71b70ed35357a2f71fe83872a7,"probabilistic framework USED-FOR dataset. Mixture Model ( MM ) HYPONYM-OF probabilistic framework. modes PART-OF dataset. Gaussian distribution FEATURE-OF modes. paintings dataset CONJUNCTION fashion images. fashion images CONJUNCTION paintings dataset. unlabelled modes PART-OF large datasets. fashion images HYPONYM-OF large datasets. paintings dataset HYPONYM-OF large datasets. plausible method USED-FOR probabilities. Generative Adversarial Network ( GAN ) framework USED-FOR plausible method. GAN USED-FOR distribution. GAN USED-FOR classification network. techniques USED-FOR unsupervised dataset. smooth linear interpolation USED-FOR outdistribution ” data. Method are Gaussian MM, GMM, and GMM paradigm. OtherScientificTerm are conditional likelihood, distribution index, Euclidean distances, responsibility distribution, latent representation of x, and dataset segments. Generic is responsibility. ",This paper proposes a probabilistic framework for training a dataset using a Mixture Model (MM) called Gaussian MM. The dataset consists of two large datasets: the paintings dataset and the fashion images. The two modes are based on the Gaussian distribution of the data and the two modes of the dataset are unlabelled modes. The authors propose a plausible method for estimating the probabilities of the two mode distributions. The proposed method is based on a GAN that predicts the distribution of each mode. The GAN is then used to train a classification network using the GAN.  The authors show that the proposed method can be used to estimate the conditional likelihood of the model. They also show that using smooth linear interpolation on the “outdistribution” data can improve the performance. ,"This paper proposes a probabilistic framework for training a dataset using a Mixture Model (MM) with a Gaussian MM. The dataset consists of two modes of the Gaussian distribution, the unlabelled modes and the ones with conditional likelihood. The authors propose a GMM paradigm, where the conditional likelihood is defined as the distribution index of the distribution over the Euclidean distances between the latent representation of x and the true distribution of x, and the responsibility distribution is defined over the dataset segments. They propose a plausible method to estimate the probabilities of the distributions of the two modes, which is based on the Generative Adversarial Network (GAN) framework. The GAN is used to compute the distribution of the parameters of the classification network. They also propose two techniques for training an unsupervised dataset using smooth linear interpolation on the “outdistribution” data."
21294,SP:2da1608209058d214f8671062cc9eb0833ba4831,method USED-FOR large capacity neural networks. accuracy CONJUNCTION dynamic computational cost. dynamic computational cost CONJUNCTION accuracy. accuracy EVALUATE-FOR method. fine - grained - level FEATURE-OF deep - learning architecture. residual block architecture USED-FOR convolutional channels. fine - grained manner FEATURE-OF residual block architecture. fine - grained manner FEATURE-OF convolutional channels. marginal aggregate posteriors of features PART-OF neural network. pre - specified prior distribution FEATURE-OF marginal aggregate posteriors of features. technique USED-FOR gates. CIFAR-10 and ImageNet datasets USED-FOR image classification. Cityscapes USED-FOR semantic segmentation. image classification CONJUNCTION Cityscapes. Cityscapes CONJUNCTION image classification. average computational cost COMPARE architecture. architecture COMPARE average computational cost. method USED-FOR architectures. method COMPARE architecture. architecture COMPARE method. ImageNet EVALUATE-FOR ResNet34 gated networks. accuracy EVALUATE-FOR ResNet18 model. top-1 accuracy EVALUATE-FOR ResNet34 gated networks. complexity EVALUATE-FOR ResNet18 model. features CONJUNCTION features. features CONJUNCTION features. features USED-FOR networks. OtherScientificTerm is convolutional maps. Generic is network. Method is batch - shaping. ,"This paper proposes a method for training large capacity neural networks in a fine-grained-level. The authors propose a residual block architecture for convolutional channels, which can be trained in a more efficient and efficient manner than a standard deep-learning architecture. The proposed method is based on batch-shaping, where the network is trained with the marginal aggregate posteriors of features in the neural network, which are defined by a pre-specified prior distribution. The technique is applied to the gates of the gates, and is shown to improve the accuracy and the dynamic computational cost of the proposed method. Empirical results on CIFAR-10 and ImageNet datasets for image classification and Cityscapes for semantic segmentation show that the proposed architecture performs better than the state-of-the-art ResNet34 gated networks on ImageNet with top-1 accuracy. The complexity of the ResNet18 model is also improved over the existing architecture. ",This paper proposes a method for large capacity neural networks with a fine-grained-level of the deep-learning architecture. The authors propose a residual block architecture for convolutional channels in the fine-rigged manner. The network is trained with marginal aggregate posteriors of features in the neural network with a pre-specified prior distribution. The method is evaluated on CIFAR-10 and ImageNet datasets for image classification and Cityscapes for semantic segmentation. Results show that the proposed method outperforms the average computational cost and dynamic computational cost of existing architectures on ImageNet and ResNet34 gated networks in terms of accuracy and top-1 accuracy. 
21303,SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"probabilistic importance inference approach USED-FOR pruning DNNs. approach COMPARE techniques. techniques COMPARE approach. lossless compression rates EVALUATE-FOR techniques. lossless compression rates EVALUATE-FOR approach. Method are Deep neural networks ( DNNs ), DNNs, DNN, and nonparemetric scoring test. OtherScientificTerm are energy and computational resources, and DNN ’s outputs. ",This paper proposes a probabilistic importance inference approach for pruning DNNs. Deep neural networks (DNNs) are a special case of DNN with energy and computational resources. The authors propose a new approach to prune the DNN’s outputs by using a nonparemetric scoring test. The proposed approach achieves better lossless compression rates than existing techniques.,This paper proposes a probabilistic importance inference approach for pruning DNNs. Deep neural networks (DNNs) can be pruned using energy and computational resources. The approach is evaluated on a nonparemetric scoring test and compared to other techniques with lossless compression rates. The authors show that the proposed approach outperforms other techniques in terms of the number of DNN’s outputs. 
21312,SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"approaches USED-FOR hierarchical reinforcement learning. approaches USED-FOR sub - goal structure. method USED-FOR iteratively compressing action trajectories. iteratively compressing action trajectories USED-FOR nested behavioral hierarchies. method USED-FOR nested behavioral hierarchies. action primitives USED-FOR deeper hierarchies. approach USED-FOR learning. transfer USED-FOR approach. Generic is perspective. OtherScientificTerm are compact code of action trajectories, and non - trivial hierarchical structure. ","This paper proposes a new approach for hierarchical reinforcement learning. The proposed method is based on iteratively compressing action trajectories into nested behavioral hierarchies, which can then be used as action primitives for learning deeper hierarchies. The approach uses a transfer between different sub-goal structure, where the subgoal structure is learned from the perspective of the compact code of actions. The authors show that the proposed approach can improve the performance of learning in the presence of non-trivial hierarchical structure.","This paper presents a new perspective on hierarchical reinforcement learning. The authors propose a method for iteratively compressing action trajectories into nested behavioral hierarchies, where the sub-goal structure can be decomposed into a compact code of action paths. The approach is based on transfer from one task to another, and the learning is performed using action primitives. The proposed approach is shown to improve the performance of learning in a non-trivial hierarchical structure."
21321,SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"generative models USED-FOR complex data. Autoencoders HYPONYM-OF generative models. images HYPONYM-OF complex data. variational autoencoder ( VAE ) HYPONYM-OF models. unimodal Gaussian decoders USED-FOR models. Hierarchical Bayes Autoencoder ( HBAE ) HYPONYM-OF probabilistic generative model. energybased model ( EBM ) USED-FOR multimodal decoder. multimodal decoder PART-OF HBAE. variational inference USED-FOR VAE. variational inference USED-FOR HBAE. conditional generator USED-FOR EBM distribution. adversarial approximation USED-FOR decoder. conditional generator USED-FOR stochastic reconstruction. code USED-FOR stochastic reconstruction. sampling steps PART-OF HBAE. HBAE USED-FOR sets. latent code USED-FOR HBAE. decoder USED-FOR realistic unconditional samples. single image and set cases EVALUATE-FOR decoder. model USED-FOR complex image sets. Set - HBAE USED-FOR complex image sets. Set - HBAE HYPONYM-OF model. OtherScientificTerm are semantic variations, and latent codes. Method is unimodal Gaussian distribution. ","This paper proposes a probabilistic generative model, Hierarchical Bayes Autoencoder (HBAE), which is a variant of Autoencoders, which are generative models for complex data (e.g., images). The models are unimodal Gaussian decoders. The authors propose a multimodal decoder based on an energybased model (EBM) that is trained with a conditional generator to predict the EBM distribution. The decoder is trained using adversarial approximation to the decoder. The model is evaluated on both single image and set cases, and is shown to be able to generate realistic unconditional samples. ","This paper proposes a probabilistic generative model, Hierarchical Bayes Autoencoder (HBAE), which is a variant of Autoencoders, which are generative models for complex data (e.g., images). The models are based on unimodal Gaussian decoders. The authors propose a multimodal decoder based on an energybased model (EBM) and a variational inference for VAE. The EBM distribution is modeled by a conditional generator that is used for stochastic reconstruction of the code, and the decoder is trained using adversarial approximation. The decoder can be used to generate realistic unconditional samples in both single image and set cases. The model is evaluated on complex image sets, including a set of sets with semantic variations, and on sets with different latent codes. "
21330,SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"deep off - policy TD algorithms CONJUNCTION feature normalization techniques. feature normalization techniques CONJUNCTION deep off - policy TD algorithms. normalization USED-FOR target networks. normalization USED-FOR optimization stability. mixture of onand off - policy transitions USED-FOR normalization. batch normalization USED-FOR It. DDPG CONJUNCTION TD3. TD3 CONJUNCTION DDPG. cross - normalization USED-FOR TD3. cross - normalization USED-FOR DDPG. MuJoCo benchmark tasks EVALUATE-FOR cross - normalization. Method are reinforcement learning ( RL ) algorithms, normalization techniques, and off - policy learning. ",This paper studies the problem of normalization in reinforcement learning (RL) algorithms. It proposes a mixture of on-and off-policy transitions to improve the optimization stability. It also proposes a batch normalization for the normalization. The authors show that cross-normalization improves the performance of DDPG and TD3 on MuJoCo benchmark tasks. ,"This paper proposes a new normalization technique for reinforcement learning (RL) algorithms. It is based on batch normalization, which is a mixture of onand off-policy transitions. The idea is to normalize the target networks in order to improve the optimization stability. The authors show that cross-normalization improves the performance of DDPG and TD3 on MuJoCo benchmark tasks. "
21339,SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"bias CONJUNCTION confounding effects. confounding effects CONJUNCTION bias. spurious associations of confounding variables HYPONYM-OF challenges. residualization CONJUNCTION stratification. stratification CONJUNCTION residualization. precomputed features USED-FOR confounding variables. techniques USED-FOR precomputed features. stratification HYPONYM-OF techniques. residualization HYPONYM-OF techniques. techniques USED-FOR statistical methods. techniques USED-FOR end - to - end deep learning methods. method USED-FOR discriminative features. adversarial training strategy USED-FOR discriminative features. adversarial training strategy USED-FOR method. synthetic data CONJUNCTION medical images. medical images CONJUNCTION synthetic data. method USED-FOR synthetic data. medical images EVALUATE-FOR method. Task are machine learning applications, and face recognition systems. Material is medical studies. Generic are datasets, and models. OtherScientificTerm are biases, confounder(s ), and bias or confounder variables. Method is adversarial loss function. ","This paper studies the problem of face recognition with spurious associations of confounding variables in the presence of bias and confounding effects in machine learning applications. The authors propose two techniques, stratification and residualization, to learn precomputed features for confounding variables, which can be used to train end-to-end deep learning methods. The proposed method is based on an adversarial training strategy to learn discriminative features from the data. The method is evaluated on synthetic data and medical images and shows that the proposed method outperforms state-of-the-art methods.","This paper proposes a novel adversarial loss function for face recognition systems. The authors propose two techniques to improve the performance of end-to-end deep learning methods: stratification and residualization. The proposed adversarial training strategy is based on adversarial learning of discriminative features and confounding variables, which are precomputed features of the training data and confounding effects of biases. The method is evaluated on synthetic data and medical images, and is shown to outperform other statistical methods. "
21348,SP:783049ff463edd1283c058c6106a3e1f9a033df4,Transformer USED-FOR Character - level language modeling. limited resources USED-FOR character - level language models. computational resources USED-FOR Transformer - based models. lightweight model USED-FOR calculation paths. GroupTransformer HYPONYM-OF lightweight model. grouped embedding operators USED-FOR lightweight model. grouped embedding operators USED-FOR calculation paths. inter - group linear operators USED-FOR Group - Transformer. enwik8 CONJUNCTION text8. text8 CONJUNCTION enwik8. LSTM - based models COMPARE Transformer - based models. Transformer - based models COMPARE LSTM - based models. benchmark tasks EVALUATE-FOR GroupTransformer. text8 EVALUATE-FOR GroupTransformer. enwik8 EVALUATE-FOR GroupTransformer. enwik8 HYPONYM-OF benchmark tasks. text8 HYPONYM-OF benchmark tasks. OtherScientificTerm is limitation of recursive operation. Method is group strategy. Task is qualitative analysis. ,"This paper proposes a lightweight model, GroupTransformer, for character-level language modeling with limited resources. The lightweight model uses grouped embedding operators to perform calculation paths using inter-group linear operators. The authors show that the proposed group strategy outperforms LSTM-based models on several benchmark tasks (e.g. enwik8 and text8). ","This paper proposes a lightweight model, GroupTransformer, for Character-level language modeling with limited resources. The main idea is to use inter-group linear operators for the calculation paths and grouped embedding operators for calculation paths. The authors also propose a group strategy that is based on the limitation of recursive operation. Experiments on several benchmark tasks, including enwik8 and text8, show that the proposed Transformer-based models outperform LSTM-based approaches. "
21357,SP:946c26d371297c88d0ac246257104099b4585edc,hierarchical - latent - variable structures FEATURE-OF Probabilistic models. approach USED-FOR models. Variational Autoencoders USED-FOR approach. Variational Autoencoders USED-FOR models. inference and optimisation schemes USED-FOR approaches. non - likelihood - based framework USED-FOR generative models. bespoke models CONJUNCTION inference networks. inference networks CONJUNCTION bespoke models. approach USED-FOR models. deep - latent hierarchies USED-FOR models. Optimal Transport USED-FOR approach. Optimal Transport USED-FOR models. it COMPARE Wasserstein Autoencoder. Wasserstein Autoencoder COMPARE it. method USED-FOR generative model. deep - latent hierarchy USED-FOR generative model. Maximum Mean Discrepancy divergence FEATURE-OF Wasserstein Autoencoder. ,"This paper proposes a non-likelihood-based framework for training generative models with hierarchical-latent-variable structures. The proposed approach is based on Variational Autoencoders, which can be used to train models with deep-latency hierarchies. Optimal Transport is used to optimize the models using Optimal Transformer. The authors show that the proposed method is able to generate a generative model with a deep- latent hierarchy, and that it can achieve better performance than the Wasserstein Autoencoder with Maximum Mean Discrepancy divergence. ","This paper proposes a non-likelihood-based framework for training generative models with hierarchical-latent-variable structures in Probabilistic models. The approach is based on Variational Autoencoders, which can be used to train models with different types of models (e.g., bespoke models, inference networks, etc.). The proposed approaches are evaluated on both inference and optimisation schemes. Optimal Transport is used for training the models, and it is shown to outperform the Wasserstein Autoencoder in terms of Maximum Mean Discrepancy divergence. The proposed method can be applied to any generative model with deep-latents hierarchies."
21366,SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"latent variable models CONJUNCTION adversarial training. adversarial training CONJUNCTION latent variable models. video - specific neural network architectures CONJUNCTION latent variable models. latent variable models CONJUNCTION video - specific neural network architectures. adversarial training CONJUNCTION methods. methods CONJUNCTION adversarial training. methods PART-OF video generation models. they USED-FOR continuations. realism FEATURE-OF continuations. benchmark datasets EVALUATE-FOR autoregressive video generation models. three - dimensional self - attention mechanism USED-FOR autoregressive video generation models. camera movement CONJUNCTION complex object interactions. complex object interactions CONJUNCTION camera movement. complex object interactions CONJUNCTION human movement. human movement CONJUNCTION complex object interactions. Kinetics HYPONYM-OF large scale action recognition dataset. Kinetics HYPONYM-OF phenomena. YouTube videos FEATURE-OF large scale action recognition dataset. Kinetics USED-FOR models. human movement HYPONYM-OF phenomena. camera movement HYPONYM-OF phenomena. complex object interactions HYPONYM-OF phenomena. Metric are statistical complexity, complexity, and fidelity. OtherScientificTerm are inherent stochasticity, and narrow domains. Task is generating natural video. Material is natural video. Generic is approaches. ","This paper studies the problem of generating natural video from video. The authors propose a three-dimensional self-attention mechanism to train autoregressive video generation models with three different methods: video-specific neural network architectures, latent variable models, and adversarial training. They show that they can achieve state-of-the-art results on a variety of benchmark datasets. They also show that their methods are able to generate continuations with high realism in terms of their inherent stochasticity. ","This paper studies the problem of generating natural video with inherent stochasticity. The authors propose two approaches to tackle this problem: (1) using video-specific neural network architectures and (2) adversarial training. Both methods are used in video generation models, and they are able to generate continuations with high realism in terms of their statistical complexity, complexity, and fidelity. The main contribution of the paper is to propose a three-dimensional self-attention mechanism for autoregressive video generation model, which can be applied to both wide and narrow domains. The paper also presents a large scale action recognition dataset with Kinetics and human movement as well as other phenomena such as camera movement, complex object interactions, etc."
21375,SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"International Classification of Diseases ( ICD ) HYPONYM-OF classification codes. noisy clinical document inputs CONJUNCTION long - tailed label distribution. long - tailed label distribution CONJUNCTION noisy clinical document inputs. Automatic ICD coding HYPONYM-OF multi - label text classification task. frequent and zeroshot codes USED-FOR fine - grained classification. long - tailed label distribution FEATURE-OF multi - label text classification task. noisy clinical document inputs FEATURE-OF multi - label text classification task. latent feature generation framework USED-FOR generalized zero - shot ICD coding. codes USED-FOR prediction. ICD code hierarchical structure CONJUNCTION cycle architecture. cycle architecture CONJUNCTION ICD code hierarchical structure. cycle architecture USED-FOR keywords. framework USED-FOR semantically meaningful features. semantically meaningful features USED-FOR zero - shot codes. cycle architecture USED-FOR framework. ICD code hierarchical structure USED-FOR framework. adversarial generative model USED-FOR generalized zero - shot learning. generalized zero - shot learning USED-FOR multi - label text classification. public MIMIC - III dataset EVALUATE-FOR methods. methods USED-FOR zero - shot codes. AUC score EVALUATE-FOR methods. F1 score EVALUATE-FOR methods. OtherScientificTerm are labeled data, and seen codes. Generic is approach. ","This paper proposes a new method for zero-shot classification of ICD codes. The proposed method is based on a latent feature generation framework, which can be used to learn a generalized zero-Shot ICD coding. The key idea is to use the ICD code hierarchical structure and a cycle architecture to learn the keywords for each code. The authors show that the proposed method outperforms existing methods on the public MIMIC-III dataset. ",This paper proposes a framework for learning zero-shot codes from labeled data. The framework is based on the International Classification of Diseases (ICD) framework. The authors propose a latent feature generation framework for generalized zero-Shot ICD coding with noisy clinical document inputs and long-tailed label distribution. The key idea is to use frequent and zeroshot codes for fine-grained classification. The proposed approach is evaluated on the public MIMIC-III dataset and shows that the proposed methods outperform existing methods on the AUC score. 
21384,SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,self - supervised representation learning USED-FOR reinforcement learning ( RL ). self - supervised representation learning USED-FOR sample efficiency. sample efficiency EVALUATE-FOR reinforcement learning ( RL ). forward prediction objective USED-FOR embeddings of states and action sequences. embeddings USED-FOR policy learning. embeddings USED-FOR structure of the environment ’s dynamics. action embeddings USED-FOR model - free RL. sample efficiency EVALUATE-FOR model - free RL. low - dimensional states USED-FOR model - free RL. sample efficiency EVALUATE-FOR action embeddings. state and action embeddings USED-FOR learning of high - quality policies. goal - conditioned continuous control USED-FOR learning of high - quality policies. pixel observations USED-FOR learning of high - quality policies. ,"This paper studies the sample efficiency of self-supervised representation learning in reinforcement learning (RL). The authors propose a forward prediction objective for embeddings of states and action sequences, which can be used for policy learning. The authors show that the embedding of the structure of the environment’s dynamics can improve sample efficiency for model-free RL with low-dimensional states. The paper also shows that the action embedding can be useful for the learning of high-quality policies with pixel observations. The goal-conditioned continuous control can also be used to improve the performance of the learned policies.","This paper proposes a self-supervised representation learning for reinforcement learning (RL) to improve sample efficiency. The authors propose a forward prediction objective for the embeddings of states and action sequences, which can be used for policy learning. The embedding of the environment’s dynamics is used to learn the structure of an environment. The proposed model-free RL is based on low-dimensional states, and the action embedding is used for learning of high-quality policies based on pixel observations. The sample efficiency of the proposed approach is shown to be better than that of the baseline. The paper also shows that goal-conditioned continuous control can improve the sample efficiency in model-based RL."
21393,SP:11ce1616e721340eea9e80dad7460c77355ac7d1,meta - learning USED-FOR tasks. hand - crafted structure design USED-FOR task - specific meta - learning methods. knowledge bases USED-FOR knowledge organization. structure knowledge USED-FOR meta - learner. framework USED-FOR task heterogeneity. model interpretability EVALUATE-FOR framework. meta - knowledge graph USED-FOR framework. meta - knowledge graph USED-FOR task heterogeneity. 2D toy regression CONJUNCTION few - shot image classification. few - shot image classification CONJUNCTION 2D toy regression. ARML COMPARE baselines. baselines COMPARE ARML. Generic is ones. Method is globally shared meta - learning methods. OtherScientificTerm is cross - task relations. ,"This paper proposes a framework for meta-learning for tasks with cross-task relations. The proposed framework is based on a meta-knowledge graph, which is a combination of knowledge bases for knowledge organization and meta-learner. The authors show that the proposed framework improves the model interpretability and reduces the task heterogeneity. The experimental results show that ARML performs better than other baselines. ","This paper proposes a meta-learning framework for tasks where the goal is to improve the model interpretability. The framework is based on the meta-knowledge graph. The authors propose a hand-crafted structure design for task-specific meta-Learning methods. The key idea is to learn knowledge bases for knowledge organization, and then use these knowledge bases to train a meta - learner. The proposed framework is evaluated on two tasks: 2D toy regression and few-shot image classification. The experimental results show that the proposed framework improves the task heterogeneity in terms of the meta -knowledge graph, and the cross-task relations. The results also show that ARML outperforms other baselines. "
21402,SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"model architecture CONJUNCTION fine - tuning. fine - tuning CONJUNCTION model architecture. attribute - specific data USED-FOR fine - tuning. Plug and Play Language Model ( PPLM ) USED-FOR controllable language generation. pretrained LM CONJUNCTION attribute classifiers. attribute classifiers CONJUNCTION pretrained LM. attribute classifiers USED-FOR text generation. pretrained LM PART-OF Plug and Play Language Model ( PPLM ). attribute models HYPONYM-OF classifiers. attribute models COMPARE LM. LM COMPARE attribute models. attribute alignment CONJUNCTION fluency. fluency CONJUNCTION attribute alignment. automated and human annotated evaluations EVALUATE-FOR attribute alignment. automated and human annotated evaluations EVALUATE-FOR fluency. differentiable attribute models USED-FOR text generation. Material are huge text corpora, and Model samples. Method are retraining, attribute model, and PPLMs. Task are Sampling, and generation. OtherScientificTerm are gradients, and hidden activations. ","This paper proposes a Plug and Play Language Model (PPLM) for controllable language generation. The model architecture and fine-tuning are based on attribute-specific data. The PPLM uses a pretrained LM and attribute classifiers to perform text generation, where the gradients are learned by retraining the attribute model. The authors show that the PPLMs perform better than other attribute models on both automated and human annotated evaluations for attribute alignment and fluency. ","This paper proposes a Plug and Play Language Model (PPLM) for controllable language generation from huge text corpora. The model architecture and fine-tuning are based on attribute-specific data. The authors propose a pretrained LM and attribute classifiers for text generation, which is a combination of two classifiers, namely, attribute models and classifiers trained on differentiable attribute models. Sampling, generation, and retraining of the attribute model is done using the PPLMs, and the gradients of the classifiers are computed using hidden activations. Experiments on both automated and human annotated evaluations show that the attribute models outperform the LM in terms of attribute alignment and fluency."
21411,SP:12d0980bfea2de880905a0b87b40856969bb1c58,"deep neural networks USED-FOR machine learning tasks. unlabeled data USED-FOR learning robust representations. unsupervised and self - supervised learning approaches USED-FOR visual data. domain knowledge USED-FOR unsupervised and self - supervised learning approaches. gradient domain FEATURE-OF clean data. clean data USED-FOR noisy input data. denoising autoencoder USED-FOR data representations. visual benchmarks EVALUATE-FOR representations. approach USED-FOR representations. representations USED-FOR vision tasks. Material is supervised data. Method is unsupervised learning framework. Generic is agent. OtherScientificTerm are data structures, and single - scale corruption. ","This paper proposes a new unsupervised learning framework for learning robust representations from unlabeled data. The agent is trained with domain knowledge, and the goal is to learn representations that are robust to noisy input data in the gradient domain. The proposed approach is based on denoising autoencoder to learn data representations that can be used to train representations for vision tasks on visual benchmarks. The authors show that the proposed approach can achieve state-of-the-art performance on a variety of vision tasks.","This paper proposes a new unsupervised learning framework for learning robust representations from unlabeled data. The authors propose to use domain knowledge to train deep neural networks for machine learning tasks. The key idea is to train an agent to predict the data structures of unlabelled data, and then learn robust representations based on the learned representations. The proposed approach is evaluated on a variety of vision tasks and on visual benchmarks. The paper shows that the clean data in the gradient domain is more robust to noisy input data than clean data from the supervised data, which is a result of single-scale corruption. "
21420,SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"Neural networks USED-FOR Natural Language Processing. under - sensitivity FEATURE-OF natural language inference. technique USED-FOR formal verification of this specification. technique USED-FOR models. interval bound propagation ( IBP ) approach USED-FOR technique. training methods USED-FOR under - sensitivity. SNLI and MNLI datasets EVALUATE-FOR IBP training. verified accuracy EVALUATE-FOR IBP training. Generic are they, specification, method, model, and metrics. Method are decomposable attention mechanism, and training. OtherScientificTerm is under - sensitivity problem. Material is SNLI test set. ","This paper studies the under-sensitivity problem in natural language inference with neural networks. The authors propose a technique for formal verification of this specification by using interval bound propagation (IBP) approach. The proposed method is based on a decomposable attention mechanism, and the authors show that they can improve the performance of existing training methods in terms of under-sensitive performance. They also show that IBP training can achieve better verified accuracy on SNLI and MNLI datasets. ","This paper proposes a novel approach to improve the robustness of neural networks for Natural Language Processing. The authors propose a decomposable attention mechanism, which can be applied to the under-sensitivity problem in natural language inference. The proposed technique is based on the interval bound propagation (IBP) approach, which is a technique for formal verification of this specification. The method is evaluated on SNLI and MNLI datasets, where they show that the proposed method can improve the performance of the model. They also show that training methods can be used to improve under-sensitive training methods. "
21429,SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"replay memory USED-FOR network updates. soft divergence FEATURE-OF structure. data graph USED-FOR transitions. Q - values FEATURE-OF Markov Decision Process ( MDP ). favorable structure FEATURE-OF subgraph. transition PART-OF MDP. transition PART-OF continuous Q - learning problem. Q - value FEATURE-OF transition. Q - value USED-FOR continuous Q - learning problem. Q - value FEATURE-OF transition. lower bounds USED-FOR method. lower bounds USED-FOR TD learning. TD learning USED-FOR method. soft divergence FEATURE-OF method. sample efficiency EVALUATE-FOR method. replay memory capacity FEATURE-OF algorithm. OtherScientificTerm are state and action spaces, QGRAPH, hyperparameters, and QGRAPHs. ","This paper studies the problem of learning the Q-values of a Markov Decision Process (MDP) from data graph. The transition in the MDP is a continuous Q-learning problem with Q-value, and the authors propose a new structure based on soft divergence between the state and action spaces of a subgraph with favorable structure. The QGRAPH is then used to learn the hyperparameters. The authors provide lower bounds on the sample efficiency of the proposed method based on TD learning. The proposed algorithm has a replay memory capacity of $O(\sqrt{T})$ and can be used for network updates. ","This paper proposes a new method for learning a Markov Decision Process (MDP) with Q-values. The proposed method is based on the Q-value of the transition in the continuous Q-learning problem. The transition consists of two subgraphs, one for state and action spaces, where the QGRAPH is a set of hyperparameters, and the other is a data graph for transitions. The structure of the subgraph is defined by soft divergence between the state and the action spaces. The authors show that the proposed method can achieve better sample efficiency than TD learning with lower bounds on the soft divergence of the structure. They also show that their algorithm has better replay memory capacity for network updates."
21438,SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,approach USED-FOR problem. domain - invariant embeddings USED-FOR approach. domain - invariant embeddings USED-FOR problem. embedding complexity FEATURE-OF generalization. theoretical framework USED-FOR multilayer neural networks. strategy COMPARE layer - dependent complexity tradeoff. layer - dependent complexity tradeoff COMPARE strategy. Task is Unsupervised domain adaptation. Generic is complexity. ,This paper studies the problem of unsupervised domain adaptation in multilayer neural networks with domain-invariant embeddings. The authors propose an approach to solve the problem using domain-irreversible domain-agnostic embedding. The key idea is to use a theoretical framework to model the embedding complexity of the generalization of multilayers neural networks. The paper shows that the proposed strategy achieves better performance than the layer-dependent complexity tradeoff. ,This paper proposes a novel approach to solve the problem of unsupervised domain adaptation with domain-invariant embeddings. The authors propose a theoretical framework for multilayer neural networks. The main idea is to reduce the embedding complexity of generalization in order to improve the generalization. The proposed strategy is based on the layer-dependent complexity tradeoff. The paper is well-written and well-motivated. 
21447,SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"framework USED-FOR algorithm - dependent generalization error bounds. PAC - Bayesian theory CONJUNCTION algorithmic stability. algorithmic stability CONJUNCTION PAC - Bayesian theory. PAC - Bayesian theory USED-FOR framework. mini - batch and acceleration CONJUNCTION Entropy - SGD. Entropy - SGD CONJUNCTION mini - batch and acceleration. momentum CONJUNCTION mini - batch and acceleration. mini - batch and acceleration CONJUNCTION momentum. Bayes - Stability method USED-FOR data - dependent generalization bounds. data - dependent generalization bounds USED-FOR stochastic gradient Langevin dynamics ( SGLD ). momentum HYPONYM-OF noisy gradient methods. Entropy - SGD HYPONYM-OF noisy gradient methods. mini - batch and acceleration HYPONYM-OF noisy gradient methods. data - dependent bounds USED-FOR randomly labelled data. randomly labelled data COMPARE normal data. normal data COMPARE randomly labelled data. bounded loss CONJUNCTION ` 2 regularization term. ` 2 regularization term CONJUNCTION bounded loss. bounded loss PART-OF total loss. ` 2 regularization term PART-OF total loss. Log - Sobolev inequality USED-FOR parameter distribution. generalization bounds USED-FOR continuous Langevin dynamic. Log - Sobolev inequality USED-FOR generalization bounds. Metric is Generalization error. OtherScientificTerm are out - of - sample error, tight generalization error bounds, generalization error bounds, and noise level. Task is statistical learning theory. Method is Bayes - Stability. Generic is bounds. ","The paper proposes a new framework for computing algorithm-dependent generalization error bounds for stochastic gradient Langevin dynamics (SGLD). The framework is based on PAC-Bayesian theory and algorithmic stability. The Bayes-Stability method is used to derive the data-dependency generalization bounds for data-dependent SGLD. The authors show that the bounds are tight for continuous Langevin dynamic under the Log-Sobolev inequality of the parameter distribution. The bounds are also tight for noisy gradient methods (e.g., momentum, Entropy-SGD, mini-batch and acceleration). The authors also show that under the bounded loss and the `2 regularization term of the total loss, the bound is tight for the bounded losses. Finally, the authors provide some theoretical guarantees for the bounds.","This paper proposes a framework for algorithm-dependent generalization error bounds based on PAC-Bayesian theory and algorithmic stability. The main idea is to use the Bayes-Stability method to learn the data-dependant generalization bounds for stochastic gradient Langevin dynamics (SGLD) with momentum, mini-batch and acceleration. Generalization error is defined as the difference between the out-of-sample error and the out of-distribution error. The authors provide tight bounds on the generalisation error bounds. The bounds are based on the Log-Sobolev inequality between the parameter distribution and the noise level. They also provide data-dependent bounds for randomly labelled data and normal data. The bounded loss and the `2 regularization term are used for the total loss."
21456,SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"spatial memory CONJUNCTION goal - directed spatial navigation. goal - directed spatial navigation CONJUNCTION spatial memory. hippocampus USED-FOR goal - directed spatial navigation. hippocampus USED-FOR spatial memory. hippocampal CA1 neurons USED-FOR continual learning. populationlevel activity FEATURE-OF hippocampal CA1 neurons. populationlevel activity USED-FOR continual learning. continual learning USED-FOR spatial navigation strategies. navigational strategies CONJUNCTION reward location. reward location CONJUNCTION navigational strategies. hippocampal neurons USED-FOR task variables. decisions CONJUNCTION navigational strategies. navigational strategies CONJUNCTION decisions. firing activity USED-FOR dPCA. decisions HYPONYM-OF task variables. reward location HYPONYM-OF task variables. navigational strategies HYPONYM-OF task variables. dPCA USED-FOR components. hippocampal features COMPARE reinforcement learning algorithms. reinforcement learning algorithms COMPARE hippocampal features. deep reinforcement learning model COMPARE animal learning. animal learning COMPARE deep reinforcement learning model. hippocampus USED-FOR reinforced spatial continual learning. biological and machine learning USED-FOR spatial continual learning. Task are continual learning of navigational strategies, and allocentric and egocentric spatial tasks. Method is Demixed Principal Component Analysis ( dPCA ). OtherScientificTerm is task switching. ","This paper studies continual learning of navigational strategies in biological and machine learning. The authors propose Demixed Principal Component Analysis (dPCA), a method for continual learning with hippocampal CA1 neurons with populationlevel activity in the hippocampus. The main idea is to use continual learning to learn spatial memory and goal-directed spatial navigation using the hippocampus, and then use the hippocampus to train the spatial navigation strategies in continual learning.  The authors show that dPCA is able to learn the components of the hippocampal neurons, including the decision-making process and the reward location of the task variables (e.g., decisions, navigation, etc.). The authors also show that the firing activity of dPCAs can be used to improve the performance of a deep reinforcement learning model compared to animal learning, and that hippocampal features are more robust to task switching than other reinforcement learning algorithms.","This paper proposes a new method for continual learning of navigational strategies. The authors propose to use the Demixed Principal Component Analysis (dPCA) framework, which is based on the idea of continual learning with hippocampal CA1 neurons with populationlevel activity. The key idea of dPCA is to learn the components of the hippocampal neurons for each of the task variables (e.g., decisions, navigation, reward location, etc.). The authors show that the hippocampus is able to learn spatial memory and goal-directed spatial navigation, and that continual learning can learn spatial navigation strategies that can be used for both allocentric and egocentric spatial tasks. Experiments are conducted on biological and machine learning to demonstrate the effectiveness of the proposed deep reinforcement learning model compared to other reinforcement learning algorithms. "
21465,SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"Monte Carlo Tree Search ( MCTS ) USED-FOR discrete environments. it USED-FOR continuous domains. Go HYPONYM-OF discrete environments. tree search based policy optimization method USED-FOR continuous environments. TPO HYPONYM-OF tree search based policy optimization method. hybrid approach USED-FOR policy optimization. hybrid approach USED-FOR TPO. continuous action space FEATURE-OF MCTS tree. off - policy MCTS trajectories USED-FOR policy gradient. pre - trained policy USED-FOR bootstrapping tree search. branching factor CONJUNCTION simulation count. simulation count CONJUNCTION branching factor. policy bootstrapping USED-FOR continuous MCTS. branching factor USED-FOR continuous MCTS. simulation count USED-FOR continuous MCTS. PPO USED-FOR baseline policy optimization algorithm. TPO USED-FOR policy. Humanoid HYPONYM-OF complex environments. Method are limiting tree search branching factor, and MCTS training. OtherScientificTerm are tree search branching factor, policy distribution, and loss function. Generic are approach, and baseline algorithm. Metric is MCTS branching factor. ","This paper proposes Monte Carlo Tree Search (MCTS), a tree search based policy optimization method for continuous environments (e.g., Go) where the tree search branching factor is bounded. The main idea of the approach is to use a pre-trained policy as a bootstrapping tree search, where the goal is to find a policy that minimizes the MCTS branching factor in a continuous action space. The authors propose a hybrid approach to the TPO, which uses a TPO to perform policy optimization in the continuous space.  The main contribution of the paper is to show that the policy gradient can be computed using off-policy mCTS trajectories, and that it can be applied to continuous domains such as Go. The paper also shows that the branching factor can be used to bootstrap the policy bootstrapped in the case where the policy distribution is unknown. ","This paper proposes Monte Carlo Tree Search (MCTS), a tree search based policy optimization method for continuous environments such as Go, where it can be applied to discrete domains such as Humanoid. TPO is a hybrid approach to policy optimization, where the tree search branching factor is a function of the MCTS tree in the continuous action space, and the policy gradient is computed using off-policy mCTS trajectories. The main idea of the approach is to use a pre-trained policy for bootstrapping tree search, and then use a tree-searching branching factor for the policy distribution. The branching factor and the simulation count are used to optimize the policy. The proposed baseline algorithm, PPO, is based on the PPO for the baseline policy optimization algorithm. The authors show that the proposed TPO can be used to learn a policy that is more efficient than the baseline algorithm. "
21474,SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,accuracy EVALUATE-FOR network. sparse subnetworks PART-OF neural networks. supervision USED-FOR generating process. winning tickets COMPARE winning tickets. winning tickets COMPARE winning tickets. ImageNet dataset EVALUATE-FOR winning tickets. ImageNet classification task EVALUATE-FOR winning tickets. ImageNet classification task EVALUATE-FOR winning tickets. Material is lottery ticket hypothesis. Method is neural network optimization. OtherScientificTerm is initializations. ,"This paper studies the lottery ticket hypothesis in the context of neural network optimization. The authors consider sparse subnetworks in neural networks, where the number of neurons in the network is small and the accuracy of the network depends on the size of the initializations. They propose to use supervision to guide the generating process. They show that winning tickets on the ImageNet dataset are better than winning tickets in the standard ImageNet classification task.",This paper proposes a lottery ticket hypothesis for neural network optimization. The paper proposes to use sparse subnetworks in neural networks to improve the accuracy of the network. The key idea is to use supervision to guide the generating process. The authors show that winning tickets on the ImageNet dataset outperform winning tickets in a simple ImageNet classification task. 
21483,SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"excessive prediction undersensitivity HYPONYM-OF complementary problem. spurious surface patterns USED-FOR models. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. adversarial training USED-FOR defence strategies. data augmentation USED-FOR defence strategies. undersensitivity attacks FEATURE-OF model. held out evaluation data EVALUATE-FOR undersensitivity attacks. held out evaluation data EVALUATE-FOR model. they COMPARE model. model COMPARE they. train / evaluation distribution mismatch FEATURE-OF biased data setting. biased data setting EVALUATE-FOR model. predictive cues USED-FOR they. biased data setting EVALUATE-FOR adversarially robust models. F1 EVALUATE-FOR they. F1 EVALUATE-FOR model. Method are Neural reading comprehension models, noisy adversarial attack, and NewsQA models. OtherScientificTerm are adversarially selected input, semantically invariant text perturbations, and semantic variations of comprehension questions. Generic is attack. Material is adversarially generated questions. ","This paper studies the problem of excessive prediction undersensitivity in Neural reading comprehension models. The authors consider the complementary problem of learning models with spurious surface patterns, where the adversarially selected input is masked by semantically invariant text perturbations. They propose two defence strategies: data augmentation and adversarial training. They show that the proposed model is robust to undersensitivity attacks on held out evaluation data, and they also show that they are robust in the biased data setting with a train/evaluation distribution mismatch. Finally, they show that their model can outperform the state-of-the-art NewsQA models on F1.","This paper studies the problem of excessive prediction undersensitivity, which is a complementary problem to Neural reading comprehension models. The authors show that models with spurious surface patterns are vulnerable to adversarially selected input and adversarial training. They also show that the noisy adversarial attack can be applied to NewsQA models. Finally, they show that a model trained on held out evaluation data can be robust to undersensitivity attacks in a biased data setting with a train/evaluation distribution mismatch. They show that they can be trained on predictive cues and that adversarial generated questions can be used to attack the model."
21492,SP:5da870060778de460c1abe91562d6f3e707efef4,"reinforcement learning ( RL ) agents USED-FOR real - world tasks. reinforcement learning ( RL ) agents USED-FOR safety. approaches USED-FOR problem. safety penalty PART-OF reward function. complex domains EVALUATE-FOR approaches. model - based approach USED-FOR safety. imaginative module HYPONYM-OF directed graph. it CONJUNCTION RL algorithm. RL algorithm CONJUNCTION it. gridworld environments CONJUNCTION self - driving car simulator. self - driving car simulator CONJUNCTION gridworld environments. approach COMPARE baseline. baseline COMPARE approach. self - driving car simulator EVALUATE-FOR approach. gridworld environments EVALUATE-FOR approach. self - driving car simulator EVALUATE-FOR proposal. gridworld environments EVALUATE-FOR proposal. OtherScientificTerm are bad incentives, unsafe scenarios, transition dynamics of the environment, baseline state, and discrete action space. Generic are they, graph, method, and task. ","This paper studies the problem of safety of reinforcement learning (RL) agents in real-world tasks. The authors propose a model-based approach to improve the safety of RL agents in the presence of bad incentives. The proposed approach is based on the idea of a directed graph, called an imaginative module, where the goal is to minimize the safety penalty in the reward function. The paper shows that the proposed approach performs better than a baseline in a self-driving car simulator and gridworld environments.","This paper proposes a model-based approach to improve the safety of reinforcement learning (RL) agents for real-world tasks. The authors propose a directed graph with an imaginative module, where the goal is to avoid bad incentives in unsafe scenarios. The problem is formulated as a problem where the agent is given a set of tasks, and they have access to a graph, where they are given a reward function with a safety penalty. The paper proposes two approaches to solve this problem. The first approach is a simple one, and it can be combined with a standard RL algorithm. The second approach is an extension of the baseline state, which is a discrete action space. The proposed method is evaluated on a self-driving car simulator and gridworld environments."
21501,SP:c2796f28fb067138303df8d424d646f4ada31558,"numerical error FEATURE-OF finite differences. deep learning models USED-FOR physics - governing observations. unstructured grid USED-FOR physics - governing observations. neighboring information USED-FOR finite differences. physics equations USED-FOR finite differences. PA - DGN USED-FOR dynamical relations. synthetic data CONJUNCTION real - world climate observations. real - world climate observations CONJUNCTION synthetic data. PA - DGN USED-FOR approximation of directional derivatives. PA - DGN USED-FOR prediction of graph signals. approximation of directional derivatives CONJUNCTION prediction of graph signals. prediction of graph signals CONJUNCTION approximation of directional derivatives. real - world climate observations USED-FOR prediction of graph signals. synthetic data USED-FOR prediction of graph signals. weather stations USED-FOR real - world climate observations. Task is dynamics of physical systems. OtherScientificTerm are discretization error, spatial and temporal differences, and sequential observations. Material is sparse data. Generic is architecture. ","This paper studies the dynamics of physical systems. The authors propose a deep learning models for physics-regulating observations on an unstructured grid. The main idea is to learn the discretization error of the finite differences between two points in the space of neighboring information, and then use the neighboring information to compute finite differences based on physics equations. The architecture is based on PA-DGN, which is able to learn dynamical relations between points in space. The paper shows that the approximation of directional derivatives and the prediction of graph signals using synthetic data and real-world climate observations from weather stations can be used to improve the performance of the architecture.","This paper studies the dynamics of physical systems in the context of deep learning models for physics-gating observations on an unstructured grid. The authors show that the discretization error of finite differences in the space of neighboring information is proportional to the dimensionality of the finite differences, and that the physics equations for finite differences can be approximated by physics equations. The paper also shows that the approximation of directional derivatives and the prediction of graph signals based on synthetic data and real-world climate observations from weather stations can be computed using PA-DGN for dynamical relations. The proposed architecture is evaluated on sparse data and on sequential observations."
21510,SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"nonsmooth regularization CONJUNCTION constraints. constraints CONJUNCTION nonsmooth regularization. nonsmooth regularization FEATURE-OF structured neural networks ( NN ). constraints FEATURE-OF structured neural networks ( NN ). interval constraints HYPONYM-OF constraints. ` 1 - norm HYPONYM-OF nonsmooth regularization. constrained nonsmooth nonconvex optimization problem USED-FOR training. ProxSGD USED-FOR sparse or binary neural networks. regularization function CONJUNCTION constraint set. constraint set CONJUNCTION regularization function. regularization function USED-FOR ProxSGD. constraint set USED-FOR ProxSGD. OtherScientificTerm are learning rates, and stationary point. Method is ProxSGD algorithm. ","This paper studies the problem of nonsmooth regularization and constraints in structured neural networks (NN). The authors propose a new regularization function and a constraint set for ProxSGD for sparse or binary neural networks. The constraints are based on interval constraints, where the learning rates are bounded by a stationary point. The authors show that the constraints are equivalent to the `1-norm' of nonsnooth regularisation, which is a well-studied problem in the literature. The paper also shows that the constraint set can be used to improve the performance of ProxSVGD. ","This paper studies the problem of nonsmooth regularization of structured neural networks (NN) with constraints, i.e., interval constraints and constraints on the learning rates. The authors propose a novel method to solve the training problem of training with constrained nonssooth nonconvex optimization problem. The main idea of the ProxSGD algorithm is to learn a stationary point with a regularization function and a constraint set, which is then used to train sparse or binary neural networks. "
21519,SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"encoder USED-FOR compression algorithms. approximate methods USED-FOR encoder. approximate methods USED-FOR algorithms. framework USED-FOR lossy image compression. non - deterministic compression codec USED-FOR framework. expected code length CONJUNCTION relative entropy. relative entropy CONJUNCTION expected code length. gradient - based optimizers USED-FOR end - to - end differentiable compression framework. it USED-FOR lossy image compression. rate - distortion curves COMPARE state - of - the - art. state - of - the - art COMPARE rate - distortion curves. low bitrates FEATURE-OF rate - distortion curves. it USED-FOR method. Kodak dataset EVALUATE-FOR rate - distortion curves. Probabilistic Ladder Networks ( PLNs ) USED-FOR it. low bitrates EVALUATE-FOR state - of - the - art. CLIC 2018 dataset EVALUATE-FOR Probabilistic Ladder Networks ( PLNs ). Material is image. OtherScientificTerm are discrete code, quantization step, continuous space, and encoding distribution. Method is decoder. Generic is process. ",This paper proposes a framework for lossy image compression based on a non-deterministic compression codec. The encoder is trained using approximate methods to approximate compression algorithms. The proposed framework is based on gradient-based optimizers. The authors show that it can achieve state-of-the-art rate-distortion curves with low bitrates on the Kodak dataset and Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset.,"This paper proposes an end-to-end differentiable compression framework with gradient-based optimizers. The framework is based on a non-deterministic compression codec, where the encoder is trained with approximate methods for compression algorithms, and the decoder is learned with discrete code. The authors show that it can be used for lossy image compression with low bitrates compared to state-of-the-art rate-distortion curves on the Kodak dataset and Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset. The key idea is to use a quantization step, which is a continuous space where the encoding distribution is a function of the expected code length and the relative entropy. This process is computationally expensive, so the authors propose a novel way to accelerate the process."
21528,SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"compressed JPG ( C - JPG ) image USED-FOR SR. SR HYPONYM-OF image processing operations. components CONJUNCTION cycle loss. cycle loss CONJUNCTION components. components PART-OF SR structure. cycle loss PART-OF SR structure. high - qualified SR images USED-FOR prevalent C - JPG images. hybrid loss function USED-FOR SR generation. cycle loss PART-OF SR solver. cycle loss USED-FOR hybrid loss function. SR solver USED-FOR hybrid loss function. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Task are Super Resolution ( SR ), and SR issue. Method are SR models, C - JPG, functional sub - model, and SR approaches. OtherScientificTerm is storage space. Material is C - JPG images. ",This paper studies the Super Resolution (SR) problem in compressed JPG (C-JPG) image processing operations. The SR structure consists of two components: a functional sub-model and a cycle loss. The authors propose a hybrid loss function for SR generation using the SR solver. The proposed approach is shown to outperform state-of-the-art methods in the SR issue. ,"This paper proposes Super Resolution (SR), a method to generate a compressed JPG (C-JPG) image from an SR. SR is a family of image processing operations, where the SR structure consists of two components: the components of the SR model and the cycle loss. The authors propose a hybrid loss function for SR generation that combines the SR solver with a cycle loss in the storage space. The proposed approach is compared to state-of-the-art methods, and is shown to outperform the SR approaches in terms of performance. The main contribution of the paper is to propose a functional sub-model to solve the SR issue. The key idea is to use high-qualified SR images to generate prevalent C-jPG images from high-quality high-quantified SR images. "
21537,SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"fully convolutional network architecture USED-FOR surface of pass probabilities. professional soccer matches USED-FOR single - location labels. single - location labels USED-FOR fully convolutional network architecture. single - location labels USED-FOR surface of pass probabilities. feature hierarchy USED-FOR network. low - level inputs USED-FOR network. approach USED-FOR weakly supervised learning. approach USED-FOR spatiotemporal decision - making analysis. spatiotemporal decision - making analysis HYPONYM-OF sports. network USED-FOR pass - selection likelihood. deep learning architecture USED-FOR sports analytics. OtherScientificTerm are sampling levels, coarse and fine detail, single pixel correspondence, and predicted probability map. ",This paper proposes a fully convolutional network architecture for the surface of pass probabilities based on single-location labels from professional soccer matches. The network is trained with low-level inputs and a feature hierarchy. The sampling levels of the network are determined by coarse and fine detail. The proposed approach is applied to weakly supervised learning and is shown to improve the performance of spatiotemporal decision-making analysis in a variety of sports. ,"This paper proposes a fully convolutional network architecture for learning the surface of pass probabilities from single-location labels from professional soccer matches. The network is based on a feature hierarchy, where the sampling levels are computed in terms of coarse and fine detail, and the network is trained with low-level inputs. The proposed approach is applied to weakly supervised learning, and is shown to improve the performance of spatiotemporal decision-making analysis in a variety of sports (e.g., spatiotenet). The proposed network learns the pass-selection likelihood from the predicted probability map, which is then used to train a deep learning architecture for sports analytics."
21546,SP:1ae31baf383fc520687b255d9cac14c3b040e253,"side information USED-FOR inductive matrix completion model. user ’s age CONJUNCTION movie ’s genre. movie ’s genre CONJUNCTION user ’s age. movie ’s genre HYPONYM-OF content ( side information ). user ’s age HYPONYM-OF content ( side information ). IGMC USED-FOR graph neural network ( GNN ). It COMPARE transductive baselines. transductive baselines COMPARE It. model USED-FOR Douban movie ratings. MovieLens dataset USED-FOR model. Long - range dependencies USED-FOR modeling recommender systems. side information USED-FOR inductive matrix completion models. OtherScientificTerm are ( rating ) matrix, low - dimensional latent embeddings, embeddings, rating matrix, subgraphs, and local graph patterns. Method are matrix completion methods, and transductive methods. Task is matrix completion. Generic is it. ","This paper proposes a new inductive matrix completion model based on the side information of the content (side information) matrix, i.e., the user’s age, movie‘s genre, and the movie ’s genre. The authors use IGMC to train a graph neural network (GNN) with low-dimensional latent embeddings. The model is trained on the MovieLens dataset, and is evaluated on Douban movie ratings. It outperforms other matrix completion methods on the Douban dataset. It also performs better than transductive baselines. Long-range dependencies are important for modeling recommender systems, and this paper proposes to use this type of side information to improve the performance of inductive matrices completion models. ","This paper proposes a new inductive matrix completion model that uses side information (side information) from the user’s age and movie ’s genre to train a graph neural network (GNN). It is shown to outperform other matrix completion methods. It also outperforms other transductive baselines on the MovieLens dataset for Douban movie ratings. The authors show that the (rating) matrix can be decomposed into low-dimensional latent embeddings and high-dimensional subgraphs, which are then used to train the GNN. Long-range dependencies are used in modeling recommender systems, and it is shown that it is able to learn local graph patterns. "
21555,SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,smooth objective function USED-FOR unconstrained minimization. heavy ball momentum USED-FOR stochastic zeroth - order method. learning to continuous control tasks EVALUATE-FOR method. STP COMPARE policy gradient methods. policy gradient methods COMPARE STP. STP COMPARE derivative - free optimization algorithms. derivative - free optimization algorithms COMPARE STP. derivative - free optimization algorithms CONJUNCTION policy gradient methods. policy gradient methods CONJUNCTION derivative - free optimization algorithms. SMTP COMPARE STP. STP COMPARE SMTP. SMTP COMPARE methods. methods COMPARE SMTP. STP CONJUNCTION methods. methods CONJUNCTION STP. importance sampling USED-FOR SMTP. OtherScientificTerm is function evaluations. Metric is complexity. Method is SMTP_IS. ,This paper proposes a stochastic zeroth-order method based on heavy ball momentum. The main idea is to learn a smooth objective function for unconstrained minimization. The method is evaluated on a variety of learning to continuous control tasks and compared to STP and other derivative-free optimization algorithms as well as policy gradient methods. The complexity of SMTP_IS is shown to be much smaller than other methods.    The main contribution of the paper is to propose importance sampling to improve SMTP. ,This paper proposes a stochastic zeroth-order method with heavy ball momentum. The main idea is to use a smooth objective function for unconstrained minimization. The method is evaluated on learning to continuous control tasks and compared to STP and other derivative-free optimization algorithms. SMTP is shown to outperform other methods such as STP with importance sampling. The complexity of SMTP_IS is also discussed. 
21564,SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"environmental stochasticity CONJUNCTION uncertainties. uncertainties CONJUNCTION environmental stochasticity. deep learning architecture USED-FOR multiagent coordination. multiagent coordination mechanisms USED-FOR deep learning architecture. Action Semantics Network ( ASN ) HYPONYM-OF network architecture. action semantics USED-FOR neural networks. neural networks USED-FOR ASN. action semantics USED-FOR ASN. ASN CONJUNCTION deep reinforcement learning ( DRL ) algorithms. deep reinforcement learning ( DRL ) algorithms CONJUNCTION ASN. StarCraft II micromanagement CONJUNCTION Neural MMO. Neural MMO CONJUNCTION StarCraft II micromanagement. DRL approaches COMPARE network architectures. network architectures COMPARE DRL approaches. Neural MMO EVALUATE-FOR ASN. StarCraft II micromanagement EVALUATE-FOR ASN. ASN COMPARE DRL approaches. DRL approaches COMPARE ASN. ASN COMPARE network architectures. network architectures COMPARE ASN. Task are multiagent systems ( MASs ), and system evolution. Material is MASs. OtherScientificTerm is co - learning agents. ",This paper studies the problem of multiagent systems (MASs). The authors propose a new deep learning architecture for multiagent coordination mechanisms. The proposed Action Semantics Network (ASN) is a network architecture based on the action semantics learned by neural networks. The authors show that ASN outperforms the state-of-the-art deep reinforcement learning (DRL) algorithms and other network architectures in StarCraft II micromanagement and Neural MMO. The ASN also outperforms other DRL approaches.,This paper proposes a new deep learning architecture for multiagent coordination. The proposed Action Semantics Network (ASN) is an extension of the multiagent systems (MASs) framework. The key idea of ASN is to use multiagent communication mechanisms to improve the performance of the network architecture. The authors show that ASN outperforms the state-of-the-art deep reinforcement learning (DRL) algorithms and other network architectures on StarCraft II micromanagement and Neural MMO. The main contribution of the paper is that the authors propose to use the action semantics of neural networks to guide the training of the co-learning agents. The paper also shows that the proposed ASN achieves better performance than other DRL approaches.
21573,SP:efaf3a440dc17e05177832083ffbc23760ed7c97,Value - based methods USED-FOR planning and deep reinforcement learning ( RL ). Q function HYPONYM-OF state - action value function. system dynamics USED-FOR global structures of the Q function. lowrank structure USED-FOR big data matrices. low - rank Q functions USED-FOR control and deep RL tasks. Matrix Estimation ( ME ) techniques USED-FOR framework. low - rank structure FEATURE-OF Q functions. low - rank structure USED-FOR framework. planning procedure USED-FOR classical control. scheme USED-FOR value - based RL techniques. scheme USED-FOR “ low - rank ” tasks. control tasks CONJUNCTION Atari games. Atari games CONJUNCTION control tasks. control tasks EVALUATE-FOR approach. Atari games EVALUATE-FOR approach. Task is planning and deep RL. Generic is structures. ,"This paper proposes a new framework for planning and deep reinforcement learning (RL) based on value-based methods. The framework is based on Matrix Estimation (ME) techniques, where the Q function is a state-action value function, and the goal is to learn the global structures of the Q functions based on system dynamics. The proposed framework uses the low-rank structure of Q functions in big data matrices to learn control and deep RL tasks. The authors propose a planning procedure for classical control and a scheme for “low-rank” tasks. Experiments on control tasks and Atari games demonstrate the effectiveness of the proposed approach.","This paper proposes a new framework for value-based methods for planning and deep reinforcement learning (RL). The proposed framework is based on Matrix Estimation (ME) techniques. The Q function is a state-action value function that is defined as the sum of the global structures of the Q function and the system dynamics. The framework uses the low-rank structure of Q functions to represent big data matrices. The authors propose a planning procedure to learn classical control and deep RL. The proposed scheme is evaluated on several “low-rank” tasks, including control tasks and Atari games. "
21582,SP:430336893b247b7bd45687d78b0d0511a7369e87,"batch reinforcement learning USED-FOR sample - efficient learning. batch reinforcement learning USED-FOR Deep Reinforcement Learning ( DRL ). off - policy DRL algorithms USED-FOR batch DRL setting. action space FEATURE-OF maximizing Q functions. state - action pairs USED-FOR policy network. it USED-FOR policy network. state - action pairs USED-FOR it. imitation learning USED-FOR it. imitation learning USED-FOR policy network. Mujoco benchmark EVALUATE-FOR BAIL. Generic is algorithm. Method are Best - Action Imitation Learning ( BAIL ), and offpolicy DRL algorithms. OtherScientificTerm is Q functions. ","This paper studies batch reinforcement learning for sample-efficient learning in Deep Reinforcement Learning (DRL). The authors propose Best-Action Imitation Learning (BAIL), an off-policy DRL algorithms for the batch DRL setting where the goal is to maximize the Q functions in the action space. BAIL uses imitation learning to learn a policy network with state-action pairs, and it is shown that it can achieve better performance on the Mujoco benchmark. The algorithm is well-written and easy to follow.","This paper proposes batch reinforcement learning for sample-efficient learning in Deep Reinforcement Learning (DRL). The algorithm is based on Best-Action Imitation Learning (BAIL), which uses off-policy DRL algorithms in the batch DRL setting. The main idea of BAIL is to learn the maximizing Q functions in the action space in the form of state-action pairs, and then use it to train a policy network using imitation learning. Experiments on the Mujoco benchmark show that BAIL outperforms other offpolicy dRL algorithms. "
21591,SP:94078964876667e8a5d9ae7728d779d5b91a576e,"feature representations CONJUNCTION classifiers. classifiers CONJUNCTION feature representations. classifiers PART-OF deep extreme multi - label learning. feature representations PART-OF deep extreme multi - label learning. deep extreme classifiers USED-FOR short text documents. word embeddings USED-FOR DeepXML. negative sub - sampling techniques USED-FOR negative training data. accuracy EVALUATE-FOR DeepXML. residual connection USED-FOR them. Slice algorithm USED-FOR DeepXML architecture. Slice algorithm USED-FOR pretrained embeddings. pretrained embeddings USED-FOR DeepXML architecture. it COMPARE XML - CNN. XML - CNN COMPARE it. XML - CNN CONJUNCTION AttentionXML. AttentionXML CONJUNCTION XML - CNN. it COMPARE AttentionXML. AttentionXML COMPARE it. DeepXML COMPARE leading techniques. leading techniques COMPARE DeepXML. leading techniques USED-FOR search engine queries. search engine queries CONJUNCTION advertiser bid phrases. advertiser bid phrases CONJUNCTION search engine queries. DeepXML USED-FOR search engine queries. Method are DeepXML algorithm, and classifier. Generic is architecture. ","This paper proposes a new DeepXML algorithm for the task of deep extreme multi-label learning, where feature representations and classifiers are used for short text documents. The key idea is to use word embeddings from the Slice algorithm to train a deep XML architecture that can be used to train pretrained embeddits for the pretrained embeddeddings. The authors show that by using negative sub-sampling techniques for negative training data, they can achieve better accuracy than existing leading techniques in terms of search engine queries and advertiser bid phrases. They also show that the proposed architecture is able to perform better than XML-CNN and AttentionXML.","This paper proposes a novel DeepXML algorithm for learning short text documents with word embeddings. The key idea is to combine feature representations and classifiers in deep extreme multi-label learning with deep extreme classifiers. The authors propose a Slice algorithm for training the Deep XML architecture with the pretrained embedding. The proposed architecture is evaluated on a number of search engine queries and advertiser bid phrases, and it outperforms XML-CNN and AttentionXML in terms of accuracy.  The authors also propose negative sub-sampling techniques for negative training data, and use them to improve the performance of the classifier. "
21600,SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"binary vector representations ( hash codes ) USED-FOR Hashing - based collaborative filtering. Hamming distance USED-FOR recommendations. Hamming distance USED-FOR hashing - based collaborative filtering. user hash code USED-FOR mask. Boolean AND operation USED-FOR user hash code. Boolean AND operation USED-FOR mask. approach COMPARE baselines. baselines COMPARE approach. NDCG EVALUATE-FOR approach. runtime overhead EVALUATE-FOR Hamming distance. self - masking COMPARE Hamming distance. Hamming distance COMPARE self - masking. OtherScientificTerm are hash codes, and binary user - level importance weighting. Task is distance computation. Generic is it. ","This paper studies the problem of hashing-based collaborative filtering with binary vector representations (hash codes). The authors propose to use Hamming distance between the hash codes of the user hash code and the mask of the mask, which is based on a Boolean AND operation. The authors show that the Hamming distances can be used to improve the performance of recommendations by reducing the runtime overhead of the distance computation. The proposed approach is evaluated on the NDCG and compared to other baselines. ","This paper proposes to use binary vector representations (hash codes) for Hashing-based collaborative filtering. The key idea is to use the Hamming distance between the hash codes of the user hash code and the hash code of the mask, which is a Boolean AND operation. The authors show that Hamming distances can be used to improve the recommendations made by the user. The paper also shows that the proposed approach outperforms other baselines on NDCG in terms of runtime overhead. The main contribution of the paper is the use of binary user-level importance weighting, and it is shown that it is more efficient than self-masking."
21609,SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"Generative adversarial networks ( GANs ) USED-FOR images. mode collapse FEATURE-OF GAN ’s learned distribution. evaluation metrics EVALUATE-FOR image synthesis. low - level perceptual quality EVALUATE-FOR evaluation metrics. statistical tools USED-FOR mode collapse. mode collapse FEATURE-OF GANs. mode collapse FEATURE-OF GANs. toolset USED-FOR GANs. toolset USED-FOR mode collapse. OtherScientificTerm are GAN learned distribution, and model parameters. ","This paper studies the problem of generating adversarial networks (GANs) for images with low-level perceptual quality. The authors propose a new evaluation metrics for image synthesis based on the mode collapse of the GAN’s learned distribution. The main contribution of the paper is a new toolset for evaluating GANs with mode collapse, which is based on statistical tools. The paper also proposes a new GAN learned distribution that can be used to estimate the model parameters.",Generative adversarial networks (GANs) are used to generate images with low-level perceptual quality. The authors propose a new evaluation metrics for image synthesis. The main idea is to measure the mode collapse of a GAN’s learned distribution. The mode collapse is measured using statistical tools. The proposed toolset is used to evaluate GANs with mode collapse. 
21618,SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"over - parametrized neural networks CONJUNCTION linearized models. linearized models CONJUNCTION over - parametrized neural networks. Neural Tangent Kernels ( NTKs ) USED-FOR linearized models. neural networks COMPARE linearized models. linearized models COMPARE neural networks. Taylor expansion FEATURE-OF network. optimization landscape FEATURE-OF randomized two - layer networks. escaping - saddle algorithms USED-FOR optimization landscape. mild distributional assumptions FEATURE-OF dimension factor. it USED-FOR networks. higher - order terms PART-OF Taylor series. networks CONJUNCTION higher - order terms. higher - order terms CONJUNCTION networks. it USED-FOR randomization technique. Method are NTK theory, Taylor expansion of the network, quadratic models, and randomized networks. Generic are theory, and them. OtherScientificTerm are NTK regime, NTK, and sample complexity bounds. Material is quadratic case. ","This paper studies the problem of over-parametrized neural networks and linearized models trained with Neural Tangent Kernels (NTKs) under the NTK theory. The authors consider the quadratic case, where the number of parameters of the network is bounded by the Taylor expansion of the neural network, and the dimension factor of the dimension of the model is unknown. They show that under mild distributional assumptions, the optimization landscape of randomized two-layer networks trained with escaping-saddle algorithms can be approximated by a Taylor expansion on the network. They also show that it can be used to train networks with higher-order terms in the Taylor series, and that it is possible to use a randomization technique to improve the sample complexity bounds. ","The paper proposes a new NTK theory, Neural Tangent Kernels (NTKs) for over-parametrized neural networks and linearized models. The theory is based on the NTK regime. The authors show that the Taylor expansion of the network is a function of the dimension factor under mild distributional assumptions. They also show that neural networks with higher-order terms in the Taylor series are more robust than neural networks that have fewer networks. The paper also shows that the optimization landscape of randomized two-layer networks with escaping-saddle algorithms is similar to that of randomized networks with Taylor expansion, and that it is equivalent to a randomization technique.  The authors also provide sample complexity bounds for the quadratic case. "
21627,SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"graph data USED-FOR downstream tasks. Graph Neural Networks ( GNNs ) USED-FOR graph data. graph filter design PART-OF GNN models. filter USED-FOR graph data. graph properties USED-FOR graph filter. filters USED-FOR graph. assessment tool EVALUATE-FOR graph convolutional filters. graph convolutional filters USED-FOR graph. node classification EVALUATE-FOR graph convolutional filters. graphs USED-FOR graph convolutional filters. model USED-FOR data - specific filters. Adaptive Filter Graph Neural Network ( AFGNN ) HYPONYM-OF model. AFGNN USED-FOR graph. base filters PART-OF AFGNN. graph filter assessment USED-FOR AFGNN. synthetic and real - world benchmark datasets EVALUATE-FOR model. model USED-FOR filter. Method are optimal filter, and Graph Filter Discriminant Score ( GFD ). Task is semi - supervised node classification task. OtherScientificTerm is loss term. ","This paper proposes a new graph filter design for graph convolutional filters. The authors propose a new loss term, called Adaptive Filter Graph Neural Network (AFGNN), which is based on the Graph Filter Discriminant Score (GFD). The authors show that AFGNN is able to learn the optimal filter for each node in a graph. They also show that the proposed model can learn the data-specific filters for different graphs. The proposed model is evaluated on both synthetic and real-world benchmark datasets.",This paper proposes a graph filter design for graph data for downstream tasks. Graph Neural Networks (GNNs) can be seen as graph convolutional filters that can be applied to graph data. The authors propose an adaptive filter graph neural network (AFGNN) that is based on the Adaptive Filter Graph Neural Network (AFGN). The authors show that the proposed model is able to learn data-specific filters for a given graph. The model is evaluated on both synthetic and real-world benchmark datasets. 
21636,SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"i.i.d. test set EVALUATE-FOR Overparameterized neural networks. Distributionally robust optimization ( DRO ) USED-FOR models. group DRO USED-FOR overparameterized neural networks. average training loss FEATURE-OF model. vanishing worst - case training loss FEATURE-OF model. natural language inference task CONJUNCTION image tasks. image tasks CONJUNCTION natural language inference task. early stopping HYPONYM-OF regularization. regularization USED-FOR group DRO models. regularization USED-FOR worst - group generalization. it USED-FOR average generalization. overparameterized regime FEATURE-OF worst - group generalization. stochastic optimization algorithm USED-FOR group DRO models. convergence guarantees FEATURE-OF stochastic optimization algorithm. OtherScientificTerm is spurious correlations. Metric are worst - case training loss, worst - group accuracies, and average accuracies. ","This paper studies the problem of group DRO for overparameterized neural networks on the i.i.d. test set. Distributionally robust optimization (DRO) is a popular method for training models with spurious correlations. The authors show that the average training loss of a model with a vanishing worst-case training loss is the best-case worst-group generalization under the overparametersized regime. They then propose a stochastic optimization algorithm to improve the performance of the group DROs by regularizing the worst-groups by early stopping. They show that this regularization improves the performance on the natural language inference task and the image tasks. They also show that it improves the average generalization of the best group accuracies. Finally, they provide convergence guarantees for the stochastically optimization algorithm.","This paper proposes Distributionally robust optimization (DRO) for models trained with group DRO on the d.i.d. test set. Overparameterized neural networks have been shown to be robust to spurious correlations. The authors show that the average training loss of a model with vanishing worst-case training loss is the same as that of the model with the vanishing best case training loss. They also show that under certain conditions, the worst-group accuracies are close to the average accuracies of the best-group generalization, and that the regularization is equivalent to early stopping. They show that this regularization can be used to improve the generalization performance of groupDRO models with stochastic optimization algorithm on the natural language inference task and image tasks. Finally, they show convergence guarantees for the stochastically optimization algorithm in the overparameterization regime."
21645,SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"local explanation methods USED-FOR decision of black - box classifiers. ad hoc constraints FEATURE-OF classification loss. ad hoc constraints USED-FOR relevance scores. neural network USED-FOR distribution of relevance scores. it CONJUNCTION neural network. neural network CONJUNCTION it. it USED-FOR distribution of relevance scores. classification loss USED-FOR predictor. strategy USED-FOR discriminative scores. features USED-FOR discriminative scores. faithfulness CONJUNCTION explainability. explainability CONJUNCTION faithfulness. method COMPARE others. others COMPARE method. faithfulness EVALUATE-FOR others. explainability EVALUATE-FOR others. faithfulness EVALUATE-FOR method. explainability EVALUATE-FOR method. Generic is methods. Method are mask predictor, and distribution controllers. OtherScientificTerm is hyperparameters. ","This paper studies the problem of local explanation methods for the decision of black-box classifiers. The authors propose a new classification loss based on ad hoc constraints on the relevance scores of the mask predictor. The proposed method uses a neural network to predict the distribution of relevance scores, and then uses it to train the neural network. The paper shows that the proposed method has better faithfulness and explainability than others. ","This paper proposes a new local explanation methods for the decision of black-box classifiers. The proposed methods are based on a mask predictor, where the mask predictor is a neural network and the distribution controllers are hyperparameters. The classification loss is based on ad hoc constraints on the relevance scores of the classification loss. The authors propose a strategy to learn discriminative scores based on the features of the features. The method is evaluated on faithfulness, explainability, and the proposed method outperforms others."
21654,SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"deep network USED-FOR image reconstruction and classification problems. task - specific network USED-FOR domain specific problem. patches USED-FOR task - specific network. auto - encoder or classifier HYPONYM-OF task - specific network. slack variable USED-FOR top - K selection. method USED-FOR recurring structures. it COMPARE state - of - the - art. state - of - the - art COMPARE it. Task are detection of multiple object instances, and training optimization problem. OtherScientificTerm is supervision. Generic are network, and It. Method are non - differentiable top - K selection process, and multi - stage training. ","This paper proposes a new deep network for image reconstruction and classification problems. The task-specific network, called auto-encoder or classifier, is a domain specific network with patches. It is a non-differentiable top-K selection process where the top-k selection process is based on a slack variable. The authors show that the proposed method is able to identify recurring structures in the training optimization problem. They also show that it performs better than state-of-the-art in terms of detection of multiple object instances. ","This paper proposes a deep network for image reconstruction and classification problems. It is a task-specific network, called auto-encoder or classifier, which is a domain specific problem. The main idea is to use patches in the network to make the network non-differentiable top-K selection process. The authors propose to use a slack variable in the top-k selection process, which allows to avoid multi-stage training. The proposed method is evaluated on the detection of multiple object instances, and it is shown to outperform state-of-the-art on recurring structures. "
21663,SP:da1c5f6351d531482e90b86c3cceb52850c520de,assembly code USED-FOR state change. CPU FEATURE-OF state change. RAM FEATURE-OF state change. self - learning reinforcement learning USED-FOR large code space. AutoAssemblet HYPONYM-OF neural program synthesis algorithm. self - learning reinforcement learning USED-FOR AutoAssemblet. Policy networks CONJUNCTION value networks. value networks CONJUNCTION Policy networks. value networks USED-FOR Monte Carlo Tree Search. Policy networks USED-FOR synthesis. value networks USED-FOR synthesis. multi - entropy policy sampling technique USED-FOR online update correlations. AutoAssemblet USED-FOR basic programming tasks. success rates EVALUATE-FOR baselines. AutoAssemblet COMPARE baselines. baselines COMPARE AutoAssemblet. success rates EVALUATE-FOR AutoAssemblet. Method is Neural inductive program synthesis. Task is task generating instructions. ,"This paper proposes AutoAssemblet, a neural program synthesis algorithm based on self-learning reinforcement learning for large code space. The authors propose a multi-entropy policy sampling technique to improve the online update correlations between different tasks. The synthesis is based on a combination of Policy networks and value networks for Monte Carlo Tree Search, which is a popular technique for task generating instructions. Experimental results show that AutoAsemblet achieves better success rates than other baselines on a variety of basic programming tasks.","This paper proposes AutoAssemblet, a neural program synthesis algorithm based on self-learning reinforcement learning for large code space. The authors propose to use the assembly code for state change in the CPU, and compute the state change using RAM. The synthesis is done by combining Policy networks and value networks for Monte Carlo Tree Search. The paper also proposes a multi-entropy policy sampling technique to learn online update correlations between the synthesis and the task generating instructions. Experiments are conducted on a number of basic programming tasks and show that AutoAssembt achieves better success rates than other baselines. "
21672,SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"speed of training CONJUNCTION resource requirements. resource requirements CONJUNCTION speed of training. accuracy CONJUNCTION speed of training. speed of training CONJUNCTION accuracy. accuracy EVALUATE-FOR model architecture. model architecture USED-FOR gradient descent optimization. speed of training EVALUATE-FOR gradient descent optimization. speed of training EVALUATE-FOR model architecture. ODE ’s coefficient matrix H USED-FOR convergence rate. first - order ODE USED-FOR gradient descent. analysis technique USED-FOR H. analysis technique USED-FOR model architecture modifications. OtherScientificTerm are neural network architecture design space, network, and model architecture parameters. Generic is architecture. Metric is speed of convergence. ","This paper studies the problem of gradient descent optimization in a neural network architecture design space. The authors propose a new model architecture based on the ODE’s coefficient matrix H, which is a first-order ODE for gradient descent. They show that the speed of training and the accuracy of the proposed model architecture are important factors in the convergence rate. They also provide an analysis technique for model architecture modifications. ","The paper proposes a new model architecture for gradient descent optimization based on the ODE’s coefficient matrix H. The main idea is to use the first-order ODE for the convergence rate of gradient descent. The paper also proposes an analysis technique to evaluate the H of the proposed H. This is done by considering the neural network architecture design space as a network, and the model architecture parameters as a function of the speed of training and the resource requirements. The authors show that the proposed model architecture improves the accuracy of the training and speed of learning in terms of accuracy. The proposed analysis technique is also applied to model architecture modifications."
21681,SP:3e3bc8f617df742a395e7d315ec3810a42071294,"overparametrized neural networks ( NNs ) CONJUNCTION kernel methods. kernel methods CONJUNCTION overparametrized neural networks ( NNs ). initialization USED-FOR overparametrized NNs. gradient descent USED-FOR overparametrized NNs. minimum complexity solution USED-FOR interpolating kernel method. squared loss USED-FOR fully - connected wide ReLU - NNs. test error EVALUATE-FOR wide NNs. minimum complexity interpolating kernel methods CONJUNCTION NNs. NNs CONJUNCTION minimum complexity interpolating kernel methods. generalization EVALUATE-FOR initialization scheme. Generic are first, and second. OtherScientificTerm are initialization variance, and generalization bounds. Method is initialization strategies. ","This paper studies the problem of initialization for overparametrized neural networks (NNs) and kernel methods. The authors propose a new initialization scheme based on gradient descent. The main idea is to use the minimum complexity solution of an interpolating kernel method with a squared loss to initialize fully-connected wide ReLU-NNs. They show that the initialization variance of wide NNs with the squared loss is bounded by the test error of the first, and the second. They also provide generalization bounds for the initialization scheme. ","This paper proposes a new initialization scheme for overparametrized neural networks (NNs) and kernel methods. The authors propose to use gradient descent to initialize the overparameterized NNs, and then use a minimum complexity solution for the interpolating kernel method. The main idea is to minimize the initialization variance between the first and second layers of the NNs. This is done by minimizing the squared loss of fully-connected wide ReLU-NNs, which is the test error of wide NNs with test error in terms of test error. The initialization scheme is evaluated on generalization, and the authors show that the proposed initialization strategies outperform other initialization strategies. "
21690,SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"cars CONJUNCTION pedestrians. pedestrians CONJUNCTION cars. Detecting objects USED-FOR autonomous driving. 3D FEATURE-OF pedestrians. 3D FEATURE-OF cars. 3D FEATURE-OF Detecting objects. pedestrians HYPONYM-OF Detecting objects. cars HYPONYM-OF Detecting objects. LiDAR sensors USED-FOR accurate depth information. LiDAR sensors USED-FOR approaches. stereo images USED-FOR pseudo - LiDAR. stereo depth estimation USED-FOR pseudo - LiDAR framework. loss function USED-FOR depth estimation of faraway objects. stereo network architecture CONJUNCTION loss function. loss function CONJUNCTION stereo network architecture. depth estimates USED-FOR depthpropagation algorithm. depth estimation CONJUNCTION stereo - based 3D object detection. stereo - based 3D object detection CONJUNCTION depth estimation. KITTI object detection benchmark EVALUATE-FOR approach. approach USED-FOR depth estimation. detection accuracy USED-FOR faraway objects. approach USED-FOR stereo - based 3D object detection. approach COMPARE detection accuracy. detection accuracy COMPARE approach. OtherScientificTerm are insufficient information, and depth map. Task is 3D detection. ",This paper proposes a pseudo-LiDAR framework based on stereo depth estimation using stereo images. The proposed approach is based on a combination of a stereo network architecture and a loss function to perform depth estimation of faraway objects. The depth estimates are then used to train a depthpropagation algorithm. The authors show that the proposed approach achieves better detection accuracy than standard depth estimation and stereo-based 3D object detection on KITTI object detection benchmark.,This paper proposes a new approach for 3D detection of objects in 3D. Detecting objects is an important problem in autonomous driving. The authors propose a pseudo-LiDAR framework based on stereo depth estimation using stereo images. The proposed approach is evaluated on the KITTI object detection benchmark and shows better detection accuracy compared to standard depth estimation and stereo-based 3D object detection. The key idea is to use LiDAR sensors to capture accurate depth information. The loss function for depth estimation of faraway objects is based on the loss function of the stereo network architecture and the depthpropagation algorithm. The depth map is computed using the depth estimates of the depth map. 
21699,SP:983d84502264633f3385d426c1d4601a0744ea9a,"models USED-FOR sensitive domains. deep neural networks USED-FOR adversarial examples. defense USED-FOR attacks. detecting adversarial samples USED-FOR methods. adversarial example detection method USED-FOR norm - constrained white - box attacks. detector USED-FOR natural data. base detectors USED-FOR K class classification problem. generative approach USED-FOR detecting / classifying adversarial examples. classconditional data USED-FOR unnormalized density model. unnormalized density model USED-FOR base detector. classconditional data USED-FOR base detector. Method are detection mechanism, one - versus - the - rest classification, k - th detector, adversarial example detection / classification methods, and GAT - Generative - Adversarial - Training. OtherScientificTerm is adversarial example. ","This paper proposes a new detection mechanism for the K class classification problem. The proposed method, GAT-Generative-Adversarial-Training (GAT-GAT), uses a generative model to generate adversarial examples for each K class. The authors show that the proposed method can be used to train a base detector on the unnormalized density model on classconditional data, which is then used as the base detector for the k-th detector. The paper also provides a theoretical analysis of the proposed GAT.","This paper proposes a novel adversarial example detection method for norm-constrained white-box attacks. The authors propose a generative approach for detecting/classifying adversarial examples from deep neural networks. The proposed methods are based on detecting adversarial samples from the detection mechanism of one-vs-the-rest classification. The defense against these attacks is based on the defense against adversarial attacks in sensitive domains. The paper proposes GAT-Generative-Adversarial-Training, which uses a k-th detector as the base detector and an unnormalized density model on classconditional data. The base detectors are used in the K class classification problem, and the detector is trained on natural data. "
21708,SP:461e9308d050bc3dc7b35233452668bb31f5d491,"Exploration PART-OF model - free reinforcement learning. sparse reward environments FEATURE-OF Exploration. intrinsic rewards USED-FOR exploration. MiniGrid FEATURE-OF procedurally - generated tasks. high - dimensional observations USED-FOR tasks. tasks EVALUATE-FOR method. procedurally - generated tasks EVALUATE-FOR method. approach COMPARE exploration methods. exploration methods COMPARE approach. exploration methods USED-FOR procedurally - generated MiniGrid environments. intrinsic reward FEATURE-OF agent. approaches COMPARE intrinsic reward. intrinsic reward COMPARE approaches. OtherScientificTerm are extrinsic rewards, and procedurally - generated environments. Method is learned state representation. Generic is it. ","This paper studies the problem of exploration in model-free reinforcement learning in sparse reward environments. The authors propose to use intrinsic rewards as intrinsic rewards to encourage exploration in exploration. They show that the intrinsic reward can be used to encourage the agent to explore in environments where extrinsic rewards are not available. They also show that their method can be applied to procedurally-generated tasks such as MiniGrid, where high-dimensional observations are available. ","This paper proposes an extension of exploration in model-free reinforcement learning to sparse reward environments. Exploration is an important part of exploration, but it is usually done in extrinsic rewards. The authors propose to use intrinsic rewards to encourage exploration in exploration. The intrinsic reward is a learned state representation of the agent’s state, and the agent is encouraged to explore in procedurally-generated environments with high-dimensional observations. The proposed method is evaluated on a variety of tasks in MiniGrid, where it outperforms other exploration methods on procedurally generated MiniGrid environments."
21717,SP:c002c20b5e8696588e029c0f65e88860418826c4,"recall EVALUATE-FOR retrieval algorithm. scoring phase COMPARE retrieval phase. retrieval phase COMPARE scoring phase. cross - attention models USED-FOR BERT - style pre - training tasks. sparse handcrafted features USED-FOR models. pre - training tasks USED-FOR embedding - based Transformer model. Transformer models COMPARE BM-25. BM-25 COMPARE Transformer models. Transformer models COMPARE embedding models. embedding models COMPARE Transformer models. BM-25 CONJUNCTION embedding models. embedding models CONJUNCTION BM-25. paragraph - level pre - training tasks EVALUATE-FOR Transformer models. Body First Selection ( BFS ) CONJUNCTION Wiki Link Prediction ( WLP ). Wiki Link Prediction ( WLP ) CONJUNCTION Body First Selection ( BFS ). Inverse Cloze Task ( ICT ) CONJUNCTION Body First Selection ( BFS ). Body First Selection ( BFS ) CONJUNCTION Inverse Cloze Task ( ICT ). Inverse Cloze Task ( ICT ) HYPONYM-OF paragraph - level pre - training tasks. Wiki Link Prediction ( WLP ) HYPONYM-OF paragraph - level pre - training tasks. Body First Selection ( BFS ) HYPONYM-OF paragraph - level pre - training tasks. Task is large - scale query - document retrieval problem. Material is large document corpus. Generic is problem. OtherScientificTerm are solution space, and TF - IDF weights. Method are Information Retrieval ( IR ) methods, and embedding - based retrieval models. ","This paper studies the large-scale query-document retrieval problem, where the goal is to retrieve documents from a large document corpus with sparse handcrafted features. The authors propose a retrieval algorithm with a score-based retrieval algorithm, which is based on a scoring phase, and a retrieval phase based on the score of the embedding-based Transformer model trained on pre-training tasks with cross-attention models. They show that Transformer models perform better than BM-25 and embedding models on paragraph-level pre-train tasks such as Inverse Cloze Task (ICT), Body First Selection (BFS) and Wiki Link Prediction (WLP). They also show that information Retrieval (IR) methods do not perform well in this problem. ","This paper studies the large-scale query-document retrieval problem. The authors propose a retrieval algorithm based on information retrieval (IR) methods. The main idea is to use cross-attention models to perform BERT-style pre-training tasks on a large document corpus. The models are trained on sparse handcrafted features. The solution space is defined as the TF-IDF weights. The retrieval phase is a scoring phase, where the number of documents in the solution space depends on the size of the document corpus, and the retrieval algorithm is based on recall. Experiments show that the proposed Transformer models outperform BM-25 and other embedding models on paragraph-level pre-train tasks such as Inverse Cloze Task (ICT), Body First Selection (BCF), Wiki Link Prediction (WLP), etc. "
21726,SP:4e161e08a624f87633dfb49dfd46bd1665e15189,social graphs CONJUNCTION molecular structures. molecular structures CONJUNCTION social graphs. point clouds CONJUNCTION social graphs. social graphs CONJUNCTION point clouds. Graph neural networks USED-FOR applications. Graph neural networks USED-FOR learning relational representations. learning relational representations CONJUNCTION modeling data on irregular domains. modeling data on irregular domains CONJUNCTION learning relational representations. modeling data on irregular domains HYPONYM-OF applications. molecular structures HYPONYM-OF modeling data on irregular domains. point clouds HYPONYM-OF modeling data on irregular domains. social graphs HYPONYM-OF modeling data on irregular domains. learning relational representations HYPONYM-OF applications. graph convolution operator USED-FOR graph neural network architectures. graph convolution operations CONJUNCTION non - parameterized pooling or expansion layers. non - parameterized pooling or expansion layers CONJUNCTION graph convolution operations. non - parameterized pooling or expansion layers USED-FOR representational hierarchy. graph convolution operations USED-FOR representational hierarchy. parameterized strided and transpose convolution operations CONJUNCTION skip connections. skip connections CONJUNCTION parameterized strided and transpose convolution operations. convolutional network architectures CONJUNCTION parameterized strided and transpose convolution operations. parameterized strided and transpose convolution operations CONJUNCTION convolutional network architectures. bipartite graph convolution operation HYPONYM-OF parameterized transformation. framework USED-FOR multi - graph aggregation. framework COMPARE graph convolution and pooling. graph convolution and pooling COMPARE framework. framework USED-FOR flexible and adaptable network architectures. BiGraphNet HYPONYM-OF flexible and adaptable network architectures. memory requirements FEATURE-OF hierarchical networks. graph convolution USED-FOR hierarchical architectures. graph convolution CONJUNCTION single parametric bipartite graph convolution. single parametric bipartite graph convolution CONJUNCTION graph convolution. graph skip connections CONJUNCTION graph autoencoders. graph autoencoders CONJUNCTION graph skip connections. BiGraphNet formalism ( iii ) USED-FOR architectures. modeling flexibility USED-FOR architectures. BiGraphNet formalism ( iii ) USED-FOR modeling flexibility. graph autoen,"This paper proposes a graph convolution operator for graph neural network architectures. Graph neural networks are widely used in applications such as learning relational representations, point clouds, social graphs, molecular structures, and social graphs on irregular domains. Graph convolution operations and non-parameterized pooling or expansion layers are used to represent the representational hierarchy between the graphs. The authors propose a framework for multi-graph aggregation and show that the proposed framework is able to achieve better performance than the existing methods in terms of memory requirements for hierarchical networks. The proposed framework also shows that graph convion and pooling are more efficient than single parametric bipartite graph convualtion and single-graph convolution with parameterized transformation. Finally, the authors show that their framework can be applied to a variety of different architectures such as BiGraphNet, graph autoencoders, and graph skip connections. ","This paper presents a graph convolution operator for graph neural network architectures that can be used for applications such as learning relational representations and point clouds. The authors propose a framework for multi-graph aggregation, which is based on the BiGraphNet formalism (ii) that allows for modeling flexibility in different architectures. The proposed framework is evaluated on a number of datasets, and compared to graph convion and pooling, and non-parameterized pooling or expansion layers for representational hierarchy. The results show that the proposed framework outperforms the baselines in terms of memory requirements of hierarchical networks, and is able to achieve better performance than single parametric bipartite Graph convolution operation and parameterized transformation. "
21735,SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"metric function USED-FOR metric - based few - shot classification algorithms. metric function USED-FOR feature embeddings. metric - based methods USED-FOR few - shot classification. domain shifts FEATURE-OF few - shot classification. feature - wise transformation layers USED-FOR image features. affine transforms USED-FOR feature distributions. feature - wise transformation layers USED-FOR feature distributions. affine transforms USED-FOR feature - wise transformation layers. learning - to - learn approach USED-FOR hyper - parameters. hyper - parameters FEATURE-OF feature - wise transformation layers. learning - to - learn approach USED-FOR feature - wise transformation layers. learning - to - learn approach USED-FOR feature distributions. Cars CONJUNCTION Places. Places CONJUNCTION Cars. CUB CONJUNCTION Cars. Cars CONJUNCTION CUB. Places CONJUNCTION Plantae. Plantae CONJUNCTION Places. mini - ImageNet CONJUNCTION CUB. CUB CONJUNCTION mini - ImageNet. Plantae HYPONYM-OF few - shot classification datasets. mini - ImageNet HYPONYM-OF few - shot classification datasets. CUB HYPONYM-OF few - shot classification datasets. Places HYPONYM-OF few - shot classification datasets. Cars HYPONYM-OF few - shot classification datasets. feature - wise transformation layer USED-FOR metric - based models. feature - wise transformation layer USED-FOR few - shot classification. domain shift FEATURE-OF few - shot classification. Task are Few - shot classification, and domain generalization setting. Generic is methods. OtherScientificTerm is feature distribution. ","This paper proposes a new metric function for few-shot classification algorithms. The metric function is used to represent the feature embeddings of the image features. The feature-wise transformation layers are trained using affine transforms to learn the feature distributions. The learning-to-learn approach learns the hyper-parameters of the feature-wise transformation layers, which are then used to train the metric-based models. The authors demonstrate the effectiveness of the proposed methods in the domain generalization setting. ",This paper proposes a new metric function for few-shot classification algorithms. The metric function is used to represent the feature embeddings of the image features. The feature-wise transformation layers for image features are based on affine transforms. The hyper-parameters of the feature-wise transformation layers are learned using a learning-to-learn approach to learn the feature distributions. The authors show that the proposed methods outperform existing metric-based methods in the domain generalization setting. 
21744,SP:df46627cb984a56bba36d510bfc52e00751e9107,approach USED-FOR Lagrangian fluid simulation. convolutional network USED-FOR approach. moving particles USED-FOR fluids. networks USED-FOR moving particles. graph structure USED-FOR particles. N - D convolutions USED-FOR continuous domain. network architecture USED-FOR inverse problems. network architecture USED-FOR arbitrary collision geometries. continuous convolutions COMPARE prior formulations. prior formulations COMPARE continuous convolutions. accuracy CONJUNCTION speed. speed CONJUNCTION accuracy. speed EVALUATE-FOR prior formulations. speed EVALUATE-FOR continuous convolutions. accuracy EVALUATE-FOR continuous convolutions. accuracy EVALUATE-FOR prior formulations. Generic is approaches. Method is spatial convolutions. ,"This paper proposes a new approach for Lagrangian fluid simulation based on a convolutional network. The proposed approach is based on moving particles through a graph structure, where the moving particles are represented as moving particles in a continuous domain. The authors use N-D convolutions to represent the continuous domain, and the network architecture is then used to solve inverse problems. The results show that the proposed continuous convolutions perform better than prior formulations in terms of accuracy and speed. ","This paper proposes a novel approach for Lagrangian fluid simulation. The approach is based on a convolutional network. The authors propose to use N-D convolutions for the continuous domain, where moving particles are represented as moving particles in a graph structure. The network architecture is then used to solve inverse problems with arbitrary collision geometries. Experiments show that the proposed continuous convolutions outperform prior formulations in terms of accuracy and speed. "
21753,SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"accuracy CONJUNCTION predictive uncertainty. predictive uncertainty CONJUNCTION accuracy. predictive uncertainty EVALUATE-FOR single neural networks. accuracy EVALUATE-FOR single neural networks. BatchEnsemble1 HYPONYM-OF ensemble method. Hadamard product USED-FOR weight matrix. ensembles COMPARE BatchEnsemble. BatchEnsemble COMPARE ensembles. BatchEnsemble COMPARE ensembles. ensembles COMPARE BatchEnsemble. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. speedup CONJUNCTION memory reduction. memory reduction CONJUNCTION speedup. CIFAR-10 EVALUATE-FOR BatchEnsemble. out - of - distribution tasks EVALUATE-FOR BatchEnsemble. BatchEnsemble USED-FOR lifelong learning. Split - CIFAR-100 EVALUATE-FOR BatchEnsemble. BatchEnsemble COMPARE progressive neural networks. progressive neural networks COMPARE BatchEnsemble. computational and memory costs EVALUATE-FOR BatchEnsemble. BatchEnsemble USED-FOR lifelong learning. Split - ImageNet USED-FOR lifelong learning. sequential learning tasks PART-OF lifelong learning. Method are Ensembles, and neural networks. Metric is ensemble ’s cost. OtherScientificTerm are mini - batch, and ensemble. ","This paper proposes a new ensemble method called BatchEnsemble1, which is a variant of the Batch Ensemble1. The key idea is to use a Hadamard product to estimate the weight matrix of the ensemble, and then use the ensemble’s cost to compute the accuracy and predictive uncertainty of the single neural networks. Ensembles can be used for lifelong learning on sequential learning tasks, where the mini-batch is used to train the neural networks, and the ensemble is used as a benchmark to compare the performance of different ensembles. The authors show that Batchensemble performs better than BatchElements on out-of-distribution tasks, and is faster than progressive neural networks on CIFAR-10 CONJUNCTION and CifAR-100. The computational and memory costs are also improved. ","This paper proposes an ensemble method called BatchEnsemble1, which is a variant of Ensembles, where each batch is a mini-batch, and the weights of the ensemble are the Hadamard product of the weight matrix of the previous batch. The authors show that the ensemble’s cost is lower than that of the ensembles. The accuracy of single neural networks and predictive uncertainty are also lower than the accuracy of Batchensemble. They also show that for out-of-distribution tasks (e.g., CIFAR-10, CIFar-100, etc.), the performance of the proposed Batch ensemble is better than the performance on ensembled. They show that speedup, memory reduction, and memory reduction are more important for BatchENsemble than the computational and memory costs of progressive neural networks. Finally, the authors propose a new lifelong learning task called Split-ImageNet, where the ensemble is trained on sequential learning tasks. "
21762,SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"neural network - based partial differential equations solver USED-FOR forward and inverse problems. mesh free and shape free FEATURE-OF solver. neural network USED-FOR solution. explicit smooth differentiable function USED-FOR solution. analytical form FEATURE-OF explicit smooth differentiable function. finite differences CONJUNCTION finite elements. finite elements CONJUNCTION finite differences. finite differences HYPONYM-OF numerical methods. finite elements HYPONYM-OF numerical methods. algorithm USED-FOR forward and inverse problems. Robust boundary conditions constraints CONJUNCTION regularizers. regularizers CONJUNCTION Robust boundary conditions constraints. Electrical Impedance Tomography ( EIT ) CONJUNCTION diffusion and wave equations. diffusion and wave equations CONJUNCTION Electrical Impedance Tomography ( EIT ). free shape 2D second order systems USED-FOR Electrical Impedance Tomography ( EIT ). method USED-FOR diffusion and wave equations. method USED-FOR Electrical Impedance Tomography ( EIT ). free shape 2D second order systems USED-FOR diffusion and wave equations. method USED-FOR free shape 2D second order systems. Method is unsupervised approach. Generic are network, framework, and methods. OtherScientificTerm are strong PDE solution, boundary conditions, derivatives of the desired function, and loss function. ","This paper proposes a neural network-based partial differential equations solver for forward and inverse problems. The solver is mesh free and shape free, and can be used to solve any strong PDE solution with boundary conditions. The solution is obtained by using a trained neural network to solve the solution using an explicit smooth differentiable function in an analytical form. The authors show that the proposed algorithm can solve both forward and inverse problems with Robust boundary conditions constraints and regularizers. The proposed framework is well-motivated and easy to follow. The numerical methods such as finite differences, finite elements, and other numerical methods are well-studied, and the proposed method can be applied to free shape 2D second order systems such as Electrical Impedance Tomography (EIT) and diffusion and wave equations.","This paper proposes a neural network-based partial differential equations solver for forward and inverse problems. The solver is mesh free and shape free, which is a strong PDE solution. The authors propose an unsupervised approach to solve the problem. The proposed framework is based on the idea that the solution of the problem can be solved by a network trained on a mesh network. The problem is formulated in terms of an explicit smooth differentiable function in an analytical form, which can be expressed as a combination of two numerical methods: finite differences and finite elements. The main idea of the proposed algorithm is to learn the desired function under boundary conditions and regularizers. Then, the authors propose a loss function that is defined as the sum of the derivatives of a desired function. Experiments show that the proposed method is able to solve diffusion and wave equations and free shape 2D second order systems. "
21771,SP:973d0ad0faadcf7298300f2758de9154205e7113,neural networks PART-OF deep learning. Binarized Neural Networks HYPONYM-OF networks. SAT solvers HYPONYM-OF logic - based reasoning tools. tools USED-FOR existential and probabilistic queries. tools USED-FOR explanation generation. existential and probabilistic queries FEATURE-OF network. they USED-FOR logic - based reasoners. training procedure USED-FOR network. network USED-FOR SAT solvers. BNN architecture CONJUNCTION training procedure. training procedure CONJUNCTION BNN architecture. approach COMPARE work. work COMPARE approach. work USED-FOR existential and probabilistic queries. approach USED-FOR existential and probabilistic queries. deep neural networks COMPARE work. work COMPARE deep neural networks. approach USED-FOR deep neural networks. deep neural networks USED-FOR existential and probabilistic queries. OtherScientificTerm is Boolean logic. Generic is methods. Method is BNNs. Metric is accuracy. ,"This paper studies the problem of explaining why BNNs are better than deep neural networks in deep learning. The authors propose Binarized Neural Networks (BNNs), a family of networks that can be used to generate explanations for Boolean logic. In particular, they are used to train logic-based reasoners, such as SAT solvers. These tools are useful tools for explanation generation and can be applied to both existential and probabilistic queries in a network. The proposed network is trained using a BNN architecture and a training procedure. The experimental results show that the proposed approach performs better than existing work for existential, probabilistically queries, and is more accurate than other methods. ","This paper proposes a new approach to learning the explanation generation of deep neural networks in the context of deep learning. Binarized Neural Networks (BNNs) are a family of networks that can be viewed as logic-based reasoning tools (e.g., SAT solvers). The authors show that they can be used to learn logic -based reasoners. The authors also show that the network can be trained using a BNN architecture and a training procedure that is similar to existing methods. The paper also shows that the proposed approach can be applied to both existential and probabilistic queries of the network, and that the work is comparable to existing work on existential and Probabilistic querying. "
21780,SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"expressive power FEATURE-OF graph neural networks. message - passing framework ( GNNmp ) FEATURE-OF graph neural networks. node attributes CONJUNCTION layer expressiveness. layer expressiveness CONJUNCTION node attributes. technique USED-FOR impossibility statements. approximation USED-FOR tasks. Method are GNNmp, and distributed computing. OtherScientificTerm is lower bounds. Material is graphs. Generic is problems. ","This paper studies the expressive power of graph neural networks with message-passing framework (GNNmp) in the presence of node attributes and layer expressiveness. The authors show that GNNmp can achieve lower bounds on the expressiveness of GNNs when the number of nodes in the graph is small. They then propose a technique to approximate the impossibility statements of these lower bounds. They show that this approximation can be used for a variety of tasks, including distributed computing. They also show that the lower bounds can be extended to graphs. ","This paper proposes a message-passing framework (GNNmp) for graph neural networks with expressive power. The authors show that GNNmp is able to achieve lower bounds on node attributes and layer expressiveness. The paper also proposes a technique for computing impossibility statements for graphs. The main contribution of the paper is that the authors propose an approximation of the lower bounds for different tasks, which can be used for distributed computing. "
21789,SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"flow - based density models USED-FOR target distributions. complicated topologies FEATURE-OF target distributions. continuous bijections USED-FOR flow - based density models. stacked continuous mixtures of bijections PART-OF LGFs. flow - based methods USED-FOR LGF model. method COMPARE flow - based methods. flow - based methods COMPARE method. normalising flows COMPARE LGFs. LGFs COMPARE normalising flows. density estimation tasks EVALUATE-FOR LGFs. Method are localised generative flows ( LGFs ), and variational scheme. OtherScientificTerm are bijection, and log likelihoods. ","This paper proposes a new localised generative flows (LGF) model for target distributions with complicated topologies. The authors propose to use continuous bijections in flow-based density models to approximate the target distributions. The proposed LGFs are composed of stacked continuous mixtures of Bijections, where the bijection is a variational scheme, and the log likelihoods are a function of the number of samples. The paper shows that the proposed method is able to learn a good LGF model with fewer parameters compared to other flow -based methods. The experimental results on density estimation tasks show that LGFs perform better than normalising flows.","This paper proposes a variational scheme for learning localised generative flows (LGFs) with continuous bijections. In particular, the authors propose to learn the target distributions with complicated topologies, where the bijection can be any one of two types: one of which is the log likelihoods of the target distribution. The authors show that the proposed LGF model outperforms flow-based density models on density estimation tasks. The main contribution of the paper is to introduce stacked continuous mixtures of Bijections in the LGFs. The proposed method is shown to outperform the flow -based methods on the density estimation task. "
21798,SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"environment re - splitting CONJUNCTION feature replacement. feature replacement CONJUNCTION environment re - splitting. environment re - splitting USED-FOR diagnosis experiments. feature replacement USED-FOR diagnosis experiments. language CONJUNCTION navigational graph. navigational graph CONJUNCTION language. ResNet features USED-FOR low - level visual appearance. low - level visual information FEATURE-OF semantic representations. features USED-FOR agent. baseline agent model CONJUNCTION training method. training method CONJUNCTION baseline agent model. R4R CONJUNCTION CVDN. CVDN CONJUNCTION R4R. R2R CONJUNCTION R4R. R4R CONJUNCTION R2R. R2R HYPONYM-OF datasets. OtherScientificTerm are naturallanguage instructions, step - by - step navigational instructions, and semantic features. Task is VLN. Method are neural agent models, and agent model. Generic is state - of - the - art models. ",This paper studies the problem of training neural agent models in the context of VLN. The authors propose to use environment re-splitting and feature replacement to perform diagnosis experiments using feature replacement in order to improve the performance of the agent. The agent model is trained using ResNet features to capture the low-level visual appearance of the semantic representations of the language and the navigational graph. These features are then used to train the agent using a baseline agent model and a training method. Experiments are conducted on two datasets: R4R and CVDN.,"This paper proposes a new way to train neural agent models. The main idea is to use environment re-splitting and feature replacement for diagnosis experiments. The authors propose to use ResNet features for low-level visual appearance of the semantic representations of the naturallanguage instructions and step-by-step navigational instructions. The agent model is trained with these features, and the training method is based on a baseline agent model and a training method based on state-of-the-art models. Experiments are conducted on two datasets, R4R and CVDN."
21807,SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"implicit human feedback USED-FOR DRL algorithm. expert labeling CONJUNCTION demonstrations. demonstrations CONJUNCTION expert labeling. agent ’s learning USED-FOR RL tasks. implicit feedback USED-FOR agent ’s learning. system USED-FOR implicit human feedback. implicit human feedback USED-FOR state - action pairs. Atari - type environment FEATURE-OF state - action pairs. error - related event potentials HYPONYM-OF implicit human feedback. auxiliary reward function USED-FOR DRL algorithm. them USED-FOR DRL algorithm. DRL algorithm USED-FOR learning of the game. them USED-FOR auxiliary reward function. electroencephalogram ( EEG ) cap USED-FOR error - potentials. definition USED-FOR game. frameworks USED-FOR error - potential based feedback system. DRL USED-FOR error - potential based feedback system. implicit human feedback USED-FOR complex environments ( games ). synthetic and real user experiments EVALUATE-FOR approach. OtherScientificTerm are human feedback, non - expert humans, human ’s intrinsic reactions, event - related electric potentials, and Atari - games. Generic is paradigm. Method is RL agent. ","This paper proposes a new DRL algorithm based on implicit human feedback for RL tasks where the agent’s learning is limited to RL tasks with expert labeling and demonstrations. The system is based on a system that learns implicit human interactions between state-action pairs in an Atari-type environment, where the human feedback is a mixture of expert and non-expert humans. The authors propose a new paradigm where the RL agent is trained to learn to predict the future state of the environment from the past state, and the system learns to learn the implicit human inputs of the agent. The paper also proposes an auxiliary reward function that can be used to guide the learning of the game. The error-related event potentials, which are an extension of the error-based feedback system in DRL, are learned by using the electroencephalogram (EEG) cap, and they are used to train the DRL algorithms.  The paper provides a definition for the game, and shows that the game can be learned using these two frameworks. They also provide some theoretical guarantees for the performance of the proposed DRL as well as a theoretical justification for the use of them.   Finally, the paper shows that their approach can be applied to a variety of complex environments (games) where the implicit humans feedback can be useful for the agent's learning. They show that the proposed approach performs well in both synthetic and real user experiments.","This paper proposes an implicit human feedback for the DRL algorithm for RL tasks. The system is based on the idea of using the system to learn implicit humans feedback for state-action pairs in an Atari-type environment. The paper introduces a new paradigm for learning the RL agent’s learning in the presence of non-expert humans and demonstrations. The main idea of the paper is to use the error-related event potentials, which are a set of state-actions pairs in the environment, and to use them as an auxiliary reward function to guide the learning of the game. The key idea of this paper is that the system is able to learn to learn the human ’s intrinsic reactions to the environment and the event-related electric potentials. The authors propose a new definition for the game and propose two frameworks for learning an error-potential based feedback system. The proposed approach is evaluated on both synthetic and real user experiments. "
21816,SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"laconic classification USED-FOR diverse image classifiers. classifier USED-FOR approximate minimal - entropy positive image. classifier USED-FOR approximate minimal - entropy positive image. colour reduction CONJUNCTION resolution reduction. resolution reduction CONJUNCTION colour reduction. reductions USED-FOR classification. crop CONJUNCTION colour reduction. colour reduction CONJUNCTION crop. crop HYPONYM-OF reductions. resolution reduction HYPONYM-OF reductions. colour reduction HYPONYM-OF reductions. complementary frameworks USED-FOR minimal - entropy positive images. cropping CONJUNCTION reduced colour. reduced colour CONJUNCTION cropping. texture bias FEATURE-OF ILSVRC - trained models. minimal - entropy positive images USED-FOR human and machine classifiers. complementary frameworks USED-FOR human and machine classifiers. reduced resolution FEATURE-OF humans. machines CONJUNCTION reduced resolution. reduced resolution CONJUNCTION machines. cropping USED-FOR machines. reduced colour USED-FOR machines. Metric are entropy, and precision. Material is ILSVRC test - set. Method are machine classifiers, and machine models. ",This paper studies the problem of laconic classification for diverse image classifiers. The authors propose two new reductions for classification: crop and colour reduction. The proposed reductions are based on complementary frameworks for learning the approximate minimal-entropy positive image from a classifier trained on the ILSVRC test-set. Theoretical results show that these two reductions are able to improve the performance of machine classifiers and reduce the texture bias of ILSVARC-trained models. ,"This paper proposes a new method for laconic classification for diverse image classifiers. The key idea is to use a classifier to generate an approximate minimal-entropy positive image for each classifier, which is then used as input to the ILSVRC test-set. The proposed method is based on two complementary frameworks: crop and colour reduction. The paper shows that these two reductions can be used for classification, as well as other reductions such as cropping, resolution reduction, etc. The authors also show that the proposed method can be applied to machine classifiers, and that it can be combined with other complementary frameworks to improve the performance of minimal-Entropy positive images for both human and machine classifier. "
21825,SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,"defenses USED-FOR Convolutional Neural Networks. instability assumption USED-FOR defense techniques. deterministic lossy compression algorithms CONJUNCTION randomized perturbations. randomized perturbations CONJUNCTION deterministic lossy compression algorithms. robustness EVALUATE-FOR randomized perturbations. deterministic lossy compression algorithms PART-OF defenses. randomized perturbations PART-OF defenses. Material is adversarial examples. OtherScientificTerm are small perturbations, and decision space. Method is perturbation defenses. ",This paper studies the problem of perturbation defenses for Convolutional Neural Networks. The authors propose a new instability assumption for defense techniques based on the instability assumption in the decision space. They show that deterministic lossy compression algorithms and randomized perturbations are robust against perturbated adversarial examples. ,This paper proposes a new defense against adversarial examples. The main idea is to use the instability assumption in the defense techniques. The authors propose two defenses: deterministic lossy compression algorithms and randomized perturbations. They show that the perturbation defenses are robust to adversarial attacks. 
21834,SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"view prediction HYPONYM-OF prediction tasks. view prediction USED-FOR 3D visual recognition. moving camera USED-FOR 2.5D ( color and depth ) video streams. 2.5D ( color and depth ) video streams USED-FOR neural 3D mapping networks. contrastive prediction losses COMPARE color regression loss. color regression loss COMPARE contrastive prediction losses. visual representations USED-FOR semi - supervised learning of 3D object detectors. model USED-FOR visual representations. visual representations USED-FOR unsupervised learning of 3D moving object detectors. model USED-FOR unsupervised learning of 3D moving object detectors. semi - supervised learning of 3D object detectors CONJUNCTION unsupervised learning of 3D moving object detectors. unsupervised learning of 3D moving object detectors CONJUNCTION semi - supervised learning of 3D object detectors. videos of dynamic scenes FEATURE-OF motion of the inferred 3D feature maps. scalable self - supervised task USED-FOR 3D object detection. view prediction USED-FOR 3D object detection. view prediction HYPONYM-OF scalable self - supervised task. Method is Predictive coding theories. Generic are task, and them. OtherScientificTerm are perception, retinas, and 3D feature maps. Material is complex photorealistic data. ","This paper studies the problem of view prediction in 3D visual recognition. The authors propose a scalable self-supervised task for 3D object detection based on view prediction, where the goal is to predict the position of objects in a scene from a moving camera. The proposed task is based on Predictive coding theories, and the authors show that contrastive prediction losses are better than color regression loss. They also show that the proposed model is able to learn visual representations for unsupervised learning of 3D moving object detectors and semi-supervision learning of threeD object detectors. ","This paper proposes a new task of view prediction for 3D visual recognition. The task is based on the prediction of the 3D feature maps of a moving camera. The authors propose to use 2.5D (color and depth) video streams as input to neural 3D mapping networks. The model is trained to predict the visual representations of 3D object detectors using visual representations, and the contrastive prediction losses are compared to the color regression loss. Predictive coding theories are used to train the model to predict 3D moving object detectors and unsupervised learning of three different types of object detectors. Experiments are performed on complex photorealistic data. The results show that the proposed scalable self-supervised task is able to perform better than the state-of-the-art on 3D objects in the videos of dynamic scenes."
21843,SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"model USED-FOR applications. Optimal Transport ( OT ) framework USED-FOR UDT. low energy transformations FEATURE-OF mappings. methods USED-FOR mappings. theoretical guarantees EVALUATE-FOR methods. approach USED-FOR UDT. dynamic formulation of OT CONJUNCTION CycleGAN. CycleGAN CONJUNCTION dynamic formulation of OT. mapping USED-FOR domain translation. translation USED-FOR problems. image captioning CONJUNCTION natural language translation. natural language translation CONJUNCTION image captioning. neural network USED-FOR mapping. hidden layers PART-OF neural network. photographs CONJUNCTION paintings. paintings CONJUNCTION photographs. model USED-FOR task. dynamical formulation USED-FOR model. Optimal Transport theory USED-FOR UDT models. model COMPARE CycleGAN - like models. CycleGAN - like models COMPARE model. Task are Unsupervised Domain Translation ( UDT ), UDT problems, and UDT problem. OtherScientificTerm are implicit biases, implicit bias, map, unwanted pairings, objective function, and well - behaved mappings. Generic are approaches, and models. Method are CycleGAN model, and networks of minimal complexity. Metric is complexity. ","This paper studies the problem of Unsupervised Domain Translation (UDT) in the context of Optimal Transport (OT) framework. The authors propose a model for UDT that can be applied to a variety of applications, including image captioning, natural language translation, and domain translation. The model is based on a dynamical formulation of OT and CycleGAN. The proposed model is able to perform well on the task of domain translation and achieves state-of-the-art theoretical guarantees. ","This paper proposes a model for unsupervised Domain Translation (UDT) that can be applied to a variety of applications. The model is based on the Optimal Transport (OT) framework for UDT, which is a dynamic formulation of OT and CycleGAN. The key idea of the model is to learn a mapping of the input domain to the target domain. The mapping is then used to solve a set of UDT problems (e.g., image captioning, natural language translation, etc.). The mapping can be used to train a neural network to solve the mapping. The proposed model is compared to CycleGAN-like models, and is shown to outperform CycleGAN model in terms of complexity. "
21852,SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,regularization method USED-FOR neural networks. RotationOut USED-FOR neural networks. RotationOut HYPONYM-OF regularization method. Dropout USED-FOR neuron / channel. Dropout COMPARE RotationOut. RotationOut COMPARE Dropout. convolutional layers CONJUNCTION recurrent layers. recurrent layers CONJUNCTION convolutional layers. RotationOut USED-FOR recurrent layers. RotationOut USED-FOR convolutional layers. RotationOut USED-FOR co - adaptation reduction. Dropout USED-FOR co - adaptation reduction. RotationOut CONJUNCTION Dropout. Dropout CONJUNCTION RotationOut. noise analysis method USED-FOR co - adaptation reduction. RotationOut / Dropout CONJUNCTION Batch Normalization. Batch Normalization CONJUNCTION RotationOut / Dropout. vision and language tasks EVALUATE-FOR method. RotationOut CONJUNCTION RotationOut. RotationOut CONJUNCTION RotationOut. Method is regularization. ,"This paper proposes a new regularization method called RotationOut for neural networks, which is based on Dropout. Dropout replaces the neuron/channel in the neuron with a recurrent layer, and the convolutional layers are replaced with recurrent layers. The authors show that Rotation out outperforms Dropout on vision and language tasks. They also provide a noise analysis method for co-adaptation reduction using Dropout as well as Rotation Out. ","This paper proposes a regularization method for neural networks called RotationOut, which is a variant of Dropout. The idea is to regularize the neuron/channel in the same way as Dropout, but with the addition of convolutional layers instead of recurrent layers. The authors also propose a noise analysis method for co-adaptation reduction. The proposed method is evaluated on vision and language tasks, and compared to Rotation out and Dropout in terms of Rotation Out/Dropout and Batch Normalization."
21861,SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"method USED-FOR Universal Adversarial Perturbations ( UAP ). Universal Adversarial Perturbations ( UAP ) USED-FOR CNN. sequential optimization USED-FOR adversarial perturbation. dilate loss USED-FOR sequential optimization. dilate loss USED-FOR adversarial perturbation. Euclidean norm EVALUATE-FOR Dilate loss. method COMPARE data - free work. data - free work COMPARE method. fooling rate EVALUATE-FOR data - free work. fooling rate EVALUATE-FOR method. Method is Data - free approaches. Task are crafting adversaries, adversary generation, and crafting UAPs. OtherScientificTerm are nonlinearity, perturbation, ReLU activation function, and limited data cases. ","This paper proposes a method to improve the performance of Universal Adversarial Perturbations (UAP) in CNN by using sequential optimization for adversarial perturbation. Dilate loss is a dilate loss for sequential optimization, which is a regularization of the Euclidean norm. The authors show that the proposed method has a better fooling rate than data-free work. They also show that their method is more robust to crafting adversaries than Data-free approaches. ",This paper proposes a method for learning Universal Adversarial Perturbations (UAP) for CNN. The authors propose a dilate loss for adversarial perturbation based on sequential optimization. The Dilate loss is based on the Euclidean norm. The paper shows that the proposed method is more robust than data-free work in terms of fooling rate compared to other Data-free approaches. The main contribution of the paper is to propose a method to learn crafting adversaries that are robust to adversarial generation. This is done by learning a ReLU activation function that is independent of the number of adversarial examples and the perturbations. The method is shown to be more robust to crafting UAPs in limited data cases.
21870,SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"Neural Architecture Search ( NAS ) COMPARE hand - designed networks. hand - designed networks COMPARE Neural Architecture Search ( NAS ). Neural Architecture Search ( NAS ) USED-FOR artificial intelligence areas. architecture USED-FOR task. NAS CONJUNCTION fast adaptation of neural architectures. fast adaptation of neural architectures CONJUNCTION NAS. T - NAS HYPONYM-OF Transferable Neural Architecture Search method. meta - learning USED-FOR Transferable Neural Architecture Search method. architecture USED-FOR task. T - NAS USED-FOR meta - architecture. few - shot learning CONJUNCTION supervised learning. supervised learning CONJUNCTION few - shot learning. supervised learning EVALUATE-FOR T - NAS. few - shot learning EVALUATE-FOR T - NAS. Method are NAS methods, and neural architectures. Metric is searching cost. Generic is method. ","This paper proposes a new transferable neural architecture search method, called T-NAS, which can be applied to both few-shot learning and supervised learning tasks. The main idea is to learn a meta-architecture for each task and then use the meta-learning to find the best architecture for the task. The proposed method is evaluated on a variety of tasks and compared to other NAS methods. The results show that the proposed method outperforms the state of the art.","This paper proposes a new architecture for artificial intelligence areas, called Neural Architecture Search (NAS), which is an extension of hand-designed networks. The main difference between NAS methods is that NAS allows for fast adaptation of neural architectures to new tasks. The authors propose a new meta-learning method, called T-NAS, which is based on the T-Nash architecture. The proposed method is evaluated on few-shot learning and supervised learning. The results show that the proposed method outperforms other NAS methods."
21879,SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"variational information bottleneck ( VIB ) CONJUNCTION noise regularized learning. noise regularized learning CONJUNCTION variational information bottleneck ( VIB ). dropout CONJUNCTION Bayesian neural networks. Bayesian neural networks CONJUNCTION dropout. Bayesian neural networks CONJUNCTION variational information bottleneck ( VIB ). variational information bottleneck ( VIB ) CONJUNCTION Bayesian neural networks. noise regularized learning HYPONYM-OF paradigms. dropout HYPONYM-OF paradigms. Bayesian neural networks HYPONYM-OF paradigms. variational information bottleneck ( VIB ) HYPONYM-OF paradigms. network compression CONJUNCTION robustness. robustness CONJUNCTION network compression. generalization CONJUNCTION network compression. network compression CONJUNCTION generalization. adversarial attack CONJUNCTION label noise. label noise CONJUNCTION adversarial attack. robustness FEATURE-OF adversarial attack. activation uncertainty CONJUNCTION activation variability. activation variability CONJUNCTION activation uncertainty. pruning CONJUNCTION adversarial defense. adversarial defense CONJUNCTION pruning. SNNs COMPARE SE - SNN. SE - SNN COMPARE SNNs. adversarial defense CONJUNCTION learning with label noise. learning with label noise CONJUNCTION adversarial defense. network compression EVALUATE-FOR SE - SNN. adversarial defense USED-FOR network compression. pruning USED-FOR network compression. Method are Stochastic neural networks ( SNNs ), and neural network variants. Generic is networks. Task is discriminative learning. ","This paper studies the problem of training Stochastic neural networks (SNNs) with label noise. The authors consider a variety of paradigms, including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. They show that SNNs are more robust than SE-SNN in terms of generalization, robustness, network compression, and adversarial defense against adversarial attack. They also show that pruning improves the performance of the network compression by a significant margin. ","This paper proposes a new way to study Stochastic neural networks (SNNs), which are a family of networks that can be used for discriminative learning. The authors propose three paradigms: variational information bottleneck (VIB), noise regularized learning, and dropout. They show that SNNs outperform SE-SNN in terms of robustness, generalization, and network compression. They also show that adversarial defense against adversarial attack and learning with label noise can be combined with network compression and pruning. "
21888,SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"inner loop USED-FOR reinforcement learning. curiosity mechanisms USED-FOR agent ’s reward signal. meta - learning USED-FOR generating curious behavior. reward signal USED-FOR reinforcement learning. reward signal USED-FOR inner loop. transferring neural network weights USED-FOR meta - RL methods. nearest - neighbor modules CONJUNCTION custom loss functions. custom loss functions CONJUNCTION nearest - neighbor modules. buffers CONJUNCTION nearest - neighbor modules. nearest - neighbor modules CONJUNCTION buffers. neural networks PART-OF rich language of programs. custom loss functions PART-OF rich language of programs. image inputs CONJUNCTION acrobot. acrobot CONJUNCTION image inputs. curiosity algorithms COMPARE human - designed published curiosity algorithms. human - designed published curiosity algorithms COMPARE curiosity algorithms. acrobot CONJUNCTION ant. ant CONJUNCTION acrobot. grid navigation CONJUNCTION acrobot. acrobot CONJUNCTION grid navigation. image inputs USED-FOR grid navigation. OtherScientificTerm are curiosity, and outer loop. Method are evolution, and meta - learn algorithms. Material is ML papers. Generic is approach. ","This paper proposes a new inner loop for reinforcement learning based on the reward signal of an agent’s reward signal. The reward signal is learned by meta-learning, where the agent is trained to generate curious behavior by using curiosity mechanisms. The inner loop is trained by transferring neural network weights from the outer loop to the inner loop. The authors show that this approach can be applied to a variety of ML papers, and that it can be used to improve the performance of existing meta-RL methods.  The authors propose a rich language of programs, which is composed of neural networks with buffers, nearest-neighbor modules, and custom loss functions. They show that the proposed curiosity algorithms perform better than human-designed published curiosity algorithms, and are able to perform grid navigation, acrobot, and ant. ","The paper proposes a meta-learning approach for generating curious behavior in reinforcement learning. The idea is to use curiosity mechanisms in the agent’s reward signal to guide the evolution of the reward signal in the inner loop of reinforcement learning, and to use meta-RL methods for transferring neural network weights to the outer loop. The approach is evaluated on a variety of ML papers. The authors propose a rich language of programs, which consists of neural networks with neural networks that are able to generate a rich representation of the environment, with buffers, nearest-neighbor modules, and custom loss functions. The experiments show that the proposed curiosity algorithms perform better than human-designed published curiosity algorithms, and outperform other meta-learn algorithms in grid navigation and acrobot."
21897,SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"approach USED-FOR AnyC2C. approach USED-FOR code snippet. strict syntax of programming languages USED-FOR code snippet. tree – structural language modeling ( SLM ) USED-FOR code snippet. strict syntax of programming languages USED-FOR approach. neural model USED-FOR conditional probabilities. AST paths USED-FOR neural model. structural techniques COMPARE approach. approach COMPARE structural techniques. structural techniques USED-FOR expressions. structured approaches USED-FOR Java and C # code. model COMPARE seq2seq. seq2seq COMPARE model. model COMPARE structured approaches. structured approaches COMPARE model. model USED-FOR Java and C # code. seq2seq CONJUNCTION structured approaches. structured approaches CONJUNCTION seq2seq. Generic are problem, it, and task. OtherScientificTerm are structural information, and programming language. Method is SLM. ",This paper proposes a new approach for AnyC2C. The proposed approach is based on tree-structural language modeling (SLM) to generate a code snippet from a strict syntax of programming languages. The authors propose a neural model to predict conditional probabilities using AST paths. The model is evaluated on both Java and C# code and compared to seq2seq and other structured approaches. The results show that the proposed approach performs better than other structural techniques for generating expressions. ,"This paper proposes a new approach for AnyC2C. The approach is based on tree-structural language modeling (SLM) to generate a code snippet from the strict syntax of programming languages. The problem is formulated as an optimization problem, where the goal is to predict the conditional probabilities of a given code snippet given the structural information. The authors propose a neural model to predict conditional probabilities using AST paths. The model is evaluated on Java and C # code, and compared to seq2seq and other structured approaches to generate expressions. The proposed approach is compared to other structural techniques to generate the expressions. "
21906,SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"gradient descent methods USED-FOR non - convex optimization problems. objective functions USED-FOR NNs. NN model space CONJUNCTION canonical space. canonical space CONJUNCTION NN model space. disparity matrix HYPONYM-OF pointwise linear transformation. pointwise linear transformation USED-FOR gradients. gradient descent methods USED-FOR global minimum of zero loss. full rank FEATURE-OF disparity matrices. learning of NNs COMPARE normal convex optimization. normal convex optimization COMPARE learning of NNs. gradient decent algorithms USED-FOR global minimum of zero loss. Method are large - scale neural networks ( NN ), large NNs, and over - parameterized NNs. OtherScientificTerm are canonical model space, full - rank condition, and singular disparity matrices. ","This paper studies gradient descent methods for non-convex optimization problems with large-scale neural networks (NN). The authors consider the case where the NN model space and canonical space are large NNs, and the objective functions for NNs are over-parameterized. The authors propose a pointwise linear transformation of the gradients, called the disparity matrix, which is a full-rank condition on the full rank of the gradient. They show that gradient decent algorithms are able to achieve a global minimum of zero loss under the singular disparity matrices. They also show that the learning of NNs is better than normal convex optimization.","This paper studies gradient descent methods for non-convex optimization problems with large-scale neural networks (NN). The authors consider the case of large NNs, where the objective functions of the NNs are large and the canonical model space is large. The authors propose a pointwise linear transformation of the gradients into a disparity matrix, called the full-rank condition. They show that the singular disparity matrices of the full rank of the gradient decent algorithms can be used to compute the global minimum of zero loss. The learning of NNs is compared to normal convex optimization, and over-parameterized NNs."
21915,SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"Large - scale ground truth data sets USED-FOR deep learning based segmentation models. interactive graph - based segmentation algorithms USED-FOR connectivity. instanceaware heuristic USED-FOR discrete Potts model. feature maps USED-FOR DCNN. feature maps USED-FOR algorithms. RGB USED-FOR algorithms. PASCAL VOC 2012 CONJUNCTION Cityscapes dataset. Cityscapes dataset CONJUNCTION PASCAL VOC 2012. Cityscapes dataset EVALUATE-FOR semantic ( and panoptic ) segmentation. PASCAL VOC 2012 EVALUATE-FOR semantic ( and panoptic ) segmentation. VOC validation set EVALUATE-FOR mIoU. mIoU EVALUATE-FOR interactive approach. VOC validation set EVALUATE-FOR interactive approach. They USED-FOR interactive annotation. They USED-FOR weakly supervised learning framework. OtherScientificTerm are global optimum, and scribbles. ","This paper studies the problem of deep learning based segmentation models on large-scale ground truth data sets. The authors propose interactive graph-based segmentation algorithms to improve the connectivity between nodes in the graph. The proposed algorithms are based on RGB, where the global optimum is defined by a discrete Potts model with an instanceaware heuristic. These algorithms are trained using feature maps generated by DCNN. They are then used for interactive annotation. The experimental results on PASCAL VOC 2012 and Cityscapes dataset demonstrate the effectiveness of the interactive approach on semantic (and panoptic) segmentation on the VOC validation set and mIoU. They also provide a weakly supervised learning framework. ","This paper proposes a novel way to learn deep learning based segmentation models on large-scale ground truth data sets. The authors propose to use interactive graph-based segmentation algorithms to improve connectivity between nodes in the graph. The key idea is to use an instanceaware heuristic to learn a discrete Potts model. The algorithms are based on RGB, and the authors show that the proposed algorithms can be trained using feature maps generated by DCNN. They also show that they can be used in a weakly supervised learning framework. They show that their interactive approach can be evaluated on the PASCAL VOC 2012 and Cityscapes dataset for semantic (and panoptic) segmentation, and on the VOC validation set for mIoU for interactive annotation. "
21924,SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"salient features FEATURE-OF image. saliency tools USED-FOR adversarial examples. salient features USED-FOR defense. it COMPARE baseline. baseline COMPARE it. gradient - based saliency tools USED-FOR adversarial defense. model USED-FOR baseline. saliency map USED-FOR baseline. learnt saliency models USED-FOR saliency. computational cost EVALUATE-FOR learnt saliency models. saliency models USED-FOR real - time defense. learnt saliency model USED-FOR defense. adversarial images CONJUNCTION natural images. natural images CONJUNCTION adversarial images. CNN USED-FOR adversarial images. CNN USED-FOR natural images. CNN HYPONYM-OF defense. salient pixels USED-FOR CNN. CIFAR-10 CONJUNCTION ASSIRA. ASSIRA CONJUNCTION CIFAR-10. defense USED-FOR adversarial attacks. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. MNIST EVALUATE-FOR defense. ASSIRA EVALUATE-FOR defense. CIFAR-10 EVALUATE-FOR defense. saliency map USED-FOR defense. C&W CONJUNCTION DeepFool. DeepFool CONJUNCTION C&W. weak defenses USED-FOR adversarial images. attacks USED-FOR adversarial images. DeepFool USED-FOR adversarial images. DeepFool HYPONYM-OF attacks. C&W HYPONYM-OF attacks. OtherScientificTerm are Adversarial perturbations, misclassification, and adversarial perturbations. ","This paper proposes a new adversarial defense based on gradient-based saliency tools for adversarial examples. Adversarial perturbations are defined as the difference between the salient features of an image and the salient features of a CNN. The authors show that the learned saliency model can be used to improve the performance of the baseline by using the saliency map from the learned model as a baseline. The proposed defense is evaluated on MNIST, CIFAR-10, and ASSIRA and shows that the learnt saliency models can improve the computational cost of real-time defense. ","This paper proposes a new adversarial defense method based on gradient-based saliency tools. The proposed method is based on the idea that adversarial perturbations can be used to improve the robustness of an image against adversarial examples. Adversarial examples are generated from a set of saliency examples, and the saliency features of the image are used as input to the adversarial example. The adversarial images are generated by CNN, which is a CNN trained on the salient pixels in the image. The authors show that the proposed method outperforms the baseline on MNIST, CIFAR-10, and ASSIRA. The paper also shows that the learned saliency model can be applied to real-time defense, and that it is more robust than the baseline. "
21933,SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"global adversarial robustness guarantees FEATURE-OF machine learning models. measurability FEATURE-OF local robustness properties. concentration inequalities USED-FOR global robustness. Fashion - MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION Fashion - MNIST. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. neural networks architectures CONJUNCTION training methods. training methods CONJUNCTION neural networks architectures. robustness / accuracy trade - off EVALUATE-FOR neural networks architectures. training methods USED-FOR MNIST. training methods USED-FOR Fashion - MNIST. robustness EVALUATE-FOR networks. accuracy EVALUATE-FOR networks. robustness CONJUNCTION accuracy. accuracy CONJUNCTION robustness. stochastic gradient descent CONJUNCTION iterative pruning techniques. iterative pruning techniques CONJUNCTION stochastic gradient descent. Bayesian settings FEATURE-OF them. iterative pruning techniques USED-FOR networks. stochastic gradient descent USED-FOR networks. Generic are model, and methods. OtherScientificTerm are adversarial attacks, and estimation error. ","This paper studies the global adversarial robustness guarantees of machine learning models. The authors consider the problem of local robustness properties of local models, i.e., their measurability in the presence of adversarial attacks. They show that the global robustness of a model is determined by concentration inequalities, and that the concentration inequalities can be used to estimate the global perturbations of the model. They then propose two training methods for MNIST, Fashion-MNIST, and CIFAR, and show that these methods improve the robustness/accuracy trade-off of these neural networks architectures and training methods. They also show that their robustness and accuracy trade-offs can be improved by stochastic gradient descent and iterative pruning techniques. ","This paper studies the global adversarial robustness guarantees of machine learning models. The authors show that the local robustness properties of the model are correlated with the concentration inequalities of the global robustness. They also show the correlation between the robustness/accuracy trade-off of different neural networks architectures and different training methods for MNIST, Fashion-MNIST, CIFAR, and MNIST. They show that these methods are robust to adversarial attacks in Bayesian settings, and they also show that their robustness and accuracy trade-offs are correlated. Finally, they show that iterative pruning techniques and stochastic gradient descent can improve the networks' robustness, accuracy, and accuracy."
21942,SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"robustness FEATURE-OF environmental dynamics. learning algorithms USED-FOR robustness. robustness FEATURE-OF system dynamics. transition probability HYPONYM-OF system dynamics. Wasserstein distance USED-FOR disturbance. infinite - dimensional optimization problem CONJUNCTION finite - dimensional risk - aware problem. finite - dimensional risk - aware problem CONJUNCTION infinite - dimensional optimization problem. risk - aware optimal Bellman equation USED-FOR optimal robust policies. sensitivity analysis USED-FOR perturbations. Wasserstein Robust HYPONYM-OF robust learning algorithm. Cart - Pole environment EVALUATE-FOR algorithm. OtherScientificTerm are optimal policy, simulated environmental parameters, reference transition kernel, transition kernel disturbance, and state disturbance. ","This paper studies the problem of robustness to perturbations in a system dynamics, i.e., the transition probability of the system dynamics. The authors propose a new robust learning algorithm, Wasserstein Robust, which is based on the risk-aware optimal Bellman equation. They show that the optimal policy can be obtained by minimizing the distance between the reference transition kernel and the transition kernel disturbance. They also provide a sensitivity analysis for the perturbation. The proposed algorithm is evaluated on the Cart-Pole environment.","This paper studies the robustness of learning algorithms to environmental dynamics. The authors propose a robust learning algorithm called Wasserstein Robust, which is based on the risk-aware optimal Bellman equation. They show that the optimal policy is robust to perturbations of the simulated environmental parameters, and that the transition probability of the system dynamics (e.g., transition probability, transition probability) is a function of the transition kernel disturbance. They prove that the disturbance is caused by a perturbation of the reference transition kernel. They also show that this disturbance can be minimized by minimizing the transition distance between the state disturbance and the target state. They provide a sensitivity analysis to show that perturbings can be reduced by a small amount. They then show that their algorithm can be applied to the Cart-Pole environment. "
21951,SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"Nash equilibrium PART-OF multi - player games. deep learning based approaches USED-FOR pure strategy Nash equilibrium. method USED-FOR mixed strategy Nash equilibria. multi - player continuous games FEATURE-OF mixed strategy Nash equilibria. pure ones PART-OF method. pushforward measure technique USED-FOR mixed strategy. continuous spaces FEATURE-OF mixed strategy. joint strategy profile CONJUNCTION Nash equilibrium. Nash equilibrium CONJUNCTION joint strategy profile. gradient descent algorithm USED-FOR approach. convexity assumption FEATURE-OF payoff functions. approach USED-FOR stationary Nash equilibrium. blotto games CONJUNCTION GAMUT games. GAMUT games CONJUNCTION blotto games. method COMPARE works. works COMPARE method. quadratic games CONJUNCTION blotto games. blotto games CONJUNCTION quadratic games. works USED-FOR Nash equilibrium. method USED-FOR Nash equilibrium. approximating Nash equilibrium USED-FOR quadratic games. approximating Nash equilibrium CONJUNCTION blotto games. blotto games CONJUNCTION approximating Nash equilibrium. GAMUT games EVALUATE-FOR method. OtherScientificTerm are continuous strategy spaces, and pure strategy weakness. Task is generative adversarial networks. Generic is equilibrium. ","This paper studies the problem of learning Nash equilibria in multi-player continuous games. The authors propose a new method for learning mixed strategy Nash equilibrium in multi -player games, where the goal is to find a Nash equilibrium that is close to the true Nash equilibrium. The main idea is to use a pushforward measure technique to learn a mixed strategy in continuous spaces, and then use a gradient descent algorithm to find the Nash equilibrium using the joint strategy profile and Nash equilibrium with the pure ones. The proposed method is evaluated on a variety of games, including quadratic games, blotto games, and GAMUT games, showing that the proposed method achieves better performance than previous works on Nash equilibrium, and is also able to achieve a stationary Nash equilibrium when the payoff functions have a convexity assumption.","This paper proposes a new method for learning mixed strategy Nash equilibria in multi-player continuous games. The authors propose to use deep learning based approaches to learn pure strategy Nash equilibrium. The main idea is to use a pushforward measure technique to learn a mixed strategy in continuous spaces, where the joint strategy profile and the Nash equilibrium are defined. The paper also proposes a convexity assumption on the payoff functions, which is an extension of the approach for stationary Nash equilibrium, and a gradient descent algorithm to learn the equilibrium. Experiments on quadratic games, blotto games, and GAMUT games show that the proposed method outperforms existing works on Nash equilibrium and approximating Nash equilibrium in the continuous strategy spaces. "
21960,SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"deep neural networks USED-FOR NLP tasks. labeled data USED-FOR data - hungry models. sufficient domain knowledge USED-FOR labeled data. supervision USED-FOR sufficient domain knowledge. supervision FEATURE-OF Natural language ( NL ) explanations. them USED-FOR augmenting model learning. modularized model USED-FOR semantics. linguistic variants FEATURE-OF NL explanations. Neural Execution Tree ( NExT ) framework1 USED-FOR text classification. NL explanations USED-FOR Neural Execution Tree ( NExT ) framework1. NExT USED-FOR actions. NL explanations USED-FOR executable logical forms. logical forms USED-FOR actions. semantic parsing USED-FOR NL explanations. semantic parsing USED-FOR executable logical forms. relation extraction CONJUNCTION sentiment analysis. sentiment analysis CONJUNCTION relation extraction. NLP tasks EVALUATE-FOR baseline methods. sentiment analysis HYPONYM-OF NLP tasks. relation extraction HYPONYM-OF NLP tasks. Task are data annotation, and multi - hop question answering. Metric is annotation time. Method is model learning. OtherScientificTerm is NL explanation. ","This paper studies the problem of training deep neural networks to perform NLP tasks on labeled data. The authors propose a novel Neural Execution Tree (NExT) framework1 for text classification, which uses NL explanations to generate executable logical forms from NL explanations. The NExT is based on a modularized model that learns the semantics of the NL explanation, and then uses them for augmenting model learning. The proposed method is evaluated on a variety of tasks, including relation extraction, sentiment analysis, and multi-hop question answering.","This paper proposes a novel approach to augmenting deep neural networks for NLP tasks with natural language (NL) explanations. The idea is to use sufficient domain knowledge to train data-hungry models on labeled data. The authors propose a modularized model to learn the semantics of NL explanations from linguistic variants, and then use them for augmenting model learning. The proposed Neural Execution Tree (NExT) framework1 is used for text classification and multi-hop question answering. NExT is able to learn actions from NL explanations, including semantic parsing, relation extraction, and sentiment analysis. Experiments show that the proposed baseline methods outperform other baseline methods on a variety of tasks, including data annotation, sentiment analysis, and relation extraction. "
21969,SP:a9b5f7257dedd719cfe341fca275776734af1d98,"robustness FEATURE-OF misclassification. machine learning models USED-FOR Formal verification. robustness HYPONYM-OF properties. it USED-FOR complex specifications. it USED-FOR recurrent neural network architectures. recurrent neural network architectures CONJUNCTION complex specifications. complex specifications CONJUNCTION recurrent neural network architectures. specifications USED-FOR temporal properties. adversarial robustness FEATURE-OF complex specifications. it USED-FOR verified training. specifications HYPONYM-OF complex specifications. verified training method USED-FOR models. verified training method USED-FOR models. training USED-FOR models. OtherScientificTerm are perturbations of the input features, and desired specifications. Method are verification procedure, verifiably robust models, and language model. ","This paper studies the robustness of machine learning models against misclassification in the context of Formal verification. The authors propose a verification procedure to verify whether a language model is robust to perturbations of the input features. The proposed method is based on a verified training method, and it can be applied to both recurrent neural network architectures and complex specifications such as the specifications for temporal properties and adversarial robustness. Experiments show that the proposed training method improves the performance of the models in terms of robustness and robustness to misclassifications.","This paper studies the robustness of misclassification in machine learning models for Formal verification. The authors propose a verification procedure where perturbations of the input features are added to the output of the language model, and the desired specifications of the model are updated. They show that it is robust to complex specifications such as recurrent neural network architectures and complex specifications with temporal properties such as adversarial robustness. They also propose a verified training method for models trained with the proposed verified training. "
21978,SP:3903680e07b676409e3cf6a1044b67291fe38630,"learned state representations USED-FOR constant. technique COMPARE domain randomization. domain randomization COMPARE technique. generalization scores EVALUATE-FOR domain randomization. generalization scores EVALUATE-FOR technique. Task are reinforcement learning, and visual domain randomization problem. Generic is method. Method are visual domain randomization, and regularization method. OtherScientificTerm are policies, and randomization parameters. ","This paper studies the problem of reinforcement learning in the context of visual domain randomization, where the goal is to learn a constant of the learned state representations. The authors propose a new method to solve this problem. The proposed technique is shown to achieve better generalization scores than the state-of-the-art in terms of the number of policies and the randomization parameters. ",This paper proposes a new regularization method for the visual domain randomization problem. The main idea is to use learned state representations as a constant for the constant of the learned policies. The authors show that the proposed method can achieve better generalization scores than the existing technique for the domain randomized problem. They also show that their method can be applied to reinforcement learning.
21987,SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"deep learning USED-FOR Deep metric learning ( DML ). complicated losses CONJUNCTION hard example mining methods. hard example mining methods CONJUNCTION complicated losses. pairwise binary classification problem USED-FOR DML. framework USED-FOR model. distributionally robust optimization USED-FOR robust loss. uncertainty decision set FEATURE-OF dual variable. uncertainty decision set USED-FOR complicated losses. benchmark data sets EVALUATE-FOR method. method COMPARE state. state COMPARE method. Task is computer vision. Generic are problem, and variants. OtherScientificTerm is imbalanced data pairs. ","This paper studies the problem of deep learning for Deep metric learning (DML). The authors consider the pairwise binary classification problem in DML, which is an important problem in computer vision. The authors propose a framework for training a model that is robust to imbalanced data pairs. The robust loss is based on distributionally robust optimization, where the uncertainty decision set of the dual variable is used to estimate the complexity of the complicated losses and hard example mining methods. The proposed method is evaluated on two benchmark data sets and shows that the proposed method performs better than the state of the art. ","This paper proposes a new framework for Deep metric learning (DML) that combines complicated losses with hard example mining methods. The authors propose a pairwise binary classification problem for DML, which is a challenging problem in computer vision. The model is based on the framework of distributionally robust optimization, where the model is trained on imbalanced data pairs. The proposed method is evaluated on two benchmark data sets, where it outperforms the state of the art on both variants of the problem. The main idea is to use the uncertainty decision set of the dual variable as a surrogate for the complicated losses."
21996,SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"local minimum PART-OF non - convex finite - sum minimization. inexact gradient and Hessian estimation USED-FOR trust region method. stochastic trust region ( STR ) algorithm USED-FOR (, √ ) -approximate local minimum. runtime complexity EVALUATE-FOR Hessian - free STR algorithms. Metric is convergence rate. Method are differential estimations, and Hessian estimator. OtherScientificTerm is stochastic Hessian oracle queries. Generic is algorithms. ","This paper proposes a new trust region method based on inexact gradient and Hessian estimation for non-convex finite-sum minimization with a local minimum. The main idea is to use a stochastic trust region (STR) algorithm to compute the (, √)-approximate local minimum for (, \epsilon, \mu) under the assumption of differential estimations. The authors show that the convergence rate of the Hessian estimator is O(1/\sqrt{T})$ and that the runtime complexity of Hessian-free STR algorithms is O(\sqrt{\T})$. The authors also provide some theoretical guarantees for the algorithms. ","This paper proposes a trust region method based on inexact gradient and Hessian estimation. The local minimum of the non-convex finite-sum minimization is computed using differential estimations. The (, √)-approximate local minimum is computed by a stochastic trust region (STR) algorithm. The convergence rate is given by the Hessian estimator. Experiments are conducted on stochy Hessian oracle queries. The authors show that Hessian-free STR algorithms have lower runtime complexity than other algorithms."
22005,SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"batch normalization CONJUNCTION weight initialization. weight initialization CONJUNCTION batch normalization. linear programming USED-FOR Farkas layers. benchmark datasets EVALUATE-FOR network sizes. ReLU activation USED-FOR residual networks. Method are deep neural networks, and geometrically motivated method. Generic is method. Metric is training capacity. OtherScientificTerm is initialization. ",This paper proposes a new method for training deep neural networks. The proposed method is based on a geometrically motivated method. The authors propose a linear programming for Farkas layers and a batch normalization and weight initialization. ReLU activation is used for residual networks. Experimental results on benchmark datasets show that the proposed network sizes are competitive with the state-of-the-art. ,This paper proposes a geometrically motivated method for training deep neural networks. The method is based on batch normalization and weight initialization. The Farkas layers are trained using linear programming. ReLU activation is used to train residual networks. Experiments are conducted on several benchmark datasets to show the network sizes and training capacity. 
22014,SP:1d325b148e3efe407241c1f1cbe8d17400499741,"decision boundary PART-OF classifier. adversarial examples FEATURE-OF robustness certificate. minimum distance FEATURE-OF robustness certificate. robustness certificates USED-FOR deep classifiers. nonconvex optimization USED-FOR it. computationally - efficient robustness certificates USED-FOR deep classifiers. differentiable activation functions USED-FOR computationally - efficient robustness certificates. eigenvalues FEATURE-OF Hessian. Hessian FEATURE-OF network. l2 norm FEATURE-OF robustness certificate. convex optimization USED-FOR robustness certificate. curvature FEATURE-OF deep network. computationallyefficient differentiable upper bound USED-FOR deep network. curvature FEATURE-OF computationallyefficient differentiable upper bound. curvature bound USED-FOR regularization term. regularization term USED-FOR network. curvature bound USED-FOR network. adversarial examples FEATURE-OF certified robustness. Curvature - based Robustness Certificate ( CRC ) CONJUNCTION Curvature - based Robust Training ( CRT ). Curvature - based Robust Training ( CRT ) CONJUNCTION Curvature - based Robustness Certificate ( CRC ). CRT COMPARE adversarial training. adversarial training COMPARE CRT. CRC COMPARE CROWN ’s certificate. CROWN ’s certificate COMPARE CRC. certified accuracy EVALUATE-FOR adversarial training. CRC COMPARE CRT. CRT COMPARE CRC. CRC COMPARE adversarial training. adversarial training COMPARE CRC. regularizer USED-FOR CRC. regularizer USED-FOR CROWN ’s certificate. certified accuracy EVALUATE-FOR CRC. certified accuracy EVALUATE-FOR CRT. OtherScientificTerm are lower bound, and classification output. ","This paper studies the problem of computing robustness certificates for deep classifiers with differentiable activation functions. The authors propose a new robustness certificate based on the minimum distance between the decision boundary of the classifier and the adversarial examples. They show that it is computationally-efficient differentiable upper bound for a deep network with curvature of the Hessian of the eigenvalues of the network with l2 norm. They also provide a lower bound for the classification output. The regularization term for the network is derived from the curvature bound of the deep network, and it is a nonconvex optimization.  The authors show that the regularizer improves the certified accuracy of CRT and CROWN’s certificate, and that the proposed Curvature-based Robustness Certificate (CRC) outperforms adversarial training in terms of certified accuracy.","This paper proposes a new robustness certificate for deep classifiers with a minimum distance between the decision boundary of the classifier and the lower bound of the classification output. The authors propose two computationally-efficient robustness certificates based on differentiable activation functions. The first one is based on convex optimization, while the second one uses nonconvex optimization. The main contribution of the paper is to show that it is computationally efficient differentiable upper bound for a deep network based on curvature of the eigenvalues of the Hessian of the network. The regularization term for the network is a curvature bound for the curvature in the network, and the regularizer for the CROWN’s certificate is a l2 norm. The paper shows that the proposed Curvature-based Robustness Certificate (CRC) outperforms CRT in terms of certified accuracy and adversarial training. "
22023,SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"method USED-FOR compressed sensing recovery. untrained deep generative models USED-FOR method. convolutional weights PART-OF network. Deep Image Prior ( DIP ) USED-FOR method. approach USED-FOR differentiable linear inverse problem. approaches COMPARE method. method COMPARE approaches. generative models USED-FOR approaches. prior information FEATURE-OF network weights. prior information PART-OF learned regularization technique. DIP optimization approach USED-FOR overparameterized single - layer networks. Method are unlearned methods, and early stopping. OtherScientificTerm are pre - training, and noisy measurements. Metric is reconstruction error. Task is fitting problem. ",This paper proposes a method for compressed sensing recovery from untrained deep generative models. The proposed method is based on Deep Image Prior (DIP) and uses convolutional weights in the network to learn the network weights. The learned regularization technique uses prior information in the learned network weights to reduce the reconstruction error. The authors show that the proposed method performs better than existing approaches on the fitting problem. They also provide a DIP optimization approach for overparameterized single-layer networks. ,This paper proposes a method for compressed sensing recovery from untrained deep generative models. The method is based on Deep Image Prior (DIP) and uses convolutional weights in the network to learn the network weights. The authors show that the proposed method outperforms other approaches on differentiable linear inverse problem. They also show that their method is more robust to noisy measurements than other unlearned methods. The proposed method is evaluated on a fitting problem and compared to other approaches that rely on the pre-training of the networks. The main contribution of the paper is the learned regularization technique that uses prior information in the weights of the network. The reconstruction error is minimized by early stopping. The paper also shows that the DIP optimization approach can be applied to overparameterized single-layer networks.
22032,SP:23c0f621e6041003b59bf0532130760694cf6a4a,"reinforcement learning ( RL ) USED-FOR real - world problems. long time horizons FEATURE-OF action - reward correlation. hand - tuned network structure CONJUNCTION pre - defined subgoals. pre - defined subgoals CONJUNCTION hand - tuned network structure. hand - tuned network structure PART-OF hierarchies. pre - defined subgoals PART-OF hierarchies. HRL framework USED-FOR temporal abstraction. TAIC USED-FOR temporal abstraction. approach USED-FOR latent space. information - theoretic constraints USED-FOR approach. latent representations of action sequences USED-FOR temporal abstraction problem. latent variables CONJUNCTION state changes. state changes CONJUNCTION latent variables. algorithm USED-FOR abstraction of the long action sequences. abstraction USED-FOR tasks. convergence rate CONJUNCTION sample efficiency. sample efficiency CONJUNCTION convergence rate. sample efficiency EVALUATE-FOR RL algorithms. convergence rate EVALUATE-FOR RL algorithms. convergence rate EVALUATE-FOR technique. sample efficiency EVALUATE-FOR technique. Method are Hierarchical reinforcement learning ( HRL ) methods, and temporal abstractions. OtherScientificTerm are task - specific knowledge, and visualization of the latent space. Metric is mutual information. Material is benchmark learning problems. ","This paper studies the problem of temporal abstraction in reinforcement learning (RL) in the context of real-world problems with long time horizons. The authors propose a new HRL framework, TAIC, to solve the temporal abstraction problem with latent representations of action sequences. The proposed approach is based on information-theoretic constraints on the latent space and the pre-defined subgoals of the hierarchies, which are the hand-tuned network structure and the set of possible pre-specified sub-goals.  The authors show that the proposed technique achieves better convergence rate and sample efficiency than existing RL algorithms.  ","This paper proposes a new approach to learn a latent space of action-reward correlation over long time horizons. The authors propose to use Hierarchical reinforcement learning (HRL) methods to learn task-specific knowledge. The key idea is to learn hierarchies with a hand-tuned network structure and pre-defined subgoals. The proposed approach is based on information-theoretic constraints on the latent space. The paper proposes to use the HRL framework to learn temporal abstraction of the long action sequences using TAIC. The temporal abstraction problem is formulated as a latent representations of action sequences with latent variables and state changes, and the authors show that the proposed algorithm can learn the abstraction of these tasks. They also show that their technique can achieve better convergence rate and sample efficiency compared to existing RL algorithms. The experimental results on benchmark learning problems demonstrate the effectiveness of the proposed approach. "
22041,SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"Graph Convolutional Network ( GCN ) USED-FOR graph representation learning. small graphs USED-FOR shallow models. acceleration methods USED-FOR GCNs. larger graphs CONJUNCTION deeper layers. deeper layers CONJUNCTION larger graphs. GCN - like models USED-FOR deeper layers. GCN - like models USED-FOR larger graphs. local bi - directional influence ( correlation ) FEATURE-OF mini - batch of nodes. recursive propagation CONJUNCTION skip connection. skip connection CONJUNCTION recursive propagation. first - order and higher - order proximities FEATURE-OF single layer propagation process. first - order and higher - order proximities PART-OF model. large benchmark graphs EVALUATE-FOR model. Task is graph - based applications. Method are layer - wise sampling strategy, and self - attention mechanism. Metric is time complexity. OtherScientificTerm is sampled nodes. ","This paper studies the problem of graph convolutional network (GCN) for graph representation learning. The authors propose a new layer-wise sampling strategy for GCNs. The proposed method is based on the self-attention mechanism, and the authors show that the proposed method can achieve better performance than existing GCNs in terms of time complexity. ","This paper proposes a graph convolutional network (GCN) for graph representation learning. The authors propose a layer-wise sampling strategy, where each node is sampled from a mini-batch of nodes with local bi-directional influence (correlation) and the time complexity of the sampled nodes is proportional to the number of nodes. They also propose acceleration methods for GCNs. They show that GCN-like models can be used to learn larger graphs and deeper layers. The proposed model is based on first-order and higher-order proximities in the single layer propagation process, recursive propagation and skip connection. Experiments on large benchmark graphs show that the proposed model outperforms other graph-based applications."
22050,SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,velocities CONJUNCTION interactions. interactions CONJUNCTION velocities. STOVE HYPONYM-OF state - space model. state - space model USED-FOR videos. image model CONJUNCTION dynamics model. dynamics model CONJUNCTION image model. dynamics model USED-FOR inference. image model USED-FOR It. dynamics model USED-FOR It. STOVE COMPARE unsupervised models. unsupervised models COMPARE STOVE. STOVE COMPARE supervised baselines. supervised baselines COMPARE STOVE. unsupervised models COMPARE supervised baselines. supervised baselines COMPARE unsupervised models. model USED-FOR model - based control. Method is physical system. Generic is models. Task is regularizing training. OtherScientificTerm is physical behavior. ,"This paper proposes STOVE, a state-space model for videos. It combines an image model and a dynamics model for inference. The model is able to learn a physical system that can be used for model-based control. The authors show that the performance of the proposed model outperforms unsupervised models and supervised baselines. ","This paper proposes a state-space model, STOVE, for videos. It consists of an image model, a dynamics model, and an inference model. The model is used for model-based control. The authors show that the proposed model outperforms supervised baselines in terms of physical behavior. The paper also shows that the models can be used for regularizing training."
22059,SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"variational autoencoders ( VAE ) CONJUNCTION generative adversarial networks ( GAN ). generative adversarial networks ( GAN ) CONJUNCTION variational autoencoders ( VAE ). variational autoencoders ( VAE ) PART-OF autoencoding model. model USED-FOR λ - Jeffreys divergence. Gaussian CONJUNCTION Laplace. Laplace CONJUNCTION Gaussian. Laplace HYPONYM-OF explicit likelihood. Gaussian HYPONYM-OF explicit likelihood. approach USED-FOR VAE model. implicit likelihood USED-FOR approach. adversarially trained discriminator USED-FOR implicit likelihood. adversarially trained discriminator USED-FOR approach. implicit likelihood USED-FOR VAE model. mode - seeking CONJUNCTION mass - covering behaviour. mass - covering behaviour CONJUNCTION mode - seeking. mode - seeking FEATURE-OF model. CIFAR-10 and TinyImagent datasets EVALUATE-FOR model. mass - covering behaviour FEATURE-OF model. Method are GAN, VAE, and adversarial training. OtherScientificTerm are mode collapsing problem, model distribution, and VAE loss. Generic are it, It, and objective. ",This paper studies the mode collapsing problem in the autoencoding model with variational autoencoders (VAE) and generative adversarial networks (GAN). The authors propose a new approach to train a VAE model with implicit likelihood (Gaussian and Laplace) and an adversarially trained discriminator. The authors show that the proposed model is able to achieve λ-Jeffreys divergence with respect to the model distribution. They also show that their model can achieve mode-seeking and mass-covering behaviour on CIFAR-10 and TinyImagent datasets.,"This paper proposes a variational autoencoders (VAE) and generative adversarial networks (GAN) approach for learning a VAE model. The VAE is an autoencoding model, and the GAN is an adversarial network. The authors propose an approach to learn an implicit likelihood of the explicit likelihood (Gaussian or Laplace) and the implicit likelihood for the VAE. The proposed approach is based on an adversarially trained discriminator. It is shown that the model is able to learn the λ-Jeffreys divergence between the Gaussian and the Laplace, and that it can learn the mode collapsing problem. The model is evaluated on CIFAR-10 and TinyImagent datasets, and is shown to achieve better mode-seeking and mass-covering behaviour compared to the original model. "
22068,SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"Adversarial attacks FEATURE-OF CNN classifiers. unreasonably linear extrapolation USED-FOR CNNs. attacks USED-FOR Bayes - Optimal classifier. Bayes - Optimal classifier USED-FOR class distributions. optimal classifier USED-FOR attacks. smooth decision surface FEATURE-OF classifier. datasets EVALUATE-FOR optimal classifier. datasets USED-FOR Bayes - Optimal classifier. adversarial examples USED-FOR it. large - margin methods USED-FOR classifier. machine learning USED-FOR adversarial vulnerability. Task is classification. OtherScientificTerm are geometry of high dimensions, data distribution, optimal decision boundary, and low dimensions. Material is digits. Method are CNN training, and suboptimal training methods. ","This paper studies the problem of adversarial attacks on CNN classifiers trained with unreasonably linear extrapolation. The authors propose a Bayes-Optimal classifier for class distributions with geometry of high dimensions. They show that the optimal classifier with smooth decision surface is robust to attacks on the data distribution. They also show that it can be trained with large-margin methods. Finally, they show that adversarial examples can be used to improve the performance of the classifier. ","This paper studies adversarial attacks on CNN classifiers based on unreasonably linear extrapolation. The authors propose a Bayes-Optimal classifier for class distributions with geometry of high dimensions. The optimal classifier has a smooth decision surface, and the authors show that it is robust to adversarial examples. They also show that the classifier can be trained with large-margin methods. "
22077,SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"top-1 accuracy EVALUATE-FOR sparse and non - sparse models. Method are Neural network pruning techniques, network, pruning, abstract representations, and fine - grained classification. Metric are top1 test set accuracy, and image quality. OtherScientificTerm are pruning identified exemplars ( PIEs ), and sparsity. Material are PIE images, and hard - to - generalize - to images. ","This paper proposes Neural network pruning techniques to improve the top-1 accuracy of sparse and non-sparse models by pruning identified exemplars (PIEs) from the network. The main idea is to prune the abstract representations of the PIE images, which are then used for fine-grained classification. Theoretical results show that the top1 test set accuracy of the pruned images is better than the original image quality. The authors also show that sparsity can be reduced by the pruning. ","This paper proposes Neural network pruning techniques to improve the top-1 accuracy of sparse and non-sparse models. The authors propose pruning identified exemplars (PIEs) by pruning the abstract representations of the network. The pruning is done in two stages: first, the network is pruned to remove the sparsity in the original PIE images, and second, the pruning of abstract representations is performed on the original images. The paper shows that the top1 test set accuracy is better than the original image quality, and the authors show that sparsity can be reduced in the case of hard-to-generalize-to images. "
22086,SP:4b17edaa7ec6201891433320d85f9a415656b763,"Interactive Fiction games HYPONYM-OF text - based simulations. natural language understanding CONJUNCTION partial observability. partial observability CONJUNCTION natural language understanding. reinforcement learning agents USED-FOR natural language understanding. partial observability CONJUNCTION action generation. action generation CONJUNCTION partial observability. combinatorially - large text - based action spaces FEATURE-OF action generation. template - based action space USED-FOR KG - A2C1. knowledge graph USED-FOR game state. knowledge graph USED-FOR natural language generation. KG - A2C COMPARE IF agents. IF agents COMPARE KG - A2C. IF games EVALUATE-FOR KG - A2C. OtherScientificTerm are natural language, dynamic knowledge graph, combinatorially large natural language actions, and action space size. Generic is They. ","This paper studies the problem of text-based simulations such as Interactive Fiction games, where the goal is to learn a natural language that can be used as a dynamic knowledge graph to guide the action generation and action generation in combinatorially-large text-by-text-based action spaces. The authors propose KG-A2C1, which is a template-based approach to learn the action space size of a game state from a knowledge graph. They show that this approach can improve the performance of reinforcement learning agents for natural language understanding and partial observability. They also show that KG - A2C outperforms IF agents in a variety of IF games. ","This paper proposes a novel approach to learning a dynamic knowledge graph for playing interactive fiction games. The key idea is to use the knowledge graph to learn a game state, and then use reinforcement learning agents to improve natural language understanding and partial observability. The authors show that KG-A2C1 outperforms IF agents in most of the IF games. They also show that combinatorially large natural language actions can be learned in combinatorial-large text-based action spaces. "
22095,SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"language generation HYPONYM-OF sequence prediction problems. maximum likelihood estimation ( MLE ) USED-FOR sequence prediction problems. data - dependent Gaussian prior CONJUNCTION detailed training prediction. detailed training prediction CONJUNCTION data - dependent Gaussian prior. data - dependent Gaussian prior USED-FOR Kullback – Leibler divergence term. text summarization CONJUNCTION storytelling. storytelling CONJUNCTION text summarization. supervised and unsupervised machine translation CONJUNCTION text summarization. text summarization CONJUNCTION supervised and unsupervised machine translation. storytelling CONJUNCTION image captioning. image captioning CONJUNCTION storytelling. language generation tasks EVALUATE-FOR method. image captioning HYPONYM-OF language generation tasks. supervised and unsupervised machine translation HYPONYM-OF language generation tasks. text summarization HYPONYM-OF language generation tasks. storytelling HYPONYM-OF language generation tasks. Generic is it. Method is MLE. OtherScientificTerm are negative diversity ignorance, and prior topological order of tokens. Metric is MLE loss. ","This paper studies the problem of maximum likelihood estimation (MLE) for sequence prediction problems such as language generation, text summarization, and detailed training prediction. The authors propose a data-dependent Gaussian prior for the Kullback-Leibler divergence term, which is a generalization of the MLE loss. They show that it can be used to reduce the negative diversity ignorance in MLE. They also show that the prior topological order of tokens can also be used as a prior topology. The proposed method is evaluated on a variety of language generation tasks such as supervised and unsupervised machine translation, textual summarization and image captioning.","This paper proposes a novel method for learning a Kullback-Leibler divergence term for sequence prediction problems with maximum likelihood estimation (MLE) under negative diversity ignorance. The key idea is to use a data-dependent Gaussian prior and a detailed training prediction. The authors show that the MLE loss does not depend on the prior topological order of tokens. They also show that it does not rely on negative diversity. The proposed method is evaluated on a variety of language generation tasks including text summarization, supervised and unsupervised machine translation, and image captioning."
22104,SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"Temperature scaling USED-FOR DNN. Temperature scaling HYPONYM-OF calibration approach. cross - entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross - entropy loss. focal loss USED-FOR models. temperature scaling CONJUNCTION focal loss. focal loss CONJUNCTION temperature scaling. confidence FEATURE-OF model. focal loss USED-FOR calibrated models. accuracy EVALUATE-FOR focal loss. network architectures USED-FOR approach. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. accuracy EVALUATE-FOR approach. calibration EVALUATE-FOR approach. Task are Miscalibration, downstream tasks, and miscalibration. Method is Deep Neural Networks ( DNNs ). Generic is networks. Material is NLP ( SST, 20 Newsgroup ) datasets. ",This paper proposes a new calibration approach called Temperature scaling for DNNs. The proposed approach uses a cross-entropy loss and a focal loss to calibrate the models. The authors show that the proposed approach improves the accuracy and calibration. ,"This paper proposes a new calibration approach, Temperature scaling, for DNNs. The key idea is to use cross-entropy loss and focal loss to calibrate the models. The proposed approach is evaluated on NLP (SST, 20 Newsgroup) datasets. The results show that the proposed approach improves the accuracy and calibration of the proposed network architectures. "
22113,SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,Lipschitz constant FEATURE-OF neural networks. LiPopt HYPONYM-OF polynomial optimization framework. sparse connectivity USED-FOR network. sparse connectivity USED-FOR complexity of computation. approach COMPARE baselines. baselines COMPARE approach. networks CONJUNCTION networks. networks CONJUNCTION networks. ` ∞-Lipschitz constant EVALUATE-FOR approach. random weights CONJUNCTION networks. networks CONJUNCTION random weights. random weights USED-FOR networks. MNIST USED-FOR networks. Task is optimization problems. Method is convolutional as well as pruned neural networks. ,"This paper proposes a polynomial optimization framework called LiPopt, which is based on sparse connectivity in neural networks with Lipschitz constant. The authors show that sparse connectivity can reduce the complexity of computation by reducing the number of parameters in the network. They also show that the proposed approach can achieve better performance than baselines on a variety of optimization problems. They show that their approach achieves better performance on MNIST than networks with random weights and networks with networks trained with networks. ","This paper proposes a polynomial optimization framework called LiPopt, which is based on the idea of sparse connectivity in a network. The authors show that sparse connectivity can reduce the complexity of computation by a factor of Lipschitz constant in neural networks. The approach is evaluated on MNIST and compared to baselines on networks with random weights and networks with networks trained with MNIST. "
22122,SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,self - supervised learning approach USED-FOR video features. video classification CONJUNCTION captioning and segmentation. captioning and segmentation CONJUNCTION video classification. tasks EVALUATE-FOR methods. self - supervised learning approach USED-FOR tasks. self - supervised learning approach COMPARE methods. methods COMPARE self - supervised learning approach. captioning and segmentation HYPONYM-OF tasks. video classification HYPONYM-OF tasks. BERT model USED-FOR text sequences. softmax loss CONJUNCTION noise contrastive estimation ( NCE ). noise contrastive estimation ( NCE ) CONJUNCTION softmax loss. sequences of real - valued feature vectors USED-FOR method. BERT model USED-FOR method. sequences of visual features CONJUNCTION sequences of words. sequences of words CONJUNCTION sequences of visual features. automatic speech recognition USED-FOR sequences of words. sequences of words USED-FOR representations. sequences of visual features USED-FOR representations. Method is cross - modal training. ,"This paper proposes a self-supervised learning approach for video features. The proposed method uses sequences of real-valued feature vectors to learn representations from sequences of visual features and sequences of words. The BERT model is then used to learn text sequences from the sequences. The authors show that the proposed methods outperform existing methods on three tasks: video classification, captioning and segmentation. ",This paper proposes a self-supervised learning approach for video features and captioning and segmentation. The proposed method uses sequences of real-valued feature vectors and softmax loss and noise contrastive estimation (NCE). The authors show that the proposed method outperforms the BERT model for text sequences and automatic speech recognition for sequences of words and sequences of visual features. The authors also show that cross-modal training improves the performance of the proposed methods.
22131,SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"selection masks CONJUNCTION neural network. neural network CONJUNCTION selection masks. Task are inference phase, and data transfer. OtherScientificTerm is public storage server. Method is machine learning models. Generic are framework, model, and masks. ",This paper proposes a new framework for training machine learning models. The main idea is to use a public storage server to store the data from the inference phase of the model. The model is trained using a combination of selection masks and a neural network. The data transfer between the two models is done via the masks. The authors show that the proposed framework is able to achieve state-of-the-art performance.,"This paper proposes a new framework for machine learning models. The framework is based on the idea of a public storage server, where the inference phase is done in a single data transfer, and the data transfer is performed in a series of steps. The model is trained with a set of masks and a neural network. The paper is well written, and easy to follow. "
22140,SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"Deep neural networks USED-FOR classification tasks. methodology USED-FOR neural network. it USED-FOR outof - distribution ( OOD ) examples. Outlier Exposure ( OE ) technique USED-FOR loss function. loss function USED-FOR out - of - distribution detection. image and text classification tasks EVALUATE-FOR out - of - distribution detection. OE USED-FOR loss function. image and text classification tasks EVALUATE-FOR loss function. OE USED-FOR out - of - distribution detection. method CONJUNCTION Mahalanobis distance - based classifier. Mahalanobis distance - based classifier CONJUNCTION method. OOD detection task EVALUATE-FOR method. Task is artificial intelligence. Method are neural networks, and classification algorithms. OtherScientificTerm is novel class distributions. Metric is classification accuracy. ","This paper studies the problem of out-of-distribution (OOD) detection in deep neural networks for classification tasks. The authors propose a methodology for training a neural network to detect outof-disentropy (OOD), where the outlier exposure (OE) technique is applied to the loss function of the neural network. They show that the OE can be used as a loss function for OOD detection on both image and text classification tasks, and that it can be applied to novel class distributions as well. The proposed method is evaluated on the OOD detector task as well as on the Mahalanobis distance-based classifier.","This paper proposes a new methodology for training deep neural networks for classification tasks. The authors propose a new loss function based on Outlier Exposure (OE) technique. The loss function can be used for out-of-distribution detection on both image and text classification tasks, and it can be applied to novel class distributions. The proposed method is evaluated on the OOD detection task and compared to a Mahalanobis distance-based classifier. "
22149,SP:89bc528ef801182365ac279e8963803afccb391d,end - to - end deep learning model USED-FOR RNA secondary structure prediction. E2Efold HYPONYM-OF end - to - end deep learning model. unrolled algorithm USED-FOR constrained programming. E2Efold USED-FOR RNA base - pairing matrix. unrolled algorithm USED-FOR deep architectures. deep architectures USED-FOR constraints. it COMPARE SOTA. SOTA COMPARE it. it COMPARE algorithms. algorithms COMPARE it. benchmark datasets EVALUATE-FOR E2Efold. E2Efold COMPARE it. it COMPARE E2Efold. SOTA USED-FOR pseudoknotted structures. E2Efold COMPARE SOTA. SOTA COMPARE E2Efold. inference time EVALUATE-FOR algorithms. ,"This paper proposes a new end-to-end deep learning model for RNA secondary structure prediction, E2Efold. The authors propose an unrolled algorithm for constrained programming, where the RNA base-pair matrix is represented as a set of RNA molecules. The proposed unrolled algorithms can be used to train deep architectures to solve the constraints in deep architectures. Empirical results on several benchmark datasets show that it outperforms SOTA and other algorithms in terms of inference time, and is able to solve pseudoknotted structures.","This paper proposes an end-to-end deep learning model for RNA secondary structure prediction, called E2Efold. The authors propose an unrolled algorithm for constrained programming, where the RNA base-pair matrix of the RNA is encoded as a set of pseudoknotted structures. They show that the proposed deep architectures are able to satisfy the constraints. They also show that it outperforms SOTA in terms of inference time on several benchmark datasets."
22158,SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"collective policies COMPARE individually trained policies. individually trained policies COMPARE collective policies. OtherScientificTerm are biases, virtual simulation, agents ’ simulations, biased representations, and internal simulations. Method is collective policy. Material is real - world environment. ","This paper studies the problem of learning a collective policy in a real-world environment. The authors propose a method to learn a policy that is robust to biases in a virtual simulation, where the agents’ simulations are trained with biased representations. They show that the proposed collective policies are more robust than individually trained policies. They also provide a theoretical analysis of the relationship between internal simulations and internal simulations.","This paper proposes a new way to measure the biases in a virtual simulation, called “agents’ simulations”. The idea is to measure how well a collective policy performs in a real-world environment compared to individually trained policies. This is done by measuring the difference between the biased representations and the internal simulations. "
22167,SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"Generic responses HYPONYM-OF open - domain dialog generation. one - to - one task USED-FOR one - to - many task. dialog generation model USED-FOR semantic latent space. prompt USED-FOR features. model USED-FOR semantically related responses. regression task USED-FOR pair relationship. model COMPARE baselines. baselines COMPARE model. coherence EVALUATE-FOR baselines. model USED-FOR generic response problem. coherence EVALUATE-FOR model. OtherScientificTerm are latent space, and MLE loss. Method is autoencoder. ","This paper proposes Generic responses HYPONYM, a new open-domain dialog generation model for semantic latent space. The proposed model learns to generate semantically related responses from a prompt. The model is trained on a one-to-one task, and then used to generate a generic response problem. The authors show that the proposed model achieves better coherence than existing baselines. ","This paper proposes Generic responses, an open-domain dialog generation model for semantic latent space. The model learns a set of semantically related responses from a prompt, and then uses the latent space to generate features from the prompt. The autoencoder is trained with a MLE loss. The proposed model is evaluated on a generic response problem with coherence. The authors show that the proposed model achieves better coherence than baselines. "
22176,SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"it USED-FOR life - affecting decisions. Gaussian light and shadow ( GLAS ) HYPONYM-OF salient explanation method. feature perturbation USED-FOR deep models. GLAS USED-FOR coarseto - fine control. scalability of Gaussian mask USED-FOR GLAS. scalability of Gaussian mask USED-FOR coarseto - fine control. GLAS USED-FOR fine - grained classification. fine - grained classification dataset EVALUATE-FOR GLAS. high speed EVALUATE-FOR GLAS. ImageNet Large Scale Visual Recognition Challenge EVALUATE-FOR GLAS. OtherScientificTerm are discriminative features, salient explanation, and 224×224 image. Task is fine - grained classification task. Method are Gaussian mask, and recursive GLAS. ","This paper proposes a new salient explanation method called Gaussian light and shadow (GLAS) that uses feature perturbation to improve the discriminative features in deep models. The authors show that GLAS can improve the coarseto-fine control by using scalability of Gaussian mask in GLAS, and it can be used to make life-affecting decisions in the fine-grained classification task.  The authors also show that the scalability improves the performance of GLAS in fine-gained classification dataset on the ImageNet Large Scale Visual Recognition Challenge.  ","This paper proposes a salient explanation method, Gaussian light and shadow (GLAS) which is based on feature perturbation in deep models. The key idea is that the discriminative features of a given image can be used to explain life-affecting decisions. The main contribution of the paper is to use the scalability of Gaussian mask in GLAS for coarseto-fine control. The authors show that GLAS can achieve high speed on the fine-grained classification dataset on the ImageNet Large Scale Visual Recognition Challenge. They also show that recursive GLAS is able to achieve better performance than the original Gaussian masks. "
22185,SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"Convolution PART-OF Convolutional Neural Networks ( CNNs ). kernel USED-FOR Convolutional Neural Networks ( CNNs ). convolutional kernels USED-FOR redundant data. procedure USED-FOR pixel - wise and channel - wise correlations. computational cost EVALUATE-FOR convolution layer. deconvolution filters PART-OF network. center - surround structure FEATURE-OF biological neurons. center - surround structure HYPONYM-OF deconvolution filters. visual regions of the brain FEATURE-OF biological neurons. Filtering USED-FOR sparse representation. kernels USED-FOR sparse representation. kernels USED-FOR Filtering. network deconvolution operation USED-FOR neural network models. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. Fashion - MNIST CONJUNCTION Cityscapes. Cityscapes CONJUNCTION Fashion - MNIST. CIFAR-100 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION CIFAR-100. Cityscapes CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION Cityscapes. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. MNIST CONJUNCTION Cityscapes. Cityscapes CONJUNCTION MNIST. Fashion - MNIST CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION Fashion - MNIST. MNIST EVALUATE-FOR network deconvolution operation. Fashion - MNIST EVALUATE-FOR network deconvolution operation. CIFAR-100 EVALUATE-FOR network deconvolution operation. ImageNet datasets EVALUATE-FOR network deconvolution operation. CIFAR-10 EVALUATE-FOR network deconvolution operation. Material is real - world image data. Task is neural network training. Method are network deconvolution, Network deconvolution, neural networks, and batch normalization. Metric is faster convergence. ","This paper studies the problem of network deconvolution in the context of neural network training. The authors propose a new procedure for learning pixel-wise and channel-wise correlations between neural network models. The proposed procedure is based on convolutional kernels, which can be used to remove redundant data from the training data. The paper shows that the proposed method can achieve faster convergence than existing methods.  The authors also show that the resulting convolution layer has a lower computational cost compared to the original network.  ","This paper proposes a new convolution part of Convolutional Neural Networks (CNNs) called Network deconvolution, which is an extension of convolutional kernels to redundant data. The authors propose a new procedure to learn pixel-wise and channel-wise correlations between pixels and channels. The proposed method is evaluated on real-world image data, where it is shown to achieve faster convergence compared to batch normalization. The computational cost of the convolution layer is also reduced. "
22194,SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"smartphones HYPONYM-OF edge devices. neural network quantization methods USED-FOR GANs. CNN quantization methods USED-FOR GAN models. CNN quantization methods USED-FOR extreme low bits. quantization method USED-FOR GANs. QGAN HYPONYM-OF quantization method. EM algorithms USED-FOR quantization method. multi - precision algorithm USED-FOR quantization precision. multi - precision algorithm USED-FOR GANs. quantization precision FEATURE-OF GANs. CIFAR-10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR-10. QGAN USED-FOR GANs. QGAN COMPARE models. models COMPARE QGAN. CIFAR-10 EVALUATE-FOR QGAN. CelebA EVALUATE-FOR QGAN. QGAN USED-FOR 1 - bit or 2 - bit representations. Method are generative adversarial neural networks ( GANs ), convolutional neural networks ( CNNs ), quantization algorithms, and generator and discriminator networks. Generic is them. OtherScientificTerm is image qualities requirements. ","This paper studies the problem of generative adversarial neural networks (GANs). The authors propose a new quantization method, QGAN, for GANs based on CNN quantization methods for the extreme low bits of edge devices (e.g., smartphones). They show that the proposed quantization algorithm is able to achieve better quantization precision than existing GAN models. The authors also show that QGAN can achieve better 1-bit or 2-bit representations than existing models on CIFAR-10 and CelebA.","This paper proposes a novel quantization method for GANs. The authors propose to use generative adversarial neural networks (GANs) instead of convolutional neural networks. The main idea is to use CNN quantization methods to improve the quantization precision of GAN models for extreme low bits in edge devices (e.g., smartphones). The authors show that the proposed quantization algorithm QGAN outperforms existing EM algorithms on CIFAR-10, CelebA, and QGAN for 1-bit or 2-bit representations. They also show that QGAN is more robust to adversarial attacks than other models. "
22203,SP:58c4905f59f04a50b30d27c99521126a6455d38a,"Generative Adversarial Networks HYPONYM-OF nonconvex applications. bilinear and convex - strongly concave settings FEATURE-OF global last - iterate convergence rates. Simultaneous Gradient Descent / Ascent HYPONYM-OF natural algorithms. linear convergence FEATURE-OF HAMILTONIAN GRADIENT DESCENT ( HGD ) algorithm. “ sufficiently bilinear ” condition FEATURE-OF convex - concave problems. convergence rates FEATURE-OF stochastic HGD. Task are convex - concave min - max optimization, and training Generative Adversarial Networks. OtherScientificTerm are averageiterate convergence results, last - iterate convergence guarantees, last - iterate convergence, and convex - concave min - max settings. Method is Consensus Optimization algorithm. ","This paper studies the problem of convex-concave min-max optimization in the bilinear and convex strongly concave settings, where the averageiterate convergence results are nonconvex. The authors propose a new algorithm, Simultaneous Gradient Descent/Ascent (SDA), which is a generalization of natural algorithms such as Simultaneously Optimized Descent and Ascent. The main contribution of SDA is that it is able to converge to the last iterate of the last-iterate converges in the convex setting. Theoretically, the authors show that SDA converges to the optimal solution of a convex problem under the “sufficient bilinearly” condition, which is an important condition for convex problems. Empirically, SDA achieves linear convergence in the case of the HAMILTONIAN GRADIENT DESCENT (HGD) algorithm. The convergence rates of stochastic HGD are also shown. ","This paper studies the convergence rates of global last-iterate convergence rates in bilinear and convex-strongly concave settings in nonconvex applications such as training Generative Adversarial Networks. The authors show that the averageiterate converges to the optimal solution of the problem under the Consensus Optimization algorithm. The main contribution of the paper is to show that, under the “satisfied” condition, the convergence rate of the stochastic HGD with linear convergence can be approximated by Simultaneous Gradient Descent/Ascent (HAMILTONIAN GRADIENT DESCENT (HGD) algorithm), which is a natural algorithms. The paper also shows that, in the case of convex -concave min-max optimization, the generalization error of the algorithm is bounded by the number of iterations. "
22212,SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"stability FEATURE-OF ResNet. stability FEATURE-OF ResNet. gradient descent USED-FOR global minima. over - parameterization requirement FEATURE-OF ResNet. ResNet COMPARE vanilla feedforward network. vanilla feedforward network COMPARE ResNet. normalization layer USED-FOR deep ResNet. normalization layer USED-FOR ResNet. Method is ResNet structure. OtherScientificTerm are ResNet block hl, ReLU activation, initialization, residual blocks, and global convergence. Metric is τ. Generic is forward process. ","This paper studies the stability of ResNet with over-parameterization requirement. The authors show that the ResNet block hl has global minima with gradient descent. They also show that a normalization layer in deep ResNet is equivalent to a vanilla feedforward network. Finally, they show that global convergence can be achieved with respect to the forward process. ","This paper studies the stability of ResNet with respect to the over-parameterization requirement of the ResNet block hl. The authors propose a new ResNet structure that is based on the ReLU activation. They show that the global minima obtained by gradient descent can be obtained by minimizing the global maxima. They also show that a normalization layer in deep ResNet is equivalent to a vanilla feedforward network, and that the forward process of the normalization can be approximated by the global convergence. "
22221,SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,Sparse neural networks COMPARE dense networks. dense networks COMPARE Sparse neural networks. they USED-FOR wall clock inference times. sparse networks USED-FOR inference. dense networks USED-FOR sparse networks. method USED-FOR sparse neural networks. fixed parameter count CONJUNCTION fixed computational cost. fixed computational cost CONJUNCTION fixed parameter count. accuracy EVALUATE-FOR dense - to - sparse training methods. fixed computational cost USED-FOR method. fixed parameter count USED-FOR method. parameter magnitudes CONJUNCTION infrequent gradient calculations. infrequent gradient calculations CONJUNCTION parameter magnitudes. method USED-FOR topology. topology FEATURE-OF network. parameter magnitudes USED-FOR topology. parameter magnitudes USED-FOR method. infrequent gradient calculations USED-FOR method. approach COMPARE prior techniques. prior techniques COMPARE approach. accuracy EVALUATE-FOR prior techniques. floating - point operations ( FLOPs ) USED-FOR approach. accuracy EVALUATE-FOR approach. MobileNet v1 CONJUNCTION MobileNet v2. MobileNet v2 CONJUNCTION MobileNet v1. ResNet-50 CONJUNCTION MobileNet v1. MobileNet v1 CONJUNCTION ResNet-50. WideResNets CONJUNCTION RNNs. RNNs CONJUNCTION WideResNets. ImageNet-2012 dataset CONJUNCTION WideResNets. WideResNets CONJUNCTION ImageNet-2012 dataset. WideResNets CONJUNCTION CIFAR-10 dataset. CIFAR-10 dataset CONJUNCTION WideResNets. MobileNet v2 CONJUNCTION WideResNets. WideResNets CONJUNCTION MobileNet v2. WikiText-103 dataset EVALUATE-FOR RNNs. ResNet-50 USED-FOR sparse training. ImageNet-2012 dataset EVALUATE-FOR MobileNet v2. Method is trainable sparse model. Task is optimization. OtherScientificTerm is local minima. ,"This paper proposes a method for training sparse neural networks in the presence of dense networks. Sparse neural networks outperform dense networks in terms of accuracy in inference. The proposed method is based on floating-point operations (FLOPs) and uses the fixed parameter count and the fixed computational cost to improve the performance of dense-to-sparse training methods. The method uses parameter magnitudes and infrequent gradient calculations to learn the topology of the network, which is then used to train a trainable sparse model. Experiments on the WikiText-103 dataset show that the proposed approach achieves better accuracy than prior techniques. The experiments also show that RNNs and WideResNets outperform MobileNet v1 and WideRNNs on the ImageNet-2012 dataset and CIFAR-10 dataset. ","This paper proposes a method for training sparse neural networks. Sparse neural networks can be seen as dense networks, but they are slower than dense networks in terms of wall clock inference times. This paper proposes to use sparse networks for inference. The key idea is to use floating-point operations (FLOPs) to optimize the topology of the network. The proposed approach is compared to prior techniques on the accuracy of dense-to-sparse training methods with fixed parameter count, infrequent gradient calculations, and fixed computational cost. The authors show that the proposed approach can achieve better accuracy than prior techniques. The paper also shows that the training of a trainable sparse model can be done with local minima. Experiments on the WikiText-103 dataset, MobileNet v1, WideResNets, RNNs, CIFAR-10 dataset, and ImageNet-2012 dataset show the effectiveness of the proposed method."
22230,SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"deep generative models USED-FOR photo - realistic images. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep generative models USED-FOR visual or textual content embeddings. photo - realistic images CONJUNCTION visual or textual content embeddings. visual or textual content embeddings CONJUNCTION photo - realistic images. deep generative models USED-FOR natural language processing. deep generative models USED-FOR computer vision. translation CONJUNCTION zoom. zoom CONJUNCTION translation. zoom CONJUNCTION color variations. color variations CONJUNCTION zoom. color variations HYPONYM-OF transformations. translation HYPONYM-OF transformations. zoom HYPONYM-OF transformations. GANs CONJUNCTION variational auto - encoders. variational auto - encoders CONJUNCTION GANs. variational auto - encoders EVALUATE-FOR method. GANs EVALUATE-FOR method. approach CONJUNCTION BigGAN model. BigGAN model CONJUNCTION approach. Method are generative process, generative models, and generative model. OtherScientificTerm are latent space, and human annotations. ","This paper proposes a generative process to learn the latent space of an image. The generative model is trained to generate a set of embeddings for each image, which are then used to train a GAN. The GANs are trained using a combination of GAN and variational auto-encoders. The proposed method is evaluated on both synthetic and real-world datasets.","This paper proposes a generative process to generate images from the latent space. The generative model is trained using GANs, variational auto-encoders, and a BigGAN model. The method is evaluated on computer vision, natural language processing, and visual or textual content embeddings from deep generative models for photo-realistic images. The authors show that the proposed approach outperforms the GAN and the Variational Auto-Encoders. "
22239,SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"model USED-FOR unsupervised physical parameter estimation of systems. differential equations USED-FOR scene dynamics. video USED-FOR unsupervised physical parameter estimation of systems. object state supervision USED-FOR physical scene understanding methods. objects CONJUNCTION state and velocity representations. state and velocity representations CONJUNCTION objects. framework USED-FOR long term extrapolative video prediction. framework USED-FOR vision - based model - predictive control. long term extrapolative video prediction CONJUNCTION vision - based model - predictive control. vision - based model - predictive control CONJUNCTION long term extrapolative video prediction. approach COMPARE unsupervised methods. unsupervised methods COMPARE approach. unsupervised methods USED-FOR long - term future frame prediction of systems. approach USED-FOR long - term future frame prediction of systems. dynamics PART-OF model. interacting objects FEATURE-OF long - term future frame prediction of systems. inductive bias USED-FOR dynamics. vision - actuated model - based control USED-FOR pendulum system. goal - driven control CONJUNCTION physical reasoning. physical reasoning CONJUNCTION goal - driven control. controller ’s interpretability USED-FOR goal - driven control. physical reasoning USED-FOR zero - data adaptation. controller ’s interpretability USED-FOR physical reasoning. controller ’s interpretability USED-FOR zero - data adaptation. goal - driven control USED-FOR zero - data adaptation. OtherScientificTerm is labeled states. Method are differentiable physics, physics - as - inverse - graphics approach, and vision - physics integration. ","This paper proposes a new model for unsupervised physical parameter estimation of systems from video. The model is based on differential equations for scene dynamics, where the labeled states are represented by objects and the state and velocity representations are learned by object state supervision. The proposed approach is able to perform long-term future frame prediction of systems with interacting objects. The framework is applied to vision-based model-predictive control, long term extrapolative video prediction, vision-actuated model-based control for pendulum system, and goal-driven control for physical reasoning. The authors show that the proposed approach can achieve better performance compared to existing state-of-the-art in terms of accuracy and interpretability compared to the state of the art in the physics-as-inverse-graphics approach. ","This paper proposes a model for unsupervised physical parameter estimation of systems in video. The model is based on differential equations for scene dynamics, where the labeled states are represented by objects and state and velocity representations. The proposed approach is evaluated on long-term future frame prediction of systems with interacting objects. The authors show that the proposed approach can outperform other state-of-the-art methods in terms of performance and interpretability of the learned dynamics. They also show that their approach can be applied to vision-based model-predictive control and vision-actuated model-based control of a pendulum system. "
22248,SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"Graph Convolutional Networks ( GCN ) USED-FOR class relevance. Graph Convolutional Networks ( GCN ) USED-FOR structure of clean and noisy data. graph per class USED-FOR structure of clean and noisy data. GCN - inferred “ clean ” probability USED-FOR relevance measure. GCN USED-FOR binary classifier. weighted binary cross - entropy loss function USED-FOR GCN. weighted binary cross - entropy loss function USED-FOR binary classifier. few - shot learning problem EVALUATE-FOR method. classification accuracy EVALUATE-FOR few - shot classification. few - shot classification EVALUATE-FOR GCN - based cleaning process. classification accuracy EVALUATE-FOR GCN - based cleaning process. GCN - based method COMPARE transductive approach. transductive approach COMPARE GCN - based method. Method is classifier. OtherScientificTerm are clean from noisy examples, and relevance. Material is noisy data. ","This paper proposes a new classifier based on Graph Convolutional Networks (GCN) for class relevance. The proposed method is based on a weighted binary cross-entropy loss function for the binary classifier. The GCN-implicit “clean” probability is used as the relevance measure, and the graph per class is used to learn the structure of clean and noisy data. The method is evaluated on a few-shot learning problem, and shows that the proposed method achieves better classification accuracy than the standard transductive approach. ",This paper proposes Graph Convolutional Networks (GCN) for class relevance. GCN is a binary classifier with a weighted binary cross-entropy loss function. The proposed GCN-implicit “clean” probability is used as the relevance measure. The authors show that the proposed method can be applied to the few-shot learning problem and achieve better classification accuracy than a transductive approach. 
22257,SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"edge features CONJUNCTION message passing channels. message passing channels CONJUNCTION edge features. differentiable objective USED-FOR MI. variational approach USED-FOR differentiable objective. variational approach USED-FOR MI. objective USED-FOR model. MI - maximized models USED-FOR learning tasks. objective USED-FOR edge information. edge information FEATURE-OF model. molecular graphs USED-FOR regression. relation prediction in knowledge graphs HYPONYM-OF learning tasks. regression HYPONYM-OF learning tasks. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are representation vector, and parameterized transform matrix. Metric is Mutual Information ( MI ). Material is knowledge graphs. ",This paper studies the mutual information (MI) problem in Graph Neural Networks (GNNs). The authors propose a differentiable objective for MI that can be used to train a model with edge features and message passing channels. The objective is based on a variational approach to learn the MI of a GNN. The model is trained using MI-maximized models on a variety of learning tasks such as relation prediction in knowledge graphs and regression on molecular graphs. The representation vector is a parameterized transform matrix. The authors show that the proposed objective is able to capture the edge information of the model. ,"This paper proposes Mutual Information (MI) as a new objective for Graph Neural Networks (GNNs). MI is a differentiable objective for learning MI. The authors propose a variational approach to learn the MI objective for a model with edge features and message passing channels. MI-maximized models are used for learning tasks such as regression on molecular graphs, relation prediction in knowledge graphs, and learning tasks with parameterized transform matrix. "
22266,SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"models USED-FOR specifying visual transformations. Generative networks USED-FOR specifying visual transformations. Generative networks HYPONYM-OF models. verification methods USED-FOR generative networks. verifier USED-FOR generative networks. APPROXLINE HYPONYM-OF verifier. deterministic and probabilistic abstract interpretation USED-FOR APPROXLINE. Task is certification of generative models. Method is generative models. OtherScientificTerm are sufficient non - convexity, non - convexity, and network ’s latent space. ","This paper studies the certification of generative models. Generative networks are models that can be used for specifying visual transformations. The authors propose two verification methods to certify the generative networks. The first verifier, APPROXLINE, is a deterministic and probabilistic abstract interpretation of the network’s latent space. The second verifier is a sufficient non-convexity.","This paper proposes a new verification method for the certification of generative models. Generative networks are models that can be used for specifying visual transformations. The authors propose two verification methods to certify generative networks. The first one is APPROXLINE, which is a verifier that can certify the non-convexity of a network’s latent space. The second one is a deterministic and probabilistic abstract interpretation of the generative network. "
22275,SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"deep architectures USED-FOR models. spectral graph convolutional operator USED-FOR graph neural networks ( GNNs ). GNNs USED-FOR suspended animation problem. GNNs USED-FOR suspended animation problem. nodes ’ raw features CONJUNCTION intermediate representations. intermediate representations CONJUNCTION nodes ’ raw features. intermediate representations PART-OF graph. intermediate representations USED-FOR model layers. GRESNET ( Graph Residual Network ) framework USED-FOR connected highways. norm preservation perspective USED-FOR graph residual terms. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. GAT CONJUNCTION LOOPYNET. LOOPYNET CONJUNCTION GAT. GRESNET framework USED-FOR GNNs. LOOPYNET HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. GAT HYPONYM-OF GNNs. Generic are problem, model, and learning settings. OtherScientificTerm are model depth, suspended animation limit, and node ’s representations. Material are graph data, and real - world benchmark datasets. Method is residual learning methods. ","This paper proposes a spectral graph convolutional operator for graph neural networks (GNNs). GNNs are used to solve the suspended animation problem, where the model depth is bounded by a suspended animation limit. The authors propose a new GRESNET (Graph Residual Network) framework to learn connected highways between nodes and intermediate representations in a graph, which are then used to train the model layers. The graph residual terms are learned from a norm preservation perspective, and the model is trained using residual learning methods. Experiments are conducted on graph data, and real-world benchmark datasets. ","This paper proposes a spectral graph convolutional operator for graph neural networks (GNNs). GNNs are used to solve the suspended animation problem, where the model depth is bounded by a suspended animation limit. The authors propose to use deep architectures to train the models. The proposed GRESNET (Graph Residual Network) framework is used to learn connected highways between the nodes’ raw features and intermediate representations of the model layers. The graph residual terms are learned from a norm preservation perspective. Experiments are conducted on both graph data and real-world benchmark datasets. Results show that the proposed residual learning methods outperform the state-of-the-art in both learning settings. "
22284,SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"face prior knowledge USED-FOR reconstruction process. limited scan data USED-FOR face prior knowledge. linear 3D morphable models ( 3DMM ) HYPONYM-OF face prior knowledge. limited scan data USED-FOR linear 3D morphable models ( 3DMM ). face prior knowledge USED-FOR ambiguity. expressions CONJUNCTION poses. poses CONJUNCTION expressions. poses CONJUNCTION lightings. lightings CONJUNCTION poses. expressions FEATURE-OF facial images. poses FEATURE-OF facial images. linear parametric models USED-FOR methods. convolutional neural networks ( CNN ) USED-FOR nonlinear parametric model. linear 3DMM USED-FOR dataset. dataset USED-FOR models. identity and expression representations PART-OF models. semi - supervised manner FEATURE-OF adversarial loss. unconstrained photo collections USED-FOR unlabeled face images. adversarial loss USED-FOR model. semi - supervised manner USED-FOR model. hybrid batches of unlabeled and labeled face images USED-FOR model. identity shape FEATURE-OF facial images. expression CONJUNCTION pose. pose CONJUNCTION expression. identity CONJUNCTION expression. expression CONJUNCTION identity. pose CONJUNCTION lighting representations. lighting representations CONJUNCTION pose. model USED-FOR facial editing applications. model USED-FOR expression. model USED-FOR identity. model USED-FOR lighting representations. expression transfer HYPONYM-OF facial editing applications. model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE model. model USED-FOR expression. Task are Recovering 3D geometry shape, ill - posed problem, and reconstruction. OtherScientificTerm is center loss. ","This paper proposes a novel reconstruction process based on face prior knowledge from limited scan data. The authors propose to use linear 3D morphable models (3DMM) to learn face pre-trained on limited scan images. The proposed method is based on convolutional neural networks (CNN) and uses a nonlinear parametric model trained on hybrid batches of unlabeled and labeled face images from unconstrained photo collections. The model is trained in a semi-supervised manner with an adversarial loss in the form of a center loss. The dataset is used to train the models and the identity and expression representations of the models. The models are evaluated on a variety of facial images (identity, pose, lightings, and expressions). The results show that the proposed model is able to learn the identity, the pose, and the lighting representations of a given facial image, and can transfer the model to facial editing applications such as expression transfer. ","This paper proposes a novel reconstruction process based on face prior knowledge from limited scan data. The paper introduces linear 3D morphable models (3DMM) which are based on linear parametric models. The authors use convolutional neural networks (CNN) to train the nonlinear parametric model. The proposed models are trained on a dataset of unlabeled face images from unconstrained photo collections. The model is trained in a semi-supervised manner in which the adversarial loss is learned in a center loss, and the identity and expression representations of the models are learned in an unsupervised way. Experiments show that the proposed model is able to learn identity, pose, and lighting representations of facial images with identity shape and poses. The models are also used for facial editing applications such as expression transfer. "
22293,SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"transition kernel USED-FOR Model - based imitation learning methods. partial knowledge FEATURE-OF transition kernel. Reinforcement Learning ( RL ) USED-FOR imitation problems. unknown transition kernel CONJUNCTION synthetic kernel. synthetic kernel CONJUNCTION unknown transition kernel. synthetic kernel USED-FOR transition of state components. kernel USED-FOR state components. transition kernel USED-FOR transition of state components. multiplayer games FEATURE-OF imitation tasks. policy gradient algorithm CONJUNCTION model. model CONJUNCTION policy gradient algorithm. policy gradient algorithm COMPARE simulation - free alternative. simulation - free alternative COMPARE policy gradient algorithm. model COMPARE simulation - free alternative. simulation - free alternative COMPARE model. Task is policy evaluation. Method are eMDP, and transition model. Generic is components. OtherScientificTerm is sr. ","This paper proposes a new transition kernel for Model-based imitation learning methods. The transition kernel is based on partial knowledge of the state components of an unknown transition kernel. The authors use Reinforcement Learning (RL) to solve imitation problems in multiplayer games, where the goal is to learn a policy evaluation. The key idea of the transition model is that the transition kernel can be used to learn the transition of state components. The paper shows that the policy gradient algorithm and the model trained with the proposed transition model outperform a simulation-free alternative. ",This paper proposes a new transition kernel for Model-based imitation learning methods. The transition kernel is based on partial knowledge. The authors show that the transition kernel can be used for imitation problems in Reinforcement Learning (RL) where the goal is to learn a policy evaluation. The main contribution of the paper is that the authors propose to use an unknown transition kernel and a synthetic kernel for the transition of state components of the eMDP. The key idea of the transition model is to combine the two components. The paper shows that the proposed policy gradient algorithm and the model outperforms the simulation-free alternative. 
22302,SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"manually - designed reward function USED-FOR Learning useful skills. OpenAI Gym CONJUNCTION navigation task. navigation task CONJUNCTION OpenAI Gym. OpenAI Gym FEATURE-OF simulated robotic manipulation tasks. navigation task HYPONYM-OF simulated robotic manipulation tasks. Gazebo simulator USED-FOR navigation task. navigation task EVALUATE-FOR approach. simulated robotic manipulation tasks EVALUATE-FOR approach. intrinsic mutual information rewards USED-FOR method. mutual information discriminator USED-FOR learning. pre - trained policy USED-FOR learning. pre - trained policy CONJUNCTION mutual information discriminator. mutual information discriminator CONJUNCTION pre - trained policy. learning USED-FOR task rewards. mutual information discriminator USED-FOR task rewards. pre - trained policy USED-FOR task rewards. sparse rewards USED-FOR robotic manipulation tasks. Method are reinforcement learning, and selfsupervised Reinforcement Learning approach. OtherScientificTerm are external reward function, intrinsic objective, context states, robot states, states of interest, and mutual information. ","This paper proposes a selfsupervised Reinforcement Learning approach to learn useful skills using a manually-designed reward function for reinforcement learning. The proposed method is based on intrinsic mutual information rewards, where the external reward function is the intrinsic objective and the goal is to maximize the mutual information between the agent and the environment. The authors demonstrate the effectiveness of the proposed approach on simulated robotic manipulation tasks such as OpenAI Gym and a navigation task on the Gazebo simulator. They also show that learning with a pre-trained policy and a mutual information discriminator can improve the performance of learning for task rewards. ","This paper proposes a selfsupervised Reinforcement Learning approach to learn useful skills using a manually-designed reward function for reinforcement learning. The proposed method is based on intrinsic mutual information rewards, where the external reward function is the intrinsic objective, and the internal objective is a set of context states. The key idea is to learn a pre-trained policy and a mutual information discriminator to guide the learning of task rewards. The authors evaluate the proposed approach on simulated robotic manipulation tasks such as OpenAI Gym and a navigation task on the Gazebo simulator. The results show that the proposed method outperforms the state-of-the-art methods in terms of learning of the context states, and also outperforms other methods on the navigation task. "
22311,SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"Neural network ( NN ) trojaning attack HYPONYM-OF attack. adversarial attacks COMPARE it. it COMPARE adversarial attacks. it USED-FOR malicious functionality. small datasets USED-FOR NN trojaning attacks. generality CONJUNCTION stealthiness. stealthiness CONJUNCTION generality. trojaning attack method USED-FOR large models. capability CONJUNCTION generality. generality CONJUNCTION capability. trojaning attack method COMPARE studies. studies COMPARE trojaning attack method. generality EVALUATE-FOR studies. capability EVALUATE-FOR studies. stealthiness EVALUATE-FOR studies. trojaning attack USED-FOR small domain. large - scale dataset USED-FOR trojaned model. biased behavior FEATURE-OF trojan. Method is NN models. OtherScientificTerm are weight parameters, fixed target classes, and malicious misclassification target. ","This paper proposes a new attack against neural network (NN) trojaning attacks. The attack is based on the idea that the weight parameters of NN models can be misclassified as fixed target classes. The authors show that it can be used to improve the malicious functionality of the NN trojaned attacks on small datasets, and that it is more robust than adversarial attacks on large models. They also show that the trojaned model trained on a large-scale dataset has biased behavior towards the target class. They show that this can be achieved by using a trojan attack on a small domain, and then using the trojans attack on the large domain. Finally, they show that their trojanking attack method can improve the performance of large models in terms of capability and stealthiness. ","This paper proposes a novel attack on neural network (NN) trojaning attack, which is a variant of the attack on NN models. The authors show that it is more robust to malicious functionality than adversarial attacks on small datasets. They also show that the trojaned model can be trained on a large-scale dataset with fixed target classes, and that it can be used to improve the generalization ability of large models. They show that a trojaned attack can improve the performance of a small domain in the presence of biased behavior of a trojan. They further show that this can be combined with other studies on the generality and stealthiness of NN, showing that it improves the performance on large models in the absence of malicious misclassification target."
22320,SP:35ea626ee4dd1a7a368a660eb852192924966b7f,prediction tasks PART-OF drug 1 discovery. few - shot regression ( FSR ) problems USED-FOR prediction tasks. modelling of biological assays HYPONYM-OF few - shot regression ( FSR ) problems. reinforcement learning methods USED-FOR applications. few - shot classification USED-FOR applications. few - shot classification CONJUNCTION reinforcement learning methods. reinforcement learning methods CONJUNCTION few - shot classification. FSR methods USED-FOR tasks. real - world constraints FEATURE-OF tasks. deep kernel learning USED-FOR FSR 6 algorithm. kernel function CONJUNCTION differentiable kernel 8 algorithm. differentiable kernel 8 algorithm CONJUNCTION kernel function. deep network CONJUNCTION kernel function. kernel function CONJUNCTION deep network. deep network CONJUNCTION differentiable kernel 8 algorithm. differentiable kernel 8 algorithm CONJUNCTION deep network. deep network PART-OF algorithm. differentiable kernel 8 algorithm PART-OF algorithm. kernel function PART-OF algorithm. kernel USED-FOR task. algorithm USED-FOR kernel. kernel USED-FOR inference. task PART-OF inference. algorithm USED-FOR task. It COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE It. It USED-FOR complex task distributions. real - world benchmarks EVALUATE-FOR state - of - the - art algorithms. real - world benchmarks EVALUATE-FOR It. FSR algorithms USED-FOR noisy and uncertain environments. biological assays USED-FOR benchmarks. drug discovery HYPONYM-OF noisy and uncertain environments. Task is data generation. ,"This paper studies few-shot regression (FSR) problems in the context of drug 1 discovery, where the goal is to find a drug that can be used for drug discovery. The authors propose a new FSR 6 algorithm based on deep kernel learning and a differentiable kernel 8 algorithm. The proposed algorithm is able to learn a kernel for each task, which is then used for inference. It is shown to outperform state-of-the-art algorithms on a variety of real-world benchmarks. ","This paper studies few-shot regression (FSR) problems for prediction tasks in drug 1 discovery and modelling of biological assays. The authors propose a new FSR 6 algorithm based on deep kernel learning and a differentiable kernel 8 algorithm. They show that the proposed algorithm outperforms state-of-the-art algorithms on both real-world benchmarks as well as on a set of synthetic datasets. They also show that it can learn complex task distributions that are more suitable for data generation. The applications of the proposed FSR methods are applied to a variety of tasks with real-real-world constraints. The experiments are conducted on both noisy and uncertain environments such as drug discovery, and on a number of different benchmarks. "
22329,SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"Neural networks USED-FOR reasoning tasks. Graph Neural Networks ( GNNs ) USED-FOR tasks. Graph Neural Networks ( GNNs ) HYPONYM-OF network structures. network structures USED-FOR tasks. expressive power FEATURE-OF they. framework USED-FOR reasoning tasks. computation structure COMPARE algorithmic structure. algorithmic structure COMPARE computation structure. algorithmic structure FEATURE-OF reasoning process. network USED-FOR reasoning tasks. sample complexity bound EVALUATE-FOR alignment. framework EVALUATE-FOR reasoning models. visual question answering CONJUNCTION shortest paths. shortest paths CONJUNCTION visual question answering. intuitive physics CONJUNCTION visual question answering. visual question answering CONJUNCTION intuitive physics. dynamic programming ( DP ) HYPONYM-OF algorithmic paradigm. dynamic programming ( DP ) USED-FOR reasoning tasks. algorithmic paradigm USED-FOR reasoning tasks. intuitive physics HYPONYM-OF reasoning tasks. shortest paths HYPONYM-OF reasoning tasks. visual question answering HYPONYM-OF reasoning tasks. GNNs USED-FOR tasks. GNNs COMPARE DP. DP COMPARE GNNs. reasoning tasks EVALUATE-FOR theory. Method is structured networks. OtherScientificTerm are network structure, and algorithmic alignment. ","This paper proposes a new framework for reasoning tasks using Graph Neural Networks (GNNs) for tasks where the network structure is different from the original network structure. The authors show that GNNs have expressive power, and that they can be used to learn tasks with different network structures. They also provide a sample complexity bound for the alignment between the network and the original neural network. Finally, the authors propose a new algorithmic paradigm called dynamic programming (DP) that can be applied to reasoning tasks such as visual question answering, shortest paths, and intuitive physics. The experimental results show that DP is able to achieve better performance than the state-of-the-art on these reasoning tasks. ","This paper proposes a new framework for learning reasoning tasks using Graph Neural Networks (GNNs) for tasks with different network structures. The authors show that GNNs have expressive power in terms of the number of neurons and their expressive power for different tasks. They also show that the computation structure of a GNN is similar to the algorithmic structure of the reasoning process, and that the network structure can be used for different reasoning tasks (e.g., intuitive physics, visual question answering, shortest paths). The authors propose a new algorithmic paradigm called dynamic programming (DP) for reasoning tasks, and show that DP is able to learn reasoning models with a higher sample complexity bound. The paper also shows that the proposed framework can be applied to other reasoning models. "
22338,SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,conditional consistency CONJUNCTION intra - conditioning diversity. intra - conditioning diversity CONJUNCTION conditional consistency. image quality CONJUNCTION conditional consistency. conditional consistency CONJUNCTION image quality. conditional consistency HYPONYM-OF metrics. metrics EVALUATE-FOR models. it USED-FOR properties. Fréchet distance FEATURE-OF joint distributions. metric USED-FOR it. metric USED-FOR properties. controllable synthetic dataset EVALUATE-FOR FJD. FJD COMPARE metrics. metrics COMPARE FJD. bounding boxes CONJUNCTION images. images CONJUNCTION bounding boxes. object masks CONJUNCTION bounding boxes. bounding boxes CONJUNCTION object masks. images CONJUNCTION text captions. text captions CONJUNCTION images. class labels CONJUNCTION object masks. object masks CONJUNCTION class labels. metric EVALUATE-FOR cGAN - based models. cGAN - based models USED-FOR conditioning modalities. text captions HYPONYM-OF conditioning modalities. images HYPONYM-OF conditioning modalities. class labels HYPONYM-OF conditioning modalities. bounding boxes HYPONYM-OF conditioning modalities. object masks HYPONYM-OF conditioning modalities. cGAN benchmarking CONJUNCTION model selection. model selection CONJUNCTION cGAN benchmarking. metric USED-FOR cGAN benchmarking. metric USED-FOR model selection. FJD USED-FOR metric. FJD USED-FOR cGAN benchmarking. Method is Conditional Generative Adversarial Networks ( cGANs ). Task is model benchmarking. Metric is Fréchet Joint Distance ( FJD ). OtherScientificTerm is conditioning. ,"This paper studies the problem of model benchmarking in Conditional Generative Adversarial Networks (cGANs). The authors propose a new metric called Fréchet Joint Distance (FJD) to measure the distance between the joint distributions of a pair of cGAN-based models. The authors show that FJD is a better metric than existing metrics for measuring the properties of models, such as image quality, conditional consistency, intra-conditioning diversity, and object masks. They also show that the FJD can be used for model selection, cGAN benchmarking, and model selection on a controllable synthetic dataset. ","This paper proposes a new metric to measure the Fréchet Joint Distance (FJD) of Conditional Generative Adversarial Networks (cGANs) for model benchmarking. The authors show that FJD is better than other metrics for measuring the properties of models such as image quality, conditional consistency, intra-conditioning diversity, and object masks. They also show that it can be used to measure joint distributions of the joint distributions. They evaluate FJD on a controllable synthetic dataset, and compare it to other metrics. They show that the proposed metric improves the performance of cGAN-based models for conditioning modalities such as bounding boxes, images, text captions, object masks, and class labels. The proposed metric is also used for cGAN benchmarking and model selection. "
22347,SP:fa822e8472efae17c7dfde8258057898383ecbbb,"decision states USED-FOR exploration. exploration USED-FOR downstream goal - driven tasks. partially observable environments FEATURE-OF downstream goal - driven tasks. Method is VIC framework. OtherScientificTerm are empowerment objective, and extrinsic rewards. Task is identification of decision states. ",This paper proposes a VIC framework for exploration in partially observable environments. The goal is to identify decision states for exploration on downstream goal-driven tasks that are partially observable. The empowerment objective is based on the notion of extrinsic rewards. The authors show that the identification of decision states leads to better exploration. ,This paper proposes a new VIC framework for exploration in partially observable environments. The main idea is to use the empowerment objective to encourage exploration in the context of extrinsic rewards. The identification of decision states for exploration is also a key part of the paper. Experiments are conducted on two downstream goal-driven tasks.
22356,SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"architectures USED-FOR irregularly - sampled and asynchronous time series. real - world datasets FEATURE-OF irregularly - sampled and asynchronous time series. healthcare applications HYPONYM-OF real - world datasets. framework USED-FOR classifying irregularly sampled time series. data efficiency EVALUATE-FOR framework. unaligned measurements USED-FOR irregularly sampled time series. method COMPARE competitors. competitors COMPARE method. runtime EVALUATE-FOR it. healthcare time series datasets EVALUATE-FOR competitors. healthcare time series datasets EVALUATE-FOR method. Method are deep neural networks, and differentiable set function learning. Task is online monitoring scenarios. ",This paper proposes a new architecture for irregularly-sampled and asynchronous time series on real-world datasets such as healthcare applications. The framework is based on unaligned measurements for classifying irregularly sampled time series. The authors show that the proposed method can achieve better data efficiency than competitors on healthcare time series datasets. The proposed method is also able to learn deep neural networks with differentiable set function learning. Experiments are conducted on online monitoring scenarios.,"This paper proposes a new architecture for classifying irregularly-sampled and asynchronous time series on real-world datasets (e.g., healthcare applications). The framework is based on deep neural networks. The authors propose to use differentiable set function learning, and use unaligned measurements for irregularly sampled time series to improve the data efficiency. The proposed method is evaluated on several healthcare time series datasets, and it is shown to achieve better runtime than competitors. The paper is also evaluated on online monitoring scenarios."
22365,SP:4ae89d64460b08749acc192004545c1fa8b7553b,Convolutional neural networks ( CNNs ) USED-FOR image recognition. inductive biases USED-FOR natural image priors. inductive biases FEATURE-OF CNNs. deep networks USED-FOR audio signals. inductive biases FEATURE-OF audio signals. inductive biases FEATURE-OF deep networks. network architectures USED-FOR audio processing. local neighborhoods USED-FOR convolutional kernels. harmonic series USED-FOR kernels. networks USED-FOR audio priors. networks USED-FOR unsupervised audio restoration. Harmonic Convolution USED-FOR networks. Harmonic Convolution USED-FOR they. they USED-FOR supervised musical source separation. Harmonic Convolution USED-FOR supervised musical source separation. generalization EVALUATE-FOR supervised musical source separation. generalization EVALUATE-FOR they. Generic is priors. OtherScientificTerm is harmonic structure. ,"This paper studies the inductive bias of convolutional neural networks (CNNs) for image recognition. The authors show that deep networks have inductive biases for natural image priors, and that they can also be used for audio signals. The main contribution of the paper is to show that they are able to perform supervised musical source separation with Harmonic Convolution, and they can be used in unsupervised audio restoration.    The authors also show that the local neighborhoods can be learned by convolutionally kernels with harmonic series, which is an interesting observation. ","This paper studies the inductive bias of convolutional neural networks (CNNs) for image recognition. The authors show that inductive biases for natural image priors are similar to those of deep networks for audio signals. They also show that they can be used for supervised musical source separation using Harmonic Convolution, and they are also used for unsupervised audio restoration. They show that the generalization of these networks to audio priors is similar to that of the priors in the harmonic structure. They further show that these network architectures can be applied to audio processing, and that they are able to achieve better generalization than the classical priors. "
22374,SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"specialized hardware accelerators USED-FOR neural network training. GPUs USED-FOR neural network training. GPUs CONJUNCTION specialized hardware accelerators. specialized hardware accelerators CONJUNCTION GPUs. disk I / O CONJUNCTION data preprocessing. data preprocessing CONJUNCTION disk I / O. workloads EVALUATE-FOR data echoing algorithms. data echoing algorithm COMPARE baseline. baseline COMPARE data echoing algorithm. upstream computation USED-FOR data echoing algorithm. upstream computation USED-FOR baseline. wall - clock time FEATURE-OF ResNet-50. ImageNet FEATURE-OF ResNet-50. Task are training pipeline, and training. OtherScientificTerm are accelerators, echoing, and batch sizes. Method are data echoing, pipeline stages, Data echoing, and network. Metric is training time. ","This paper studies the problem of data echoing in deep neural networks. The authors propose a new training pipeline, called ResNet-50, which uses a combination of GPUs and specialized hardware accelerators for neural network training. They show that the proposed data echoing algorithm performs better than the baseline on a variety of workloads, including disk I/O, data preprocessing, and data streaming. They also provide a theoretical analysis of the performance of the proposed training pipeline.","This paper proposes a new training pipeline for neural network training with GPUs and specialized hardware accelerators. The authors propose a new data echoing, which is a combination of two pipeline stages: (1) the initial training stage, where the network is trained on the input data, and (2) the final training stage where the training pipeline is updated on the output data. They show that the proposed data echoing algorithm outperforms the baseline in terms of wall-clock time on ResNet-50 and ImageNet. They also show that their data echoing algorithms outperform the baseline on different workloads, including disk I/O and data preprocessing. The main contribution of the paper is that the authors propose to use the idea of data echoing in the final stage of the training, rather than in the first stage. The paper also shows that the performance of the data echoing is better than the baseline when the number of batch sizes is small."
22383,SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"policy COMPARE policies. policies COMPARE policy. controllable subspace FEATURE-OF Markov decision process. Markov decision process FEATURE-OF behaviors. controllable subspace FEATURE-OF behaviors. Successor features USED-FOR generalization problem. grounded feature space FEATURE-OF reward function. generalization CONJUNCTION task inference. task inference CONJUNCTION generalization. algorithm USED-FOR generalization. algorithm USED-FOR task inference. Variational Intrinsic Successor FeatuRes ( VISR ) USED-FOR task inference. Variational Intrinsic Successor FeatuRes ( VISR ) HYPONYM-OF algorithm. successor features framework USED-FOR task inference. Atari suite EVALUATE-FOR VISR. human - level performance EVALUATE-FOR VISR. Generic are formulation, tasks, techniques, and method. OtherScientificTerm are controllable features, and limited feedback. ","This paper studies the problem of generalization in a Markov decision process with controllable subspace, where the goal is to learn a policy that performs better than other policies. Successor features are used to solve the generalization problem in the grounded feature space of the reward function. The authors propose a new algorithm called Variational Intrinsic Successor FeatuRes (VISR) for task inference based on the successor features framework. The proposed method is evaluated on the Atari suite and achieves state-of-the-art human-level performance. ","This paper proposes a new formulation of the generalization problem in the context of the controllable subspace of the Markov decision process. Successor features are used to solve the problem of generalization in the grounded feature space of the reward function. The authors propose a new algorithm, Variational Intrinsic Successor FeatuRes (VISR) for task inference based on the successor features framework. The proposed method is evaluated on the Atari suite and shows improved generalization and task inference with human-level performance. "
22392,SP:83500230586a9134f910ad067b7233dc563dc1ba,"functional view USED-FOR networks. functional view USED-FOR them. smoothness of the functional approximation CONJUNCTION flat initial approximation. flat initial approximation CONJUNCTION smoothness of the functional approximation. smoothness of the functional approximation USED-FOR generalization. flat initial approximation USED-FOR generalization. Method are deep neural networks, functional view of these networks, and massively overparamaterized networks. OtherScientificTerm are initializations, loss surface, and smoothness. ",This paper proposes a new functional view of deep neural networks. The authors show that the smoothness of the functional approximation and flat initial approximation can be used to improve generalization. They also show that these two initializations can be combined to improve the generalization performance of the network. The paper also shows that the functional view can be applied to massively overparameterized networks.,"This paper proposes a new functional view of deep neural networks, which is a functional view on the networks. The authors show that the smoothness of the functional approximation and the flat initial approximation can be used for generalization. They also show that these initializations do not change the loss surface of the network. They show that massively overparameterized networks can be learned with the functional view. "
22401,SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"Generative Adversarial Network ( GAN ) USED-FOR image - to - image translation problem. imbalance problem FEATURE-OF GAN - based methods. mode collapse CONJUNCTION diminished gradients. diminished gradients CONJUNCTION mode collapse. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. relative model capacities FEATURE-OF generator. relative model capacities FEATURE-OF discriminator. attention mechanism USED-FOR GuideGAN. it USED-FOR attention map. attention mechanism USED-FOR discriminator. attention map USED-FOR generator. image transfer tasks EVALUATE-FOR GuideGAN framework. Task are supervised and unsupervised manner, and prediction. Generic is approach. ","This paper studies the problem of image-to-image translation in a supervised and unsupervised manner. The authors propose a generative Adversarial Network (GAN) to solve the problem. The proposed approach is based on the notion of imbalance problem in GAN-based methods, which is the imbalance between mode collapse and diminished gradients. The generator and discriminator are trained with relative model capacities, and the generator is trained with an attention mechanism that learns the attention map of the generator and the discriminator. Experiments on image transfer tasks demonstrate the effectiveness of the proposed GuideGAN framework.","This paper proposes a generative Adversarial Network (GAN) for the image-to-image translation problem. The proposed approach is motivated by the imbalance problem of GAN-based methods, where mode collapse and diminished gradients are observed in both the supervised and unsupervised manner. The generator and discriminator are trained with relative model capacities, and the attention mechanism of the GuideGAN is used to learn the attention map of the generator. Experiments on image transfer tasks demonstrate the effectiveness of the proposed GuideGAN framework. "
22410,SP:41c089ba65393174dae1dc136f79030a0a4fc532,"attention layers CONJUNCTION hypernetworks. hypernetworks CONJUNCTION attention layers. hypernetworks CONJUNCTION dynamic convolutions. dynamic convolutions CONJUNCTION hypernetworks. multiplicative interaction USED-FOR neural network architectural motifs. gating CONJUNCTION attention layers. attention layers CONJUNCTION gating. dynamic convolutions HYPONYM-OF neural network architectural motifs. hypernetworks HYPONYM-OF neural network architectural motifs. attention layers HYPONYM-OF neural network architectural motifs. gating HYPONYM-OF neural network architectural motifs. Multiplicative interaction layers HYPONYM-OF primitive operations. multiplicative interactions USED-FOR inductive bias. multiplicative interactions USED-FOR neural network architectures. them USED-FOR multiplicative interactions. Generic are layers, and they. Method is neural networks. OtherScientificTerm are conditional computation, and concatenation operation. ","This paper studies the problem of inductive bias in neural networks. The authors propose a multiplicative interaction for neural network architectural motifs such as gating, attention layers, hypernetworks, and dynamic convolutions. Multiplicative interaction layers are primitive operations in neural network architectures, and the authors propose to use them to learn multiplicative interactions between layers. They also propose a conditional computation for the concatenation operation.","This paper proposes a new way of modeling neural networks. The main idea is to use multiplicative interaction in neural network architectural motifs such as attention layers, hypernetworks, dynamic convolutions, and gating. Multiplicative interaction layers are primitive operations that can be applied to any neural network architectures. The authors show that they can be used to improve the inductive bias of neural networks by using them for multiplicative interactions. They also show that conditional computation can be performed in the concatenation operation."
22419,SP:5144391584e6d3825e12684b7c053e4e282cff2b,"algorithm USED-FOR batch active learning. deep neural network models USED-FOR algorithm. deep neural network models USED-FOR batch active learning. predictive uncertainty CONJUNCTION sample diversity. sample diversity CONJUNCTION predictive uncertainty. algorithm USED-FOR Batch Active learning. predictive uncertainty PART-OF strategy. Diverse Gradient Embeddings ( BADGE ) USED-FOR Batch Active learning. uncertainty CONJUNCTION diversity. diversity CONJUNCTION uncertainty. diversity EVALUATE-FOR BADGE. uncertainty EVALUATE-FOR BADGE. it USED-FOR real world active learning problems. approaches COMPARE BADGE. BADGE COMPARE approaches. approaches USED-FOR batch sizes. BADGE USED-FOR real world active learning problems. OtherScientificTerm are hallucinated gradient space, and hand - tuned hyperparameters. ","This paper proposes a new algorithm for batch active learning using deep neural network models. The proposed algorithm, Diverse Gradient Embeddings (BADGE), combines predictive uncertainty and sample diversity to improve the performance of Batch Active learning. The authors show that BADGE improves the performance in terms of uncertainty and diversity in the hallucinated gradient space, and it can be applied to real world active learning problems with different batch sizes. ","This paper proposes a novel algorithm for batch active learning with deep neural network models. The proposed algorithm, Diverse Gradient Embeddings (BADGE), combines predictive uncertainty and sample diversity to improve the performance of Batch Active learning. The authors show that BADGE improves the diversity and uncertainty of BADGE in terms of the hallucinated gradient space, and that it can be applied to real world active learning problems with different batch sizes. "
22428,SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"models USED-FOR decision making parameters. obscure feature extraction CONJUNCTION transformation process. transformation process CONJUNCTION obscure feature extraction. complex architectures CONJUNCTION obscure feature extraction. obscure feature extraction CONJUNCTION complex architectures. non - linearity FEATURE-OF activation functions. low level features CONJUNCTION high level features. high level features CONJUNCTION low level features. low level features FEATURE-OF hidden layer. high level features FEATURE-OF hidden layer. feature leveling architecture USED-FOR low level features. low level features CONJUNCTION high level features. high level features CONJUNCTION low level features. GLM layer PART-OF architecture. GLM layer USED-FOR feature leveling architecture. per - layer basis USED-FOR high level features. models COMPARE main - stream architectures. main - stream architectures COMPARE models. Method are Self - explaining models, General Linear Models ( GLMs ), deep neural networks ( DNNs ), and deep architectures. Task is model reasoning process. OtherScientificTerm is model weights. ","This paper studies the problem of self-explaining models for decision making parameters. The authors propose a new architecture, called General Linear Models (GLMs), which uses a feature leveling architecture to extract low level features and high level features from the hidden layer of a GLM layer in a deep neural networks (DNNs). The main idea is to use a per-layer basis to learn the high-level features and the low-level ones from the GLM layers. The main contribution of the paper is to show that the non-linearity of activation functions in activation functions can be used to explain the model reasoning process. The paper also shows that the performance of the proposed models is comparable to main-stream architectures.","This paper proposes a new model for explaining the decision making parameters of Self-Explaining models. The main idea is to use General Linear Models (GLMs) to explain the activation functions of activation functions with non-linearity. The proposed architecture consists of a GLM layer and a hidden layer with low level features and high level features. The feature leveling architecture is based on a per-layer basis, where the low-level features are computed by the model reasoning process and the high-level feature is computed by using the model weights. Experiments show that the proposed models outperform other main-stream architectures and deep neural networks (DNNs)."
22437,SP:b70ceead1bf6c7dc684c74501716e7012b891022,"gradient cost FEATURE-OF softmax regression. uniform negative sampling USED-FOR scalable softmax approximation. training method USED-FOR gradient signal. adversarial model USED-FOR data distribution. adversarial model USED-FOR negative samples. negative samples USED-FOR training method. adversarial sampling mechanism USED-FOR negative samples. adversarial sampling USED-FOR gradient variance. large scale data sets EVALUATE-FOR competitive baselines. adversarial sampling mechanism USED-FOR gradient updates. training time EVALUATE-FOR competitive baselines. non - uniform sampling USED-FOR bias. Method are classifier, and extreme classification. Task is technology. OtherScientificTerm is slow convergence. Metric is signal - to - noise ratio. ","This paper studies the gradient cost of softmax regression with uniform negative sampling. The authors propose a training method to estimate the gradient signal from the data distribution using an adversarial model. The training method uses negative samples from the adversarial sampling mechanism to reduce the gradient variance of the classifier. The bias is reduced by using non-uniform sampling, and the authors show that the gradient updates are competitive in terms of training time on large scale data sets. ",This paper studies the gradient cost of softmax regression with uniform negative sampling for scalable softmax approximation. The main contribution of the paper is to propose a training method to estimate the gradient signal from the data distribution using an adversarial model. The training method uses negative samples from the adversarial sampling mechanism to reduce the gradient variance of the classifier. The authors show that the bias of the bias is due to non-uniform sampling. The paper also shows that the training time of competitive baselines on large scale data sets with high training time is much faster than that of the standard training time. 
22446,SP:29b52fee83309268d9864f3b1fc3617948577d41,approach USED-FOR efficient exploration. lowdimensional encoding of the environment USED-FOR approach. modelbased and model - free objectives USED-FOR lowdimensional encoding of the environment. weighted distance of nearest neighbors USED-FOR novelty. intrinsic rewards USED-FOR novelty. low dimensional representational space FEATURE-OF weighted distance of nearest neighbors. weighted distance of nearest neighbors USED-FOR intrinsic rewards. intrinsic rewards USED-FOR approach. intrinsic rewards USED-FOR sample - efficient exploration. intrinsic rewards USED-FOR planning routines. representational space FEATURE-OF planning routines. planning routines USED-FOR sample - efficient exploration. exploration approach COMPARE baselines. baselines COMPARE exploration approach. maze tasks CONJUNCTION control problem. control problem CONJUNCTION maze tasks. control problem EVALUATE-FOR approach. maze tasks EVALUATE-FOR approach. Metric is model accuracy. ,This paper proposes a novel approach for efficient exploration in lowdimensional encoding of the environment using modelbased and model-free objectives. The novelty of the proposed approach is based on the weighted distance of nearest neighbors in the low dimensional representational space. The intrinsic rewards of the intrinsic rewards are used to encourage novelty and to encourage the exploration to be more efficient. The proposed approach uses intrinsic rewards to guide the planning routines in the representation space of planning routines. The experimental results on maze tasks and a control problem show that the proposed exploration approach performs better than baselines. ,This paper proposes a novel approach for efficient exploration based on lowdimensional encoding of the environment using modelbased and model-free objectives. The key idea is to use the weighted distance of nearest neighbors in the low dimensional representational space as a measure of novelty. The proposed approach uses intrinsic rewards to encourage intrinsic rewards for sample-efficient exploration and planning routines in a low dimensional representation space. Experiments on maze tasks and a control problem show that the proposed exploration approach outperforms other baselines in terms of model accuracy.
22455,SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,learning model USED-FOR few - shot classification. it USED-FOR out - of - distribution inputs. few - shot classification CONJUNCTION out - of - distribution detection. out - of - distribution detection CONJUNCTION few - shot classification. tasks USED-FOR outof - distribution detection. few - shot setting FEATURE-OF outof - distribution detection. few - shot classification datasets USED-FOR tasks. few - shot classification datasets USED-FOR benchmark datasets. methods USED-FOR task. benchmark datasets EVALUATE-FOR metrics. benchmark datasets EVALUATE-FOR baseline out - of - distribution detection. metrics USED-FOR baseline out - of - distribution detection. ,"This paper proposes a new learning model for few-shot classification. The main idea is to use it to learn out-of-distribution inputs, and then use it as a proxy for the true distribution of the data. The authors show that the proposed methods are able to perform well on a variety of tasks in the few-shooting setting, including few-shots classification, out- of-distributions detection, and out of distribution detection in the many-shot setting. They also show that their methods can be used to improve the performance of the task on a number of benchmark datasets. ","This paper proposes a learning model for few-shot classification, where it learns to predict out-of-distribution inputs. The authors show that it is able to perform better than existing methods for the task. The paper also shows that it can perform well in the few-shooting setting, and that it performs better in the out of-of-)distribution detection setting. The proposed tasks are evaluated on several benchmark datasets for outof-disentanglement detection, including few-shots classification, out- of-distortion detection, and few-Shot classification datasets for other tasks. "
22464,SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,question - answering CONJUNCTION natural language inference. natural language inference CONJUNCTION question - answering. Undirected neural sequence models USED-FOR discriminative natural language understanding tasks. BERT HYPONYM-OF discriminative natural language understanding tasks. BERT HYPONYM-OF Undirected neural sequence models. natural language inference HYPONYM-OF discriminative natural language understanding tasks. question - answering HYPONYM-OF discriminative natural language understanding tasks. monotonic generation USED-FOR directed sequence models. models USED-FOR generating sequences. decoding PART-OF directed and undirected models. decoding PART-OF generalized model of sequence generation. framework USED-FOR generation. framework USED-FOR neural sequence models. refinement - based non - autoregressive models HYPONYM-OF neural sequence models. autoregressive HYPONYM-OF neural sequence models. decoding algorithms USED-FOR directed sequence models. decoding algorithms USED-FOR undirected models. decoding strategies USED-FOR cross - lingual masked translation model. framework USED-FOR undirected sequence models. approach USED-FOR constant - time translation. approach COMPARE linear - time translation. linear - time translation COMPARE approach. constant - time translation COMPARE linear - time translation. linear - time translation COMPARE constant - time translation. model USED-FOR linear - time translation. Material is WMT’14 English - German translation. Method is autoregressive model. ,"This paper proposes a framework for training undirected neural sequence models, such as BERT, BERT+ and BERT+, for discriminative natural language understanding tasks such as question-answer and natural language inference. The authors propose to use monotonic generation to train directed sequence models with a cross-lingual masked translation model, and then use these models for generating sequences. They show that the proposed framework can be used for generation and decoding in both directed and undirecting models, and that the decoding in the generalized model of sequence generation can be combined with the decoding strategies in the autoregressive model. They also show that their approach is able to achieve better performance than linear-time translation, and can also perform better than the existing approach for constant-times translation. ","This paper proposes a framework for training undirected neural sequence models for discriminative natural language understanding tasks (e.g., question-answer, natural language inference). The authors propose a generalized model of sequence generation with decoding and monotonic generation. The authors also propose two decoding algorithms to train directed sequence models and two decoding strategies to train the cross-lingual masked translation model. The proposed approach is evaluated on WMT’14 English-German translation and compared to linear-time translation. The results show that the proposed framework is able to improve the performance of the trained and unsupervised neural sequence model compared to the autoregressive model."
22473,SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"real scenes USED-FOR MEs recognition. two - stage approach USED-FOR LaTeX sequence. printed mathematical expression image USED-FOR two - stage approach. method USED-FOR math symbols. object detection algorithm USED-FOR method. object detection algorithm USED-FOR math symbols. seq2seq model USED-FOR LaTeX sequences. attention mechanism USED-FOR seq2seq model. position information USED-FOR math symbols. two - stage method COMPARE end - to - end method. end - to - end method COMPARE two - stage method. ExpRate(expression recognition rate ) EVALUATE-FOR model. model COMPARE end - to - end model. end - to - end model COMPARE model. ExpRate(expression recognition rate ) EVALUATE-FOR end - to - end model. Task are mathematical expressions ( MEs ) recognition, and detection of mathematical symbols. Method is neutral network. OtherScientificTerm are mathematical symbols, and mathematical formulas. Metric are recognition accuracy, and generalization ability. ",This paper proposes a two-stage approach for LaTeX sequence recognition from a printed mathematical expression image. The method uses an object detection algorithm to learn the math symbols from position information. The seq2seq model is then used to generate LaTeX sequences using an attention mechanism. The model is evaluated on ExpRate(expression recognition rate) against an end-to-end method.,This paper proposes a two-stage approach for MEs recognition from real scenes. The first stage is to learn mathematical symbols from a printed mathematical expression image. The second stage is a seq2seq model that predicts the LaTeX sequences using an attention mechanism. The authors show that the proposed model achieves better ExpRate(expression recognition rate) than the end-to-end method for learning math symbols from position information. The paper also shows that the recognition accuracy is better than the generalization ability.
22482,SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"memory footprint FEATURE-OF convolutional network architectures. loss reconstruction error FEATURE-OF in - domain inputs. it USED-FOR in - domain inputs. it USED-FOR loss reconstruction error. bytealigned codebooks USED-FOR compressed weights. method USED-FOR inference. unlabelled data USED-FOR quantization time. CPU USED-FOR inference. bytealigned codebooks USED-FOR method. unlabelled data USED-FOR method. bytealigned codebooks USED-FOR inference. top-1 accuracy EVALUATE-FOR approach. Method are vector quantization method, ResNet-50 model, and Mask R - CNN. OtherScientificTerm is memory size. Metric is compression factor. Task is ImageNet object classification. ","This paper proposes a vector quantization method based on the ResNet-50 model. The proposed method uses bytealigned codebooks for compressed weights, and it uses unlabelled data to reduce the loss reconstruction error of in-domain inputs. The authors show that the proposed method improves the performance of inference using the CPU in terms of top-1 accuracy. The compression factor is also used to improve the memory footprint of convolutional network architectures. ","This paper proposes a vector quantization method for convolutional network architectures with memory footprint. The authors propose a ResNet-50 model, where the memory size is a function of the size of the network and the compression factor is the number of epochs. The proposed method is based on bytealigned codebooks for compressed weights, and it is shown to reduce the loss reconstruction error for in-domain inputs. The paper also shows that the proposed method can reduce the quantization time by using unlabelled data for inference on the CPU. The approach is evaluated on ImageNet object classification and achieves top-1 accuracy with Mask R-CNN."
22491,SP:74850ad70241948f93fed95ba1f0ac11360437c1,Tensor - Product Representations USED-FOR explicit representation of relation structure. Tensor - Product Representations PART-OF Transformer. free - form math wordproblems FEATURE-OF Mathematics Dataset. Mathematics Dataset EVALUATE-FOR TensorProduct Transformer ( TP - Transformer ). TP - Attention HYPONYM-OF attention mechanism. attention USED-FOR ambiguities. TP - Transformer ’s attention maps USED-FOR it. Generic is model. Task is representation - building. Method is Pretrained models. ,"This paper proposes a new attention mechanism, TP-Attention, which uses Tensor-Product Representations in the Transformer to generate an explicit representation of relation structure. The model is based on the TP-Transformer’s attention maps, and it is shown that the attention can be used to capture ambiguities in the representation-building. The paper also shows that the TensorProduct Transformer (TP-T) can be applied to free-form math wordproblems in the Mathematics Dataset. Pretrained models are also shown to be able to learn representations that are more interpretable.","This paper proposes a Transformer that uses Tensor-Product Representations to generate an explicit representation of relation structure. The model is based on the idea of representation-building. The authors show that the TensorProduct Transformer (TP-Transformer) outperforms Pretrained models on Mathematics Dataset with free-form math wordproblems. The attention mechanism, TP-Attention, is a simple extension of the attention maps of the original model. The paper also shows that it can be combined with “TP-transformer’s attention maps”."
22500,SP:d319df820c6630c409fab32097652a083e8f53ea,"identically distributed training and test sets EVALUATE-FOR Deep artificial neural networks. training and test accuracies EVALUATE-FOR Deep artificial neural networks. training and test sets COMPARE empirical sample set. empirical sample set COMPARE training and test sets. real - world input samples PART-OF empirical sample set. procedure USED-FOR source code. learning algorithm USED-FOR source code. procedure USED-FOR learning algorithm. Kolmogorov complexity USED-FOR universal cognitive similarity metric. information distance HYPONYM-OF universal cognitive similarity metric. optimization problem USED-FOR classification function. condition USED-FOR optimization problem. features USED-FOR classifier. empirical sample set FEATURE-OF training and test sets. model COMPARE model. model COMPARE model. model USED-FOR corruptions. corruptions CONJUNCTION adversarial perturbations. adversarial perturbations CONJUNCTION corruptions. corrupted or perturbed input features PART-OF empirical sample set. model USED-FOR adversarial perturbations. uncoded input features USED-FOR model. uncoded input features USED-FOR model. projected gradient descent USED-FOR adversarial perturbations. encoded input features USED-FOR model. Gaussian and shot noise HYPONYM-OF corruptions. Task are generalization, inference, and image classification. Metric is training and inference accuracies. OtherScientificTerm is channel codes. ","This paper studies the problem of generalization of Deep artificial neural networks with identically distributed training and test sets. The authors propose a procedure to learn the source code from the empirical sample set with real-world input samples. The learning algorithm is based on the Kolmogorov complexity of the universal cognitive similarity metric, the information distance between the channel codes. The optimization problem is formulated as a classification function with a condition on the features of the classifier. The model is trained with corrupted or perturbed input features and adversarial perturbations such as Gaussian and shot noise. The proposed model is able to recover the corrupted input features, and the model can also recover the corruptions with projected gradient descent. ","This paper studies the generalization performance of Deep artificial neural networks on identically distributed training and test sets. The authors show that the empirical sample set of real-world input samples is more robust to adversarial perturbations (e.g. Gaussian and shot noise) than the training and inference accuracies. The paper proposes a new universal cognitive similarity metric based on the Kolmogorov complexity of the channel codes. The proposed learning algorithm is based on a simple procedure to learn source code from source code. The optimization problem of the classification function is formulated as a condition on the features of the classifier. The model is trained with corrupted or perturbed input features and uncoded input features, and the model is used to detect corruptions and adversarial attacks. "
22509,SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,Deep Graph Neural Networks ( GNNs ) USED-FOR graph - based regression tasks. Deep Graph Neural Networks ( GNNs ) USED-FOR graph classification. graph classification CONJUNCTION graph - based regression tasks. graph - based regression tasks CONJUNCTION graph classification. graph pooling USED-FOR GNNs. GNNs USED-FOR graphs. HaarPooling HYPONYM-OF graph pooling operation. compressive Haar transforms USED-FOR graph pooling operation. sequential clusterings USED-FOR HaarPooling. compressive Haar basis USED-FOR clustering. compressive Haar basis USED-FOR pooling layer. HaarPooling USED-FOR fine detail information. compressive Haar transforms USED-FOR HaarPooling. synthesis of nodes USED-FOR HaarPooling. compressive Haar transforms USED-FOR fine detail information. transforms USED-FOR structure information. sparsity of the Haar basis USED-FOR HaarPooling. linear complexity FEATURE-OF HaarPooling. HaarPooling CONJUNCTION graph convolution layers. graph convolution layers CONJUNCTION HaarPooling. diverse graph classification problems EVALUATE-FOR GNN. graph convolution layers USED-FOR GNN. HaarPooling USED-FOR GNN. Generic is tasks. OtherScientificTerm is cluster. ,"This paper proposes a new graph pooling operation called HaarPooling, which is based on compressive Haar transforms to improve the performance of GNNs on graph classification and graph-based regression tasks. The key idea is to use the sparsity of the Haar basis for the pooling layer, and to use sequential clusterings for the clustering. The authors show that the resulting GNN achieves better performance on diverse graph classification problems than the state-of-the-art GNN. ","This paper proposes a graph pooling operation called HaarPooling, which is a variant of Deep Graph Neural Networks (GNNs) for graph-based regression tasks and graph classification. GNNs are trained on graphs. The authors propose to use sequential clusterings for clustering, and use a compressive Haar basis for the pooling layer. They also propose a novel synthesis of nodes for the Haarpooling operation. They show that the resulting GNN has linear complexity with respect to the number of nodes, and that the transforms for the fine detail information of the structure information can be obtained with the sparsity of the HaAR basis. They evaluate the proposed GNN on diverse graph classification problems."
22518,SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"Point clouds USED-FOR 3D objects. encoder networks USED-FOR semantics of their input point clouds. fully - connected networks USED-FOR shape representations. fully - connected networks USED-FOR point - cloud decoders. decoder architectures USED-FOR semantics of variable sized point clouds. sample - based point - cloud decoders USED-FOR shape representation. Metric is precision. OtherScientificTerm are point feature distribution, and sampled features. Method are sample - based decoder architectures, and feedforward architectures. ","This paper studies the problem of learning the shape representations of 3D objects from point clouds. The authors propose to use encoder networks to learn the semantics of their input point clouds, and then use fully-connected networks to generate shape representations using sample-based point-cloud decoders. The key idea is that the point feature distribution of the input point cloud is sampled from the sampled features, and that the precision of the decoder architectures can be improved by using these sampled features.  The authors show that the shape representation learned by sample- based point-Cloud decoder can be used to learn a better shape representation than the one learned by feedforward architectures. ","This paper proposes a new way to learn the shape representation of point clouds for 3D objects. The key idea is to use encoder networks to encode the semantics of their input point clouds, and then use fully-connected networks to generate shape representations from these point-cloud decoders. The paper shows that sample-based decoder architectures can be used to generate a shape representation from sampled features. The precision of the sampled features can be improved by using feedforward architectures."
22527,SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"controlled synthetic noise USED-FOR deep learning. controlled noise levels FEATURE-OF realworld noisy labels. Deep Neural Networks ( DNNs ) USED-FOR real - world noise. noisy data EVALUATE-FOR ImageNet architectures. Robust learning methods USED-FOR synthetic noise. Robust learning methods USED-FOR real - world noise. OtherScientificTerm are noise levels, networks, and Real - world noise. Material are benchmark of realworld noisy labels, and real - world noisy data. Method are DNNs, and robust DNN methods. ","This paper studies the problem of controlling synthetic noise in deep learning. The authors propose a new benchmark of realworld noisy labels, where the noise levels of the networks are controlled. They show that Deep Neural Networks (DNNs) can be used to learn real-world noise in the presence of controlled noise levels. Robust learning methods are also used to train synthetic noise.  The authors also show that robust DNN methods can be trained on noisy data. ","This paper studies the problem of controlled synthetic noise in deep learning. The authors propose to use Deep Neural Networks (DNNs) to reduce real-world noise in realworld noisy labels. They show that DNNs can reduce the noise levels of the networks. They also show that Robust learning methods can be used to reduce synthetic noise. The paper also shows that robust DNN methods can also reduce the synthetic noise, which is a nice contribution. "
22536,SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"pain - staking human supervision USED-FOR labeled data. rule - exemplar method USED-FOR collecting human supervision. it USED-FOR learning. training algorithm USED-FOR rules. coverage and label variables FEATURE-OF soft implication loss. latent coverage variables USED-FOR training algorithm. soft implication loss USED-FOR model. latent coverage variables USED-FOR rules. model USED-FOR inference. denoised rules CONJUNCTION model. model CONJUNCTION denoised rules. denoised rules USED-FOR inference. coupled rule - exemplar supervision USED-FOR denoising rules. algorithm COMPARE methods. methods COMPARE algorithm. tasks EVALUATE-FOR algorithm. tasks EVALUATE-FOR methods. clean and noisy supervision USED-FOR algorithm. clean and noisy supervision USED-FOR methods. OtherScientificTerm are human supervision, and supervision. ",This paper proposes a rule-exemplar method for collecting human supervision for labeled data. The training algorithm is based on a soft implication loss with coverage and label variables. The model is trained with latent coverage variables and denoised rules. The proposed algorithm is shown to outperform existing methods on a variety of tasks. The authors also show that the proposed algorithm can be combined with clean and noisy supervision.,"This paper proposes a new method for training a model that is able to learn a set of denoising rules from labeled data without human supervision. The idea is to use a rule-exemplar method for collecting human supervision, and then use it for learning the rules. The training algorithm is based on a soft implication loss that combines the coverage and label variables of the training algorithm with the latent coverage variables in the model. The authors show that the proposed algorithm outperforms other methods on a number of tasks. The proposed algorithm is evaluated on both clean and noisy supervision. "
22545,SP:6f2c656dbb7629f652a4291d6971625184d8118b,"Graph neural networks ( GNNs ) HYPONYM-OF deep models. memory layer USED-FOR GNNs. GNNs USED-FOR node representations. memory layer USED-FOR node representations. graph memory network ( GMN ) USED-FOR hierarchical graph representations. memory - based GNN ( MemGNN ) CONJUNCTION graph memory network ( GMN ). graph memory network ( GMN ) CONJUNCTION memory - based GNN ( MemGNN ). networks USED-FOR hierarchical graph representations. graph memory network ( GMN ) HYPONYM-OF networks. memory - based GNN ( MemGNN ) HYPONYM-OF networks. layer USED-FOR networks. graph classification and regression benchmarks EVALUATE-FOR models. chemical features PART-OF molecule data. chemical features FEATURE-OF representations. OtherScientificTerm are graphs, and graph. ","Graph neural networks (GNNs) are a class of deep models. GNNs are trained with a memory layer to learn node representations from graphs. The authors propose two networks, a memory-based GNN (MemGNN) and a graph memory network (GMN) to learn hierarchical graph representations. The paper shows that the proposed layer improves the performance of these networks on graph classification and regression benchmarks. The representations are based on chemical features in molecule data.","Graph neural networks (GNNs) are one of the most popular deep models. GNNs are able to generate node representations from chemical features in molecule data. The authors propose two networks, a graph memory network (GMN) and a memory-based GNN (MemGNN) that can generate hierarchical graph representations. The memory layer is used to train the networks. The proposed models are evaluated on graph classification and regression benchmarks. "
22554,SP:81bc52d734c86975d741b6482d65ca71a9d81620,"initial parameter values USED-FOR gradient - based optimization. convergence times CONJUNCTION model. model CONJUNCTION convergence times. gradient - based optimization USED-FOR deep neural networks. initialization USED-FOR deep linear networks. convergence COMPARE Gaussian initialization. Gaussian initialization COMPARE convergence. iid weights USED-FOR Gaussian initialization. Gaussian initializations USED-FOR convergence. initialization USED-FOR learning. dynamical isometry USED-FOR deep non - linear networks. Method are deep learning systems, initialization schemes, deep networks, and orthogonal initializations. OtherScientificTerm are orthogonal group, and global minimum. ","This paper studies the problem of initialization in deep learning systems. The authors propose a new initialization scheme, called orthogonal group initialization, which is based on the notion of orthogonality. The main idea is to learn a set of initial parameter values for gradient-based optimization for deep neural networks, which are then used to compute the convergence times of the model. The paper shows that the orthogonic group initialization can achieve better convergence than Gaussian initialization with iid weights, and that the global minimum of the global optimum is also better than the optimal global minimum. In addition, the authors also show that the resulting initialization can be used for learning deep non-linear networks with dynamical isometry. ",This paper proposes a novel initialization scheme for deep learning systems. The key idea is to use initial parameter values for gradient-based optimization for deep neural networks. The authors propose two initialization schemes: orthogonal group initialization and Gaussian group initialization. They show convergence times and a model for both deep networks and deep non-linear networks based on dynamical isometry. They also show that the convergence is better than Gaussian initialization with iid weights. 
22563,SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"quantization methods USED-FOR deep neural networks. coarse quantization USED-FOR layers. 2 - bit quantization HYPONYM-OF high rate compression. additivity property FEATURE-OF deep neural networks. method USED-FOR optimization problem. Lagrangian Formulation USED-FOR method. joint framework USED-FOR optimal bit allocation problem. Lagrangian Formulation USED-FOR optimization problem. deep neural networks EVALUATE-FOR method. It USED-FOR deep CNN ResNet-50. accuracy loss EVALUATE-FOR It. Metric is accuracy. Generic is methods. OtherScientificTerm are equal bit rate, quantization, and additivity of output error. Task is deep CNNs compression. Method is deep CNNs. ","This paper studies the problem of high rate compression with 2-bit quantization in deep neural networks. The authors propose a joint framework to solve the optimal bit allocation problem with Lagrangian Formulation, which is a well-studied optimization problem in deep CNNs. The main contribution of the paper is to show that the additivity property of deep neural network has a significant impact on the accuracy of the output error. The paper also provides a theoretical analysis of the effect of coarse quantization on the performance of the layers. ","This paper proposes a new quantization methods for deep neural networks. The authors propose 2-bit quantization, which is a variant of high rate compression, i.e., 2-bits quantization. The main difference between the two methods is that the authors propose a joint framework for the optimal bit allocation problem. The proposed method uses Lagrangian Formulation to solve the optimization problem. It is evaluated on deep CNN ResNet-50 and achieves better accuracy loss than other methods. "
22572,SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"metric USED-FOR convergence. Wasserstein distance USED-FOR Wasserstein GAN ( WGAN ). auto - encoders CONJUNCTION WGANs. WGANs CONJUNCTION auto - encoders. framework USED-FOR auto - encoders. framework USED-FOR WGANs. encoder network CONJUNCTION generative network. generative network CONJUNCTION encoder network. encoder network PART-OF iWGAN. generative network PART-OF iWGAN. iterative primal dual optimization process USED-FOR generative network. iterative primal dual optimization process USED-FOR iWGAN. generalization error bound FEATURE-OF iWGANs. maximum likelihood estimation USED-FOR model. iWGAN COMPARE autoencoder GANs. autoencoder GANs COMPARE iWGAN. stopping criteria FEATURE-OF iWGAN. model USED-FOR convergence. model USED-FOR mode collapse. measurement of quality check EVALUATE-FOR model. benchmark datasets EVALUATE-FOR state - of - the - art. benchmark datasets EVALUATE-FOR iWGANs. Method are Generative Adversarial Networks ( GANs ), minmax two - player training of GANs, and inference WGAN ( iWGAN ) model. OtherScientificTerm is unstable training. ","This paper proposes a new metric for measuring the convergence of a Wasserstein GAN (WGAN) model. The authors propose a new framework for measuring WGANs, iWGAN, which is a combination of auto-encoders with a generative network and an iterative primal dual optimization process. The paper shows that the proposed model achieves better generalization error bound than autoencoder GANs in terms of maximum likelihood estimation. The model also achieves better performance on the measurement of quality check. ","This paper proposes a new metric to measure the convergence of GANs in the presence of unstable training. The Wasserstein distance is defined as the difference between the generalization error bound of an autoencoder GAN and that of a generative GAN. The authors propose a new framework for training WGANs that combines auto-encoders and the generative network in an iWGAN with an iterative primal dual optimization process. The model is evaluated on a number of benchmark datasets for state-of-the-art on the measurement of quality check, and the model is shown to be robust to mode collapse. The paper also proposes an inference WGAN (iWGAN) model based on maximum likelihood estimation."
22581,SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,coreference CONJUNCTION NLP. NLP CONJUNCTION coreference. Crowdsourcing COMPARE expert annotation. expert annotation COMPARE Crowdsourcing. classification tasks USED-FOR annotation. adjudication USED-FOR crowdsourcing. MPA USED-FOR sparsity. sparsity FEATURE-OF crowdsourcing environments. stick breaking process USED-FOR nonparametric partially pooled structure. large - scale crowdsourced anaphora dataset EVALUATE-FOR model. crowdsourcing setups FEATURE-OF model. annotation tasks USED-FOR classification. model USED-FOR annotation tasks. Task is anaphoric annotation. OtherScientificTerm is coreference chains. Generic is it. ,"This paper studies the problem of anaphoric annotation in the context of crowdsourcing, where the goal is to train a model that can be used for both coreference and NLP tasks. Crowdsourcing is an important problem in the community, as it can be difficult to train models that are robust to sparsity in crowdsourcing environments due to the MPA. In this paper, the authors propose a new model that uses a stick breaking process to learn a nonparametric partially pooled structure. The proposed model is evaluated on a large-scale crowdsourced anaphora dataset. The authors show that the proposed model outperforms the state-of-the-art on a variety of annotation tasks for classification and expert annotation.","This paper proposes a new model for anaphoric annotation. The authors propose a nonparametric partially pooled structure with a stick breaking process, where the coreference chains are non-parametric. The coreference and NLP are combined to improve the performance of Crowdsourcing compared to expert annotation on classification tasks. The model is evaluated on a large-scale crowdsourced anaphora dataset, where it is shown to outperform the expert annotation in terms of sparsity in several crowdsourcing environments (e.g., MPA). The authors also show that the model can be applied to other annotation tasks such as classification and adjudication."
22597,SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"Exploration FEATURE-OF sparse reward reinforcement learning. intrinsic motivation USED-FOR sparse extrinsic reward signal. drives USED-FOR stabilize learning. exploration CONJUNCTION stabilize learning. stabilize learning CONJUNCTION exploration. successor feature control ( SFC ) HYPONYM-OF intrinsic reward. It COMPARE methods. methods COMPARE It. local information USED-FOR intrinsic motivation. statistics over complete trajectories USED-FOR It. local information USED-FOR methods. DeepMind Lab CONJUNCTION DeepMind Control Suite. DeepMind Control Suite CONJUNCTION DeepMind Lab. VizDoom CONJUNCTION DeepMind Lab. DeepMind Lab CONJUNCTION VizDoom. environments EVALUATE-FOR scheduled intrinsic drive ( SID ) agent. pure visual inputs USED-FOR environments. pure visual inputs USED-FOR scheduled intrinsic drive ( SID ) agent. DeepMind Lab HYPONYM-OF pure visual inputs. VizDoom HYPONYM-OF environments. DeepMind Control Suite HYPONYM-OF environments. DeepMind Lab HYPONYM-OF environments. exploration efficiency EVALUATE-FOR SFC. Generic is signals. OtherScientificTerm are bonus rewards, and intrinsic drives. Method are mixture policy, and intrinsic and extrinsic task policies. ","This paper studies the problem of exploration in sparse reward reinforcement learning with intrinsic motivation in sparse extrinsic reward signal. The intrinsic reward is a successor feature control (SFC) which is a combination of intrinsic reward and bonus rewards. It is a mixture policy where the intrinsic rewards are learned from local information and the bonus rewards are learnt from the intrinsic drives. It uses statistics over complete trajectories to learn the intrinsic motivation and stabilize learning. It outperforms other methods that use local information to learn intrinsic motivation. The exploration efficiency of SFC is evaluated in three environments: DeepMind Lab, VizDoom and DeepMind Control Suite.","This paper proposes a new method for sparse reward reinforcement learning. It is based on intrinsic motivation for sparse extrinsic reward signal. The intrinsic reward is modeled as successor feature control (SFC) which is a combination of intrinsic reward and bonus rewards. It uses statistics over complete trajectories and local information for intrinsic motivation. The authors show that the proposed method outperforms other methods in terms of exploration efficiency and stabilize learning. Experiments are conducted on three environments: DeepMind Lab, VizDoom, and DeepMind Control Suite. "
22613,SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"video - sentence pairs USED-FOR model. visual and language representations USED-FOR latent correspondence. multi - level co - attention mechanism USED-FOR multimodal representations. Frame - By - Word interaction module CONJUNCTION Word - Conditioned Visual Graph ( WCVG ). Word - Conditioned Visual Graph ( WCVG ) CONJUNCTION Frame - By - Word interaction module. Word - Conditioned Visual Graph ( WCVG ) PART-OF mechanism. Frame - By - Word interaction module PART-OF mechanism. positional encodings USED-FOR visual - semantic representations. positional encodings USED-FOR Transformers. iterative message - passing USED-FOR positional encodings. positional encodings PART-OF approach. iterative message - passing USED-FOR visual - semantic representations. wMAN model COMPARE weakly - supervised method. weakly - supervised method COMPARE wMAN model. DiDeMo and Charades - STA datasets EVALUATE-FOR representations. Recall@1 accuracy metric EVALUATE-FOR wMAN model. Task is weakly - supervised video moment retrieval. OtherScientificTerm are temporal annotations, and temporal sequence. Material is DiDeMo dataset. ","This paper proposes a method for weakly-supervised video moment retrieval. The authors propose a multi-level co-attention mechanism to learn multimodal representations from video-sentence pairs. The proposed mechanism consists of a Frame-By-Word interaction module, Word-Conditioned Visual Graph (WCVG) and a Word-Conditional Visual Graph. The key idea is to learn the latent correspondence between visual and language representations. Transformers are learned using positional encodings for Transformers. The approach is based on iterative message-passing to learn visual-semantic representations. The representations are evaluated on DiDeMo and Charades-STA datasets on the Recall@1 accuracy metric. ",This paper proposes a multi-level co-attention mechanism for learning multimodal representations of video-sentence pairs. The proposed mechanism consists of a Frame-By-Word interaction module and a Word-Conditioned Visual Graph (WCVG) to capture the latent correspondence between visual and language representations. Transformers are learned using positional encodings for Transformers and temporal annotations for temporal sequence. Experiments on DiDeMo and Charades-STA datasets show that the proposed wMAN model outperforms the weakly-supervised video moment retrieval on the Recall@1 accuracy metric.
22629,SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"image - based rendering CONJUNCTION GAN - based image synthesis. GAN - based image synthesis CONJUNCTION image - based rendering. image - based rendering USED-FOR learned image - guided rendering technique. GAN - based image synthesis USED-FOR learned image - guided rendering technique. virtual showrooms CONJUNCTION virtual tours. virtual tours CONJUNCTION virtual showrooms. virtual tours CONJUNCTION digital inspection of historical artifacts. digital inspection of historical artifacts CONJUNCTION virtual tours. digital inspection of historical artifacts HYPONYM-OF virtual and augmented reality applications. virtual tours HYPONYM-OF virtual and augmented reality applications. virtual showrooms HYPONYM-OF virtual and augmented reality applications. handling of view - dependent effects PART-OF work. object - specific deep neural network USED-FOR view - dependent appearance. video USED-FOR proxy geometry. multi - view stereo USED-FOR proxy geometry. diffuse surfaces USED-FOR warping. specular highlights HYPONYM-OF view - dependent effects. deep neural network USED-FOR view - dependent effects. EffectsNet HYPONYM-OF deep neural network. pipeline USED-FOR view - dependent effects. composition network USED-FOR photo - realistic results. image - guided approach USED-FOR network. it USED-FOR appearance of captured images. real data EVALUATE-FOR approach. Generic are method, and estimations. OtherScientificTerm are 3D proxy, appearance, and diffuse images. ","This paper proposes a method to learn a 3D proxy geometry from video. The proxy geometry is learned from video by using a multi-view stereo, where each view is represented as a video. This proxy geometry can then be used to train a deep neural network to predict the view-dependent effects (e.g., specular highlights) on the diffuse surfaces of the video, which are then used for warping. The paper shows that the proposed method is able to achieve state-of-the-art performance on both synthetic and real-world datasets. ","This paper proposes a new method for image-based rendering and GAN-based image synthesis. The main contribution of the work is the handling of view-dependent effects in the form of specular highlights, which is a learned image-guided rendering technique based on an object-specific deep neural network. The proposed approach is evaluated on both virtual and augmented reality applications (e.g., virtual showrooms, virtual tours, and digital inspection of historical artifacts) and is shown to achieve better results than existing estimations. The paper also proposes a pipeline to capture view-dependant effects in a deep neural net, called EffectsNet. The authors show that it can capture the appearance of captured images, and the composition network can capture photo-real results. "
22645,SP:257d124367b1da9a595dc11a9df750d6bade298e,"sparse representation of model uncertainty USED-FOR deep neural networks ( DNNs ). diagonal correction of the Kronecker - factored eigenbasis PART-OF scalable Laplace Approximation scheme. scalable Laplace Approximation scheme USED-FOR model uncertainty. operation USED-FOR full Bayesian analysis. low - rank approximation USED-FOR spectral sparsity. spectral sparsity FEATURE-OF DNNs. low - rank approximation USED-FOR eigenbasis. Methods USED-FOR sparsification. Methods USED-FOR memory - wise tractable sampling computations. approach COMPARE methods. methods COMPARE approach. OtherScientificTerm are information form, Kronecker - factored eigenbasis, and information matrix. Task is inversion of the information matrix. ","This paper proposes a sparse representation of model uncertainty for deep neural networks (DNNs) based on diagonal correction of the Kronecker-factored eigenbasis in a scalable Laplace Approximation scheme. This operation is used for full Bayesian analysis. The authors show that spectral sparsity of DNNs with low-rank approximation can be reduced by using low-ranks approximation to the eigenbais of the information matrix. They also show that sparsification can be used for memory-wise tractable sampling computations. Finally, they show that their approach is more efficient than existing methods.",This paper proposes a sparse representation of model uncertainty for deep neural networks (DNNs). The authors propose a scalable Laplace Approximation scheme based on diagonal correction of the Kronecker-factored eigenbasis of the information form. This operation is used for full Bayesian analysis. The authors show that the spectral sparsity of DNNs is reduced by using a low-rank approximation of the eigenbases of the k-means of the input information matrix. The proposed approach is evaluated on memory-wise tractable sampling computations and compared to other methods for sparsification.
22661,SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"Minwise Hashing ( MinHash ) USED-FOR set similarities. compact high - dimensional data USED-FOR learning and searching. set similarities CONJUNCTION compact high - dimensional data. compact high - dimensional data CONJUNCTION set similarities. Minwise Hashing ( MinHash ) USED-FOR compact high - dimensional data. MinHash USED-FOR MinHash values. permutation ( hash function ) USED-FOR MinHash values. permutation ( hash function ) USED-FOR Permutation Hashing ( OPH ). strategies USED-FOR densification. densification HYPONYM-OF remedial strategy. Amortization Hashing ( AHash ) USED-FOR empty bins. Amortization Hashing ( AHash ) HYPONYM-OF load - balanced hashing. AHash COMPARE densification strategies. densification strategies COMPARE AHash. AHash COMPARE OPH. OPH COMPARE AHash. OPH CONJUNCTION densification strategies. densification strategies CONJUNCTION OPH. runtime efficiency EVALUATE-FOR densification strategies. runtime efficiency EVALUATE-FOR AHash. Material are high - dimensional data, and real datasets. OtherScientificTerm are bins, and unbalanced load. Task is false similarity computation. ","This paper studies the problem of false similarity computation in high-dimensional data. The authors propose Minwise Hashing (MinHash) to learn set similarities between two sets of set similarities and compact high-dense data for learning and searching. MinHash uses a permutation (hash function) to compute MinHash values for each pair of sets, and then uses Permutation Hashing to compute the MinHash value for each set. They show that AHash is more efficient than OPH and other densification strategies for densification. They also provide a remedial strategy called densification, which is a variant of Amortization Hashing for empty bins. ","This paper introduces Minwise Hashing (MinHash) for set similarities and compact high-dimensional data for learning and searching. MinHash uses a permutation (hash function) to compute MinHash values for each set. The authors propose a remedial strategy called densification, which is a variant of Permutation Hashing [1]. The authors show that AHash outperforms OPH and other densification strategies in terms of runtime efficiency. They also show that Amortization Hashing can be used for empty bins. The paper also shows that the performance of AHash is comparable to OPH, and that densification can be applied to unbalanced load. "
22677,SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,"feature extraction USED-FOR periodic signals. power generation CONJUNCTION industrial machine. industrial machine CONJUNCTION power generation. industrial machine CONJUNCTION robotic system. robotic system CONJUNCTION industrial machine. mechanized transportation vehicle CONJUNCTION power generation. power generation CONJUNCTION mechanized transportation vehicle. rotating shafts PART-OF robotic system. rotating shafts PART-OF mechanized transportation vehicle. multi - layer perceptron HYPONYM-OF methods. robust method USED-FOR features. machine learning architecture USED-FOR graph data. robust method USED-FOR phase shift data. cyclic permutation USED-FOR machine learning architecture. phase shift data USED-FOR features. cyclic permutation FEATURE-OF graph data. graph structure USED-FOR robust method. OtherScientificTerm are periodicity, shaft ’s rotation, Imprecise timing, phase shifts, and phase shift. ","This paper proposes a new method for feature extraction for periodic signals. The proposed methods are based on a multi-layer perceptron, which is a combination of rotating shafts in a robotic system and a mechanized transportation vehicle. The key idea is to use a robust method to extract features from phase shift data using cyclic permutation on the graph data. The authors show that the proposed method is able to recover the periodicity of the shaft’s rotation and the phase shifts. ","This paper proposes a novel method for feature extraction for periodic signals. The authors propose two methods: a multi-layer perceptron and a mechanized transportation vehicle with rotating shafts. The proposed method is based on the cyclic permutation of the graph data. The key idea is to use a robust method to extract features from the phase shift data using a machine learning architecture. Imprecise timing is used to measure the periodicity of the shaft’s rotation, and phase shifts are measured using the graph structure. "
22693,SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,them USED-FOR real world systems. controllability FEATURE-OF systems. precision USED-FOR real world systems. confidence score FEATURE-OF confidence oriented decoder. calibration technique USED-FOR faithful generation. calibration technique USED-FOR inference time. approach COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE approach. automatic metrics CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic metrics. structured data - to - text dataset EVALUATE-FOR approach. structured data - to - text dataset EVALUATE-FOR state - of - the - art approaches. WikiBio HYPONYM-OF structured data - to - text dataset. human evaluation EVALUATE-FOR approach. human evaluation EVALUATE-FOR state - of - the - art approaches. automatic metrics EVALUATE-FOR state - of - the - art approaches. automatic metrics EVALUATE-FOR approach. Method is Neural conditional text generation systems. OtherScientificTerm is variational Bayes objective. ,"This paper studies the problem of controlling the controllability of Neural conditional text generation systems. The authors propose a new calibration technique for faithful generation, which is based on the variational Bayes objective. The calibration technique is used to reduce the inference time and improve the precision of real world systems. They show that the proposed approach outperforms state-of-the-art approaches on a structured data-to-text dataset, WikiBio, and human evaluation. ","This paper proposes a new approach to improve the precision of real world systems by measuring the controllability of systems with respect to the confidence score of the confidence oriented decoder. The approach is evaluated on a structured data-to-text dataset, WikiBio, and compared to state-of-the-art approaches on automatic metrics and human evaluation. The authors propose a variational Bayes objective and a calibration technique for faithful generation. "
22709,SP:03307deac29173b2968fbd08f95fc77eb1f82410,Magnitude - based pruning USED-FOR pruning neural networks. magnitude - based pruning USED-FOR pruning modern architectures. Frobenius distortion FEATURE-OF linear operator. magnitude - based pruning USED-FOR Frobenius distortion. single layer optimization USED-FOR multi - layer optimization. single layer FEATURE-OF linear operator. lookahead pruning HYPONYM-OF pruning method. single layer optimization USED-FOR pruning method. method COMPARE magnitude - based pruning. magnitude - based pruning COMPARE method. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. networks EVALUATE-FOR magnitude - based pruning. networks EVALUATE-FOR method. VGG HYPONYM-OF networks. ResNet HYPONYM-OF networks. Method is neural networks. OtherScientificTerm is high - sparsity regime. ,"This paper proposes a new pruning method called lookahead pruning, which is based on the notion of Frobenius distortion in the linear operator of a linear operator with a single layer. The authors show that the proposed method is able to achieve better performance than magnitude-based pruning for pruning modern architectures. The main contribution of the paper is that it uses single layer optimization for multi-layer optimization instead of multi-layered pruning. The paper also provides a theoretical analysis of the performance of the method on a variety of networks including VGG and ResNet.","This paper proposes a new pruning method called lookahead pruning, which is based on magnitude-based pruning for pruning modern architectures with Frobenius distortion. The main idea is to use single layer optimization for multi-layer optimization of the linear operator, and then use the magnitude of the single layer for the Frobius distortion in the high-sparsity regime. The proposed method is evaluated on two networks, VGG and ResNet."
22725,SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"parallel workers PART-OF graph. technique USED-FOR decentralized SGD. quantized communication USED-FOR technique. quantized communication USED-FOR decentralized SGD. asymptotic rate EVALUATE-FOR algorithm. Moniqua COMPARE algorithm. algorithm COMPARE Moniqua. full - precision communication USED-FOR algorithm. Moniqua COMPARE quantized decentralized algorithms. quantized decentralized algorithms COMPARE Moniqua. wall clock time EVALUATE-FOR Moniqua. bit - budgets FEATURE-OF Moniqua. 4 - bits - per - parameter communication USED-FOR Moniqua. CIFAR10 EVALUATE-FOR VGG16. Method is Decentralized stochastic gradient descent ( SGD ). OtherScientificTerm are memory, biased or linear quantizers, and convergence. Task is non - convex objectives. ","This paper proposes Decentralized stochastic gradient descent (SGD), a technique for decentralized SGD with quantized communication between parallel workers in a graph. The authors show that the proposed algorithm has an asymptotic rate of $O(\sqrt{T})$ with full-precision communication, which is much faster than Moniqua with 4-bits-per-parameter communication. The paper also shows that the algorithm has a better wall clock time than the existing quantized decentralized algorithms with bit-budget. ","The paper proposes Decentralized stochastic gradient descent (SGD), a technique for decentralized SGD with quantized communication between parallel workers in a graph. The authors show that the algorithm has an asymptotic rate of 1/\sqrt{n}^n with respect to the memory, and that the convergence is faster than biased or linear quantizers. The algorithm is evaluated on CIFAR10 with full-precision communication, and Moniqua outperforms other quantized decentralized algorithms in terms of wall clock time, bit-budget, and convergence. "
22741,SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"Method are reinforcement learning, and partial models. Generic are it, and they. Task is jointly modeling future observations. Material is images. ","This paper studies the problem of jointly modeling future observations in the context of reinforcement learning. The authors propose a new approach to this problem, where the goal is to learn a partial model that can be used to predict future observations from the current state of the art. The main idea is to use the partial model as a surrogate for the full model. The partial model is then used to train a full model that predicts future observations. The proposed approach is evaluated on a variety of datasets, and the results show that the proposed approach outperforms the baselines.","This paper proposes a method for jointly modeling future observations. The main idea is to combine reinforcement learning with partial models. The authors show that it is possible to jointly model future observations, and they show that they can achieve better performance than existing partial models on images."
22757,SP:c70479b2096a52584b242de58272ca8d8565feea,"succinct common representation of two correlated data variables USED-FOR conditional and joint generation tasks. variational autoencoder ( VAE ) model USED-FOR succinct common representation of two correlated data variables. distributed simulation CONJUNCTION channel synthesis. channel synthesis CONJUNCTION distributed simulation. information theoretic problems USED-FOR Wyner VAE model. channel synthesis HYPONYM-OF information theoretic problems. distributed simulation HYPONYM-OF information theoretic problems. common representation CONJUNCTION local representations. local representations CONJUNCTION common representation. Wyner VAE USED-FOR correlated data variables. mutual information FEATURE-OF regularization term. common representation PART-OF correlated data variables. local representations PART-OF Wyner VAE. shared concept HYPONYM-OF common representation. synthetic data CONJUNCTION real images. real images CONJUNCTION synthetic data. approach USED-FOR joint and conditional generation. real images USED-FOR style control. synthetic data USED-FOR style control. style control USED-FOR joint and conditional generation. VAE variants CONJUNCTION variational information bottleneck method. variational information bottleneck method CONJUNCTION VAE variants. model COMPARE VAE variants. VAE variants COMPARE model. model COMPARE variational information bottleneck method. variational information bottleneck method COMPARE model. succinct common representation USED-FOR generative. OtherScientificTerm are Wyner ’s common information, and data variables. ","This paper proposes a new variational autoencoder (VAE) model for conditional and joint generation tasks that uses a succinct common representation of two correlated data variables. The model is based on the Wyner VAE, which combines a common representation (the shared concept) with local representations (e.g., the common representation in the shared concept). The common representation is then used as a regularization term to ensure that the shared information between the two variables is not lost. The paper shows that the proposed model outperforms VAE variants as well as a variational information bottleneck method in both joint and conditional generation.  The paper also provides a theoretical analysis of the performance of the model on a variety of information theoretic problems such as distributed simulation and channel synthesis. ","This paper proposes a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed variational autoencoder (VAE) model is based on the common representation and local representations of the Wyner VAE model. The common representation is a shared concept. The local representations are the “Wyner’s common information” between the two data variables, and the mutual information between the data variables is a regularization term. The paper presents a number of information theoretic problems, including distributed simulation, channel synthesis, and style control on synthetic data and real images for joint and conditional generation. The model is shown to outperform VAE variants and a variational information bottleneck method for generative."
